title,authors,doi,year,abstract
Systematic Literature Review: Deep Learning and Machine Learning Analysis for Batik Peranakan Tionghoa Datasets,M. H. Widianto; M. Karmagatri; R. K. Widyasari; A. Darisman; H. Hasan,10.1109/ICORIS63540.2024.10903900,2024,"This research focuses on searching for Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) for Systematic Literature Reviews (SLR). After that, the use of Deep Learning (DL) and Machine Learning (ML) will be analyzed. This study also presents a dataset that makes it possible to utilize ML and DL. SLR results obtained 21 research papers focusing on ML and DL analysis (classification and pattern recognition). However, both algorithms also have their respective advantages and disadvantages. For this reason, this research presents the advantages and disadvantages of both ML and DL analysis for Chinese Peranakan Batik."
Developing an AI Readiness Model for Software Project Management: A Thematic Analysis,R. B. M. T. Bandara; R. Wickramarachchi,10.1109/ICARC64760.2025.10962923,2025,"Integrating emerging technologies, such as Artificial Intelligence (AI), Big Data, and Blockchain, is transforming software project management by optimizing processes, improving quality, and enabling greater automation. AI tools, including Generative AI, Machine Learning, and Natural Language Processing, are increasingly applied across project life cycle stages to address tasks such as resource allocation, stakeholder identification, documentation, and risk management. This study employed a systematic literature review to explore AI applications and key adoption determinants, including technical feasibility, organizational culture, data quality, and leadership support. Additionally, semi-structured interviews with project managers and leaders provided practical insights into organizational preparedness and AI applications. Thematic analysis of the findings identified critical readiness factors, such as technical feasibility, the ability to refine AI outputs, and effective prompt engineering. A comprehensive framework was developed to assess AI readiness across technical, organizational, and human dimensions. The proposed framework offers actionable insights for organizations to evaluate and improve their readiness for AI adoption systematically. This study provides a foundation for enhancing organizational preparedness and driving successful AI implementation by addressing the complexities of integrating AI into software project management."
The Influence of Agile Methodology (Scrum) on Software Project Management,F. Hayat; A. U. Rehman; K. S. Arif; K. Wahab; M. Abbas,10.1109/SNPD.2019.8935813,2019,"Software project management has main role in the Software industry. It includes different processes and knowledge areas. The triple constraint of the software project like time, cost and scope is directly dependent on the requirement of the project. Agile methodology is the iterative way for developing the software project for frequent changes, fast delivery and reduce risk. Software project management also plays important role in agile based software project. Agile methodology influence software project management at 10 knowledge areas. In this study we carried out survey from different software companies and it shows that almost every software company uses agile development (Scrum) and has a positive impact on the software project management."
Software Project Management Using Machine Learning Technique - A Review,M. Z. M. H; M. N. Mahdi; M. S. Mohd Azmi; L. K. Cheng; A. Yusof; A. R. Ahmad,10.1109/ICIMU49871.2020.9243543,2020,"Project management planning assessment is of great significance in project performance activities. The creation of project management cannot be effectively handled without a practical and rational strategy. This paper offers a large-scale review analysis of articles based on machine learning and risk evaluation management for software projects. The reviews are presented and classified into two groups. The first group covers project management analysis and survey articles. The second group contains works on the steps and experimental criteria that are widely used in the management of machine learning projects. The paper provides a deeper insight and an important framework for future work in the project risk assessment, highlights the estimation of project risk using machine-learning is more efficient in reducing the project's fault and provides a further way to reduce the probability chances effectively and to increase the software development performance ratio."
Quality Assurance for LLM-Generated Test Cases: A Systematic Literature Review,H. Edirisinghe; D. Wickramaarachchi,10.1109/SLAAI-ICAI63667.2024.10844968,2024,"The rapid advancements in artificial intelligence have transformed software testing, with Large Language Models (LLMs) emerging as powerful tools for automating test case generation. This paper explores Quality Assurance (QA) for LLM-generated test cases in black-box testing through a systematic literature review. Though LLMs are increasingly used for test case generation, challenges in ensuring their quality remain. Following PRISMA guidelines, relevant studies were selected from databases focusing on critical quality attributes, QA frameworks, metrics, and challenges. LLMs demonstrate high efficiency but face numerous issues. A recommendation for future research is given on addressing standardized metrics and improving human-AI collaboration for enhanced testing outcomes."
A Systematic Literature Review on AI-Based Recommendation Systems and Their Ethical Considerations,E. Masciari; A. Umair; M. H. Ullah,10.1109/ACCESS.2024.3451054,2024,"With the rise of social media, individuals face challenges in decision-making due to the abundance of options available. Recommender Systems (RSs) leverage Artificial Intelligence (AI) to provide users with personalized suggestions aligned with their preferences and interests. This study presents a systematic review of AI-based Recommender Systems, focusing on recent advancements and primary studies published between 2019 and 2024. While several review papers have addressed various aspects of RSs, the rapid evolution of AI techniques necessitates an updated review to capture the latest trends and innovations. We systematically gathered data from five major databases: IEEE, Springer, Science Direct, ACM, and Wiley. Through the PRISMA methodology, we selected 85 relevant studies. Our analysis addresses several key research questions: the types of datasets and data sources used, major application fields, prevalent machine learning and AI techniques, overall research productivity, and the limitations and future trends in AI-based RSs. Our findings indicate that advanced AI techniques, particularly those incorporating deep learning with multiple hidden layers and transformer models like BERT, significantly enhance the accuracy and effectiveness of Recommender Systems. Furthermore, we observed a trend towards integrating contextual and real-time data to improve recommendation relevance. Additionally, we discuss ethical considerations such as privacy, data security, bias, and transparency, emphasizing the need for responsible AI development to ensure fair and equitable recommendations. These insights can guide future research and development efforts in the field."
WIP: Generative and Custom Chatbots in Computer Programming Education and Their Effectiveness A Systematic Literature Review,S. H. Tanvir; G. J. Kim,10.1109/FIE61694.2024.10893425,2024,"This work-in-progress research-to-practice paper describes a systematic literature review on the use of chatbots in programming courses. We scoped existing peer-reviewed and published articles focusing on the strength of custom-built chatbots and generative AI chatbots in computer engineering education - specifically, courses that employ C++, Java, and Python programming languages. The purpose of this work is to obtain in-depth information about chatbots, with a focus on their functionality, application, and effectiveness. We were also interested in the extent to which learning taxonomies and frameworks are integrated with chatbots. This study adopted the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) framework to review n=20 articles. Based on the reviews, we provide a set of guidelines on how learning taxonomies and instructional design frameworks can be mapped while developing or implementing chatbots in programming courses."
Advancements in Deep Learning Optimization for Post-Earthquake Building Damage Detection A Systematic Review,S. Sonang; Y. Yuhandri; M. Tajuddin,10.1109/ICAISD63055.2024.10894853,2024,"This study conducts a systematic literature review (SLR) that focuses on optimizing deep-learning techniques for post-earthquake building damage detection. By employing the PRISMA protocol, this review aimed to collect, analyze, and summarize the findings of various studies published between 2014 and 2024. This research identifies and evaluates optimization methods such as hyperparameter tuning, transfer learning, and data augmentation, highlighting their effectiveness and implementation. Challenges such as data quality, computational costs, and model interpretability were also discussed. The findings indicate that while deep learning techniques significantly enhance the accuracy and efficiency of damage detection, further research is required to address existing challenges and improve model robustness in diverse earthquake scenarios."
A Systematic Short Review of Machine Learning and Artificial Intelligence Integration in Current Project Management Techniques,H. Sarwar; M. Rahman,10.1109/SEAI62072.2024.10674089,2024,"In the ever-evolving landscape of project management, integrating machine learning (ML) and artificial intelligence (AI) within contemporary project management software (PMS) is a transformative frontier that holds significant promise. As the complexity and scope of a project develop, conventional approaches can encounter difficulties in delivering the required agility and predictive skills. ML and AI provide sophisticated data analysis, predictive modeling, and automation capabilities that enable enterprises to efficiently utilize resources, proactively identify hazards, and effectively respond to evolving needs. This connection provides project managers with practical information and allows them to make decisions based on data in high-speed contexts. This short systematic literature review (SLR) rigorously analyzes the present level of ML integration in PMS. The inquiry involves formulating research inquiries, devising a comprehensive search plan, selecting rigorous studies, and integrating data according to a well-defined review methodology that is influenced by established norms. This short review outlines the primary domains in which ML improves project management, emphasizes the practical difficulties encountered, and reveals the growing patterns of incorporating cutting-edge technology. We present a strategic framework for guiding future research directions, providing a basis for practitioners and scholars to comprehend the significant influence of integrating ML and AI on project management. In essence, this study highlights the capacity of ML and AI to revolutionize project management by enabling the discovery of novel efficiencies, enhancing the decision- making process, and fundamentally altering the administration of projects in a world driven by data."
The Convergence of AI and Wireless Communication System in Modern Aquaculture: A Systematic Review,A. P. Junfithrana; A. K. Mahamad; W. M. Utomo; A. Suryana,10.1109/ICCED64257.2024.10983111,2024,"This research examines the synergy between artificial intelligence (AI) technology and wireless communication in aquaculture through the Systematic Literature Review (SLR) method. With the increasing demand for fishery products, aquaculture offers a sustainable solution to meet global food needs. The problem faced is the need for sustainable solutions to meet the growing demand for fishery products. The methods used include data collection from the Scopus database, the use of PRISMA diagrams for literature selection, and data analysis using RStudio, resulting in 36 documents analyzed. The research results show that AI and wireless communication technology have great potential in improving the efficiency and productivity of aquaculture. The integration of AI and wireless communication allows real-time monitoring of environmental conditions and fish health, increasing efficiency and productivity. The contribution of this research is to provide a comprehensive insight into how this technology can improve aquaculture yields and product quality, as well as identify challenges in its implementation."
Systematic Literature Review on Software Effort Estimation Using Machine Learning Approaches,P. Sharma; J. Singh,10.1109/ICNGCIS.2017.33,2017,"Accurate effort estimation is amongst the key activities in the software project development. It directly impacts the time and cost of the software projects. This paper presents a systematic literature review of software effort estimation techniques using machine learning. This review presents a discussion about the research trends in machine learning inspired software effort estimation. The results of the systematic review has concluded prominent trends of machine learning approaches, size metrics, benchmark datasets, validation methods etc. used for software effort estimation."
Systematic Literature Review and Bibliometric Analysis on Ethical Policies for Generative Artificial Intelligence (GAI) in Higher Education Institutions (HEIs),A. Arista; L. Shuib; M. A. Ismail,10.1109/ICIMCIS63449.2024.10956736,2024,"Generative Artificial Intelligence (GAI) has revolutionized higher education with features like instant feedback, resource and material creation, adaptive learning, interactivity, and more. However, GAI also brings with it a number of serious challenges that raise ethical and moral concerns regarding academic integrity. To carry out this transition process, it will be necessary to develop clear guidelines that adhere to integrity standards and ethical codes of higher education institutions. In addition to offering responses to a number of research questions, the purpose of this study is to further the current discourse surrounding the moral guidelines for GAI in higher education. Using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) technique and Bibliometric Analysis using VOSviewer, 37 related research studies were reviewed and evaluated. The results demonstrate that the number of articles published has been increasing over time, with the largest number of scientific publications occurring in 2024. This can be explained by the fact that, despite the fact that GAI offers benefits and drawbacks for the educational sector, particularly for higher education, it also makes clear how important it is for professionals and organizations to use GAI tools in order to handle ethical issues by continuously developing policies. By creating norms and policies, future ethical problems with GAI in education may be more accurately predicted. The engagement and dedication of academic experts and other relevant stakeholders are essential to achieving this goal of increasing public awareness and effectively utilizing GAI technology."
Exploring the Impact of Generative AI on Customer Engagement in Digital Marketing,R. Wiputra; M. Rafindio; A. R. Kurniawan; A. R. Rahardjo,10.1109/ICIC64337.2024.10956926,2024,"Generative Artificial Intelligence (Gen AI) can increase the impact of digital marketing. By adopting Gen AI, marketers can create dynamic content tailoring to personalized strategies to optimize customer interactions, which increases engagement and sales. Despite challenges such as data privacy concerns and the potential loss of human interaction, the advantages of Gen AI in boosting customer engagement are substantial. Therefore, we did a systematic literature review using the PRISMA methodology to reveal how Gen AI improves digital marketing. The review highlights the role of gen AI in digital marketing and how it affects the tools marketers use in marketing. The urgency of adapting gen AI in digital marketing is to gain a benefit over competitors and tackle the challenges. Based on Bowden’s customer engagement framework, our findings reveal that Gen AI significantly impacts customer delight and affective commitment, which are critical components of long-term customer loyalty. Additionally, this review examines how gen AI increases customer engagement by affecting their environments and manipulating the tools integrated by AI to increase the likelihood of customer delight and affective commitment."
Identifying the Factors Affecting Student Academic Performance and Engagement Prediction in MOOC Using Deep Learning: A Systematic Literature Review,S. Rizwan; C. K. Nee; S. Garfan,10.1109/ACCESS.2025.3533915,2025,"The increasing reliance on Massive Open Online Courses (MOOCs) has transformed the landscape of education, particularly during the COVID-19 pandemic, where e-learning became essential. However, the effectiveness of MOOCs in enhancing student academic performance and engagement remains a key challenge, compounded by high dropout rates and low retention. This study presents a systematic literature review (SLR) conducted over a five-year period (2019–2024) to identify factors affecting student academic performance and engagement prediction in MOOCs, utilizing Deep Learning (DL) methods. The review follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, systematically analyzing articles from five major academic databases: ScienceDirect, SpringerLink, Scopus, Taylor & Francis, and Wiley Online. A total of 70 articles were selected for in-depth analysis, focusing on key predictors of student performance and engagement, including demographic data, behavioral patterns, learning activities, and clickstream data. The review highlights the capabilities of DL techniques in predicting student outcomes, such as retention, dropout, and engagement, offering valuable insights for educators and policymakers aiming to improve MOOC-based learning environments. By conducting SLR using PRISMA model, we identified research findings and gaps by proposing a conceptual framework for developing future personalized and adaptive e-learning environment for the inclusive MOOC based hard of hearing and low vision learners. This paper concludes by discussing implications for future personalized and adaptive e-learning environments and the necessity of comprehensive teacher training programs to navigate these evolving educational technologies."
"Discussing the present, past, and future of Machine learning techniques in livestock farming: A systematic literature review",R. Roy; M. M. Baral; S. K. Pal; S. Kumar; S. Mukherjee; B. Jana,10.1109/COM-IT-CON54601.2022.9850749,2022,"The digital revolution of livestock production has converted various planning functions into artificially intelligent systems to obtain information from different sources. Machine learning (ML), a subset of artificial intelligence, has a high potential for handling multiple difficulties for the organisations of information-based systems. The current study provides insights on ML in livestock farming by comprehensively reviewing recent academic articles using keyword combinations and in full compliance with PRISMA guidelines. This article extracted 216 articles from the literature review and, after the proper selection process, only considered 144 articles for further study. This study presents the past, present, and future of ML in livestock farming. This study will act as a source of information in both the research world and industry to know the future of ML in livestock production."
"ESG Score Prediction Using Machine Learning, Gap Analysis",K. M. Edhrabooh; M. Alromaihi; F. Binzaiman; M. AlShammari,10.1109/ICDABI63787.2024.10800665,2024,"This study aims to identify the main research gaps in Environmental, Social, and Governance (ESG) score prediction using Machine Learning (ML). To achieve this, a qualitative approach is adopted based on a Systematic Literature Review (SLR) using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, alongside gap analysis employing the seven research gaps model developed by Miles. Considering the detailed gap analysis conducted above, several research problems can be derived from the identified practical knowledge, empirical, and population research gaps for further analysis and future studies: the lack of proper input features, the absence of novel ML models, the lack of diverse datasets testing, inadequate market diversity in testing, and a lack of recent years testing. This study not only contributes to the body of knowledge by finding the main gaps identified in recent studies but also has the potential to inspire researchers to address these gaps and potentially revolutionize the field, thereby fostering innovation and advancement in ESG score prediction using ML. By addressing these gaps, researchers can significantly enhance the accuracy and reliability of ESG score prediction models, leading to more informed decision-making in the field."
A Systematic Review on Application of Multimodal Learning and Explainable AI in Tuberculosis Detection,B. Nansamba; J. Nakatumba-Nabende; A. Katumba; D. P. Kateete,10.1109/ACCESS.2025.3558878,2025,"Physicians rely on various data sources when diagnosing Tuberculosis (TB). This includes the patient’s historical data, demographic data, clinical laboratory results, and imaging data. Traditionally, the application of machine learning and deep learning in detecting TB has focused more on using single modes of data. This constrains the capabilities of the artificial intelligence (AI) techniques to replicate the clinical practice of incorporating multiple sources of information in decision-making. Recent advancements in deep learning and machine learning have enabled the integration of multimodal data which has led to the development of applications that more accurately reflect the clinician’s approach. However, the operations of deep learning techniques are still blackbox in nature, which makes it hard to understand their internal work mechanisms. As a result, it is necessary to incorporate explainable AI techniques to assist AI model users understand how the models make decisions. In this paper, we carried out a systematic review of two areas: First, we reviewed recent studies on the application of multimodal learning in TB detection. Here we have provided a summary of the public datasets used in the studies, data modalities used, the fusion techniques, and finally identified AI techniques that can be used with multimodal data. Then we looked at papers that used explainable AI techniques in TB diagnosis and prognosis. This study followed PRISMA guidelines to ensure replicability and accurate reporting of the main findings of the reviewed studies. To stay up-to-date with the state of the art, we specifically examined papers published between 2019 and June 2024. We reviewed thirty-one journal and conference papers we found using Web of Science, Scopus and Pubmed databases. The review indicated that models trained on multiple data modalities outperformed those trained on single data modalities. This is due to the additional information extracted from each data modality. Therefore, multimodal learning can improve clinical decision-making and TB diagnostic precision, but faces challenges like insufficient datasets and interpretability issues in model prediction processes."
Machine Learning Approaches for Fake Reviews Detection: A Systematic Literature Review,M. Ennaouri; A. Zellou,10.13052/jwe1540-9589.2254,2023,"These days, most people refer to user reviews to purchase an online product. Unfortunately, spammers exploit this situation by posting deceptive reviews and misleading consumers either to promote a product with poor quality or to demote a brand and damage its reputation. Among the solutions to this problem is human verification. Unfortunately, the real-time nature of fake reviews makes the task more difficult, especially on e-commerce platforms. The purpose of this study is to conduct a systematic literature review to analyze solutions put out by researchers who have worked on setting up an automatic and efficient framework to identify fake reviews, unsolved problems in the domain, and the future research direction. Our findings emphasize the importance of the use of certain features and provide researchers and practitioners with insights on proposed solutions and their limitations. Thus, the findings of the study reveals that most approaches focus on sentiment analysis, opinion mining and, in particular, machine learning (ML), which contributes to the development of more powerful models that can significantly solve the problem and thus enhance further the accuracy and efficiency of detecting fake reviews."
"Utilization of Deep Learning and Machine Learning Algorithms in Detection, Segmentation, and Classification of Brain Metastasis: A Systematic Literature Review",D. Kaur; J. Singh,10.1109/ICICAT62666.2024.10923420,2024,"One-tenth to around a quarter of cancer patients lose their lives to Brain Metastasis (BM) globally. Accurate neuroimaging of BM is essential to encourage early detection, decide treatment options, and increase survival rates. This systematic literature review (SLR) examines the utilization of machine/deep learning algorithms and imaging modalities in the detection, segmentation, and classification of BM. The study adheres to the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analysis Statement) and meticulously evaluates 32 relevant articles spanning from 2018 to 2023. The investigation explores scholarly databases including Google Scholar, SCOPUS, Web of Science, PubMed, SCISPACE, IEEE, Science Direct, Hindawi, and EBSCOhost to pull out relevant studies. The four aspects viz. employment of various imaging modalities, utilization of machine/deep learning algorithms, evaluation of performance metrics, and investigation of software tools for the identification of BM considered in this study. The review identifies Convolutional Neural Networks (CNN) as the most frequently utilized algorithm with the highest AUC-ROC (0.98) and demonstrates its efficacy for BM detection. Furthermore, out of 32, 15 articles highlight the significance of contrast-enhanced T1weighted images (T1CE/ T1c). This SLR unearths the transformative impact of machine/deep learning algorithms and emphasizes the importance of imaging modalities for enhancing the accuracy of diagnostics."
Applications of Artificial Intelligence in Conversational Agents: A Systematic Literature Review of AI in Chatbots,M. J. Samonte; R. D. C. Arlando; N. A. P. Joquiño; J. B. Manongas; J. O. Poblete,10.1109/SEAI62072.2024.10674229,2024,"Artificial Intelligence in Chatbots has been growing interest to a lot of people in different industries and fields. A software application that could hold a conversation with a human agent through text or text-to-speech. However there has been no clear definition of how AI is applied to these chatbots. This study aims to analyze on the applications of AI in Chatbots and understands its influence on diverse fields. The goal is to gather data on the fields in which AI chatbots are mostly utilized and understand the techniques, methods, and relevancy in applying AI chatbots in that field. The methods used in the study is the Systematic Literature Review (SLR) approach to address the proposed research question and the Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) approach to help in the selection and acquisition of studies and articles. Findings show that the most common application of AI is Natural Language Processing (NLP), Machine Learning (ML), and AI Algorithms. The study also discovered a significant increase in published research about AI Chatbots every year. In the different fields, results show that Customer Support, Finance, and Education are the three fields that had the most studies. Chatbots have a better opportunity of being incorporated into other fields, as chatbots improve the workload and offer a more efficient method for handling the large workload. For future research, it is recommended to find more studies on the topic to validate and check the variety of applications of AI chatbots in various fields."
Research and Application of GPT-Based Large Language Models in Business and Economics: A Systematic Literature Review in Progress,Y. Han; J. Hou; Y. Sun,10.1109/ICOCO59262.2023.10397642,2023,"Represented by ChatGPT and GPT-4, Large Language Models (LLM) based on the Generative Pre-trained Transformer (GPT) have revolutionized the capability of Artificial Intelligence (AI) in natural language processing. In the fields of business and economics, large amounts of research and applications of GPT-based LLMs have been developed and published to automate tasks that mandate advanced human-machine interaction. Nevertheless, there has not been a systematic literature review on GPT-based LLMs in business and economics. To fill this gap, we present our in-progress literature review in this paper focusing on these two related fields. This paper analyzed 30 published research articles and delineated the trends in research, application, prompt engineering and ethical considerations. Our goal is to provide a research framework as well as an application guideline for the fast-growing audience of GPT and LLMs in business and economics. Results of the literature review indicate that many studies are: (1) engaged in creating new applications of GPT-LLM; (2) empirical-qualitative research based on evidenced-oriented data sources; (3) applying diverse methods of prompt engineering; (4) concerned about ethical challenges of GPT-based LLMs."
A Systematic Review on Machine Learning (ML) and Artificial Intelligence (AI) in Understanding and Assessing Women's Health,J. Yeboah; S. Bampoh; F. A. Yeboah; I. K. Nti,10.1109/CSCI62032.2023.00242,2023,"While depression remains a prominent health concern with its associated difficulties, recent years have witnessed the utilization of artificial intelligence (AI) and machine learning (ML) to analyze extensive datasets, thereby enhancing the detection and diagnosis of women's health risks. We conducted a systematic review of AI and ML applications in the context of women's health. This review was carried out using various databases, including PubMed, Scopus, ACM Digital Library, Web-of-Science, and IEEE Explore Digital. We employed specific search terms related to mental health and ML, supplementing our database searches with manual exploration. Our inclusion criteria encompassed articles in English published between 2010 and 2022, focusing on machine learning and artificial intelligence applications and algorithms in the domain of women health, particularly in relation to depression among women. Initially, we identified a total of 495 records based on abstract searches. After removing duplicates, 425 unique reports remained, of which 380 were subsequently excluded following abstract screening. We then assessed 45 full-text articles for eligibility, revealing a wide range of methodologies and outcomes. Notably, the results indicated a high level of accuracy in risk classification, exceeding 90%. Our findings provide valuable perspectives on the implementation and application of AI and ML within the domain of women's health. These insights underscore the potential of these technologies to propel advancements in interventions related to women's health. In summary, we present initial insights and propose future pathways for harnessing AI and ML technologies to tackle depression challenges to enhance women's health on a global scale."
Online Solution Based on Machine Learning for IT Project Management in Software Factory Companies,A. H. Marchinares; C. R. Rodriguez,10.1109/CICN51697.2021.9574682,2021,"Project Portfolio Management is relevant for the growth of companies since it favors planning. Project Portfolio Management manages the resources to plan, control, and execute projects and obtain the strategic objectives of the organizations. In Project Portfolio Management, a large amount of data is forged, important for planning new projects in companies; therefore, the need arises to create models that help process and interpret the data. In this context, Machine Learning is presented as a technological enabler that allows a system, by itself and in an automated way, to learn to discover trends, patterns, and relationships between data; it is an engine of digital transformation of business and that organizations are embracing. Therefore, this article aims to compile and review proposals made to implement machine learning in the management of the project portfolio and apply algorithms that allow the development of models that help in the management and evaluation of projects to be developed in a Software Factory. The CRISP-DM methodology is applied to process the data of costs, times, and types of Projects; the Python programming language is used, the dataset corresponds to a Software Factory. The results validate the models implemented using Machine Learning algorithms, such as regression and decision trees, and thereby obtain the best model for predictions, establishing the correlation between variables and the benefit to be achieved. It is concluded, the implementation of Machine Learning improves the IT Project Portfolio Management, helping to identify which projects are more profitable and beneficial."
Brain Tumor Classification Based on Deep Learning Algorithms: A Systematic Literature Review,M. R. Ramadhani; I. Soesanti; I. Hidayah,10.1109/ICITDA60835.2023.10427361,2023,"Along with the times, the medical field has changed and developed with the help of technology that is very helpful and makes it easier for both medical personnel and patients. The application of Artificial Intelligence (AI) also helps in the medical field, especially in medical imaging of Magnetic Resonance Imaging (MRI) images in the diagnosis of brain tumors, but in practice, there have been quite a number of studies trying to apply the application of AI to the diagnosis of brain tumors with results which is quite good with a variety of approaches to Deep Learning methods. Therefore, this is a challenge for the authors and this study aims to apply the right and best deep learning method to produce a higher quality brain tumor classification than previous studies so that the diagnosis of brain tumor disease in patients can improve even better. To identifying and extracting relevant and important data from recent studies, we conducted a systematic literature review using the PRISMA 2020 guidelines. The comparative analysis of recent studies correlated with brain tumor classification using deep learning techniques is considered in this systematic literature review. The outcome of this paper states the various research gaps identified from the systematic literature review."
Review of Hospital Outpatient No-Show Explainable Prediction Using Machine Learning,K. Toffaha; M. C. E. Simsekler; M. Omar,10.1109/ICTMOD63116.2024.10878195,2024,"Patient no-shows present significant challenges and costs to healthcare systems, prompting a focused exploration of hospital outpatient no-show prediction using explainable machine learning (ML) in this review. The systematic methodology aligns with Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, ensuring transparency, replicability, and thoroughness. A comprehensive search across major literature databases yielded 92 potential studies on ML models for outpatient no-show prediction. Following screening and eligibility assessments, 51 studies met inclusion criteria, emphasizing outpatient no-shows, leveraging explainable ML, featuring predictive modeling, and reporting quantitative metrics. Examining the publication distribution from 2010 to 2024 reveals a discernible upward linear trend, indicating the growing relevance of this research domain. The preference for more interpretable ML models over time, such as regression, decision trees, and ensemble methods, underscores their transparency and actionability. Despite challenges like limited model generalizability and clinician hesitation, proposed solutions, including collaborative learning and feature engineering pipelines, aim to enhance the reliability and applicability of outpatient noshow prediction models. This comprehensive strategy addresses existing challenges, facilitating the effective integration of explainable ML in healthcare settings and contributing to improved outpatient appointment attendance. In conclusion, this systematic review provides a robust foundation for understanding and implementing explainable ML approaches in predicting hospital outpatient no-shows, offering valuable insights for healthcare practitioners and researchers alike."
The Implementation of Artificial Intelligence for Online Review: A Systematic Literature Review,D. Syamsuar; Marcello,10.1109/ICIMTech63123.2024.10780904,2024,"In the digital era, many researchers are examining the usefulness of online reviews, which show the importance of online reviews. This has led many researchers, practitioners, and companies to implement Artificial Intelligence (AI) technology for automating and optimizing various aspects of online reviews. The implementation of AI for online review has become a hot topic. However, there is a necessity to classify and synthesize existing insights to identify the latest trends and opportunities for further research in the future. Therefore, this study will conduct a systematic literature review (SLR) to address this gap by examining the current state of research in the implementation of AI for online review. The study utilized the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) flow diagrams by submitting 3 Research Questions (RQ). The final results obtained from 27 selected primary studies show that the research trend in the implementation of AI for online review is still highly relevant today, and retail is the most utilized industry by researchers. In addition, it was revealed that current research focuses on five topics, i.e., sentiment analysis, fake detection, information extraction, dataset analysis, and review helpfulness. Hopefully, this study can contribute to the academic side for future research and practical side for insightful information in implementing AI for online review."
AI-Powered IT Project Management: Analyzing the Effectiveness of Advanced Project Management Tools to Ensure Project Efficiency,J. K. Das; I. Elegbe; L. Coffie; R. Khadka; L. Chen; Y. Ji,10.1109/SoutheastCon56624.2025.10971718,2025,"The integration of Artificial Intelligence (AI) into project management has significantly transformed traditional methodologies, particularly within the Information Technology (IT) sector. This study investigates the effectiveness of AI-powered tools, specifically Asana, Jira, and Monday.com to improve project management efficiency through automation, resource optimization, and decision-making processes. Using a systematic literature review and quantitative analysis, the research highlights the capabilities and limitations of these tools in the management of complex IT projects. The study findings reveal that AI-powered platforms facilitate real-time tracking, predictive analytics and dynamic resource allocation, which collectively contribute to reduced project timelines and improved outcomes. However, challenges such as data quality, integration complexities, and the need for skilled personnel remain prevalent. The study was summarized with strategic recommendations for organizations to incorporate automation, leverage predictive analytics, and improve data integrity, thus maximizing the benefits of AI in project management. This research underscores the critical role of AI in modernizing project management practices, which ultimately leads to increased productivity and project success rates in the IT domain."
Application of Infrared Thermography and Artificial Intelligence in Healthcare: A Systematic Review of Over a Decade (2013–2024),J. Vicnesh; M. Salvi; Y. Hagiwara; H. Y. Yee; H. Mir; P. D. Barua; S. Chakraborty; F. Molinari; U. Rajendra Acharya,10.1109/ACCESS.2024.3522251,2025,"Infrared thermography (IRT) is a non-invasive, radiation-free imaging technique that uses an infrared (IR) camera to record and produce an image using IR radiation emitted from the body. IRT imaging has shown promise as a screening method for breast cancer, diabetic foot ulcers, and dry eye disease, among other medical disorders. The aim of this systematic review is to present a complete overview of the applications of artificial intelligence (AI) techniques with IRT imaging for medical decision support systems over the course of the last ten years (2013–2024). Several scientific databases, including PubMed, IEEE, and Google Scholar, were searched using Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. After meeting the requirements for inclusion, 131 papers were selected. The reviewed studies demonstrated how various AI techniques, including deep learning and classical machine learning, can be used to develop automated diagnosis systems using IRT images. The efficacy of these AI systems differed depending on the medical field; for example, they could identify dry eye disease with 90–100% accuracy, classify diabetic foot ulcers with 85–95% accuracy, and detect breast cancer with 80–100% accuracy. This review highlights the value of IRT imaging in early disease detection, especially when combined with AI techniques. This work discusses challenges in using deep learning (DL) models in healthcare, including data scarcity and ethical considerations. It also, proposes three main recommendations: dataset standardization for ethical data management, clear governance models for ethical practices, and the use of Multimodal Large Language Models (MLLMs) to address explainability issues."
Being Proactive for Responsible AI: Analyzing Multiple Sectors for Innovation via Systematic Literature Review,L. J. Wiese; D. S. Schiff; A. J. Magana,10.1109/ETHICS57328.2023.10154947,2023,"Background: Questions surrounding the ethics of artificial intelligence (AI) have been debated for decades [1]. However, in recent years there have been multiple initiatives, scholarly reviews, and policy documents to identify and define ethical issues in play [2]. The efforts to bring high-level principles to applicable practice are complex and can be lost in translation [3]. Moreover, a call to be proactive, rather than reactive, stems from a deduction of intentions behind responsible innovation, value-centric design principles, education efforts, and representative data management techniques. Contemporary applications of AI are complex and difficult to explain, edit, and deal with once integrated in a natural system [4] [5]. Therefore, the analysis conducted within this systematic literature review (SLR) will clarify methods to promote and engage practice on the front end of ethical and responsible AI. As such, the research question is explored: How does each helix in the Quintuple Innovation model address responsible and ethical AI technology with anticipatory or proactive approaches? Methods: To conduct this ongoing research, an adaptation of the PRISMA framework and Hess & Fore's 2017 methodological approach guides the SLR [6] [7]. We included journal articles that were written in English and published between 2018-2023. The collected studies aim to examine how academic scholarship approaches to responsible AI within academia, government, industry, civil society, or the natural environment (the Quintuple Helix). The Web of Science, Google Scholar, and PhilPapers databases were used to identify a set of prominent publications in this field: AI & Society, Nature Machine Intelligence, Minds and Machines, IEEE Transactions on Technology and Society, AI and Ethics, Science and Engineering Ethics, and Communications of the ACM. A key limitation of this study is that it cannot gather the entirety of literature written about the topics of proactively promoting ethical AI due to the vast size and definitional complexity of the associated fields. These inclusion criteria allow the researchers to manage the data and draw meaningful insights from the most current thinking that is reflected in the rapid development of AI innovation we see today. Results and discussion: This poster will present preliminary results and the theoretical framework that guided the qualitative coding process. Additionally, this poster will serve as a forum to collect experts' opinions about what they would like to see from this SLR dataset, and how we can incorporate those elements into our coding. As a result, this data will be able to inform future work to investigate multiple gaps in the literature. For instance, U.S. Government work not protected by U.S. copyright this study will result in a theoretical framework that identifies proactive approaches to responsible and sustainable AI aligned with the five sectors for innovation. Inspired from [8], the effects of investments in education, and other sectors, will be mapped as a chain of responsible AI innovation across all innovation sectors. Finally, we can draw informed conclusions about the use and misuse of experts in AI, ethics, education, and policy. By working towards these objectives, we can see how the interdisciplinary field has made (or not made) a collective effort toward promoting responsible AI-filling a gap in the literature that highlights proactive approaches, rather than reactive. In conclusion, this data will inform experts across multiple domains about how to approach and organize a concerted effort to promote ethical and responsible AI in a pragmatic way."
Artificial Intelligence and Crisis Management a Systematic Literature Review using PRISMA,I. Dakhli; A. Sedqui; M. Derrhi; B. Karroumi,10.1109/IRASET60544.2024.10549274,2024,"This research presents the effects of artificial intelligence (AI) on crisis and risk management in the industrial sector, offering an international view. We examine how AI can improve industries' preparedness and responsiveness to unforeseen challenges. Using Preferred Reporting Items for Systematic Reviews (PRISMA) standards, our systematic review compiled data from research conducted between 2019 and 2023, mainly sourced from the Web of Science and Scopus databases. The analysis, which encompasses 66 articles, reveals a notable concentration of research worldwide. A major challenge identified is the alignment of AI capabilities with the specific crisis management requirements of industries. The results suggest that the advancement of AI is essential for developing innovative strategies in Industry 4.0 and highlight the importance of AI innovation and collaborative practices in strengthening industrial resilience to crises. The aim is to explore how integrating AI into industrial strategies can not only prevent risks but also improve crisis management."
A Systematic Literature Review of Cyber Security Monitoring in Maritime,R. Vaarandi; L. Tsiopoulos; G. Visky; M. Ur Rehman; H. Bahşi,10.1109/ACCESS.2025.3567385,2025,"In recent years, many cyber incidents have occurred in the maritime sector, targeting the information technology (IT) and operational technology (OT) infrastructure. One of the key approaches for handling cyber incidents is cyber security monitoring, which aims at timely detection of cyber attacks with automated methods. Although several literature review papers have been published in the field of maritime cyber security, none of the previous studies has focused on cyber security monitoring. The current paper addresses this research gap and surveys the methods, algorithms, tools and architectures used for cyber security monitoring in the maritime sector. For the survey, a systematic literature review of cyber security monitoring studies is conducted following the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) protocol. The first contribution of this paper is the bibliometric analysis of related literature and the identification of the main research themes in previous works. For that purpose, the paper presents a taxonomy for existing studies which highlights the main properties of maritime cyber security monitoring research. The second contribution of this paper is an in-depth analysis of previous works and the identification of research gaps and limitations in existing literature. The gaps and limitations include several dataset and evaluation issues and a number of understudied research topics. Based on these findings, the paper outlines future research directions for cyber security monitoring in the maritime field."
Implementation of Artificial Intelligence on Air Traffic Control - A Systematic Literature Review,R. Emha Abdillah; H. Moenaf; L. Fadullah Rasyid; S. Achmad; R. Sutoyo,10.1109/IMCOM60618.2024.10418350,2024,"Humans can now do jobs more efficiently, and many problems can be solved with the assistance of Artificial Intelligence (AI). AI also plays an important role in air traffic control because it can help improve the safety and efficiency of air travel. AI can be used for various tasks, such as predicting weather patterns, identifying potential conflicts, and recommending optimal routes. Machine learning can also be applied in air traffic control to help systems learn from data and improve performance. This research aims to show the role of artificial intelligence (AI) in the Air Traffic Control (ATC) system and explore ways to implement AI to improve the system's performance. This study reviewed literature by looking at relevant articles and journals to achieve the result. This research reviews previous literature based on keywords Such as Implementation, Air Traffic Control, and Machine learning. Based on the literature review results, the various aspects of air traffic control and management are increasingly dependent on AI and machine learning. This study found that 7 aspects of ACT have adopted AI and have the potential to continue to develop their use."
A Systematic Review and Analysis on the Potentials and Challenges of GenAI Chatbots in Higher Education,K. Wangsa; R. Sandu; S. Karim; M. Elkhodr; E. Gide,10.1109/ITHET61869.2024.10837608,2024,"AI in education has been advantageous for both learners and teachers. Yet, some key things require exceptional skills to make it accessible and fully functioning. This paper discusses the potential and drawbacks of AI and the impact of GenAI chatbots in higher education. We examined 20 scholarly articles from the PRISMA systematic review. The findings indicate the benefits of AI for higher education, including career opportunities, positive learning outcomes, personalised learning, learning support, detection of plagiarism, real-time feedback, and an enhanced teaching experience for teachers, while the drawbacks suggest limited technologies, privacy, and ethical concerns, plagiarism, loss of jobs, biased algorithms, and readiness and competencies are the challenges. We also explored generative AI chatbots to provide more comprehensive AI applications in higher education. This research is suitable for those who would like to investigate the usefulness of GenAI for higher education."
"Machine Learning Applications in Manufacturing—Challenges, Trends, and Future Directions",A. Manta-Costa; S. O. Araújo; R. S. Peres; J. Barata,10.1109/OJIES.2024.3431240,2024,"The emergence of Industry 4.0 (I4.0) has significantly transformed manufacturing landscapes, introducing interconnected, dynamic, and data-rich environments. This article focuses on the application of industrial machine learning (I-ML) within these evolving manufacturing contexts, exploring both the challenges and future prospects of its integration. A systematic literature review, following the preferred reporting items for systematic reviews and meta-analyzes (PRISMA) guidelines, forms the foundation of our analysis, characterizing the role of machine learning (ML) in modern manufacturing, its current challenges, and future trends. This research delves into the implications of I-ML in various manufacturing scenarios, including predictive maintenance, anomaly detection, and quality control, providing a comprehensive overview of practical applications along with an identification of related emerging technologies and trends. We also address the critical need for sustainable, reproducible, and reliable performance in industrial applications and explore strategies for overcoming barriers to ML adoption in the industry. Recommendations for future research directions are provided, aiming to bridge the gap between ML advancements and their practical, scalable implementation in industrial settings, paving the way to future research in the field. Lastly, we aim to contribute to the identification of challenges and future research directions for the ongoing digital transformation of manufacturing industries, offering insights into how ML can be effectively leveraged in the era of I4.0."
Perspectives on Teaching and Learning in Higher Education in the Era of Generative AI,H. Chithara; S. Madzvamuse,10.1109/IMITEC60221.2024.10851078,2024,"This paper presents a systematic literature review (SLR) exploring perspectives on teaching and learning in higher education within the context of Generative AI advancements. By focusing on key areas such as teaching methodologies, student engagement, and ethical challenges, this research offers a detailed analysis of the changes that AI technologies, like GPT-4 and DALL-E, bring to academic settings. The findings indicate that while Generative AI provides opportunities for personalized learning and adaptive teaching, it also introduces significant challenges related to academic integrity, algorithmic bias, and data privacy. The paper provides insights into how educators and institutions can embrace Generative AI while addressing its ethical complexities. The paper employed a qualitative approach where key themes were identified such as Transformation in teaching Methodologies, Student Engagement and Learning Outcome and Ethical Challenges. The researcher adopted a purposive sampling method where several papers were purposefully selected based on the research topic. The PRISMA framework also played a critical role in selecting the relevant papers."
The Role of Artificial Intelligence in Future Rehabilitation Services: A Systematic Literature Review,C. Mennella; U. Maniscalco; G. De Pietro; M. Esposito,10.1109/ACCESS.2023.3236084,2023,"Artificial intelligence technologies are considered crucial in supporting a decentralized model of care in which therapeutic interventions are provided from a distance. In the last years, various approaches have been proposed to support remote monitoring and smart assistance in rehabilitation services. Comprehensive state-of-the-art of machine learning methods and applications is presented in this review. Following PRISMA guidelines, a systematic literature search strategy was led in PubMed, Scopus, and IEEE Xplore databases. The search yielded 519 records, resulting in 35 articles included in this study. Supervised and unsupervised machine learning algorithms were identified. Unobtrusive capture motion technologies have been identified as strategic applications to support remote and smart monitoring. The main tasks addressed by algorithms were activity recognition, movement classification, and clinical status prediction. Some authors evidenced drawbacks concerning the low generalizability of the results retrieved. Artificial intelligence-based applications are likely to impact the delivery of decentralized rehabilitation services by providing broad access to sustained and high-quality therapy. Future efforts are needed to validate artificial intelligence technologies in specific clinical populations and evaluate results reliability in remote conditions and home-based settings."
Machine Learning Application for Classification of Rice Varieties: A Systematic Literature Review,Y. Cahyana; A. Setiawan; I. Sembiring; C. Dewi,10.1109/CENIM64038.2024.10882802,2024,"The diversity of rice creates challenges in identifying rice varieties. Rice quality is usually judged by price, which does not always reflect the actual quality. This research aims to identify the most optimal and efficient algorithm for rice classification. It can also highlight research gaps that can be used to develop further research questions. The approach to searching and selecting articles followed the PRISMA guidelines (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) are a set of evidence-based recommendations designed to improve the quality and transparency of reporting in systematic reviews and meta-analyses. The research was conducted through a systematic literature review that focused on factors important for identifying rice types based on shape. The process was divided into several steps: selecting the database, reviewing the objectives and research questions, creating a search strategy, establishing search criteria, defining inclusion and exclusion criteria, and extracting data. The results were examined and analyzed to answer the research questions. This research involved the classification of rice types using QNN (Quantized Neural Network). Quantized Neural Networks (QNN) offer high computational efficiency, making them ideal for edge devices, but this technology is still in the early stages of development and requires specialized hardware."
Software Development and Education: Transitioning Towards AI Enhanced Teaching,J. Israilidis; W. -Y. Chen; M. Tsakalerou,10.1109/EDUCON60312.2024.10578564,2024,"This paper investigates the impact of large language model (LLM) AI tools, such as ChatGPT and Copilot, on software development education, focusing on usability, efficiency, and effectiveness in real-world scenarios. The research employs a quantitative approach, utilizing a survey of 50 software developers with varying levels of experience. Preliminary findings suggest that AI tools have a positive influence on expediting coding tasks and automating text generation, particularly in the early stages of product development. Challenges related to customization, accuracy, and transparency, as well as concerns about their potential impacts on employment, personal privacy, and ethical boundaries, have been identified. Pointers and initial recommendations for transitioning to AI-enhanced teaching and optimizing interactions between learners and generative AI practices are provided."
"Evaluation Metrics for XAI: A Review, Taxonomy, and Practical Applications",M. A. Kadir; A. Mosavi; D. Sonntag,10.1109/INES59282.2023.10297629,2023,"Within the past few years, the accuracy of deep learning and machine learning models has been improving significantly while less attention has been paid to their responsibility, explainability, and interpretability. eXplainable Artificial Intelligence (XAI) methods, guidelines, concepts, and strategies offer the possibility of models' evaluation for improving fidelity, faithfulness, and overall explainability. Due to the diversity of data and learning methodologies, there needs to be a clear definition for the validity, reliability, and evaluation metrics of explainability. This article reviews evaluation metrics used for XAI through the PRISMA systematic guideline for a comprehensive and systematic literature review. Based on the results, this study suggests two taxonomy for the evaluation metrics. One taxonomy is based on the applications, and one is based on the evaluation metrics."
A Systematic Literature Review and Meta-analysis on Project Management Platform,Yunofri; N. B. Kurniawan,10.1109/ICITSI.2018.8695958,2018,"Every project implementation must involve its management process. Project management is a system that enables organizations to manage their projects. The development of information technology encourages the development of techniques and methodologies used in project management. This study aims to provide a detailed insight into the trends of project management with systematic literature review and has a particular focus on the project management platform. Therefore, it is necessary to understand the features, technology and architecture of the project management platform. Meta-analysis is needed to determine the optimal layer of project management platform architecture with principal component analysis techniques. Trends in the project management platform literature were analyzed from 2000 to 2018. The findings from the analysis highlighted some of the generic features that project management platforms should have such as schedule/monitoring management, process management, configuration/user management, document management, financial/cost management and resources management. By doing principal component analysis, the number of optimal layers in the project management platform is obtained four: application layer, service layer, data integration layer and management layer."
Big Data Governance Challenges Arising From Data Generated by Intelligent Systems Technologies: A Systematic Literature Review,Y. A. Bena; R. Ibrahim; J. Mahmood; A. Al-Dhaqm; A. Alshammari; M. Nasser; M. Nura Yusuf; M. O. Ayemowa,10.1109/ACCESS.2025.3528941,2025,"The exponential growth of intelligent systems technologies, including Artificial Intelligence (AI), Internet of Things (IoT), Machine Learning (ML), and Smart Connected Products, has intensified the difficulties of data governance. Organizations adopting these technologies face challenges in managing the volume, variety, and velocity of data while ensuring quality, security, compliance, and ethical integrity. This study investigates the state of big data governance (BDG) programs in organizations leveraging intelligent systems technologies, using a systematic literature review guided by PRISMA. A synthesis of insights from 74 peer-reviewed articles, conferences, and industry reports reveals both the significant benefits of BDG such as enhanced decision-making, operational efficiency, and regulatory compliance and persistent challenges, including fragmented governance frameworks, limited scalability, data quality issues, and ethical concerns. To address these gaps, this study proposes the big data governance maturity assessment model (BDG MAM), a novel framework developed to assess and enhance the maturity of BDG programs. The BDG MAM evaluates governance maturity across four key dimensions: people, process, data, and technology. It provides a structured roadmap for organizations to benchmark governance practices, prioritize improvements, and implement effective strategies. The model was validated through a pilot study conducted with a public higher learning institution, demonstrating its practical applicability and effectiveness in real-world scenarios. By bridging theoretical insights with practical implementation, this study advances academic discourse and provides practitioners with a robust approach to navigate the challenges of modern data governance practices, ensuring sustainable and effective management of heterogeneous data environment."
A Comparison of Artificial Intelligence-Based Methods in Traffic Prediction,P. Diamanta; G. Avila; M. I. Hudaya; E. Irwansyah,10.1109/ICCSAI53272.2021.9609739,2021,"Traffic plays an important role in our society as its state can affect individuals and industries in various ways. Traffic congestion can bring negative impacts to the society and can lead to bigger problems if let be without a solution to mitigate it. Thus, traffic prediction serves as a solution to said problem. In this systematic literature review, AI-based traffic prediction methods are compared in order to find which ones serve as the better solutions for predicting traffic. Using the PRISMA Flowchart methodology, which helps authors systematically analyze relevant publications and improve the quality of reports and meta-analyses. By conducting further analysis on the screened references, it is found that the methods that integrates Convolutional Neural Network or Recurrent Neural Network with Long Short-Term Memory along with error-recurrent Neural Network proved to be good candidates for an optimal traffic prediction."
User-Centered Design in AI Applications: A Systematic Literature Review,B. Wongso; K. N. Lienaka; V. Firstian; Y. Magdalena,10.1109/ICIMTech63123.2024.10780823,2024,"This study aims to look at the implementation of User Centered Design (UCD) in Artificial Intelligence (AI) application development, the methods used, and the results of using UCD approach in AI application development. We conducted a systematic literature review using the PRISMA checklist, resulting in 21 studies selected based on the criteria of the studies containing UCD methods in AI development. The results reveal an increasing interest in UCD in AI application development, with most UCD implementation in AI occurring in the medical field. The study also highlights the prevalence of various UCD methods used, such as interviews, surveys, prototypes, and usability testing. Results shows that implementing UCD positively impacts the usability and user satisfaction levels in AI applications, as shown by improved performance metrics, compliance with design principles, and enhanced willingness by users to interact with the systems."
Design and Development of Machine Learning Technique for Software Project Risk Assessment - A Review,M. N. Mahdi; M. Zabil M.H.; A. Yusof; L. K. Cheng; M. S. Mohd Azmi; A. R. Ahmad,10.1109/ICIMU49871.2020.9243459,2020,"Accurate assessment of software project risk is amongst the key activities in a software project. It directly impacts the time and cost of software projects. This paper presents a literature review of designing & developing machine learning techniques for software project risk assessment. The results of the review have concluded prominent trends of machine learning approaches, size metrics, and study findings in the growth and advancement of machine learning in project management. Besides that, this research provides a deeper insight and an important framework for future work in the software project risk assessment. Furthermore, we demonstrated that the assessment of project risk using machine-learning is more efficient in reducing a project's fault. It also increases the probability for the software project's prediction and response, provides a further way to reduce the probability chances of failure effectively and to increase the software development performance ratio."
"Convergence of Blockchain, IoT, and AI for Enhanced Traceability Systems: A Comprehensive Review",Y. Saidu; S. M. Shuhidan; D. A. Aliyu; I. Abdul Aziz; S. Adamu,10.1109/ACCESS.2025.3528035,2025,"The need for sophisticated traceability systems has become essential in increasingly complex and globalized supply chains. The convergence of Blockchain (BC), Internet of Things (IoT), and Artificial Intelligence (AI) technologies offers promising solutions to enhance traceability systems across various sectors, particularly supply chain management (SCM). This paper presents a bibliometric and systematic literature review (SLR) to examine trends, research patterns, and methodologies in integrating BC IoT and AI into traceability systems. Bibliometric analysis of 530 documents from SCOPUS (2014–2024) identified key trends, while the SLR, conducted across multiple databases following PRISMA guidelines, refined the dataset to 43 peer-reviewed studies based on inclusion criteria. Recent research output has notably increased, focusing on agricultural supply chains and SCM, with India and China leading in publications. The analysis shows a predominance of experimental and hybrid methodologies, using Ethereum and Hyperledger Fabric as key platforms. Key trends include AI-driven analytics, real-time IoT data collection, and the need for secure, tamper-proof data by BC. However, interoperability, scalability, and standardization challenges hinder adoption. The paper proposes a four-layer framework for integrating BC, IoT, and AI to improve transparency, security, and efficiency and highlights the need for more empirical studies, industry-specific frameworks, and standardization to overcome existing limitations."
An investigation into the Impact of Artificial Intelligence on the Future of Project Management,A. Alshaikhi; M. Khayyat,10.1109/WiDSTaif52235.2021.9430234,2021,"The purpose of the study is to investigate the impact of Artificial Intelligence on the future of Project Management. This study provides detailed conceptual information about Artificial Intelligence and different perspectives. Artificial Intelligence is defined as the new technical discipline, which would develop an application system, a technological method in order to simulate the expansion and extension of human intelligence. This research is a review that discusses how artificial intelligence affects project management. The paper has discussed various benefits of AI adoption and its implementation. The results show that technology and AI cannot replace the human mind. Machine and other AI robots can automate tools and tasks, but at the end of the day, machines need human help to operate and monitor."
"Systematic Review of Fake News, Propaganda, and Disinformation: Examining Authors, Content, and Social Impact Through Machine Learning",D. Plikynas; I. Rizgelienė; G. Korvel,10.1109/ACCESS.2025.3530688,2025,"In recent years, the world has witnessed a global outbreak of fake news, propaganda and disinformation (FNPD) flows on online social networks (OSN). In the context of information warfare and the capabilities of generative AI, FNPDs have proliferated. They have become a powerful and quite effective tool for influencing people’s social identities, attitudes, opinions and even behavior. Ad hoc malicious social media accounts and organized networks of trolls and bots target countries, societies, social groups, political campaigns and individuals. As a result, conspiracy theories, echo chambers, filter bubbles and other processes of fragmentation and marginalization are polarizing, radicalizing, and disintegrating society in terms of coherent politics, governance, and social networks of trust and cooperation. This systematic review aims to explore advances in using machine and deep learning to detect FNPD in OSNs effectively. We present the results of a combined PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) review in three analysis domains: 1) propagators (authors, trolls, and bots), 2) textual content, 3) social impact. This systemic research framework integrates meta-analyses of three research domains, providing an overview of the wider research field and revealing important relationships between these research domains. It not only addresses the most promising ML/DL research methodologies and hybrid approaches in each domain, but also provides perspectives and insights on future research directions."
What Factors Contribute to the Acceptance of Artificial Intelligence on Healthcare Sector: A Systematic Review,G. B. Anugerah; A. N. Hidayanto,10.1109/ICICyTA60173.2023.10428744,2023,"Artificial intelligence adoption has typically been seen as an opportunity for the public health sector due to its potential to facilitate and improve quality of service. However, there are still many factors and challenges from the health care use cases themselves that cause the low application and utilization of artificial intelligence in society, even in the most common use cases. Acceptance of the use of technology in the health industry can be a factor that can accelerate the implementation of artificial intelligence in society, especially in the healthcare sector. This study aims to summarize the factors that influence artificial intelligence acceptance from the perspective of healthcare diagnostics and clinical decision support. The technology, organization, people, and environment (TOPE) framework was used as a guideline in selecting the appropriate factors and definition. Through a systematic literature review process, 27 relevant papers were found to be discussed in order to find the answer. Nine factors were found in the diagnostics use case, while in the clinical decision support use case, thirteen factors were found. The study reveals that technology and people are a key dimension for acceptance AI in healthcare sector."
"Advancements and Challenges in Video-Based Deception Detection: A Systematic Literature Review of Datasets, Modalities, and Methods",Y. D. Rahayu; C. Fatichah; A. Yuniarti; Y. P. Rahayu,10.1109/ACCESS.2025.3533545,2025,"Video-based deception detection has emerged as a promising field that leverages advances in computer vision, machine learning, and multimodal analysis to capture a wealth of nonverbal cues for identifying deceptive behavior. However, the field faces significant challenges related to dataset development, methodological approaches, and ethical considerations. This systematic literature review (SLR) aims to provide a comprehensive analysis of video-based deception detection research, with five distinct contributions: 1) an unprecedented analysis of 21 datasets, revealing critical gaps and opportunities in data resources; 2) a novel evaluation framework for assessing dataset quality and ecological validity; 3) a systematic comparison of multimodal integration approaches, identifying optimal strategies for combining visual, audio, and textual cues; 4) a critical examination of temporal modeling techniques for capturing the dynamic nature of deceptive behavior; and 5) a roadmap for addressing ethical challenges in deployment. Following the PRISMA guidelines, we reviewed studies published between 2019 and 2024 in major databases, including IEEE Xplore, ACM Digital Library, ScienceDirect, and Springer Link. The review process involved a rigorous two-stage screening, which resulted in the inclusion of 42 primary research papers. Our analysis revealed several key findings: 1) only 52.4% of identified datasets are publicly accessible, highlighting a critical gap in research reproducibility; 2) multimodal approaches consistently outperform unimodal methods, with accuracy improvements of 10-15%; 3) deep learning architectures, particularly LSTM variants and attention mechanisms, demonstrate superior performance in capturing temporal aspects of deception; 4) the Real-Life Trial Dataset emerged as the most frequently used dataset (65% of studies), indicating a preference for high-stakes ecologically valid data; and 5) significant ethical challenges remain unaddressed, particularly regarding privacy, bias, and cross-cultural validity. This review makes several novel contributions to advance the field: 1) provides a comprehensive framework for dataset evaluation and development; 2) identifies optimal strategies for multimodal integration and temporal modeling; 3) presents a structured approach to addressing ethical considerations; and 4) offers a detailed roadmap for future research priorities. These contributions will guide researchers in developing more robust, ethical, and generalizable deception detection systems, while addressing critical gaps in current methodologies and datasets."
Towards Leveraging Explainable Artificial Intelligent (XAI) in Requirements Engineering (RE) to Identify Aspect (Crosscutting Concern): A Systematic Literature Review (SLR) and Bibliometric Analysis,A. A. Magableh,10.1109/ICIT58056.2023.10226000,2023,"The software development life cycle contains many phases, among the most notable ones is requirements engineering (RE), it is considered a very important activity because poorly implemented RE steps can result in poor software quality which might lead to expensive maintenance costs and so many other undesirable issues. Thus, owing to the importance of RE in general and aspect (crosscutting concerns) identification in particular, researchers have proposed many approaches that can help to produce a clear and complete set of requirements document. On the other hand, artificial intelligence methods have a proven record of handling complicated text. Yet there is insufficient understanding of how AI and explainable AI can be employed and incorporated to be utilized in RE. Nevertheless, join forces to produce research for beneficial outcomes in this theme. The present study is a comprehensive SLR that reviewed the current state-of-the-art of AI/XAL in RE and aspect orientation. We reviewed the research from 2007 till 2022, PRISMA methodology is utilized to obtain a final set of (n =23) articles from four major scientific databases (Scopus, and Web of Science). Clear and new taxonomies were invented during the review discussion. Additionally, to enhance the contribution of this research, a bibliometric comprehensive analysis was conducted on the basis of annual scientific production, country scientific production, and co-occurrence. As a result, it is deemed a noteworthy topic of extensive exploration especially after we identify critical research gaps with their corresponding solutions and generic thoughts to serve as a guide for future directions."
Digital Transformation of Micro Business in Retail Sectors: A Systematic Literature Review,N. P. Istyanto; M. ER,10.1109/ICADEIS65852.2025.10933229,2025,"There have been many papers and SLR studies on Digital Transformation (DT), but there is still nothing specific about Micro-Business (MB), especially Retail. DT papers in this field are still very limited compared to other sectors, such as the manufacturing industry, banking, higher education, health or DT in general. This study aims to enrich DT literature in this sector to realize an inclusive DT by supporting the micro-retail business. The study follows the PRISMA method by reviewing five indexing databases: ScienceDirect, Emerald, IEEExplore, Springer, and Tandfonline. The result of this study is the identification of research trends related to micro-retail DT based on the number of publications per year, country of author and top publisher for this study. In addition, the findings of this study also identify the differences in the perspective and context of micro-retail DT between the global north and the global south, which can be used as a general guideline for academics and practitioners who are interested in further research and implementation in the micro-retail sector."
Systematic Review of Recent ChatGPT updates on Reporting of Radiology Cases with Reference to Magnetic Resonance Imaging of the Lumbar Spine,G. V. Mishra; A. A. Luharia; W. Naqvi; A. Sood,10.1109/IDICAIEI61867.2024.10842881,2024,"This systematic review aims to evaluate the recent advancements and applications of ChatGPT in reporting radiology cases, focusing on magnetic resonance imaging (MRI) of the lumbar spine. The review adheres to the PRISMA guidelines and explores how artificial intelligence (AI) can improve the accuracy, efficiency, and consistency of radiology reports. A comprehensive literature search was conducted across databases such as PubMed, Scopus, and IEEE Xplore, covering articles published from January 2022 to September 2024. The inclusion criteria were studies that examined the use of ChatGPT or similar AI models in radiology reporting, specifically focusing on MRI of the lumbar spine. Studies not involving ChatGPT or AI, non-radiology-related studies, and articles not available in full text were excluded. Data extraction and synthesis were performed in line with PRISMA guidelines. A total of 18 studies met the inclusion criteria, with 12 focusing on MRI of the lumbar spine. The findings indicate that ChatGPT shows potential in standardizing radiology reports, improving diagnostic accuracy, and reducing the time needed for report generation. However, challenges such as the necessity for clinical context, risks of over-reliance on AI, and issues regarding the interpretability of AI outputs were noted. ChatGPT and similar AI models have considerable potential in radiology reporting, especially for lumbar spine MRI. However, careful implementation, continuous updates, and rigorous validation are crucial to ensure these tools enhance rather than replace the expertise of radiologists."
"Opportunities, Applications, and Challenges of Edge-AI Enabled Video Analytics in Smart Cities: A Systematic Review",E. Badidi; K. Moumane; F. E. Ghazi,10.1109/ACCESS.2023.3300658,2023,"Video analytics with deep learning techniques has generated immense interest in academia and industry, captivating minds with its transformative potential. Deep learning techniques and the deluge of video data enable the mechanization of tasks that were once the exclusive domain of human effort. Furthermore, edge intelligence is emerging as an interdisciplinary technology that drives the fusion of edge computing and artificial intelligence (AI). Edge computing allows the Internet of Things (IoT) devices with limited resources to offload their compute-intensive AI applications to the network edge servers for execution. Specifically, AI workloads for video analytics can be moved to the network edge from the cloud, providing improved latency and bandwidth savings, among other benefits. This article reviews current technologies used in Edge AI-assisted video analytics in smart cities. It examines the various artificial intelligence models and privacy-preserving techniques used in edge video analytics. It identifies the various applications of video analytics in smart cities, including security and surveillance, transportation and traffic management, healthcare, education, sports and entertainment, and many more. Besides, it highlights the challenges of edge video analysis and open research issues. It is expected that this review will be valuable for researchers, engineers, and decision-makers who want to understand the landscape and scale of edge video analytics in smart cities."
Leveraging Machine Learning for Breast Cancer Prognosis: A Systematic Review,A. Kajala,10.1109/DELCON64804.2024.10867085,2024,"Breast cancer, a complex ailment, necessitates early diagnosis and prognosis for effective treatment. This systematic review examines the application of machine learning (ML) for breast cancer prognosis, emphasizing precise risk assessment in clinical decision-making. Author evaluates existing ML methods, highlighting their strengths, limitations, and potential clinical integration. Following PRISMA guidelines, author conducted a literature search in databases like Scopus, IEEE Xplore, and PubMed, focusing on studies from 2018–2023. Among the studies screened, 25 articles met the eligibility criteria for inclusion in this review. The findings from the author's review highlight that ensemble models are the most popular and accurate strategy for breast cancer prognosis. The two primary metrics used to assess model performance in the reviewed studies are accuracy and the C-index. The study also identified the datasets utilized for breast cancer prognosis and the role of explainable ML algorithms in guiding cancer treatment decisions. Finally, some challenges observed in the review are also investigated to provide valuable insights for researchers and clinicians in this dynamic field."
Deep Learning Techniques for Lung Cancer Detection: A Systematic Literature Review,S. H. Mahmud; I. Soesanti; R. Hartanto,10.1109/ICOIACT59844.2023.10455848,2023,"Lung cancer has been a leading cause of cancer-related deaths, with the number of fatalities in the United Kingdom between 2017 and 2019 reaching 34771, as reported by Cancer Research UK. Lung cancer is when cells inside the lung grow uncontrollably. Detecting nodules in lung cancer at an early stage can increase the chances of survival for humans. Researchers have been investigating the potential of artificial intelligence and deep learning to develop computer-aided detection (CAD) systems for automated lung cancer detection and classification. CAD systems could help radiologists detect lung cancer and improve lung cancer diagnosis accuracy. Our systematic literature review provided an overview of the performance of current deep-learning methods and datasets for detecting and classifying lung cancer using CT images. We conducted a systematic literature review using the PRISMA 2020. This paper gives the reader insights into various facets of lung cancer detection and motivates researchers to further explore opportunities for crafting models that can be seamlessly integrated into a CAD system."
Machine Translation Performance for Low-Resource Languages: A Systematic Literature Review,T. O. Tafa; S. Z. M. Hashim; M. S. Othman; H. Alhussian; M. Nasser; S. J. Abdulkadir; S. H. Huspi; S. O. Adeyemo; Y. A. Bena,10.1109/ACCESS.2025.3562918,2025,"Machine translation (MT) for low-resource languages continues to face significant challenges because of limited digital resources and parallel corpora, despite remarkable developments in neural machine translation (NMT). Addressing these challenges requires a thorough review of existing research to identify effective strategies and methods. To achieve this, a systematic literature review (SLR) is conducted following PRISMA guidelines and systematically analysing studies published in various academic databases in the last five years (between 2020 and 2024). A total of 69 relevant articles were examined to evaluate the performance of MT, explore persistent challenges and assess the effectiveness of proposed or used solutions. The analysis shows that while NMT has emerged as the predominant approach, its effectiveness is often reduced by the scarcity of training data and the structural complexity of low-resource languages. Strategies such as active learning, data augmentation, multilingual models and transfer learning are identified as critical for improving translation performance. Additionally, emerging research trends, including data pre-processing, optimization of decoder and rule-based approach demonstrate promising directions for addressing existing limitations. In terms of evaluation, most of the studies used Character n-gram F-score (ChrF), Translation Edit Rate (TER), Metric for Evaluation of Translation with Explicit Ordering (METEOR), Word Error Rate (WER) and Bilingual Evaluation Underscore (BLEU) as techniques’ validation metrics. This review provides a detailed evaluation of the current state of MT for low-resource languages and emphasizes the need for further research into underrepresented languages and the development of comprehensive datasets."
A Review of Signature Recognition Using Machine Learning,E. A. Soelistio; R. E. Hananto Kusumo; Z. V. Martan; E. Irwansyah,10.1109/ICCSAI53272.2021.9609732,2021,"Signatures have been used for years for transactions and consenting to responsibilities. Yet, online or offline, signatures can easily be falsified as there are no security measures in place to prevent this. Numerous researches have been carried out to find the most accurate and reliable signature recognition and verification system. This study examines the two problems previously mentioned. A primary goal of this study is to determine the best algorithms for recognizing signatures based on the signature type. This systematic literature review is conducted using a PRISMA flow diagram. The results indicate that offline signatures mostly use Convolutional Neural Networks (CNN) for their recognition, while online signatures use Recurrent Neural Networks (RNN) with other architectures."
Automatic Classification of Cardiac Arrhythmias Using Deep Learning Techniques: A Systematic Review,F. Vásquez-Iturralde; M. J. Flores-Calero; F. Grijalva; A. Rosales-Acosta,10.1109/ACCESS.2024.3408282,2024,"Cardiac arrhythmias are one of the main causes of death worldwide; therefore, early detection is essential to save the lives of patients who suffer from them and to reduce the cost of medical treatment. The growth of electronic technology, combined with the great potential of Deep Learning (DL) techniques, has enabled the design of devices for early and accurate detection of cardiac arrhythmias. This article presents a Systematic Literature Review (SLR) using a Systematic Mapping study and Bibliometric Analysis, through a set of relevant research questions (RQs), in relation to DL techniques applied to the automatic detection and classification of cardiac arrhythmias using electrocardiogram (ECG) signals, during the period 2017-2023. The PRISMA 2020 methodology was employed to identify the most pertinent scholarly articles, by querying the following databases: Scopus, IEEE Xplore, and PhysioNet Challenges, resulting in 494 publications being retrieved. This study also included a bibliometric analysis aimed at tracing the evolution of the primary technologies utilized in the automatic detection and recognition of cardiac arrhythmias. Additionally, it evaluates the performance of each technology, offering insights crucial for guiding future research."
Applied Machine Learning in Geophysics Taxonomy Review Bibliometrics and Trends in Generative AI,A. Shakhatova; M. Tolkyn; Z. Gulnara; S. Ozhigin; M. Amir; M. Kozhanov,10.1109/SISY62279.2024.10737556,2024,"This article presents a methodology to identify key studies using machine learning (ML) in geophysics. We created a comprehensive database of fundamental articles for a systematic review. The main goal is to classify the applications and methods of ML in geophysics through a modified PRISMA approach. The study results offer current insights into the trends and developments of ML applications in geophysics. This article presents a systematic method for identifying and analyzing significant studies that apply ML in geophysics. We created a detailed database of essential articles relevant to this field. Using a modified PRISMA guideline, we conducted a thorough review to evaluate and categorize the literature. The main goal of this review is to provide a clear classification of ML applications and methods in geophysics. We documented how ML techniques are used for various geophysical problems, including data analysis, pattern recognition, and predictive modeling. This classification helps clarify the range of ML applications in geophysics and highlights the specific methods and tools used. Our findings offer an updated view of current trends and developments in ML applications within geophysics. By analyzing the progress of ML and their effectiveness in geophysical applications, this study reveals emerging trends and suggests future research directions."
Enabling Predication of the Deep Learning Algorithms for Low-Dose CT Scan Image Denoising Models: A Systematic Literature Review,M. Zubair; H. B. Md Rais; F. Ullah; Q. Al-Tashi; M. Faheem; A. Ahmad Khan,10.1109/ACCESS.2024.3407774,2024,"Computed Tomography (CT) is a non-invasive imaging modality used to detect abnormalities in the human body with high precision. However, the electromagnetic radiation emitted during CT scans poses health risks, potentially leading to the development of metabolic abnormalities and genetic disorders, which increase the risk of cancer. The Low-Dose CT (LDCT) scanning technique was developed to address these hazards, but it has several limitations, including noise, artifacts, reduced contrast, and structural changes. These drawbacks significantly reduce the diagnostic capabilities of Computer-Aided Diagnosis (CAD) systems. Eliminating these noises and artifacts while preserving critical features poses a significant challenge. Traditional CT denoising algorithms struggle with edge blurring and high computational costs, often generating artifacts in flat regions as noise levels increase. Consequently, deep learning-based methods have emerged as a promising solution for LDCT image denoising. In this study, a comprehensive Systematic Literature Review (SLR) following PRISMA guidelines was conducted to explore the latest advancements in deep learning algorithms for LDCT image denoising. This SLR spans LDCT image-denoising research from 2018 to 2024, providing a detailed summary of methodologies, benefits, limitations, parameters, and trends. This study delves into the acquisition process of CT scans, investigating radiation absorption across various anatomical regions, as well as identifying sources of noise and its distribution within the LDCT images. Additionally, it enhances our understanding of LDCT image denoising trends and provides valuable insights for future research, thus making a substantial contribution to ongoing efforts to enhance the quality and reliability of LDCT images."
Understanding the Benefits of Deep Learning in Drug Discovery: A Scoping Review,M. Msayi; L. T. P. Salamntu,10.1109/ICTAS59620.2024.10507136,2024,"The use of the traditional drug discovery method was seen to be inaccurate, time-consuming, and costly. As a result, deep learning was argued to be the accurate method of discovering drugs. Deep learning is a subset of machine learning and focuses on data fed into the algorithm to learn and make automated decisions. This review seeks to understand the benefits of using deep learning in drug discovery. A scoping review based on the PRISMA-ScR diagram was adopted. Four databases were selected, and a single search string was applied across all four chosen databases. The study only considered 17 articles to be relevant for this review. Of the 17 selected articles, the majority of the studies were quantitative (76%), and 24% of the studies were qualitative in nature. Deep learning in drug discovery is conceptualised as automatically detecting drugs using an artificial neural network-based approach. Human intervention is eliminated as the model makes inferences and decisions unsupervised. In this review, six benefits of using deep learning in drug discovery emerged: reduced cost and time, accurate diagnosis, unbiasedness, data-driven insights, virtual screening, and drug repurposing. Reduced time and cost emerged among the top benefits, and the benefit of virtual screening appeared to be the least."
Systematic Literature Review: On Measuring the Level of Emotional Experience Based on EEG Signals,Y. Devianto; A. Setiawan; C. Dewi; A. Iriani; H. Dwi Purnomo; I. Sembiring,10.1109/ICTIIA61827.2024.10761523,2024,"This research aims to produce a literature review using a structured approach, Systematic Literature Review (SLR), so that readers can easily understand the aim and objectives of this research article. Furthermore, this study aims to identify “what methodologies are available to measure emotional experi- ences via EEG signals.” It can also indicate research gaps that can be used to develop research questions. The article search and selection approach follows the PRISMA guidelines., considering established criteria. This research was conducted through a systematic literature review focusing on essential factors for measuring emotional experience. The process is divided into several steps: selecting online database sources for research literature, reviewing research objectives and questions, creating a search strategy, setting search criteria, defining inclusion and exclusion criteria, and extracting data; the results are then examined and analysed to address the research inquiries. We can conduct further research by looking at the resulting mapping references; for this research, we opted to utilize the dataset SEED and GNN algorithm. The research we will carry out is to detect individual emotional and mental levels using the SEED dataset and using the Graph Neural Network (GNN) algorithm. Measuring the intensity of emotional experience is a popular research area; many have carried out such research with various algorithms and datasets generated from EEG signals."
"IT Infrastructure Anomaly Detection and Failure Handling: A Systematic Literature Review Focusing on Datasets, Log Preprocessing, Machine & Deep Learning Approaches and Automated Tool",D. A. Bhanage; A. V. Pawar; K. Kotecha,10.1109/ACCESS.2021.3128283,2021,"Nowadays, reliability assurance is crucial in components of IT infrastructures. Unavailability of any element or connection results in downtime and triggers monetary and performance casualties. Thus, reliability engineering has been a topic of investigation recently. The system logs become obligatory in IT infrastructure monitoring for failure detection, root cause analysis, and troubleshooting. This Systematic Literature Review (SLR) focuses on detailed analysis based on the various qualitative and performance merits of datasets used, technical approaches utilized, and automated tools developed. The full-text review was directed by Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) methodology. 102 articles were extracted from Scopus, IEEE Explore, WoS, and ACM for a thorough examination. Also, a few more supplementary articles were studied by applying Snowballing technique. The study emphasizes the use of system logs for anomaly or failure detection and prediction. The survey encapsulates the automated tools under various quality merit criteria. This SLR ascertained that machine learning and deep learning-based classification approaches employed on selected features enable enhanced performance than traditional rule-based and method-based approaches. Additionally, the paper discusses research gaps in the existing literature and provides future research directions. The primary intent of this SLR is to perceive and inspect various tools and techniques proposed to mitigate IT infrastructure downtime in the existing literature. This survey will encourage prospective researchers to understand the pros and cons of current methods and pick an excellent approach to solve their identified problems in the field of IT infrastructure."
Convolutional Neural Network (CNN) Algorithm Development for Quality Measurement in Tire Product Classification: A Systematic Literature Review,A. T. Dwilaga; L. Y. Banowosari; Sudaryanto; A. S. Talita,10.1109/ICIC64337.2024.10956823,2024,"Convolutional Neural Networks (CNN) have shown potential in tire defect classification, but challenges like image quality variations and model limitations persist. This Systematic Literature Review (SLR) employs the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology and a qualitative approach. It uses the Population, Intervention, Comparison, Outcome, and Context (PICOC) framework to analyze 30 articles from Google Scholar, published between 2018 and 2024 in Q1-Q4 journals. The review examines article distribution by publisher, evaluates the impact of key pre-training parameters (such as data split ratio, dropout rate, optimization algorithm, learning rate, and pre-trained models on CNN performance), and identifies challenges in defect classification. MDPI (Multidisciplinary Digital Publishing Institute) is the most frequent publisher, with the highest number of publications occurring in 2021. AlexNet and VGG16 are the most common models, and an 80:20 data split ratio, 0.5 dropout rate, and a 0.001 learning rate are frequently used. The Adam (Adaptive Momentum) optimization algorithm is widely adopted, but challenges remain in detecting small defects. Further improvements in CNN models, lighting techniques, and larger datasets are needed to enhance defect detection. This review highlights current trends and future research opportunities in tire quality measurement."
Factors Affecting the Effectiveness of Generative Artificial Intelligence Apps on University Students' Programming Language Learning in Sri Lanka: A Systematic Literature Review,K. G. D. K. Perera; J. Wijayanayake; J. Prasadika,10.1109/ICARC61713.2024.10499744,2024,"In today's era, technology has become pervasive worldwide, significantly facilitating access to learning resources. Notably, the emergence of Generative Artificial Intelligence (AI) has garnered rapid attention and interest in a short period with the introduction of ChatGPT. Many individuals have extensively discussed and evaluated this AI-powered language model, from researchers to casual internet users. Importantly, Generative AI applications are increasingly recognized for their potential in educational contexts. In the realm of education, AI has the potential to significantly broaden and improve teaching and learning in higher education. However, while numerous studies have explored the effectiveness of Generative AI applications in programming language learning, an absence of research examining their impact comprehensively exists. Hence, this study aims to identify the factors that affect the successful utilization of Generative AI applications in the context of undergraduate programming language learning, with a particular emphasis on the viewpoints of university students. A systematic literature review was undertaken to obtain the research objectives, adhering to the Prisma 2020 guidelines, which involved selecting and analyzing 47 prior studies. Mainly this study utilized a systematic literature review to comprehend the factors influencing the effective utilization of Generative AI apps by undergraduate students in their programming learning experiences. Furthermore, the study discusses the advantages and challenges university students face when learning programming using generative AI applications."
"Approaches, Applications, and Challenges of Using Sentiment Analysis for False Information Detection: A Systematic Literature Review",N. E. Fanlau; M. Hiu; S. Achmad; R. Sutoyo,10.1109/IMCOM64595.2025.10857547,2025,"In today's digital age, the prevalence of false information on the internet, spanning from fake news to fake user accounts, poses a pressing concern that undermines the reliability of online information. Such misinformation can have harmful consequences, such as promoting biased ideologies, swaying public opinion, and inciting unrest. Implementing sentiment analysis technology in false information detection significantly impacts combating misinformation on digital platforms. It analyzes textual content in real time, which is crucial for effectively identifying and mitigating false information such as fake news, fake reviews, fake accounts, and fake websites on digital platforms. This paper explores sentiment analysis's approaches, applications, and challenges for false information detection, providing insights for future research and application. A systematic literature review was conducted using the PRISMA methodologies to identify and evaluate relevant studies. The results of this paper contribute to understanding the current state of sentiment analysis in combating misinformation. This paper provided the common and available applications of sentiment analysis, such as fake news detection, fake review detection, fake social media account detection, and more. Approaches that can be taken to achieve these applications include the machine learning approach, neural network approach, hybrid approach, and more. Moreover, this work highlights the need for more advanced algorithms and ongoing research to address challenges such as sarcasm detection, bot account identification, and effectively interpreting informal language and slang in texts."
Effectiveness of Machine Learning Algorithms in Threat Detection and Mitigation in Cyberspace: A Systematic Review,M. A. Adaji; F. S. Bakpo; M. E. Ezema; E. Etuh; C. Markus; S. O. Olofu; B. I. Sambo,10.1109/NIGERCON62786.2024.10927069,2024,"The rapid digitalization and proliferation of internet-connected devices have not only revolutionized our daily lives but also exponentially expanded the attack surface for cybercriminals, creating an increasingly complex and treacherous cybersecurity landscape. This review critically examines the effectiveness of machine learning (ML) algorithms in cyber threat detection and mitigation, addressing the need for advanced cybersecurity measures in our interconnected digital landscape. Following Kitchenham's guidelines and utilizing the PRISMA model, the study analyzed 907 initial records from Science Direct, Google Scholar, IEEE Xplore, Springer, and ACM Digital Library, ultimately including only 15 studies that met stringent criteria. The thematic analysis revealed varying effectiveness of supervised learning (SVM and Random Forests) with accuracy rates of 85-90%, but with moderate false positive rates (15-20%). Unsupervised learning techniques (like K-means and DBSCAN) demonstrated lower accuracy (70-75%) yet excelled in detecting novel threats, albeit with higher false positive rates (25-30%). Reinforcement learning showcased adaptive defense capabilities but suffered from lower accuracy (65-70%) due to adversarial manipulation. Deep learning methods achieved high accuracy (up to 99%) in specific tasks, yet faced challenges related to resource demands and limited interpretability. The study highlights data quality difficulties, high false positive and negative rates, and complex model interpretability issues. ML can automate incident response and security orchestration, but the evaluation stresses the need for a balanced approach with human experience. Increasing model interpretability and defending against adversarial attacks are research gaps. The paper emphasises the need of hybrid approaches and explainable AI techniques for cyber threat identification and mitigation using ML. This thorough study lays the groundwork for ML-based cybersecurity in a complex threat landscape, despite limitations owing to the continuous evolution of cyber threats and ML technology."
Advances of Deep learning in Breast Cancer Modeling,S. Ardabili; A. Mosavi; I. Felde,10.1109/SISY60376.2023.10417961,2023,"Deep learning (DL) has recently gained popularity in forecasting, detecting, categorizing, and diagnosing for breast cancer with promising results. Developing a review paper to assess the efficacy of DL methods in this context is essential. We've established a standardized database initially containing fundamental publications for methodical reviews. The primary objective of this review is to systematically present the current state-of-the-art, using an updated PRISMA guidelines to better review and evaluate the DL's effectiveness in breast cancer applications. The research follows three main stages: data collection, data analysis, and summarization of initial outcomes. The results highlight accuracy as the prevailing and comprehensive metric used in evaluating DL tools across varied breast cancer applications. Convolutional neural networks (CNNs) have found to have widespread utility, notably surpassing other DL methods. In contrast, collaborative teamwork and employing advanced DL techniques yield optimal performance."
A Systematic Review on Hindi to English Machine Translation Using Machine Learning,I. Rathi; P. Goel; J. S. Batth,10.1109/GlobalAISummit62156.2024.10947850,2024,"Effective cross-language communication is vital for fostering collaboration between countries. This paper reviews various machine translation (MT) techniques, focusing on the challenging task of translating Hindi to English. Techniques covered include “Rule-Based Machine Translation (RBMT)”, “Statistical Machine Translation (SMT)”, “Example-Based Machine Translation (EBMT)”, and “Hybrid Machine Translation”, each with distinct advantages and limitations. The literature review highlights hybrid models combining RBMT, SMT, and EBMT for enhanced translation quality, as well as NMT systems leveraging linguistic features. A case study on a Transformer-based NMT system, trained with the Opennmt-tf toolkit on NVIDIA Tesla K80 GPUs, examines word and subword-level tokenization and incorporates back-translated data to enhance performance. Results show that subword-level tokenization outperforms word-level tokenization, achieving a peak BLEU score of around 22, compared to approximately 21 for word-level tokenization. These findings underscore the benefits of subword tokenization and back-translation in improving translation accuracy. The systematic review, following PRISMA guidelines, includes 15 selected research publications. This study emphasizes the need for ongoing innovation in Hindito-English MT systems to improve accuracy and support crosslanguage communication."
"Fake Review Detection Using Extravagant Words: A Comparative Study of K-means, K-mode, and Hierarchical Classification",M. Ennaouri; M. Raoui; A. Zellou; M. H. El Yazidi,10.1109/ICECCE63537.2024.10823434,2024,"In the digital age, online reviews playa critical role in influencing consumer decisions and business reputations. How-ever, the prevalence of fake reviews undermines the credibility of online platforms. In this study, we propose a new method based on the use of extravagant words as indicators of fake reviews, leveraging their tendency to exaggerate and manipulate readers. We aim to evaluate the effectiveness of this approach by comparing three distinct methods: K-means, K-mode, and Hierarchical clustering. Utilizing two comprehensive datasets, we preprocess the data, extract features, and implement the tree classification methods. Additionally, we incorporate BERT for improved feature extraction. The performance of each method is assessed using metrics such as accuracy, precision, recall, and Fl-score. Our findings reveal significant insights into the utility of extravagant words and BERT in distinguishing fake reviews and highlight the comparative strengths and weaknesses of K-means, K-modes and Hierarchical clustering. This study provides a novel approach to enhance review authenticity verification, with implications for improving consumer trust and platform integrity."
A Systematic Literature Review on Text Generation Using Deep Neural Network Models,N. Fatima; A. S. Imran; Z. Kastrati; S. M. Daudpota; A. Soomro,10.1109/ACCESS.2022.3174108,2022,"In recent years, significant progress has been made in text generation. The latest text generation models are revolutionizing the domain by generating human-like text. It has gained wide popularity recently in many domains like news, social networks, movie scriptwriting, and poetry composition, to name a few. The application of text generation in various fields has resulted in a lot of interest from the scientific community in this area. To the best of our knowledge, there is a lack of extensive review and an up-to-date body of knowledge of text generation deep learning models. Therefore, this survey aims to bring together all the relevant work in a systematic mapping study highlighting key contributions from various researchers over the years, focusing on the past, present, and future trends. In this work, we have identified 90 primary studies from 2015 to 2021 employing the PRISMA framework. We also identified research gaps that are further needed to be explored by the research community. In the end, we provide some future directions for researchers and guidelines for practitioners based on the findings of this review."
"A Systematic Review of Machine Learning, Deep Learning, and Natural Language Processing for Sentiment Analysis",M. M. Ali; E. S. Mohamed,10.1109/ICSSAS64001.2024.10760614,2024,"Advanced sentiment analysis is crucial for modern applications as it provides deep insights into human emotions, opinions, and sentiments expressed in text. Its impact spans across various industries and domains. Staying updated with the latest methods, such as transformer-based models, is essential for improving model performance, addressing challenges, and exploring new applications. By integrating modern pretrained model systems, performance can be significantly enhanced. This research study systematically reviews the status of sentiment analysis studies nowadays, employing the PRISMA model for articles to improve the calibre of the literature review procedure. This comprehensive overview of the literature provides useful insights into research gaps and helps researchers and academia stay updated with current trends in sentiment analysis. The review of 80 peer-reviewed articles covering 2019 to 2024 reveals significant findings that can inspire further research in the sentiment study."
Systematic Literature Review on Deep Learning for Weather Phenomenon Classification,U. Hasanah; C. -M. Liu,10.1109/ICORIS63540.2024.10903764,2024,"Nowadays, weather image classification is very important, it can help many sectors, such as daily weather forecasting, disaster control, agriculture, aviation, etc. Deep learning has emerged as an effective method for image classification, providing significant increases in efficiency and accuracy. This study presents a thorough examination of the application of deep learning in weather image classification, with an emphasis on methodology, challenges, and future works. The PRISMA approach was used to systematically search and filter 25377 publications (2018–2024) from IEEE, Elsevier, Springer, Wiley Library, and MDPI. The result was a selection of 16 important studies that showcase cutting-edge methodologies and the effectiveness of those measures. Our goal is to present a comprehensive overview of how deep learning methods have improved weather prediction while identifying potential topics for future research."
XAI Unveiled: Revealing the Potential of Explainable AI in Medicine: A Systematic Review,N. Scarpato; P. Ferroni; F. Guadagni,10.1109/ACCESS.2024.3514197,2024,"Nowadays, artificial intelligence in medicine plays a leading role. This necessitates the need to ensure that artificial intelligence systems are not only high-performing but also comprehensible to all stakeholders involved, including doctors, patients, healthcare providers, etc. As a result, the explainability of artificial intelligence systems has become a widely discussed subject in recent times, leading to the publication of numerous approaches and solutions. In this paper, we aimed to provide a systematic review of these approaches in order to analyze their role in making artificial intelligence interpretable for everyone. The conducted review was carried out in accordance with the PRISMA statement. We conducted a BIAS analysis, identifying 87 scientific papers from those retrieved as having a low risk of BIAS. Subsequently, we defined a classification framework based on the classification taxonomy and applied it to analyze these papers. The results show that, although most AI approaches in medicine currently incorporate explainability methods, the evaluation of these systems is not always performed. When evaluation does occur, it is most often focused on improving the system itself rather than assessing users’ perception of the system’s effectiveness. To address these limitations, we propose a framework for evaluating explainability approaches in medicine, aimed at guiding developers in designing effective human-centered methods."
A Systematic Review of Computational Intelligence Approaches for Prediction of Chronic Kidney Disease,M. B. L. Prasanna; G. Lavanya Devi,10.1109/SCOPES64467.2024.10990929,2024,"CKD is a global health issue with rising prevalence and severe morbidity and mortality. Detecting and addressing diseases early is crucial to minimize disease progression and enhance patient outcomes. In the field of chronic kidney disease (CKD) prediction, computational intelligence (CI) algorithms have shown significant potential as effective tools, with the assurance of non-invasive and precise diagnosis. The use of CI methodologies, such as machine learning and deep learning techniques, for the prediction of chronic kidney disease (CKD) is examined in this systematic review. The advancement of CKD is linked to a number of significant effects such as an increased incidence of various ailments, Anemia, and death. There is a risk of hyperlipidaemia, nerve damage, pregnancy difficulties, and possibly renal failure. It kills millions of individuals each year. Diagnosis of CKD is difficult because there are no obvious symptoms to diagnose the illness. However, some results may be overlooked when the diagnosis is chronic. This study investigated CI approaches for CKD prediction using clinical data, biomarkers, or imaging aspects to diagnose CKD. Age, glucose level, number of red blood cells, and other symptoms were included when training the algorithm with 500 participants. The experimental results show that the suggested model excels at categorization. The objective is to develop computational intelligence (CI) algorithms for learning from attribute reports in datasets and effectively detect CKD that is more accessible. Eligible studies used CI to predict CKD based on clinical data, biomarkers, or imaging characteristics. This systematic investigation contributes to a better understanding of the landscape of CI applications in predicting CKD and provides light on potential future advances in the field."
AI-Analyst: An AI-Assisted SDLC Analysis Framework for Business Cost Optimization,N. Faruqui; P. Thatoi; R. Choudhary; I. Roncevic; H. Alqahtani; I. H. Sarker; S. Khanam,10.1109/ACCESS.2024.3519423,2024,"Managing the System Development Lifecycle (SDLC) is a complex task because of its involvement in coordinating diverse activities, stakeholders, and resources while ensuring project goals are met efficiently. The complex nature of the SDLC process leaves plenty of scope for human error, which impacts the overall business cost. This paper introduces AI-Analyst, an AI-assisted framework developed using the transformer-based model with more than 150 million parameters to assist with SDLC management. It minimizes manual effort errors, optimizes resource allocation, and improves decision-making processes, resulting in substantial cost savings. The statistical analysis shows that it saves around 53.33% of costs in an experimental project. The transformer model has been trained with a uniquely prepared dataset tailored for SDLC through transfer learning. It achieved impressive results, with an accuracy of 91.5%, precision of 91.9%, recall of 91.3%, and an F1-score of 91.5%, demonstrating its high reliability and performance. The perplexity score of 15 further indicates the model’s strong language understanding capabilities to retrieve relations from complex characteristics of Natural Language Processing (NLP). The AI-Analyst framework represents a significant advancement in integrating Large Language Models (LLMs) into SDLC, offering a scalable and cost-effective solution for optimizing business processes."
A Systematic Literature Review: Object Detection in Semi-Autonomous Car,E. Sunjaya; Kurniadi; C. Daniel; A. Kurniawan; M. S. Anggreainy,10.1109/IC2IE60547.2023.10331575,2023,"Traffic accidents are a serious problem in developing countries. These accidents can be caused by poor infrastructure and the arrogance of drivers. In addition, semi-autonomous systems are automotive technologies that have been developed in recent years and are starting to be implemented in existing vehicles. This literature review paper will mainly discuss how object detection works in semi-autonomous systems, how semi-autonomous systems operate, and whether semi-autonomous cars can reduce the number of traffic accidents. Its main objective is to build driver confidence in semi-autonomous systems by assisting in driving and supporting the development of them. In the process of writing this paper, the method we use follows the guidelines stated in the Preferred Reporting Items for Systematic Reviews and Meta-Analyses Protocols (PRISMA-P)."
The Implementation of Artificial Intelligence in Knowledge Management: A Systematic Literature Review,A. Novalin; A. Gunawan; D. Prihandoko,10.1109/IATMSI60426.2024.10502986,2024,"Knowledge Management (KM) is crucial since it is used for managerial decisions that affect the organization’s success. To improve the quality of KM, there is an innovation that implements Artificial Intelligence (AI) for the KM process. AI is a machine learning tool that can execute human tasks, adapt to new inputs, and learn from experience. This study aims to investigate the growth of AI in KM and evaluate how AI can be applied in KM to manage information and knowledge. The study used a systematic literature review method with PRISMA and data sources from Scopus. A total of 30 articles were examined in the review analysis. The research results found that the implementation of AI in KM was already conducted on various continents and most of the previous studies discussed this topic in the General field. Furthermore, the review discovered that AI can be applied to fundamental knowledge management process, decision-making, knowledge forecasting, and knowledge exchanges. This research also indicates that the implementation of AI in KM is growing and the topic of AI research in KM continues to develop. This study provides insight into prospects for innovation and improvement by offering evaluations for the future development of AI in KM."
Work in Progress: A Rapid Review of the Scholarship on Generative AI in Engineering Workplaces—Implications for Engineering Education,C. Mohammed,10.1109/EDUNINE62377.2025.10980845,2025,"If engineering programs are to be fit for purpose, they must be informed by actual engineering workplace practices. A rapid review was employed to investigate how, according to the scholarship, GAI is being used in engineering workplaces. The selection of scholarship was informed by PRISMA guidelines. Seven articles met the inclusion criteria. Scholars are advocating for centering human agency in the deployment of GAI. The scholarship reports generic uses of GAI in engineering and uses specific to civil and software engineering. This review provides several take-aways for educators and researchers: the ethics of GAI use must figure in curricula; engineering classrooms must foster GAI literacy; and there is need for more research on GAI deployment in the workplace. Also, distinctions are needed between industrial AI and GAI. While students will eventually use industrial AI in the workplace, it is GAI that they are leveraging in the classroom."
The Role of Artificial Intelligence in Project Management,M. Odeh,10.1109/EMR.2023.3309756,2023,"This article discusses the role of artificial intelligence (AI) in project management. The acceleration of AI adoption being a driver for innovation will force the project management profession to change its approach. Project managers and leaders must meet new and evolving stakeholders' expectations of utilizing new tools and techniques to deliver successful projects. AI must be viewed not only as a project differentiator, but also as the value it adds to the delivered project."
Technology-Enhanced Multimodal Learning Analytics in Higher Education: A Systematic Literature Review,O. R. Yürüm,10.1109/ACCESS.2025.3572467,2025,"Multimodal learning analytics (MMLA) is an emerging field of learning analytics and promises a more comprehensive analysis of the learning process thanks to advances in technological devices and data science. The purpose of this study was to explore technology-enhanced multimodal learning analytics in higher education systematically. A systematic literature review was performed using the PRISMA guidelines, and 45 studies published between January 2012 and June 2024 were determined. The findings demonstrated that China, the USA, Australia, and Chile were the leading contributors to MMLA research, with a notable surge in publications in 2021. Audio recorders, cameras, webcams, eye trackers, and wristbands were the most used devices. Most studies were conducted in experiment rooms or laboratories, though studies in authentic classroom settings have been growing. Data were primarily collected during activities such as programming, simulation exercises, presentations, discussions, writing, watching videos, reading, or exams, as well as throughout the entire instructional process, predominantly in computer science, health, and engineering courses. The studies were mainly predictive or descriptive whereas quite a few studies were prescriptive. Frequently tracked data types included audio, gaze, log, facial expression, physiological, and behavioral data. Traditional machine learning and basic statistics were the commonly used analytical methods whilst advanced statistics and deep learning were relatively less utilized. Test performance, engagement, emotional state, debugging performance, and learning experience were the popular target variables. The studies also pointed out several implications and future directions, with a significant portion highlighting the development of interventions, frameworks, or adaptive systems using MMLA."
Intelligent Monitoring and Diagnosing Capability in Healthcare: Systematic Literature Review,P. Aryawibowo; A. F. Hidayanto; Y. M. Toemali; Anderies; K. E. Setiawan; A. A. S. Gunawan,10.1109/ICIMTech59029.2023.10277846,2023,"Recently, Artificial Intelligence (AI) development has become more advanced and has been extensively used in various kinds of necessities in healthcare, supporting medical workers in many cases. AI can be used for monitoring and diagnosing because of the AI's ability to find solutions by using patterns that have been trained from datasets. Thus, this research reviewed the related research papers about AI role in the healthcare field, which focused on monitoring and diagnosing diseases. This study collected many relevant studies from various academic database sources, such as Google Scholar, Springer, IEEE Xplore, PubMed, Semantic Scholar, and Scopus Elsevier by using PRISMA 2009 (Preferred Reporting Items of Systematic reviews and Meta-Analyses) guidance. Our review discovered that AI can be helpful in certain areas of expertise, such as infectious disease using algorithms such as neural networks, fuzzy clustering networks, and Naïve Bayesian network; cancer using algorithms such as SVM, regression, and random forest classifier; COVID-19 using algorithms such as Naïve Bayesian Network, 3D CNN segmentation models, and ResNet-based models; and other diseases. In this review, the implementation of AI in healthcare brings both benefits and drawbacks. From the benefits side, AI can be very impactful by enhancing patient treatment and reducing costs. Meanwhile, on the downside, AI has some problems, such as a lack of information that makes the AI model not represent the real data. Because of that, AI will not replace medical workers in five years but only fill the role of support for them in monitoring and diagnosing, allowing them to concentrate on the most critical areas. Based on result analysis, assorted needs appear to exist to upgrade the implementation of AI in the healthcare field to support human health resources. This research suggests that AI researchers should be focused on specific fields in the healthcare field, especially in the diagnosing and monitoring fields, to find new insights."
Radiographic Imaging and AI: A Systematic Review on Applications in Medical Imaging,P. Garai; J. Das,10.1109/ISACC65211.2025.10969418,2025,"The advent of deep learning has ushered in trans-formative advancements in medical imaging, particularly in radiography. This literature survey systematically reviews the application of deep learning techniques in radiographic analysis, emphasizing methodologies, outcomes, challenges, and future directions. Utilizing the PRISMA framework, this study synthesizes findings from a comprehensive range of 30 selected studies to provide an insightful overview of the current landscape and identify gaps for future research. The review examines various deep learning models and their impact on diagnostic accuracy, as well as the challenges faced in clinical integration. It also identifies emerging trends and proposes avenues for improving model robustness and interpretability in medical imaging."
"Artificial Intelligence in Project Management: Impacts on Efficiency, Innovation & Competitive Edge",T. Saxena; M. W. Totaro,10.1109/AIxB62249.2024.00022,2024,"Integrating AI with Project Management presents a unique and transformational opportunity to enhance organizations’ business delivery and value stream. This study examines the growing usage of AI in Agile environments to improve decision-making, process automation, and risk reduction. This survey presents data-driven skills and logic-centric thinking that leverage AI’s potential for organizations to develop solid customer solutions, enhance operational efficiency, receive profound insights, and advance the workforce with rising technologies. Collecting general information, this study will explore how effectively utilizing Agile and SAFe 6.0 in project management, and AI could yield noticeable advantages for enterprises in terms of efficiency, innovation, and competitive edge."
Artificial Intelligence in OSCE: Innovations and Implications for Medical Education Assessment – A Systematic Review,G. V. Mishra; A. A. Luharia; W. Naqvi; A. Sood,10.1109/IDICAIEI61867.2024.10842789,2024,"Objective Structured Clinical Examinations (OSCEs) are cornerstone for evaluation of competencies pertaining to clinical medicine. The recent advent of artificial intelligence has given rise to newer opportunities which evidently have improved the reliability, validity, objectivity of OSCE. This draft deals into the concept of integration of artificial intelligence into OSCE with proper focus on technology technology innovations there in such as provision of real-time feedback and incorporation of virtual simulations as well. As per the PRISMA guidelines, we conducted a literature search and analysed studies available in literature from 2018 till 2024. We have concluded that artificial intelligence as a technological innovation in the world of assessment of clinical competencies. In medical education has a very strong potential to reduce examiner bias, provision of detailed feedback, and simulating realistic clinical scenarios. Still, certain challenges and limitation such as lack of emotional intelligence and the human touch to the entire assessment. Scenario seems to be lagging and is therefore much needed to retain the learning curve of the modern day learners."
Emerging Trends in Realistic Robotic Simulations: A Comprehensive Systematic Literature Review,S. M. Kargar; B. Yordanov; C. Harvey; A. Asadipour,10.1109/ACCESS.2024.3404881,2024,"Simulation plays a pivotal role in providing safely reproducible scenarios to evaluate the ever-advancing domain of computer science and robotics. It was an essential part of the pandemic when no access to physical spaces was available. The advent of AI-powered platforms in conjunction with enhanced graphics, physics and other sensory engines attracts a new breed of interdisciplinary researchers to enter the robotic field, most notably from computer science, engineering and social sciences. Integration of ROS as a uniform middleware to deploy achieved outcomes in real practice provides an opportunity to move one step closer to the sim-to-real experiences that enables researchers to test ideas beyond the close laboratory spaces. There is a lack of comprehensive evaluation of ROS-enabled simulators, and the integration of advanced AI techniques for realistic scenario replication. This paper addresses this challenge by evaluating ROS-enabled simulators in the design and implementation of AI techniques through an in-depth systematic literature review (SLR). This SLR is guided by the research and commercial market demands, employing Population, Intervention, Comparison, Outcome, and Context (PICOC) and Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) frameworks with a major focus on Wheeled Mobile Robots (WMRs). We also highlight the increasing importance of game engines like Unity and Unreal in future of robotic simulations, especially under modelling close to real experiences. By comparing simulation platform features and capabilities, this paper offers guidance to developers and researchers, enabling them to select the most suitable platform for their projects efficiently that contradicts the commonly in use “one size fits all” approach. Finally, based on the thorough insights from the review, we identify and suggest some key future research directions in AI-enhanced realistic robotic simulations."
A Comprehensive Review of Deep Learning-Based Anomaly Detection Methods for Precision Agriculture,K. Gkountakos; K. Ioannidis; K. Demestichas; S. Vrochidis; I. Kompatsiaris,10.1109/ACCESS.2024.3522248,2024,"Anomaly detection is a challenging problem in various application domains of Artificial Intelligence, such as in video surveillance, the Internet of Things, and notably, precision agriculture. The effectiveness of anomaly detection in each field is intricately linked to the domain-specific data, adhering, at the same time, to the core objective of detecting outliers. In the precision agriculture domain, anomalies range from plant diseases in image data to fluctuating environmental conditions in time-series datasets. This review provides a detailed examination of deep learning-based anomaly detection methods within precision agriculture, adopting the PRISMA methodology for a structured and comprehensive analysis. We employ a novel taxonomy categorizing recent literature by agricultural application, anomaly relevance, data modality, deep learning architecture, supervision level, and dataset usage. Our findings highlight a predominant reliance on visual data and uncover a potential alignment between methods originally devised for classification or detection and the anomaly detection challenge. The review also signals a pressing need for large-scale datasets to address precision agriculture challenges effectively. By mapping the current landscape and suggesting directions for future research, our work aims to facilitate advancements in anomaly detection techniques, enabling enhanced decision-making and operational efficiency in precision agriculture."
"Online Extremism Detection: A Systematic Literature Review With Emphasis on Datasets, Classification Techniques, Validation Methods, and Tools",M. Gaikwad; S. Ahirrao; S. Phansalkar; K. Kotecha,10.1109/ACCESS.2021.3068313,2021,"Social media platforms are popular for expressing personal views, emotions and beliefs. Social media platforms are influential for propagating extremist ideologies for group-building, fund-raising, and recruitment. To monitor and control the outreach of extremists on social media, detection of extremism in social media is necessary. The existing extremism detection literature on social media is limited by specific ideology, subjective validation methods, and binary or tertiary classification. A comprehensive and comparative survey of datasets, classification techniques, validation methods with online extremism detection tool is essential. The systematic literature review methodology (PRISMA) was used. Sixty-four studies on extremism research were collected, including 31 from SCOPUS, Web of Science (WoS), ACM, IEEE, and 33 thesis, technical and analytical reports using Snowballing technique. The survey highlights the role of social media in propagating online radicalization and the need for extremism detection on social media platforms. The review concludes lack of publicly available, class-balanced, and unbiased datasets for better detection and classification of social-media extremism. Lack of validation techniques to evaluate correctness and quality of custom data sets without human interventions, was found. The information retrieval unveiled that contemporary research work is prejudiced towards ISIS ideology. We investigated that deep learning based automated extremism detection techniques outperform other techniques. The review opens the research opportunities for developing an online, publicly available automated tool for extremism data collection and detection. The survey results in conceptualization of architecture for construction of multi-ideology extremism text dataset with robust data validation techniques for multiclass classification of extremism text."
The Constraints of The Adoption of Gamification for Education and Training in Higher Education Institutions: A Systematic Literature Review,S. Oguta; A. Akinyinka; S. Ojo; B. Maake,10.1109/icABCD59051.2023.10220568,2023,"Gamification, a concept that describes the use of game design elements in non-game scenarios, inan attempt to induce fun and motivation in non-game scenarios, has found application in a variety of contexts. In education, popular gamification elements that have been introduced to make learning fun includes the use of points, levels, missions, leaderboards, badges, and avatars. New gamification such as the use of social robots is now in vogue. Despite the enormous potential of these gaming concepts, researchers have unearthed some challenges that face the adoption of gamification in educational systems of Higher Education Institutions. In this study's systematic literature review, the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) framework was used to explore the difficulties encountered when using gamification in training and education at colleges and universities and tosuggest potential solutions. Several researches that employed gamification were elicited from databases such as google scholar, Scopus and research gate. The shortcomings encountered during the deployment of gamification in these studies were summarized. The study's findings reveal that he difficulties encountered when adopting gamification in education and training in higher education institutions can be divided into three categories: design concerns, issues with short-term engagement, and problems with user adaptability. Likewise, potential fixes for the issues were mapped out for future designers of educational gamified systems to follow."
Defect Detection in Industrial Soldering Processes Using Machine Learning: A Critical Literature Review,L. Eisentraut; J. Hosch; M. Roytenberg; A. Benecke; P. Penava; R. Buettner,10.1109/ACCESS.2025.3547847,2025,"As electrical devices take on more life-critical roles, such as in autonomous driving, ensuring the quality of solder joints during production becomes increasingly important. Recently, there has been a growing interest in using machine learning techniques for this purpose. However, current research lacks a comprehensive overview that categorizes and analyzes relevant studies based on their specific intervention points within the production process. This literature review aims to examine and evaluate research coverage along three dimensions: intervention points in the process, non-destructive testing methods, and machine learning techniques employed. For this review, 112 conference papers and journal articles published since 2010 were selected from three databases using the PRISMA methodology. These publications were classified into the three dimensions previously mentioned, summarized, and analyzed. Furthermore, the literature core is critically evaluated to identify research gaps and limitations. The analysis shows that most studies focus on solder joint control, with few addressing intervention points in solder paste and component placement. Visual imaging and neural networks are the dominant techniques for non-destructive testing and machine learning, respectively. Despite a variety of literature that uses high-performance neural networks, meeting industrial detection standards often requires tolerating high false alarm rates. The findings contribute to structuring existing research and identifying research needs, particularly in validating these systems and integrating data from various testing methods and intervention points."
Utilizing AI for Enhancing Diabetes-Related Quality of Life: A Systematic Review of Measurement Tools,S. Sagar; S. W. Tiwari; A. Mittal; G. Chand; T. Rawat; P. K. Sharma,10.1109/ICCES63552.2024.10860146,2024,"It is significant to estimate the extent to which chronic disease management makes a contribution to human health employing the quality of life as a health outcome. The first aim was to conduct a comprehensive analysis of the content and measurement of all those instruments that are focused on diabetes-specific HRQOL. That is why in the present work we adhere to the requirements of Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA). A systematic strategy was used to isolate studies that contain indexes of HRQOL for diabetic patients. A total of 17 instruments were identified in the review that we anticipated our included studies may have used to assess diabetes-specific QOL: Self-completed assessment questionnaires such as Appraisal of Diabetes Scale (ADS), Audit of Diabetes-Dependent QOL measure of a similar kind, Problem Areas in Diabetes PAID questionnaire along with Welfare Monitoring Questionnaire as part of standardized survey London General Practice Group Study Rehabilitation Health Profile 1 Poorly Controlled Hypertension. A total of 17 instruments were included in the review: For diabetes self-management, some recommended measures are; Appraisal of Diabetes Scale (ADS), Audit of Diabetes-Dependent QOL measure (ADDQOL), including ADDQOL version I, II, and III. There are six components and tools in ADAQ, which were regarded as one tool for all types of surveys based on the specified inclusion criteria; ADDFRMS is a short form of MINI Addqol with proper validation and accuracy as compared with full length added but made specially since the last two years by the significant difference between both versions distribution, Diabetes Health Profile (DHP); Problem Areas in Diabetes (PAI-D), “Well-being/Quality.[2] We found seven reviews that assessed a more advanced Structure Drawing methodological aspects aggregation of available scores; however, the review defaulted slightly due to including most of the listed tools; psychological questionnaires also resulted in duplicate information and recent findings were restrictive, designating the current practices that were top best possible ways of assessing declines in modifying the Strategy that was quite general."
Smart Villages in Indonesia in the Light of the Literature Review,W. Agustiono,10.1109/ICISS55894.2022.9915061,2022,"In the last few years, smart village has received increased attention from both researchers and practitioners. Especially with the advancement of digital technology that allows the interconnection between rural areas, smart village discussion is getting popular. The objective of this study, therefore, is to gain insights into the existing literature and to present the knowledge of how the smart village research in Indonesia context has been carried out. With more than 74,000 villages and most diverse country, Indonesia offers fruitful context study. This study in particular adopted the recent PRISMA framework to identify eligible articles for further systematic review. Based on this framework, this study was able to identify 133 eligible articles. Analysis indicated that research on smart village in Indonesia has been started since a decade ago. Thematic analysis revealed that three research topics that received most attention from the researches in smart village including application development, IT/IS management, Strategy and Implication for society. Considering the complex problem in rural development, future research with more focus on the implementation of emerging technology such as IoT, AI and big data in producing smart solution."
Identifying the Challenges of Using Artificial Intelligence in Providing Healthcare Services: A Systematic Review,R. Hejazinia; Z. Heydari,10.1109/ICWR65219.2025.11006218,2025,"Artificial intelligence tools in healthcare have gained significant attention recently. However, before adopting any new technology, it is essential to identify the challenges associated with its successful implementation. Numerous studies have examined the challenges of using artificial intelligence in healthcare, but a comprehensive classification is lacking. This study aims to summarize these challenges in healthcare services systematically. A systematic review was conducted following the PRISMA standard. Information sources were searched in PubMed, WOS, and Scopus databases using keywords such as “Medicine,” “Diagnosis,” “Health,” “AI,” “artificial intelligence,” “ChatGPT,” “chatbot,” “risk,” “problem,” and “challenge.” The search covered studies published between 2020 and 2025. Initially, 720 articles were identified. After the initial screening and duplicate removal using EndNote21, 52 articles remained. Following a final screening and quality assessment using the CASP checklist, 29 articles were selected for systematic review. Findings revealed significant challenges in AI adoption in healthcare, including data challenges such as low quality and restricted access, technical issues such as system complexity and high costs, managerial obstacles including unclear responsibilities, social concerns such as trust and inclusivity issues, and legal considerations such as a lack of standardization and oversight. These challenges negatively impact AI efficiency, trust, and acceptability in healthcare. Addressing these issues is crucial to enhancing AI adoption in the field. Proper management of these challenges can improve confidence, efficiency, and acceptance, ultimately facilitating the development of AI technologies in healthcare."
Exploring STEM Career Competencies with the Assistance of Generative AI,N. Karacan-Ozdemir; C. Park; S. Solberg,10.1109/ISEC61299.2024.10665084,2024,"This paper presents preliminary findings of an analysis of STEM career competencies and skills using generative AI. According to PRISMA methodology, 65 research papers and 38 reports dealing with STEM career competencies and skills that met inclusion and exclusion criteria were analyzed with Claude AI, one of Large Language Models. Because of Claude AI's restrictions, some research papers and reports were categorized by their specific focus, such as K-12 studies, studies regarding diverse STEM careers, and fab-lab and maker-space studies, and then they were analyzed separately. Through a combination of the skills and competencies that emerged from each step of the analysis, a final analysis was carried out. Being aware of limitations of LLMs, the authors verified the themes based on the literature and further verification is in process with topic modeling analysis with LDA in R. The collective findings indicated six key STEM skills and competencies: technical skills, problem solving and critical thinking, research skills, soft skills, lifelong learning and adaptability, and other competencies. Researchers found some specific themes in the analysis, including creativity and innovation, communication and collaboration from fab-lab and makerspace research, mathematical and analytical skills, ethical and social responsibility, passion, and dedication from research into diverse STEM careers, self-efficacy from K-12 STEM research, and more social emotional learning (SEL) skills from STEM and employability reports. The findings indicated the importance of foundational technical skills as well as SEL skills for STEM career identity."
"An Iterative PRISMA Review of Deep Learning, Sparse Bayesian, and Compressed Sensing Methods for Channel Estimation in Modern Wireless Systems",V. Niranjane; A. Khobragade,10.1109/ICCES63552.2024.10860064,2024,"Accurate channel estimation is basic to efficient functioning of modern wireless communication systems, particularly in the case of 5G, mmWave, MIMO, and vehicular networks. However, most conventional techniques are usually affected by the complexity of such methods, pilot overhead, and limitations in high mobility or dynamic environments. On the other hand, in spite of a large number of proposed methods, there is a gap in covering a comprehensive and systematic survey to compare several techniques under various performance metrics. Most of the existing review articles focus on very limited sets of models and do not critically compare or contrast models based on their efficiency, accuracy, and computational complexity. To fill these gaps, this paper bases its review on state-of-the-art channel estimation models PRISMA, and included methods span traditional signal processing approaches such as LMMSE, adaptive frequency-domain estimators, and compressed sensing-based techniques, and prominent approaches based on deep learning frameworks including CNN and hybrid estimators. With the PRISMA methodology, this review provides a comprehensive analysis of the methods involved in various applications, such as MIMO, mmWave, RIS-aided systems, and underwater communications. The proposed review, on giving a number of models, discusses the pros and cons of those models. Models based on deep learning bring in an improved accuracy along with the potential of improved computational performance in estimation, whereas sparse and hybrid models have shown impressive performances in heavily resource-constrained environments such as underwater acoustic and high-mobility vehicular communications. This work will prove to be very informative for the researcher and engineer, both in showing optimal models for various applications of communication and in laying down the basis for future integration of GAN and advanced machine-learning methods into channel estimation. The outcomes derived from this review will shape and inform the future design and deployment of wireless systems through the development of a strong platform for harnessing optimization strategies in diverse applications of channel estimation."
A Review of Explainable Recommender Systems Utilizing Knowledge Graphs and Reinforcement Learning,N. Tiwary; S. A. Mohd Noah; F. Fauzi; T. S. Yee,10.1109/ACCESS.2024.3422416,2024,"This review paper addresses the research question of the significance of explainability in AI and the role of integrating KG and RL to enhance Explainable Recommender Systems (XRS). It surveys articles published from January 2015 to March 2024 on XRS, focusing on knowledge graphs (KGs) and reinforcement learning (RL) for achieving explainability in recommender systems. Employing a systematic methodology, it introduces a custom Python-based web scraper to efficiently navigate and extract relevant academic research papers from IEEE, ScienceDirect (Elsevier), ACM, and Springer online databases. The study encompasses the PRISMA methodology to conduct a thorough analysis and identify pertinent research works. This systematic literature review aims to provide a unified view of the field by reviewing eight existing XRS literature reviews and 29 pertinent XRS studies involving KG and RL from the specified period. It categorizes and analyses relevant research papers based on their implementation methodologies and explores significant contributions, encompassing perspectives on model-agnostic and model-intrinsic explanations."
"Machine Learning for Modeling Service Life: Comprehensive Review, Bibliometrics Analysis and Taxonomy",M. Mudabbir; A. Mosavi,10.1109/INES59282.2023.10297884,2023,"This article presents a comprehensive review of the machine learning methods used to model the service life of various products, which is a critical aspect of product development and production. With the recent advances in machine learning, it has become increasingly feasible to utilize these methods for modeling service life accurately. This review provides a detailed examination of the existing literature on machine learning applications for modeling service life, including a bibliometric analysis of the most frequently cited works. Furthermore, this review presents a taxonomy of the various machine learning methods employed in service life modeling, highlighting the fundamental methods such as Artificial neural networks, Support vector machine, and decision trees. The results of this review demonstrate the potential of machine learning methods for accurately modeling service life, while also emphasizing the need for further research in this field. Overall, this article provides a valuable resource for researchers and practitioners looking to apply machine learning methods for modeling service life."
Bibliometric Analysis to Explore the Influence of Artificial Intelligence on Consumer Behavior and Marketing Research: A Comprehensive Review and Suggestions for Future Exploration,C. -J. Fu; A. D. K. Silalahi; I. -T. Shih; D. T. Thanh Phuong; I. J. Eunike,10.1109/ICIMTech63123.2024.10780894,2024,"In the nexus of AI, marketing, and consumer behavior, a systematic consolidation of the theoretical and practical implications remains elusive. Addressing this, our study embarks on a Systematic Literature Review (SLR) with bibliometric analysis, leveraging the PRISMA framework to unearth and integrate findings from existing research. The objective is to map out the scholarly landscape, identifying key themes and gaps within the expansive body of literature on AI's role in shaping marketing strategies and consumer perceptions. Our analysis uncovers a dual impact of AI: operational enhancement and deep-rooted changes in consumer-brand dynamics, highlighting a shift toward more sophisticated theoretical frameworks like the Uses and Gratification Theory and the UTAUT model. The study not only synthesizes these insights but also sets the stage for future research, calling for a broader inclusion of databases to mitigate current limitations and empirical studies to validate and refine theoretical models, ensuring a comprehensive understanding of AI's transformative potential and its ethical implications in the marketing domain."
Explainable Artificial Intelligence in Healthcare Applications: A Systematic Review,B. Costa; P. Georgieva,10.1109/COMSCI59259.2023.10315829,2023,"Current artificial intelligence (AI) advances and progress in medicine created a new challenge for medical AI. The” black-box” nature of AI methods has created a discussion on the use of explainability techniques to build trust and provide transparency in the AI decision-making process. A study of current state-of-the-art approaches in Explainable Artificial Intelligence (XAI) was conducted using Preferred Reporting Items on Systematic Reviews and Meta-analysis (PRISMA) research technology. In this systematic review, we provide an overview of current XAI techniques based on different taxonomies. Finally, we discuss the applications and challenges that come with the application of explainability methods in the healthcare industry."
A Systematic Literature Review on Requirements Engineering Practices and Challenges in Open-Source Projects,M. Tasnim; M. Rayhan; Z. Zhang; T. Poranen,10.1109/SEAA60479.2023.00050,2023,"Open-source software (OSS) development has become increasingly influential in the software industry, promoting collaboration and knowledge sharing among developers and users. Along with rapidly evolving OSS projects, this paper explores requirements engineering (RE) practices and challenges through a systematic literature review (SLR). Synthesizing data from 43 selected papers, the study reports practices, techniques, and methods that assist RE activities in OSS projects, and also addresses challenges faced by practitioners and the potential solutions. The results of the literature review indicate a growing interest in using machine learning and statistical methods to assist RE activities, focusing on automated requirements identification and analysis using information from project discussion forums, issue reports, and other online resources. The findings also highlight the importance of community involvement, with many studies examining developers’ interaction patterns, expertise levels, and influence on projects. These findings provide valuable insights for OSS project managers and researchers, offering guidance on effectively handling requirements in OSS projects."
Literature Review on Optimizing Cash Flow Forecasting Using Machine Learning in Small and Medium Enterprises,E. Fernaldy; Santy; K. Deniswara,10.1109/ICMLAS64557.2025.10968322,2025,"This research aims to assess how the application of machine learning (ML) can enhance the accuracy and efficiency of cash flow forecasting in small and medium enterprises (SMEs) and to identify the relationship between ML adoption and improved financial decision-making within these enterprises. The methodology used is a systematic literature review following the PRISMA framework. The analysis is based on selected papers that meet specific criteria and address the research questions of this study. The findings reveal that various ML models can enhance cash flow forecasting by utilizing financial data and capturing underlying patterns. The study concludes that ML is beneficial for SMEs, helping to reduce insolvency rates through improved cash flow forecasting and enabling better financial decision-making based on wellmanaged financial data."
Systematic Literature Review of Software Estimation in Global Software Development,A. Kasastra; R. Ferdiana; S. Abu Ishaq Alfarozi,10.1109/ICITCOM62788.2024.10762359,2024,"Software development is always characterized by certain parameters. In the context of Global Software Development (GSD), one of the key challenges for software developers is predicting the development effort of a software system based on developer details, size, complexity, and other measures. When development teams are spread apart, it presents a number of challenges. Communication and coordination become more complex, leading to hidden costs associated with managing software development across different locations. Hence, the software estimation models used for co-located software development are not suitable for estimation in GSD. Software estimation in GSD has become a crucial area of research. This paper presents the findings of a systematic literature review on software estimation in the context of GSD, with the aim of providing researchers and practitioners with a comprehensive understanding of the current state of the art in this area. Over the last decade, only a handful of researchers have devoted their attention to estimating efforts in GSD. The results show that estimation methods in GSD have not been widely explored. The COCOMO-II method is the most widely adapted method for GSD, and Time Zone is the most agreed Cost Driver for GSD."
The Future of Software Development With GenAI: Evolving Roles of Software Personas,T. Şimşek; Ç. Gülşeni; G. A. Olcay,10.1109/EMR.2024.3454112,2024,"Recent advancements in Generative AI (GenAI) algorithms have been reshaping the Software Development Lifecycle (SDLC) landscape. These transformations not only enhance the speed, accuracy, and productivity of SDLC tasks but also influence the roles and existence of software personas within the SDLC ecosystem. This study evaluates GenAI's effects on the roles of software personas engaged in the typical SDLC phases: planning, design, development, testing, and deployment. When evaluating these effects, it becomes evident that GenAI's impact varies across roles and their relation to specific SDLC phases and activities. While some roles experience a substantial impact, achieving full AI-driven automation remains a challenge for roles that demand critical thinking, empathy, and other human supervision. Overall, GenAI's penetration will necessitate new skills and a redefinition of roles for many tasks within SDLC, where humans and AI will collaborate to create better, faster, and more efficient software."
"A Systematic Literature Review of Digital Twin Research for Healthcare Systems: Research Trends, Gaps, and Realization Challenges",M. D. Xames; T. G. Topcu,10.1109/ACCESS.2023.3349379,2024,"Using the PRISMA approach, we present the first systematic literature review of digital twin (DT) research in healthcare systems (HSs). This endeavor stems from the pressing need for a thorough analysis of this emerging yet fragmented research area, with the goal of consolidating knowledge to catalyze its growth. Our findings are structured around three research questions aimed at identifying: (i) current research trends, (ii) gaps, and (iii) realization challenges. Current trends indicate global interest and interdisciplinary collaborations to address complex HS challenges. However, existing research predominantly focuses on conceptualization; research on integration, verification, and implementation is nascent. Additionally, we document that a substantial body of papers mislabel their work, often disregarding modeling and twinning methods that are necessary elements of a DT. Furthermore, we provide a non-exhaustive classification of the literature based on two axes: the object (i.e., product or process) and the context (i.e., patient’s body, medical procedures, healthcare facilities, and public health). While this is a testament to the diversity of the field, it implies a specific pattern that could be reimagined. We also identify two gaps: (i) considering the human-in-the-loop nature of HSs with a focus on provider decision-making and (ii) implementation research. Lastly, we discuss two challenges for broad-scale implementation of DTs in HSs: improving virtual-to-physical connectivity and data-related issues. In conclusion, this study suggests that DT research could potentially help alleviate the acute shortcomings of HSs that are often manifested in the inability to concurrently improve the quality of care, provider wellbeing, and cost efficiency."
Achieving Peak Performance for Large Language Models: A Systematic Review,Z. R. K. Rostam; S. Szénási; G. Kertész,10.1109/ACCESS.2024.3424945,2024,"In recent years, large language models (LLMs) have achieved remarkable success in natural language processing (NLP). LLMs require an extreme amount of parameters to attain high performance. As models grow into the trillion-parameter range, computational and memory costs increase significantly. This makes it difficult for many researchers to access the resources needed to train or apply these models. Optimizing LLM performance involves two main approaches: fine-tuning pre-trained models for specific tasks to achieve state-of-the-art performance, and reducing costs or improving training time while maintaining similar performance. This paper presents a systematic literature review (SLR) following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement. We reviewed 65 publications out of 983 from 2017 to December 2023, retrieved from 5 databases. The study presents methods to optimize and accelerate LLMs while achieving cutting-edge results without sacrificing accuracy. We begin with an overview of the development of language modeling, followed by a detailed explanation of commonly used frameworks and libraries, and a taxonomy for improving and speeding up LLMs based on three classes: LLM training, LLM inference, and system serving. We then delve into recent optimization and acceleration strategies such as training optimization, hardware optimization, scalability and reliability, accompanied by the taxonomy and categorization of these strategies. Finally, we provide an in-depth comparison of each class and strategy, with two case studies on optimizing model training and enhancing inference efficiency. These case studies showcase practical approaches to address LLM resource limitations while maintaining performance."
The Impact of Artificial Intelligence on Tourism Sustainability: A Systematic Mapping Review,F. Dalipi; Z. Kastrati; T. Öberg,10.1109/ICCIKE58312.2023.10131818,2023,"Recent advancements in big data, algorithms, and computing power have triggered significant enhancements in artificial intelligence (AI). Almost every aspect of travel and tourism is currently impacted by AI, which can be evidenced in a variety of applications including robots, conversational systems, smart travel agents, prediction and forecasting systems, voice recognition, and natural language processing. In this article, we examine how AI has altered and continues to alter the key operations and processes in the tourism industry, with special emphasis on sustainability. After applying the PRISMA framework to guide our search process, the study identified 69 relevant articles published between January 1, 2018, and November 1, 2022. The mapping results revealed that the field is expanding quickly, despite the noted obstacles and challenges. We identified several factors and challenges that should be considered in order to advance the level of research and development in this area. Among these factors, we emphasize the importance and the need for standardized and multimodal datasets, transformer-based and more advanced representation techniques, and standardized performance evaluation metrics for AI models. Also, based on these challenges, some recommendations are provided, and future research directions are identified."
An Assessment of The Talent Management Practices At Aceh Government: A Way Forward In Leveraging AI,V. Rinata; M. Othman; R. Thinakaran,10.1109/CIITE62244.2024.10987583,2024,"This study aims to assess talent management practices within the Aceh Government and look into the potential use of Artificial Intelligence (AI) to improve existing strategies. The utilization of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) to summarize relevant research aims to enhance talent acquisition, retention, and development within the government sector. This study will conduct a literature review on methods for managing talent and the integration of AI in government, using the PRISMA approach. This article suggests integrating AI into talent management in the Aceh Government. The text is a thorough review that provides practical advice for effectively tackling challenges in talent management. The article seeks to suggest solutions for optimizing talent processes and enhancing organizational performance by exploring the benefits and limitations of AI integration in the Aceh Government. The assessment will provide guidance for the integration of AI in talent management within the Aceh Government, assisting decision-makers in aligning policies with government objectives. The paper will explore the ramifications of AI in talent management, encompassing advancements in efficiency, augmented decision-making capabilities, and tailored development initiatives. This study will tackle issues such as data protection and workforce preparedness, providing suggestions for optimizing the advantages of AI. This study offers a thorough evaluation of talent management within the Aceh Government and proposes the incorporation of AI to improve its efficacy."
Blended Learning Technology During Disease Outbreak: A Systematic Literature Review,T. H. Sirait; I. Gamayanto; A. Ramadhan,10.1109/ICoDSA58501.2023.10276457,2023,"Reflecting on the period of light, universities face various challenges, one of which is the disease outbreak. This challenge forces universities to adapt and think of alternative solutions in handling it. One of the solutions in facing these challenges is using blended learning. Blended learning can be said to be an alternative solution that can be applied in the world of education. This learning method combines face-to-face learning with online learning using information technology. This combination can provide great benefits for universities in anticipating possible obstacles experienced. The purpose of this research is to find out what technologies have been used in the past related to obstacles the disease outbreaks that have occurred before. This research uses the PRISMA flow to conduct an assessment related to the process of systematic review, review, structured evaluation, classification, and categorization based on the results obtained. This study shows that the technologies used are very diverse such as Blackboard (Learning Management System or LMS technology), Zoom for synchronous-based technology, Google Docs for collaboration-based technology, and Whatsapp for social media technology. In addition, there are also other technologies such as Microsoft Hololens (Augmented Reality/Virtual Reality). Not only that, data utilization such as data analysis using LMS data logs (user engagement), the use of Artificial Intelligence (AI), and Simulation Tools are also alternatives that can be utilized."
A systemic approach to machine learning project management,M. P. Uysal; E. Akturk,10.1109/EMR.2024.3503677,2024,"Despite their increasing popularity, artificial intelligence (AI) and machine learning (ML) projects often fail to deliver expected outcomes, frequently ending prematurely or not being deployed. Beyond data and model requirements, AI/ML projects necessitate cross-domain performance, business analysis, effective stakeholder and team management, risk and quality management, and a tailored development approach. These factors underline the need for a systemic project management (PM) approach that addresses the interconnections among business, organizational, team, and PM factors, rather than focusing on isolated AI/ML achievements. Current PM methods like CRISP-DM, TDSP, Scrum, and Kanban may not adequately meet these challenges. Therefore, a systemic PM framework is essential for AI/ML project success, particularly in critical areas such as healthcare. This study argues for a change toward a principle-based, holistic, and systemic PM approach tailored to the performance domains of AI/ML projects. It explores the adoption of the Project Management Body of Knowledge (PMBOK V7) for an ML project at Baskent University Hospital Ankara (BUHA). By combining Systems Engineering and Soft Systems Methodology with a survey on performance domains, we enhance PMBOK V7's applicability to AI/ML, proposing solutions that include conceptual and process models. Findings indicate PMBOK V7 meets AI/ML project requirements but needs adaptations for ML processes."
A Systematic Literature Review of Machine Learning Techniques for Software Effort Estimation Models,P. Brar; D. Nandal,10.1109/CCiCT56684.2022.00093,2022,"Estimating software projects is a challenging but necessary process in software development. Predicting the effort needed to build software is an essential part of the project life cycle. This paper examines a variety of machine learning algorithms for estimating effort. There has been a significant increase in research on effort estimation with machine learning approaches during the last two decades, with the objective of improving estimation accuracy. To forecast effort, the estimation techniques such as expert judgment, COCOMO, analogy based, putnam model, and machine learning are used. The algorithmic models’ low accuracy and unreliable architecture resulted in substantial software project risks. As a result, it is essential to predict the cost of project on an annual basis and compare it to alternative methods. However, the effort prediction using machine learning is still limited because a single technique cannot be treated as best. This paper’s main goal is to present a review of several machine learning approaches for predicting effort."
Exploring Applications of Convolutional Neural Networks in Analyzing Multispectral Satellite Imagery: A Systematic Review,A. Ivanda; L. Šerić; M. Braović,10.26599/BDMA.2024.9020086,2025,"Remote sensing is of great importance for analyzing and studying various phenomena occurrence and development on Earth. Today is possible to extract features specific to various fields of application with the application of modern machine learning techniques, such as Convolutional Neural Networks (CNN) on MultiSpectral Images (MSI). This systematic review examines the application of 1D-, 2D-, 3D-, and 4D-CNNs to MSI, following Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. This review addresses three Research Questions (RQ): RQ1: “In which application domains different CNN models have been successfully applied for processing MSI data?”, RQ2: “What are the commonly utilized MSI datasets for training CNN models in the context of processing multispectral satellite imagery?”, and RQ3: “How does the degree of CNN complexity impact the performance of classification, regression or segmentation tasks for multispectral satellite imagery?”. Publications are selected from three databases, Web of Science, IEEE Xplore, and Scopus. Based on the obtained results, the main conclusions are: (1) The majority of studies are applied in the field of agriculture and are using Sentinel-2 satellite data; (2) Publications implementing 1D-, 2D-, and 3D-CNNs mostly utilize classification. For 4D-CNN, there are limited number of studies, and all of them use segmentation; (3) This study shows that 2D-CNNs prevail in all application domains, but 3D-CNNs prove to be better for spatio-temporal pattern recognition, more specifically in agricultural and environmental monitoring applications. 1D-CNNs are less common compared to 2D-CNNs and 3D-CNNs, but they show good performance in spectral analysis tasks. 4D-CNNs are more complex and still underutilized, but they have potential for complex data analysis. More details about metrics according to each CNN are provided in the text and supplementary files, offering a comprehensive overview of the evaluation metrics for each type of machine learning technique applied."
AI Techniques and Applications for Online Social Networks and Media: Insights From BERTopic Modeling,P. Nedungadi; G. Veena; K. -Y. Tang; R. R. K. Menon; R. Raman,10.1109/ACCESS.2025.3543795,2025,"This study examines the role of Artificial Intelligence (AI) in enhancing personalization, analyzing information dynamics, and developing scalable methodologies within Online Social Networks and Media (OSNEM), with a focus on user protection. Through a systematic review using the PRISMA framework and BERTopic modeling, key AI applications in OSNEM were identified, including fake news detection, sentiment analysis, hate speech detection, big data analysis, bot detection, and insights into public health, disaster relief, and mental health. Although AI techniques and multimodal frameworks have significantly improved content personalization, challenges like algorithmic bias and echo chambers remain. To address these, the implementation of fairness-aware learning models is recommended to ensure personalization stays ethical. Advanced AI techniques, such as Dynamic Memory Networks and Temporal Convolutional Networks, have shown strong capabilities in tracking opinion dynamics and combating misinformation. Additionally, Generative AI offers opportunities for content creation but also raises concerns about misinformation, requiring robust moderation frameworks. Emerging technologies like Artificial Real Intelligence (ARI), which simulate human reasoning and decision-making, could further improve the management of complex online interactions. The study highlights the need for scalable AI methodologies, such as multitask learning frameworks, to efficiently handle the vast amounts of real-time data generated by social media while addressing cross-platform adaptability and computational efficiency."
"A Systematic Literature Review of Deep Learning Approaches for Sketch-Based Image Retrieval: Datasets, Metrics, and Future Directions",F. Yang; N. A. Ismail; Y. Y. Pang; V. R. Kebande; A. Al-Dhaqm; T. W. Koh,10.1109/ACCESS.2024.3357939,2024,"Sketch-based image retrieval (SBIR) utilizes sketches to search for images containing similar objects or scenes. Due to the proliferation of touch-screen devices, sketching has become more accessible and therefore has received increasing attention. Deep learning has emerged as a potential tool for SBIR, allowing models to automatically extract image features and learn from large amounts of data. To the best of our knowledge, there is currently no systematic literature review (SLR) of SBIR with deep learning. Therefore, the aim of this review is to incorporate related works into a systematic study, highlighting the main contributions of individual researchers over the years, with a focus on past, present and future trends. To achieve the purpose of this study, 90 studies from 2016 to June 2023 in 4 databases were collected and analyzed using the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) framework. The specific models, datasets, evaluation metrics, and applications of deep learning in SBIR are discussed in detail. This study found that Convolutional Neural Networks (CNN) and Generative Adversarial Networks (GAN) are the most widely used deep learning methods for SBIR. A commonly used dataset is Sketchy, especially in the latest Zero-shot sketch-based image retrieval (ZS-SBIR) task. The results show that Mean Average Precision (mAP) is the most commonly used metric for quantitative evaluation of SBIR. Finally, we provide some future directions and guidance for researchers based on the results of this review."
A Systematic Review of Data-driven Methods in Emergency Control of Power Systems,A. B. Khan; R. Zhang; Y. Zhang; Z. Y. Dong,10.1109/AUPEC62273.2024.10807390,2024,"Emergency control in power systems is crucial for maintaining stability and preventing widespread outages. Data-driven methods have emerged as vital tools in optimizing response-driven and event-driven emergency controls, particularly in Under-Voltage Load Shedding (UVLS), Under-Frequency Load Shedding (UFLS), and event-driven load shedding (ELS). This systematic review aims to synthesize the current state of research on data-driven methods in the emergency control of power systems, focusing on UVLS, UFLS, and ELS. A systematic review was conducted using the PRISMA framework, examining research papers published between 2014 and 2024. Databases such as IEEE Xplore and ScienceDirect were searched using relevant keywords. Inclusion and exclusion criteria were applied to ensure the relevance and quality of selected studies. The review identified various data-driven techniques, including optimization algorithms, machine learning approaches, probabilistic methods, and real-time data analytics and control, applied in both responsedriven and event-driven emergency controls. Each cluster is analyzed for its application, advantages, disadvantages, and complexity. This review highlights the advancements in these methodologies and suggests future research directions to enhance power system stability during emergencies."
Deep Learning on Automatic ICD Coding for Clinical Decision Support Systems: Review,A. Hanifa; A. E. Permanasari; I. Hidayah,10.1109/iBioMed62485.2024.10875824,2024,"The International Code of Disease is a coding method for diagnosing diseases using a clinical text with many benefits, such as insurance claims, medical bills, and hospital finances. However, this coding method is a high-complexity process with a high risk of failure. Recently, researchers have explored the use of Natural Language Processing and various Classification Models to develop Automatic ICD Coding to encounter the manual coding process by assisting doctors and medical staff. This literature review aims to analyze and provide a comprehensive overview of Automatic ICD Coding research. We conducted the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) diagram as a guiding principle. We created keywords to search relevant papers between 2021 and 2024 on the SCOPUS database. Our review included 25 relevant papers to be reviewed. This literature review shows that the 10th version of the ICD code is the most popular version, with 14 studies, and the highest number of categories is 17080 categories. Our analysis concluded that MIMIC-III, a publicly available dataset, is the most frequently utilized dataset in this research field. Researchers have used English as a dataset language to develop automatic ICD coding systems. Researcher have conducted their novel classification model to be implemented on Automatic ICD Coding Systems. Future research is able to focus on applying deep learning methods to automatic ICD coding systems using datasets in different languages, ICD versions & categories. Addressing these research gaps is crucial for meeting the evolving needs of clinical and healthcare systems."
A Systematic Review of Machine Learning in Substance Addiction,N. F. Zulkifli; Z. C. Cob; A. A. Latif; S. M. Drus,10.1109/ICIMU49871.2020.9243581,2020,"Substance addiction affects millions of people worldwide and there is no cure for addiction. With the emergence of machine learning, it has open doors for healthcare industry to incorporate technology to help healthcare workforce to make better decision in treating patients. By applying machine learning in understanding patients with substance addiction, it can help in determining their treatment. This paper aims to provide a summary of how effective machine learning method is applied in addiction studies in which 11 studies are included in this paper by using PRISMA methodology to find sources."
Harnessing Blockchain and Generative AI to Prevent Certificate Forgery and Enhance Educational Integrity,R. Wiputra; A. Gunawan; Ervin; A. Wijaksana,10.1109/ICSPC63060.2024.10862981,2024,"The growing prevalence of certificate forgery in today’s digital age poses a significant threat that undermines the integrity of educational qualifications. This study explores the potential of blockchain and generative AI to optimize the certificate validation process and combat certificate falsification. The research adopts the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines for literature review. The findings reveal that integrating blockchain and generative AI can effectively reduce certificate forgery, enhance the accuracy and reliability of certificate data, and streamline the employee recruitment process by providing faster and more precise validation. This approach curbs certificate fraud and improves the quality of education and the workforce."
"New Opportunities, Challenges, and Applications of Edge-AI for Connected Healthcare in Smart Cities",M. M. Kamruzzaman,10.1109/GCWkshps52748.2021.9682055,2021,"Revolution in healthcare can be experienced with the advancement of smart sensorial things, Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL), edge analytics with the integration of cloud computing; and connected healthcare is receiving extraordinary contemplation from the industry, government, and the healthcare communities. In this study, several studies are selected published in the last 6 years from 2016 to 2021 The selection process is represented through the Prisma flow chart. It has been identified that these increasing challenges of healthcare can be overcome by the implication of AI, ML, DL, Edge AI, IoT, 6G, and cloud computing. Still, limited areas have implicated these latest advancements and also experienced improvements in the outcomes. These implications have shown successful results not only in resolving the issues from the perspective of the patient but also from the perspective a of healthcare professionals. It has been recommended that the different models which have been proposed in several studies must be validated further and implicated in different domains to validate the effectiveness of these models and to ensure that these models can be implicated in several regions effectively."
"Machine Learning and Deep Learning Approaches for Fake News Detection: A Systematic Review of Techniques, Challenges, and Advancements",O. Bashaddadh; N. Omar; M. Mohd; M. N. A. Khalid,10.1109/ACCESS.2025.3572051,2025,"In response to the escalating threat of fake news on social media, this systematic literature review analyzes recent advancements in machine learning and deep learning approaches for its automated detection. Following the PRISMA guidelines, we examined 90 peer-reviewed studies published between 2020 and 2024 to evaluate model effectiveness, identify limitations, and highlight emerging trends. Our analysis shows that deep learning models, particularly transformer-based architectures such as BERT, consistently outperform traditional machine learning methods, often achieving high accuracy (Acc), precision (P), recall (R), and F1-score (F1). For instance, a BERT-based model reported up to 99.9% accuracy on the Kaggle fake news dataset and above 98% on other public datasets, including ISOT, Fake-or-Real, and D3. Similarly, the GANM model demonstrated robust performance on the FakeNewsNet dataset by integrating text and social features. Transfer learning and multimodal models that incorporate user behaviour and network information significantly improve detection in diverse and low-resource environments. However, challenges persist in terms of dataset quality, model interpretability, domain generalisability, and real-time deployment. This review also underscores the limited adoption of few-shot and zero-shot learning techniques, highlighting a promising direction for future research on handling emerging misinformation with minimal training data. To support practical deployment, we advocate the development of explainable, multilingual, and lightweight models with a greater emphasis on human-centred evaluation and ethical considerations. Our findings provide a foundation for researchers and practitioners aiming to build scalable, trustworthy, and context-aware fake news detection systems for global use."
Algorithms and Approaches for the Vehicle Routing Problem with Pickup and Delivery (VRPPD): A Survey,R. Imam Muslem; M. K. M. Nasution; Sutarman; Suherman,10.1109/ICIC64337.2024.10956290,2024,"This systematic literature review (SLR) investigates recent advancements and trends in Vehicle Routing Problem with Pickup and Delivery (VRPPD), a crucial area in operations research and logistics. VRPPD optimization impacts operational costs, service quality, and environmental sustainability, especially with the growing demand from ecommerce. This review aims to address gaps in current literature, particularly focusing on real-time data integration, weather-related constraints, vehicle capacity, and sustainability. Utilizing the PRISMA methodology, 72 peer-reviewed articles from Scopus-indexed journals (2014-2024) were analyzed. The findings reveal significant progress in hybrid and metaheuristic algorithms, which combine multiple techniques to handle constraints like time windows, route efficiency, and fleet utilization. Despite these advancements, limited attention has been given to integrating weather data and dynamic constraints. The survey also highlights the need for more sustainable and real-time adaptive algorithms. Future research should explore multi-objective optimization, AI integration, and quantum computing to improve the efficiency and scalability of VRPPD solutions, especially in dynamic and high-demand environments such as e-commerce and logistics."
The Use of Artificial Intelligence in Diagnostic Medical Imaging: Systematic Literature Review,L. Hafizović; A. Čaušević; A. Deumić; L. S. Bećirović; L. G. Pokvić; A. Badnjević,10.1109/BIBE52308.2021.9635307,2021,"Diagnostic medical imaging and the interpretation of the imaging results pose a great challenge for the medical profession as the final conclusions are highly susceptible to human error and subjectivity. The necessity for standardization of interpretation of medical images is very necessary to bypass these problems. The only way of achieving this is using a methodology which excludes the human eye and employs artificial intelligence. However, another challenge is selecting the most suitable AI algorithm fit for the challenging task of imaging results interpretation. This study was conducted following PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines published in 2020. Research was done using PubMed, ScienceDirect and Google Scholar databases where the key inclusion criteria were language, journal credibility, open access to full-text publications and the most recent papers. In order to focus on only the most recent research, only the papers published in the last 5 years were evaluated. The search through PubMed, ScienceDirect and Google Scholar has yielded 81, 205, and 520 papers respectively. Out of this number of papers, 26 of them have met all of the inclusion criteria and were included in the research. The observed accuracies of the models and the overall rising interest in the topic denote that this field is rapidly growing and has a great potential to be applied in daily medical practice in the future."
The Adoption and Impact of AI by SMEs for New-Product Development,R. G. Cooper,10.1109/EMR.2024.3478479,2024,"Artificial Intelligence (AI) is transforming business, including new product development (NPD), yet smaller firms face challenges in adoption. This article presents findings from a survey of Irish SMEs (small and medium enterprises) conducted with the Industry Research Development Group (IRDG). Despite AI's potential, the study reveals low AI implementation across 13 NPD applications, but more positive intentions to adopt AI in the near future. Performance improvements from AI are modest, with an average of 27% improvement across five key metrics. Readiness to adopt AI for NPD, however, is weak with SMEs exhibiting low trust, limited senior management commitment, and minimal demonstrated value from AI. The article concludes with five managerial recommendations, emphasizing the urgency for SMEs to leverage AI to boost innovation, and provides a simple process map for AI deployment."
Development of a Framework for Metaverse in Education: A Systematic Literature Review Approach,R. Roy; M. D. Babakerkhell; S. Mukherjee; D. Pal; S. Funilkul,10.1109/ACCESS.2023.3283273,2023,"A more interactive learning environment is made possible by the metaverse, a made-up world with vastly expanding digital spaces. The metaverse is a development in synchronous communication that enables many users to share different experiences. This study proposes a research framework for adopting metaverse in education. A systematic literature review using the PRISMA methodology identified seventy-three research papers on metaverse and education. Also, this research provided various applications, challenges, dominant themes of research, and future perspectives of a metaverse in education. The proposed framework discusses multiple drivers for adopting a metaverse in education. There are few papers in the metaverse for education, so this research tries to fill the gap. This research also proposed twenty-seven future research questions which can addressed by future researchers. This research will benefit students and teachers across universities/ colleges and schools."
Co-Pilot for Project Managers: Developing a PDF-Driven AI Chatbot for Facilitating Project Management,K. Alam; M. Haq Bhuiyan; M. Shafiqul Islam; A. Hossain Chowdhury; Z. Ahmed Bhuiyan; S. Ahmmed,10.1109/ACCESS.2025.3548519,2025,"Our AI-driven PDF Chatbot is specialized for Project Management (PM) Automation and acts as a virtual Project Manager that offers continuous support to global teams. It interprets PDF data like SRS reports and interview transcripts by utilizing Open-Assistant’s SFT-1 12B Model. Insights from interviews of 15 project managers have enriched the knowledge base of our chatbot and ultimately enabled informative responses to the stakeholders of the project. Advanced AI techniques ensure efficient text preprocessing, including tokenization, numerical normalization, lowercasing, removing punctuation, removing extra spaces, recursive character text splitter, and lemmatization. It is primarily tailored for e-commerce project and provides precise guidance based on e-commerce data and risk management factors. With an average cosine similarity of 80.80% and semantic similarity score of 85.21%, it consistently aligns with PDF Contents and optimize the project management phases & methodologies. This innovation enhances Human-Robot Interaction, PM Automation, facilitates decision-making, and enables uninterrupted communication. While AI-driven PDF chatbots like ChatPDF and SciSummary exist, our chatbot is uniquely focused on automating project management tasks, providing tailored insights for e-commerce projects and decision-making, thus offering a breakthrough in PM automation. To ensure the chatbot’s robustness in context-aware responds, we compare our chatbot with ChatPDF and Sci-summary which are some PDF driven chatbots. Making our work available open-source on https://github.com/codewithkhurshed/SPM-project-repo can enhance its accessibility and promote future research opportunities in PDF driven chatbot development."
"The Generative AI Landscape in Education: Mapping the Terrain of Opportunities, Challenges, and Student Perception",Z. Ahmed; S. S. Shanto; M. H. K. Rime; M. K. Morol; N. Fahad; M. J. Hossen; M. Abdullah-Al-Jubair,10.1109/ACCESS.2024.3461874,2024,"Generative AI (GAI) technologies like ChatGPT are permanently changing academic education. Their integration opens up vast opportunities for bespoke learning and better student interaction but also brings about academic honesty issues and the application of real-life educators. This study aims to fill the literature gap regarding the use of multiple GAI tools and their effect on academic outcomes via a comprehensive review. A systematic literature review was performed following PRISMA guidelines to synthesize results on the potential and drawbacks of GAI in educational domains. We included theoretical and empirical papers that used qualitative, quantitative, or mixed-methods study designs. We have also explored conceptual frameworks and the most creative AI applications with a special emphasis on uniqueness and practicability. Experiences, and Perceptions Concerning To compile the information needed we gathered insights into what students were going through by conducting the survey which contains 200 respondents of undergraduate university students gathering insights into the college students’ experiences and perceptions related to GAI used for educational purposes. At the basic level, GAI comprises areas like personalization, task automation, teacher assistance, and efficiency among others, and respective solutions for the immersion of a learner in learning processes to reform directions. However, it generates plenty of challenges such as the question of assessment integrity, the risk that too much automated grading could overwhelm educational value, and relevantly the veracity of AI-generated content as well as the potential disruption to skills like critical thinking, in addition to data privacy and ethical issues. Student Perception Survey the text also indicates that most students, as per the student perception survey found AI systems useful in academic support. However, they also know the other side of the coin and are very familiar with the technology constraints and challenges."
Evaluating the Inclusiveness of Artificial Intelligence Software in Enhancing Project Management Efficiency – A review and examples of quantitative measurement methods,V. Alevizos; I. Georgousis; A. Simasiku; A. Messinis; S. Karypidou; D. Malliarou,10.1109/ACDSA59508.2024.10467463,2024,"The escalating integration of Artificial Intelligence (AI) in various domains, especially Project Management (PM), has brought to light the imperative need for inclusivity in AI systems. This paper investigates the role of AI software in augmenting both the inclusiveness and efficiency within the realm of PM. The research pivots around specific criteria that define and measure the inclusiveness of AI in PM, highlighting how AI, when developed with inclusiveness in mind, can significantly enhance project outcomes. However, there are inherent challenges in achieving this inclusiveness, primarily due to biases embedded in AI learning databases and the design and development processes of AI systems. The study offers a comprehensive examination of AI's potential to revolutionize PM by enabling managers to concentrate more on people-centric aspects of their work. This is achieved through AI’s ability to perform tasks such as data collection, reporting, and predictive analysis more consistently and efficiently than human counterparts. However, the incorporation of AI in PM extends beyond mere efficiency; it represents a paradigm shift in the epistemology of PM, calling for a deeper understanding of AI's role and impact on society. Despite these advantages, the adoption of AI comes with significant challenges, particularly in terms of bias and inclusiveness. Biased AI learning databases, which use shared and reusable datasets, often perpetuate initially discriminatory algorithms. Moreover, unconscious biases and stereotypes of AI designers, developers, and trainers can inadvertently influence the behavior of the AI systems they create. This necessitates a paradigmatic shift in how AI systems are developed and governed to ensure they do not replicate or exacerbate existing social inequalities. The research proposes a methodological approach involving the development of criteria for inclusion and exclusion, alongside data extraction, to evaluate the inclusiveness and efficiency of AI software in enhancing PM. This approach is crucial for understanding and addressing the challenges and limitations of AI in the context of PM. By focusing on inclusiveness, the study underscores the importance of a synergy between technological advancement and ethical consideration, demanding a comprehensive understanding and regulation to mitigate risks and maximize benefits. In conclusion, this paper presents a detailed exploration of AI’s role in PM highlighting both its potential benefits and the ethical challenges it poses. The findings and recommendations of this study contribute to the growing discourse on the need for inclusive AI systems in PM, offering insights for AI developers and Project Managers (PMs) alike."
An Empirical Review of Supervised and Reinforcement Learning Algorithms for Personalized Exoskeleton Robot Training Systems in Neurorehabilitation,S. Narula; R. S. Pol; R. V. Patil,10.1109/ICACRS62842.2024.10841642,2024,"There is an increasing need for developing more personalized machine learning algorithms for the training system of exoskeleton robots that are in use for the purpose of human neurorehabilitation patients. With increased neurological disorders, demand for effective and individually tailored rehabilitation solutions parallels. Traditional methods of rehabilitation are inadequate to keep up with the adjustment of treatment based on the needs of a patient that may result in very poor recuperation results. Current reviews on the applicability of machine learning in neurorehabilitation rarely fully consider the needs and complexities involved in the design of personalized exoskeleton training systems. There are several major limitations in the literature reviews: very often, they do not consider the possibility of integrating machine learning personalization into the processes of exoskeleton training; besides, in most cases, they lack a methodical way to compare algorithm effectiveness in various patient profiles; lastly, they do not give an adequate meta-analysis of the strong and weak points for the existing methodologies. These gaps impede the construction of more effective and more individualized rehabilitation programs. In this respect, the systematic review that will be presented herein will involve a systematic literature review, meta-analysis, and critical appraisal following machine learning algorithms specific for exoskeleton robot training systems. The methods include a systematic literature review by searching, categorizing, and the relevant studies; then meta-analysis is carried out based on the data gathered to give a quantitative assessment of the performance and effectiveness of these algorithms. This review is guided by the PRISMA guidelines for rigorous and transparent reporting. Furthermore, a critical appraisal framework enhances the review so that strengths and weaknesses of the many different machine learning techniques—from supervised learning to reinforcement learning and deep learning models—may be identified and appraised. This review process aims at the following: it outlines procedures for a systematic and comprehensive assessment of personalized ML algorithms and responses to major shortcomings, and as such, identifies new ideas, methods, and solutions potentially useful for further research. The purpose of this work is to follow on the existing constraints systematically and provide an in-depth understanding of the performance of the various algorithms to fully enhance the development on personalized neurorehabilitation solutions. This reviewer's contribution will thus have an effect on improved patient outcomes, optimize rehabilitation processes, and chart a course for future research toward ever more effective and personalized exoskeleton training systems"
"Systematic Literature Review of Software Effort Estimation : Research Trends, Methods, and Datasets",Hariyanto; A. Marjuni; N. Rijati; Z. A. Hasibuan,10.1109/iSemantic63362.2024.10762707,2024,"Without sufficient effort, the development of a software project will be hindered and may even significantly fail, thus jeopardizing the quality of the software developers. Therefore, Software Effort Estimation (SEE) is a crucial activity in software engineering. One of the most important tasks in software engineering is software effort estimation (SEE). SEE has been the subject of several investigations, which have produced a substantial body of new literature quickly. Determining the primary research topics is essential to get a thorough grasp of the frameworks and trends in Software Effort Estimation (SEE) research today. A systematic literature review (SLR) was utilized in this study as an all-encompassing method to identify and examine every significant research issue in this field. Three research topics were identified from 58 journals, including algorithmic techniques, machine learning implementation, and dataset analysis. The algorithmic techniques and machine learning approaches are the most frequently discussed topics."
Post-Covid-19 Pandemic IT Project Management Skills and Challenges,L. Ali; N. Taleb; A. Ali; I. A. Abu-AlSondos; H. Naseem; F. Yousaf; M. Abdelhakim,10.1109/ICBATS57792.2023.10111320,2023,"Since December 2019, the COVID-19 pandemic has harmed social, financial, and work life. The novel coronavirus has caused problems in all business sectors, including Information Technology(IT). Many Projects have been stopped or delayed due to the impact of this pandemic. Most of the companies recognized the importance of IT to achieve competitive advantage and to survive. Companies are investing a lot of money in IT projects. More than 60% of IT project fail. Lack of professional IT project management is one of the main reasons for that fail. Managing IT projects is a complex problem. Crises such as COVID 19 and uncertainty are increasing the complexity and challenges in IT projects management. This paper investigates what are the IT project management Key success factors required during and after the Pandemic. Secondary data were collected from literature review in the field of IT project management. an online questionnaire was used to collect the primary data from 107 IT firms. 323 respondents were participated. It is concluded that the following Key success factors are required: e-communication, centralization of data, online project monitoring, designing new policies and guidelines for incorporating new work culture, gaining and establishing project risk management exercises, especially cybersecurity and data protection by gaining access to the latest tools, establishing a culture for strict usage of Personal Protective Equipment (PPEs) to help project managers complete projects safely, and top management support."
Time-Series Large Language Models: A Systematic Review of State-of-the-Art,S. Abdullahi; K. Usman Danyaro; A. Zakari; I. Abdul Aziz; N. Amila Wan Abdullah Zawawi; S. Adamu,10.1109/ACCESS.2025.3535782,2025,"Large Language Models (LLMs) have transformed Natural Language Processing (NLP) and Software Engineering by fostering innovation, streamlining processes, and enabling data-driven decision-making. Recently, the adoption of LLMs in time-series analysis has catalyzed the emergence of time-series LLMs, a rapidly evolving research area. Existing reviews provide foundational insights into time-series LLMs but lack a comprehensive examination of recent advancements and do not adequately address critical challenges in this domain. This Systematic Literature Review (SLR) bridges these gaps by analysing state-of-the-art contributions in time-series LLMs, focusing on architectural innovations, tokenisation strategies, tasks, datasets, evaluation metrics, and unresolved challenges. Using a rigorous methodology based on PRISMA guidelines, over 700 studies from 2020 to 2024 were reviewed, with 59 relevant studies selected from journals, conferences, and workshops. Key findings reveal advancements in architectures and novel tokenization strategies tailored for temporal data. Forecasting dominates the identified tasks with 79.66% of the selected studies, while classification and anomaly detection remain underexplored. Furthermore, the analysis reveals a strong reliance on datasets from the energy and transportation domains, highlighting the need for more diverse datasets. Despite these advancements, significant challenges persist, including tokenization inefficiencies, prediction hallucinations, and difficulties in modelling long-term dependencies. These issues hinder the robustness, scalability, and adaptability of time-series LLMs across diverse applications. To address these challenges, this SLR outlines a research roadmap emphasizing the improvement of tokenization methods, the development of mechanisms for capturing long-term dependencies, the mitigation of hallucination effects, and the design of scalable, interpretable models for diverse time-series tasks."
Searching for Malware Dataset: a Systematic Literature Review,L. M. Zagi; B. Aziz,10.1109/ICITSI50517.2020.9264929,2020,"Malware is one of the exciting topics widely discussed by both academicians and researchers, but the source list of malware rarely provided. Therefore, this paper aims to write a Systematic Literature Review (SLR) to find which datasets are commonly used by previous researchers. The three journal databases were used in this study, including IEEE, science direct, and ACM. The PRISMA statement was applied to maintain transparency during the literature review. To facilitate the search, the authors also provide limitations during the SLR process (inclusion and exclusion). The inclusion includes: (1) full article fully written in English; (2) peer-reviewed papers; (3) explicitly mentioning the name of dataset or database; and (4) explicitly mentioning the method to find malware characteristics and behavior. While the exclusion consists of: (1) articles written before 2015; (2) book and white paper; (3) article already indexed in another database journal; and (4) paper which is less than four pages. After both filter processes, there are 42 out of 245 articles eligible to answer the stated research question (RQ), which were: (1) where does the researcher usually find the malware database or dataset?; (2) what kind of methods applied by previous researchers to find the malware's characteristics or behavior?; and (3) which platforms that malware usually attacks are? Based on the three RQs, we could conclude that RQ1 recorded for 37 datasets, RQ2 recorded for 47 methods, and RQ3 recorded for six platforms."
Motor Imagery Signal Classification Using Adversarial Learning: A Systematic Literature Review,S. Mishra; O. Mahmudi; A. Jalali,10.1109/ACCESS.2024.3421569,2024,"This paper presents a comprehensive Systematic Literature Review (SLR) on the utilization of adversarial learning techniques in Motor Imagery (MI) signal classification, a key component for enhancing Brain-Computer Interface (BCI) systems. Adversarial learning has shown promise in overcoming the challenges posed by inter-subject variability and limited data, which traditional machine learning techniques often struggle with. By adhering to PRISMA guidelines, a meticulous search across multiple databases, including Scopus, Web of Science, IEEEXplore, PubMed, and ScienceDirect, was conducted, and relevant articles, published and indexed by April 2023, were rigorously selected and reviewed. In total 49 articles have been selected by following PRISMA among which 45 were shortlisted for review after quality check. Our findings highlight a substantial growth in the domain, particularly driven by research contributions from the Asian region, and identify four primary use cases of adversarial learning: data augmentation, domain adaptation, feature extraction, and artifact removal. Popular datasets such as BCI Competition IV’s 2a and 2b are frequently employed alongside advanced pre-processing techniques. Two main adversarial strategies, GAN and adversarial training, have been recognized for their effectiveness in various scenarios. The study reveals high accuracy levels in data augmentation and domain adaptation, demonstrating the potential of these techniques to enhance MI classification. In addition, this review critically examines publication trends, challenges in the field, and the reproducibility of research. The insights gained from this SLR aim to guide future researchers in selecting appropriate datasets, pre-processing methods, and adversarial techniques, ultimately aiding in the design of more robust and accurate BCI systems. This could have significant implications for improving the quality of life for individuals with motor impairments through enhanced practical applications of BCIs."
Code-Mixed Sentiment Analysis Using Machine Learning Approach – A Systematic Literature Review,C. Tho; H. L. H. S. Warnars; B. Soewito; F. L. Gaol,10.1109/ICICoS51170.2020.9299004,2020,"Code-mixed language is ubiquitous. Having been commonly practiced among bilingual communities, code-mixed language has emerged as a common language among social media users. Despite its popularity, the analysis of a code-mixed text is challenging as the text does not typically comply with the monolingual grammar. Therefore, the popularity of social media in the past ten years has raised wide attention to develop methods for analyzing code-mixed text such as extracting popularity sentiment from the text. Machine learning-based classifier such as Support Vector Machine, Naïve Bayes, Decision Tree, Logistic Regression have been widely used to analyze the sentiment. This paper intends to further explore machine learning classifiers, their performances, variables, and most common classifiers for the code-mixed sentiment analysis. Prisma Methodology was used in this paper, extracting 12 from 230 papers that met predefined required criteria, including publication year within the last 5 years. Our findings suggested that the most common classifiers found in the papers were Support Vector Machine, Naïve Bayes, and Logistic Regression. By using the accuracy and F1 as the performance measures, the Support Vector Machine exhibited a better performance compared to Naïve Bayes and Logistic Regression. Thus, this study supported the use of Support Vector Machine, Naïve Bayes and Logistic Regression as the main classifiers for the code-mixed sentiment analysis."
Beyond Automation: A Systematic Review of AI in Employee Recruitment,A. A. N. Eddy Supriyadinata Gorda; K. D. K. A. Wardani; I. G. N. W. H. Saputra,10.1109/ICSCC62041.2024.10690629,2024,"This systematic review delves into the realm of AI's integration within employee recruitment processes. Through bibliometric and content analyses, this study explores the development of research in the implementation of AI in employee recruitment and sheds light on its transformative potential and underlying challenges. A total of 24 related publications obtained using the PRISMA method from the Scopus Database were analyzed. The research findings indicate that the research in implementation of AI on employee recruitment has transformed over the time. The integrating AI into recruitment processes enhances the recruitment process through efficiency, decision-making, reduction of biases, and improvement in candidate experience. However, challenges such as concerns regarding technological complexities, ethical concerns, need for human oversight still need to be carefully addressed. This study provides in-depth insights into both the positive impacts and the barriers that must be navigated when adopting AI in employee recruitment. It highlights the importance of addressing technological and ethical challenges and suggests a hybrid approach that combines the strengths of AI with human expertise. Future research should focus on resolving these issues and exploring the unintended consequences of AI integration to align with strategic goals and ethical standards."
Identifying Abnormal Eating Behavior Patterns with Machine Learning for Early Detection of Eating Disorders: A Systematic Literature Review,K. A. Santoso; M. Maharani; S. Sidharta,10.1109/ICIMTech63123.2024.10780879,2024,"In the field of health, there are common diseases that can have adverse effects on the body and last a lifetime, one of which is eating disorders (ED), which is a mental illness with features of varying degrees related to eating behavior and body image. As such, prediction and early detection of eating disorders are critical to a patient's recovery. To date, machine learning has been implemented to predict and identify eating disorders using a variety of different models and datasets such as interviews/surveys, social media posts and observations. This paper is written according to the PRISMA 2020 guidelines, and aims to answer four research questions, which are Whether ML is an effective way to detect eating disorders, What scales and data types are utilized in the ML methods used to analyze the datasets, What factors should be considered in determining the ML method used, and Between social media and interview datasets, which dataset give more accuracy in predicting eating disorders. There are 30 papers that are material for analysis in the literature review. The results of the analysis indicate that the methods used by the researchers are random forest, support vector machine and logistic regression. In addition, the accuracy achieved by machine learning in predicting eating disorders is very promising and effective. This paper also concluded that ML models used on social media and interview/survey datasets have an almost identical accuracy"
The Application of Data Science at Original Equipment Manufacturers: A Literature Review,C. Haertel; V. Donat; D. Staegemann; C. Daase; M. Finkendei; K. Turowski,10.1109/ACCESS.2024.3444700,2024,"The role of data as a valuable resource has caused significant transformations in various areas of life. Data Science (DS) aims to extract knowledge from data and thus, has gained attraction from organizations aiming to optimize existing processes and uncover previously unknown potentials. DS can be beneficially integrated into the business processes of Original Equipment Manufacturers (OEMs). Therefore, in this study, a structured literature review is conducted to assess the current state-of-the-art of DS in OEMs, especially in the automotive industry and in procurement, offering valuable insights for both researchers and practitioners in DS and OEMs. Several financial, operative, and strategic potentials of DS in the context of OEMs are identified and described. Examples are operational cost reduction, supplier selection and evaluation, forecasts of product demand, and promoted collaboration between stakeholders. Nevertheless, the literature also suggests several challenges in the execution of DS projects. It was observed that OEMs face both technological and procedural obstacles in this area, including the lack of data-driven work culture, inappropriate systems, and deficits with data collection and integration. Mitigating these challenges will be valuable in improving the success rates of DS projects. Further measures to enrich the results of this article are provided. Due to the rapidly evolving character of DS, the application possibilities and challenges might change in the future."
E-Commerce Fraud Detection Based on Machine Learning Techniques: Systematic Literature Review,A. Mutemi; F. Bacao,10.26599/BDMA.2023.9020023,2024,"The e-commerce industry's rapid growth, accelerated by the COVID-19 pandemic, has led to an alarming increase in digital fraud and associated losses. To establish a healthy e-commerce ecosystem, robust cyber security and anti-fraud measures are crucial. However, research on fraud detection systems has struggled to keep pace due to limited real-world datasets. Advances in artificial intelligence, Machine Learning (ML), and cloud computing have revitalized research and applications in this domain. While ML and data mining techniques are popular in fraud detection, specific reviews focusing on their application in e-commerce platforms like eBay and Facebook are lacking depth. Existing reviews provide broad overviews but fail to grasp the intricacies of ML algorithms in the e-commerce context. To bridge this gap, our study conducts a systematic literature review using the Preferred Reporting Items for Systematic reviews and Meta-Analysis (PRISMA) methodology. We aim to explore the effectiveness of these techniques in fraud detection within digital marketplaces and the broader e-commerce landscape. Understanding the current state of the literature and emerging trends is crucial given the rising fraud incidents and associated costs. Through our investigation, we identify research opportunities and provide insights to industry stakeholders on key ML and data mining techniques for combating e-commerce fraud. Our paper examines the research on these techniques as published in the past decade. Employing the PRISMA approach, we conducted a content analysis of 101 publications, identifying research gaps, recent techniques, and highlighting the increasing utilization of artificial neural networks in fraud detection within the industry."
A Systematic Literature Review of Sentiment Analysis in The Malay Language and Its Approach,Z. A. Rahman; H. Sarirah Husin; B. A. Talip; A. H. Mustaffa,10.1109/IVIT62102.2024.10692587,2024,"Sentiment analysis attracts researchers to analyse people's feelings and emotions in written text and classify them as positive, negative, or neutral using natural language processing. Experts broadly apply it in social media analytics, web mining, data mining, and text analysis. Sentiment analysis studies have grown tremendously in research topics, primarily in English, while researchers have conducted limited studies in Malay sentiment analysis. Thus, this research aims to systematically review past studies in Malay sentiment analysis. The study is guided using Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) and combines several research designs. The study uses three notable databases: Web of Science, Science Direct, Scopus, and Google Scholar. There are three approaches most previous studies use in sentiment analysis, namely, lexicon-based sentiment analysis, machine learning, and hybrid approach. 54% use lexicon-based, while 29% implement machine learning, and the remaining about 17% apply a hybrid approach. This paper discusses Malay sentiment analysis and synthesis works of literature domain, data sources, and Malay languages in conjunction with several other language uses and sentiment approaches. In addition to the insights into Malay sentiment, the existing research trends and possible future research are emphasised."
Artificial Intelligence in Analyzing Medical Imaging in Detecting Cancers,M. J. C. Samonte; D. M. D. Antonio; E. J. P. Del Rosario; P. V. E. Contreras; L. H. Salvador; L. A. T. Ogaya,10.1109/ICSTE63875.2024.00038,2024,"Challenging to diagnose early on due to its nature of being our own cells; we likely only realize we have it when it gets bad. Artificial Intelligence is a trending field in computer science whose relevance is attributed to its ability to be a solution to almost every problem. Research is trying to implement Artificial Intelligence in diagnosing cancer from image samples of patients. Through the PRISMA protocol, we systematically obtained literature from 2018 until now for a systematic literature review. Various AI architectures and features are presented and discussed. We also examine the overall reception of research into the application of AI in clinical practices. The analysis and results of this paper are meant to serve as a basis for anyone deciding if AI would be reliable for clinical practice. Our results show that the AI models have enough capability to be a feasible solution to assist doctors in diagnosis, and many researchers show confidence in future applications of AI in the clinic. In our statistical analysis, even when their technique is different, the different implementations perform similarly to others, they have accuracies scores above 80 and they deviate similarly, except for SVM. The analysis and results of this paper are meant to serve as a basis for anyone deciding if AI would be reliable for clinical practice. Our results show that the AI models have enough capability to be a feasible solution to assist doctors for diagnosis, and many researchers show confidence in future applications of AI in the clinic."
Deep Learning for Detecting Building Defects,M. Mudabbiruddin; F. Imre; M. Amir; H. Perez,10.1109/SISY62279.2024.10737579,2024,"This review article identifies the deep learning methods for building defects detection, adapting an updated PRISMA guideline to ensure a rigorous and transparent review process. The review reveals that convolutional neural networks (CNNs) and their variations are the most popular in this field. By systematically identifying and categorizing the most relevant articles, we present a detailed taxonomy of the methods and applications. Additionally, the article explores current trends and discusses future directions, including advancements in real-time defect detection and the utilization of more diverse and comprehensive datasets."
Artificial Intelligence for Improved Maternal Healthcare: A Systematic Literature Review,M. Chemisto; T. J. Gutu; K. Kalinaki; D. Mwebesa Bosco; P. Egau; K. Fred; I. Tim Oloya; K. Rashid,10.1109/AFRICON55910.2023.10293674,2023,"The integration of artificial intelligence (AI) in maternal health is a promising avenue for improving pregnancy, early childhood, and postnatal care. This systematic review analyzed 31 articles retrieved from Web of Science, PubMed, and Scopus, which were classified using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method and Mendeley referencing tool. Our interpretive study found that AI applications in maternal health can predict 48% of maternal complications using electronic medical records (EMR), 29% using medical images, 19% using genetic markers, and 4% using other medical features such as fetal heart rates and sensors. The accuracy of prematurity prediction using AI was 95.7%, while the XGBoost technique predicted neonatal mortality with 99.7% accuracy. The study underscores the potential benefits of AI in maternal healthcare and highlights the need for further research to improve maternal and child health outcomes, especially in resource-constrained sub-Saharan African regions where maternal mortality rates are significantly high."
"Applications of AI-Enabled Deception Detection Using Video, Audio, and Physiological Data: A Systematic Review",S. L. King; T. Neal,10.1109/ACCESS.2024.3462825,2024,"Artificial intelligence-enabled deception detection is an emerging tool for identifying dishonest behavior in a wide range of applications, from security and forensics to politics and lower-risk everyday interactions, addressing the pressing need for enhanced trust and security in an increasingly digital and interconnected world. However, to date, approaches to achieve deception detection with AI have not been evaluated by application area, leaving a disconnect between approaches leveraged and their potential real-world use. Thus, this paper provides a systematic review of application areas for AI-enabled deception detection approaches following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology. Specifically, we discuss 93 articles in detail, (1) identifying common applications of automated deception detection in the literature, (2) enumerating deception detection approaches by application area, and (3) describing publicly available datasets per application area. We also identify open challenges, such as the lack of datasets that support cross-domain deception detection research. By focusing on the application areas of automated deception detection, this review helps to contextualize the surveyed literature and understand the specific challenges and requirements associated with different domains. Further, by examining various application areas, researchers can tailor their approaches and techniques to address the unique characteristics and constraints of each domain. This targeted approach increases the practical relevance and applicability of deception detection methods in real-world scenarios."
Multi-Criteria Measurement of AI Support to Project Management,V. Čančer; P. Tominc; M. Rožman,10.1109/ACCESS.2023.3342276,2023,"This paper aims to measure the level of artificial intelligence (AI) support to project management (PM) in selected service sector activities. The exploratory factor analysis was employed based on the extensive survey on AI in Slovenian companies and the multi-criteria measurement with an emphasis on value functions and pairwise comparisons in the analytic hierarchy process. The synthesis and performance sensitivity analysis results show that in the service sector, concerning all criteria, PM is with the level 0.276 best supported with AI in services of professional, scientific, and technical activities, which also stand out concerning the first-level goals in using AI solutions in a project with the value 0.284, and in successful project implementation using AI with the value 0.301. Although the lowest level of AI support to PM, which is 0.220, is in services of wholesale and retail trade and repair of motor vehicles and motorcycles, these services excel in adopting AI technologies in a project with a value of 0.277. Services of financial and insurance activities, with the level 0.257 second-ranked concerning all criteria, have the highest value of 0.269 concerning the first-level goal of improving the work of project leaders using AI. The paper, therefore, contributes to the comparison of AI support to PM in service sector activities. The results can help AI development policymakers determine which activities need to be supported and which should be set as an example. The presented methodological frame can serve to perform measurements and benchmarking in various research fields."
Toward a Method Engineering Framework for Project Management and Machine Learning,M. P. Uysal,10.1109/COMPSAC57700.2023.00179,2023,"Adopting or tailoring a project management (PM) method for the domain-specific requirements of Machine Learning (ML) and Software Engineering (SE) has been a major challenge. However, the review of the literature cannot provide sufficient work, and thus engineering PM processes for ML remains neglected. Therefore, this paper presents the theoretical and methodological aspects of a study, which is also the third and last part of an integrated research project conducted for Baskent University Hospital Ankara (BUHA). The outputs of the study are twofold: (a) a method engineering (ME) framework for ML PM; and (b) a new hybrid PM for ML projects. The research approach combined the guidelines and principles of Design Science Research and Action Research methods. It is thought that this study may be regarded as promising, and an attempt to improve SE and PM processes of ML in the healthcare domain."
Review of Intelligent Methods for Sign Language-to-Speech Translation,E. C. C. Trujillo; L. F. Castillo; J. Aquino,10.1109/ICALTER65499.2024.10819213,2024,"In today's digital age, technology has significantly transformed communication. However, for the deaf community, the communication barrier remains a persistent challenge. This article addresses the critical need to develop intelligent systems that translate sign language into speech, highlighting its potential to improve the social inclusion of people with hearing impairment. The study conducts a systematic review of advanced methods, such as the use of convolutional neural networks (CNNs) and recurrent neural networks (RNNs), to identify promising techniques and persistent challenges in this field. The research is based on the analysis of 32 articles selected using strict inclusion and exclusion criteria, following the PRISMA statement. The results reveal that these systems can improve the autonomy of deafblind people, facilitating their access to essential services and promoting their social and occupational inclusion. Furthermore, it highlights the need for solutions adapted to specific local contexts due to the geographical variability in the prevalence of hearing impairment. This work not only contributes to scientific knowledge, but also lays the foundation for the development of more effective and accessible technologies, potentially transforming areas such as education, healthcare and employment for the deaf community."
Echo Chambers in Online Social Networks: A Systematic Literature Review,A. Mahmoudi; D. Jemielniak; L. Ciechanowski,10.1109/ACCESS.2024.3353054,2024,"Echo chambers, a recent phenomenon in the realm of social networks, have garnered significant attention from researchers due to their profound implications. Their role in propagating information, reinforcing beliefs and opinions, and potentially fostering inequality within networks and societies underscores the critical need for comprehensive understanding. Despite the lack of a clear definition, existing research has primarily concentrated on five aspects of echo chambers: their attributes, underlying mechanisms, modeling, detection, and mitigation strategies. The main objectives of this systematic review are to identify terminology, examine the effects of echo chambers, analyze approaches to echo chamber mechanisms, assess modeling and detection techniques, and evaluate metrics used to specify echo chambers in online social networks. By doing so, this article aims to illuminate the strengths and weaknesses of current approaches. To conduct this study, a systematic review was conducted of studies published from 2013 to October 2022, peer-reviewed in five prestigious publishers, including ACM Digital Library, IEEE Xplore, Science Direct, Springer, and Nature. The methodology of this systematic review was guided by the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement. Ultimately, 28 studies were selected for the final review. The findings of this study highlight several main limitations. Firstly, there is a lack of an accurate definition for echo chambers. Secondly, there is a lack of a solid approach to address the components of echo chambers. Thirdly, there is a controversial issue regarding the effect of echo chambers. Lastly, the measures used mostly did not adequately specify echo chambers."
Quality Factors Enhancement of Requirement Engineering: A Systematic Literature Review,S. M. Abbas; K. A. Alam; U. Iqbal; S. Ajmal,10.1109/FIT47737.2019.00013,2019,"Software requirement engineering is among the most important issues for starting any software project. The most-reported problem in requirement engineering (RE) is the difficulty to identify quality requirements. Sometime analysts may face incorrect and incomplete requirements, which may become the reason for project failures from the perspective of dissatisfaction of stakeholders. Hence, the quality in each phase of RE is important. The main focus of this article is to categorizing, Identifying, and synthesizing the existing researches on quality enhancement of the RE process. The systematically reviewing the relevant studies based on the Kitchenham systematic review methodology is the main objective of this study. This review identifies the methods which are used for enhancing the quality of RE by improving requirement elicitation, requirement analysis and specification, and requirement validation process. In this research, we are not specific to any phase of RE. Hence, the techniques obtained for quality enhancement purpose can target different phases as we are going to deal with the overall quality of requirements to enhance different quality factors as defined by IEEE standards. In general, 3 defined research questions have been invested for the sake of discussion on explored results. Likewise, 44 articles were selected from an initial set of 101 research papers. In particular, research articles answering formulated question have been included in this SLR."
Intelligent Risk Management in Construction Projects: Systematic Literature Review,L. Chenya; E. Aminudin; S. Mohd; L. S. Yap,10.1109/ACCESS.2022.3189157,2022,"As digitalization leads to the development of the global economy and artificial intelligence stimulates technological innovation, integrating smart management methods into project management is becoming more and more critical to the development of the construction industry. As an influential branch of engineering management, intelligent risk management in the construction field will be a future research direction. Hence, the aim of this study is to find the gaps and future research trends in intelligent risk management field by Systematic Literature Review. In order to achieve these objectives, 436 articles were selected from the WOS and Scopus databases to be analyzed by CiteSpace scientometric software, and the results were classified into collaborative network analysis, co-citation analysis, and public network analysis. From the analysis, China had the highest number of publications, while the United States was the most influential country in this field. The researchers at different institutions in this field have formed research teams despite the lack of collaboration between the authors and their institutions. In co-cited references, the emphasis was on traditional methods and applications of risk analysis, with fewer citations of novel methods. According to the keywords, the clusters of keywords, and the temporal evolution of the keywords, and combining the conceptual model, 5 research gaps areas were discovered. In order to fulfill these challenges and barriers, this study found the future research trends, including: developing digital management platform for intelligent construction management and risk management; building a decision-making system for risk management to find an optimum solution; refining building digital models which would be the basis of digital management platform; identifying and category the characteristic construction risk factors by using machine learning techniques, just like text mining or knowledge graph; building an API to embed decision making system into digital management platform, and the intelligent risk management system in real-time projects to verify its possibility."
A Systematic Literature Review on Machine Learning Algorithms for Human Status Detection,S. K. Sardar; N. Kumar; S. C. Lee,10.1109/ACCESS.2022.3190967,2022,"Human status detection (HSD) is important to understand the status of users when interacting with various systems under different conditions. Recently, although various machine learning algorithms have been applied to analyze and detect human status, there are no guidelines to utilize machine learning algorithms to analyze physical, cognitive, and emotional aspects of human status. Therefore, this study aimed to investigate measures, tools, and machine learning algorithms for HSD by applying a systematic literature review method. We followed the preferred reporting items for systematic reviews and meta-analysis (PRISMA) model to answer three research questions related to the research objective. A total of 76 articles were identified using two hundred keyword combinations addressing topics under HSD in the fields of human factors and human-computer interaction (HCI). The results showed that research on HSD becomes important in industrial systems, focusing on how intelligent systems based on machine learning (ML) differ from earlier generations of automated systems, and what these differences necessarily imply for HCI to design and evaluation. The tools used to collect data for HSD on different parameters are broadly discussed. Recent HSD studies seem to focus on cognitive load and emotion, whereas prior studies have focused on the detection of physical effort. This research assists domain researchers in identifying HSD approaches using different ML algorithms that are suitable for use in their research."
Dimensions of Automated ETL Management: A Contemporary Literature Review,G. S. S. Kumar; M. R. Kumar,10.1109/ICACRS55517.2022.10029274,2022,"ETL solutions are becoming more widely used because of the increasing complexity of data systems and the importance of high-quality data sources and data processing for making decisions. The raw data is retrieved and placed into a designated warehouse for efficient information analysis and processing. Using data management technologies to acquire insights into functional and operational elements is one of the critical components of software engineering in the present environment. There is a need to improve ETL performance for real-time applications. However, several scholarly and business investigations have been into the efficacy of data management dynamics and ETL technologies. As a result, it is essential to look at methods of increasing the effectiveness of ETL processes to meet the immediate demands of operations. This paper explores existing ETL systems' history, constraints, potential, and how machine learning models are integrated into ETL processes. However, as evidenced by a review of relevant academic literature, there is widespread support for using machine learning models to improve and perfect ETL applications. Although many machine learning models can be helpful in ETL processing, only a tiny fraction of market products use them all. Machine learning-based models optimize ETL-based data management, concentrating on limits and future potential."
Leveraging Extravagant Linguistic Patterns to Enhance Fake Review Detection: A Comparative Study on Clustering Methods,M. Ennaouri; I. Ettahiri; A. Zellou; K. Doumi,10.1109/ICECCE63537.2024.10823574,2024,"Online reviews are crucial in shaping consumer behavior and business success, but the growing presence of fake reviews threatens the credibility of these platforms. This paper introduces a novel approach to fake review detection by focusing on the use of extravagant language words that are excessively positive or negative, often used to manipulate opinions. We explore how extravagant words can serve as strong indicators of review authenticity. By leveraging advanced clustering methods and BERT embeddings for linguistic feature extraction, we examine patterns in deceptive and genuine reviews. Two diverse datasets are utilized to validate our approach, and the model's performance is evaluated through accuracy, precision, recall, and F1-score. Our findings demonstrate the effectiveness of extravagant words in enhancing the detection of fake reviews and provide valuable insights into improving the reliability of online review systems. This approach offers practical implications for boosting consumer trust and enhancing the integrity of digital platforms."
A Systematic Review on Crop Leaf Disease Identification Using Machine Learning and Deep Learning Techniques,S. K. Shah; V. Kumbhar; T. P. Singh,10.1109/ICCUBEA58933.2023.10392042,2023,"World Population is increasing drastically and is expected to be around 12.3 billion in 2100. With the increase in population, it becomes important to foster the basic needs of mankind suitably. To withstand larger food requirements, getting higher crop yields is important. However, challenges like crop diseases, environmental factors like soil type, soil moisture, nutrient contents in soil, climate change, etc., affect various crop yields severely. The modern technologies can help in minimizing the adverse effects of these factors on crop yields. The advent of advanced automation techniques in Artificial Intelligence (AI) & Machine Learning (ML) can help in automated detection of crop diseases more accurately. However, use of such automated techniques face several challenges. Current study attempts to present a systematic literature review (SLR) of the existing body of knowledge in crop disease recognition & classification using automated techniques. The review followed PRISMA guidelines & searched four major databases with a comprehensive search query. 60 relevant research studies were identified after filtering through systematic inclusion & exclusion criteria for further analysis. Each research article is examined for the crop species used, the dataset used, & the methodology used. In each of the studied research papers, we looked at the performance measures utilized to assess the overall acquired findings for crop disease diagnosis & classification. It is observed that such systems have great potential to improve food security & agricultural productivity. However, more research is needed to overcome the challenges & limitations of the current methods and to ensure their validity & reliability in different settings and platforms. The readers of this study could acquire challenges faced by researchers in automated crop disease identification systems & their potential solutions to improve the performance of advanced algorithms."
A Systematic Review of Process Mining in Healthcare,H. Al-Badarneh; A. Arif,10.1109/ICTCS65341.2025.10989313,2025,"Process Mining (PM) is a technique that integrates both data analysis and business process management to analyze processes. It includes a set of activities that are important for improving processes, especially in the healthcare sector as it faces many challenges. Healthcare processes usually suffer from inefficiencies, which can cause harm to patients or increase expenses. Process mining is a powerful approach for analyzing and optimizing processes to solve inefficiencies, with benefits such as: optimizing workflows, improving patient care, and reducing cost. Recent reviews did not focus on PM in healthcare in general, whereas this systematic literature review does, providing a holistic view of how PM can be applied to healthcare. It was conducted following the PRISMA methodology. First, 59 studies were identified across many databases (IEEE Xplore, ACM, and Emerald Insight), using appropriate search strings. Results were filtered to the last 5 years and only considered peer-reviewed Journals. Which resulted in 14 studies that were chosen for analysis. The following information was gathered from each study: Objectives and outcomes, PM activities, PM perspectives, algorithms, tools, facilities, fields, and process type. Studies were categorized based on their common objective, and the following categories were found: Clinical pathway analysis and healthcare Simulation for Crisis Management, Predictive Analytics, Process Conformance, Cross-Organizational Process Management, and Operational Efficiency & Resource Management. Key findings imply that process mining is taking place in various healthcare settings—from emergency departments to cancer centers to ICUs—and that it's a powerful tool to enhance clinical pathways, and improve resource management. Also, the use of process mining in ML/AI has improved predictions for patient outcomes. However, challenges remain in data standardization, data quality, scalability, and integration with AI and predictive analytics as diverse EHR formats hinder establishing a unified framework for PM."
A Systematic Review on Federated Learning in Medical Image Analysis,M. F. Sohan; A. Basalamah,10.1109/ACCESS.2023.3260027,2023,"Federated Learning (FL) obtained a lot of attention to the academic and industrial stakeholders from the beginning of its invention. The eye-catching feature of FL is handling data in a decentralized manner which creates a privacy preserving environment in Artificial Intelligence (AI) applications. As we know medical data includes marginal private information of patients which demands excessive data protection from disclosure to unexpected destinations. In this paper, we performed a Systematic Literature Review (SLR) of published research articles on FL based medical image analysis. Firstly, we have collected articles from different databases followed by PRISMA guidelines, then synthesized data from the selected articles, and finally we provided a comprehensive overview on the topic. In order to do that we extracted core information associated with the implementation of FL in medical imaging from the articles. In our findings we briefly presented characteristics of federated data and models, performance achieved by the models and exclusively results comparison with traditional ML models. In addition, we discussed the open issues and challenges of implementing FL and mentioned our recommendations for future direction of this particular research field. We believe this SLR has successfully summarized the state-of-the-art FL methods for medical image analysis using deep learning."
Using Artificial Intelligence to Encourage Creativity in Student Decision-Making: A Literature Review,V. Huayllani-Palomino; G. Bernardo-Santiago; W. Auccahuasi,10.1109/ICACRS62842.2024.10841663,2024,"Artificial intelligence is causing a change in the way of working in educational processes, one with the use of new technological tools, based on the use of artificial intelligence, is causing changes in the creative processes in students. The objective of this article is to identify and analyze articles selected between the years 2020 to 2024 that propose programs and strategies to systematically insert the creative capacity of students without the use of artificial intelligence, as well as the strategies that are being taken as a result of the presence of artificial intelligence. The search was carried out in journals indexed in the Scopus, Scielo, Ebesco, Wos and Dialmat databases, applying inclusion and exclusion criteria through the Prisma Method, in the same way, the boolean operators ""and"" and ""or"", ""quotation marks"" were used, which resulted in 36 articles, which are divided into 2 groups of 20 articles for each group. Concluding that almost all the authors of the articles reviewed have similarities in their proposals and strategies regarding creative thinking because it is of utmost importance within education, with the help of the tools that artificial intelligence provides us, these proposals are changing in such a way that we must improve the strategies to be able to exploit the tools to benefit from improving the creative aspects of the students."
Condition Monitoring and Fault Diagnosis of Wind Turbine: A Systematic Literature Review,M. Hussain; N. Hussain Mirjat; F. Shaikh; L. Luxmi Dhirani; L. Kumar; A. K. Sleiti,10.1109/ACCESS.2024.3514747,2024,"Wind energy penetration has considerably increased in the recent past. However, wind turbines are often prone to various faults which may lead to failures causing huge production and economic losses with increased downtime. To reduce this production and economic loss. It is therefore clear that early detection of these failures can be achieved through an appropriate condition monitoring approach. Various approaches are reported for predicting the condition of wind turbines. However, deploying a costly condition monitoring system with additional data accusation devices poses a challenge for windfarm owners. To address this challenge this study employing Preferred Reporting Item for Systematic Literature Review and Meta Analysis (PRISMA) provides a detailed review of various approaches used for the wind turbine condition monitoring. The key objective of this study is to find out the most frequently used and reliable method of wind turbine condition monitoring, focusing particularly on the SCADA-based approach due to its practical advantages and widespread adoption in the industry. Additionally, this review considers the distinctive concept of machine learning model building which includes data input and its processing, feature selection, model building and its evaluation to analyze the research issues. The review findings concluded that amongst various condition monitoring techniques, SCADA based data driven approach is most popular as it does not require additional sensors, blade mount cameras, unmanned arial vehicles and a separate data accusation unit. Nevertheless, condition monitoring results based on SCADA approach to provide varying predications for differently located wind farms which is a pertinent knowledge gap. This review study provides some detailed insight into various condition monitoring approaches of wind turbines and recommendation to consider any of these based on available resources."
Commit Classification Into Software Maintenance Activities: A Systematic Literature Review,T. Heričko; B. Šumak,10.1109/COMPSAC57700.2023.00254,2023,"Commits represent an essential part of software development practices, serving as the means for collaboration and management of software changes made to a software project’s codebase. Changes are necessary for the survival of software, for instance, to fix faults, address security vulnerabilities, meet new functional requirements, and improve software performance. To aid software research and practice, several research endeavors attempted to automate the process of identifying the nature of the maintenance tasks performed in a commit based on the data associated with the commit. This paper presents a systematic literature review of supervised-learning-based models for commit classification into maintenance activities. Through the study selection process, 19 primary studies were identified, published between 2008 and 2023 in various journals and conference proceedings. For the independent variables for classification, features extracted from commit messages are prevalent, followed by features extracted from source code and commit metadata. In the majority of existing studies, multi-class classification approaches are used, whereas multi-label approaches are considered rarely. The most commonly used classification algorithms include Decision Tree, Random Forest, Naive Bayes, and Neural Network. Several datasets exist, consisting mainly of commits from different open-source projects based on the Java programming language. Some research gaps and open challenges were identified that can guide future research efforts."
Automated Essay Scoring Using Machine Learning,J. S. Kusuma; K. Halim; E. J. P. Pranoto; B. Kanigoro; E. Irwansyah,10.1109/ICORIS56080.2022.10031338,2022,"Essays are frequently employed in the educational system to gauge students' comprehension of particular subjects. However, marking essays requires a lot of time and work and could be prejudiced. In order to save time, lessen human effort, and eliminate biased scoring, automated essay scoring tries to automate scoring. Due to its lack of transparency, limited language support, and requirement for tagged data for the target prompt, which is not always available, AES is still not frequently utilized. This study's goal is to examine automated essay scoring methods. The PRISMA Flow Diagram is used in this study to conduct a systematic literature review. Studies that were released between 2016 and 2021 were found. Information pertinent to the research topics is taken from these studies and then processed to provide a response. Datasets, methods, and models are found in the publications. The performance score of models utilizing the same dataset is then used to compare them. According to the study, AES uses feature engineering and deep learning as its two core methodologies. More scholars are currently researching the deep-learning methodology. CNN, LSTM, and BERT are a few examples of neural network models used in the deep learning method. Most studies use the average QWK and the ASAP dataset as performance metrics. SBLSTMA (Siamese Bidirectional LSTM Neural Network Architecture) and BERT + handcrafted-features, both with 0.801 average QWK, are the models with the highest performance score on the ASAP datasets."
Trends Of Machine Learning Techniques for Enhancing Court Decision Making,J. Meza; M. C. N. Cejas; M. Vaca-Cardenas; M. F. L. Saltos; J. C. M. Intriago,10.1109/ICEDEG61611.2024.10702057,2024,"Artificial intelligence (AI) techniques have increased in different fields and lately it supports several Decision Support Systems (DSS). It can assist the judicial decision-making process. Prediction is possible in various cases, such as predicting the outcome of construction litigation, crime-cases, parental rights, divorces, and tax law. Machine learning (ML) methods can function as support decision tools in the legal system. Aimed to impart a systematic literature review (SLR) of studies concerning the prediction of court decisions via ML. It analyzed the ML methods and techniques used in predicting court decisions. It used Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA). Data were chosen from three databases: IEEE Digital Library, Scopus, and Science Direct. Outcomes found: types of judicial decisions using the ML techniques for law text interpretation, transit, crime, human right, legal assistance and electoral field; on the other hand, scholars agree on the most used ML method have been supervised, unsupervised, and deep learning. The ML has been applied in developed countries such as China, India, US, Russia, and, in recent years, in undeveloped countries: Thailand, Morocco, and Colombia. A few of society agree on the application to legal tasks of the empirical and quantitative methods on which the algorithmic models of ML. The AI is working as (DSS) in the first time, but the final decision is the judge; therefore, judges and artificial intelligence practitioner’s need work together to improve the second stage and improve the court decision."
Machine Learning in Diabetes Modeling,S. Ardabili; A. Mosavi; I. Felde,10.1109/SISY60376.2023.10417934,2023,"Machine learning (ML) has become an integral component of diabetes research. Its applications encompass prediction, identification, classification, and diagnosis of diabetes-related conditions. To comprehensively explore the real-world implications of machine learning in diabetes, it is imperative to initiate the publication of an efficient survey article from computer science perspective. A standardized database tailored for systematic reviews has been established, encompassing a substantial collection of 1150 articles. Consequently, the predominant objective of this study is to introduce an enhanced PRISMA-based methodology for reviewing and evaluating the performance of machine learning in diabetes contexts. The results of this research cannot be used for the purpose of meta-analysis. Instead, its suitable to have an insight on general performance of ML models which generally outperform other ML methods. The results indicate the most pupular and effective ML in diabetes research. The findings underscore accuracy as the most reliable and comprehensive yardstick for assessing machine learning techniques across various diabetes applications. Furthermore, artificial neural networks (ANN) and random forests (RF) exhibit exceptional suitability, particularly when juxtaposed against alternative machine learning approaches. The majority of investigations predominantly align with detection or estimation pursuits. Additionally, we advocate for the integration of hybrid machine learning techniques in ongoing diabetes research endeavors."
Machine Learning and Mathematical Models for Prediction of Structural Aging Process,M. Mudabbiruddin; A. Mosavi,10.1109/SACI58269.2023.10158652,2023,"This paper is a thorough and organized overview of the literature on the development of mathematical models to evaluate aging and the necessity of machine learning techniques in many failure prediction applications. It discusses the important elements of the aging phenomena, such as the mechanical, electrical, and thermal aging processes and how they affect the system’s efficiency. Throughout the research approach, the classification of the aging process and a description of its mathematical modeling are included as state-of-the-art of aging models, performance, and its evaluations. Whereas, in today’s world, the adaptation and importance of different machine learning (ML) methods like deep learning (DL), decision trees (DT), conventional neural networks (CNN), support vector machines (SVM), regression analysis, and artificial neural networks (ANN) are described briefly which helps to increase the efficiency of modeling of aging process. In order to give a comprehensive review of the many ML models utilized in this domain, the research where ML models are compared based on a qualitative evaluation of durability, quality, efficiency, and its rapid performance is specifically explored in this work. The analysis results of ML models provide a detailed overview of the various methodologies within the context of a review. As a result, the research reveals that using ML and artificial intelligence techniques is much more efficient than using simple mathematical modeling. The development of forecasting analysis offering excellent efficiency and economical solutions was significantly assisted by the use of ML techniques to simplify the complicated mathematical equations describing physical phenomena of structural aging. Due to the enormous advantages and capabilities of ML, researchers are applying ML techniques and combining them with its other methods in an effort to find prediction models that are more precise and effective. Researchers and engineering managers can use this survey as a reference to help them select the best machine ML technique for their particular prediction problem."
The Effects of Cyber Security Attacks on Data Integrity in AI,R. Vadisetty,10.1109/ICEC59683.2024.10837148,2024,"The benefits of new technology are becoming increasingly apparent to organisations as digital transformation continues. However, as technology becomes more widely used, cybersecurity threats and attacks are also becoming more common. Therefore, in order to combat constantly changing threats, increasingly advanced protections are needed. One potential solution could be to use AI. In order to evaluate the efficacy of artificial intelligence (AI)-based solutions in comparison to more conventional methods of cyber defence, the researchers in this study conducted a systematic literature review (SLR). Using the PRISMA flow diagram, the review procedures were sketched out. The other seventy-three papers were culled from scholarly publications that were searchable in EBSCO Host, Google Scholar, Science Direct, ProQuest, and SCOPUS between 2018 and 2023. Findings suggest that AI has the potential to improve cybersecurity in several areas, including automation, threat intelligence, and increased cyber defence. On the other hand, it raises new challenges, like adversarial attacks and the need for high-quality data, both of which might make AI less effective."
Web-based systems for inventory control in organizations: A Systematic Review,G. Misahuaman; A. Daza; E. Zavaleta,10.1109/SNPD51163.2021.9704993,2021,"Currently, large companies in the sales and warehouse environment, there are realities where they perform their inventories in a physical way, is have technological service, therefore, not running a stock management, etc. That is why Web Systems or Web Applications for inventory control, is very important for the efficiency of the areas they represent in small, medium and large organizations, where a change is reflected through the use of web technologies. The objective of a Web System is to manage the different processes that are represented in a company, where primarily based on their requirements, it is necessary to study, analyze, apply, validate, all the factors that compose it, and thus generates an impact, with the benefit of generating cost savings in hardware and software, and as a result there is efficiency, at the same time can include all the needs of the business. Therefore, the present work aims to collect and review the approaches made for the implementation of a Web System, the various methodologies or frameworks and frameworks that are used in the implementation of web systems, from a literature review and an analysis of the current state of the art of web systems. Eighty-five articles were found and 26 articles were selected, including those meeting the inclusion and exclusion criteria corresponding to the research questions. As a result, we obtained the proposed information, as a proposal for a Web system that can be implemented and evaluated later in another research."
Challenges of Implementing Project Management Frameworks in Small and Medium-sized Software Enterprises,U. S. Jui; N. Akther; N. Jahan; M. Rahman; H. Sarwar,10.1109/SEAI62072.2024.10674233,2024,"Project management studies and practical applications have thoroughly analyzed organizations in both the public and private sectors. However, the growing need to improve the Small and Medium-sized Enterprises (SMEs) sector is crucial for enhancing national development and ensuring its sustainability. Research has shown that a large proportion of SMEs struggle with project management challenges, resulting in a concerning failure rate of around 80% during the first five years of their operations. This statistic highlights the crucial requirement for a comprehensive project management methodology to enhance global success rates and extend the existence of SMEs. This study involved conducting a survey-based investigation into the challenges encountered by SMEs when incorporating project management frameworks into their way of operating. In the dynamic field of software development have faced many challenges when it comes to implementing organized project management frameworks. This research explores the challenges in software development, including limited resources, skill gaps, and the need for specialized tools that align with the distinctive characteristics of the industry. A comprehensive examination, based on careful data collection, reveals the challenges presented by combining traditional project management strategies with the agile and iterative nature of software projects. This investigation aims to strengthen SMEs by bridging the gap between theory and practice. It seeks to help SMEs navigate their growth path more effectively and efficiently in a competitive market. This paper aims to enhance scholarly discourse by offering a comprehensive understanding of the complex challenges faced by SMEs in the software sector, as well as providing practical solutions."
Supervising multidisciplinary final-year engineering students to develop CubeSats with an innovative project management method,S. Luo; E. K. Soh; A. P. Loh,10.1109/FIE.2018.8658780,2018,"It has been shown that developing nano-satellites is a good platform to motivate and educate students in Science, Technology, Engineering and Mathematics (STEM) disciplines. In this paper, the authors have explained an innovative project management method, called major-project-review, and its implementations to supervise different batches of multidisciplinary final year engineering students to develop a CubeSat as their Final Year Projects (FYPs) in three to four years. The major-project-review approach has been successfully verified in our first CubeSat project, Galassia, and is continuously used to develop our second CubeSat, Galassia2. From the success of Galassia and student feedbacks, the method has been proven to be very efficient to control and monitor the CubeSat development progress, and to motivate and educate engineering students. With our continuous implementations in Galassia2 project, we hope to collect more experiences and data to further optimize the approach."
Application of Forecasting Models in Electrical Engineering: A Systematic Literature Review,Z. Koubaa; A. E. Amraoui; F. Delmotte; A. Frikha,10.1109/IC_ASET61847.2024.10596246,2024,"Electricity demand forecasting stands as a paramount factor in the operation and strategic planning of energy production and distribution systems. This article aims to present the application of forecasting models in electrical engineering area through a systematic literature review of 50 papers (45 journal articles and 5 conference papers) published between 2016 and 2023. In fact, electric demand forecasting plays a crucial role in decision making in the realization of the next-generation power system concept, such as robust sizing of the power system and efficient energy management. Nevertheless, the existence of various forecasting models poses a challenge for researchers, making it difficult to select the best model. Therefore, a main objective of this work is to identify new potential applications for addressing challenges in the selection of forecasting electricity models. To conduct our analysis, we employed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology as a formal guideline for data collection. The results reveal that Artificial Intelligence models are frequently utilized for long-term electricity forecasting, comprising 42% of the cases studied. Moreover, the review papers typically used key performance indicators to compare and select the best forecasting model. However, researchers are confronted with the task of selecting the most appropriate model. Based on the findings from our review, we highlight novel research direction the sustainability aspect within the domain of forecasting electricity demand."
The Artificial Intelligence Revolution in New-Product Development,R. G. Cooper,10.1109/EMR.2023.3336834,2024,"Artificial Intelligence (AI) is poised to revolutionize all aspects of business, particularly new-product development (NPD). Currently, our approach to NPD has remained largely unchanged for decades, yielding stubbornly poor results: only 30% of NP development projects become commercial successes. However, the AI revolution is set to alter this landscape significantly! Leading early adopter firms demonstrate that AI not only finds many applications in NPD but also offers substantial payoffs, such as 50% reductions in development times. This article provides an outline of the diverse and powerful applications of AI in NPD, offering numerous examples from leading companies. Examples include GE's use of digital models and twins to quickly test product designs in turbine development; BASFs use of AI to identify new molecules for use in customer formulations; and AI to generate new-product ideas, identify new-product opportunities, and even create new-product concepts. Our exploratory journey begins at the idea stage and traverses the entire new-product process to the postlaunch period. While AI might still resemble science fiction to many, that future is no longer fiction—it is here now. AI has arrived in full force! With an adoption window of about 13 years, the time is now to embrace AI in NPD in your business. AI will become a major milestone in NPD, perhaps the most important, within the decade."
Technological Evolvement in AAC Modalities to Foster Communications of Verbally Challenged ASD Children: A Systematic Review,W. Farzana; F. Sarker; T. Chau; K. A. Mamun,10.1109/ACCESS.2021.3055195,2021,"Augmentative and Alternative Communication (AAC) emerged as a combination of methods or strategies that constitute any device, such as Speech Generating Device (SGD), Program (mobile applications), Procedure (PECS, Picture Exchange Communication System), which enhances individual’s communication ability. Autism Spectrum Disorder (ASD) is a spectrum of comprehensive neurodevelopment disorder that leads to speech impairments, repetitive behavior, and social communication difficulties; therefore, it is imperative to underscore that at the core of all impediments are communication impairment. This article represents a systematic review of research initiatives that investigate multi-modal AAC strategies and functionality, features of mobile applications to reinforce communication and communal skills in verbally challenged ASD children because other researches are focused only on low or high-tech AAC or interventions to provide insights on ASD children respond to a particular approach. Following the PRISMA method, a total of 60 (January 2015 to October 2020) research articles were reviewed, indexed by Springer, Science Direct, Scopus, ACM, IEEE databases, and published in the AAC journal. The selected research articles are categorized into different themes where most of them focused on interactive mobile applications to improve emotional, social, learning, and overall communication skills in verbally challenged ASD children. This systematic review provides an outline of the paradigm shift in AAC modalities from PECS to Artificial Intelligence (AI), Machine Learning (ML), and Augmented Reality (AR) based applications. It opens up underline future opportunities to integrate intelligent analytics features in mobile applications to strengthen communication skills in verbally undermined ASD children."
The Impact of Artificial Intelligence in the Creative Industries: Design and Editing,E. Clarencia; T. G. Tiranda; S. Achmad; R. Sutoyo,10.1109/iSemantic63362.2024.10762015,2024,"The rapid development of artificial intelligence has had a significant influence in various ways, one of which is in the creative design and editing industry. The use of AI in the creative sector has multiple impacts and new challenges for the creative industry. AI as a tool can increase the efficiency and productivity of creative workers. However, the rapid development of AI can also cause job displacement, which becomes a challenge for creative workers. Therefore, creative workers need to adapt to the use of AI in the creative industry by equipping themselves with skills and knowledge in operating AI as a collaboration tool. In addition, it is also necessary to consider the ethics of using AI in the creative industry. For example, privacy, copyright, and transparency should be maintained so as not to cause ethical problems such as plagiarism and data biases. This research aims to determine the impact of AI and provide an understanding of its ethical use in the creative industry. This research was conducted using the PRISMA method to collect research papers related to this study. This research also investigates how creative workers deal with the challenges that arise due to the use of AI in the creative industry."
Review: Using AI for Structural and Morphological Analysis of the Features of Deposits,Z. Gulnara; A. Shakhatova; A. Makasheva; A. Temirgali; A. Zhalel,10.1109/LINDI63813.2024.10820428,2024,"This article provides a review of the application of artificial intelligence (AI) methods in geological research over the past 20 years (2004-2024). Using the Scopus database and following the PRISMA methodology, a systematic assessment of publications was conducted based on the keywords “geology” and “artificial intelligence.” A total of 900 relevant documents were identified. The analysis of publications revealed that the leading countries in this field are China, the United States, and India. The reviewed studies highlight various AI techniques, including neural networks, genetic algorithms, and machine learning models, applied to different geological challenges such as mineral exploration, landslide prediction, and seismic analysis. The findings demonstrate significant improvements in prediction accuracy and automation in geological research. The article emphasizes the potential of AI to transform geoscience by enhancing data processing, decision-making, and environmental management."
A Systematic Literature Review of Deep Learning Algorithms for Personality Trait Recognition,I. M. Artha Agastya; D. O. Dwi Handayani; T. Mantoro,10.1109/ICCED46541.2019.9161107,2019,"Personality trait recognition has an essential role in the job screening process. The psychologies perform the analysis based on the survey, the handwriting of the participant, or conduct the interview. The process takes a lot of time and money. Consequently, it inspires that researchers develop a tool to help the screening process faster. The current review papers lack an explanation of deep learning algorithms in personality trait recognition. So we perform this study to classify the latest deep learning algorithms in personality trait recognition. Using Preferred Reporting Items for Systematic reviews and MetaAnalyses Method (PRISMA), we collected 25 key papers which are from Scopus, IEEE, Science Direct, Emerald Insight, and ACM. In this paper, we focus on discussing the problems, deep learning methods, and open issue in personality trait recognition. Based on the finding, the primary problem in personality trait recognition is the complexity of the text and audiovisual data. It induces the low performance of personality trait recognition. In detail, the text-based trait recognition has lower accuracy than the visual and audio based methods. Therefore we have a bigger opportunity to improve the performance of text-based personality recognition than audiovisual based. Moreover, the real experiment in audiovisual trait recognition still in the early stages and it can be explored in more detail."
"Exploring the Intersection of Big Data and AI With CRM Through Descriptive, Network, and Contextual Methods",D. Ozay; M. Jahanbakth; S. Wang,10.1109/ACCESS.2025.3554549,2025,"As artificial intelligence (AI) continues to gain prominence, understanding its application in customer relationship management (CRM) has become increasingly critical. Advances in computing power, big data availability, AI methodologies, and the growing adoption of CRM systems have driven significant interest in AI-based CRM from both academia and industry. However, comprehensive reviews of this rapidly evolving domain remain limited. Using a systematic review approach, this study analyzes 840 documents from the Web of Science (WoS) database, following the PRISMA framework for study selection. Through descriptive, network, and contextual analyses, we identify key research clusters, including Enhancing Customer Experience with AI, predictive analytics in CRM, AI-CRM adoption and digital transformation, and Emerging AI Techniques in CRM. The findings highlight a significant shift toward AI-powered hyper-personalization, explainable AI, federated learning, and IoT-enhanced CRM. This study contributes by mapping the research landscape, uncovering emerging trends, and providing future research directions on the adoption and effectiveness of AI in CRM, AI ethics, the integration of IoT with AI in predictive CRM and AI-driven sentiment analysis, offering valuable insights for scholars and practitioners."
"Machine Learning in Heat Transfer: Taxonomy, Review and Evaluation",S. Ardabili; A. Mosavi; I. Felde,10.1109/SACI58269.2023.10158650,2023,"In the field of heat transfer, machine learning (ML) is used to analyze the large amounts of data that are collected through experiments, field observations, and simulations. It’s important to write a review paper that looks at how ML techniques are used in different heat transfer applications. We made a standard database with 900 publications for systematic reviews. So, the main goal of this review is to show a systematic state-of-the-art by analyzing how well ML works in heat transfer applications using PRISMA guidelines. Based on the results, most studies used the correlation coefficient as the most reliable and overall way to judge the ML tools in different heat transfer applications. Also, the Decision Tree (DT), the Random Forest (RF), and the Artificial Neural Network (ANN) have the most uses. On the other hand, the best performance is when people work together and use hybrid ML techniques. We’ll also publish and keep updating the latest research results so we can keep up with how quickly technology changes."
Industrial Engineering and Management Students Envision AI's Role in the Industry,P. Åhag; L. Hed; R. Leijon; O. Nordenfors; L. Olsson,10.1109/IEEM58616.2023.10406717,2023,"This study explores the perceptions of master's program students in Industrial Engineering and Management (IEM) at Umeå University, Sweden, concerning the current and future impact of artificial intelligence (AI) on their discipline. Employing a descriptive, cross-sectional survey design, we collected quantitative data from participants asked to assess AI's influence on decision-making, human-computer interactions, and information management, among other areas. While ordinal regression analysis revealed no significant correlation between the student's academic year and their survey responses, a Wilcoxon signed-rank test indicated a statistically significant belief that AI's impact on all surveyed areas would intensify within the next decade. Our findings suggest a need for engineering education to evolve to adequately equip future professionals for the expanding influence of AI in IEM. Furthermore, the results add to the ongoing discussion of AI's role in engineering education and the broader industrial engineering and management field."
Review on the Significance of Artificial Intelligence in Construction Engineering and Management,S. Kediya; M. P. Bhorkar; P. Deshpande; R. Jain; C. Puri; A. A. Gudadhe,10.1109/ICCSAI59793.2023.10420911,2023,"This extensive research investigates the tremendous significance of artificial intelligence (AI) in the field of construction engineering and management (CEM). It explores how artificial intelligence technologies, like IoT integration, machine learning, and blockchain applications, have the potential to completely revolutionize the way that building is done. The review emphasizes how crucial AI is to improve project management, efficiency, safety, and promoting environmentally friendly building methods. To fully realize AI's promise, the construction sector needs to raise its knowledge of and acceptance of the technology."
Antecedents of PMO Effectiveness in Technology Projects in the Age of AI,A. A. Baswan; A. Seetharaman,10.1109/ISMSIT63511.2024.10757286,2024,"As per Harvard Business Review (HBR 2023), each year USD 48 trillion are invested in projects, however, it is widely recognised that less than 35% of projects achieve, even partially, their strategic objectives (Standish Group's Annual CHAOS 2020 report). This responsibility falls, in a large part, to the effectiveness of Project Management Offices (PMOs) running these large projects. Additionally, there are rapid advances being made in the field of Artificial Intelligence (AI). At present, there is lack of wide-spread use cases in wide array of industries, however, Gartner's research (2023) indicates that in a decade, over 80% of standard project management tasks will be carried out using AI, machine learning and other digital advances. Project management offices (PMOs) are a feature of large banking and financial service organisations and offer many services, including portfolio prioritisation, project governance standards, tracking project risks and budget consumption and monitoring internal or external influences. A critical role of the PMO is to move the organisation closer to its strategic goals and report the achievement of enterprise objectives to shareholders and stakeholders. This research identifies and studies the factors that determine the effectiveness of PM Os - organisation maturity, performance metrics, PMO processes, its environment and adoption of AI tooling - so appropriate actions can be taken to improve the performance of PM Os operating in the age of Artificial Intelligence (AI)"
Artificial Intelligence in Project Management: Insights from Croatia,B. Vegar; T. Mijač,10.1109/MIPRO60963.2024.10569357,2024,"Artificial intelligence (AI) technology has become integral to everyday life, extending its influence into various business aspects. The concept of AI in Project Management (PM) has been discussed since the 1980s, portraying AI as a tool with the potential to expedite, optimize, and enhance project management, aiding decision-making processes. Project management is a pivotal process in nearly every organization, and integrating AI into this process can offer numerous advantages, such as heightened efficiency, precision, speed in decision-making, and improved risk assessment. Data from Eurostat in 2020 indicates that only 6% of businesses in Croatia used some form of AI in their operations, reflecting the limited adoption of this technology in the business sector. This paper explores the current state of AI application in project management in Croatia, exploring both perceived benefits and barriers. A survey conducted in Croatia with 115 respondents revealed that currently, only 29.1% of correspondents utilize some form of AI in project management. Unsurprisingly, AI is predominantly used in the IT industry (52%), followed by the educational sector (28%) and the healthcare sector (4%)."
Moving from the Internet of Things to the Industrial Metaverse: A Systematic Literature Review,A. Viola; J. B. Hauge; G. Bugár; D. Uckelmann; G. Romagnoli,10.1109/ICE/ITMC61926.2024.10794235,2024,"The concept of the industrial metaverse represents a revolutionary fusion of digital and physical realms within the industrial sector. This paper delves into the intersection of the internet of things (IoT) and the metaverse, aiming to define its emerging landscape through an analysis of current literature and identification of future research directions. Employing a combination of quantitative and qualitative methods, including bibliometric analysis and the Visualization of Similarities (VOS) technique, our study maps the field of the industrial metaverse. The PRISMA method, which we used for our contribution, provides a systematic approach to our research process, and provides a base for the sequent thematic analysis, where a thesaurus is used to account for keyword variations, enriching our understanding of the metaverse's potential in industrial applications. We argue that the industrial metaverse is not merely a technological advancement but opens the way in which industries operate, collaborate, and innovate. This paper aims to establish a foundational framework for future research and development in the industrial metaverse, offering a clear and precise overview about the current state of research on the metaverse and internet of things."
Prediction techniques for power plant failure and availability: A concise systematic review,B. M. Boshoma; P. O. Olukanmi,10.23919/SAIEE.2025.10755051,2025,"Electricity demand continues to exceed supply in many sub-Saharan countries like South Africa, and frequent plant failures further reduce energy availability. To address this issue, it is essential to proactively predict plant failures and inform decisions on when to plan for outages. Given a myriad of prediction techniques, this study systematically analyzed various literature to provide a collective view of prediction approaches, their use cases, and context. Following the PRISMA guideline, relevant literature was searched using the Scopus database, and retrieved from the corresponding publisher sites. The selected studies focused on predicting the unplanned capability loss factor or the availability of power plants within the electricity industry domain. A thematic analysis was performed to identify emerging patterns related to current knowledge. Results revealed that prediction studies focus more on predicting availability than failure in coal-fired plants. The prediction horizon is mainly short-term, mostly in renewable plant. Artificial neural network, Bayesian analysis, and fuzzy rules are the prevalent technique found in most studies. Scholars and researchers can benefit from this study as it provided a simplified summary of power plant prediction techniques in a consolidated view."
Challenges in Implementing Cross-Border Digital Identity Systems for Global Public Infrastructure: A Comprehensive Analysis,S. H. Supangkat; H. S. Firmansyah; I. Rizkia; R. Kinanda,10.1109/ACCESS.2025.3547373,2025,"The increasing demand for secure and interoperable cross-border digital identity systems has led to challenges in standardization, cybersecurity, and regulatory alignment. Despite advancements in frameworks like eIDAS (EU) and Aadhaar (India), the lack of globally unified standards remains a significant barrier to adoption. This study conducts a systematic literature review (SLR) following the PRISMA framework, analyzing 45 peer-reviewed studies (2018–2024) from Scopus, Web of Science, IEEE Xplore, and Springer. The analysis categorizes challenges into four key domains: technical (interoperability and cybersecurity issues), regulatory (data protection inconsistencies), organizational (financial and governance constraints), and societal (digital inclusion and trust concerns). Findings reveal that 45% of reported challenges stem from interoperability gaps, while 35% result from regulatory fragmentation. Economic disparities further limit digital identity adoption, particularly in developing regions. PPPs are being investigated as a strategic approach to advance blockchain technology and AI-driven identity verification, harmonize regulations, and promote global cooperation to address these issues. This research proposes a scalable and secure digital identity framework, offering actionable recommendations for policymakers, industry stakeholders, and researchers to build inclusive and interoperable global identity ecosystems."
Addressing Activation Outliers in LLMs: A Systematic Review of Post-Training Quantization Techniques,P. Czakó; G. Kertész; S. Szénási,10.1109/ACCESS.2025.3568702,2025,"Large Language Models (LLMs) have transformed natural language processing, yet their deployment remains challenging due to substantial computational, memory, and energy demands. Post-training quantization has emerged as a key strategy for enabling efficient inference, particularly in resource-constrained settings. This systematic review focuses on weight-activation quantization, with a unique emphasis on the emergent outlier phenomenon in LLM activations. This work evaluates recent techniques that mitigate activation outliers and improve quantization efficiency, distinguishing itself from prior reviews. Using the PRISMA methodology, we examine 52 recent studies to uncover key trends and evaluate the effectiveness of different approaches. By synthesizing insights from these works, this review presents a diverse set of techniques and their implications for activation quantization, laying the groundwork for future research and practical advancements in LLM deployment."
Deep learning for 5G and 6G,S. Ardabili; A. Mosavi; I. Felde,10.1109/SACI58269.2023.10158628,2023,"Deep learning (DL) is a promising technology for enhancing the development of fifth generation (5G) and sixth generation (6G) mobile networks, as it can improve their capabilities, security, and performance. However, there are still significant challenges to be addressed in the implementation of DL techniques in these networks. To address these challenges, we conducted a systematic review of the literature on DL techniques in 5G and 6G applications following the PRISMA guidelines. The review was conducted in three stages: data collection, analysis, and reporting of primary findings. After evaluating and reviewing the databases, we found that hybrid DL and ensemble techniques show promise in optimizing 5G and 6G networks, given proper implementation. Finally, we discussed the open issues and challenges in this field. This review provides important insights into the potential of DL techniques in improving 5G and 6G networks, and it highlights the need for further research to overcome the remaining challenges. The results of this primary communication will be further developed and extended into a journal article."
Review: Adaptive Mobile Learning Algorithms Current State-of-the-Art,A. W. Altaher; H. M. Al-Jawahry,10.1109/IICETA57613.2023.10351367,2023,"Adaptive mobile learning algorithms offer a promising approach to addressing the challenges of mobile learning, such as learner engagement and personalized learning. Recent advances and innovations in these algorithms have shown promising results in improving learning outcomes and user satisfaction. However, there are still several limitations and challenges to overcome, including the need for standard evaluation metrics, large-scale data and computational resources, and addressing ethical and privacy concerns. Future research should focus on developing effective and efficient adaptive mobile learning algorithms that can optimize the learning experience for all learners. To review and compare the current state-of-the-art adaptive mobile learning algorithms, we conducted a systematic literature review following the PRISMA guidelines. We searched multiple academic databases, including IEEE Xplore, ACM Digital Library, and ScienceDirect, using specific keywords related to adaptive mobile learning algorithms. After applying inclusion and exclusion criteria, we selected a total of 10 studies published between 2017 and 2021 for our analysis as in table 2, and studies that were published before the year 2010 as in table 1. We extracted data from each study, including the research design, sample size, algorithm used, evaluation metrics, and results. We then synthesized the data and identified common themes and patterns across the studies."
Enhancing Emotional Well-Being With IoT Data Solutions for Depression: A Systematic Review,S. Zamani; R. Sinha; M. Nguyen; S. Madanian,10.1109/JBHI.2024.3501254,2025,"Effectively caring for adults with depression is challenging. While technology offers potential improvements in emotional well-being through better monitoring, standardised methods to gather and analyse relevant data are highly fragmented. This Systematic Literature Review (SLR) explores using Internet of Things (IoT) based data collection and analysis to enhance emotional well-being and manage depression effectively. Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework, we report in-depth findings from 42 studies, which were selected from an initial set of 559 published works. We find that current literature extensively covers important topics like IoT for detecting, analysing, and monitoring emotions, therapeutic interventions for emotional well-being, and predicting, detecting, and managing depression. IoT-based data collection and analysis solutions predominantly employ sensors and AI, respectively. The literature review identifies a gap in prioritising active systems that engage users, highlighting the need to address key aspects such as privacy and security."
"Artificial Intelligence Techniques for Securing Fog Computing Environments: Trends, Challenges, and Future Directions",D. Alsadie,10.1109/ACCESS.2024.3463791,2024,"Fog computing, an extension of cloud computing, enhances capabilities by processing data closer to the source, thereby addressing latency and bandwidth issues inherent in traditional cloud models. However, the integration of Artificial Intelligence (AI) into fog computing introduces challenges, particularly in resource management, security, and privacy. This paper systematically reviews AI applications within fog computing environments, adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology to ensure rigorous analysis. The studies were selected based on predefined inclusion criteria, including research published between 2010 and 2024 in peer-reviewed journals and conference papers, with searches conducted in databases like IEEE Xplore, ACM Digital Library, SpringerLink, and Scopus. The review identifies critical issues such as resource constraints, transparency in AI-driven security systems, and the need for adaptable AI models to address evolving security threats. In response, innovative solutions such as lightweight AI models (e.g., Pruned Neural Networks, Quantized Models, Knowledge Distillation), Explainable AI (XAI) (e.g., Model-Agnostic Methods, Feature Importance Analysis, Rule-Based Approaches), and federated learning are proposed. Additionally, a novel taxonomy is introduced, categorizing AI techniques into resource management, security enhancement, and privacy-preserving methods, offering a structured framework for researchers and practitioners. The paper concludes that effective AI integration in fog computing is essential for developing secure, efficient, and adaptable distributed systems, with significant implications for both academia and industry."
Retinal Health Screening Using Artificial Intelligence With Digital Fundus Images: A Review of the Last Decade (2012–2023),S. Islam; R. C. Deo; P. Datta Barua; J. Soar; P. Yu; U. Rajendra Acharya,10.1109/ACCESS.2024.3477420,2024,"Prolonged diabetic retinopathy (DR), glaucoma, and age-related macular degeneration (AMD) may lead to vision loss. Hence, early detection and treatment are crucial to prevent irreversible vision loss. Fundus retinal images have been widely used to help detect these diseases. Manual screening is susceptible to human errors, tedious, and expensive. Hence, artificial intelligence (AI) techniques have been widely employed to overcome these constraints. This paper reviewed the work published on automated retinal health detection models using various machine learning (ML) and deep learning (DL) techniques. We reviewed 142 papers and 262 studies (124 on glaucoma, 60 on AMD, and 78 on DR) from January 2012 to June 2024 using Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. We found that Convolutional Neural Networks (CNN) and Support Vector Machines (SVM) models were widely used in DL and ML techniques, respectively. To the best of our knowledge, this is the first review developed for detecting AMD, DR, and glaucoma using AI techniques over the last decade. We have discussed the limitations of the present methods and also suggested future directions for accurately detecting eye diseases."
"Artificial Intelligence for Detecting COVID-19 With the Aid of Human Cough, Breathing and Speech Signals: Scoping Review",M. Husain; A. Simpkin; C. Gibbons; T. Talkar; D. Low; P. Bonato; S. S. Ghosh; T. Quatieri; D. T. O'Keeffe,10.1109/OJEMB.2022.3143688,2022,"Goal: Official tests for COVID-19 are time consuming, costly, can produce high false negatives, use up vital chemicals and may violate social distancing laws. Therefore, a fast and reliable additional solution using recordings of cough, breathing and speech data for preliminary screening may help alleviate these issues. Objective: This scoping review explores how Artificial Intelligence (AI) technology aims to detect COVID-19 disease by using cough, breathing and speech recordings, as reported in the literature. Here, we describe and summarize attributes of the identified AI techniques and datasets used for their implementation. Methods: A scoping review was conducted following the guidelines of PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews). Electronic databases (Google Scholar, Science Direct, and IEEE Xplore) were searched between 1st April 2020 and 15th August 2021. Terms were selected based on the target intervention (i.e., AI), the target disease (i.e., COVID-19) and acoustic correlates of the disease (i.e., speech, breathing and cough). A narrative approach was used to summarize the extracted data. Results: 24 studies and 8 Apps out of the 86 retrieved studies met the inclusion criteria. Half of the publications and Apps were from the USA. The most prominent AI architecture used was a convolutional neural network, followed by a recurrent neural network. AI models were mainly trained, tested and run-on websites and personal computers, rather than on phone apps. More than half of the included studies reported area-under-the-curve performance of greater than 0.90 on symptomatic and negative datasets while one study achieved 100% sensitivity in predicting asymptomatic COVID-19 from cough-, breathing- or speech-based acoustic features. Conclusions: The included studies show that AI has the potential to help detect COVID-19 using cough, breathing and speech samples. The proposed methods (with some time and appropriate clinical testing) could prove to be an effective method in detecting various diseases related to respiratory and neurophysiological changes in the human body."
Non-Functional Requirement Extracting Methods for AI-based Systems: A Survey,R. Damirchi; A. Amini,10.1109/ICCKE60553.2023.10326269,2023,"Artificial intelligence (AI) systems are growing rapidly in a variety of fields and have become a part of everyday human life. Traditional software development methods, especially in the requirements engineering domain, are not very efficient in the development of AI-based Systems. Given that the elicitation and analysis of software project requirements is the key to producing a high-quality and efficient product, there must be new approaches to identifying and extracting requirements in the field of AI-based systems. This is done in the research area of Requirements Engineering for Artificial Intelligence (RE4AI). Non-functional requirements are defined in accordance with the system's objectives and operational requirements. They specify the behaviors, limitations, and overall quality of the system; for this reason, they are very important for the success of the system, and lack of attention or neglect to accurately identify it will increase the probability of project failure. In this Paper, we aim to provide a comprehensive overview of the approaches and techniques for extracting non-functional requirements for AI-based systems by reviewing recent papers in this field. The review of these studies shows that there are currently no accurate methods for identifying and extracting non-functional requirements of AI-based Systems, but by relying on traditional methods and redefining their concepts, most of these methods can also be used in this domain. Additionally, new requirements for the field of AI have been introduced that are of high importance."
Machine Learning Algorithms for Epilepsy Detection Based on Published EEG Databases: A Systematic Review,A. Miltiadous; K. D. Tzimourta; N. Giannakeas; M. G. Tsipouras; E. Glavas; K. Kalafatakis; A. T. Tzallas,10.1109/ACCESS.2022.3232563,2023,"Epilepsy is the only neurological condition for which electroencephalography (EEG) is the primary diagnostic and important prognostic clinical tool. However, the manual inspection of EEG signals is a time-consuming procedure for neurologists. Thus, intense research has been made on creating machine learning methodologies for automated epilepsy detection. Also, many research or medical facilities have published databases of epileptic EEG signals to accommodate this research effort. The vast number of studies concerning epilepsy detection with EEG makes this systematic review necessary. It presents a detailed evaluation of the signal processing and classification methodologies employed on the different databases and provides valuable insights for future work. 190 studies were included in this systematic review according to the PRISMA guidelines, acquired from a systematic literature search in PubMed, Scopus, ScienceDirect and IEEE Xplore on 1st May 2021. Studies were examined based on the Signal Transformation technique, classification methodology and database for evaluation. Along with other findings, the increasing tendency to employ Convolutional Neural Networks that use a combination of Time-Frequency decomposition methodology images is noticed."
Explainable Artificial Intelligence Methods in Combating Pandemics: A Systematic Review,F. Giuste; W. Shi; Y. Zhu; T. Naren; M. Isgut; Y. Sha; L. Tong; M. Gupte; M. D. Wang,10.1109/RBME.2022.3185953,2023,"Despite the myriad peer-reviewed papers demonstrating novel Artificial Intelligence (AI)-based solutions to COVID-19 challenges during the pandemic, few have made a significant clinical impact, especially in diagnosis and disease precision staging. One major cause for such low impact is the lack of model transparency, significantly limiting the AI adoption in real clinical practice. To solve this problem, AI models need to be explained to users. Thus, we have conducted a comprehensive study of Explainable Artificial Intelligence (XAI) using PRISMA technology. Our findings suggest that XAI can improve model performance, instill trust in the users, and assist users in decision-making. In this systematic review, we introduce common XAI techniques and their utility with specific examples of their application. We discuss the evaluation of XAI results because it is an important step for maximizing the value of AI-based clinical decision support systems. Additionally, we present the traditional, modern, and advanced XAI models to demonstrate the evolution of novel techniques. Finally, we provide a best practice guideline that developers can refer to during the model experimentation. We also offer potential solutions with specific examples for common challenges in AI model experimentation. This comprehensive review, hopefully, can promote AI adoption in biomedicine and healthcare."
Modeling Remaining Service Life and Structural Health Monitoring of Roads with Machine Learning and Deep Learning,M. Mudabir; A. Mosavi; F. Imre; N. Moniz; K. Iskakov; A. Mohamadreza,10.1109/SAMI63904.2025.10883327,2025,"The integration of machine learning (ML) and deep learning (DL) in structural health monitoring (SHM) and remaining service life (RSL) has revolutionized the ability to assess and maintain critical infrastructure. This review looks at the current state of SHM methods that use ML and DL. This is done by providing a detailed taxonomy that groups these methods into groups based on algorithmic strategies, data sources, and specific SHM and RSL applications. Using Scopus as the primary source for literature, we conducted a systematic review following PRISMA guidelines to ensure thorough screening and quality assessment of most relevant studies. The review covers key areas that include supervised and unsupervised learning techniques, neural networks, and their applications to structural damage detection, failure prediction, improving precision in monitoring. Based on the trend analysis and highlighting of some of the challenges in this context, this review has identified a few future opportunities for applying advanced learning techniques to SHM to improve infrastructure safety and management."
A Systematic Literature Review on Mining LTL Specifications,S. Germiniani; D. Nicoletti; G. Pravadelli,10.1109/ACCESS.2025.3551607,2025,"Linear Temporal Logic (LTL) specifications play a crucial role in the verification process of cyber-physical systems, increasing the guarantees of their correctness. These specifications are vital for ensuring that both hardware and software components behave as expected, especially in complex real-world scenarios. In the last decades, researchers have developed several methodologies and tools to automatically generate LTL specifications, creating an urgent need to organize and synthesize existing literature to ease entry into this field and guide future research efforts. Therefore, starting from a pool of over 3000 papers extracted from the Scopus database in the temporal range 2000–2024, this paper employs the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology to produce a systematic review of mining LTL specifications of hardware and software systems. In particular, we provide a taxonomy of the methods and describe with significant detail all the relevant techniques present at the state of the art. Finally, we discuss the challenges of mining LTL specifications and explore potential directions and opportunities for future research."
A Systematic Review of Experimental Protocols: Towards a Uniform Framework in Virtual Reality Affective Research,A. Bayro; H. Jeong,10.1109/TAFFC.2025.3554496,2025,"The integration of affective computing with virtual reality (VR) often uses machine learning to analyze users' emotional responses through physiological and behavioral signals, enabling personalized interactions within VR environments. However, current research in this field is characterized by inconsistent experimental protocols, which hinders comprehensive conclusions and cross-study comparisons. To address this gap, a systematic review was conducted following the PRISMA guidelines, identifying 24 studies that used physiological measures and machine learning to predict emotions in VR settings. The review covers five key areas: experimental protocols, VR environments, implicit measurements, emotion models, and machine learning approaches. In addition, it provides guidelines for standardizing data collection, biosignal processing, and emotion modeling. These proposed guidelines aim to establish consistent reporting practices and experimental protocols, thus improving the comparability and reproducibility of future VR affective computing research."
Unveiling the Efficacy of Unani Medicine in Female Disorders Through Machine Learning: Current Challenges and Opportunities,A. Sultana; F. Akhtar; M. B. Bin Heyat; P. Singh; S. Parveen; S. Kumar; E. Sayeed; S. Akhtar; B. Singh; K. Rahmani; J. Li,10.1109/ICCWAMTIP60502.2023.10385245,2023,"In the past decade, the healthcare sector has seen a noteworthy increase in the application of artificial intelligence (AI) technologies across various therapeutic domains. AI-powered technologies have been increasingly used in the context of female diseases within Unani medicine. This study unveils the efficacy of Unani medicine in female diseases through machine learning. This review abides by the preferred reporting items for systematic reviews and meta-analysis extensions for scoping reviews. Studies published after January 1, 2000, were included in 2000 to 2023. A total of 402 full-length articles were searched from the PubMed and Science direct. Two studies from published articles and two from the proposed dataset exclusively focused on the efficacy of Unani medicine, along with one cross-sectional study. This review demonstrates that this research has been conducted using various machine-learning techniques to detect the efficacy of Unani treatment in female disoder. Machine learning was also applied in a cross-sectional study on abnormal vaginal discharge and Mizaj. Machine learning models were used. While research on machine learning models in Unani herbal medicine is in its preliminary stages, the findings from the selected research suggest that ML methods can enhance the services provided by Unani practitioners to their patients. Nevertheless, future advancements should involve the application of diverse ML models across a broad spectrum of Unani treatments."
Software Cost and Effort Estimation: Current Approaches and Future Trends,C. H. Rashid; I. Shafi; J. Ahmad; E. B. Thompson; M. M. Vergara; I. de la Torre Diez; I. Ashraf,10.1109/ACCESS.2023.3312716,2023,"Software cost and effort estimation is one of the most significant tasks in the area of software engineering. Research conducted in this field has been evolving with new techniques that necessitate periodic comparative analyses. Software project success largely depends on accurate software cost estimation as it gives an idea of the challenges and risks involved in the development. The great diversity of ML and Non-ML techniques has generated a comparison and progressed into the integration of these techniques. Based on varying advantages it has become imperative to work out preferred estimation techniques to improve the project development process. This study aims to present a systematic literature review (SLR) to investigate the trends of the articles published in the recent one and a half decades and to propose a way forward. This systematic literature review has proposed a three-stage approach to plan (Tollgate approach), conduct (Likert type scale), and report the results from five renowned digital libraries. For the selected 52 articles, artificial neural network model (ANN) and constructive cost model (COCOMO) based approaches have been the favored techniques. The mean magnitude of relative error (MMRE) has been the preferred accuracy metric, software engineering, and project management are the most relevant fields, and the promise repository has been identified as the widely accessed database. This review is likely to be of value for the development, cost, and effort estimations."
Popular LLM-Large Language Models in Enterprise Applications,R. Pasupuleti; R. Vadapalli; C. Mader; N. Timothy,10.1109/FLLM63129.2024.10852443,2024,"For the public, understanding Large Language Models (LLMs) can be likened to recognizing how a well-trained assistant works—one that has read an extensive library of information on virtually every topic imaginable. Imagine an assistant that not only reads and remembers all this information but also learns the nuances of how words and ideas are connected across different contexts. This assistant can then use this knowledge to write articles, answer questions, compose emails, or even generate creative stories, all in a manner that feels surprisingly human. This capability comes from what's known as ""transformer architecture,"" a type of design that helps the model pay attention to different parts of the text as it reads, making it adept at understanding and generating language. LLMs are a breakthrough in technology because they can understand and produce language with a level of subtlety and complexity that was previously unachievable, making them valuable tools across various industries. The paper aims to provide a comprehensive analysis of the transformative impact of LLMs across various enterprise sectors. It intends to contribute to the understanding of how LLMs can enhance efficiency, innovation, and decision-making processes in industries such as healthcare, finance, education, and in the software engineering sector. It also provides a comprehensive overview of current popular LLMs in Enterprise applications, in various domains, and discusses the Ethical, Technical, and Regulatory challenges, future trends, and developments in this dynamic field."
A Systematic Review of Feature Selection Techniques in Software Quality Prediction,H. Alsolai; M. Roper,10.1109/ICECTA48151.2019.8959566,2019,"Background: Feature selection techniques are important factors for improving machine learning models because they increase prediction accuracy and decrease the time to create a model. Recently, feature selection techniques have been employed on software quality prediction problems with different results and no clear indication of which techniques are frequently used.Objective: This study aims to conduct a systematic review of the application of feature selection techniques in software quality prediction and answers eight research questions.Method: The review evaluates 15 papers in 9 journals and 6 conference proceedings from 2007 to 2017 using the standard systematic literature review method.Results: The results obtained from this study reveal that the filter feature selection method was the most commonly used in the studies (60%) and RELIEF was the most employed among this method, and a limited number of studies employed an ensemble method. Several studies used public datasets available in the PROMISE software project repository (60%). Most studies focused on software defect prediction (classification problem) using area under curve (AUC) as a primary evaluation measure, whereas only two studies focused on software maintainability prediction (regression problem) using mean magnitude of relative error (MMRE) as a primary evaluation measure. All selected studies performed k-fold cross-validation to evaluate model accuracy. Individual prediction models were mostly employed and ensemble models appeared only in three studies. Naive Bayes was the most investigated among individual models, whereas Random forest was the most investigated among ensemble models.Conclusion: Feature selection techniques used by selected primary studies have a positive impact on the performance of the prediction models. Further, both ensemble feature selection method and ensemble models have the ability for increasing prediction accuracy over single methods or individual models and have reported improvement in the prediction accuracy; however, the application of these techniques in software quality prediction is still limited."
A Machine Learning-Oriented Survey on Tiny Machine Learning,L. Capogrosso; F. Cunico; D. S. Cheng; F. Fummi; M. Cristani,10.1109/ACCESS.2024.3365349,2024,"The emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly, we will examine the three different workflows for implementing a TinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly, we propose a taxonomy that covers the learning panorama under the TinyML lens, examining in detail the different families of model optimization and design, as well as the state-of-the-art learning techniques. Thirdly, this survey will present the distinct features of hardware devices and software tools that represent the current state-of-the-art for TinyML intelligent edge applications. Finally, we discuss the challenges and future directions."
Aritificial Inteligence Challenges in COPD management: a review,L. S. Bećirović; A. Deumić; L. G. Pokvić; A. Badnjevic,10.1109/BIBE52308.2021.9635374,2021,"Machine learning algorithms have been drawing attention in lung disease research. However, due to their algorithmic learning complexity and the variability of their architecture, there is an ongoing need to analyze their performance. This study reviews the input parameters and the performance of machine learning applied to diagnosis of chronic obstructive pulmonary disease (COPD). One research focus of this study was on clearly identifying problems and issues related to the implementation of machine learning in clinical studies. Following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) protocol, 179, 1032, and 36,500 titles were identified from the PubMed, Scopus, and Google Scholar databases respectively. Studies that used machine learning to detect COPD and provided performance measures were included in our analysis. In the final analysis, 24 studies were included. The analysis of machine learning methods to detect COPD reveals the limited usage of the methods and the lack of standards that hinder the implementation of machine learning in clinical applications. The performance of machine learning for diagnosis of COPD was considered satisfactory for several studies; however, given the limitations indicated in our study, further studies are warranted to extend the potential use of machine learning to clinical settings."
A Meta-Analytic Quantitative Review of Methods and Techniques for Sign Language Recognition Approaches,T. G. Moape; A. Muzambi; B. Chimbo,10.23919/IST-Africa63983.2024.10569745,2024,"While numerous machine learning methods and techniques have been employed for sign language recognition systems, there has not yet been a study that consolidates the methods to provide insight into trends and advancements in this domain. This gap remains as current reviews and survey papers do not adopt the format of a meta-analysis. Hence, the primary objective of this study was to fill this knowledge gap by conducting a meta-analysis and review of sign language recognition, deep learning, machine learning, and hybrid-based methods. The extraction of relevant articles was carried out following the technique of preferred reporting items for systematic reviews (PRISMA). Statistical analysis was performed using Stata version 15. The results of the meta-analysis showed an overall pooled estimate of 40%, with a range of 1% to 87% within a 95% confidence interval. The high pooled effect size of $\mathrm{I}^{2}=95.78\%$ suggests a significant statistical heterogeneity across all the studies."
Application of Natural Language Processing Techniques in Agile Software Project Management: A Survey,R. Younisse; M. Azzeh,10.1109/ICICS60529.2023.10330468,2023,"Software effort estimation has long been an important task for better software management. Most of the constructed effort estimation models were based on data collected from software projects that had been developed using traditional software development processes. The structure of this data is usually in the form of tabulated data. Recently, the Agile management framework invaded software production lines as an effective and productive management method. It helps software development teams to complete their tasks in a highly effective way. One of the main components of this success is predicting accurate story points from textual user stories. User stories and story points have become an essential component over which the project planning process is built. Machine learning, artificial intelligence, and deep learning are used to enhance the process of using user story context to put a close estimate of the required resources to finish the project. Using these models has become popular and remarkable in the field of effort estimation. The textual nature of user stories directed the research into the natural language processing path. Natural language processing models can be used to understand textual user story context in order to produce effort estimates. This study reviews the usage of natural language processing methodology in the context of Agile project effort estimation based on the contextual content of user stories."
Practices for Managing Machine Learning Products: A Multivocal Literature Review,I. Alves; L. A. F. Leite; P. Meirelles; F. Kon; C. S. R. Aguiar,10.1109/TEM.2023.3287759,2024,"Machine learning (ML) has grown in popularity in the software industry due to its ability to solve complex problems. Developing ML systems involves more uncertainty and risk because it requires identifying a business opportunity and managing source code, data, and trained models. Our research aims to identify the existing practices used in the industry for building ML applications and comprehending the organizational complexity of adopting ML systems. We conducted a multivocal literature review and then created a taxonomy of the practices applied to the ML system life cycle discussed among practitioners and researchers. The core of the study emerged from 41 selected posts from the grey literature and 37 selected scientific papers. Applying Initial Coding and Focused Coding techniques into these data, we mapped 91 practices into six core categories related to designing, developing, testing, and deploying ML systems. The results, including a taxonomy of practices, provide organizations with valuable insights to identify gaps in their current ML processes and practices and a roadmap for improving, optimizing, and managing ML systems."
Efficacy of Wearable Devices in Monitoring and Treating Parkinson’s Disease Symptoms: A Systematic Review and Meta Analyses,M. A. Terry; E. Alvarez Vazquez,10.1109/ACCESS.2025.3569056,2025,"Parkinson’s disease (PD) is a progressive neurodegenerative disorder characterized by motor and non-motor symptoms that significantly impact quality of life. Effective symptom management requires continuous monitoring and timely interventions, which traditional clinical assessments often fail to provide. Wearable devices have emerged as promising solutions, offering real-time, objective data collection and personalized interventions. This systematic review and meta-analysis evaluate the efficacy of wearable devices in monitoring and treating PD symptoms. A comprehensive search of PubMed, ScienceDirect, IEEE, Scopus, and Web of Science was conducted following PRISMA guidelines. Studies assessing wearable devices for PD symptom tracking or therapeutic intervention were included. The meta-analysis quantified the accuracy of symptom detection and the effectiveness of wearable-assisted interventions in improving clinical outcomes. Results indicate that wearable devices demonstrate high sensitivity and specificity in tracking motor fluctuations, gait abnormalities, and tremors. Wearable-based interventions, such as cueing and feedback mechanisms, have shown promise in enhancing motor control, reducing bradykinesia, and improving medication adherence. However, challenges such as device adherence, user-friendliness, and data integration into clinical workflows remain barriers to widespread adoption. Wearable technology represents a valuable tool for enhancing PD management through continuous monitoring and personalized interventions. While evidence supports wearables’ potential to improve outcomes, further research is needed to validate long-term efficacy, optimize usability, and assess cost-effectiveness to ensure broader clinical adoption."
Factors Influencing Perceived Switching Cost and User Switching Behavior Towards AI-Based Solutions and Technologies - A Systematic Review,E. Đerić; D. Frank; Z. Tomić,10.1109/SISY60376.2023.10417867,2023,"The swift progress of artificial intelligence (AI), deep learning, and natural language processing brought massive changes across diverse domains. A substantial body of research underscored the advantages of AI technologies for consumers and identified motivational factors for their adoption. Current systematic literature reviews usually examined the traditional Technology Acceptance Model (TAM), Unified Theory of Acceptance and Use of Technology (UTAUT), and AI Device Use Acceptance (AIDUA), focused on a single solution or technology. On the other hand, the importance of perceived switching cost and measurement of switching behavior is an essential part of well-established and widely used theories (Push-Pull-Mooring (PPM) framework, Status Quo Bias theory, Lazy User Theory, etc.). By following the PRISMA 2020 guidelines, this literature review highlights studies that observe switching costs and switching behavior towards AI-based solutions. $\mathrm{N} = 2,739$ articles were initially identified in prominent databases (IEEE Xplore, Scopus, Web of Science, and EBSCO Host). After screening, $n = 7$ articles were deemed suitable for inclusion, and most of them relied on the Dual-factor framework and PPM framework. In many cases, these frameworks were integrated with theories and models such as the Status Quo Bias theory, UTAUT, TAM, and TAM3 to provide an understanding of underlying factors affecting user behavior and decision-making. Positive factors influencing switching behavior towards AI adoption include recognizing human limitations, while simultaneously perceiving advantages of AI technology, such as ease of use, efficiency, and personalized experiences. Negative factors, which hinder AI acceptance, encompass privacy risks, inertia, uncertainty, regret avoidance, and cognitive biases."
Success Factors of Enterprise Resource Planning: A State-of-the-Art Mini Review,A. BaniMustafa; O. Bulkrock,10.1109/ICAMAC62387.2024.10829272,2024,"Enterprise Resource Planning Systems (ERP) are vital for today’s businesses. However, the successful implementation of ERP systems faces several challenges, which can determine its success or failure. This paper provides a brief state-of-theart review of the success factors of ERP implementation using various applications that span different contexts. The paper discusses and compares the aims defined and the techniques applied in these studies, as well as the data used and the results obtained from each of these studies. This review paper reviews published work to tackle these challenges, covering the literature published in 2020-2023. A special effort was also made to focus on high-quality papers published in top journals with high-impact factors and indexed in Scopus and Web of Science. The review process is repeatable, as special keywords were used to search the literature using publishers’ databases and three specialized tools. The paper concludes by discussing the future of the successful implementation of ERP systems. The paper argues that ERP systems will continue to have a major impact on the success of today’s businesses. Therefore, they must carefully consider various factors that influence their successful implementation."
A Systemic Review of Healthcare Challenges Due to Ethical Implications: A PRISMA Analysis,A. Ara; A. Thomas,10.1109/ICAISC64594.2025.10959586,2025,"In this rapidly evolving field of healthcare, Artificial Intelligence(AI) presents significant ethical challenges. There is an urgent need to address these ethical concerns as they are directly affecting patient care, As per Healthcare Tech Outlook, 80% of healthcare data cannot be used as it is not structured, resulting in biased outcomes due to its inability to be analyzed. The integration of artificial intelligence (AI) into healthcare has the potential to revolutionize medical services, enhancing patient outcomes, operational efficiency, and overall healthcare management. However, this technological advancement also introduces significant ethical challenges that must be addressed to ensure responsible and equitable use. This research explores the systemic impacts of AI ethics in healthcare by examining five key themes: ethical challenges, influential contributors and collaboration networks, emerging themes, cooccurrence of ethical concepts, and influential publications. The study identifies critical ethical issues such as bias, privacy, transparency, and accountability, highlighting the need for robust ethical guidelines. It also maps the contributions of leading authors and institutions, revealing collaboration networks that drive research in this field. Emerging themes such as transparency, data privacy, fairness, and the environmental impact of AI are discussed, reflecting the evolving landscape of AI ethics. The cooccurrence of ethical concepts is analyzed to understand their interconnected nature, emphasizing the complexity of ethical AI implementation. Finally, influential publications are reviewed to assess their impact on the development of ethical guidelines and best practices. By addressing these research themes, the study aims to contribute to the development of comprehensive ethical frameworks for AI in healthcare, ensuring that the benefits of this technology are realized while minimizing potential risks and ethical concerns."
Quantum Computing in Project Management: Transforming Risk Assessment and Decision-Making,R. E. Indrajit,10.1109/ICIC64337.2024.10957024,2024,"Quantum computing has emerged as a transformative technology with the potential to significantly impact the field of project management, particularly in the areas of risk assessment and decision-making. Unlike classical computing, which processes information in binary formats, quantum computing leverages the principles of superposition and entanglement that allow simultaneous processing of multiple states. This unique capability enables more accurate predictive analytics, providing project managers with the tools to evaluate complex scenarios with unprecedented precision. This paper presents a comprehensive literature review of quantum computing ap- plications within project management, examining how these advancements can optimize resource allocation, enhance decision-making frameworks, and lead to more effective project outcomes. The findings suggest that integrating the concept of quantum computing into project management methodologies could fundamentally reshape traditional approaches, offering new pathways for managing uncertainty and improving project success rates. Future research directions and practical implications for project managers are also discussed."
Machine Learning based Detection of Post Traumatic Stress Disorder of Mental Health,K. Nagarajaiah; M. H. Krishnappa; K. R. Asha,10.1109/ICESC57686.2023.10193036,2023,"Researchers have been driven to investigate the possible role that machine learning may play in the treatment of mental health illnesses as a result of the increasing prevalence of mental health issues and the need for more effective medical care. This article provides a comprehensive examination of the current state of the art regarding the use of a wide variety of machine learning algorithms to the process of diagnosing mental health problems. In addition to this, this study will discuss about the obstacles, restrictions, and opportunities that lie ahead in the field of machine learning when it comes to mental health. This study constructs a library of research articles and studies on machine learning approaches employed for the prediction of mental health concerns by searching credible sources. Moreover, this systematic review is conducted by using the PRISMA methodology. After the process of elimination and selection, a total of 30 unique research articles are reviewed. Schizophrenia, bipolar illness, anxiety, depression, PTSD, and other mental health issues are only some of the categories into which we’ve placed the collected research articles. This section will first analyse the findings and then reflect on the obstacles and restrictions that have been encountered by the community doing research on the application of machine learning to mental health. In addition, this study outlines the potential directions that future study and development efforts in the field of applying machine learning toward mental health may take in order to take advantage of the opportunities presented."
The Mediating Effect of Knowledge Management on the Relationship between Risk Management and Project Performance,A. Z. R. Chin; T. H. Yi; N. Zakuan; Z. Sulaiman; M. Z. M. Saman; T. A. Chin,10.1109/ICIM49319.2020.244719,2020,"Risk management is the fundamental part of project management where it aims to discover the potential risks affiliated with a project and take suitable actions against those risks. Risk management is a comprehensive and systematic way to manage risks, which is implemented continuously throughout the construction project in order for a construction company to achieve their project objective. The purpose of this paper is to study the mediating effect of knowledge management (KM) on the relationship between risk management (RM) and project performance (PP). This paper is a quantitative research whereby questionnaire survey is used. The population of this research is the construction industry (CI) in Malaysia. This paper had proposed a framework of the relationship between RM, KM and PP. The results of PLSSEM showed that (1) there is a positive and significant relationship between RM and PP; (2) there is a positive and significant relationship between risk management and knowledge management; (3) there is a positive and significant relationship between KM and PP; (4) KM has partially mediated the relationship between RM and PP. Hence, this paper concludes that implementation of RM will drive the implementation of KM in construction companies, which eventually improve the PP."
Toward an AI Knowledge Assistant for Context-Aware Learning Experiences in Software Capstone Project Development,A. Neyem; L. A. González; M. Mendoza; J. P. S. Alcocer; L. Centellas; C. Paredes,10.1109/TLT.2024.3396735,2024,"Software assistants have significantly impacted software development for both practitioners and students, particularly in capstone projects. The effectiveness of these tools varies based on their knowledge sources; assistants with localized domain-specific knowledge may have limitations, while tools, such as ChatGPT, using broad datasets, might offer recommendations that do not always match the specific objectives of a capstone course. Addressing a gap in current educational technology, this article introduces an AI Knowledge Assistant specifically designed to overcome the limitations of the existing tools by enhancing the quality and relevance of large language models (LLMs). It achieves this through the innovative integration of contextual knowledge from a local “lessons learned” database tailored to the capstone course. We conducted a study with 150 students using the assistant during their capstone course. Integrated into the Kanban project tracking system, the assistant offered recommendations using different strategies: direct searches in the lessons learned database, direct queries to a generative pretrained transformers (GPT) model, query enrichment with lessons learned before submission to GPT and large language model meta AI (LLaMa) models, and query enhancement with Stack Overflow data before GPT processing. Survey results underscored a strong preference among students for direct LLM queries and those enriched with local repository insights, highlighting the assistant's practical value. Furthermore, our linguistic analysis conclusively demonstrated that texts generated by the LLM closely mirrored the linguistic standards and topical relevance of university course requirements. This alignment not only fosters a deeper understanding of course content but also significantly enhances the material's applicability to real-world scenarios."
Blockchain and Machine Learning in EHR Security: A Systematic Review,U. Zukaib; X. Cui; M. Hassan; S. Harris; H. J. Hadi; C. Zheng,10.1109/ACCESS.2023.3333229,2023,"Background: The rapid development of modern technologies renders a convenient and efficient solution to implement Electronic Health Records (EHRs) systems. The rapid growth of healthcare data has a distinctive attribute of digital transformations. The big datasets of healthcare, their complexity and their dynamic nature have posed severe challenges associated with the analysis, pre-processing, privacy, security, storage, usability and data exchange. Material and Methods: We have performed the Systematic Literature Review (SLR) and followed the Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) methodology. SLR refers to the methodology that discovers, analyses and accesses recent research literature related to the subject field. The research papers were searched from academic repositories like IEEE, WOS, Scopus and PubMed for the previous five years on March 2023. Results: The designed search string provides 199 research articles in total. We filter the research articles based on inclusion-exclusion strategies and quality assessment metrics. Six main criteria for research inclusion-exclusion for SLR are formulated. These works of literature insight into 1) the issues associated with interoperability and security of EHRs by using the Blockchain (BC) technology, 2) different frameworks and tools to improve privacy and security in the healthcare domain, 3) the open issues of using BC technology in the electronic healthcare domain, 4) the standardized ways to store EHRs, 5) various ways to handle the big data using the BC systems and 6) the usage of Federated Learning (FL) to preserve the privacy of EHRs in the healthcare domain. We acquired 46 research articles based on the criteria (inclusion-exclusion) that investigate the above-mentioned issues. Conclusion: The SLR will serve as the state-of-the-art (SOTA) for future researchers in the field of BC in healthcare. Additionally, the paper provides insights to the new researchers to revolutionize the healthcare domain by adopting the latest digitalized technologies. The proposed study identified various reflections. It analyzed the architectural mechanism that supports the security and interoperability of EHRs. Secondly, the study described different tools and frameworks to improve the privacy and security of EHRs using the BC. Thirdly, the open issues of storing and preserving the EHRs using BC in the healthcare system were determined. Fourth, it analyzed and provided a detailed view of using standardized ways for storing and handling big data by using the BC system. Lastly, the usage of FL to preserve the privacy of EHRs was analyzed."
Leveraging AI for Enhanced Software Effort Estimation: A Comprehensive Study and Framework Proposal,T. N. Tran; H. T. Tran; Q. N. Nguyen,10.1109/ICCD59681.2023.10420603,2023,"This paper presents an extensive study on the application of AI techniques for software effort estimation in the past five years from 2017 to 2023. By overcoming the limitations of traditional methods, the study aims to improve accuracy and reliability. Through performance evaluation and comparison with diverse Machine Learning models, including Artificial Neural Network (ANN), Support Vector Machine (SVM), Linear Regression, Random Forest and other techniques, the most effective method is identified. The proposed AI-based framework holds the potential to enhance project planning and resource allocation, contributing to the research area of software project effort estimation."
Artificial Intelligence Project Success Factors: Moral Decision-Making with Algorithms,G. J. Miller,10.15439/2021F26,2021,"The algorithms implemented through artificial intelligence (AI) and big data projects are used in life-anddeath situations. While research exists to address varying aspects of moral decision-making with algorithms, the definition of project success is not readily available. Nevertheless, researchers place the burden of responsibility for ethical decisions from AI systems on the system developers. Using a systematic literature review, this research identified 71 AI project success factors in 14 groups related to moral decision-making with algorithms. It contributes to project management literature, specifically for AI projects. Project managers and sponsors can use the results during project planning and execution."
A Comprehensive Review of Deepfake Detection In Advanced Neural Network Architectures and Deep Learning Strategies,D. Singh; P. Singh; R. Bhandari,10.1109/GlobalAISummit62156.2024.10947978,2024,"The proliferation of deepfakes, facilitated by advancements in machine learning and artificial intelligence, poses a significant challenge to online security and information integrity. This study reviews current deep fake detection techniques, focusing on methodologies for identifying manipulated content in images, audio, and video. 17 articles were selected for this study using Prisma guidelines. The function of sophisticated machine learning models, such as Long Short-Term Memory (LSTM) networks and (CNNs), is highlighted for automating detection processes. The review also examines notable algorithms and datasets, including FaceForensics++, MesoNet, and XceptionNet, and their effectiveness in enhancing detection accuracy. A case study using frame extraction and resizing techniques demonstrates a detection accuracy of 62%, with precision at 61 %, recall at 63%, and an F1 score of 62% is also included. The findings underscore the need for ongoing refinement and adaptation of detection systems to counteract the evolving nature of deepfake technologies. The study providing thorough overview of the state-of-the-art in deepfake detection, offering insights into effective methodologies and future research directions."
Advance Machine Learning Methods for Dyslexia Biomarker Detection: A Review of Implementation Details and Challenges,O. L. Usman; R. C. Muniyandi; K. Omar; M. Mohamad,10.1109/ACCESS.2021.3062709,2021,"Dyslexia is a neurological disorder that is characterized by imprecise comprehension of words and generally poor reading performance. It affects a significant population of school-age children, with more occurrences in males, thus, putting them at risk of poor academic performance and low self-esteem for a lifetime. The long-term hope is to have a dyslexia diagnostic method that is informed by neural-biomarkers. In this regard, large numbers of machine learning methods and, more recently, deep learning methods have been implemented across various types of dataset with the above-chance classification accuracy. However, attainment of clinical acceptability of these state-of-the-art methods is bedeviled by certain challenges including lack of biologically-interpretable biomarkers, privacy of dataset and classifiers, hyper-parameter selection/optimization, and overfitting problem among others. This review paper critically analyzes recent machine learning methods for detecting dyslexia and its biomarkers and discusses challenges that require proper attentions from the users of deep learning methods in order to enable them to attain clinically relevance and acceptable level. The review is conducted within the premise of implementation and experimental outcomes for each of the 22 selected articles using the Preferred Reporting Items for Systematic review and Meta-Analyses (PRISMA) protocol, with a view to outlining some critical challenges for achieving high accuracy and reliability of the state-of-the-art machine learning methods. As an evidence-based protocol for reporting in systematic reviews and meta-analyses, PRISMA helps to ensure clarity and transparency of this paper by showing a four-phase flow diagram of the selection process for articles used in this review. It is therefore, envisaged that higher classification performance of clinical relevance can be achieved using deep learning models for dyslexia and its biomarkers by addressing identified potential challenges."
Prediction of Speeding Intention Based on the Theory of Planned Behavior,E. H. Abderrahim; R. Taoufik,10.1109/SITA60746.2023.10373749,2023,"Speeding remains a critical factor contributing to road crashes. Therefore, understanding drivers’ intentions to speed is crucial for designing effective interventions helping drivers to respect speed limit on road. Demographic and psychological factors are widely used to explain several human behaviors. However, their predictive power change in relation to the kind of targeted behavior. Moreover, many studies consider psychological factors as primary contributors to behaviors and intentions and consider demographics variables as background factors. This paper investigates the predictive power of demographic and psychological factors using machine learning algorithms to predict drivers speeding intentions. A survey based on Theory of Planned Behavior (TPB) was conducted to collect data about the speeding intention of 254 drivers in Morocco. The survey includes questions about demographic and psychological factors. Five machine learning algorithms were used to assess the accuracy of speeding prediction with the combination background and psychological factors. Also, the correlation between all variables was assessed. Finally, a Structural Equation Modeling (SEM) is used to have a deep understanding of speeding intention. The results of this study highlight the importance of considering demographic features as background factors when predicting the intention to speed, using psychological factors. Moreover, the study underlines the potential of the TPB in the understanding of speeding intention. Overall, the findings contribute to the ongoing efforts to develop interventions aimed at combating speeding behavior and promoting safer driving practices."
User Experience Design Using Machine Learning: A Systematic Review,A. M. H. Abbas; K. I. Ghauth; C. -Y. Ting,10.1109/ACCESS.2022.3173289,2022,"User experience (UX) is the key to increased productivity by enhancing the usability and interactivity of the product. Machine learning (ML) solutions have raised user and academic awareness of technical innovation. As a result, ML is becoming increasingly popular to improve the quality of UX. Several investigations have highlighted a potential lack of studies on the overall challenges and recommendations for UX using ML. Therefore, more attention should be paid to ML’s existence and potential applications across various applications to get the most out of ML techniques to improve the UX design process. To this objective, a systematic review of the literature was performed as to determine the challenges faced by UX designers when incorporating ML in their design process. Recommendations that help UX designers incorporate ML into UX design will be highlighted. Furthermore, the PRISMA approach is used (a process that has been established in the literature), to restrict the chance of bias at the selection stage. Relevant articles in the following four databases were searched: IEEE Xplore, Scopus, Web of Science, and ACM. The findings revealed that the number of publications on issues linked to UX with ML had advanced exponentially. This review highlights the challenges, recommendations, tools, algorithms, techniques and datasets used in different studies. In addition, suggestions are given for future investigations."
"Explainable Predictive Maintenance: A Survey of Current Methods, Challenges and Opportunities",L. Cummins; A. Sommers; S. B. Ramezani; S. Mittal; J. Jabour; M. Seale; S. Rahimi,10.1109/ACCESS.2024.3391130,2024,"Predictive maintenance is a well studied collection of techniques that aims to prolong the life of a mechanical system by using artificial intelligence and machine learning to predict the optimal time to perform maintenance. The methods allow maintainers of systems and hardware to reduce financial and time costs of upkeep. As these methods are adopted for more serious and potentially life-threatening applications, the human operators need trust the predictive system. This attracts the field of Explainable AI (XAI) to introduce explainability and interpretability into the predictive system. XAI brings methods to the field of predictive maintenance that can amplify trust in the users while maintaining well-performing systems. This survey on explainable predictive maintenance (XPM) discusses and presents the current methods of XAI as applied to predictive maintenance while following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines. We categorize the different XPM methods into groups that follow the XAI literature. Additionally, we include current challenges and a discussion on future research directions in XPM."
Systematic Review of Artificial Intelligence in Acute Respiratory Distress Syndrome for COVID-19 Lung Patients: A Biomedical Imaging Perspective,J. S. Suri; S. Agarwal; S. K. Gupta; A. Puvvula; K. Viskovic; N. Suri; A. Alizad; A. El-Baz; L. Saba; M. Fatemi; D. S. Naidu,10.1109/JBHI.2021.3103839,2021,"SARS-CoV-2 has infected over ∼165 million people worldwide causing Acute Respiratory Distress Syndrome (ARDS) and has killed ∼3.4 million people. Artificial Intelligence (AI) has shown to benefit in the biomedical image such as X-ray/Computed Tomography in diagnosis of ARDS, but there are limited AI-based systematic reviews (aiSR). The purpose of this study is to understand the Risk-of-Bias (RoB) in a non-randomized AI trial for handling ARDS using novel AtheroPoint-AI-Bias (AP(ai)Bias). Our hypothesis for acceptance of a study to be in low RoB must have a mean score of 80% in a study. Using the PRISMA model, 42 best AI studies were analyzed to understand the RoB. Using the AP(ai)Bias paradigm, the top 19 studies were then chosen using the raw-cutoff of 1.9. This was obtained using the intersection of the cumulative plot of “mean score vs. study” and score distribution. Finally, these studies were benchmarked against ROBINS-I and PROBAST paradigm. Our observation showed that AP(ai)Bias, ROBINS-I, and PROBAST had only 32%, 16%, and 26% studies, respectively in low-moderate RoB (cutoff>2.5), however none of them met the RoB hypothesis. Further, the aiSR analysis recommends six primary and six secondary recommendations for the non-randomized AI for ARDS. The primary recommendations for improvement in AI-based ARDS design inclusive of (i) comorbidity, (ii) inter-and intra-observer variability studies, (iii) large data size, (iv) clinical validation, (v) granularity of COVID-19 risk, and (vi) cross-modality scientific validation. The AI is an important component for diagnosis of ARDS and the recommendations must be followed to lower the RoB."
MLHOps: Machine Learning Health Operations,F. K. Khattak; V. Subasri; A. Krishnan; C. Pou-Prom; S. Akinli-Kocak; E. Dolatabadi; D. Pandya; L. Seyyed-Kalantari; F. Rudzicz,10.1109/ACCESS.2024.3521279,2025,"Machine Learning Health Operations (MLHOps) is the combination of processes for reliable, efficient, usable, and ethical deployment and maintenance of machine learning models in healthcare settings. This paper provides both a survey of work in this area and guidelines for developers and clinicians to deploy and maintain their own models in clinical practice. We cover the foundational concepts of general machine learning operations and describe the initial setup of MLHOps pipelines (including data sources, preparation, engineering, and tools). We then describe long-term monitoring and updating (including data distribution shifts and model updating) and ethical considerations (including bias, fairness, interpretability, and privacy). This work therefore provides guidance across the full pipeline of MLHOps from conception to initial and ongoing deployment. We also include a checklist to ensure thorough verification of each step in the process."
Review of measures for improving ML model interpretability: Empowering decision makers with transparent insights,S. Arsic; M. Mihic; D. Petrovic; Z. Mitrovic; S. C. Kostic; O. Mihic,10.1109/ACDSA59508.2024.10467907,2024,"This paper investigates actionable measures to enhance the interpretability of machine learning models, addressing the critical need for transparency in decision-making processes. By proposing and briefly comparing specific measures, this paper aims to empower common knowledge with clearer insights into model predictions, fostering trust and understanding. Theoretical findings and overall discussion encompass techniques for model explanation, feature importance, and interpretability tools, offering a comprehensive guide for practitioners seeking to clarify the black box nature of machine learning outputs. Findings suggest three methods for improving model interpretability. The outlined approaches prioritize real-world applicability, enabling managers to make informed decisions with confidence."
Enhancing Engineering Cost Risk Management Through Artificial Intelligence-Based Warning and Control Systems,W. Xu,10.1109/ICPICS62053.2024.10795935,2024,"With the growing importance of engineering cost risk management in the construction sector, the integration of artificial intelligence (AI) technology has emerged as a crucial tool for enhancing the effectiveness of risk alerts and control measures. This research endeavors to delve into the utility of AI in managing engineering cost risks and to suggest appropriate warning and control approaches. By conducting a thorough literature review and case studies, the study identified and compared three AI models: the Long Short-term Memory (LSTM) Network, Support Vector Machine (SVM), and Random Forest (RF), evaluating their performance against key metrics such as Cost Variance Ratio (CVR), Schedule Performance Index (SPI), and Risk Response Efficiency (RRE). The research methodology encompassed data collection and preprocessing, model training and validation, and comprehensive performance assessment. The findings revealed that the LSTM model achieved a minimum CVR of 1.1%, a maximum SPI of 1.03, and a remarkable risk response efficiency as fast as 12.5 minutes. The study concludes that LSTM significantly enhances the precision of engineering cost risk warnings, offering a novel insight and robust technical foundation for project management in the construction industry."
Bots and their Uses in Software Development: A Systematic Mapping Study,R. Moguel-Sánchez; C. S. Martínez-Palacios; J. O. Ocharán-Hernández; X. Limón; Á. J. Sánchez-García,10.1109/CONISOFT55708.2022.00027,2022,"Tools that help software developers in their activities have continually been created for tasks in the Software Development Life Cycle. Over the last few years, there has been an increasing interest in the research and development of bots designed to boost the software development process. Consequently, there has been a recent surge in published studies regarding bots in the software development process. Our aim is to present the state of the art of bots for software development. We detail uses, benefits, challenges, and intelligence levels of bots involved in software development reported in academic papers. In this work, we conducted a Systematic Mapping Study (SMS). From the primary studies, we have identified nine activities related to the software development phases in which bots are used, seven benefits, and seven challenges in using bots for software development. The most popular activities for bot applications in software development, in descending order, are project management, code analysis, debugging, and project onboarding. The main benefits are generating bug fixes, providing helpful information, and automating tasks. Alongside the benefits, the primary drawbacks were low effectiveness of fixes, poor bot performance in a task, and a dependency on third-party technologies. We seek to help developers learn about bots for their use in future projects, and investigators identify future research topics through this SMS."
AI-Powered Post-Discharge Monitoring to Prevent Hospital Readmissions: A Global Scoping Review in Hospital Care,M. M. Santos; M. Peyroteo; L. V. Lapão,10.1109/ICE/ITMC61926.2024.10794414,2024,"Hospital readmissions management poses a significant burden on healthcare systems, straining resources and negatively impacting patient treatment. Traditional post-discharge follow-up methods, such as phone calls, often lack efficiency and personalization. Artificial Intelligence (AI) (e.g., Machine Learning (ML)) offers a transformative approach with the potential to improve post-discharge care. This scoping review aimed to comprehensively map existing research on AI-powered solutions for post-discharge monitoring, focusing on their application in risk stratification and their potential for reducing hospital read-missions. Following PRISMA-ScR guidelines, a comprehensive search was conducted across four databases using keywords related to AI, post-discharge, and risk stratification. Studies published between 2018 and January 2024 were included if they described the development and application of an ML model in a post-discharge setting for risk stratification. Sixteen studies met the inclusion criteria. Tree-based algorithms, particularly XGBoost, were the predominant ML approach, with promising performance metrics for readmission risk prediction (AUC ranging from 0.69 to 0.994). However, limitations were identified, including restricted generalizability due to data source limitations and a lack of real-time implementation in many studies. This review highlights the potential of AI in post-discharge moni-toring, particularly the use of ML for risk stratification. While challenges remain regarding generalizability and real-world implementation, the findings suggest AI holds immense promise for improving post-discharge care, potentially reducing readmissions, and optimizing resource allocation. Future research with broader datasets and real-time applications can further solidify the role of AI in revolutionizing post-discharge monitoring and patient outcomes."
Deep Learning for Diabetic Retinopathy in Fundus Images,K. Rahimi; R. Rituraj; D. Ecker,10.1109/CINTI-MACRo57952.2022.10029554,2022,"Clinically, using fundus pictures for predicting and detecting blind illnesses such as diabetic retinopathy (DR) is crucial. Deep learning (DL) is becoming a more common and promising technique in the different applications of DR, such as prediction, detection, classification, and disease diagnosis. Developing a review paper to analyze the DL techniques and their performance in the field is essential. We prepared a standard systematic review database including 341 publications. Accordingly, the main aim of the present review work is to present a systematic state-of-the-art by relying on PRISMA guidelines for the performance analysis of the DL in DR applications. The study has been shown in three main steps. The first step is to collect the database, the second step is to analyze the databases, and the last step is to conclude the study’s main findings. According to the results, most studies employed accuracy as the most reliable and general evaluation metric for analyzing the DL techniques in different DR applications. Also, CNN has the most share of applications compared to other DL techniques. On the other hand, the best performance is related to the ensemble and advanced DL techniques. We’ll also publish and regularly update the most recent discoveries in future studies to stay up with the quick technological improvements."
Fake News Prediction Using Hybrid Model–Systematic Literature Review,M. I. Elias; Y. Mahmud; S. Mutalib; S. N. Kamaliah Kamarudin; R. Maskat; S. A. Rahman,10.1109/AiDAS60501.2023.10284628,2023,"The proliferation of fake news is recognized as a deliberate strategy to manipulate and misinform audiences, thereby undermining the consumption of authentic information. The repercussions of this phenomenon span from mere annoyance to the significant potential for distorting societal and even national perspectives. The contemporary technological landscape has expedited the dissemination of information, underscoring the urgency of discerning evolving techniques in fake news detection. In the context of prevalent social media platforms, a sole reliance on content-based methodologies proves inadequate. This study employs a systematic literature review following the PRISMA protocol to illuminate the contemporary landscape of fake news detection methodologies. The investigation reveals a spectrum of strategies categorized under the rubric of hybrid models, wherein multiple features or models are amalgamated. The discerned hybrid models exhibit a diversity of methodologies, coalescing into two overarching paradigms: the fusion of diverse features and the integration of multiple models. The former primarily encompasses composite feature-based ensembles, often amalgamating content-based features with complementary attributes. The latter paradigm predominantly entails the synthesis of various deep learning approaches, culminating in enhanced performance metrics for fake news detection. The synthesized findings not only provide a comprehensive overview of the prevalent hybrid approaches but also establish a benchmark for forthcoming researchers embarking on predictive endeavors involving hybrid methodologies."
Machine Learning Methods for Cybersecurity: Review and Bibliometric Analysis,E. Kail; A. Banati; R. Fleiner; A. Mosavi; C. Mako,10.1109/SISY60376.2023.10417894,2023,"This article presents a bibliometric analysis and review of the applications of machine learning methods in cybersecurity. A taxonomy of the methods and applications is presented, and fundamental studies are introduced and reviewed. The taxonomy of the methods classifies the machine learning methods in four fundamental groups based on the methods' architecture, i.e., artificial neural networks-based, decision trees-based, support vectors machines-based, and ensembles. The specific cybersecurity application of each group is further discussed. The deep learning and hybrid methods are not included in this study. The bibliometric analysis is performed considering the Scopus database. For selection and evaluation of the articles a modified version of PRISMA is proposed."
Multimodal Machine Learning for Stroke Prognosis and Diagnosis: A Systematic Review,S. Shurrab; A. Guerra-Manzanares; A. Magid; B. Piechowski-Jozwiak; S. F. Atashzar; F. E. Shamout,10.1109/JBHI.2024.3448238,2024,"Stroke is a life-threatening medical condition that could lead to mortality or significant sensorimotor deficits. Various machine learning techniques have been successfully used to detect and predict stroke-related outcomes. Considering the diversity in the type of clinical modalities involved during management of patients with stroke, such as medical images, bio-signals, and clinical data, multimodal machine learning has become increasingly popular. Thus, we conducted a systematic literature review to understand the current status of state-of-the-art multimodal machine learning methods for stroke prognosis and diagnosis. Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines during literature search and selection, our results show that the most dominant techniques are related to the fusion paradigm, specifically early, joint and late fusion. We discuss opportunities to leverage other multimodal learning paradigms, such as multimodal translation and alignment, which are generally less explored. We also discuss the scale of datasets and types of modalities used to develop existing models, highlighting opportunities for the creation of more diverse multimodal datasets. Finally, we present ongoing challenges and provide a set of recommendations to drive the next generation of multimodal learning methods for improved prognosis and diagnosis of patients with stroke."
aiSTROM–A Roadmap for Developing a Successful AI Strategy,D. Herremans,10.1109/ACCESS.2021.3127548,2021,"A total of 34% of AI research and development projects fail or are abandoned, according to a recent survey by Rackspace Technology of 1,870 companies. In this perspective paper, a new STrategic ROadMap, aiSTROM, is presented that empowers managers to create an AI strategy. A comprehensive approach is provided that guides managers and lead developers through the various challenges in the implementation process. In the aiSTROM framework, the top  $n$  potential projects (typically 3-5) are first identified. For each of those, seven areas of focus are thoroughly analysed. These areas include creating a data strategy that takes into account unique cross-departmental machine learning data requirements, security, and legal requirements. aiSTROM then guides managers to think about how to put together an interdisciplinary artificial intelligence (AI) implementation team given the scarcity of AI talent. Once an AI team strategy has been established, it needs to be positioned within the organization, either cross-departmental or as a separate division. Other considerations include AI as a service (AIaas) and outsourcing development. Looking at new technologies, one has to consider challenges such as bias, the legality of black-box models, and keeping humans in the loop. Next, like any project, value-based key performance indicators (KPIs) need to be defined to track and validate the progress. Depending on the company’s risk strategy, a SWOT analysis (strengths, weaknesses, opportunities, and threats) can help further classify the shortlisted projects. Finally, one should make sure that the strategy includes continuous education of employees to enable a culture of adoption. This unique and comprehensive framework offers a practical tool for managers and lead developers."
VSEST 29110 Tool: Using ChatGPT to Evaluate the Implementation of the ISO/IEC 29110 Work Products,J. Mejía; V. Terrón-Macias; M. Muñoz; M. Terrón-Hernández; M. Canseco-Pérez,10.1109/ACCESS.2024.3449252,2024,"The global software industry is predominantly composed of micro, small, and medium-sized enterprises (MSMEs), highlighting the need for software quality management to ensure the proper functioning and quality of the software. This research focuses on the evaluation of the implementation of the ISO/IEC 29110 standard work products, which is a standard tailored by the ISO/IEC specifically for MSMEs, which improves the software development process by implementing two processes in its basic profile: Project Management (PM) and Software Implementation (SI). Despite this standard being tailored specifically for this type of enterprise, implementing ISO/IEC 29110 faces several challenges, such as a lack of knowledge and difficulties in adequately implementing the work products regarding the compliance of standard criteria, among others. To address these challenges, we introduce VSEST 29110, a web tool designed to evaluate the ISO/IEC 29110 standard implementation work products by leveraging Artificial Intelligence (AI) technologies, specifically the ChatGPT model, provide detailed feedback on compliance with standard criteria, offer suggestions for improvement based on ChatGPT analysis and streamline the implementation process for MSMEs. To achieve this, our research incorporates a systematic literature review and validation through a case study by document analysis, demonstrating VSEST 29110’s effectiveness in enhancing compliance and providing comprehensive feedback compared to auditor recommendations, which impacts 69.33% on average."
A Systematic Review of Electroencephalography Open Datasets and Their Usage With Deep Learning Models,A. Nogales; Á. J. García-Tejedor,10.1109/ACCESS.2023.3293421,2023,"Data are the main headache for machine learning, both because of their varied nature and their limited availability. The medical field brings together both situations: tables, images, text, or signals that are difficult to acquire due to the number of patients, the complexity and time of acquisition, or ethical constraints. The existence of open datasets is the best option for researchers in this field. Electroencephalograms are a good example of this situation. This paper identifies the primary open datasets of electroencephalography tests and how they are used in deep learning models. The aim is to provide structured information that can be consulted by researchers in the field (both physicians and computer scientists) to know which datasets are available, which characteristics they have, or which deep learning models could be applied to them. The process followed the PRISMA methodology for systematic reviews applying different inclusion and exclusion criteria to obtain a set of high-quality papers on which the data sets used were analyzed. The databases included in the searches were Scopus, PubMed, Web of Science (WOS), Science Direct, IEEE Explorer, and SpringerLink. In total, 37 papers were selected which included 30 datasets that have been considered. Then, the DL models used in the papers and the different characteristics of the datasets have been statistically analyzed by obtaining different measures and graphs. The most relevant conclusions are the widespread use of convolutional neural networks (the less innovative among the different models) as the main tool for EEG data analysis. Against this position, we found the use of hybrid models and the family of RNNs as techniques to use in cases of brain stimuli, classification of levels of fatigue, and diagnosis of diseases. Related to the datasets’ features, we demonstrate the difficulty in compiling this data due to the number of tests and that the minimum of channels or sampling frequency recommended to obtain good accuracies in the model should be studied."
"Machine Learning in the Prevention, Diagnosis and Management of Diabetic Foot Ulcers: A Systematic Review",J. Tulloch; R. Zamani; M. Akrami,10.1109/ACCESS.2020.3035327,2020,"Diabetic foot ulcers (DFUs) are a serious complication for people with diabetes. They result in increased morbidity and pressures on health system resources. Developments in machine learning (ML) offer an opportunity for improved care of individuals at risk of DFUs, to identify and synthesise evidence about the current uses and accuracy of ML in the interventional care and management of DFUs, and, to provide a reference for areas of future research. PubMed, Google Scholar, Web of Science and Scopus were searched using the Preferred Reporting Items for a Systematic Review and Meta-analysis of Diagnostic Test Accuracy Studies (PRISMA-DTA) guidelines for papers involving ML and DFUs. In order to be included, studies needed to mention ML, DFUs, and report relevant outcome measures regarding ML algorithm accuracy. Bias in included studies was assessed using the quality assessment tool for diagnostic accuracy (QUADAS-2). 37 out of 3769 papers were included after applying eligibility criteria. Included papers reported accuracy measures for multiple types of ML algorithms in DFU studies. Whilst varying across the ML algorithm used, all studies reported at least 90% accuracy compared to gold standards using a minimum of one reported ML algorithm for processing or recording data. Applications where ML had positive effects on DFU data analysis and outcomes include image segmentation and classification, raw data analysis and risk assessment. ML offers an effective and accurate solution to guide analysis and procurement of data from interventions which are designed for the care of DFUs in small samples and study conditions. Current research is limited, and, for the development of more applicable ML algorithms, future research should address the following: direct comparison of ML applications with current standards of care, health economic analyses and large scale data collection. There is currently no evidence to confidently suggest that ML methods in DFU diagnosis are ready for implementation and use in healthcare settings."
Exploration of EEG-Based Depression Biomarkers Identification Techniques and Their Applications: A Systematic Review,A. Dev; N. Roy; M. K. Islam; C. Biswas; H. U. Ahmed; M. A. Amin; F. Sarker; R. Vaidyanathan; K. A. Mamun,10.1109/ACCESS.2022.3146711,2022,"Depression is the most common mental illness, which has become the major cause of fear and suicidal mortality or tendencies. Currently, about 10% of the world population has been suffering from depression. The classical approach for detecting depression relies on the clinical questionnaire, which depends on the patients’ responses as well as observing their behavioral activities. However, there is no established method to detect depression from EEG biomarkers. Therefore, exploration of EEG biomarkers for depression assessments is vital and has a great potential to improve our understanding and clinical interventions. In this study, we have conducted a systematic review of 52 research articles using the PRISMA-P systematic review protocol, where we analyzed their research methodologies and outcomes. We categorized the experimentations in these articles according to their physical and psychological aspects scaled by the commonly used clinical questionnaire-based assessments. This study finds that the negative stimuli are the better identification strategies for evaluating depression through EEG signals. From this exploration, researchers observed that the Neural Connectivity Analysis and Brain Topological Mapping have huge potentials for finding depression biomarkers, and it is evident that the right-side hemisphere and frontal and parietal-occipital cortex are distinct regions to detect depression using EEG signals. For this mechanism, researchers are using many signal processing and machine learning approaches. In the case of filtering, Independent Component Analysis (ICA) is commonly used to eliminate physiological and non-physiological artifacts. Among machine learning approaches, Convolutional Neural Network (CNN) and Support Vector Machine (SVM) showed better performance for classifying healthy and depressed brains. The authors hope, this study will create an opportunity to explore more in the future for EEG as diagnostic tool by analyzing brain functional connectivity for focusing on clinical interventions."
"Developing an Ethical Regulatory Framework for Artificial Intelligence: Integrating Systematic Review, Thematic Analysis, and Multidisciplinary Theories",J. Wang; Y. Huo; J. Mahe; Z. Ge; Z. Liu; W. Wang; L. Zhang,10.1109/ACCESS.2024.3501332,2024,"Artificial intelligence (AI) ethics has emerged as a global discourse within both academic and policy spheres. However, translating these principles into concrete, real-world applications for AI development remains a pressing need and a significant challenge. This study aims to bridge the gap between principles and practice from a regulatory government perspective and promote best practices in AI governance. To this end, we developed the Ethical Regulatory Framework for AI (ERF-AI) to guide regulatory bodies in constructing mechanisms, including role setups, procedural configurations, and strategy design. The framework was developed through a systematic review, thematic analysis, and the integration of interdisciplinary concepts. A comprehensive search was conducted across four electronic databases (PubMed, IEEE Xplore, Web of Science, and Scopus) and four additional sources containing AI standards and guidelines from various countries and international organizations, focusing on studies published from 2014 to 2024. Thematic analysis identified and refined key themes from the included literature and integrated concepts from process control theory, computer science, organizational management, information technology, and behavioral psychology. This study adhered to the PRISMA guidelines and employed NVivo for thematic analysis. The resulting framework encompasses 23 themes, particularly emphasizing three feedback-loop processes: the ethical review process, the incentive and penalty process, and the mechanism improvement process, offering theoretical guidance for the construction of ethical regulatory mechanisms. Based on this framework, a seven-step process and case examples for mechanism design are presented, enhancing the practicality of ERF-AI in developing ethical regulatory mechanisms. Future research is expected to explore customization of the framework to remain responsive to emerging AI trends and challenges, supported by empirical studies and rigorous testing for further refinement and expansion."
Unveiling The Digital Realm: A Systematic Review on Regulating Blockchain for Online Breast Milk Services,A. ‘. B. Hisham; N. A. M. Yusof; Z. Shaari; H. Abas; S. Mustapa; S. Zahar; W. R. Salehudin; S. Saliman; M. Almusawi; W. A. Mustafa; F. Norwahidah Mohd Yunus,10.1109/ICTEASD57136.2023.10585005,2023,"The traditional or modern practice of sharing breastmilk raises concern regarding security, traceability, safety, and quality assurance. This systematic review explores the application of blockchain technology in addressing these issues, aiming to enhance the security and efficiency of breastmilk sharing in healthcare settings. This systematic review delves into the burgeoning intersection of blockchain technology and online breast milk services, exploring the multifaceted landscape encompassing healthcare data management, privacy, safety, security, quality assurance, governance, interoperability, and legal and regulatory challenges. Employing a comprehensive research methodology, we conducted an extensive review of scholarly articles of Scopus, Web of Science and PubMed databases using Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) technique. The final main data is n = 26 and analyzed using synthesis approach. Four main themes were generated: (i) blockchain for healthcare data management, (ii) privacy of data, safety, security, and quality assurance for donated breastmilk, (iii) governance and interoperability, (iv) legal and regulatory challenges. This systematic review highlighted the vast potential, capabilities, opportunities, and challenges of using blockchain for a noble cause of sharing or donating breastmilk. The inputs are useful for policymakers, healthcare professionals, and researchers in fostering an introducing better policy, policy interventions and legal mechanisms for a better, secure, transparent, and ethical digital ecosystem in maternal and child healthcare."
Cattle Breeding Management using Smart System: A Systematic Literature Review,D. Arnaldy; H. Sukoco; S. N. Neyman; Muladno; K. B. Seminar,10.1109/IC2IE56416.2022.9970169,2022,"This paper uses a smart system to present a methodical literature review on cattle breeding management. Indonesia is a large country with a veritably large population. Indonesian people like to eat beef; thus the demand for beef in Indonesia always increases. Therefore, Indonesia needs a system to manage livestock data nationally. Sekolah Peternakan Rakyat (SPR) is one of the Bogor Agricultural Institute's programs to strengthen the people's livestock business. In addressing this issue, experimenters proposed using an intelligence system for cattle breeding management in SPR. The method used in this research is a systematic literature review (SLR). The approach used is PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses). Hence, this paper analyses the literature to fill the gap by conducting a scoping study to answer three pre-defined research questions. First, search for papers using publish or perish tools with a database of papers used for the last five years (2017 - 2022). Grounded on the search results, the total composition data for all data from the composition database used was 5513 papers, with an aggregate of 3010 papers in the form of journals. In comparison, the number of proceedings amounted to 239 papers. It is known that the content of cattle breeding management using information technology has been extensively carried out. Inquiries are generally carried out outside Indonesia. Although several papers discuss cattle breeding in Indonesia, the discussion is more towards livestock, not information technology."
A Review of Machine Learning Models for Predicting Agile Methodology,H. A. Zaidi; P. Jain,10.1109/ICDT61202.2024.10489437,2024,"Agile methodologies., particularly the Scrum framework, have emerged as integral tools for addressing complex challenges and delivering high value software products. This research delves into the intersection of agile methodologies and machine learning, proposing a predictive approach to forecast Scrum Agile adoption. As agile methodologies gain prominence across diverse sectors, the integration of predictive and prescriptive analytics emerges as a potent strategy for unraveling intricate interdependencies impacting agile project outcomes. This study follows the data science lifecycle, employing the scientific method to iteratively evaluate and enhance the predictive model for Scrum Agile adoption. Beginning with issue definition and data collection, the research progresses through stages of data preparation, exploration, and feature extraction. Leveraging machine learning techniques, predictive models are developed, tested, and assessed to provide insights into the likelihood of successful Scrum Agile adoption."
Prioritization Based Taxonomy of DevOps Challenges Using Fuzzy AHP Analysis,M. A. Akbar; W. Naveed; S. Mahmood; A. A. Alsanad; A. Alsanad; A. Gumaei; A. Mateen,10.1109/ACCESS.2020.3035880,2020,"The DevOps (development and operations) is a collaborative software development environment which offers the continues development and deployment of quality software project within short time. The DevOps practices are not yet mature enough, and the software organizations hesitate to adopt it. This study aims: 1) to explore the DevOps challenges by conducting systematic literature review (SLR) and to get the insight of industry experts via questionnaire survey study and 2) to prioritize the investigated challenges using fuzzy analytical hierarchy process (FAHP). The study findings provide the set of critical challenges faced by the software organizations while adopting DevOps and a prioritization-based taxonomy of the DevOps challenges. The application of FAHP is novel in this research area as it assists in addressing the vagueness of practitioners concerning the influencing factors of DevOps. We believe that the finding of this study will serve as a body of knowledge for real world practitioners and researchers to revise and develop the new strategies for the successful implementation of DevOps practices in the software industry."
Exploring the Role of Generative Artificial Intelligence in the Energy Sector: A Comprehensive Literature Review,S. Surathunmanun; W. Ongsakul; J. G. Singh,10.1109/ICUE63019.2024.10795598,2024,"Generative Artificial Intelligence (GenAI) enhances productivity by creating data, forecasting, optimizing, and understanding human language. In the energy sector, it is projected to have a $240 billion global economic impact, though research remains limited. This paper reviews GenAI's benefits, challenges, and research gaps in the energy sector, also focusing on climate change efforts. A PRISMA-SCR-based literature review from January 2022 to May 2024 was conducted using IEEE Xplore, ScienceDirect, ACM Digital Library, and Google Scholar. GenAI tools extracted data, verified by researchers. Analysis of 33 papers shows GenAI excels in knowledge integration and prediction. It generates synthetic electricity demand data, manages grids, forecasts energy demand, and optimizes renewable energy systems. Key challenges include hallucinations, data biases, privacy concerns, misuse, and system errors. Solutions involve improving training data, system fine-tuning, human oversight, and security measures. Research gaps include synthetic data realism, model evaluation standards, and integrating GenAI with blockchain and IoT."
Sensors and Machine Learning Methods for Intent Detection in Lower Limb Prosthetic Devices: A Review,M. R. Islam; X. Shen; E. Sazonov,10.1109/JSEN.2025.3565195,2025,"Advances in lower limb prosthetic (LLP) devices have greatly enhanced mobility for amputees, but achieving smooth, intuitive control remains a significant hurdle. A vital aspect of this challenge is the ability of prosthetic devices to detect user intentions, such as starting or stopping movement, changing locomotion modes, or adjusting speed. Reliable intent detection (ID) is crucial for improving adaptability, reducing the cognitive effort required from users, and maintaining safety, especially in changing environments. This systematic review, conducted in accordance with PRISMA guidelines, involved a comprehensive analysis of 60 full-text scientific articles. The review focuses on four critical areas: (1) how ID is defined across various studies, (2) the sensor technologies used and sensor placement, (3) the machine learning methods developed for ID, and (4) the metrics used to evaluate the performance of these methods. In summary, this review provides an in-depth look at the current developments in LLP ID, identifies critical gaps, and proposes areas for future research to enhance prosthesis functionality."
A Taxonomy of AI-Based Assessment Educational Technologies,M. M. Hammad; M. Al-Refai; W. Musallam; S. Musleh; E. Faouri,10.1109/ICICS63486.2024.10638295,2024,"Artificial Intelligence (AI) is increasingly applied across various domains, including education, where it enhances numerous aspects of the learning process, from course design to assessment. Despite its benefits in efficiency, scalability, and consistency, AI in education is applied in different learning and educational stages. This paper focuses on the use of AI in the assessment stage. To that end, this paper proposes a taxonomy of AI-based learner assessment educational technologies (EduTech) from both research and industrial perspectives. The taxonomy provides a comprehensive understanding and identifies gaps in the field. Using the PRISMA framework, we systematically review related research papers and tools."
Systematic Review of Qualitative and Quantitative Studies on Perceived Employability of Graduates,F. K. Khaiser; A. Saad; C. Mason,10.1109/IMCOM56909.2023.10035634,2023,"The assessment of future students' employability by the Institute of Higher Learning in collaboration with career centres is one of the most crucial steps in the educational industry for establishing an active and ascendable plan. Predictive analysis for this project is done using machine learning. This study investigates the Employability Signals of Undergraduates in accordance with the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) criteria. The findings demonstrate that higher education was where the most accurate predictor of undergraduate students' employability was initially examined. The study's conclusions can be used to develop a roadmap that will make it simpler to use predictive analytics. The findings of this study may also facilitate the creation and application of predictive analytics, one of the possible approaches for analysing the education data gathered during the pre-covid period for this study. Systematic literature reviews should be trustworthy, repeatable, and valid when used in scientific investigations. As a result, the inquiry will reach a conclusion based on the evaluations found on pertinent and customized dates."
Automatic Speech Recognition: Systematic Literature Review,S. Alharbi; M. Alrazgan; A. Alrashed; T. Alnomasi; R. Almojel; R. Alharbi; S. Alharbi; S. Alturki; F. Alshehri; M. Almojil,10.1109/ACCESS.2021.3112535,2021,"A huge amount of research has been done in the field of speech signal processing in recent years. In particular, there has been increasing interest in the automatic speech recognition (ASR) technology field. ASR began with simple systems that responded to a limited number of sounds and has evolved into sophisticated systems that respond fluently to natural language. This systematic review of automatic speech recognition is provided to help other researchers with the most significant topics published in the last six years. This research will also help in identifying recent major ASR challenges in real-world environments. In addition, it discusses current research gaps in ASR. This review covers articles available in five research databases that were completed according to the preferred reporting items for systematic reviews and meta-analyses (PRISMA) protocol. The search strategy yielded 82 conferences and articles related to the study’s scope for the period 2015–2020. The results presented in this review shed light on research trends in the area of ASR and also suggest new research directions."
Science Mapping of Social Media Analytics in Health Through Artificial Intelligence,R. R. Tobias; R. E. Roxas; M. Abisado,10.1109/TENCON54134.2021.9707362,2021,"This paper presents a systematic literature review and bibliometric analyses of Scopus-indexed documents in social media analytics in health during the CoVid-19 pandemic that used artificial intelligence methodologies. From the 179 extracted Scopus-indexed publications in August 2021, 128 were left after removing 51 documents using the Preferred Reporting Items for Systematic Reviews and Meta-analysis (PRISMA) procedure. Analyses and visualizations using VOSviewer reveal research productivity, affiliation and collaboration networks, and the corresponding relationship between research productivity and the research networks. Conclusions and recommendations for future work are presented to further nurture the current research environment of social media analytics through artificial intelligence methodologies."
Empirical Investigation of Influencing Factors Regarding Offshore Outsourcing Decision of Application Maintenance,H. U. Rahman; M. Raza; P. Afsar; H. U. Khan,10.1109/ACCESS.2021.3073315,2021,"Global Software Development (GSD) has been an emerging trend in the development of software globally, for the last two decades. Information Technology (IT) outsourcing includes application development, application maintenance, infrastructure management and business process outsourcing. Software maintenance aims to keep the IT system operational and to fulfill the client requirements. The maintenance is considered the longest phase of software life cycle that consumes about 60-70% of the total software budget. Maintenance of software is not only time consuming but also requires a significant human resources' ratio. Mostly, software acquisition and maintenance consume a big portion of the total IT budget. The current study aims to evaluate the findings of the systematic literature review and to derive a list of critical success factors regarding offshore outsourcing decision of application maintenance. Thus, an empirical study is performed to validate the influencing factors that were identified by using systematic literature review. These factors are further validated by 93 outsourcing experts from 30 different countries. The collected data through online survey is analyzed based on variables such as respondents experience level, respondents' locations (continents), experts' positions. Similarly, the data is analysed based on Chi square test (linear by linear association) and Spearman Rank Correlation. Additionally, the identified factors through survey and systematic literature review are ranked by two different methods. Consequently, a project assessment model is proposed, based on the critical success factors for the sourcing decision of application maintenance."
Pros and Cons of Artificial Intelligence–ChatGPT Adoption in Education Settings: A Literature Review and Future Research Agendas,I. Maita; S. Saide; A. M. Putri; D. Muwardi,10.1109/EMR.2024.3394540,2024,"The integration of artificial intelligence, particularly ChatGPT, in education presents both promising opportunities and notable challenges. Through a systematic review employing the PRISMA method, this article analyzed 45 references published of ChatGPTs impact on educational environments. While ChatGPT offers teachers a versatile learning tool, aiding in tasks, such as lesson planning and content generation, concerns regarding academic integrity and over-reliance on technology have emerged. Ethical considerations, including the potential for cheating in assignments and exams, highlight the need for clear guidelines and ethical frameworks to govern its use. Institutions or related organizations must address issues, such as plagiarism and data privacy, to ensure responsible integration of ChatGPT. Nurturing a growth mindset among educators and learners is crucial to effectively navigate ChatGPT integration. By aligning strategies to leverage ChatGPTs’ capabilities while mitigating risks, educators, institutions, and policymakers can enhance the quality of education in an evolving technological landscape. This article contributes to a deeper understanding of ChatGPTs’ implications in education, providing insights into its advantages and challenges. Informed decision making and proactive measures are essential to harness ChatGPTs potential for transformative impact while safeguarding educational integrity and ethics."
The Working Principle of Artificial Intelligence in Video Games,V. Levytskyi; M. Tsiutsiura; A. Yerukaiev; N. Rusan; T. Li,10.1109/SIST58284.2023.10223491,2023,"In this paper, we delve into the concept of artificial intelligence (AI) in video games and its working principles. Our research aims to explore the various ways in which AI is implemented in games to create non-playable characters (NPCs) that simulate intelligence and entertain the player. We also investigate the different approaches used by game developers in creating AI and how they strive to make NPCs adapt to different situations and change their behavior accordingly. Furthermore, we analyze the limitations of game AI in terms of creativity and problem-solving abilities, which are still far from achieving human-like intelligence. However, we argue that despite these limitations, AI in games is an essential aspect of game development as it adds depth, immersion, and replayability to a game. To achieve our research objectives, we conducted a comprehensive review of existing literature on the topic, including academic papers, industry reports, and online sources. We also examined various case studies of popular games that employ AI to enhance the gaming experience."
Systematic Review of the published Explainable Educational Recommendation Systems,I. Pesovski; A. M. Bogdanova; V. Trajkovik,10.1109/ITHET56107.2022.10032029,2022,"The goal of this paper is to systematically review the available literature on the topic of explainable recommendation systems in education, especially when recommendation systems are integrated as a part of learning management systems (LMS). The focus years for analyzing available literature are the years between 2010 and 2022, period when online learning is expanding and online learning platforms are continuously being developed, which makes these years relevant for scoping this review. The topic of interest in this research are recommendation algorithms whose results can be explained and interpreted. The first part of the methodology used in the paper utilizes an NLP-powered toolkit that automates a big part of the review process by automatically analyzing articles indexed in the IEEE Xplore, PubMed, Springer, Elsevier and MDPI digital libraries. The toolkit relies on the PRISMA methodology for standardizing systematic reviews. First, a quantitative analysis of all available literature is performed, followed by a qualitative analysis of the few selected articles which indeed focus on the explainability when implementing recommendation systems in educational context. The relevant articles are analyzed in detail and compared on multiple indicators like the field of work, tools and techniques used, and how explainability is achieved. The results show that although the amount of available research is growing and new learning management systems are being developed at a fast pace in the last few years, the explainability of the machine learning techniques used in the recommendation systems is not a popular topic among the researchers and developers with research interest in educational context. The amount of the available literature for explainable recommendation systems in educational environment is scarce, but is expected to grow following the global trend of explainable artificial intelligence $(\mathrm{x}\mathrm{A}\mathrm{I})$ as key technique for practical implementation of advanced AI models."
Enhancing Project-Based Learning in Engineering Disciplines Through Emerging Technologies: A Systematic Literature Review,M. G. Ntunka; R. W. Maladzhi; N. H. Mthombeni; F. Nemavhola,10.1109/WEEF-GEDC63419.2024.10854923,2024,"Project-based learning (PBL) is a widely recognized pedagogical approach that fosters active learning and problem-solving skills among engineering students. Over the past few decades, PBL has brought a significant transformation in engineering education by becoming a core methodology for moving toward engineering programs that support student-centered learning. PBL has shifted the focus of engineering education from being teacher-centered to student-centered. Instead of simply imparting knowledge to students, PBL encourages students to engage in active, collaborative learning through real-world projects. In addition, PBL methods place a strong emphasis on practical skills development, as students work on projects that simulate real-world engineering problems. This helps to ensure that engineering graduates are equipped with the skills they need to succeed in the workforce.The study aims to investigate how emerging technologies could play a pivotal role in enhancing project-based learning (PBL) within the field of engineering. Emerging technologies such as Virtual Reality (VR), Augmented Reality (AR), Internet of Things (IoT), Artificial Intelligence (AI), Machine Learning (ML), Robotics, Automation, Collaborative Tools, and Cloud Computing can further enrich the PBL experiences.This literature review-based research examines the role of emerging technologies in enhancing PBL experiences within engineering education. By synthesizing research findings from various disciplines, including education, engineering, and technology, this review identifies key trends, challenges, and opportunities associated with integrating emerging technologies into PBL curricula. The review explores how technologies such as virtual reality, augmented reality, simulation tools, and collaborative platforms can enrich PBL environments by providing immersive, interactive, and engaging learning experiences. Additionally, it discusses strategies for effectively incorporating these technologies into engineering courses, addressing issues related to accessibility, affordability, and pedagogical alignment.Through an analysis of empirical studies, theoretical frameworks, and best practices, this review offers insights into the potential benefits of leveraging emerging technologies to optimize PBL implementation in engineering education. Moreover, it highlights the need for further research to explore the long-term impact and scalability of these technological interventions in enhancing student learning outcomes, fostering innovation, and preparing future engineers for real-world challenges."
Exploring the Impact of Data Mining Techniques in Healthcare and Medical Data: A Systematic Literature Review,F. S. Dizayee; Z. N. Ismael; S. T. Bilal; B. H. Hussein,10.1109/IEC57380.2023.10438817,2023,"This systematic literature review demonstrates the impact of data mining techniques on patients’ medical data science and healthcare treatment services. Identifying data mining techniques in medical science has faced many challenges because of the large volume of data collected daily, especially during the development and implementation of Electronic Health Records. Choosing a suitable technique for medical data is a complex process as each tool provides a different rate of success. This systematic literature review aims to examine the techniques of data mining has been applied in healthcare management systems. The research method used in this study is structured according to PRISMA guidelines. It illustrates key elements of data mining in medical data sciences and healthcare, supporting health professionals in diagnosing diseases, cost reduction and future organization trends. Also describes the benefits of using data mining in disease diagnoses and how treatments of patients have been improved. This study has contributed to the existing knowledge body of data mining techniques in the healthcare field. Furthermore, it identifies the most appropriate techniques that have been used and provides an assessment of current evidence based on discoveries of various recent studies. The Regression Model in Machine Learning Algorithms is an appropriate tool, a more accurate technique for extracting knowledge accumulation and predicting disease diagnoses in the medical field. The Support Vector Machine is used broadly in healthcare data classification because the output is binary choices in diabetes prediction, which supports disease diagnoses at the early stages of the symptoms."
DESIS and PRISMA: A study of a new generation of spaceborne hyperspectral sensors in the study of world crops,I. Aneece; P. S. Thenkabail,10.1109/IGARSS47720.2021.9553718,2021,"Advances in agricultural mapping and characterization are now possible through 1) new generation spaceborne hyperspectral sensors, 2) cloud computing platforms, and 3) advanced machine learning algorithms. The German Aerospace Center (DLR) Earth Sensing Imaging Spectrometer (DESIS) and the Italian Space Agency�s PRecursore IperSpettrale della Missione Applicative (PRISMA) provide unprecedented high-quality hyperspectral data. While their numbers of bands, spatial resolutions, and signal to noise ratios are comparable, important differences between the two may affect classification accuracy. DESIS is mounted on the International Space Station while PRISMA is polar-orbiting. DESIS has a higher spectral resolution (2.55 nm vs 10 nm), but a shorter spectral range (400-1000 nm vs 400-2505 nm). The challenges of using such high data volumes are ameliorated by cloud computing platforms such as Amazon Web Services, Microsoft's Azure, and Google Earth Engine (GEE), which allow users to access and process data stored in the cloud using the platform�s computing power. Lastly, recent advances in machine learning algorithms lead to higher classification accuracies even in such complex systems as agriculture. We demonstrate agricultural crop type classification using PRISMA and DESIS data in GEE using different machine learning algorithms. We compared classification accuracies using a DESIS image acquired on June 18, 2020 and a PRISMA image acquired on June 17, 2020 in the Central Valley, California. In GEE, we trained several machine learning models (including Support Vector Machine and Random Forest) using the USDA Cropland Data Layer as reference. Results of these analyses are presented here."
Applications Of Artificial Intelligence In Firefighting Management Systems: A Bibliometric Review,R. -N. Boştinaru; N. Bizon; S. Dragusin; F. M. Enescu,10.1109/ECAI61503.2024.10607495,2024,"In the paper, the last generation technologies in the field of Smart fire management systems are debated. Paper presents fire prevention and control technologies in areas such as: forestry areas, cities, buildings and in transport. The processed data are extracted from databases such as Google, WOS and Scopus. The paper presents the advantages and disadvantages of the technologies used in fire prevention and control, projects and architectures. The work also analyzes, using the Prisma Smart fire management systems method, the summary of graphs revealed in the field of interest. The conclusions of the work are that it is a vast and intensively researched field in continuous expansion and remodeling in trend with the latest technologies. The work is composed of three chapters, so chapter 1 presents the analyzed technologies, chapter 2 presents the Prisma analysis and chapter 3 presents the conclusions."
Neurotechnology in Gaming: A Systematic Review of Visual Evoked Potential-Based Brain-Computer Interfaces,A. Keutayeva; C. Jesse Nwachukwu; M. Alaran; Z. Otarbay; B. Abibullaev,10.1109/ACCESS.2025.3564328,2025,"Brain-computer interfaces (BCIs) have received considerable attention in gaming, enabling innovative interactions with digital environments. Visual Evoked Potentials (VEPs)—robust, noninvasive neural responses to visual stimuli—offer high information transfer rates, making them particularly promising. This systematic review, guided by the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework, examines VEP-based BCIs in gaming. We searched the Web of Science and Google Scholar, identifying 16 347 studies from the past decade, with 46 selected for in-depth analysis after rigorous screening. The review explores VEP response modeling, electroencephalography (EEG) signal acquisition and processing, stimulation paradigms, and their gaming applications. These systems enhance accessibility for players with physical or cognitive impairments, support adaptive difficulty scaling, personalize gameplay, aid neurorehabilitation, and enable multiplayer interactions. However, challenges remain, including technical limitations, complex data interpretation, user adaptability, and ergonomic issues. Advances in signal processing, personalized calibration, and hybrid multimodal approaches could improve usability. Future research should focus on integrating VEP-based BCIs with emerging technologies, optimizing user comfort, and developing adaptive interaction models to enhance immersion and accessibility. By addressing these challenges and utilizing neuroscience and computational advancements, VEP-based BCIs promise to transform gaming into a more inclusive and immersive experience for diverse users."
Optimizing ERP Deployment with Intelligent Tutoring Systems,S. F. Wijaya; J. Wiratama; R. I. Desanti,10.1109/ICAIIC64266.2025.10920665,2025,"The rapid digital transformation in industries has made Artificial Intelligence (AI) an essential tool for improving the accuracy and speed of strategic decision-making, ultimately enhancing company performance. Using Enterprise Resource Planning (ERP) systems is crucial and necessary for automating business operations toward digitalization. However, many companies still face challenges with technological innovation, leading to ERP implementation failures. While previous research has addressed Critical Success Factors (CSF) for ERP success, further empirical studies are needed, particularly from a technological adoption perspective, emphasizing the role of Intelligent Tutoring Systems (ITS). ITS can analyze data and predictive capabilities to boost productivity and support faster, more accurate decision-making for management. This research employed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA), conducted experimental testing with respondents, and processed data using the Smart-PLS method to identify critical factors. This research results in developing an intelligent tutoring systems model to measure and provide recommendations for improvement to achieve successful ERP implementation in a company."
Visual Analytics for Systematic Reviews According to PRISMA,L. B. Sina; K. Nazemi,10.1109/IV56949.2022.00059,2022,"Systematic reviews play an essential role in various disciplines. Particularly, in biomedical sciences, systematic reviews according to a predefined schema and protocol are how related literature is analyzed. Although a protocol-based systematic review is replicable and provides the required information to reproduce each step and refine them, such a systematic review is time-consuming and may get complex. To face this challenge, automatic methods can be applied that support researchers in their systematic analysis process. The combination of artificial intelligence for automatic information extraction from scientific literature with interactive visualizations as a Visual Analytics system can lead to sophisticated analysis and protocoling of the review process. We introduce in this paper a novel Visual Analytics approach and system that enables researchers to visually search and explore scientific publications and generate a protocol based on the PRISMA protocol and the PRISMA statement."
Navigating Class Disparity: A Systematic Literature Review of Credit Risk Prediction,M. A. Al-Romaihi; A. I. Alalawi,10.1109/ICETSIS61505.2024.10459528,2024,"Purpose: The purpose of this paper is to systematically review of the current research on the topic of credit risk prediction. The aim is to identify the models commonly used for predicting credit risk and to understand the types of variables incorporated into these models. The review will also explore the methodologies that have been employed to tackle the issue of class imbalance, a significant challenge that can affect the performance and reliability of prediction models. Furthermore, this study is aimed to examine the different approaches used to interpret the models' outcomes. This will contribute significant insights to the academic community, financial professionals, and policymakers. Design/methodology/approach: The PRISMA approach is used to guide the review process. Findings: The results of the systematic review present a variety of models for predicting credit risk, with the CatBoost and Random Forests models combining to exhibit the best results. The findings underline the challenge of dealing with imbalanced datasets in machine learning and present a number of solutions. The current review highlights that one-class classifiers might be the most effective way to address class imbalance. The study additionally emphasizes how crucial interpretability is to enhance the understanding of prediction and improve model performance. Originality/value: To the authors' best knowledge, this is likely the first work that suggests the use of one-class classifiers to address the issue of class imbalance in data derived from real-life scenarios like credit risk and fraudulent activity."
A Review of Ethical Considerations in Using Artificial Intelligence in E-Learning,L. Homayouni; Y. Hejazi; N. Zarifsanaiey,10.1109/ICeLeT62507.2024.10493100,2024,"The implementation of artificial intelligence in education (AIED) is in the spotlight due to being the most substantial advancement of the century. But, despite being beneficial to e-learning, AI has imposed ethical risks and challenges that need to be addressed. Hence, we aim to present a literature review concerning ethical considerations in using AI in e-learning. In this regard, we restored all relevant articles from 5 databases (Google Scholar, Scopus, PubMed, Web of science and Eric) based on PRISMA guidelines and predefined inclusion criteria, from January 2013 till June 2023. Each reviewer conducted the process of selecting studies and extracting data, which was then synthesized using a narrative method. Initially, 169 articles were identified through the search, with 38 articles meeting the predetermined inclusion criteria for data analysis. The majority of these articles centered on a broad discourse concerning ethics and artificial intelligence. Nevertheless, in most retrieved studies, there was a limited examination of ethical considerations regarding using AI in e-learning. Among them, respecting human rights, privacy, equity and fairness, trust and accountability, transparency; enhancing learning outcomes through surveillance of learners' performance and their learning process; providing equal opportunities to have access to technologies, and avoiding any discrimination and bias were the main ethical considerations regarding using AI in e-learning. We have explained a set of general principles and ethical considerations related to the application of AI in e-learning that can assist in designing new autonomous systems and also help to consider ethical concerns when integrating AI in online educational settings. More research, however, are needed to secure the safety and appropriateness of these systems for the students, teachers, institutions, and society altogether."
The Role of Intelligent Technologies in Early Detection of Autism Spectrum Disorder (ASD): A Scoping Review,M. Kohli; A. K. Kar; S. Sinha,10.1109/ACCESS.2022.3208587,2022,"Background: Two-year delay is reported between the first developmental concern raised by the parents and the diagnosis of ASD (Autism Spectrum Disorder), delaying the start of early intervention programs most beneficial within the first three years. Aim: Evaluate the role of technology in ASD detection by answering four research questions analyzing 1) evolution of technology, 2) use of various bio-behavioral data sources, 3) demographic categories, databases, controls, comparators, and assessment instruments, and 4) data collection, processing, and outcomes of the technology-based methods in ASD detection. Methods: Scoping review included behavioral-based ASD screening and diagnostic studies, published between 1st January 2011 to 31st December 2021 in PUBMED, SCOPUS, and IEEE Xplore databases for children under six years. The studies were evaluated using the Critical Appraisal Skills Programm (CASP) and the PRISMA scoping review checklist (PRISMA-ScR). Results: The shortlisted 35 studies were categorized into seven bio-behavioral categories. The review highlighted the extensive use of machine learning (ML) and Deep Learning (DL) to detect infants (as young as 9 to 12 months) at risk of ASD and Other developmental delays (ODD) using multimodal structured and unstructured data. However, the review reported various internal and external validity threats. Conclusion: Technology can significantly improve the current ASD detection process. The validation and adoption of technology can be fast-tracked by 1) designing robust study protocols, 2) executing multi-cultural field trials, 3) standardizing datasets, data quality, and feature engineering methods, 4) recruiting statistically significant participants from ASD, typically developing (TD) and other developmental disorders (ODD) groups to ensure technological generalization, validation, and adoption outside laboratory settings."
Explainable AI for Audio and Visual Affective Computing: A Scoping Review,D. S. Johnson; O. Hakobyan; J. Paletschek; H. Drimalla,10.1109/TAFFC.2024.3505269,2024,"Affective computing often relies on audiovisual data to identify affective states from non-verbal signals, such as facial expressions and vocal cues. Since automatic affect recognition can be used in sensitive applications, such as healthcare and education, it is crucial to understand how models arrive at their decisions. Interpretability of machine learning models is the goal of the emerging research area of Explainable AI (XAI). This scoping review aims to survey the field of audiovisual affective machine learning to identify how XAI is applied in this domain. We first provide an overview of XAI concepts relevant to affective computing. Next, following the recommended PRISMA guidelines, we perform a literature search in the ACM, IEEE, Web of Science and PubMed databases. After systematically reviewing 1190 articles, a final set of 65 papers is included in our analysis. We quantitatively summarize the scope, methods and evaluation of the XAI techniques used in the identified papers. Our findings show encouraging developments for using XAI to explain models in audiovisual affective computing, yet only a limited set of methods are used in the reviewed works. Following a critical discussion, we provide recommendations for incorporating interpretability in future work for affective machine learning."
Utilizing Deep Learning and Advanced Machine Learning Methods in Economic Data Analysis,H. Sharma; A. Garg; R. K. Singhal; M. Priya Gaur; H. Sharma; N. Sharma,10.1109/ACET61898.2024.10730103,2024,"This extensive research explores the most recent developments in data science as they relate to developing nations. It carefully looks at four different categories: In an effort to find the cutting edge of technological innovation, collaborative models, mixed neural architectures, neural networks, and combination machine learning approaches are used. These innovative approaches are applied in a wide range of economic domains, including corporate banking, cryptocurrency, e-commerce, stock markets, and marketing. The study uses the Prisma method, a systematic literature review technique, to guarantee the highest level of rigor and accuracy in its findings. After further examination, a pattern becomes apparent that makes hybrid models the preferred option over conventional learning algorithms. The better performance of these hybrid models highlights a trajectory of rising complexity in the area and points to a trajectory towards more sophisticated hybrid deep learning architectures."
Artificial Intelligence in the IoT Era: A Review of Edge AI Hardware and Software,T. Sipola; J. Alatalo; T. Kokkonen; M. Rantonen,10.23919/FRUCT54823.2022.9770931,2022,"The modern trend of moving artificial intelligence computation near to the origin of data sources has increased the demand for new hardware and software suitable for such environments. We carried out a scoping study to find the current resources used when developing Edge AI applications. Due to the nature of the topic, the research combined scientific sources with product information and software project sources. The paper is structured as follows. In the first part, Edge AI applications are briefly discussed followed by hardware options and finally, the software used to develop AI models is described. There are various hardware products available, and we found as many as possible for this research to identify the best-known manufacturers. We describe the devices in the following categories: artificial intelligence accelerators and processors, field-programmable gate arrays, system-on-a-chip devices, system-on-modules, and full computers from development boards to servers. There seem to be three trends in Edge AI software development: neural network optimization, mobile device software and microcontroller software. We discussed these emerging fields and how the special challenges of low power consumption and machine learning computation are being taken into account. Our findings suggest that the Edge AI ecosystem is currently developing, and it has its own challenges to which vendors and developers are responding."
A Systematic Literature Review of Requirements Volatility Prediction,A. M. Alsalemi; E. -T. Yeoh,10.1109/CTCEEC.2017.8455174,2017,Requirements volatility is a crucial risk factor in software projects as it directly results in cost and time overruns. Accurately predicting requirements volatility is important for better project management. This paper presents a systematic literature review that focuses on the prediction of the requirements volatility. This literature review aims to answer four research questions: 1) how is requirements volatility prediction applied to different software development methods? 2) What are the machine learning algorithms used to predict requirements volatility in software development? 3) What are the attributes (predictors) used to predict requirements volatility in software development? 4) What are the performance metrics for evaluating existing prediction models? This study presents predictors used in the literature and their performances.
“Estimating Software Project Effort Using Analogies”: Reflections After 28 Years,M. Shepperd,10.1109/TSE.2025.3534032,2025,"This invited paper is the result of an invitation to write a retrospective article on a “TSE most influential paper” as part of the journal's 50th anniversary. The objective is to reflect on the progress of software engineering prediction research using the lens of a selected, highly cited research paper and 28 years of hindsight. The paper examines (i) what was achieved, (ii) what has endured and (iii) what could have been done differently with the benefit of retrospection. While many specifics of software project effort prediction have evolved, key methodological issues remain relevant. The original study emphasised empirical validation with benchmarks, out-of-sample testing and data/tool sharing. Four areas for improvement are identified: (i) stronger commitment to Open Science principles, (ii) focus on effect sizes and confidence intervals, (iii) reporting variability alongside typical results and (iv) more rigorous examination of threats to validity."
"A Systematic Review on Multimodal Emotion Recognition: Building Blocks, Current State, Applications, and Challenges",S. Kalateh; L. A. Estrada-Jimenez; S. Nikghadam-Hojjati; J. Barata,10.1109/ACCESS.2024.3430850,2024,"Emotion recognition involves accurately interpreting human emotions from various sources and modalities, including questionnaires, verbal, and physiological signals. With its broad applications in affective computing, computational creativity, human-robot interactions, and market research, the field has seen a surge in interest in recent years. This paper presents a systematic review of multimodal emotion recognition (MER) techniques developed from 2014 to 2024, encompassing verbal, physiological signals, facial, body gesture, and speech as well as emerging methods like sketches emotion recognition. The review explores various emotion models, distinguishing between emotions, feelings, sentiments, and moods, along with human emotional expression, categorized in both artistic and non-verbal ways. It also discusses the background of automated emotion recognition systems and introduces seven criteria for evaluating modalities alongside a current state analysis of MER, drawn from the human-centric perspective of this field. By selecting the PRISMA guidelines and carefully analyzing 45 selected articles, this review provides comprehensive perspectives into existing studies, datasets, technical approaches, identified gaps, and future directions in MER. It also highlights existing challenges and current applications of the MER."
Computer Vision-based Applications in Modern Cars for safety purposes: A Systematic Literature Review,L. Nkuzo; M. Sibiya; E. Markus,10.1109/ICTAS56421.2023.10082722,2023,"Human error, fatigue, and negligence cause the majority of road accidents. Modern automobiles are outfitted with advanced driver assistance systems (ADASs) to help drivers and other vehicle occupants improve safety, enforce the law, and provide comfort. The purpose of this paper is to identify research gaps by highlighting the challenges of computer vision-based application techniques in modern automobiles for safety purposes. This study will also highlight publicly available datasets that can be used for research purposes. As a guideline, the study uses the Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA 2020) protocol. Our study drew on seventy sources of literature, fifty of which focused on modern car applications (Lane, Pedestrian, and Traffici sign detection) and 20 on publicly available datasets. Using search criteria, the literature was mined in Google Scholar and IEEE Explore. The boolean operators and keywords listed below were employed. The inclusion and exclusion criteria used in the study are detailed in Section II. To understand the research gaps between the presented applications and the availability of public datasets, a comparison analysis was performed. Deep learning techniques are more accurate and robust than traditional computer vision techniques, according to the results. The results also show that there are available public datasets. The study, however, was restricted to English papers, lane, pedestrian, and traffic sign applications. Other languages and applications could be future research topics."
A Systematic Review on the Use of Consumer-Based ECG Wearables on Cardiac Health Monitoring,R. Wang; S. C. M. Veera; O. Asan; T. Liao,10.1109/JBHI.2024.3456028,2024,"This systematic review aims to summarize the consumer wearable devices used for collecting ECG signals, explore the models or algorithms employed in diagnosing and preventing heart-related diseases through ECG analysis, and discuss the challenges and future work related to adopting health monitoring using consumer wearable devices. Following the PRISMA method, we identified and reviewed 102 relevant papers from PubMed, IEEE, and Web of Science databases, covering the period from May 2013 to May 2023. This review comprehensively summarizes consumer wearable devices with ECG functions, available ECG datasets, and various algorithms for detecting cardiac diseases and monitoring long-term health. It also discusses the integration challenges and future directions in cardiac health monitoring. The results highlight a preference for deep learning algorithms, such as Convolutional Neural Networks (CNNs) and their variations, in analyzing ECG data due to the ability to automate feature extraction and reduce memory requirements. The review also discusses potential limitations of the current literature, including lack of reasoning and comparison of algorithms and limited data generalizability. By analyzing the current literature, this review provides an overview of state-of-the-art technologies, identifies key findings, and suggests potential avenues for future research and implementation."
MLOps in Data Science Projects: A Review,C. Haertel; D. Staegemann; C. Daase; M. Pohl; A. Nahhas; K. Turowski,10.1109/BigData59044.2023.10386139,2023,"Data Science (DS) has gained increased relevance due to the potential to extract useful insights from data. Quite commonly, this involves the utilization of Machine Learning (ML). The challenging pursuit of developing and productionizing ML models can be supported and automated through MLOps, a specialization of the DevOps paradigm from software development. Therefore, MLOps offers significant potential for DS projects, which are suffering from notable failure rates. Accordingly, this literature review focuses on examining the current state-of-the-art of the publications in this area. Most importantly, the analysis showed that the current MLOps approaches in the literature predominantly emphasize model development and deployment, while organizational aspects (business understanding, evaluation) in a DS project are neglected. As DS project success is not exclusively dependent on technical matters, advancing the MLOps field by bridging the gap between business objectives and the modeling perspective through appropriate frameworks should be pursued in future research."
Expanding the ‘A’ in STEAM: Integrating Poetry and AI for Educational Evolution,T. Kouvara; V. Fotopoulos; C. Karachristos; T. Orphanoudakis,10.1109/EDUCON60312.2024.10578751,2024,"In this systematic literature review spanning the last five years (2018–2023), the role of poetry within the ‘A’ for Arts in STEAM (Science, Technology, Engineering, Arts, Mathematics) education is examined. Specifically, the primary purpose of this study is to delve into the integration of poetry and Artificial Intelligence into the STEAM teaching method. The research objectives which guide this study are to understand the methods and depth of including poetry in STEAM and evaluate how Artificial Intelligence (AI) can enhance poetry's role in STEAM. Using the PRISMA methodology, an extensive search was conducted in key databases such as Scopus and Google Scholar. This search identified 26 crucial papers, then analyzed qualitatively and quantitatively. Current STEAM teaching approaches focus on merging literary expressions with essential scientific disciplines. A large part of the findings points to new teaching methods that bring together poetic elements and the usual STEAM subjects. These methods increase student participation, bridge the gap between arts and sciences and encourage creative problem-solving. In the realm of STEAM education, the significance of Artificial Intelligence (AI) is growing. The findings suggest that AI can boost teaching methods centred on poetry in STEAM. AI allows for lesson adjustments based on students' engagement with poetry, creates platforms for more student participation, and offers feedback to help educators improve their teaching methods. However, there are challenges in merging the exactness of AI algorithms with the complexities of poetry, especially when trying to maintain poetry's essence. As STEAM education moves towards a more integrated model, the interplay between AI's technical abilities and the nuances of poetry calls for further scholarly research based on these findings."
Take Loads Off Your Developers: Automated User Story Generation using Large Language Model,T. Rahman; Y. Zhu; L. Maha; C. Roy; B. Roy; K. Schneider,10.1109/ICSME58944.2024.00082,2024,"Software Maintenance and Evolution (SME) is moving fast with the assistance of artificial intelligence (AI), especially Large Language Models (LLM). Researchers have already started automating various activities of the SME workflow. Un-derstanding the requirements for maintenance and development work i.e. Requirements Engineering (RE) is a crucial phase that kicks off the SME workflow through multiple discussions on a proposed scope of work documented in different forms. The RE phase ends with a list of user stories for each unit task and usually created and tracked on a project management tool such as GitHub, Jira, AzurDev, etc. In this research, we collaborated with Bell Mobility to develop a tool “Geneus” (Generate UserSory) using GPT-4-turbo to automatically create user stories from software requirements documents. Requirements documents are usually long and contain complex information. Since LLMs typically suffer from hallucination when the input is too complex, this paper proposes a new prompting strategy, “Refine and Thought” (RaT), to mitigate that issue and improve the performance of the LLM in prompts with large and noisy contexts. Along with manual evaluation using RUST (Readability, Understandability, Specificity, Technical-aspects) survey questionnaire, automatic evaluation with BERTScore, and AlignScore evaluation metrics are used to evaluate the results of the “Geneus” tool. Results show that our method with RaT performs consistently better in most of the cases of interactions compared to the single-shot baseline method. However, the BERTScore and AlignScore test results are not consistent. In the median case, Geneus performs significantly better in all three interactions (requirements specifi-cation, user story details, and test case specifications) according to AlignScorebut it shows slightly low performance in requirements specifications according to BERTScore. Distilling RE documents requires significant time & effort from the senior members of the team through multiple meetings with stakeholders. We believe automating this process will certainly reduce additional loads off the software engineers and increase the ultimate productivity allowing them to utilize their time on other prioritized tasks."
How Visual Stimuli Evoked P300 is Transforming the Brain–Computer Interface Landscape: A PRISMA Compliant Systematic Review,J. Kalra; P. Mittal; N. Mittal; A. Arora; U. Tewari; A. Chharia; R. Upadhyay; V. Kumar; L. Longo,10.1109/TNSRE.2023.3246588,2023,"Non-invasive Visual Stimuli evoked-EEG-based P300 BCIs have gained immense attention in recent years due to their ability to help patients with disability using BCI-controlled assistive devices and applications. In addition to the medical field, P300 BCI has applications in entertainment, robotics, and education. The current article systematically reviews 147 articles that were published between 2006-2021*. Articles that pass the pre-defined criteria are included in the study. Further, classification based on their primary focus, including article orientation, participants’ age groups, tasks given, databases, the EEG devices used in the studies, classification models, and application domain, is performed. The application-based classification considers a vast horizon, including medical assessment, assistance, diagnosis, applications, robotics, entertainment, etc. The analysis highlights an increasing potential for P300 detection using visual stimuli as a prominent and legitimate research area and demonstrates a significant growth in the research interest in the field of BCI spellers utilizing P300. This expansion was largely driven by the spread of wireless EEG devices, advances in computational intelligence methods, machine learning, neural networks and deep learning."
Prediction and Detection of Cancer Through Machine Learning: A review,N. Saxena; S. S. Yadav; A. Dujawara; A. Kumar,10.1109/ICAC2N63387.2024.10895035,2024,"Expertise in the medical field is growing thanks to artificial intelligence. Free health data has led to the development of ways by experts to help in tumor detection and prediction. For those difficult illnesses, deep learning and machine learning models offer a trustworthy, quick, and efficient solution. Dissertations via Web of Science, EBSCO, and EMBASE that became available between 2009 and 2021 were selected using PRISMA criteria. Leveraging a useful discovery technique, the scientific publications for this investigation that employed AI-based learning algorithms for tumor forecasting were located. A collection of 185 studies demonstrate the substantial influence of neural network-based processors and conventional neural network-based taxonomies on carcinoma forecasting. The survey also looked at earlier studies and pointed out flaws in those studies. Forecasting rate, preciseness, specificity, sensitivity, rolling rating, recognition velocity, territory illustrated accuracy, recall, and F1-score were among the metrics utilized to gauge the results. The five scheduled examinations' remedies have been looked into. Considering the high likelihood of success for multiple of the treatments suggested in studies, the death rate from carcinoma has not dropped. Therefore, more investigation is needed to address the problems with carcinoma forecast."
Predictive Accuracy of Digital Biomarker Technologies for Detection of Mild Cognitive Impairment and Pre-Frailty Amongst Older Adults: A Systematic Review and Meta-Analysis,S. -K. Teh; I. Rawtaer; H. P. Tan,10.1109/JBHI.2022.3185798,2022,"Digital biomarker technologies coupled with predictive models are increasingly applied for early detection of age-related potentially reversible conditions including mild cognitive impairment (MCI) and pre-frailty (PF). We aimed to determine the predictive accuracy of digital biomarker technologies to detect MCI and PF with systematic review and meta-analysis. A computer-assisted search on major academic research databases including IEEE-Xplore was conducted. Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines were adopted for reporting in this study. Summary receiver operating characteristic curve based on random-effect bivariate model was used to evaluate overall sensitivity and specificity for detection of the respective age-related conditions. A total of 43 studies were selected for final systematic review and meta-analysis. 26 studies reported on detection of MCI with sensitivity and specificity of 0.48–1.00 and 0.55–1.00, respectively. On the other hand, there were 17 studies that reported on the detection of PF with reported sensitivity of 0.53–1.00 and specificity of 0.61–1.00. Meta-analysis further revealed pooled sensitivities of 0.84 (95% CI: 0.79–0.88) and 0.82 (95% CI: 0.74–0.88) for in-home detection of MCI and PF, respectively, while pooled specificities were 0.85 (95% CI: 0.80–0.89) and 0.82 (95% CI: 0.75–0.88), respectively. Besides MCI, and PF, in this work during systematic review, we also found one study which reported a sensitivity of 0.93 and a specificity of 0.57 for detection of cognitive frailty (CF). The meta-analytic result, for the first time, quantifies the predictive efficacy of digital biomarker technologies for detection of MCI and PF. Additionally, we found the number of studies for detection of CF to be notably lower, indicating possible research gaps to explore predictive models on digital biomarker technology for detection of CF."
A Review of Trends and Practices in Using Visual Data for Construction-Related Machine Learning Models,A. Mohammadi; S. Golazad; A. Rashidi,10.1109/WSC63780.2024.10838973,2024,"This paper systematically reviews image-based analysis in the construction industry, examining 136 articles through 2023. The findings reveal a marked increase in the use of machine learning (ML), deep learning (DL), and reinforcement learning (RL) models, which utilize image and video data to enhance worker safety, monitor construction progress, and improve project management. The study identifies a significant shift towards integrating real and synthetic data, enhancing model robustness. It also highlights the rising adoption of data-sharing practices, with an increase in publicly available datasets. However, the review highlights underexplored areas such as synthetic data use and advanced privacy-preserving methods. These gaps suggest opportunities for further research to leverage technology more effectively in the construction sector."
The Impact of Digitalization on the Sustainability of the Supply Chain,R. M. Radi; R. Aydin; S. A. Khan,10.1109/ICTMOD63116.2024.10878255,2024,"Digitalization is transforming supply chains by introducing advanced technologies that enhance sustainability. This study assesses the impact of digitalization on supply chain sustainability by identifying and analyzing key factors across environmental, social, economic, and digital dimensions. Using a hybrid methodology-PRISMA for a systematic literature review, Delphi for expert validation, and DEMATEL for analyzing interrelationships among factors-we reveal critical drivers of sustainability. In the environmental dimension, energy efficiency and resource utilization are key drivers, influencing waste management and material recycling. Social factors like safety and automation drive diversity and collaboration, while economic factors such as operational costs and product quality influence customer satisfaction and competitiveness. In the digital dimension, data privacy and real-time monitoring drive database scalability. Our findings highlight the role of IoT, blockchain, AI, and cloud computing in optimizing resource use, enhancing transparency, and improving operational efficiency. Based on these insights, we develop a comprehensive framework to guide managers in leveraging these technologies to foster more sustainable, resilient, and efficient supply chains. This research contributes new empirical evidence on the relationships among factors influencing sustainability and offers practical recommendations for aligning digital transformation with long-term sustainability goals."
Enhancing Data Science Education: A Systematic Review of Gamified Learning Platforms for Decision-Making Skills Development,T. Selvakumar; C. Rajapakse; N. Jayalath,10.1109/ICAC64487.2024.10850893,2024,"Data science is rapidly growing and impacting nearly every industry and research domain. As demand for skilled data scientists rises, data science education must prepare students not only with technical proficiency but also with the ability to make informed, data-driven decisions. Understanding the consequences of their decisions is crucial for data scientists. However, many data science students struggle to develop decision making skills due to insufficient practical experience and lack of exposure to real-world scenarios in their training. To address this gap, integrating realistic, interactive learning environments into educational platforms is essential. This paper presents a systematic literature review, conducted in accordance with PRISMA guidelines, to explore notable features of learning environments that enhance practical learning in data science education. The review identifies a significant need for platforms that enable students to engage with scenarios replicating real world decision-making complexities. Evidence from fields like business, healthcare, and engineering shows that gamification can enhance decision-making skills, suggesting similar approaches could be effective in data science education. By analyzing existing literature, this paper demonstrates the potential of gamified platforms to bridge the gap between theoretical knowledge and practical application, better preparing future data scientists with technical skills and informed decision-making abilities."
Sentiment Analysis for Malay Language: Systematic Literature Review,D. Handayani; N. S. Awang Abu Bakar; H. Yaacob; M. A. Abuzaraida,10.1109/ICT4M.2018.00063,2018,"Recent research and developments in Sentiment Analysis (SA) have simplified sentiment detection and classification from textual content. The related domains for these studies are diverse and comprise fields such as tourism, costumer review, finance, software engineering, speech conversation, social media content, news and so on. SA research and developments field have been done on various languages such as Chinese and English language. However, SA research on other languages such as Malay language is still scarce. Thus, there is a need for constructing SA research specifically for Malay language. To understand trends and to support practitioners and researchers with comprehension information with regard to SA for Malay language, this study exhibit to review published articles on SA for Malay language. From five online databases including ACM, Emerald insight, IEEE Xplore, Science Direct, and Scopus, 2433 scientific articles were obtained. Moreover, through the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) Statement, 10 articles have been chosen for the review process. Those articles have been reviewed depend on a few categories consisting of the aim of the study, SA classification techniques, as well as the domain and source of content. As a result, the conducted systematic literature review shed some light about the starting point to research in term of SA for Malay language."
"AI in Sustainable Construction: Techniques, Impacts and Solutions",T. Dagadkar; S. Dagadkar; A. Barhate; A. Tale; P. Verma; A. Dagadkar,10.1109/IDICAIEI61867.2024.10842895,2024,"Artificial Intelligence (AI) application in sustainable construction is a revolutionary way to improve construction methods and practices, minimize the negative effects on the environment, and manage the resources efficiently. This review presents the responsibilities, advantages and uses of AI in green construction. Based on such technologies as predictive modeling, the design of smart buildings, and efficient management systems of construction projects, the construction sector can decrease CO2 emissions and advance environmental sustainability. In addition, the paper also examines barriers to the applications of AI such as technology constraints, high cost of deployment, and inadequate talent. In conclusion, this paper also discusses how AI has impacted sustainable construction and its implications and further direction for encouraging its application in the construction industry."
Embodied Conversational Agents in Extended Reality: A Systematic Review,F. -C. Yang; P. Acevedo; S. Guo; M. Choi; C. Mousas,10.1109/ACCESS.2025.3566698,2025,"Embodied conversational agents (ECAs) that can interact with users in a human-like manner have demonstrated promising potential in various endeavors. With the ongoing advancement in extended reality (XR) and artificial intelligence (AI), ECAs are becoming increasingly sophisticated. Although previous reviews have predominantly focused on ECAs for non-XR applications, a growing number of research papers are exploring the capabilities of ECAs that utilize XR technologies. However, no prior systematic review has focused explicitly on XR ECAs, leading to a gap in understanding how ECAs are designed, implemented, and evaluated within immersive environments. Our work identified the gap between the existing reviews and the current trends in XR ECAs. We began with 1,717 related papers from January 2014 to June 2024. We narrowed down the selection to 23 papers using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework, which employed an iterative screening procedure and criteria defined by our research team. The resulting papers were analyzed and discussed in terms of the features of the ECA application, its design and implementation, and use cases. Our analysis highlights key trends in XR ECA design, including the dominance of VR-based implementations using head-mounted displays, the prevalence of human-like and female-presenting agents, the move from rule-based to neural-based conversational systems, and the primary use cases in training, therapy, and social interaction. We also summarize the evaluation methods employed across studies and discuss future research directions for developing more adaptive and human-like ECAs in XR environments."
Project Portfolio Management Studies Based on Machine Learning and Critical Success Factors,A. H. Marchinares; I. Aguilar-Alonso,10.1109/PIC50277.2020.9350787,2020,"Project Portfolio Management is very important for the growth of companies, because it favors to plan several possibilities in each scenario. The purpose of the Project Portfolio Management is to manage all resources in order to plan and execute successful projects and achieve the strategic objectives of the organizations. In the Project Portfolio Management, a lot of data is generated daily, which is important for the planning of new projects in companies; consequently, this need arises to create models that help to process and interpret this data. In this context, Machine Learning as an expression of Artificial Intelligence, is presented as an alternative and technological enabler that allows a system, by itself and in an automated way, to learn to discover patterns, trends and relationships in data, it is presented as an engine of digital transformation of business, which is being adopted by many organizations and its demand is growing. Therefore, this paper aims to compile and review the proposals made for the implementation of Machine Learning and critical success factors to improve Project Management, based on a literature review and an analysis of the current state of the art of Machine Learning. 122 articles were found and 21 articles were selected that are related to the research questions. As a final result, 7 ML methods and 18 critical success factors for PPM have been identified."
Digital assets for project-based studies and data-driven project management,G. J. Miller,10.15439/2020F94,2020,"Projects offer learning opportunities and digital data that can be analyzed through a multitude of theoretical lenses. They are key vehicles for economic and social action, and they are also a primary source of innovation, research, and organizational change. This research involves a survey of digital assets available through a project; specifically, it identifies sources of data that can be used for practicing data-driven, context-specific project management, or for project-based academic research. It identified four categories of data sources – communications, reports/records, model representations, and computer systems – and 51 digital assets. The list of digital assets can be inputs in the creation of project artifacts and sources for monitoring and controlling project activities and for sensemaking in retrospectives or lessons learned. Moreover, this categorization is useful for decision support and artificial intelligence systems model development that requires real-world data."
Autonomic Computing Based Respiratory Disorders Assessment Using Speech Parameters: A Systematic Review,P. Shrivastava; N. Tripathi; B. K. Dewangan; B. K. Singh; T. Choudhury; K. Kotecha; S. Dewangan,10.1109/ISMSIT58785.2023.10304972,2023,"Respiratory disorders (RD) are common health problems, and they represent a huge health burden to people around the world. Nowadays, speech analysis is a very important research area in the biomedical field. Therefore, speech analysis can be a useful tool for pre-diagnosing of RD. This review paper aimed to select the features of speech affected by RD and to define autonomic computing in terms of self-care and self-management in disease assessment using the machine learning (ML) paradigm. Selected articles in this work have been published in the last decade and are written in the English language.722 articles in total were obtained, of which 15 were chosen based on the meta-analyses (PRISMA) approach, and for systematic literature reviews (SLR), the reporting items were preferred. Articles were compiled by searching different databases such as Google Scholar, Crossref, and PubMed. This SLR mainly contributed to identifying the role of autonomic computing in the application of self-management for the assessment of RD in ML classifiers. It was observed that most of the research work on respiratory disorders with self-management applying speech analysis was done during the year 2012. Then gradually less work was done in this area until 2018 and in 2019, it increased again and stayed finally stable until 2022. It was concluded that the important information provided by this SLR is efficient model to detect and characterize RDs by applying speech parameters using autonomic computing."
Project Zone : An Advanced Undergraduate Project Management System For Software Development,T. N. E. Amarasekara; H. G. P. Isurindi; E. H. D. T. D. Navanjana; O. M. Gamage; U. Samarakoon; A. Kugathasan,10.1109/ICter53630.2021.9774820,2021,"Project Management System important in largescale projects. There are existing project management tools such as Redmine, Microsoft Project, Jira. Most of the existing project management systems only configured for general purposes such as Project Management, Task management, Time line management. None of them capable of generating project groups, track student progress or track client meetings. These functions are very helpful in tracking project progress as well as individual member progress. Hence, the purpose of this research is to introduce set of new features to a project management system with accurate and effective project management capabilities. This system is capable of generating project groups using student’s skills, Grade point average (GPA). Smart project tracking system where it uses project repositories and system generated timeline for the project. Using the project tracking system, Supervisors can manage groups remotely. Automatic peer review also added to identify each student’s contribution to the project and finally a client portal where clients can request solutions for their requirements and students can use them as their module project. Client meetings will be tracked using voice-to-text algorithm and also emotional recognition where it will identify client’s satisfaction. All These system modules will be used to calculate group performance."
"Development Status, Frontier Hotspots, and Technical Evaluations in the Field of AI Music Composition Since the 21st Century: A Systematic Review",W. Yang; L. Shen; C. -F. Huang; J. Lee; X. Zhao,10.1109/ACCESS.2024.3419050,2024,"In recent years, “Artificial Intelligence (AI)” has become a focal point of discussion. AI music composition, an interdisciplinary field blending computer science and musicology, has emerged as a prominent area of research. Despite rapid advancements in AI music creation technology, there remains a dearth of comprehensive surveys addressing the core technologies within this domain. To address this gap, this study conducted a comprehensive search across multiple databases spanning a 23-year period (2000–2023) on the topic of “AI music composition.” Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) standard for literature screening, the study systematically organized the development status, frontier hotspots, and technical evaluations of the field. Drawing from literature data, the study verified Price’s Law, Lotka’s Law, and Bradford’s Law—three scientific productivity laws—while summarizing the current landscape from four perspectives: authors, organizations, countries, and journals. Subsequently, utilizing VOSviewer and CiteSpace, two technical software tools, the study conducted an in-depth analysis consisting of four steps: clustering, time zone, burst words, and high-frequency referenced literature. The study presented the evolution trajectory of frontiers and hotspots through visualization maps. Finally, building upon quantitative statistical insights, the study qualitatively expanded research efforts by organizing and evaluating the latest AI music generation algorithm technologies. The systematic literature analysis, both quantitative and qualitative, aims to furnish researchers and practitioners in related fields with systematic references."
Collabcrew — An intelligent tool for dynamic task allocation within a software development team,S. Samath; D. Udalagama; H. Kurukulasooriya; D. Premarathne; S. Thelijjagoda,10.1109/SKIMA.2017.8294131,2017,"Currently in the IT industry, the people factor has become very critical when determining the quality of a software project. It is highly important that the correct person performs the relevant task and proper human resource allocation happens within the software project team to obtain successful outcome. This often needs critical thinking, regular team meetings and discussions. Typically a software project manager needs to be highly experienced with the team for this purpose and can be really complex and time consuming with the limited project schedules. This research work introduces a task management tool - CollabCrew specially designed for the software development teams which dynamically allocate tasks based on the skills and previous work done by the team members. This uses historical data from its' own repository or from an external source to find useful information of the previous work done by the project team members to automatically allocate them for new tasks. This proposed system will be containing an Extract, Transform and Load (ETL) tool which will extract data from different data sources, a prediction model to predict the aptness of each team member for a given task and a peer review mining and summarization component to provide a viable way to extract features from peer reviews. Then based on the result, the task allocation component will do the allocations in the most optimal and the feasible way for the project. Even though there are several commercially available task management tools, none has an intelligent component to automatically delegate work within the team. The scope of this work extends beyond the IT domain and a similar procedure can be adopted to develop a task allocation framework in other fields as well."
State of Art: Climate and Wave Monitoring Tools,P. N. Banegas-Dubón; M. Cardona; M. E. Perdomo-Perdomo,10.1109/ICMLANT59547.2023.10372972,2023,"The climate and wave conditions are influenced by various meteorological variables that have an impact on economic, social, and human activities around the world. The study of waves plays an important role in understanding climatic conditions due to the exchange of heat and energy between the atmosphere and the ocean. The purpose of this research is to determine the tools and technologies implemented for climate and wave monitoring. The methodology used in this study is the PRISMA methodology, which can be used in systematic reviews of any kind to ensure transparency in research. It was determined that the most commonly used monitoring tools are remote sensing, artificial intelligence, and in situ monitoring. The technological components of each tool were found to belong to remote sensing, with satellites; artificial intelligence, including neural networks and machine learning, and in the case of in situ monitoring, weather stations. Furthermore, significant sectors affected by climate and wave conditions were identified, with the agricultural sector being the primary one, along with the energy sector and coastal areas. Last but not least, the most studied climate elements were established, resulting in wave conditions, rainfall, drought, radiation, and wind. Finally, the situation of Honduras regarding climate and wave monitoring was analyzed, revealing that the country does not have a favorable position, with a severely limited availability of monitoring tools."
Systematic Literature Review: Small Medium Enterprises (SMEs) Towards Industry 4.0 and Industry 4.0 Maturity Model,M. A. Insani; L. E. Nugroho; W. W. Winarno,10.1109/ICITISEE58992.2023.10404935,2023,"Industry 4.0 can potentially serve as an effective solution for enhancing the productivity of small and mediumsized enterprises (SMEs). However, the implementation of Industry 4.0 poses several challenges for these enterprises. These challenges include the limited availability of financial resources, knowledge resources, and the selection of appropriate technologies. The primary objective of this research paper is to determine the level of existing research that addresses the application of Industry 4.0 in SMEs and its ability to tackle the challenges mentioned above. In addition, this paper aims to explore the concept of the maturity model for Industry 4.0, which is used as one of the methods to guide the journey of SMEs in the transformation to Industry 4.0, as well as explore the various dimensions of the maturity model. To achieve these objectives, the PRISMA methodology has been employed, encompassing a systematic literature review that consists of three stages: identification, screening, and inclusion. The literature searches were conducted on databases such as IEEE Explorer, Scopus, and ACM Library. After a thorough search process, a total of 32 papers were selected, all of which were deemed relevant to the goals and objectives of this study."
Artificial Intelligence in Pregnancy: A Scoping Review,A. M. Oprescu; G. Miró-amarante; L. García-Díaz; L. M. Beltrán; V. E. Rey; M. Romero-Ternero,10.1109/ACCESS.2020.3028333,2020,"Artificial Intelligence has been widely applied to a majority of research areas, including health and medicine. Certain complications or disorders that can appear during pregnancy can endanger the life of both mother and fetus. There is enough scientific literature to support the idea that emotional aspects can be a relevant risk factor in pregnancy (such as anxiety, stress or depression, for instance). This paper presents a scoping review of the scientific literature from the past 12 years (2008-2020) to identify which methodologies, techniques, algorithms and frameworks are used in Artificial Intelligence and Affective Computing for pregnancy health and well-being. The methodology proposed by Arksey and O'Malley, in conjunction with PRISMA-ScR framework has been used to create this review. Despite the relevance that emotional status can have as a risk factor during pregnancy, one of the main findings of this study is that there is still not a significant amount of literature on automatic analysis of emotion. Health enhancement and well-being for pregnant women can be achieved with artificial intelligence or affective computing based devices, hence future work on this topic is strongly suggested."
The Influence of Computer Vision on Road Traffic,M. V. Mulia; H. D. Tirta; S. Achmad; R. Sutoyo,10.1109/ICIMCIS63449.2024.10957360,2024,"Implementing computer vision in road traffic management brings significant impacts, making transportation systerms more efficient, safe, and sustainable. This advanced technology analyzes real-time traffic conditions, providing crucial data for effective traffic management. Using computer vision, authorities can monitor traffic flow, detect incidents, and manage congestion more effectively. The primary aim of this paper is to explore the challenges and impacts associated with using computer vision in road traffic management while offering insights for future research and practical applications. A Systematic Literature Review (SLR) was conducted, adhering to the PRISMA Flowchart Diagram methodology, to identify and evaluate relevant studies in this field. The systematic approach ensures a comprehensive understanding of the current state of computer vision in the transportation sector. The findings from this review underscore the transformative potential of computer vision in enhancing traffic management systems. Moreover, this work highlights several existing challenges that need to be addressed to maximize the benefits of computer vision. These challenges include the need for more advanced algorithms capable of handling diverse and complex traffic scenarios and integrating this technology with existing infrastructure. By identifying these challenges, the paper provides valuable insights for future research, emphasizing the importance of developing more robust and adaptive computer vision systems. Ultimately, this research aims to contribute to advancing computer vision in road traffic management, promoting safer and more efficient transportation systems worldwide."
Using Chatbots to teach English as a foreign language: A systematic literature review from 2010 to 2023,X. Xu; Y. Wang; Z. Shang; L. Jiang; H. Luo,10.1109/ISET61814.2024.00028,2024,"This study conducts a systematic review of the research on the utilization of chatbots for supporting English as a Foreign Language (EFL) instruction in K-12 and higher education settings from 2010 to 2023. The study adheres to the PRISMA methodology guidelines and searches open access empirical studies in three renowned databases: Web of Science, Scopus, and IEEE Xplore. Ultimately, a total of 42 empirical studies are selected as the research sample library for this comprehensive review. The analysis focuses on four interconnected aspects of chatbot-supported EFL teaching: instructional context, instructional design, technical features, and application effects. The findings unveil current research trends and shed light on the teaching environment surrounding chatbots in EFL instruction. Furthermore, this review highlights the technological affordances offered by chatbots while reporting both their effects and limitations based on existing studies. Finally, new avenues for future research in chatbot-based EFL instruction are proposed."
Comparative Analysis of Advancements in Patient Pain Detection using Deep Learning,A. Ghosh; N. Gulati,10.1109/ICEECT61758.2024.10739113,2024,"This review provides a systematic overview of the current state of sentiment analysis as applied to detecting pain from facial expression A key feature of this work is the employment of an analytical framework informed by PRISMA guidelines, Our research consisted of an extensive search of multiple high-profile databases for all articles published within the last five years. In light of selection criteria which rejected articles devoid of any machine or deep learning techniques for gaining knowledge of pain through facial expressions. To select the approximate papers, our evaluation process applied a strict set of inclusion/exclusion criteria. In addition, focus was placed on methodology, Datasets, feature extraction approaches, classification, evaluation metrics, etc. A critical approach was used that compared the advantages and disadvantages of each of the methodologies. However, to achieve the goal of improving patients’ outcome, one must be aware of the threats listed above and would be prepared to apply the suggested innovations to sentiment analysis for pain detection. Ultimately, this approach is likely to help make this tool much more important for healthcare professionals, which would bring more benefits for pain management and patient care."
“Project smells” - Experiences in Analysing the Software Quality of ML Projects with mllint,B. van Oort; L. Cruz; B. Loni; A. van Deursen,10.1145/3510457.3513041,2022,"Machine Learning (ML) projects incur novel challenges in their development and productionisation over traditional software applications, though established principles and best practices in ensuring the project's software quality still apply. While using static analysis to catch code smells has been shown to improve software quality attributes, it is only a small piece of the software quality puzzle, especially in the case of ML projects given their additional challenges and lower degree of Software Engineering (SE) experience in the data scientists that develop them. We introduce the novel concept of project smells which consider deficits in project management as a more holistic perspective on software quality in ML projects. An open-source static analysis tool mllint was also implemented to help detect and mitigate these. Our research evaluates this novel concept of project smells in the industrial context of ING, a global bank and large software- and data-intensive organisation. We also investigate the perceived importance of these project smells for proof-of-concept versus production-ready ML projects, as well as the perceived obstructions and benefits to using static analysis tools such as mllint. Our findings indicate a need for context-aware static analysis tools, that fit the needs of the project at its current stage of development, while requiring minimal configuration effort from the user."
Role of Critical Success Factors in Offshore Quality Requirement Change Management Using SLR,J. Ahmad; A. W. Khan; H. U. Khan,10.1109/ACCESS.2021.3096663,2021,"In software engineering field, requirement change management is a challenging job. Ignoring incoming changes results in customer displeasure. It may also result in late product transportation. Managing requirement changes in poor way is the main cause of product failure. It has more diverse effect in global software outsourcing. In software quality requirement change management, it is necessary to address success factors in order to accomplish the requirements of the customers. In this paper, systematic literature review approach is used for documentation and scrutinization of success factors. Total sixteen success factors were recognized having great impact on quality software requirement change management. Our identified success factors like `Proper Requirement Change Management', `Rapid Delivery', `Quality Software Product, Access to Market', `Project Management', `Skills and Methodologies', `Low Cost/Effort Estimation', `Clear Plan and Road Map', `Agile Processes', `Low Labor Cost', `User Satisfaction', `Communication/Close Coordination', `Proper Scheduling and Time Constraints', `Frequent Technological Changes', `Robust Model', `Geographical juncture/Cultural differences' are the crucial factors that affect software quality requirement change. Company size and different database have been used for the analysis of success factors. The databases/search engine used are Google scholar, Science Direct, IEEE Explore and Springer for the exploration of success factors. Companies are analyzed on the basis of their size such as small, medium and large."
Deep Learning Based Techniques for Breast Cancer Classification: A Systematic Review,C. Elmejgari; Y. NADIR; M. Qbadou,10.1109/IRASET60544.2024.10548784,2024,"Cancer represents a disease characterised by the uncontrolled growth of abnormal cells, often due to genetic mutations and environmental factors. It has the potential to infiltrate surrounding tissue and metastasize to other areas of the body. There are various types of cancer with distinct characteristics, risk factors, and treatment options. Early detection improves survival rates and reduces treatment costs. Advances in imaging technology have facilitated diagnosis, Computer-assisted systems utilize various modalities such as CT scans, MRI, ultrasound, mammography, X-rays, and histopathology to identify abnormalities.Deep learning has shown impressive results in processing large data sets within biomedicine over the past decade. The purpose of this review is to analyze and evaluate a range of pertinent research papers focusing on the application of deep learning methods for cancer detection, following the Prisma methodology. Additionally, we aim to assess current approaches to cancer diagnosis with a particular focus on techniques developed for breast cancer detection."
Extracting Insights from Big Source Code Repositories with Automatic Clustering of Projects by File Names and Types,Y. Yakhno; S. Metin,10.1109/SmartNets58706.2023.10215598,2023,"Software project delivery requires a set of related activities to be conducted. The output of these activities form a collection of unstructured data such as specifications, requirements, manuals, source code and packaging files which is stored in configuration management systems. Software repositories are infrastructures to support project management activities and can be composed with several systems that include code change management, bug tracking, code review, build system, release binaries, wikis, forums, etc. This large and variable data collection provides opportunities for text mining tasks which further can be utilized for software delivery or business intelligence related goals. The proposed approach uses machine learning methods to inspect large software repositories and classifies the results to propose insights to project managers as an aide to make strategic business decisions. In the present work every software project is described by a certain representation (vector or set of vectors), which is constructed from names and types of the files from project content. These representations are used to find clusters of similar projects in big code repositories and highlight specific properties of the found groups: most used names and types of the file."
"A Systematic Review of Technology-Aided Stress Management Systems: Automatic Measurement, Detection and Control",A. A. Jiménez-Ocaña; A. Pantoja; M. A. Valderrama; L. F. Giraldo,10.1109/ACCESS.2023.3325763,2023,"Even though stress response is a defense mechanism of the body to deal with adverse daily situations, prolonged exposure to these effects can trigger significant detriments to physical and mental health. The aim of this systematic review is to identify the use of technological tools in stress management, with a special focus on feedback control systems that include detection, control, and intervention phases. The databases selected for this systematic review, which applies the PRISMA protocol, are Scopus, IEEE Xplore, Web of Science, and Science Direct. We include research works that have experiments involving automated physiological data collection through non-invasive methods and an intervention technique to manage stress. Applying these criteria, a total of 75 articles are included in the final analysis. The quality of the included articles was assessed in the search strategy, the selection process and the data collection process, following the eligibility criteria. Summarizing some results, almost half of the studies included fifty or fewer participants in the experiments and twelve physiological variables were identified, being HR and ECG the most important ones. The most used technique of stress management was breathing and 16 articles used some type of feedback control, mainly biofeedback. Several promising physiological variables and intervention techniques are identified for implementing stress management systems. Although using machine learning in stress detection is common, its application to develop feedback control systems is limited. Moreover, it was found that the theory of control in dynamical systems has not been applied yet to design automatic stress management systems."
Automated Diagnosis of Alzheimer’s Disease Using OCT and OCTA: A Systematic Review,Y. Turkan; F. Boray Tek; F. Arpaci; O. Arslan; D. Toslak; M. Bulut; A. Yaman,10.1109/ACCESS.2024.3434670,2024,"Retinal optical coherence tomography (OCT) and optical coherence tomography angiography (OCTA) have emerged as promising, non-invasive, and cost-effective modalities for the early diagnosis of Alzheimer’s disease (AD). However, a comprehensive review of automated deep learning techniques for diagnosing AD or mild cognitive impairment (MCI) using OCT/OCTA data is lacking. We addressed this gap by conducting a systematic review using the Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) guidelines. We systematically searched databases, including Scopus, PubMed, and Web of Science, and identified 16 important studies from an initial set of 4006 references. We then analyzed these studies through a structured framework, focusing on the key aspects of deep learning workflows for AD/MCI diagnosis using OCT-OCTA. This included dataset curation, model training, and validation methodologies. Our findings indicate a shift towards employing end-to-end deep learning models to directly analyze OCT/OCTA images in diagnosing AD/MCI, moving away from traditional machine learning approaches. However, we identified inconsistencies in the data collection methods across studies, leading to varied outcomes. We emphasize the need for longitudinal studies on early AD and MCI diagnosis, along with further research on interpretability tools to enhance model accuracy and reliability for clinical translation."
Exploring the Ethical Landscape: Safeguarding Data Privacy in the Realm of Virtual Reality,L. S. Tionanda; B. J. Sin; S. Achmad; R. Sutoyo,10.1109/ICIEE63403.2024.10920350,2024,"As VR becomes more immersive and widespread, the fact that it collects vast amounts of personal and sensitive data from users, have raised concerns about how this information is managed and protected. This paper delves into the complex landscape of VR, focusing on the collection, management, and protection of user data. Through a systematic literature review using the PRISMA framework filtered by inclusion and exclusion criteria which are based on our research questions, this research identifies key privacy concerns, evaluates current security measures, and examines the ethical implications of VR usage. Our findings reveal several critical issues: the potential for misuse of personal data, insufficient transparency in data handling practices, and the lack of robust security protocols to safeguard user information. We also explore the ethical dimensions of VR, such as the potential for manipulation of user behavior and the responsibilities of VR companies to protect user welfare. To address these challenges, we recommend the development of comprehensive privacy policies, implementation of more advanced security measures, and establishment of clear ethical guidelines. Additionally, it is essential for VR companies to prioritize privacy and ethic in their products. By prioritizing these aspects, VR industry can create a safer and more trustworthy virtual environment for users which would in turn ensure a secure and ethical future for this technology. This paper aims to provide insights and practical recommendations for policymakers, developers, and researchers, encouraging a collaborative effort to enhance data privacy and ethical standards in the rapidly evolving field of Virtual Reality."
The Integration of Deep Learning Techniques and Big Data Analytics for Improved Breast Cancer Diagnosis and Treatment: A Systematic Review,H. M. Gebre; G. M. Wegari,10.1109/ICT4DA62874.2024.10777243,2024,"Background:- Breast cancer is one of the most common and lethal diseases in the world. Traditional breast cancer diagnostic and prognosis procedures usually involve significant human talent and can be time-consuming and subjective, leading to potential errors and treatment delays. Two recent technical advancements, deep learning, and big data analytics are promising to improve breast cancer diagnosis and therapy. As a result, this systematic review aims to examine specific papers that describe deep learning techniques and big data analytics in the context of breast cancer diagnosis and prediction.Methods: Using Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA), peer-reviewed articles published in the English language from January 2020 to August 2023 were selected from electronic databases such as PubMed, ACM, Digital Library, and Science Direct, as well as citations and manual searches. This review paper takes into account papers about deep learning algorithms, big data analytics, the efficacy of deep learning and big data analytics, as well as the problems and limitations of merging deep learning methods with big data analytics. Articles that were not original or in English were not included.Result: - Ten articles were identified for this review. The finding showed that deep-learning techniques play a great role in analyzing vast datasets to identify malignant cells or tumors, aiding radiologists in accurate diagnoses and improving patient outcomes. Big data analytics in breast cancer diagnosis and treatment can improve accuracy, efficiency, and patient-centered care. Deep learning techniques are utilized with big data, enhancing screening test accuracy and guiding diagnostic procedures, especially for image-based scanning that leads to early breast cancer identification, improving patient outcomes, and potentially enhancing diagnostics. Obtaining huge volumes of high-quality data for training deep learning models is a challenge and limitation of integrating deep learning algorithms with big data analytics; due to data privacy, data fragmentation across multiple healthcare systems, and restricted access to annotated datasets.Conclusion: Deep learning techniques combined with big data analytics have a significant potential for improving breast cancer detection and therapy."
A Review on Machine Learning Methods for Water Quality Prediction,V. L. Dharma; N. K. Nurtanio; F. S. Nugroho; M. S. Anggreainy; A. Kurniawan,10.1109/AiDAS60501.2023.10284659,2023,"Water is an important substance for the human body. Clean water is important for not just the human body, but also for the environment. In this paper, Prisma is used to filter many of the reference paper where the paper left are used for the research. Machine learning is one of many ways to predict water quality. Using algorithms to process large amounts of data and patterns, water quality can be predicted. Prediction on water temperature, pH, and others can be used to find bad quality water and potentially find the cure for it. There are also different kinds of environment in predicting water quality, where depending on the environment certain machine learning method is better than the others. This paper contains machine learning methods, water parameters, and areas of water for water quality prediction."
Computational Techniques for Cardiovascular Diseases Prediction: Systematic Review,I. D. Olusoji; R. M. Alade; O. A. Abiodun; O. O. Olabisi,10.1109/SEB-SDG57117.2023.10124568,2023,"Cardiovascular disease (CVD) occurs when the heart is unable to circulate the appropriate amount of blood to other parts of the body in order to perform normal functions. The purpose of this research is to conduct a systematic review of computational methods for predicting the risk of cardiovascular disease. A systematic search strategies was adopted, as reported by the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) checklist. The articles were chosen from Google Scholar, PubMed, and Web of Science and evaluated purely computational techniques for CVD risk prediction. The inclusion criteria were satisfied by 46 studies: 9 on fuzzy logic or hybrids, 5 on genetic algorithms or hybrids, 21 on machine learning, ensembles, or hybrids, 3 on data mining, 2 on intelligent systems, and 7 on neural networks and their hybridized forms. Research results indicated that better CVD prediction in humans will be assured by the use of computational techniques with higher performance accuracy."
Survey on Predictive Algorithms to Detect Insider Threat on a Network Using Different Combination of Machine Learning Algorithms,F. Femi-Oyewole; V. Osamor; D. Okunbor,10.1109/SEB4SDG60871.2024.10630366,2024,"This study explores the efficacy of predictive algorithms for insider threat detection in organizational networks. Leveraging machine learning and deep learning techniques, the research identifies state-of-the-art models and assesses challenges associated with scalability and imbalanced datasets. Following the Prisma International Standards methodology, 531 articles were initially retrieved, with 59 high-quality articles selected for detailed analysis. The findings reveal a multifaceted approach, with 58.8% proposing new models, 23.5% implementing and evaluating, and 17.6% lacking explicit metrics. SVM and RNNs emerged as frequently used algorithms, reflecting versatility and effectiveness in network traffic analysis for insider threat detection. The study provides insights into current trends, challenges, and potential avenues for future research in the realm of insider threat detection."
Edge Intelligence in Enhancing Last-Mile Delivery Logistics,J. Reis,10.1109/ACCESS.2025.3570894,2025,"The last-mile delivery phase, the final stage where goods move from a distribution center to customers, is pivotal but faces significant inefficiencies and high costs due to its complexity. Recent advancements in Edge AI or Edge Intelligence (EI) offer promising solutions to these challenges. This study explores how AI-driven technologies and real-time data processing, combined with EI, can enhance last-mile delivery operations. A thorough literature review was conducted to assess technological advancements by using PRISMA 2020, and a Delphi method was used to systematically and empirically assess the impact of EI solutions on operational efficiency and customer satisfaction. Although EI technologies offer substantial benefits, EU companies are hesitant to adopt these innovations due to high implementation costs. However, firms that have embraced these technologies report significant improvements, including better route optimization, reduced delivery times, and enhanced service reliability. These findings highlight the need for a culture of innovation and the recruitment of experts with advanced qualifications to drive technological advancement in last-mile logistics. The integration of EI represents a significant step towards more efficient, cost-effective, and customer-focused last-mile delivery solutions. Future research should refine these technologies and explore their long-term impacts on the logistics industry."
A Systematic Review of Bimanual Motor Coordination in Brain-Computer Interface,P. Tantawanich; C. Phunruangsakao; S. -I. Izumi; M. Hayashibe,10.1109/TNSRE.2024.3522168,2025,"Advancements in neuroscience and artificial intelligence are propelling rapid progress in brain-computer interfaces (BCIs). These developments hold significant potential for decoding motion intentions from brain signals, enabling direct control commands without reliance on conventional neural pathways. Growing interest exists in decoding bimanual motor tasks, crucial for activities of daily living. This stems from the need to restore motor function, especially in individuals with deficits. This review aims to summarize neurological advancements in bimanual BCIs, encompassing neuroimaging techniques, experimental paradigms, and analysis algorithms. Thirty-six articles were reviewed, adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. The literature search result revealed diverse experimental paradigms, protocols, and research directions, including enhancing the decoding accuracy, advancing versatile prosthesis robots, and enabling real-time applications. Notably, within BCI studies on bimanual movement coordination, a shared objective is to achieve naturalistic movement and practical applications with neurorehabilitation potential."
Technical use of Smart Contracts in Blockchain-Based Project Management,V. Sharma; O. J. Oyebode; V. Uniyal; M. Rajyalaxmi; M. Al-Taee; M. B. Alazzam,10.1109/ICACITE57410.2023.10182588,2023,"Smart contracts have emerged as a key element in blockchain technology, offering a new paradigm for managing projects and executing transactions in a decentralized manner. This article explores the role of smart contracts in blockchain-based project management, highlighting their potential to streamline processes, reduce costs, and increase transparency and trust among stakeholders. The article analyzes the benefits and limitations of using smart contracts in project management and provides insights into the various use cases and applications of smart contracts, such as decentralized autonomous organizations, supply chain management, and project funding. Additionally, the article discusses the challenges and risks associated with smart contracts, including security, scalability, and regulatory compliance. Overall, this article provides a comprehensive overview of the role of smart contracts in blockchain-based project management and its implications for the future of the industry."
3D Film and Television Animation Production Cloud Management System Based on Artificial Intelligence,W. Shen; C. Chen,10.1109/CSNT60213.2024.10545896,2024,"3D film and television animation production is a complex and time-consuming task. Traditional production methods have low efficiency and quality control problems. In order to solve these problems, an artificial intelligence-based 3D film and television animation production cloud management system came into being. This study adopted a comprehensive research methodology, including literature review, user research, and field testing. First of all, this study conducts an in-depth study of relevant literature, and understands the current situation and problems of 3D film and television animation production. Secondly, this study conducted a field test on the artificial intelligence-based 3D film and television animation production cloud management system, and collected user feedback and opinions. The research results show that the artificial intelligence-based cloud management system for 3D film and television animation production has a positive impact on user experience. The system improves production efficiency and production quality through intelligent auxiliary tools and automatic functions. User feedback shows that the real-time preview and feedback function of the system enables users to adjust the work in time and obtain more detailed and realistic effects. In addition, the system's interface design and personalized customization options are also appreciated by users, which improves the user's work comfort and satisfaction."
Comprehensive Review and Meta-Analysis of Machine Learning Applications in Screening for Diabetic Retinopathy Analysis,A. T. Siddiqui; H. Kaur; S. Naaz; S. Tanveer,10.1109/SCEECS61402.2024.10482064,2024,"The leading cause: diabetic retinopathy global blindness, affects 10% to 24% of individuals with type 1 or type 2 diabetes in primary care. Early detection using deep learning methods is critical for timely intervention and preserving vision. Our comprehensive review, including a meta-analysis, evaluates the effectiveness of these algorithms in DR detection. This study extensively assessed machine learning’s diagnostic accuracy in identifying diabetic retinopathy across diverse cases using color fundus images, aiming to pinpoint the most advanced ML strategy available. We extensively reviewed relevant literature from January 2015 to December 2022, utilizing EMBASE, PubMed, Google Scholar, and Scopus. Following PRISMA guidelines, we focused on machine learning-based study designs. Two authors independently assessed articles for inclusion based on predefined criteria, and data were collected using a standardized form. The meta-analysis reveals strong performance of machine learning in detecting diabetic retinopathy in color fundus photos, indicating readiness for clinical use. However, caution is advised due to methodological limitations in some earlier studies, such as lack of external validation and potential biases in participant selection"
Electric Power Project Correlation Mining and Question Answering: An Unsupervised Method,P. Tian; X. Dai; Y. Huo; Y. Wu; X. Gan; Y. Xiao; J. Han; S. Li,10.1109/ACFPE63443.2024.10801066,2024,"With rapid growth in the number of project applications in the electric power field, duplicate projects and cross-applications occur repeatedly, wasting financial resources. The traditional duplicate check methods and human review are increasingly unable to handle the problem. Companies need methods to discover project correlation and answer project content-related questions to help them review the declared projects. The emergence of large language models (LLMs) provides new ideas for understanding the projects’ content and mining project relations. However, the current LLM-based methods mainly focus on general knowledge and lack the knowledge of project review. This work analyzes the characteristics of project text in the electric power field and proposes an unsupervised project correlation mining and question-answering method based on LLMs. Then, we test the proposed method for the project review in the electric power field. Experiments show that this method can precisely discover the project correlations and is beneficial to improving the efficiency of project review in the electric power field."
Privacy Threats and Countermeasures in Federated Learning for Internet of Things: A Systematic Review,A. ElZemity; B. Arief,10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics62450.2024.00072,2024,"Federated Learning (FL) in the Internet of Things (IoT) environments can enhance machine learning by utilising decentralised data, but at the same time, it might introduce significant privacy and security concerns due to the constrained nature of IoT devices. This represents a research challenge that we aim to address in this paper. We systematically analysed recent literature to identify privacy threats in FL within IoT environments, and evaluate the defensive measures that can be employed to mitigate these threats. Using a Systematic Literature Review (SLR) approach, we searched five publication databases (Scopus, IEEE Xplore, Wiley, ACM, and Science Direct), collating relevant papers published between 2017 and April 2024, a period which spans from the introduction of FL until now. Guided by the PRISMA protocol, we selected 49 papers to focus our systematic review on. We analysed these papers, paying special attention to the privacy threats and defensive measures – specifically within the context of IoT – using inclusion and exclusion criteria tailored to highlight recent advances and critical insights. We identified various privacy threats, including inference attacks, poisoning attacks, and eavesdropping, along with defensive measures such as Differential Privacy and Secure Multi-Party Computation. These defences were evaluated for their effectiveness in protecting privacy without compromising the functional integrity of FL in IoT settings. Our review underscores the necessity for robust and efficient privacy-preserving strategies tailored for IoT environments. Notably, there is a need for strategies against replay, evasion, and model stealing attacks. Exploring lightweight defensive measures and emerging technologies such as blockchain may help improve the privacy of FL in IoT, leading to the creation of FL models that can operate under variable network conditions."
"How Can Digital Twins Support the Economic, Environmental, and Social Sustainability of Healthcare Systems: A Systematic Review Focused on the Triple Bottom Line",M. D. Xames; T. G. Topcu,10.1109/ACCESS.2025.3559502,2025,"Digital twins (DTs) are transforming healthcare systems (HSs) by enabling real-time, data-driven decision-making. Despite their potential, research on DTs’ role in long-term HS sustainability remains nascent. This study systematically reviews DT use cases in HSs through the lens of the triple-bottom-line framework of sustainability, identifying their economic, environmental, and social contributions. Additionally, it maps these use cases to the United Nations (UN) Sustainable Development Goals (SDGs) to assess their alignment with global sustainability policies. A systematic literature review following the PRISMA framework was conducted across four databases (Scopus, Web of Science, Engineering Village, and PubMed), identifying 81 peer-reviewed studies. DT use cases were categorized into sustainability dimensions and qualitatively mapped to UN SDGs. We identify 28 unique DT use cases supporting HS sustainability – 13 contributing to economic (e.g., precision medicine, early diagnosis), 8 to environmental (e.g., energy-efficient hospital operations, waste management), and 7 to social sustainability (e.g., provider burnout prevention, equitable access). Our mapping reveals that DTs could support 11 of 17 UN SDGs, including SDG 3 (good health and well-being), SDG 8 (economic growth), SDG 9 (innovation), and SDGs 12–15 (environmental impact mitigation), among others. This study documents the significant potential of DTs to enhance HS sustainability across economic, environmental, and social dimensions while supporting multiple SDGs. However, most existing DT studies overlook explicit sustainability linkages, with limited attention to assessing or prioritizing DTs’ impact on HS sustainability. Future research should develop standardized sustainability metrics, conduct empirical studies, and create frameworks linking DT outcomes to SDGs."
A Systematic Literature Review of Cloud Brokers for Autonomic Service Distribution,M. Hamzah Khan; M. H. Habaebi; M. Rafiqul Islam,10.1109/ACCESS.2024.3458829,2024,"In recent years, cloud computing has become an essential distributed computing platform and has achieved enormous popularity. Within cloud computing, the Cloud service broker creates an abstraction layer between provider and consumer so that customers notice the cloud service providers’ offered services’ solitary view. The brokers of cloud service help connect the cloud’s substantial resources and select the data centers of the cloud that meet the user’s requirement while maximizing the entire response time and reducing cost. The landscape of autonomic cloud brokers has been reviewed in this systematic literature review study, while the PRISMA approach is used to analyze the literature. This comprehensive review of cloud brokerage mechanisms is tailored towards the autonomic distribution of services. To emphasize autonomic computing and cloud-access security brokers, the evolving paradigms of cloud service selection are detailed and critically analyzed to enhance service distribution efficiency. Further, the role of cloud brokers in load-balancing services is also highlighted in this study. A new taxonomy for the structured framework of cloud brokerage mechanisms is introduced based on functionalities, deployment models, and architecture for the autonomic service distribution. Finally, the study offers valuable insights for future research challenges and best practices in cloud security."
Unveiling Research Trends in Stack Overflow: A Comprehensive Analysis of General Discussion Theme,G. G. Giwangkoro; Y. S. Nugroho,10.1109/SIML61815.2024.10578280,2024,"Stack Overflow (SO), as technical Question & Answer (Q&A) platform, plays a pivotal role in facilitating knowledge exchange between software developers. Since the release of SO, numerous studies have been conducted to gain insights into the discussions of its users. Despite its significance, a comprehensive review in terms of the evolving trends, impact, sources, and themes within this domain is lacking. This paper presents a systematic literature review of SO-related studies from 2019 to 2023, exploring the evolution, impact assessment, relevant sources, and thematic explorations within this domain. Our study analyzes 804 SO-related journal and conference articles, employing Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) technique to collect data from Scopus. The findings show that the evolution of SO-related studies reveals a sustained academic interest, with 172 publications in 2020 and a continued engagement with 153 publications in 2023. The declining trend in citations over recent years suggests the need for further exploration into factors influencing the impact of SO studies on the broader field of software development. The identification of relevant sources highlights different contributions from journals and conferences, such as Empirical Software Engineering and the ACM International Conference Proceeding Series. Thematic exploration reveals dynamic changes, with a dominant focus on Codes (Symbols) in 2019 and subsequent diversification into themes like Deep Learning and Software Design. This literature review provides valuable insights for academics, practitioners, and developers, emphasizing the importance of SO as a platform for collaborative knowledge sharing within the software development community."
Build Up the Sustainable Talent Ecosystem: An Empirical Study of Taiwan AITalent Demand Survey,N. -D. Lin; Y. -C. Chang; C. -H. Yeh; L. -F. Huang,10.1109/iSTEM-Ed62750.2024.10663136,2024,"The study aims to investigate the current state of AI applications in the Taiwanese information service industry and AI startups through a questionnaire. It delves into the present and future demand for AI talent in these sectors. The study also seeks to identify innovative training methods based on survey results to address the shortage of AI talent. The survey findings indicate that with the emergence of generative AI (GAI) and the maturation of AI technologies, the trend in AI usage is shifting towards vertical applications and productization. As a result, there is an increasing demand for application-oriented and interdisciplinary AI talent. It is crucial to develop suitable approaches to equip the workforce with AI skills and establish a reliable talent certification mechanism to promote the widespread adoption of AI."
Factors Responsible for the Success of a Start-up: A Meta-Analytic Approach,A. K. Pasayat; B. Bhowmick; R. Roy,10.1109/TEM.2020.3016613,2023,"This article aims to determine factors crucial for the performance of new ventures by performing a meta-analysis. It evaluates the performance of machine learning and statistical models by selecting 19 studies through preferred reporting items for systematic reviews and meta-analyses (PRISMA). The results obtained were graphically represented using forest plots. A subjective analysis of the studies revealed “business plan,” “market scope,” “team size,” “service timing,” “market growth,” and “age of the entrepreneur” as crucial factors for deciding the performance of the new ventures. In machine learning model-based studies, the combined model was found to be statistically significant. The variables, namely, “seed funding,” “funding rounds,” “location,” “social media presence,” “team size,” “number of founding members,” and “defunct date” affect the predictive capability of these models. The implications of this article can be applied by start-ups to reduce uncertainty and enhance performance."
A Systematic Literature Review of Skyline Query Processing Over Data Stream,M. A. Mohamud; H. Ibrahim; F. Sidi; S. N. M. Rum; Z. B. Dzolkhifli; Z. Xiaowei; M. M. Lawal,10.1109/ACCESS.2023.3295117,2023,"Recently, skyline query processing over data stream has gained a lot of attention especially from the database community owing to its own unique challenges. Skyline queries aims at pruning a search space of a potential large multi-dimensional set of objects by keeping only those objects that are not worse than any other. Although an abundance of skyline query processing techniques have been proposed, there is a lack of a Systematic Literature Review (SLR) on current research works pertinent to skyline query processing over data stream. In regard to this, this paper provides a comparative study on the state-of-the-art approaches over the period between 2000 and 2022 with the main aim to help readers understand the key issues which are essential to consider in relation to processing skyline queries over streaming data. Seven digital databases were reviewed in accordance with the Preferred Reporting Items for Systematic Reviews (PRISMA) procedures. After applying both the inclusion and exclusion criteria, 23 primary papers were further examined. The results show that the identified skyline approaches are driven by the need to expedite the skyline query processing mainly due to the fact that data streams are time varying (time sensitive), continuous, real time, volatile, and unrepeatable. Although, these skyline approaches are tailored made for data stream with a common aim, their solutions vary to suit with the various aspects being considered, which include the type of skyline query, type of streaming data, type of sliding window, query processing technique, indexing technique as well as the data stream environment employed. In this paper, a comprehensive taxonomy is developed along with the key aspects of each reported approach, while several open issues and challenges related to the topic being reviewed are highlighted as recommendation for future research direction."
Quality Requirement Change Management’s Challenges: An Exploratory Study Using SLR,J. Ahmad; T. M. Ghazal; A. W. Khan; M. A. Khan; M. Inairat; N. Sahawneh; F. Khan,10.1109/ACCESS.2022.3224593,2022,"Requirement change management plays an important role in the business world where management in the business scenario is a hard-hitting assignment because of continuously changing customer choices in respect of requirements. Giving no attention to requirement change management challenges result in consumers’ discontent. Main cause of business products is that there is no planning regarding requirement change management. Furthermore, it also affects market value. Dealing in requirement change management, it’s far important to cope with those challenges to undertake the requirement of the business consumers. This article focuses on documentation and control of quality requirement challenges by using an approach of systematic literature review. The main goal of this article is to classify critical challenges being faced by vendor companies in global software development. A total of fourteen challenges have been documented which have a severe effect on the management of quality requirements. Challenges documented like ‘Incomplete requirements’, ‘Lack of Communication & Coordination, etc. are the key challenges harming managing quality requirement changes. Among these fourteen challenges, nine challenges are marked as critical challenges whose ratios are above 25%. The identified challenges were analyzed decade wise where we categorized them into three decades i.e. first decade (1992-2002), the second decade (2003-2012), and the third decade (2013-2021)."
Robot-Based Intervention for Children With Autism Spectrum Disorder: A Systematic Literature Review,K. D. Bartl-Pokorny; M. Pykała; P. Uluer; D. E. Barkana; A. Baird; H. Kose; T. Zorcec; B. Robins; B. W. Schuller; A. Landowska,10.1109/ACCESS.2021.3132785,2021,"Children with autism spectrum disorder (ASD) have deficits in the socio-communicative domain and frequently face severe difficulties in the recognition and expression of emotions. Existing literature suggested that children with ASD benefit from robot-based interventions. However, studies varied considerably in participant characteristics, applied robots, and trained skills. Here, we reviewed robot-based interventions targeting emotion-related skills for children with ASD following the guidelines of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement. We systematically searched for all relevant articles published in English language until May 2021, using the databases Scopus, Web of Science, and PubMed. From a total of 609 identified papers, 60 publications including 50 original articles and 10 non-empirical articles including review articles and theoretical articles were eligible for the synthesis. A total of 892 participants were included in the robot-based intervention studies; 570 of them were children with ASD. Nao and ZECA were the most frequently used robots; recognition of basic emotions and getting into interaction were the most frequently trained skills, while happiness, sadness, fear, and anger were the most frequently trained emotions. The studies reported a wide range of challenges with respect to robot-based intervention, ranging from limitations for certain ASD subgroups and security aspects of the robots to efforts regarding the automatic recognition of the children’s emotional state by the robotic systems. Finally, we summarised and discussed recommendations regarding the application of robot-based interventions for children with ASD."
A Comparative Study Using Discriminant Analysis on a Questionnaire Survey Regarding Project Managers’ Cognition and Team Characteristics,A. Masuda; T. Matsuodani; K. Tsuda,10.1109/COMPSAC.2017.11,2017,"The purpose of this study is to create a model of a relationship in which the dependent variable is the result of a project and the independent variables are the characteristics of human resources. We attempted a comparative evaluation of discriminant analyses with a statistical model and a machine learning model using assessments of the results of projects and team characteristics derived from questionnaire survey data. The results of the evaluation demonstrate that the machine learning model shows a higher discrimination rate within the range of the data used in the analysis, but it became clear that the discrimination rate worsens in comparison with the statistical model when extrapolated."
Data-Driven Decision-Making in IT Projects: A Regression Application in Software Estimation,O. Bulkrock; A. BaniMustafa; A. Qusef,10.1109/ICTCS65341.2025.10989401,2025,"IT-related projects are among the most challenging to manage due to their inherent complexity, uncertainty, and the intangibility of their outcomes. However, a data-driven decision-making approach can help tackle these challenges and increase the chance of IT project success. Machine learning techniques can be used to predict uncertain aspects of IT projects, such as the estimation of time, cost, and effort, when applied to historical data collected from previous or similar projects. This approach can allow the project manager to make informed decisions based on facts rather than human judgment or hunch. In this work, we review the potential of data-driven approaches in the management of IT projects, and we demonstrate a use case that applies three regression algorithms, including polynomial regression, linear regression, and partial least squared (PLS), to predict software effort estimation based on the NASA93 public dataset. The best results were achieved using polynomial regression which scored 82% in R2, 81% in MAPE, 152 in MAE, 276 in RMSE and 76387 in MSE metrics."
Design of College Students' Innovation and Entrepreneurship Management System Based on Deep Learning,X. Wang,10.1109/IIoTBDSC60298.2023.00016,2023,"In order to strengthen the application, evaluation, mid-term inspection, final acceptance and fund management of college students' innovation and entrepreneurship training programs, it is necessary to develop a project management system for college students' innovation and entrepreneurship training programs by using modern computer technology. The content of this paper is to develop a management system for college students' innovation and entrepreneurship based on deep learning, including reasonable database design and friendly interface design, which mainly realizes the functions of information release, student registration, information inquiry, project review, project conclusion and system background management. The system is a B/S structure and a Web server. The application included in this system architecture can be divided into three parts, namely, data acquisition, application server and data server. Predictive project process mining can help managers identify abnormal activities that are not in compliance with the project process. BiLSTM is combined with self-attention mechanism by using deep learning technology, and then the event log in the project process is used as input to realize accurate multi-task activity and time prediction. The test results all meet the expectations, indicating that the system is relatively stable and reliable. And it can solve some problems faced by students through a management platform that can meet the daily management functions for all teachers and students, such as project teaming."
Exploring the Blockchain Applications in Healthcare Sector: A Systematic Literature Review,S. V. Urkude; D. Saravanan,10.1109/ICIMIA60377.2023.10426154,2023,"In the era of Web 3.0, Blockchain Technology (BT) applications are admired in various sectors including banking, production, government and healthcare sector etc. Blockchain features such as availability, storage, accessibility and uniqueness of data are more suitable to use in the healthcare sector. Using PRISMA systematic literature procedure, the research articles related to blockchain applications are extracted from reputed databases including Scopus, Web of Science and Google Scholar. The shortlisted research papers were analyzed to explore the application of blockchain technology and adoption challenges in the healthcare sector. The findings shows blockchain technology is popularly used to track electronic health records, tracking drugs & surgical equipment and storage of patient data. The findings reveal a lack of various applications in combining blockchain technology with the Internet of Things (IoT) for tracking patient activity, helping in emergencies and Artificial Intelligence (AI) based systems for disease diagnosis."
IT Trends: A Literature Review,A. X. S. López; I. G. H. Pérez; J. C. H. Mora; J. Velandia,10.1109/CONIITI61170.2023.10324125,2023,"Identification of the principal IT trends is crucial for setting goals in the educational and organisational fields. For example, Higher Educational Institutions (HEIs), namely, programmes related to the IT field should establish programmes that respond to the current and future demand based on IT trends. Thus, this study aims to define the IT trends for the coming three years through a literature review. PRISMA methodology is adopted to guarantee a structured mechanism to gather, analyse and consolidate data. In addition, triangulation and saturation methods were applied to define a sufficient number of studies with a certain degree of quality. The outcomes present themes and codes that consolidate the IT trends that HEIs and organisations may use to set their strategies and goals."
An A.I. Based Cost Estimation System for Material Sourcing with Product Recommendations for Architectural Designs,A. J. Aquino; D. E. Balajediong; C. Centeno; C. C. Manalo; B. R. de Joya; A. A. R. Sison,10.1109/ICCA62237.2024.10928032,2024,"This research showcases the creation of ArchEstimate, an AI-based online tool made to help architects with estimating project expenses and choosing materials. The system utilizes Artificial Neural Networks (ANN) to generate precise cost predictions using project details like gross floor area and material choices. Moreover, it includes fuzzy logic for preliminary assessment and material choice, improving the accuracy of cost prediction. Studies demonstrate the high accuracy of ANN for cost estimation, ranging from 87% to 98% in the literature review. ArchEstimate successfully reached a 90%-97% precision rate in cost estimation for this project. The system received outstanding ratings in functional suitability, compatibility, interaction capability, and maintainability when evaluated according to ISO 25010:2023 standards. The findings show that ArchEstimate greatly enhances cost estimation in architectural projects, providing architects with a reliable tool for generating precise estimates and material suggestions. Future advancements may broaden its capabilities to involve subcontractors and architects, thereby enhancing its range and utility in the construction sector."
A Systematic Literature Review on Balinese Residential Addresses Without Predefined Gazetteers for Last Mile Delivery,M. I. Ansori; W. Anggraeni; R. A. Vinarti,10.1109/ICITISEE63424.2024.10729903,2024,"The accuracy and speed of parcel delivery in courier business services are the keys for customers to determine which courier service to use. In several cases, failed deliveries are partly due to non-standard address writing or using other address terms. Home addresses in Bali still use the ‘Banjar’ address as a reference for parcel delivery, while the Banjar itself is not a standard address for residence. Several methods are used to find the address with local wisdom, namely by combining non-standard and standard addresses so that the actual location can be found and the package is delivered to the end destination. Using geotag technology through map coordinates as a complement to address data can be used to overcome the difficulty of finding an address. However, this is not always applicable because the sender mostly does not know the actual map position of the recipient. Therefore, the purpose of this study is to investigate existing research that solves the problem of non-standard addresses in the context of package delivery. This article uses the PRISMA methodology to find related literature. In the last 10 years, hundreds of articles were found, and 16 of them use specific methods to find solutions to deliver to Balinese residential addresses without predefined gazetteers."
Technology-Assisted Emotion Recognition for Autism Spectrum Disorder (ASD) Children: A Systematic Literature Review,M. A. Rashidan; S. N. Sidek; H. M. Yusof; M. Khalid; A. A. A. Dzulkarnain; A. S. Ghazali; S. A. M. Zabidi; F. A. A. Sidique,10.1109/ACCESS.2021.3060753,2021,"The information about affective states in individuals with autism spectrum disorder (ASD) is difficult to obtain as they usually suffer from deficits in facial expression. Affective state conditions of individuals with ASD were associated with impaired regulation of speech, communication, and social skills leading towards poor socio-emotion interaction. It is conceivable that the advance of technology could offer a psychophysiological alternative modality, particularly useful in persons who cannot verbally communicate their emotions as affective states such as individuals with ASD. The study is focusing on the investigation of technology-assisted approach and its relationship to affective states recognition. A systematic review was executed to summarize relevant research that involved technology-assisted implementation to identify the affective states of individuals with ASD using Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) approach. The output from the online search process obtained from six publication databases on relevant studies published up to 31 July 2020 was analyzed. Out of 391 publications retrieved, 20 papers met the inclusion and exclusion criteria set in prior. Data were synthesized narratively despite methodological and heterogeneity variations. In this review, some research methods, systems, equipment and models to address all the related issues to the technology-assisted and affective states concerned were presented. As for the consequence, it can be assumed that the emotion recognition with assisted by technology, for evaluating and classifying affective states could help to improve efficacy in therapy sessions between therapists and individuals with ASD. This review will serve as a concise reference for providing general overviews of the current state-of-the-art studies in this area for practitioners, as well as for experienced researchers who are searching for a new direction for future works."
Normalization Strategies in Multi-Center Radiomics Abdominal MRI: Systematic Review and Meta-Analyses,J. Panic; A. Defeudis; G. Balestra; V. Giannini; S. Rosati,10.1109/OJEMB.2023.3271455,2023,"Goal: Artificial intelligence applied to medical image analysis has been extensively used to develop non-invasive diagnostic and prognostic signatures. However, these imaging biomarkers should be largely validated on multi-center datasets to prove their robustness before they can be introduced into clinical practice. The main challenge is represented by the great and unavoidable image variability which is usually addressed using different pre-processing techniques including spatial, intensity and feature normalization. The purpose of this study is to systematically summarize normalization methods and to evaluate their correlation with the radiomics model performances through meta-analyses. This review is carried out according to the PRISMA statement: 4777 papers were collected, but only 74 were included. Two meta-analyses were carried out according to two clinical aims: characterization and prediction of response. Findings of this review demonstrated that there are some commonly used normalization approaches, but not a commonly agreed pipeline that can allow to improve performance and to bridge the gap between bench and bedside."
Research Trends in Software Development Effort Estimation,Y. Swandari; R. Ferdiana; A. E. Permanasari,10.1109/EECSI59885.2023.10295716,2023,"Developing a software project without the appropriate amount of effort would significantly impede and even fail the project, putting the software developer's quality at risk. Therefore, software development effort estimation (SDEE) is the most critical activity in software engineering. SDEE has seen extensive research, resulting in a massive rise in the literature in a relatively short period. In this regard, it is crucial to identify the significant study topics in software development effort estimation that will assist researchers in understanding and recognizing research trends. This research applied a systematic literature review (SLR) to compile all journals from the predefined search directory about software development effort estimation thoroughly and unbiasedly from 2018 to 2022. This review was a prelude to further research activities in software development effort estimation. Five research topics out of 71 papers have been revealed, including the machine learning approach, algorithmic technique, expert judgement, dataset analysis, and evaluation metric. With 27 journals, deploying a machine learning approach for SDEE is the most discussed research topic. The potential research described in this study can be investigated further in software development effort estimation field."
"UNet Deep Learning Architecture for Segmentation of Vascular and Non-Vascular Images: A Microscopic Look at UNet Components Buffered With Pruning, Explainable Artificial Intelligence, and Bias",J. S. Suri; M. Bhagawati; S. Agarwal; S. Paul; A. Pandey; S. K. Gupta; L. Saba; K. I. Paraskevas; N. N. Khanna; J. R. Laird; A. M. Johri; M. K. Kalra; M. M. Fouda; M. Fatemi; S. Naidu,10.1109/ACCESS.2022.3232561,2023,"Biomedical image segmentation (BIS) task is challenging due to the variations in organ types, position, shape, size, scale, orientation, and image contrast. Conventional methods lack accurate and automated designs. Artificial intelligence (AI)-based UNet has recently dominated BIS. This is the first review of its kind that microscopically addressed UNet types by complexity, stratification of UNet by its components, addressing UNet in vascular vs. non-vascular framework, the key to segmentation challenge vs. UNet-based architecture, and finally interfacing the three facets of AI, the pruning, the explainable AI (XAI), and the AI-bias. PRISMA was used to select 267 UNet-based studies. Five classes were identified and labeled as conventional UNet, superior UNet, attention-channel UNet, hybrid UNet, and ensemble UNet. We discovered 81 variations of UNet by considering six kinds of components, namely encoder, decoder, skip connection, bridge network, loss function, and their combination. Vascular vs. non-vascular UNet architecture was compared. AP(ai)Bias 2.0-UNet was identified in these UNet classes based on (i) attributes of UNet architecture and its performance, (ii) explainable AI (XAI), and, (iii) pruning (compression). Five bias methods such as (i) ranking, (ii) radial, (iii) regional area, (iv) PROBAST, and (v) ROBINS-I were applied and compared using a Venn diagram. Vascular and non-vascular UNet systems dominated with sUNet classes with attention. Most of the studies suffered from a low interest in XAI and pruning strategies. None of the UNet models qualified to be bias-free. There is a need to move from paper-to-practice paradigms for clinical evaluation and settings."
Sign Language Translation Techniques Using Artificial Intelligence for the Hearing Impaired Community in Sri Lanka: A Review,M. Priyankara; A. Gunasekara; K. Ilmini,10.1109/SLAAI-ICAI59257.2023.10365012,2023,"Hearing Impaired individuals routinely encounter limitations in their involvement in social interactions, access to intriguing information, and participation in everyday activities, among various other aspects. However, the hardest part of their interactions with regular people is communication, because sign language is the primary language of those who are hearing impaired. However, the general public is unaware of sign language. Each country has its own sign language. However, there are some striking similarities between them. In Sri Lanka, hearing impaired people use Sri Lankan Sign Language (SLSL) as their communication language. There is several research done on Sign Language recognition and translation. But no fully functioning system is utilized for Sri Lankan Sign Language translation. To find the gap in this area, we conducted a systematic literature review using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method that analyses 12 studies on Sign Language Translation (SLT). As per the literature review, Image Processing (IP) and Convolutional Neural Networks (CNN) are the most used techniques for Sign Language translation. But these methods have limitations: not enough data, differences in how people use sign language, difficulty in translating in real-time, not capturing cultural aspects, needing specific equipment, and understanding the context of conversations. Recognizing and solving these problems is important, especially for languages like SLSL. Future research should focus on getting more data, making translation work for different cultures, and improving real-time translation. This will help hearing impaired people communicate better with others."
Artificial Intelligence Applications in Quality Management System: A Bibliometric Study,J. Minglana; R. R. Tobias; R. E. Roxas,10.1109/TENCON54134.2021.9707340,2021,"This paper presents a systematic literature review and bibliometric analyses of publications in the field of Quality Management System (QMS) by authors who applied artificial intelligence (AI) in ISO 9001:2015 audits. Scopus-indexed papers that were published from 1998 to 2021 were evaluated based on the research publication metrics made available by Scopus. From the 142 extracted Scopus-indexed publications in September 2021, 109 or 76 percent of the publications remained after Preferred Reporting Items for Systematic Reviews and Meta-analysis (PRISMA) procedure. Analyses and visualizations using VOSviewer reveal research productivity, affiliation and collaboration networks in various countries, and the corresponding relationship between research networks in the field of AI-enabled QMS. Findings reveal that QMS is leaning towards sustainability, big data, and applied technological innovations."
Social Media Practices on Construction Sites: A New Conceptual Model of Social Media Impact on Team Performance,R. Karimi; M. B. Shishehgarkhaneh; R. C. Moehler; Y. Fang; S. Ahmad,10.1109/EMR.2025.3539324,2025,"The role of social media in construction site management is not thoroughly understood, particularly in its capacity to influence team dynamics and performance. This study investigates the impact of social media use (SMU) on team feedback (TF), knowledge sharing (KS), and member support (MS) within construction site teams in Australia. Drawing from existing literature and integrating Social Exchange Theory and the Theory of Communication Visibility, this research develops a conceptual model that illustrates the relationships between SMU and team performance (TP), with coordination (COO) as a moderating variable. The study identifies that SMU enhances TF, KS, and MS, which subsequently improve TP. COO is shown to amplify or mitigate these effects based on task complexity and interdependencies. This research provides a robust framework for understanding the multifaceted influence of SMU on construction teams and sets the stage for empirical validation across diverse project environments. The findings offer valuable insights for leveraging social media to optimize communication and performance on construction site management, while also highlighting areas for future exploration, such as technological integration and comparative regional studies."
Software Fault Detection Algorithms Using Artificial Intelligence: A Review and Classification,L. Ndlovu; B. Cossa; C. Makokoe; L. Khumalo; N. Ntshangase; S. Myaka; N. Ndhlovu; M. Mtshali,10.1109/ICECET61485.2024.10698379,2024,"With the rise of software applications, software development has become a rapidly evolving field, propelled by technological advancements and the increasing demand for innovative solutions to sustain our digital era. Detecting software faults has emerged as a critical attribute in the software development cycle to ensure system reliability, quality, and user satisfaction. However, traditional fault detection methods often suffer from drawbacks such as time consumption and error proneness, particularly in the context of large and complex software systems. In response to these hurdles, artificial intelligence (AI) presents itself as a promising methodology, with the aim of improving accuracy, scalability, automation, and proactiveness in fault detection. This paper systematically reviews the literature on AI-based software fault detection algorithms. Leveraging the PRISMA framework (Preferred Reporting Items for Systematic Reviews and Meta-Analyses), a curated set of 50 peer-reviewed research publications was identified in Google Scholar and subsequently analyzed using a classification coding framework. The findings reveal that AI-based software fault detection algorithms played an important role in improving software system reliability and performance."
Neuro-fuzzy ensembles: A systematic mapping study,H. Ouifak; A. Idri,10.1109/AICCSA56895.2022.10017811,2022,"In the last decade, neurofuzzy networks have received considerable attention from academia. These systems strike a tradeoff between the performance of artificial neural networks and the interpretability of fuzzy inference systems expressed through fuzzy rules. Many researchers, however, are still dissatisfied with the performance of single neuro-fuzzy systems and are constantly working to improve them using ensemble techniques. This study conducts a systematic mapping of relevant studies on the application of ensemble techniques to neuro-fuzzy systems. Many aspects of the study are highlighted, including publication years, sources, contribution types, and application domains. As result, 48 articles published from 2000 to Mars 2022 were selected from six digital libraries (Science Direct, IEEE Xplore, ACM Digital Library, PubMed, Wiley, and Google Scholar. The findings revealed that the number of studies employing ensemble techniques with neuro-fuzzy systems is low and unstable. As the most used neuro-fuzzy system, ANFIS, from the Takagi-Sugeno-Kang category, is being studied in a wide range of application domains, most notably finance, medicine, and ecology."
Prisma Analysis of Innovative Technologies on Advancement of Optimisation of Manufacturing Industry,N. Mulongo; M. Matlala; K. Mushavhanamadi,10.1109/SmartNets58706.2023.10215876,2023,"The swift advancement of modern innovation has created a significant change within various sectors, particularly in manufacturing industry. Consequently, several companies have started to investigate for innovative administrative configuration and policy in order to implement artificial intelligence technologies into their production routes. Hence, the tool such as, the Computer Aided Manufacturing is considered as a key player in optimizing the manufacturing industry and mostly in the change to smart manufacturing. The advent of industry 4.0 has resulted in novel business patterns, which are more and more adopted in the manufacturing sector. To this end, the present research paper sought to use the Prisma approach to critically review the existing literature in the field of artificial intelligence to demonstrates and outlines its key role on the advancement and optimising the production processes of goods within the manufacturing industry. The review process demonstrates that the advent of artificial intelligence has the ability to significantly improve the productivity, flexibility, and efficiency whilst enabling smart decision-making of the operations related to supply chain. The study sought to offer a clear idea for company and organization that wishes to provide a roadmap for digitizing the various manufacturing operations and techniques. It is also expected that by presenting this review, both academics and industrial practitioners will gain access to a hands-on library of information on impact of new technology and Artificial Intelligence. In assuring the credibility of the Prisma approach, each search term or keyword was separately examined."
Detecting Frauds and Payment Defaults on Credit Card Data Inherited With Imbalanced Class Distribution and Overlapping Class Problems: A Systematic Review,S. N. Kalid; K. -C. Khor; K. -H. Ng; G. -K. Tong,10.1109/ACCESS.2024.3362831,2024,"Credit card payments are one popular e-payment option apart from cash payments. Recent reports show that credit card fraud and payment defaults are increasing annually and are alarming. Thus, researchers have attempted various machine learning techniques to address these two challenges. However, they are challenged to mitigate the two major problems inherited in credit card data: (i) imbalanced class distribution and (ii) overlapping classes. Mitigating these problems shall effectively detect credit card frauds and payment defaults, thus benefiting card issuers and holders. Hence, this paper aims to develop a systematic review using PRISMA to identify and compare various credit card datasets, machine learning techniques, and evaluation metrics. Subsequently, we provide recommendations for handling these two problems. We extracted research papers from 2016 to 2023 from ScienceDirect, Springer, Association and Computing Machinery (ACM), and IEEE databases. The papers shall be included if written in English and published in peer-reviewed and indexed journals or conference proceedings. Finally, 87 papers were selected based on the eligibility criteria. Based on our findings, the European and Taiwan datasets are widely used in the research community. However, most researchers focus on tackling imbalanced class distribution rather than two problems together. We recommended to the research community the application of deep learning, ensemble learning, and sampling methods to effectively detect fraud and payment defaults on credit card datasets that inherit the two problems. In evaluating the machine learning algorithms, we recommend using metrics that can separately evaluate the algorithms’ performance in detecting frauds/payment defaults and normal transactions."
A Systematic Literature Review on Social Media Slang Analytics in Contemporary Discourse,A. Sundaram; H. Subramaniam; S. H. A. Hamid; A. M. Nor,10.1109/ACCESS.2023.3334278,2023,"Social media slang, encompassing informal language, words, phrases, and acronyms on digital platforms, reflects the dynamic nature of online communication. Analyzing social media slang offers valuable insights for organizations and researchers, enabling a deeper understanding of communication trends, sentiment analysis, and user behavior in the digital sphere. It plays a pivotal role in shaping effective marketing strategies and enhancing communication, ultimately facilitating informed decision-making in the digital age. In our study, we conducted a systematic review of research articles from the Web of Science and Scopus databases, spanning the years 2016 to 2023. Our rigorous selection process, based on quality assessments as per PRISMA guidelines, revealed several key findings. Social media slang exhibits a remarkable adaptability to different platforms, mirroring the communication styles and user cultures found on each. Notably, it influences user behavior, impacting interactions, content engagement, and decision-making, particularly in marketing and communication strategies. Furthermore, our research highlights the value of social media slang in sentiment analysis, providing insights into public sentiment and supporting well-informed decision-making. Our study underscores the versatile applications of slang analytics across various industries and research domains, emphasizing its pivotal role in providing specialized insights and enhancing communication strategies. In conclusion, our research offers a comprehensive understanding of the dynamic landscape of informal language in the context of contemporary digital communication, furnishing valuable insights that inform decision-making, refine marketing strategies, and enhance communication."
"5G for Smart Grids: Review, Taxonomy, Bibliometrics, Applications and Future Trends",R. Rituraj; D. T. Varkonyi; A. Mosavi; A. V. Koczy,10.1109/INES59282.2023.10297699,2023,"This comprehensive literature review analyzes the current state-of-the-art research on 5G applications and technologies in smart grids. Adhering to PRISMA guidelines, the review presents a detailed taxonomy of applications and methodologies being utilized in this field. The review searched various electronic databases from 2012 to 2022, and conducted bibliometric analysis to identify research trends, influential authors, journals, and institutions in this field. The review identifies various 5G applications and technologies used in Smart grids, and the potential benefits of 5G technology in enhancing the efficiency, reliability, and security of smart grid systems. The study provides valuable insights into the research trends, research gaps, and future research directions in this field, and can guide researchers and practitioners in developing and implementing 5G-enabled smart grid systems."
A Review: State-of-the-Art of Integrating AI Models with Moving-target Defense for Enhancing IoT Networks Security,A. Hamada; S. M. Hassan; S. Samy; M. Azab; E. Fathalla,10.1109/UEMCON62879.2024.10754728,2024,"The rapid proliferation of IoT devices across various sectors has significantly expanded the attack surface for cyber threats. Traditional security strategies, which rely on static defenses, often fail to adequately protect these diverse and resource-constrained devices, rendering them vulnerable to attacks. To address this challenge, proactive security mechanisms like Moving-Target Defense (MtD) introduce dynamic, real-time changes, making systems less predictable and more challenging for attackers to exploit. However, deploying MtD strategies in IoT networks presents various limitations and trade-offs between the network’s trustworthiness, performance, and compatibility. To overcome these challenges, AI-based MtD strategies offer a promising solution by dynamically adapting and optimizing defense mechanisms, thereby enhancing system resilience and unpredictability. This paper systematically reviews the use of Artificial Intelligence (AI) models to improve security in IoT networks, with a particular focus on integrating AI with MtD techniques to bolster network resilience. The review adheres to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) model principles and employs a systematic mapping process to analyze AI’s effectiveness in predicting and mitigating IoT attacks while exploring the potential of MtD mechanisms."
Human-like Social Learning for Social Robots: A Systematic Review,D. Burkart; B. Bruno,10.1109/RO-MAN60168.2024.10731225,2024,"Social learning is a learning paradigm aiming to make the process of teaching new skills to an artificial intelligent agent (such as a robot) as close as possible to the process we employ when teaching skills to other humans. Equipping robot companions with the ability to learn from social interactions with their users would enable naive users to effortlessly teach skills to robots and support personalization and long-term use. Aiming to support research on the field, in this article we present a systematic literature review of human-like social learning for social robots, with a specific focus on works employing real robots and real human-robot social interactions. The review examines relevant papers from perspectives including the type of robot used, the task to be learned, and the method and assessment metrics employed, and allows for the identification of open research avenues."
Software effort estimation using machine learning techniques,Monika; O. P. Sangwan,10.1109/CONFLUENCE.2017.7943130,2017,"Effort Estimation is a very important activity for planning and scheduling of software project life cycle in order to deliver the product on time and within budget. Machine learning techniques are proving very useful to accurately predict software effort values. This paper presents a review of various machine-learning techniques using in estimation of software project effort namely Artificial Neural Network, Fuzzy logic, Analogy estimation etc. Machine learning techniques consistently predicting accurate results because of its learning natures form previously completed projects. This paper summarizes that each technique has its own features and behave differently according to environment so no technique can be preferred over each other."
Virtual Influencers Revolutionizing Marketing: A Bibliometric Deep Dive into Emerging Trends and Future Prospects,S. Rumangkit; R. Astari Rahmatika,10.1109/ICIMCIS63449.2024.10956725,2024,"Within the academic literature stream of influencer marketing, studies around virtual influencers are gaining momentum concurrently with the emergence of virtual influencers and their popularity, especially among young audiences on social media. As this study domain is vastly developing, more research is needed to examine this topic from various spectrums. This study examines current and future research trends regarding virtual influencers in marketing and advertising literature. A bibliographic analysis using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework is used to reveal themes regarding virtual influencers based on 167 papers dated from 2020–2024. The analysis results highlight the annual production of academic literature concerning virtual influencers, most productive journal outlets, country productivity, paper co-citation, and emerging and future trends presented in various mapping and visualizations. Based on the findings, this research offers insights into the trend and future research area around the emerging virtual influencer marketing topic."
Artificial Intelligence in Radar Technology and Communication Networks,A. Dutta; A. Misra; S. Deka; K. K. Sarma,10.1109/MITADTSoCiCon60330.2024.10575148,2024,"Artificial intelligence, together with Machine learning, Big Data analysis, Data Science, Mathematical modeling, Optimal control theory, and Simulation techniques, play very significant roles in the cutting-edge research fields of Radar technology and Communication networks. In this article, we review some recent research articles/ papers in this field, discuss their core findings, highlight some gaps and open problems, and elaborate the hierarchy of methodology by applying the principles ""PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses.) and ""PICOS (Population, Intervention, Comparison, Outcome, Study design),"" describe the rationale of outcomes, and finally draw the conclusion with the broader future spectrum of Radar technology and Communication networks."
Extreme Learning Machine Applied to Software Development Effort Estimation,H. D. P. De Carvalho; R. Fagundes; W. Santos,10.1109/ACCESS.2021.3091313,2021,"The project management process has been used in the area of Software Engineering to support project managers to keep projects under control. One of the essential processes in Software Engineering is to conduct an accurate and reliable estimation of the required effort to complete the project. This article’s objectives are: i) to identify the variables that influence the estimation based on the correlation, and ii) to apply the Extreme Learning Machine - ELM model for effort estimation and compare it with the literature models. Thus, it was investigated which technique has better effort prediction accuracy. The models were compared with each other based on predictive precision in the criterion of absolute mean residue (MAR) and statistical tests. The main findings in this study were: i) important variables for effort estimation and; ii) the results indicated that the ELM model presents the best results compared to the models in the literature for estimating software design effort. In this way, the use of Machine Learning techniques in the effort estimation process can increase the chances of success in the accuracy of the time estimates and the project’s costs."
Application of Artificial Intelligence in ERP System's Fulfillment of the Purchasing Function,L. Hanane; M. Noureddine; D. Mostafa; L. Imane,10.1109/SITA60746.2023.10373719,2023,"Artificial intelligence (AI) has the potential to have an impact on every aspect of a business, including the purchasing division. This article analyzes how AI is used in the fulfillment of the purchasing function in enterprise resource planning (ERP) systems, especially SAP editor (Systems, Applications, and Products in Data Processing), and its impact on the system's performance. First, a review is undertaken of how AI is used in different fields in companies. Thereafter, the paper follows a deep research to collect the information on SAP editor from any sources such as industry reports, article ad information systems site webs. The results indicate that the application of AI can be integrated with the purchasing function of ERP systems almost throughout their entire lifecycle, helping overcome many problems that ERP systems are not able to automate and streamline."
Breast Cancer Detection Using Mammography: Image Processing to Deep Learning,S. A. Qureshi; Aziz-Ul-Rehman; L. Hussain; T. Sadiq; S. T. H. Shah; A. A. Mir; M. A. Nadim; D. K. A. Williams; T. Q. Duong; Q. -U. -A. Chaudhary; N. Habib; A. Ahmad; S. A. H. Shah,10.1109/ACCESS.2024.3523745,2025,"Breast cancer stands as a predominant health concern for women globally. As mammography is the primary screening tool for breast cancer detection, improving the detection of breast cancer at screening could save more lives. This mammography review paper comprehensively reviews computer-aided techniques during a specific time frame for the segmentation and classification of microcalcification, evaluating image processing, machine learning, and deep learning techniques. The review is meticulously carried out, adhering closely to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. This article focuses on mammographic breast cancer detection approaches based on automated systems, discussed chronologically from 1970 through 2023. This article encompasses the breadth of artificial intelligence-based methods from the most primitive to the most sophisticated models. Image processing and machine learning-based methods are comprehensively reviewed. Evaluating a deep learning architecture based on self-extracted features for classification tasks demonstrated outstanding performance. Large-scale datasets required for a broader and in-depth analysis of novel methods for breast cancer detection are also discussed in this article. This research work is aligned with the United Nations’ sustainability development goals."
Smart and Sustainable Grids Using Data-Driven Methods; Considering Artificial Neural Networks and Decision Trees,R. Rituraj; D. Ecker; V. K. Annamaria,10.1109/SISY56759.2022.10036258,2022,Sustainability is the essential part of smart grids and the ultimate future of energy systems. Providing a state-of-the-art review on the progress of advanced learning systems which contribute to the sustainability of smart grid is essential. This paper reviews the applications of data-driven methods of machine learning in sustainable smart grid systems. The machine learning methods had been classified and reviewed in various groups based on the proposed taxonomy. The applications and methods had been identified and systematically reviewed based on the PRISMA guideline.
"Machine Learning in RADAR-Based Physiological Signals Sensing: A Scoping Review of the Models, Datasets, and Metrics",A. Nocera; L. Senigagliesi; M. Raimondi; G. Ciattaglia; E. Gambi,10.1109/ACCESS.2024.3482690,2024,"In the field of physiological signals monitoring and its applications, non-contact technology is often proposed as a possible alternative to traditional contact devices. The ability to extract information about a patient’s health status in an unobtrusive way, without stressing the subject and without the need of qualified personnel, fuels research in this growing field. Among the various methodologies, RADAR-based non-contact technology is gaining great interest. This scoping review aims to summarize the main research lines concerning RADAR-based physiological sensing and machine learning applications reporting recent trends, issues and gaps with the scientific literature, best methodological practices, employed standards to be followed, challenges, and future directions. After a systematic search and screening, two hundred and seven papers were collected following the guidelines of PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses). The included records covered two macro-areas being regression of physiological signals or physiological features (n=77 papers) and the other a cluster of papers regarding the processing of RADAR-based physiological signals and features; the latter cluster concerns four fields of interest, being RADAR-based diagnosis (n=77), RADAR-based human behaviour monitoring (n=25), RADAR-based biometric authentication (n=19) and RADAR-based affective computing (n=9). Papers collected under the diagnosis category were further divided, on the basis of their aims: in breath pattern classification (n=41), infection detection (n=10), sleep stage classification (n=9), heart disease detection (n=9) and quality detection (n=8). Papers collected under the human behaviour monitoring were further divided based on their aims: fatigue detection (n=9), human detection (n=7), human localisation (n=4), human orientation (n=2), and activities classification (n=3)."
Nine Questions to Evaluate a Data Science Team’s Process: Exploring a Big Data Science Team Process Evaluation Framework Via a Delphi Study,J. Saltz,10.1109/BigData55660.2022.10020499,2022,"While the lack of an effective team process is often noted as one of the key drivers for data science project inefficiencies and failures, there has been minimal research on how to evaluate a data science team’s process. Without an evaluation framework, it is difficult for data science teams to understand their team process strengths and weaknesses. To help address this challenge, this exploratory research, via a Delpha study, identified nine key questions a data science team could answer to help evaluate their process. In short, the study identified questions evaluating the team’s communication (within the team and with stakeholders). The study also identified team process questions (e.g., the use of iterations, life cycles and a prioritization process for potential tasks). Future research could explore how data science teams can best improve their process by leveraging and refining these questions as well as defining an overall data science project management evaluation framework."
"A Systematic Review of Social Engineering Attacks & Techniques: The Past, Present, and Future",F. Femi-Oyewole; V. Osamor; D. Okunbor,10.1109/SEB4SDG60871.2024.10629836,2024,"Social engineering, a prevalent cybercrime tactic, employs deceptive techniques to extract sensitive information, manipulating human decision-making. This study provides an in-depth review of social engineering attacks, countermeasures, challenges, and future trends. Utilizing the PRISMA methodology, a systematic literature review was conducted across diverse databases, resulting in the selection of 59 articles from an initial pool of 1,020. The findings reveal the adoption of various strategies, including machine learning, education, topic blacklisting, logo identification, visual similarities, search engine-based techniques, and identity management, to mitigate social engineering threats. Notably, machine learning emerges as the most utilized method (43.8%), followed by education and awareness programs (18.8%), highlighting their efficacy, scalability, adaptability, and cost-effectiveness in addressing this cybersecurity menace. Machine learning algorithms effectively identify patterns indicative of social engineering attacks, while educational initiatives empower users to recognize and thwart such tactics, thereby reducing susceptibility to manipulation and enhancing organizational resilience."
An Overview of Machine Learning Approaches to Software Development Cost Estimation,M. Maher; J. S. Alneamy,10.1109/ICCITM56309.2022.10032008,2022,"Software cost estimation still presents a real challenge to software development firms and practitioners. One of the crucial activities in project management is software cost estimation. The accuracy of the estimated cost has a critical impact on the project’s success or failure. Overestimation and underestimation are serious challenges to project managers, especially in today’s competitive marketplace. Inaccurate estimation may lead to over budget, missing deadlines, business loss, and even project failure. The precise cost estimation will enable the project manager to forecast and allocate the necessary resources to deliver a successful product. Many researchers have published research papers in this area. In this paper, the study’s goal is to outline the recent machine learning approaches to software cost estimation. The study will address the problems with conventional cost estimation models and provide a comprehensive review of the researches regarding software cost estimation with machine learning. The current study will discuss the intelligence models for software cost estimation and highlight the models that excel in this area."
How does Machine Learning Change Software Development Practices?,Z. Wan; X. Xia; D. Lo; G. C. Murphy,10.1109/TSE.2019.2937083,2021,"Adding an ability for a system to learn inherently adds uncertainty into the system. Given the rising popularity of incorporating machine learning into systems, we wondered how the addition alters software development practices. We performed a mixture of qualitative and quantitative studies with 14 interviewees and 342 survey respondents from 26 countries across four continents to elicit significant differences between the development of machine learning systems and the development of non-machine-learning systems. Our study uncovers significant differences in various aspects of software engineering (e.g., requirements, design, testing, and process) and work characteristics (e.g., skill variety, problem solving and task identity). Based on our findings, we highlight future research directions and provide recommendations for practitioners."
Application of Artificial Intelligence Techniques for Brain–Computer Interface in Mental Fatigue Detection: A Systematic Review (2011–2022),H. Yaacob; F. Hossain; S. Shari; S. K. Khare; C. P. Ooi; U. R. Acharya,10.1109/ACCESS.2023.3296382,2023,"Mental fatigue is a psychophysical condition with a significant adverse effect on daily life, compromising both physical and mental wellness. We are experiencing challenges in this fast-changing environment, and mental fatigue problems are becoming more prominent. This demands an urgent need to explore an effective and accurate automated system for timely mental fatigue detection. Therefore, we present a systematic review of brain-computer interface (BCI) studies for mental fatigue detection using artificial intelligent (AI) techniques published in Scopus, IEEE Explore, PubMed and Web of Science (WOS) between 2011 and 2022. The Boolean search expression that comprised (((ELECTROENCEPHALOGRAM) AND (BCI)) AND (FATIGUE CLASSIFICATION)) AND (BRAIN-COMPUTER INTERFACE) has been used to select the articles. Through the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) methodology, we selected 39 out of 562 articles. Our review identified the research gap in employing BCI for mental fatigue intervention through automated neurofeedback. The AI techniques employed to develop EEG-based mental fatigue detection are also discussed. We have presented comprehensive challenges and future recommendations from the gaps identified in discussions. The future direction includes data fusion, hybrid classification models, availability of public datasets, uncertainty, explainability, and hardware implementation strategies."
State-of-the-Art and Challenges in Pancreatic CT Segmentation: A Systematic Review of U-Net and Its Variants,C. Zhang; A. Achuthan; G. M. S. Himel,10.1109/ACCESS.2024.3392595,2024,"In medical image analysis, segmenting pancreatic CT images presents a significant challenge due to the complex anatomy of the pancreas and the generally low contrast of these images. Accurate pancreas segmentation is crucial in clinical scenarios, particularly for the diagnosis and treatment of pancreatic cancer. The U-Net architecture and its variations have achieved significant progress in deep learning-based image segmentation, especially in the context of pancreatic CT image segmentation. However, there is a noticeable gap in the comprehensive evaluation of their performance, limitations, and potential improvements specifically in this area. This systematic review aims to address this gap in the literature, focusing particularly on U-Net and its variants in pancreatic CT image segmentation. Adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, this review includes relevant studies published since 2019 in the field of pancreatic segmentation. The findings illuminate the current limitations of these methods and establish a theoretical foundation for future research directions."
A Review of Literature on Critical Factors that Drive the Selection of Business Intelligence Tools,B. Gina; A. Budree,10.1109/icABCD49160.2020.9183852,2020,"Business Intelligence Systems (BIS) has gained importance across many companies, from small-medium sized to well-established organizations. While there is extensive literature studying the adoption, implementation, and critical success factors (CSFs) of BIS, research focusing on factors influencing the selection of business intelligence (BI) software tools has been limited. The main objective of this paper is to discover and explore literature addressing the issue and concerns of software selection experienced by organizations. Furthermore, constructs and factors influencing business intelligence tools selection will be discovered and explored. The study also seeks to determine and examine important variables of interest relating to technical and non-technical factors in order to improve the selection of appropriate software tools that will assist in meeting the organization needs and objectives so as to successfully implement BIS. A narrative literature review was employed and a total of 32 studies was found significant; these studies are divided into three categories 1) selection methodology, 2) evaluation criteria, and 3) evaluation technique. Moreover, factors relating to a software tool, vendor and opinion emerged as influential in the BI tool's selection process. Lastly, the business intelligence conceptual model has emerged from the reviewed literature."
A Comprehensive Machine Learning Framework for Evaluating Agility of a Software Development Organization,S. De,10.1109/EMR.2024.3487007,2024,"Software Project Management has undergone significant development over the years due to the progress made in various techniques. Scrum is widely regarded as one of the most often employed methodologies, encompassing a range of stages from refinement to retrospective. It empowers individual teams to take ownership of their work and deliver on their commitments within a specified timeframe. This presents numerous issues for upper-level management regarding product delivery and engineering. This study examines a novel method of estimating work based on complexity instead of estimating effort based on hours. It presents a new approach to combining this framework and using it to measure and predict organizational efficiency using K-Nearest Neighbor and Decision Tree algorithms. This article will examine scenarios that elucidate how velocity-driven planning can assist a software organization in effectively delivering business value to its customers. It will utilize a synthetic dataset to demonstrate how current datasets can assist a Scrum Master or Project Lead in forecasting and organizing future sprints and dividing backlogs into manageable tasks using tools like Jira. This results in higher accuracy, as seen for Organization 2 with 87% and 95% for Organization 3, respectively via experiments, and showcases the maturity growth over time."
Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review,M. Golchoubian; M. Ghafurian; K. Dautenhahn; N. L. Azad,10.1109/TITS.2023.3291196,2023,"Planning an autonomous vehicle’s (AV) path in a space shared with pedestrians requires reasoning about pedestrians’ future trajectories. A practical pedestrian trajectory prediction algorithm for the use of AVs needs to consider the effect of the vehicle’s interactions with the pedestrians on pedestrians’ future motion behaviours. In this regard, this paper systematically reviews different methods proposed in the literature for modelling pedestrian trajectory prediction in presence of vehicles that can be applied for unstructured environments. This paper also investigates specific considerations for pedestrian-vehicle interaction (compared with pedestrian-pedestrian interaction) and reviews how different variables such as prediction uncertainties and behavioural differences are accounted for in the previously proposed prediction models. PRISMA guidelines were followed. Articles that did not consider vehicle and pedestrian interactions or actual trajectories, and articles that only focused on road crossing were excluded. A total of 1260 unique peer-reviewed articles from ACM Digital Library, IEEE Xplore, and Scopus databases were identified in the search. 64 articles were included in the final review as they met the inclusion and exclusion criteria. An overview of datasets containing trajectory data of both pedestrians and vehicles used by the reviewed papers has been provided. Research gaps and directions for future work, such as having more effective definition of interacting agents in deep learning methods and the need for gathering more datasets of mixed traffic in unstructured environments are discussed."
AI-based Paediatric Teledermatology Analysis and Proposed Framework,N. A. C. Andryani; S. Achmad; M. K. Ario; Jerikho; S. Sun; L. Daniel; N. Constantine; M. Mazaya; S. P. Gondokaryono; D. Puspitosari,10.1109/IBITeC59006.2023.10390932,2023,"Teledermatology technology development has been increasing due to the pandemic of COVID-19 which limit human physical contact. In addition, it is designed to cope with the unbalanced distribution of dermatologists especially in rural country including Indonesia. It implies that the dermatology health care are provided by general practitioners. The competence of general practitioners in dealing with dermatological diseases is limited. Therefore, the probability of misdiagnosis cases is considerably high. The misdiagnosis will delay the recovery which will affect the quality of life. Specifically for paediatric cases, misdiagnosis could affect the child’s growth and development. Connecting the general practitioner to the specialist or consultant and supporting them with AI-based technology through teledermatology technology becomes urgent due to these issues. This paper aims to present a literature review on the development of teledermatology worldwide with its scope and objectives. The PRISMA flowchart method is used to outline the effectiveness of the teledermatology system and its features. The presented literature review provides insight into the urgency of teledermatology system in each country, framework modelling and significant features. In addition, this paper contributes to propose paediatric teledermatology design in Indonesia based on the local background and insight of recent teledermatology technology development point of views."
Software Cost Estimation: A Literature Review and Current Trends,S. Singh; K. Kumar,10.1109/ICSCCC58608.2023.10176495,2023,"Software cost estimation is a challenging and complex task during software development. It directs project managers and developers to analyze and predict costs at the beginning of the software development life cycle. The most important job for developing software projects is correctly estimating cost, time duration, and needed effort. This paper aims to review different models used for software cost estimation, including algorithmic, non-algorithmic, and learning-oriented models, which have been published over the last ten years (i.e., from 2011 to 2022). The comparison is done based on the methods, selected datasets, and metrics used in different techniques. We observed that machine learning-based models perform better compared to other existing methods. However, to the best of our knowledge, analysis, and literature survey, we found that no technique in the literature provides a fit-all solution."
Team Formation in Software Engineering: A Systematic Mapping Study,A. Costa; F. Ramos; M. Perkusich; E. Dantas; E. Dilorenzo; F. Chagas; A. Meireles; D. Albuquerque; L. Silva; H. Almeida; A. Perkusich,10.1109/ACCESS.2020.3015017,2020,"Context: Software team formation is an important project management activity. However, forming appropriate teams is a challenge for most of the companies. Objective: To analyze and synthesize the state of the art on the software team formation research. Additionally, we aim to organize the identified body of knowledge in software team formation as a taxonomy. Method: Using a Snowballing-based systematic mapping study, 51 primary studies, out of 2516, were identified and analyzed. We classified the studies considering the research methods used, their overall quality, and the characteristics of the formed teams and the proposed solutions. Results: The majority of the studies use search and optimization techniques in their approaches. Also, technical attributes are the most frequent type considered to build individuals’ profiles during the team formation process. Furthermore, we proposed a taxonomy on software team formation. Conclusion: There is a predominant use of search-based approaches that combine search and optimization techniques with technical attributes. However, the adoption of non-technical attributes as complementary information is a tendency. Regarding the research gaps, we highlight the level of subjectivity in software team formation and the lack of scalability of the proposed solutions."
Multi-Sensor Approach for Cobalt Exploration in Asturias (Spain) Using Machine Learning Algorithms,M. Carvalho; A. Azzalini; J. Cardoso-Fernandes; P. Santos; A. Lima; A. C. Teodoro,10.1109/IGARSS53475.2024.10640581,2024,"This study explores dimensionality reduction techniques, namely, PCA (Principal Component Analysis) and ICA (Independent Component Analysis), to condense Earth Observation (EO) data obtained from Landsat 9 and PRISMA satellites to detect alteration zones related to Cobalt (Co) mineralization in the Áramo mine, situated in Asturias, Spain, by employing Support Vector Machine (SVM) Machine Learning (ML) algorithm. The ICA-based models exhibit slightly better performance than PCA-based ones, particularly in delineating alteration zones in the Landsat 9 image, showing promising results in distinguishing alteration zones from host rocks, demonstrating the viability of these techniques applied to mineral exploration. However, the results show the need for refined field data collection methodologies to enhance prediction accuracy for more robust results, in the scope of the HORIZON Europe S34I project (https://s34i.eu/)."
Energy Demand Forecasting and Optimizing Electric Systems for Developing Countries,S. S. Arnob; A. I. M. S. Arefin; A. Y. Saber; K. A. Mamun,10.1109/ACCESS.2023.3250110,2023,"Currently, developing countries are experiencing a massive shift toward industrialization. Developing countries lack the technical sophistication and infrastructure to encourage low-carbon and sustainable economic growth because of weak public awareness, regulations, and technology. Developing countries must plan the industrialization process for maximum energy efficiency of production, thereby reducing their CO textsubscript 2 emissions significantly by increasing energy efficiency. This paper presents a systematic survey on the current pragmatic methods for forecasting the future load demands from minutes to years ahead in developing countries, following the Preferred Reporting Items for Systematic review and Meta-Analysis Protocols (PRISMA-P). The primary focus of this systematic survey paper is to provide an optimal forecasting model selection strategy for potential researchers and forecasters. Based on the strengths and weaknesses of the different models, we will discuss the most suitable methods to tailor them to multiple applications and scenarios of load forecasting. The comparison elements are Forecast horizons, Spatio-temporal resolutions, factors affecting the load, different dimensional reduction techniques, model complexity analysis, and the MAPE for error analysis. From the results, We have found ANN hybridized with meta-heuristic techniques to be superior in most of the analysis cases. ANN’s ability to handle non-linear data, flexibility, and robustness is why. Consumption data aggregated at the national level can capture trends efficiently. Meteorological and calendar features influence short-term forecasting extensively, whereas economic factors influence long-term load patterns. Finally, we have identified the trends and research gaps from the existing literature, presenting relevant technical recommendations for improvement."
Recent Trends and Challenges in Assistive Applications for Sinhala-Speaking Adults with Dyslexia: A Decade in Review,P. Perera; D. Sumanathilaka,10.1109/ICARC64760.2025.10963092,2025,"This paper discusses assistive technologies developed for Sinhala-speaking dyslexics, focusing on the challenges they face and how existing tools attempt to address them. Using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework, this review analyzed research publications from 2015 to 2024, highlighting topics such as phonetic processing, text normalization, and error detection. Of the 85 initially reviewed, 32 were included in the meta-analysis to represent major challenges posed by the lack of annotated datasets and Natural Language Processing (NLP) resources tailored to the cognitive and linguistic needs of the adult patient population. This review points out the need for the extension of low-resource language toolkits and proper integration of assistive NLP applications. The study concludes that while significant progress has been made, a collaborative, interdisciplinary approach is crucial to improving literacy, independence, and social participation for Sinhala-speaking adults with dyslexia."
"Improved Software Effort Estimation Through Machine Learning: Challenges, Applications, and Feature Importance Analysis",P. V. Terlapu; K. K. Raju; G. Kiran Kumar; G. Jagadeeswara Rao; K. Kavitha; S. Samreen,10.1109/ACCESS.2024.3457771,2024,"Effort estimations are a crucial aspect of software development. The tasks should be completed before the start of any software project. Accurate estimations increase the chances of project success, and inaccurate information can lead to severe issues. This study systematically reviewed the literature on effort-estimating models from 2015-2024, identifying 69 relevant studies from various publications to compile information on various software work estimation models. This review aims to analyze the models proposed in the literature and their classification, the metrics used for accuracy measurement, the leading model that has been chiefly applied for effort estimation, and the benchmark datasets available. The study utilized 542 relevant articles on software development, cost, effort, prediction, estimation, and modelling techniques in the search strategy. After 194 selections, the authors chose 69 articles to understand ML applications in SEE comprehensively. The researchers used a scoring system to assess each study’s responses (from 0 to 5 points) to their research questions. This helped them identify credible studies with higher scores for a comprehensive review aligned with its objectives. The data extraction process identified 91% (63) of 69 studies as either highly or somewhat relevant, demonstrating a successful search strategy for analysis. The literature review on SEE indicates a growing preference for ML-based models in 59% of selected studies. 17% of the studies chosen favor hybrid models to overcome software development challenges. We qualitatively analyzed all the literature on software effort estimation using expert judgment, formal estimation techniques, ML-based techniques, and hybrid techniques. We discovered that researchers have frequently used ML-based models to estimate software effort and are currently in the lead. This study also explores the application of feature importance and selection in machine learning models for Software Effort Estimation (SEE) using popular algorithms like support Vector Machine (SVM), AdaBoost (AB), Gradient Boost (GB), and Random Forest (RF) with six benchmark datasets like CHINA, COCOMO-NASA2, COCOMO, COCOMO81, DESHARNAIS, and KITCHENHAM. We analyze the dataset descriptions and feature importance of the dataset analysis using ML models for choosing crucial play attributes in SEE."
Realtime Safety Analysis System using Deep Learning for Fire Related Activities in Construction Sites,U. K. Dwivedi; C. Wiwatcharakoses; Y. Sekimoto,10.1109/ICECCME55909.2022.9987855,2022,"The era of digital transformation focuses on the integration of digital and AI based technology in construction industry for sustainable economic growth and high quality of life. This paper aims to provide a real-time detection and tracking of various construction activities and provide immediate practical safety guidelines and alert for probable accidental scenarios to ensure the safety of construction site and workers by using deep learning algorithms with vision-based edge devices and smartphone. Proposed paper develops a hybrid algorithm using scene classification first, and dependent object detection and tracking second to analyze vast category of fire related activities from video and images in real-time using computationally challenging devices. To cover the ever-changing construction location, easy to move smartphone-based applications were developed with AI as an API solution. The review of the results confirms superior real-time performance in successfully identifying and providing clear safety guidelines for indoor and outdoor fire related activities such as welding work and fire safety equipment and workers safety gear such as hardhat helmet. The study validated the practicality of IoT and deep learning-based solutions for construction jobsites with indoor and outdoor locations."
Agro-Technological Systems in Traditional Agriculture Assistance: A Systematic Review,N. Montalvo-Romero; A. Montiel-Rosales; R. Purroy-Vásquez; P. Quechulpa-Pérez,10.1109/ACCESS.2023.3329087,2023,"Guaranteeing food security from agriculture in an uncertain context, derived from the effects of multiple factors, is a challenge. Traditional agricultural production is the one that faces the greatest challenges, derived from the scarce evolution in agricultural practices, despite being the one that contributes the most to the availability of food, at 80%. This systematic review aims to identify and analyze agrotechnological systems belonging to precision agriculture, which may be potentially adaptable to traditional rural agriculture. Contributions that improved crop yields from scientific and technological studies were analyzed. The PRISMA statement was used as a formal outline to collect and analyze 114 studies from the period 2018-2023. From the review, it was identified that there is a growing trend in the adoption of intelligent systems that help producers in the management of crops, accentuated in the increase of crop yield, in the determination of product quality, and in the management of water resources, mainly. Likewise, it was identified that the preponderant approach is the monitoring and control of crop development. This is achieved through emerging technologies, such as the Internet of Things, artificial intelligence, and machine learning, with information mainly collected by sensors embedded in drones, algorithms, decision support systems, sensors, and Arduino technology systems. Finally, this review shows that there are five viable systems that can be adapted to traditional agriculture to strengthen agricultural production. Therefore, the adoption of scientific-technological contributions from precision agriculture contributes to ensuring food security."
Business Analysis: From Tools to Application,G. Xiao; J. Wu; S. -P. Tseng,10.1109/ICOT64290.2024.10936929,2024,"Business analysis requires various tools, each with unique features. This paper provides a comprehensive overview of tools, algorithms, and application fields in business analysis, highlighting their importance in addressing the growing complexity and volume of business data. Finally, the application of large language models in business analysis is discussed."
Stress Detection among Higher Education Students: A Comprehensive Systematic Review of Machine Learning Approaches,E. M. Arias; J. Parraga-Alava; D. Z. Montenegro,10.1109/ICEDEG61611.2024.10702055,2024,"In today’s world of high academic expectations and societal pressures, spotting stress in university students early on is crucial, especially in the context of modern education in the era of Smart Society. In this paper, we outline a thorough review of how machine learning methods are being used to measure stress levels in university students. We followed the PRISMA framework to search through various databases from 2019 to 2024, such as ACM, IEEE Explore, and others, sifting through 249 articles. We chose 38 articles to look at closely, while we excluded 179 and identified 32 duplicates. Our review found that supervised models, like Random Forest (RF), K-Nearest Neighbor (KNN), and support vector machine (SVM), are the most common and effective, detecting stress with about 95% accuracy. Physiological datasets, such as WESAD, and real-time data collection are the most common sources for measuring stress. While the most significant considerations for measuring the level of stress include the use of tools such as the Perceived Stress Scale (PSS) and the SISCO Inventory, along with sociodemographic aspects such as gender, age, marital status and educational level. Our study highlights how machine learning can help us find stress in students early, which is crucial for supporting them in higher education. Our findings also contribute to improving how we understand and deal with stress in education."
A Systematic Review and Meta-Analysis of Intelligent Irrigation Systems,H. Hammouch; M. A. El-Yacoubi; H. Qin; H. Berbia,10.1109/ACCESS.2024.3421322,2024,"The water crisis, global warming and climate changes have become recently prominent world issues. Saving and conserving water have become, therefore, an imperative for water resources’ sustainability. In this context, dramatic innovations have come to light, e.g., Precision Agriculture. Numerous research projects on this subject have been conducted, as illustrated not only by the huge number of research works in this topic, but also by the dozens of surveys dedicated to. Surveys regarding smart agriculture are of two categories, surveys on smart agriculture in general in which smart irrigation is a subtopic among others, and surveys dedicated to smart irrigation. Existing surveys, however, suffer from limitations including low paper numbers, linear description without structural classification, and missing key information. To fill this gap, we propose, in this paper, a survey on SI that classifies the large body of literature into categories in a structured way. To this end, we have employed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology to determine the inclusion or exclusion of articles. We have reviewed a total of 610 publications out of which 227 met the inclusion criteria. We have categorized the selected references into three conceptual clusters corresponding to: (1) 41.8% focusing on field measurements, based on IoT sensors, (2) 37% involving Remote Sensing (RS) and (3) 21.1% Artificial Intelligence (AI) methods. Each category is thoroughly described, including the inputs and outputs, along with a description of the various tasks employed in irrigation management. This systematic re-view is expected to serve as a useful reference for research on smart irrigation management methods."
Leveraging Artificial Intelligence for Knowledge Management A Systematic Literature Analysis,D. Prihandoko; M. Arief; E. Elidjen; F. Alamsjah; Z. S. Rizky,10.1109/ICCIT62134.2024.10701138,2024,"This research examines the growth of Artificial Intelligence (AI) and Knowledge Management (KM) capabilities, evaluating how AI can enhance KM by managing information, data, and knowledge within organizations. Utilizing a systematic literature review method with PRISMA guidelines, the study reviewed 72 articles from Scopus (1994-2024). Findings indicate that AI implementation in KM spans various fields, predominantly within technology. AI integration is identified as a catalyst for innovation, efficiency, and organizational learning. The study highlights the increasing application of AI in KM and the evolving nature of this research area. Insights are provided into the potential for innovation and future development of AI in KM, emphasizing the necessity for organizational members to leverage AI to enhance performance."
Project Management Framework for Digitalization of Logistics Processes,K. Muthutantrige; S. M. Hasan; S. R. Shah,10.1109/ICTMOD59086.2023.10472907,2023,"The review focuses on the impact of digitalization on logistics processes and the role of project management in facilitating successful digitalization initiatives. Existing studies highlight the transformative power of digitalization in the logistics industry, with scholars emphasizing the integration of advanced technologies such as artificial intelligence, big data analytics, and cloud computing etc. These technologies enable real-time data collection, analysis, and decision-making, leading to improved operational efficiency, cost reduction, and enhanced customer experiences. The alignment of digitalization efforts with business strategies, organizational goals, and competitive advantage is also underscored. Furthermore, the literature identifies various challenges and considerations in the digitalization of logistics process, such as business complexity, organizational maturity, and the need for robust risk management practices. Scholars emphasize the importance of addressing these factors to ensure smooth digitalization implementation and mitigate potential risks. These articles provide valuable insights into the subject matter and contribute to the body of knowledge on digitalization, logistics, and project management."
Quantitative Classification of Cognitive Behaviors for Industrial Projects' Managers in the MENA Region,R. Abousamra; O. Hosam,10.1109/ITT56123.2022.9863957,2022,"One of the main challenges existing in the adoption of the artificial intelligence in recruitment and selection and talent acquisition is the ability to codify a unified number of features to recruit the style of project managers that can fit with the job post as well as the strength of the project. This paper is a deductive design using quantitative methodology by applying a monomethod of surveying more than 400 project managers in the MENA area. Our aim in this research is to classify the cognitive behavior of project managers in the MENA region quantitatively to be able to use this classification for a better recruitment result using the AI tools. The philosophy of the research is a realism one and the MENA region is chosen due to its increasing need to plan and implement developing projects. The conclusion of this study reveals the existence of four basic project manager's cognitive behavioral styles: the innovative, risk-averse, specialized, and experiential styles. The reliability of the measurement is tested by Cronbach 'alpha and the classification of items of the survey is implemented using the EFA & CFA. This research is adding a new generalized tool for measuring the knowledge creation style of project managers to enable talent acquisition seekers to have a more valuable adoption of AI in the process of recruitment and selection for different small and medium sized projects. Keywords: Cognitive Behavior; Knowledge Creation; Project Management; AI."
A Review of Advanced Deep Learning Methods of Multi-Target Segmentation for Breast Cancer WSIs,Q. Xu; A. Adam; A. Abdullah; N. Bariyah,10.1109/ACCESS.2025.3565648,2025,"Breast cancer is one of the most common cancers among women, with its heterogeneity posing significant challenges for diagnosis and treatment, profoundly impacting patient prognosis and quality of life. Whole Slide Imaging (WSI) in digital pathology provides high-resolution images that enable a comprehensive examination of the tumor microenvironment, offering advanced tools for breast cancer diagnosis and prognostic evaluation. However, manually reviewing whole slide images (WSIs) for tissue segmentation is time-consuming and prone to errors, highlighting the need for multi-target deep learning models to automate the segmentation of these complex structures. Multi-target segmentation offers distinct advantages by simultaneously processing multiple interrelated tissue regions within a single image, thereby enhancing accuracy and efficiency. Despite the potential of deep learning techniques in automating pathological analysis, their clinical adoption faces significant challenges. To address these, this paper proposes six criteria focused on clinical acceptability of deep learning methods: inherent limitations of WSIs, feature extraction, annotation requirements, efficiency, automated quantification, and interpretability. A rigorous review of publicly available datasets and deep learning methods identifies key challenges for clinical adoption. Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, this review analyzes 29 core articles, highlighting the critical role of multi-target segmentation in breast cancer digital pathology while assessing the limitations of these techniques in clinical applications. Based on this analysis, this paper proposes six criteria to enhance the diagnostic performance of deep learning methods in multi-target segmentation for breast cancer digital pathology and to improve the clinical acceptability of deep learning methods."
Automatic Emotion Recognition in Clinical Scenario: A Systematic Review of Methods,L. Pepa; L. Spalazzi; M. Capecci; M. G. Ceravolo,10.1109/TAFFC.2021.3128787,2023,"BACKGROUND - Automatic emotion recognition has powerful and interesting opportunities in the clinical field, but several critical aspects are still open, such as heterogeneity of methodologies or technologies tested mainly on healthy people. This systematic review aims to survey automatic emotion recognition systems applied in real clinical contexts (i.e., on a population of people with a pathology). METHODS - The literature review was conducted on the following scientific databases: IEEE Xplore®, ScienceDirect®, Scopus®, PubMed®, ACM®. Inclusion criteria were the presence of an automatic emotion recognition algorithm and the enrollment of at least 2 patients in the experimental protocol. The review process followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Moreover, the works were analysed according to a reference model in the form of a class diagram, to highlight the most important clinical and technical aspects and relationships among them. RESULTS - 52 scientific papers passed the inclusion criteria. Based on our findings, most clinical applications involved neuro-developmental, neurological and psychiatric disorders with the aims of diagnosing, monitoring, or treating emotional symptoms. The study design seems to be mostly related to the aim of the study (it is generally observational for monitoring and diagnosis, interventional for treatment), the most adopted signals are video and audio, and supervised shallow learning emerged as most used approach for emotion recognition algorithm. DISCUSSION - Tiny samples, absence of a control group and of tests in real-life conditions emerged as important clinical limitations. Under a technical point of view, a great heterogeneity of performance metrics, datasets and algorithms challenges the comparability, robustness, reliability and reproducibility of results. Suggested guidelines are identified and discussed to help scientific community to overcome limitations and provide direction for future works."
Adaptation of Classical Machine Learning Algorithms to Big Data Context: Problems and Challenges : Case Study: Hidden Markov Models Under Spark,I. SASSI; S. OUAFTOUH; S. ANTER,10.1109/ICSSD47982.2019.9002857,2019,"Big Data Analytics presents a great opportunity for scientists and businesses. It changed the methods of managing and analyzing the huge amount of data. To make big data valuable, we often use Machine Learning algorithms. Indeed, these algorithms have shown, in the past, their processing speed, efficiency and accuracy. But today, with the complex characteristics of big data, new problems have emerged and we are facing new challenges when developing and designing a new Machine Learning algorithm for Big Data Analytics. Therefore, it is essential to review the classical algorithms to adapt them to this new context. One of the methods of adaptation is the coupling between new technologies (i.e., distributed computing by GPU, Hadoop, Spark) and the Machine Learning algorithms to reduce the computational cost of data analysis. This paper highlights main challenges of adaptation of Machine Learning algorithms to the Big Data context and describes a novel method to make these algorithms efficient and fast in Big Data processing by taking as a case study the Hidden Markov Models using Spark framework. The results of complexity comparison of classical algorithms and those adapted to the Big Data context using Spark show a great improvement."
Arabic Sentiment Analysis of Eateries’ Reviews: Qassim region Case study,L. M. Alharbi; A. M. Qamar,10.1109/NCCC49330.2021.9428788,2021,"Social media plays an essential role in daily life. It allows people to express their thoughts and feelings about available products on e-commerce websites, which is often called an opinion or review. The aim is to see the author’s mood or the speaker’s attitude, which can be positive or negative about a product. This expression by the people is called sentiment. Sentiment analysis is a machine learning method in which the machine learns and analyzes some textual data’s sentiment and emotions (for example, reviews about movies or products). In this research, a real-world data set was collected from customers in the Qassim region using a Microsoft Form survey, containing approximately 1,785 Arabic reviews related to the Qassim region, Saudi Arabia. The overall aim is to find people’s opinions regarding different eateries (cafes and restaurants) using sentiment analysis of the customers’ reviews written in the Arabic language. The experiments are run using 10-fold cross-validation. The performance of the algorithms was measured using accuracy, recall, and F-measure. Different supervised machine learning classifiers were applied. Support Vector Machine, Logistic regression, and Random Forest achieved the best results. Furthermore, we analyzed the differences between all results in this study."
The Review for Visual Analytics Methodology,Z. Ahmad; S. Yaacob; R. Ibrahim; W. F. Wan Fakhruddin,10.1109/HORA55278.2022.9800100,2022,"Big data usage evolves from previously looking into the capacity of big data's descriptive and diagnostic perspectives into currently feeding the demands for predictive big data analytics. The needs come about due to organizations that crave predictive analytics capabilities to reduce risk, make intelligent decisions, and generate different customer experiences. Similarly, visual analytics play an essential role in understanding and fitting the analytics prediction in their business decision. Hence, the combination of descriptive, diagnostics and predictive within Visual Analytics emerges as a balanced field to provide understandable predictive insight. Due to the organizational demand and multi-discipline area, the approach to developing visual analytics is still uncertain in the Big Data Project Lifecycle from methodological perspectives. While there are a few potential methodological approaches that could be used for visual analytics, they are scattered across numerous academic research and industrial practice. To date, there is no coherent review and analysis of the work that has been explored specifically for Visual Analytics methodology. This paper reports on a review of previous literature concerning how Visual Analytics has been executed in the big data life cycle to address the gap. The review is organized in this study from three perspectives: i) general ICT -related methodology (e.g. SDLC, Agile, DevOps), ii) Data Science-related methodology (e.g. CRISP-DM, SEMMA, KDD) and iii) Visual Analytics-related methodologies in which each method will be benchmarked based on the Visual Analytics major part of reality, computer and human, in terms of its width, depth, and flows. This study found insufficiencies, non-specific and vague conditions in handling the Visual Analytics when using current methodological approaches based on the review conducted. The paper also highlights the Visual Analytics-related methodological review, which can shed some light on the approaches and ways of implementing analytics in the big data lifecycle, which can be beneficial for future studies in proposing a more comprehensive methodology for Visual Analytics in the big data lifecycle."
Agile Software Development Effort Estimation: A Comprehensive Survey,V. Malik; R. Arora; D. Mehrotra; H. Singh,10.1109/EmergIN63207.2024.10961288,2024,"This survey paper examines the evolution of effort and cost estimation methodologies in Agile software development, with a focus on Serum-based projects. Traditional techniques such as Story Points and Function Points are evaluated against modern approaches that integrate machine learning (ML), fuzzy logic, and optimization algorithms. The paper synthesizes findings from various studies, including those by Arora et al. (2018), Owais and Ramakishore (2016), and Dave et al. (2021), to assess the effectiveness of these methods in improving estimation accuracy. It also considers the influence of human factors, team dynamics, and customer involvement on project success, as highlighted by Tam et al. (2020) and Meckenstock (2024). The survey identifies a trend towards data-driven estimation models, as confirmed by Fernandez-Diego et al. (2020), and suggests that future research should explore hybrid ML models and deep learning algorithms to further refine Software Cost Estimation (SCE) processes. The paper contributes to the literature by providing a comprehensive overview of the current state of Agile estimation practices and by outlining directions for future research to enhance the precision and applicability of these techniques. This abstract encapsulates the essence of the survey, summa- rizing the research question, the approach taken, the key findings, and the significance of the work within the field of Agile software development."
Machine Learning in Sports Talent Identification: A Systematic Review,N. Reyaz; G. Ahamad; N. J. Khan; M. Naseem,10.1109/ICEFEET51821.2022.9848247,2022,"Sports talent identification is drawing an increasing research interest from the academic community. The traditional approaches are being systematized into the scientific approaches. The scientific approaches project an analytical understanding of the Talent Identification and Development (TID) process. This paper reviews the work done in this field from 1996 – 2021. We have reviewed the articles in accordance with the PRISMA guidelines. We trace the computational procedures that have evolved with time, for the sports talent identification. The special focus has been given to the role of machine learning in Cricket Talent Identification. The review delivers and discusses some key findings."
Methods for Analysis of EMG Signals: Systematic Review,M. Herak; T. Zubčević,10.1109/MECO55406.2022.9797169,2022,"The aim of this paper was to get familiar with the basic methods used for the EMG signal analysis. The research followed PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines for a clearer and concise search strategy. Grading of Recommendations of Assessment, Development and Evaluation strategy was used to ensure evidence certainty. Data that was summarized and presented in the review are gathered from three different electronic databases. Doing this systematic review, we summarized methods such as Fourier transform, wavelet analysis, the use of artificial networks, etc. We discussed the main advantages and disadvantages when it comes to each of the methods, but also a comparison between some of these methods. It will aid researchers in choosing the most appropriate method of EMG signal analysis which is important when it comes to neurological diagnostics, biomedical research, and prosthetic arm control."
"Contemporary Trend Analysis on Depression Detection Using EEG, Eye Gazing and Facial Emotion Recognition",S. Dey,10.1109/IC3I59117.2023.10397718,2023,"Depression, being one of the scorching issues of the present era and contributes as a major cause of suicidal death over the world, affects around 280 million people worldwide. Like the other mendable disease, depression can be cured completely with proper and proactive diagnosis. The lack of awareness and reluctance to visit a clinical practitioner may make the situation more critical for a patient having depressive disorder. In this paper, we have reviewed available literature on three prime focuses- Eye gaze measuring, EEG and facial emotion recognition. The main aim of this paper is to perform a systematic review of the available literatures in the selected area and explore the trend in this area and try to indicate the future direction also. Around 130 papers from peer-reviewed, High-Impact journals and conferences like Elsevier, IEEE, Springer, Lancet, NCBI, MDPI etc. have been selected and a systematic review has been performed on those papers to determine the trends as well as to indicate the future direction of this research."
The Contribution of ICTs and Digitalization to the Energy Transition: Realities and Opportunities for Project Management in Colombia,G. -R. Yaqueline; R. C. Ruth Esperanza; R. U. Leonardo,10.1109/COLCOM59909.2023.10334277,2023,"This paper describes the Energy Transition (ET) paradigm, its dimensions, and the contribution of digitalization and Information and Communication Technologies (ICT) to increase the energy efficiency of energy systems [1]. It starts from a literature review to establish theoretical links between these concepts, identify contemporary problems that evidence the need for ET in the global context, and the contribution of ICT use to the development of ET. After synthesizing the strategies proposed to potentiate ET at the global level, the normative progress and the profile of ET projects in Colombia are shown, in order to identify fields of potential use and ICT application projects for ET. Finally, local alternatives for financing projects in this field are identified, as well as a set of opportunities for project management in the ET era."
Involvement of Surveillance Drones in Smart Cities: A Systematic Review,A. Gohari; A. B. Ahmad; R. B. A. Rahim; A. S. M. Supa’at; S. Abd Razak; M. S. M. Gismalla,10.1109/ACCESS.2022.3177904,2022,"Drones, or unmanned aerial vehicles (UAVs), are among the most beneficial and emerging technologies, with a wide range of applications that can support the sustainability concerns of smart cities and ultimately improve citizens’ quality of life. The goals of this systematic review were to explore the involvement of surveillance drones in smart cities in terms of application status, application areas, proposed models, and characteristics of drones. We conducted this systematic review based on the preferred reporting items for systematic reviews and meta-analyzes (PRISMA) guidelines. We systematically searched the Web of Science and Scopus for journal articles and conference papers written in English and published up to August 2021. Of the 323 records identified, 43 met the inclusion criteria. Findings showed that surveillance drones were used in seven distinct research fields (transportation, environment, infrastructure, object or people detection, disaster management, data collection, and other applications). Air pollution and traffic monitoring were the dominant application areas. The majority of reviewed models were based on the application of rotary-wing single-drones with the camera as the aerial sensor. Reviewed models showed that the adoption of a single or multiple UAVs, either as a stand-alone technology or integrated with other technologies (e.g., internet of things, wireless sensor networks, convolutional neural networks, artificial intelligence, machine learning, computer vision, cloud computing, web applications), can offer efficient and sustainable solutions compared to conventional surveillance methods. This review can benefit academic researchers and practitioners."
An Extended Review of the Manufacturing Transition Under the Era of Industry 5.0,H. Mouhib; S. Amar; S. Elrhanimi; L. E. Abbadi,10.1109/CiSt56084.2023.10410003,2023,"In order to improve the industrial performance and satisfy customer demand in the most competitive way, the fourth industrial revolution integrates big data, artificial intelligence, advanced robotics and cyber physical systems allowing to manufacturing industries to be both smart and connected. Industry 4.0 is responding to the challenge of mass customization and while this transition is still in the beginning, the fifth industrial revolution has been introduced. The purpose of this paper is to provide a conceptual framework and to determine the current research trends of this new concept in the manufacturing field via an extended literature review elaborated using the PRISMA flow chart. The analysis of selected articles shows that societal and environmental needs not considered in Industry 4.0 are the inputs of the transition to Industry 5.0, and additional enabling technologies are required to reinforce the human-machine interaction towards an adaptive, flexible and human centric production enhancing hyper personalization, resilience and sustainability."
Thesis Management System Website for Suggesting Title and Summary from Abstracts,M. N. Ali; N. Jahan; A. B. Siddik; M. S. Akter; N. A. Nasir; M. T. Reza; M. A. Quader; M. S. Kabir; M. M. Rahman,10.1109/ICAECT60202.2024.10469125,2024,"This paper describes the implementation of a website designed to assist university seniors in selecting a project and thesis topic, as well as providing guidelines for thesis submission and completion. Through the usage of artificial intelligence and web programming, the system facilitates project administration while maintaining easy to use interface. The platform also includes a title generator that employs advanced NLP models such as T5 and BART to generate relevant and original titles based on the abstracts of students’ topics. In addition, it provides summarising tools to help users create clear summaries of their work. The study includes a thorough survey of 40 students to determine their requirements and expectations of the system. We also discuss various aspects such as HCI usability, UX design, and common problems encountered by graduating college students. In addition, they describe the common thesis and project management systems in universities around the globe. The system features an interface for project management, a search bar for topic searching, and the BART and T5 models for title generation. The results of the evaluation of the system indicate a high degree of necessity and usability based on student feedback. The study ends with ideas for improvements that could be made in the future, such as a search bar that uses NLP to make it easier to find topics, students, or teachers."
Human and Cognitive Factors involved in Phishing Detection. A Literature Review,D. Arévalo; D. Valarezo; W. Fuertes; M. Fernanda Cazares; R. O. Andrade; M. Macas,10.1109/CSCE60160.2023.00105,2023,"Human and cognitive factors considerably influence social engineering attacks. Cybercriminals take advantage of the innocence, carelessness, stress, lack of knowledge, and other aspects that make human beings vulnerable. Also, there exists difficulty for users to identify an email with phishing. However, the causes and solutions are not only technological; they also depend on human perception. Within this context, in this paper, we perform a systematic literature review using the PRISMA guidelines of the recent studies applying security and cognitive psychology, aiming to identify the human and cognitive factors that are part of a Phishing attack. The main findings of this research are focused on developing future research in cybersecurity, which we believe should go in hand with human cognitive and psychological factors."
Sensor Fusion Used in Applications for Hand Rehabilitation: A Systematic Review,I. Herrera-Luna; E. J. Rechy-Ramirez; H. V. Rios-Figueroa; A. Marin-Hernandez,10.1109/JSEN.2019.2897083,2019,"Medical conditions and accidents might cause immobility in certain parts of the body. In order to assist people in the rehabilitation process, sensors obtaining bio-signals from the body have been merged to create assistive technology. This systematic review is focused on presenting the state-of-the-art regarding sensor fusion used in the applications for hand rehabilitation. Articles were searched in four databases: IEEE Xplore, Web of Science, ACM Digital Library, and PubMed. Moreover, PRISMA and QualSyst protocols were employed to filter the articles and assess their qualities. As a result, of the 102 articles initially retrieved, only 29 articles performed sensor fusion for hand rehabilitation. Specifically, three application areas were found: sensor fusion for detecting hand movements, sensor fusion for exoskeleton control applications, and sensor fusion on serious games for hand rehabilitation. The studies included in this review measured two key aspects for rehabilitation purposes: user's limb strength and user's limb position. Regarding the sensors employed in the fusion, the most widely used sensor was electromyography sensor followed by the inertial sensors. Furthermore, the studies included in this review have focused on the following hand movements used for rehabilitation: flexion, extension, pronation, supination, power grasp, radial/urnal, open hand, single-finger contractions, multi-finger contractions, pinch grip, hand at rest, and hand grip."
Optimization Model PPM for Financial Goals with Machine Learning Literatur Review,N. H. Harani; A. Z. R. Langi; A. A. Arman,10.1109/ICISS59129.2023.10291364,2023,"This literature review consolidates key insights on Information Technology (IT) portfolio management, smart IT investment, cost estimation, and optimization techniques, specifically focusing on the applications of neural networks. As businesses increasingly pivot towards digital landscapes, the strategic role of IT portfolio management in aligning IT investments with business objectives has become paramount. Various sources underline the necessity of smart IT investment decisions, acknowledging their potential to drastically reduce operational costs and enhance business value. Our review further discusses how the predictive capabilities of neural networks have begun to shape cost estimation in IT portfolio management, highlighting the precision they bring to resource allocation. The integration of neural networks into IT portfolio management has shown promise in the existing literature, offering the potential to optimize smart IT investments and revolutionize traditional business models. This review concludes by identifying areas where more research is needed and explores the future implications of this integration"
Evaluation of fruit selection with ensemble model in the Peruvian industry,R. A. Velásquez; J. Vanessa Mejía Lara; R. A. Velásquez,10.1109/INTERCON59652.2023.10326089,2023,"This paper allows to evaluate fruits with the ensemble algorithm with artificial vision, besides, it evaluates the systematic review with the PRISMA methodology thought the last 10 years. Our findings are the application of TensorFlow and self-supervised multi-network fusion classification models. Some researchers describe classification errors. The deep learning methodology is also an efficient way of classifying and applying algorithms that optimally analyze features such as color, shape, and texture of the object. On the other hand, we can implement artificial intelligence to this work since in recent years it has been developing in all industrial sectors and is bringing positive results. Finally, it uses a case study with the limitations to admit safe practices in a company dedicated to agricultural production and also over time that when studying the last cycle and at the same time working, the availability of time is limited to be able to use it in a work area. The results are an mean absolute error for size evaluation of 92.92%, color evaluation with 93.23%, and the assessment of the maturity, defects and quality of 97.72% after the evaluation of 1271 research articles with the systematic review process."
Drone Transportation System: Systematic Review of Security Dynamics for Smart Mobility,S. O. Ajakwe; D. -S. Kim; J. -M. Lee,10.1109/JIOT.2023.3266843,2023,"The intelligence and integrity of a real-time cyber–physical system depend on how trustworthy the data’s legitimacy, appropriation, and authorization are during end-to-end communication between the participating nodes in its network. With the recurrent repugnant global violations of the airspace by drones and their derivatives, there is an urgent need to empirically evaluate the underlying security architectures that govern drone usage operations for priority logistics. This review examines the significant contribution of artificial intelligence models and blockchain to the development of trustworthy and reliable intelligent and secure autonomous systems by integrating cyberspace, intelligence space, and airspace security. PRISMA-SPIDER methodology was adopted for the systematic review of 133 articles based on the inclusion criteria consisting of 91 (68.4%) quantitative studies, 19 qualitative studies (14.2%), and 23 (17.3%) mixed method studies to balance article selection sensitivity and specificity. The review outcome shows a significant disconnect between model proposals and actual implementation. Through the incorporation of zero-trust architecture into the existing blockchain technology and the convergence of newer AI models, dynamic security issues like drone ownership authentication, drone package delivery verification, drone operation authorization, and drone jurisdiction accountability, can be achieved seamlessly for secure smart mobility via drone transportation systems."
AI-Driven Stroke Rehabilitation Systems and Assessment: A Systematic Review,S. Rahman; S. Sarker; A. K. M. N. Haque; M. M. Uttsha; M. F. Islam; S. Deb,10.1109/TNSRE.2022.3219085,2023,"Post-stroke therapy restores lost skills. Traditionally, patients are supported by skilled therapists who monitor their progress and evaluate the program’s effectiveness. Due to a shortage of qualified therapists, rehabilitation facilities are both expensive and inadequate. Furthermore, evaluations may be subjective and prone to errors. These limitations motivate the researchers to devise automated systems with minimal human intervention, therapist-like assessment, and broader outreach. This article reviews seminal works from 2013 onwards, qualitatively and quantitatively adapting the PRISMA approach to examine the potential of robot-assisted, virtual reality-based rehabilitation and automated assessments through data-driven learning. Extensive experimentation on KIMORE and UI-PRMD datasets reveal high agreement between automated methods and therapists. Our investigation shows that deep learning with spatio-temporal skeleton data and dynamic attention outperforms others, with an RMSE as low as 0.55. Fully automated rehabilitation is still in development, but, being an active research topic, it could hasten objective assessment and improve outreach."
"Datasets, Machine Learning Recognition Performance, and Key Issues in Nursing Care Activity Recognition: A Systematic Review",Y. Yoshida; H. Miwa; S. Nakae; S. Ogiso; R. Ichikari; T. Okuma,10.1109/ACCESS.2025.3550579,2025,"Countries experiencing rapid aging face severe caregiver shortages, necessitating effective workforce optimization. Nursing care activity recognition (NCAR) offers a promising solution by automatically detecting caregiving tasks over time and visualizing workflows. Based on this information, NCAR can facilitate improvements in caregiving processes. This review systematically assessed 32 studies following PRISMA guidelines, analyzing dataset characteristics and recognition performance of algorithms in NCAR to identify factors influencing recognition accuracy. The review revealed significant challenges, including a lack of publicly available datasets, class imbalance, and decreased recognition accuracy in real-world environments. Additionally, sensor configuration and annotation quality were found to significantly impact recognition performance. The limited availability of field data restricts model generalization, highlighting the need for novel algorithms and efficient data collection methods. Expanding datasets, designing appropriate classes, and improving algorithms are critical steps for advancing NCAR research and practical applications. The findings of this study demonstrate the potential of NCAR in optimizing caregiving workflows and mitigating workforce shortages."
Improving Programmer Work Quality with ChatGPT Assistance,A. D. Rumondor; A. Abimanyu; M. Abiyyu’Ammaar; S. Achmad; R. Sutoyo,10.1109/CHIuXiD64022.2024.10860622,2024,"The potential of artificial intelligence as a tool that can be an assistant for humans is very close. This is reinforced by the breakthrough of a generative model that is considered to be able to help many tasks that are usually done by humans, namely ChatGPT. ChatGPT successfully demonstrated its potential by providing complete and intact code generation. ChatGPT, with GPT-3, is one of many language models currently available for people. In previous research, ChatGPT was tested to see if it could provide appropriate programming code results and perform large-scale data analysis on a specific dataset described in sentence form. The limitations of ChatGPT include understanding and generating code, which requires further training data. This opportunity positions ChatGPT as a provider of programming materials and a place to seek solutions to problems faced by programmers. This paper investigates the capabilities and also limitations of ChatGPT to help with the works of programmer’s daily tasks by examining overall ChatGPT capabilities within the assigned environment as a Programmer Assistant and examining the overall probability of ChatGPT limitations and failures to generate the results from specified prompts from the programmer. This study formulated the research question and performed the PRISMA step to answer the question. This study found significant potential for ChatGPT as a programmer’s assistant, catering to requests related to programming and design, and also functions as a tool for designing programming code to be implemented in software development."
Review and Empirical Analysis of Machine Learning-Based Software Effort Estimation,M. Rahman; H. Sarwar; M. A. Kader; T. Gonçalves; T. T. Tin,10.1109/ACCESS.2024.3404879,2024,"The average software company spends a huge amount of its revenue on Research and Development (R&D) for how to deliver software on time. Accurate software effort estimation is critical for successful project planning, resource allocation, and on-time delivery within budget for sustainable software development. However, both overestimation and underestimation can pose significant challenges, highlighting the need for continuous improvement in estimation techniques. This study reviews recent machine learning approaches employed to enhance the accuracy of software effort estimation (SEE), focusing on research published between 2020 and 2023. The literature review employed a systematic approach to identify relevant research on machine learning techniques for SEE. Additionally, comparative experiments were conducted using five commonly employed Machine Learning (ML) methods: K-Nearest Neighbor, Support Vector Machine, Random Forest, Logistic Regression, and LASSO Regression. The performance of these techniques was evaluated using five widely adopted accuracy metrics: Mean Squared Error (MSE), Mean Magnitude of Relative Error (MMRE), R-squared, Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE). The evaluation was carried out on seven benchmark datasets: Albrecht, Desharnais, China, Kemerer, Mayazaki94, Maxwell, and COCOMO, which are publicly available and extensively used in SEE research. By carefully reviewing study quality, analyzing results across the literature, and rigorously evaluating experimental outcomes, clear conclusions were drawn about the most promising techniques for achieving state-of-the-art accuracy in estimating software effort. This study makes three key contributions to the field: firstly, it furnishes a thorough overview of recent machine learning research in software effort estimation (SEE); secondly, it provides data-driven guidance for researchers and practitioners to select optimal methods for accurate effort estimation; and thirdly, it demonstrates the performance of publicly available datasets through experimental analysis. Enhanced estimation supports the development of better predictive models for software project time, cost, and staffing needs. The findings aim to guide future research directions and tool development toward the most accurate machine learning approaches for modelling software development effort, costs, and delivery schedules, ultimately contributing to more efficient and cost-effective software projects."
Factors Affecting the Project Performances: A Post-Pandemic Scenario Towards Sustainability,H. H. Shaikh; N. Y. Zainun; S. H. Khahro,10.1109/ICIKM59709.2023.00012,2023,"The construction industry has a well-known industry and has major contributions to the growth of the country's economy. During COVID-19 construction industry suffers globally around world and had a high impact on project performance. This paper aims to investigate factors that highly affected the project performance during COVID period in the construction industry in Pakistan. Extensive literature was reviewed to identify the factors that affect the project performance of the construction industry in Pakistan. Questionnaire survey forms were designed and sent to employees working with different stakeholders. Data were analyzed using Average Index (AI) method. The results show four major factors highlighted by all stakeholders that affected the project and impacted its performance during COVID-19 (1) Un-safe Working Environment; (2) Shortage of material; (3) Shortage of funds; and (4) Increasing project cost. The stakeholders should revise the specification and contract to manage the project performance according to factors. The stakeholders should implement the rules and regulations mentioned by the government to provide a safe environment on construction sites. The client and government should support the contractor and consultant stakeholders by allowing to use of alternate available material to completion of a project and revise the project budget to manage and run a project during COVID-19 period. By taking early steps from stakeholders, project performance could manage during COVID-19 period in the construction industry of Pakistan."
An Empirical Study on Various Workflow Scheduling Techniques in Cloud Computing,M. G; R. Suma,10.1109/ACET61898.2024.10730234,2024,"Cloud computing (CC) is a modern technology which deploys networks of servers, positioned in extensive remote areas, for conducting operations on a larger quantity of data. In CC, a workflow module is employed for representing diverse scientific and web applications. One of the major problems is scheduling larger workflows of tasks on the heterogeneous CC environment. This article summarizes some important research works in the field of CC associated with workflow scheduling (WS). At first, this survey represents the common structure of WS in cloud and then, scheduling approaches are catalogued into four techniques like Machine Learning (ML), Deep Learning (DL), Hybrid and Algorithmic-based approaches are discussed in detail. Thereafter, the research gaps faced by the traditional approaches for WS will help the researchers in future for building an innovative model. This study considers 25 research articles focused on different methodologies employed for WS in cloud. Lastly, the evaluation is performed in the survey regarding year of publication, performance measures, research technique, and attainment of the research models. It is acknowledged that algorithmic based modules are usually employed approaches, while cloudsim is repeatedly applied tool for WS in CC. Moreover, makespan is the utilized analytic measure with superior value."
Analyzing Factors That Influence Offshore Outsourcing Decision of Application Maintenance,H. U. Rahman; M. Raza; P. Afsar; H. U. Khan; S. Nazir,10.1109/ACCESS.2020.3029501,2020,"Application maintenance consumes a considerable amount of an organization's time and resources each year. Almost 60% of IT budget is spent alone on application maintenance. The reason of offshore outsourcing of application maintenance is not only the reduction of maintenance cost but to free up the resources and to keep the focus on core products. Offshore outsourcing is a common business strategy that is used by companies to achieve cost savings about 20-50%. However, the decision making process of application maintenance is a complex phenomenon. It is based on a set of influencing factors, clients' requirements and nature of the project. Hence, the current study is aimed at the in-depth investigation of the complex sourcing decision process of application maintenance. Accordingly, a systematic literature review is performed to determine the influencing factors and critical success factors that will be used by the decision makers for the evaluation of projects before making the outsourcing decisions. A total of 15 influencing factors out of 52 selected papers were identified. Based on the defined criteria, amongst the identified factors, only 10 factors were ranked as critical success factors, which are employees' skills, cost, legal requirements, infrastructure, communication, knowledge transfer, maturity level, project management, language barrier and frequent requirements changes. Consequently, a sourcing model was proposed based on the identified critical success factors that help the IT managers and domain experts in making appropriate outsourcing decisions."
Advances in Electroencephalography for Post-Traumatic Stress Disorder Identification: A Scoping Review,J. A. Salazar-Castro; D. H. Peluffo-Ordóñez; D. M. López,10.1109/OJEMB.2025.3538498,2025,"Background: Post-traumatic stress disorder (PTSD) is a psychophysiological condition caused by traumatic experiences. Its diagnosis typically relies on subjective tools like clinical interviews and self-reports. Objectives: This scoping review analyzes computational methods using EEG signal processing for PTSD diagnosis, differentiation, and therapy. It provides a comprehensive overview of the entire EEG analysis pipeline, from acquisition to statistical and machine learning techniques for PTSD diagnosis. Methods: Using the PRISMA-ScR protocol, studies published between 2013 and 2024 were reviewed from databases including Scopus, Web of Science, and PubMed. A total of 73 studies were analyzed: 52 on diagnosis, 8 on differentiation, and 15 on therapy. Results: EEG Bands and Event-Related Potentials (ERP) were the dominant techniques. The Alpha band demonstrated strong performance in diagnosis and therapy. LPP ERP was most effective for diagnosis, and P300 for differentiation. Supervised SVM models achieved the highest accuracy in diagnosis (ACC = 0.997), differentiation (ACC = 0.841), and psychotherapy (ACC = 0.78). Random Forest multimodal models integrating EEG with other modalities (e.g., ECG, GSR, Speech) achieved ACC = 0.993. Unsupervised approach is employed to cluster patients to identify PTSD subtypes or to differentiate PTSD from other mental disorders. Veterans and combatants were the primary study population, and only three studies reported open datasets. Conclusions: EEG-based methods hold promise as objective tools for PTSD diagnosis and therapy. The review identified limitations in the use of ERP, sleep characterization and full-band EEG. Broader datasets representing diverse populations are essential to mitigate bias and facilitate robust inter-model comparisons. Future research should focus on deep learning, adaptive signal decomposition, and multimodal approaches."
Healthcare Facility Location Selection: A Bibliometric Analysis and Scoping Review,R. Fourie; S. S. Grobbelaar,10.1109/IEEM55944.2022.9989879,2022,"Selecting suitable locations for healthcare facilities hinges on various selection criteria that requires multidimensional or multi-criteria decision-making (MCDM) methods. This study aims to present a scoping review and bibliometric analysis by exploring methods employed for the healthcare facility location selection problem. The Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews (PRISMA-ScR) is used as a reference framework. Articles of interest were retrieved by searching electronic databases, including Scopus, PubMed, Web of Science, and Science Direct, up to early 2022. A final database was compiled based on inclusion and exclusion criteria, after which a bibliometric analysis was executed to answer specific research questions. The full text of the papers was studied to identify which mathematical, machine learning (ML), or MCDM methods were employed and which MCDM environments were used. The results concluded that Analytical Hierarchy Process (AHP) combined with Geographical Information System (GIS) are the most prolific method when selecting a location for healthcare facilities, fuzzy triangular numbers are the most employed, and fuzzy and grey MCDM environments are preferred as they take uncertainty into account."
Systematic review of a voice-to-Ecuadorian sign language translator,C. R. Salamea Palacios; F. J. Viñanzaca Figueroa; M. A. Peralta Marin,10.1049/icp.2025.1252,2025,"In Ecuador, deaf people are facing several communication barriers due to the lack of tools for sign language translation. This study aims to make a systematic review of the current state of the field in Ecuador, analyzing how technologies have been applied to other translators in other countries, to have an approach of what is missing in Ecuador. Using the PRISMA methodology, searches were conducted in IEEE Xplore and Scielo. These findings highlight the use of artificial intelligence in these translators, also including 3D models, where most of this research is focused on India and the United States. Valuable information for the future development of a sign language translator for Ecuador."
An Empirical Investigation on Software Cost Estimation Techniques and Barriers on Agile Software Development in Software Industry of Pakistan,F. Aizaz; U. I. Janjua; H. Zafar; J. A. Khan; I. Kazim,10.1109/FIT53504.2021.00044,2021,"Software cost estimation is very crucial for the success of the software. For this reason, many studies have been conducted to identify various cost estimation techniques and barriers affecting the software estimation process in traditional development. However, these days, most software development organizations are working following agile methodologies. Therefore, there is a need to identify the critical success factors and barriers of cost estimation for agile methodology followers. In this study, a systematic literature review and survey is conducted to determine the crucial success factors and barriers of cost estimation. Therefore, this study might help an agile project manager plan the cost and effort of the software project by getting an idea from the list we have provided after validating the techniques used from the state of the art and industry so that we can come up with practical guidelines."
Machine Learning-based Software Effort Estimation: An Analysis,Z. Polkowski; J. Vora; S. Tanwar; S. Tyagi; P. K. Singh; Y. Singh,10.1109/ECAI46879.2019.9042031,2019,"Estimating the effort behind a software project is the approximation time and resources an engineer need to create a software application. The estimation is one of the most important phase in the developing process to set the cost of project and ultimately to attract the client. In the preliminary stage of a project, the accuracy of estimation is to be extremely precise and dependable, which may not be easy to achieve. Therefore, use of machine learning algorithms is a possible solution for the estimation process on which the decision can be made. In this study, we have analyzed various studies and machine learning trends conducted in this field. Doing this effective reductions in the cost and parameter for the project to be accomplished. Accuracy, root mean and relative absolute errors are used to compute the effort estimation accuracy."
Sentiment analysis methods for politics and hate speech contents in Spanish language: a systematic review,E. del Valle; L. de la Fuente,10.1109/TLA.2023.10068844,2023,"The political debate in social networks, and its derivatives such as hate speech, has surfaced at the top of the social agenda due to its impact on public opinion and, consequently, in the communication strategies of political parties, public institutions, media corporations, and lobbies. The scientific community has been working to respond to the demand for tools that allow studying the political attitude of citizens in these networks, focusing on sentiment analysis methodologies. However, their work has been hampered by several significant challenges, such as the absence of standardized investigation methodologies, the filtering of content created by bots and spammers, or the interpretation of slang and other conventionalisms that are specific to microblogging platforms. In addition to these challenges and the generic problems related to the interpretation of human language, researchers from the Spanish-speaking community have found themselves with the additional problem of developing strategies and methodologies suitable for Spanish text, in a scenario dominated by research aimed at the English language. In this paper, we present a systematic review that describes the state of the art in sentiment analysis methods for politics and hate speech contents in the Spanish language, by systematically reviewing the relevant papers available."
Natural Language Processing in Solving Resource Constrained Project Scheduling Problems,S. Ghai; S. Lakhanpal; B. Ramola; R. R; M. Al-Taee; M. B. Alazzam,10.1109/ICACITE57410.2023.10182623,2023,"This paper investigates the potential of using Natural Language Processing (NLP) in resolving one of the crucial issues in project management - Resource Constrained Project Scheduling Problems (RCPSP). The conventional methods used to solve RCPSP involve mathematical models that require structured data inputs, which can be time-consuming and may lead to errors. The authors of the paper aim to demonstrate the benefits of using NLP to extract and process project data from unstructured sources, such as natural language written project plans, to improve the accuracy and efficiency of scheduling solutions. The study concludes that NLP can play a significant role in solving RCPSP and provides new avenues for future research in this field. The paper contributes to the literature by highlighting the importance of NLP in project scheduling and its potential to provide more effective solutions for resource-constrained projects."
"Application of Artificial Intelligence to Reduce Production Shortcomings, Reworks and Returns, Logistics Errors",M. Tshihwela; N. Y. Mulongo,10.1109/ISKE60036.2023.10481271,2023,"The advent of artificial intelligence within the manufacturing industry has been a major swift and is the philosophy of machines to reason, conduct, and achieve either the same output as or comparable to mankind. Numerous flaws and faults that impair product quality and, as a result, lower sales occur during manufacturing and long-product production. Reworking is the wasteful attempt involved in redoing a procedure or action that was done wrong the first time. This study adopts the PRISMA approach to critically review artificial intelligence as a tool to detect products defects, and to help on the returns and reworks, and eliminating errors in the logistics sector. The result from the database focuses more on the current application of artificial intelligence in the manufacturing industries of metal and are mostly based on eliminating products defects, products returns and reworks and eliminating logistic errors also enhancing relationships between the suppliers and the customers."
Systematic Review of Extended Reality for Smart Built Environments Lighting Design Simulations,E. Mohammadrezaei; S. Ghasemi; P. Dongre; D. Gračanin; H. Zhang,10.1109/ACCESS.2024.3359167,2024,"This systematic literature review paper explores the use of extended reality (XR) technology for smart built environments and particularly for smart lighting systems design. Smart lighting is a novel concept that has emerged over a decade now and is being used and tested in commercial and industrial built environments. We used PRISMA methodology to review 270 research papers published from 1968 to 2023. Following a discussion of historical advances and key modeling techniques, a description of lighting simulation in the context of extended reality and smart built environment is given, followed by a discussion of the current trends and challenges."
Future Job Market of Information Technology in the Kingdom of Bahrain,A. AlMosawi; H. Al-Arayedh; L. Aljasmi; J. Khalifat; A. Alrayes; A. Alqaddoumi; O. Catuiran; A. Alomary,10.1109/3ICT51146.2020.9312017,2020,"The college of Information Technology at University of Bahrain conducted a job market study to investigate the Information Technology job market needs at the Kingdom of Bahrain. Twelve fields have been identified to be the most needed by the global IT job market. The most important job roles have been identified for each field. An online survey has been distributed to employers at different key organizations in Bahrain. The study revealed that the fields of IT Systems and Projects Management, Security, and System Analysis, Design, and Development are the most fields in demand in Bahrain for employment. Required job roles for each of those fields have been also identified."
New Generation and Old Generation Hyperspectral Remote Sensing Data and their Comparisons with Multispectral Data in the Study of Global Agriculture and Vegetation,P. S. Thenkabail; I. Aneece; P. Teluguntla; A. Oliphant; D. Foley,10.1109/IGARSS46834.2022.9883556,2022,Great advances in remote sensing are taking place with new generation of spaceborne hyperspectral sensors such as the DESIS and PRISMA which are already acquiring data for over a year now and the upcoming launch of EnMAP. Understanding the characteristics of these data for a wide array of applications is of great importance to advance the twenty-first century satellite remote sensing. In this paper we will make a comprehensive assessment of new generation hyperspectral sensors such as the DESIS and the PRISMA and compare the same with the older generation hyperspectral sensors such as the Hyperion as well as with various multispectral sensors in study of major world agricultural crops.
Smart Control Models Used for Nutrient Management in Hydroponic Crops: A Systematic Review,P. Catota-Ocapana; C. Minaya-Andino; P. Astudillo; D. Pichoasamin,10.1109/ACCESS.2025.3526171,2025,"In recent years, agriculture has significantly evolved with the integration of technology, enabling the development of new cultivation techniques that respond to the growing demand for food and the need to conserve natural resources. In this context, we conducted a comprehensive review of models of intelligent control for managing nutrients in hydroponic systems by analyzing studies from the last five years. The selection of articles was based on the guidelines of PRISMA and research questions, focusing on control techniques based on fuzzy Logic, Artificial Intelligence and artificial Vision. These models are essential to automatically adjust the concentrations of nutrients, adapting to the needs of the plants at each stage of their growth. The review results highlight essential advances but also identify significant challenges, such as the need for precise sensors, the management of large volumes of data, and adapting the models to different crops and conditions. Despite these challenges, the benefits include a more efficient use of nutrients, a reduction in the consumption of water, and increased crop yields. Continuous research in this field is essential to improve the sustainability and productivity of hydroponic systems, offering new opportunities for agriculture in the future. The findings of this review provide a solid basis for evaluating the effectiveness of the control models and their application in real agricultural scenarios."
Enabling and Inhibiting Leadership Behaviors in Continuous Improvement,B. A. Lameijer; C. T. Boon; D. H. van Dun; D. N. den Hartog,10.1109/TEM.2025.3546696,2025,"Leaders play a key role in continuous improvement (CI) implementations. However, the research on CI-enabling versus CI-inhibiting leadership behaviors remains fragmented. Our research reviews empirically validate and extend the understanding of CI leadership behaviors by examining low versus high maturity and capability building versus value maximization CI implementations. After providing a literature review focused on leadership behaviors that affect CI implementation processes, we did expert interviews to see whether informants recognized these behaviors. Then, we did a survey among 144 key informants to empirically validate the relative importance of CI leadership behaviors and the moderating effect of CI implementation archetype and maturity level. Robustness analyses comprised instrument variable-based endogeneity treatment and secondary data-based common method bias assessment. No overall direct relationship was found between “command-and-control” behaviors and CI implementation success, although these behaviors did have positive effects depending on the type of CI implementation. Also, “inspirational role modeling, coaching, and empowering” behaviors relate positively to implementation success, especially in low-maturity CI implementations, regardless of their archetype. Unexpectedly, “serving” behaviors were found to relate negatively to implementation success and were mostly relevant for low maturity and economically driven CI implementation archetypes. The results provide a prioritization of significant CI leadership behaviors and a novel measurement tool. We show how these leadership behaviors are contextually contingent upon different CI implementation situations. This mixed-methods study answers calls for more systematic scientific attention to the social components of CI implementation."
Application of Entropy for Automated Detection of Neurological Disorders With Electroencephalogram Signals: A Review of the Last Decade (2012–2022),S. J. J. Jui; R. C. Deo; P. D. Barua; A. Devi; J. Soar; U. R. Acharya,10.1109/ACCESS.2023.3294473,2023,"An automated Neurological Disorder detection system can be considered as a cost-effective and resource efficient tool for medical and healthcare applications. In automated Neurological Disorder detection, electroencephalograms are commonly used, but their low signal intensity and nonlinear features are difficult to analyze visually. A promising approach for processing of electroencephalogram signals is the concept of entropy, a nonlinear signal processing method to measure the chaos in the signal. The aim of this study was to find out the effective entropy measures and the machine learning approaches that produced promising output. Using Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines as our method, we have identified 84 studies published between 2012 and 2022 that has investigated epilepsy, Parkinson’s disease, autism, Attention Deficit Hyperactive disorder, schizophrenia, Alzheimer’s disease, depression, and alcohol use disorder with machine learning approaches considering entropy measures. We show that Support Vector Machines was the most commonly used machine learning model, with consistent performance in most of the studies whereas sample entropy was the most commonly used entropy measure, followed by the approximate entropy. For epilepsy detection, the most used entropy feature was the log energy entropy, whereas the multi-scale entropy was commonly used for Alzheimer’s Disease, approximate and sample entropy used for Parkinson’s Disease, multi scale and Shannon entropy applied for autism, approximate and Shannon entropy used for attention deficit hyperactive disorder, sample entropy used for depression, approximate and spectral entropy adopted for schizophrenia, and the approximate and sample entropy employed for alcohol use disorder. According to the majority of the studies, there is growing concern about the increase in neuro patients and the heavy resource burden that is associated with their prevalence and diagnosis. Based on these studies, we conclude that Computer-Aided Design systems would be economically advantageous in detecting Neurological Disorders. To incorporate Computer-Aided Design system into the mainstream health care system, future research could focus on multi-modal approaches to the disorder and its interpretation and explanation. We believe this is the first review that has combined the electroencephalograms, entropy, and automated detection possibility of the 8 distinct neurological disorders. The study is limited to the papers that used accuracy as their performance evaluation metric. The findings and synthesis of previous studies provides a clear pathway that identifies the entropy approach as a practical solution for automated detection of neurological disorder using electroencephalograms with potential applications in other kinds of signal analysis."
Face Detection to Recognize Students’ Emotion and Their Engagement: A Systematic Review,B. Abdellaoui; A. MOUMEN; Y. EL BOUZEKRI EL IDRISSI; A. Remaida,10.1109/ICECOCS50124.2020.9314600,2020,"Following the Coronavirus COVID pandemic 19, many countries have adopted distance education to ensure pedagogical continuity for which they have not been ready yet to deal with. So, to reduce the spread of the virus, since March 2020, Morocco has called for state health emergency and imposed strict confinement on the population. In response to this, all training establishments have been locked down. In such a context of crisis and disability, teachers, students, and their families have found themselves in a delicate situation to manage, where new distance learning needs and practical training have been imposed without adequate measures for conducting an education as an integral part of a normative academic curriculum. In this article we will discuss the teachers' inability to asses the students' emotional state and determine their engagement which can be clearly read from their faces in normal teaching situation. It aims at analyzing and responding to problems related to the detection of faces to automatically deduce emotions and study student engagement while exploring and analyzing the references available on the some databases cited in the article. This literature review is supplemented by thematic analysis and a meta-analysis of the corpus. This study has identified both quantitative and qualitative and/or experimental work."
Construction Project Management Using Natural Language Processing Technology,M. Balasubramani; M. R. Palav; K. T. V. Rao; R. Maranan; A. R. Salve; R. A. M,10.1109/IC3I59117.2023.10397999,2023,"The effectiveness of the structure business is generally because of innovative headways. Utilizing an intelligent thought of natural language processing innovation, this study expects to determine correspondence cooperation issues to foster a stage for project management. This paper looks at the review and execution of Natural Language Processing (NLP) innovation in the management of construction projects, determined to resolve issues like the developing size and volume of construction projects and records, the intrinsic trouble and intricacy of their management, the tedious and blunder inclined nature of manual resemblance and acceptance, and the absence of mastery among experts to give complete, opportune, and precise conferences. The most predominant issues in design can be tended to by changing over unstructured reports into organized data."
Study of Learning Techniques for Effort Estimation in Object-Oriented Software Development,S. Shukla; S. Kumar,10.1109/TEM.2022.3217570,2024,"Software effort estimation (SEE) is helpful for project managers to decide the cost and effort needed to complete the project. The techniques used for estimating effort in conventional software development are not very useful for estimating effort in object-oriented projects because of their varying nature. The machine learning (ML) approaches are achieving greater recognition in SEE research because they can demonstrate the complex relationship between software effort and other attributes. So, there is a need for a systematic literature review (SLR) that can discuss the applicability of ML techniques for SEE in object-oriented projects, which is not available in the literature. This research aims to provide a specific review and analysis of various ML-based SEE works in the object-oriented software development (OOSD) paradigm based on different perspectives: type of learning technique used, type of performance measure used, performance level achieved, the dataset used, etc. Purposefully, we have chosen appropriate articles after applying selection and quality evaluation criteria. After investigation, we found that different ML techniques have been applied in these works, and their performance is better than the classical models. Hence, more efforts are needed to encourage the application of ML techniques for SEE in the OOSD paradigm. Also, most of the works have used small-sized datasets for effort estimation in OOSD, due to which their model's performance cannot be generalized. So, the researchers should collect more large-sized datasets working in line with the software organizations."
The Opportunism-Inhibiting Effects of the Alignment Between Engineering Project Characteristics and Contractual Governance: Paired Data From Contract Text Mining and Survey,C. Xu; Y. Chen; H. Yao; L. Zhang,10.1109/TEM.2024.3480254,2024,"Engineering projects are vulnerable to opportunistic behavior due to their one-off and uncertain nature. Contractual governance is the crucial mechanism for matching the two key project characteristics, i.e., asset specificity and uncertainty, to curtail opportunism. However, the existing studies failed to agree on the above matching principle. In this article, we divide contractual governance into control, coordination, and adaptation from the functional perspective and employ machine learning to code actual contract texts. Based on the paired data from the text-mining results and survey, this study uses qualitative comparative analysis to investigate the aligning (or misaligning) combinations that lead to low (or high) opportunism. The results show that, for projects with low uncertainty, detailed contractual coordination is essential and it should be complemented by less detailed adaptation or detailed control. In such projects, low asset specificity reinforces the significance of contractual coordination alone. This study also finds the limitations of contractual governance in projects with high uncertainty, especially combined with low asset specificity, which necessitates other governance mechanisms. This study helps to resolve previous contradictory matching principles from the view of contract dimensions, contract measurement, and data analysis methods. Project managers can benefit from this study to effectively reduce opportunism and avoid disputes."
Applying machine learning methods to understand unstructured information system big data,K. D. Strang; N. Rao Vajjhala,10.23919/CISTI54924.2022.9820116,2022,"Over 50% of all information systems (IS) projects fail around the world. The research question was: Can machine learning (ML) find the failure causes by searching unstructured IS project big data? A pragmatic mixed methods research design was developed. After a literature review, structured programming, random forest ML and parametric statistics were applied to a large big data source containing unstructured IS project metrics. A statistically significant model was created, identifying 7 features from ML at an 80% classification accuracy, and 4 predictors of IS project failure, with a 27% effect size."
Impact of IoT Integration on Enterprise Resource Planning (ERP) Systems: A Comprehensive Literature Analysis,S. Wijesinghe; I. Nanayakkara; R. Pathirana; R. Wickramarachchi; I. Fernando,10.1109/SCSE61872.2024.10550684,2024,"The integration of Internet of Things (IoT) technology with Enterprise Resource Planning (ERP) systems has gained significant attention in recent years. This research study aims to provide a comprehensive analysis of the impact of IoT integration on ERP systems. The study explores the benefits, challenges, and potential solutions associated with combining IoT and ERP. The findings highlight that IoT integration with ERP offers several advantages, such as real-time data collection, improved supply chain visibility, enhanced asset tracking, and predictive maintenance capabilities. These benefits lead to increased operational efficiency, reduced costs, and better decision-making. The integration of IoT with ERP also presents challenges that need to be addressed. These challenges include data security and privacy concerns, IoT traffic, and data management. The research identifies potential solutions and best practices to overcome these challenges. Furthermore, the study discusses the implications of IoT integration on various functional areas of ERP systems, such as healthcare, manufacturing, logistics, inventory management, and customer relationship management. The research methodology includes an extensive review of existing literature and case studies. This research provides valuable insights into the impact of IoT integration on ERP systems, offering guidance for organizations considering already implemented IoT-enabled ERP solutions or currently implementing ERP solutions."
Measures of Organizational Process Performance in the Capability Maturity Model Integration,A. -R. Al-Ghuwairi; D. Al-Fraihat; Y. Sharrab; A. Kittaneh; A. Ali; A. Alsarhan; M. Alkhalidy,10.1109/ACIT58888.2023.10453667,2023,"The Capability Maturity Model Integration (CMMI) is widely utilized by organizations to facilitate process improvement. Among its 22 process areas, this paper focuses on the Organizational Process Performance (OPP) area, which assesses the effectiveness of a process. Specifically, this study examines one goal (establishing performance baselines and models) and two practices (establishing quality and process performance objectives and selecting processes within OPP. To measure these practices, the Goal Questions Metrics (GQM) approach is employed. Data was collected using a questionnaire, and the defined measures are validated quantitatively. 250 responses were used for analysis. The results obtained indicate that implementing the defined measures in companies would be beneficial for achieving high-quality processes in OPP and advancing the maturity level of CMMI."
Urgency Detection of Events Through Twitter Post: A Research Overview,F. W. Edlim; G. Edo; R. K. P. Wiratama; R. Mahmudin; A. Solihin; A. D. P. Ariyanto; D. Purwitasari,10.1109/ICECOS63900.2024.10791202,2024,"Indonesia is located in volatile Pacific fire ring and stands as one of the most global disaster-prone areas. The government and public are in constant vigilance of the potential threats. Social media generates large amounts of data from its users and is one of the prime candidates to be used as a disaster monitoring and response system. Several studies have demonstrated the effectiveness of using such an approach for developing disaster monitoring systems. but due to the number of messages generated, filtering is needed to extract useful information from the message. Hence, it's needed to detect an urgency from a message. This review, which was carried out following the PRISMA model, focused on the detection of the urgency of Indonesian Twitter during crisis events. From studies selected, several recommendations are suggested for future endeavor in this topic. Future systems are advised to employ three or four class schemes rather than binary class to gain more nuanced insights into tweet urgency. On-Event training and testing approaches are suitable for frequently occurring events, whereas out-of-event approaches offer broader applicability for real-world scenarios. Both machine learning and deep learning methods, including multimodal approaches, have demonstrated efficacy in urgency detection. Addressing pre-processing challenges, such as abbreviations and slang, is critical, and ensuring consistency in manually annotated data is essential. Additionally, the selection of appropriate word embeddings, such as fastText, IndoBERT, or crisis-specific embeddings, is crucial for achieving optimal model performance."
A Methodical Investigation into the COVID-19 Risk Prediction Models,S. Deshmukh; S. Kaur,10.1109/IC3I59117.2023.10397898,2023,"As a result of the COVID-19 pandemic, there is an urgent need for precise risk prediction models that can assist with the identification of individuals who are at a high risk of contracting the illness or experiencing severe outcomes. In this article, a comprehensive review of the previous research on COVID-19 risk prediction models has been done in the academic literature. Following the principles laid forth by the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA), we have undertaken this analysis, and it features an exhaustive search of numerous databases The various models are identified, their quality is evaluated, and throughout the process, both their benefits and drawbacks are emphasized. These findings present recommendations for directions that future research should take and provide insights into the current state of COVID-19 risk prediction modeling. In this study, we also provide 10 recent research articles that evaluate a variety of machine learning models applied to healthcare, as well as COVID-19."
A Literature Review on Enterprise Credit Assessment Using Random Forest,H. Guamán-Lloacana; A. Muzo-Bombón; C. Sánchez-Briceño; J. Varela-Aldás,10.1109/ETCM63562.2024.10746188,2024,"This article presents a literature review on enterprise credit assessment using the Random Forest model, distinguishing it from general credit assessment, which includes a broader range of entities. The study highlights the limitations of traditional methods in credit risk evaluation. The primary objective of this research is to assess the technical configurations, predictive capabilities, and ethical considerations of applying Random Forest in credit assessment. Methodologically, a literature review approach guided by PRISMA principles was adopted, focusing on relevant studies published between 2018 and 2024. The findings indicate that Random Forest models enhance predictive accuracy and effectively manage high-dimensional data, outperforming traditional statistical methods. Furthermore, the study emphasizes the need for transparency and bias mitigation in automated credit scoring systems."
"A Systematic Review of Contemporary Indoor Positioning Systems: Taxonomy, Techniques, and Algorithms",J. Singh; N. Tyagi; S. Singh; F. Ali; D. Kwak,10.1109/JIOT.2024.3416255,2024,"Due to the increasing need for accurate location-based services, indoor positioning systems (IPSs) have evolved rapidly. This study reviews the literature published from 2010 to 2024, employing the preferred reporting items for systematic reviews and meta-analyses (PRISMA) methodology for the identification, screening, validation, and inclusion of research literature. By exploring the complexities of IPS methodologies, algorithms, technologies, and challenges, this study offers a comprehensive introduction for researchers, scholars, and specialists. Additionally, the scope of this study expands its focus to encompass mapping techniques, such as crowdsourcing, geographic information systems (GISs), remote sensing, LiDAR, cartography, and augmented reality (AR). The findings of this study will contribute to the increased applicability of these methods and techniques in the field of urban planning and environmental management. Contributing to the expanding pool of knowledge on indoor positioning, this work is a valuable resource for those exploring the ever-changing field of IPS. This work not only contributes to the academic but also bridges the gap between scientific discoveries and practical, real-world applications."
A Critical Analysis on Vertebra Identification and Cobb Angle Estimation Using Deep Learning for Scoliosis Detection,R. Kumar; M. Gupta; A. Abraham,10.1109/ACCESS.2024.3353794,2024,"Scoliosis is a complicated spinal deformity, and millions of people are suffering from this disease worldwide. Early detection and accurate scoliosis assessment are vital for effective clinical management and patient outcomes. The Cobb Angle (CA) measurement is the most precise method for calculating scoliotic curvature, which plays an essential role in diagnosing and treating scoliosis. This letter has conducted a systematic review to analyze scoliosis detection by vertebra identification and CA estimation using the Preferred Reporting Item for Systematic Review and Meta-Analysis (PRISMA) guidelines. The major scientific databases such as Scopus, Web of Science (WoS), and IEEE Xplorer are explored, where 2017–2023 publications are considered. The article selection process is based on keywords like “Vertebra Identification,” “CA Estimation,” “Scoliosis Detection,” “Deep Learning (DL),” etc. After rigorous analysis, 413 articles are extracted, and 44 are identified for final consideration. Further, several investigations based on the previous work are discussed along with its Proposed Solutions (PS)."
Managing and Composing Teams in Data Science: An Empirical Study,T. Aho; T. Kilamo; L. Lwakatare; T. Mikkonen; O. Sievi-Korte; S. Yaman,10.1109/BigData52589.2021.9671737,2021,"Data science projects have become commonplace over the last decade. During this time, the practices of running such projects, together with the tools used to run them, have evolved considerably. Furthermore, there are various studies on data science workflows and data science project teams. However, studies looking into both workflows and teams are still scarce and comprehensive works to build a holistic view do not exist. This study bases on a prior case study on roles and processes in data science. The goal here is to create a deeper understanding of data science projects and development processes. We conducted a survey targeted at experts working in the field of data science (n=50) to understand data science projects’ team structure, roles in the teams, utilized project management practices and the challenges in data science work. Results show little difference between big data projects and other data science. The found differences, however, give pointers for future research on how agile data science projects are, and how important is the role of supporting project management personnel. The current study is work in progress and attempts to spark discussion and new research directions."
Securing the Metaverse: A Bibliometric Analysis of Cybersecurity Challenges and Research Trajectories,K. Kostelić; D. Etinger,10.1109/EMR.2024.3453974,2024,"As an emergent virtual shared environment, the metaverse offers a novel approach to digital communication. However, its rapid expansion and development raise complex cybersecurity challenges. To address this issue, the goal of this paper is to map the scholarly contributions of cybersecurity within the metaverse through a comprehensive bibliometric analysis and examine trends, gaps, and future research directions. Following the PRISMA guidelines, a broad range of documents is included in the analysis, and a bibliometric software package was used for science mapping. The results reveal a significant increase in publications and citations in recent years, with contributors from across the globe. The analysis of keywords and abstracts reveals core themes and citation analysis identifies the most prominent sources and articles. The thematic analysis identifies the existing and emerging areas of research that remain to be explored in future studies."
Design Recommendations for Gate Security Systems and Health Status: A Systematic Review,A. M. Almuhaideb; M. Elhussein; R. Osman; F. AlHolyal; L. AlGhamdi; M. Al-Ismail; M. Alawami; Z. Kadour; R. Zagrouba,10.1109/ACCESS.2023.3335115,2023,"Gate security systems use authentication methods to operate hardware components that grant or deny access to restricted areas. Each context has specific requirements to determine user admissibility. There are currently no design recommendations available for these systems despite their significance. Most research proposes designs based on their recommended authentication scheme without providing general guidance on constructing these systems. This study follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines to conduct a systematic literature review, focusing on recent smart gate research. Studies published between 2016 and 2023 are analyzed and evaluated to identify their main components and authentication schemes. A total of 52 studies published in various journals and conferences are collected. After conducting the review, three main design themes are identified: smartphones, tags, and biometrics. These themes are the focal point of the study. Of all the designs, 66% consider using only one-factor authentication. These designs primarily rely on biometric-based methods. During the COVID-19 crisis, some designs used biometric authorization instead of identity authentication to incorporate health status, with a focus on detecting whether the person wore a face mask and had a normal body temperature. Furthermore, the review reveals that most studies disregard the system’s hardware components and focus on authorization. Additionally, only 25% of the studies conduct an implementation for their design and produce results evaluating their performance. The study concludes that a successful smart gate design must consider and balance cost, usability, and security. Furthermore, health status needs to be verified as an additional layer of protection after determining the existing authentication requirements."
Technologies of Industry 4.0 and Opportunities They Create for Product Innovation,M. W. Condry,10.1109/EMR.2023.3315657,2023,"The technologies that enable products and services for Industry 4.0 create many new products and services for industrial, professional, and consumer markets. These are growing rapidly across multiple markets. With all this rapid change, it is a challenge for developers to “keep up” so that their products and services are current when entering the market. This article intends to overview the elementary concepts for the practical reader, scoping out some of the potential product and service areas enabled by these technologies. The key takeaway is that the technical concepts behind Industry 4.0 apply to the industrial markets as well as many others, including consumer, medical, and areas outside of industry solutions."
Identifying Critical Dimensions for Project Success in R&D Environment Using Delphi Study and Validation Techniques,H. Hanif; A. Hanif; A. Ahsan; A. S. Sadiq; S. Mirjalili; B. Alkazemi,10.1109/ACCESS.2021.3112112,2021,"In the current century, organizations face ever increasing dynamic ecosystems and are constantly devising strategies to meet their challenges. These include the implementation of the right organizational structure and avoid project schedule delays to achieve projects’ success. Unfortunately, the classification of significant project success dimensions in the R&D environment is still an elusive concept. This study adopts a multi-dimensional qualitative and quantitative approach to explore the critical dimensions of organizational structure and schedule management that enhance or hinder the project success in R&D organizations. In Phase 1, a Delphi Study is conducted, results of reliability and other tests are the input of Phase 2. On the basis of these tests, variables have been selected for the next phase or final questionnaire. In Phase 2, through a survey of 285 responses in a R&D environment, the proposed framework is validated by conducting face, content and construct validity. The results indicated that formalization, specialization, differentiation, coordination mechanism, decentralization and authority of managers have a significant effect on the schedule management and successful execution of R&D projects; whereas, centralization and departmentalization do not correlate strongly. The results also imply that decentralized organizational structures (organic) are more preferable than centralized structures (mechanistic) for the execution of R&D projects when proposed timelines are to be met timely. The proposed framework will act as a supporting mechanism for engineering managers to deal with organizational structure and schedule management factors in a highly uncertain R&D environment where projects deviate frequently from their anticipated timeline."
Literature Review on Constraint Satisfaction Problems Solving,E. Qefalija; H. Snopce; A. Dermaku,10.1109/ISMSIT63511.2024.10757207,2024,"Problems known as Constraint Satisfaction issues (CSPs) are the ones on which every variable needs to be given e certain value in order to satisfy all the given constraints. In real life, many requirements such as resource allocation, scheduling, and planning, revolve around these Constraints Satisfaction Issues. This review offers a thorough rundown of the main approaches used to solve CSPs up to date, including heuristic techniques, contemporary and classical algorithms, and underlying theories for such issues. It focuses on specific CSPs and their solving methods, looks at real-life uses, and looks at potential paths for further study. The paper focuses on how the efficiency and scalability of CSP solvers are continuously being improved by developments in methods like constraint programming, SAT solvers, and machine learning"
Comparative Study of the Impact of Robot-Human Integration in the Context of Learning Scenarios: Literature Review,S. Ouafa; Y. Nadir; M. Qbadou; K. Mansouri,10.1109/IRASET60544.2024.10548633,2024,"The integration of robotics into educational settings has witnessed a significant surge in recent years, presenting a transformative shift in traditional learning paradigms. As technology continues to advance, robots are increasingly finding their place in classrooms and virtual learning environments, sparking a growing interest in understanding their impact in the learning process. This comparative study delves into the multifaceted aspects of the integration of robots and humans within the context of learning scenarios, by shedding light on the various effects on student engagement, learning outcomes, and the roles of educators. The aim of this paper is to review the literature of the integration of robotic agents in a learning context. To this end, the PRISMA method was used to exhaustively select a list of articles studying the subject. In the end, a total of 10 articles were retained using multiple selection criteria the PRISMA method, and an experiment was proposed to demonstrate the impact of a verbal interaction scenario between a NAO robot and a group of children."
Early Software Defects Density Prediction: Training the International Software Benchmarking Cross Projects Data Using Supervised Learning,T. Tahir; C. Gencel; G. Rasool; T. Umer; J. Rasheed; S. F. Yeo; T. Cevik,10.1109/ACCESS.2023.3339994,2023,"Recent reviews of the literature indicate the need for empirical studies on cross-project defect prediction (CPDP) that would allow aggregation of the evidence and improve predictive performance. Most empirical studies predict defects at granularity levels of method, class, file, and module/package during the coding phase, and thereby avoid external failure costs. The main goal of this study is to perform an empirical study on early defect prediction at the beginning of a project at the product level of granularity for using it as input in planning quality activities of the project. Hence, both internal and external failure costs could be avoided as much as possible through proper planning of quality. We first made a systematic mapping study (SMS) on secondary studies (literature reviews) on defect prediction to identify the most used datasets, the project attributes and metrics utilized as estimators, and the supervised learning methods employed for training the data. Then, we made an empirical study on defect density prediction using cross-project data. We collected 760 project data from the International Software Benchmarking (ISBSG) dataset version 11, which reported both defects and functional size attributes. We trained the prediction models using: i) the complete set of project attributes, ii) the individual attributes, and iii) multiple subsets of attributes. We employed classification and regression approaches of machine learning. The machine learning models are trained using original values of the dataset, and z-score and logged transformations of original values to explore the effects of data normalization on prediction. Most machine learning models trained on the z-score transformation of the dataset performed best for classifying defects. The Multilayer-Perceptron (Neural Network) model trained on the z-score transformation of complete dataset predicted defects with the highest F1-score of 0.89 using binary classification. The logged transformation and feature selection methods improved the results for multivariable regression. The multivariable regression predicted defects with the highest Root Mean Squared Error (RMSE) and R2 (r-squared) values of 0.4 and 0.9, respectively, with a subset of 11 features using logged transformation. The results of classification and regression approaches indicate that defects can be predicted with reasonable accuracy at the software product level using cross-project data."
Stoppage to the Implementation of Internet of Things (IoT) in Construction Projects: from Lens of Civil Engineers in the United Kingdom,N. Zainordin; S. Salleh; N. Farsana; S. L. Khoo; A. F. Omar; P. D. Heka Ardana,10.1109/AiDAS63860.2024.10730249,2024,"This research intends to examines the barriers faced by Civil Engineers in the United Kingdom when implementing Internet of Things (IoT) technology in construction projects. The study aims to identify and analyze the barriers to IoT integration, and provide valuable recommendations. The research methodology includes a systematic literature review (SLR) to explore existing research, a survey conducted via Mentimeter to gather opinions from professionals, and analysis using the Relative Importance Index (RII) method. The comprehensive literature review acknowledges the advantages of IoT technology while also recognizing subjective aspects. The survey, targeting 680 professionals, however, only 10% will come back as respond on identification of barriers to the implementation of internet of things (IoT) in construction projects an insights from Civil Engineers in the United Kingdom. The primary findings highlight the importance of overcoming barriers such as lack of knowledge, shortage of skilled labor, management challenges, and reliance on technology. There are 20 items of barriers has been identified from this study. By comparing the SLR and survey findings, a comprehensive understanding of the barriers is provided. The study also underscores the value of quantitative research techniques for gaining deeper insights and explores the potential of alternative survey platforms. These conclusions serve as a foundation for further research in this evolving field and contribute to deeper understanding of IoT adoption in the construction industry from Civil Engineers in the United Kingdom perspectives."
"AI and BIM-based Construction defects, rework, and waste optimization",P. Desai; S. Sandbhor; A. Kaushik,10.1109/ESCI56872.2023.10099726,2023,"Any country's economic progress, especially economic expansion, depends heavily on the building industry. The construction industry consumes tremendous amount of money, time and energy. Over the last two decades, many reports and studies have concluded that quality and productivity are decreasing due to defective work. The corrective activities of addressing defects and rework, consume time and cost. Even though there are several reasons for cost and time overrun, rework has a significant effect. It is essential to emphasize the impact of construction rework, construction defects and waste generated through research. Use of soft computing methods is recommended to increase the general efficiency of the construction projects. The goal of this research study is to undertake a bibliographic survey of the relevant literature on construction rework, construction defects and the application of Artificial Intelligence (AI) and Building Information Modeling (BIM) to optimize the output. The time considered for this survey is from the year 2007–2022. This bibliographic analysis contains statistics on citations, important journals, countries, authors contributing to the domain of knowledge based on search engine on Web of Science database. The results of the study highlight the current publication trends emphasizing on the necessity of applying BIM-based AI techniques to defects, rework and waste management. Study shows the quantum of work done in the domain from Indian context, highlighting the need for research."
A Novel LSTM-Based Approach to PERT Analysis in Construction Project Management,D. K. K; S. P. S; K. P; S. G. G,10.1109/ICAIT61638.2024.10690559,2024,"In order to anticipate Optimistic (O), Pessimistic (P), and Most Likely (M) values, this research study introduces a novel technique to PERT (Project Evaluation and Review Technique) analysis in construction projects using Long Short-Term Memory (LSTM) models. Recurrent neural networks (RNNs) of the Long Short-Term Memory (LSTM) variety are highly proficient in processing sequential input, which makes them perfect for modeling the intricate temporal connections present in building projects. For construction projects to be completed successfully, precise cost and schedule estimation is essential. The study focuses on project cost and schedule forecasting using LSTM, which is a crucial part of PERT analysis. Because the LSTM model can capture the long-term dependencies in project data, it can yield predictions that are more reliable and accurate than those produced by traditional statistical techniques. This research improves the field by demonstrating how LSTM may enhance project planning and management and offer a more sophisticated tool for decision-making in construction projects. This study provides a potential direction for further investigation and implementation by highlighting the significance of incorporating cutting-edge machine learning algorithms into sustainable construction project management."
Adversarial Network-Based Classification for Alzheimer’s Disease Using Multimodal Brain Images: A Critical Analysis,M. Gupta; R. Kumar; A. Abraham,10.1109/ACCESS.2024.3381956,2024,"Alzheimer’s disease (AD) is a progressive neurodegenerative disorder that represents a significant and growing public health challenge. This work concisely summarizes AD, encompassing its pathophysiology, risk factors, clinical manifestations, diagnosis, treatment, and ongoing research. The main goal of managing AD is to reduce symptoms while improving the lives of those impacted. This letter has conducted a systematic review to analyze the prediction of AD using the Preferred Reporting Item for Systematic Review and Meta-Analysis (PRISMA) guidelines. The major scientific databases such as Scopus, Web of Science (WoS), and IEEE Xplorer are explored, where 2018–2023 publications are considered. The article selection process is based on keywords like “Alzheimer’s disease,” “Brain Images,” “Deep Learning (DL),” etc. After rigorous analysis, 946 articles were extracted, and 42 were identified for final consideration. Further, several investigations based on the previous work are discussed along with its Proposed Solutions (PS). Finally, a case study on AD detection using the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset and AD Detection Network (ADD-NET) implementation is presented."
The Role of ChatGPT and Artificial Intelligence in Education,M. -A. Quiroz-Martinez; D. -S. Tumaille-Quintana; A. -D. Moran-Burgos; M. Gomez-Rios,10.1109/COLCOM62950.2024.10720308,2024,"This study delves into the role and impact of ChatGPT in education, exploring its benefits and challenges. The PRISMA model was employed to select pertinent articles, resulting in an in-depth review of 30 studies from an initial pool of 238 identified documents. The advantages of ChatGPT encompass immediate access to information, personalized learning experiences, and continuous support. However, concerns have surfaced regarding academic integrity, potential dependency, and content accuracy. Furthermore, there is an emphasis on adapting assessment methodologies and reinforcing teacher training programs. A significant observation from the reviewed literature is the lack of student perspectives, underscoring the need for broader inclusion of these voices in future research endeavors. This work serves as a foundational understanding of the evolving educational landscape in the era of artificial intelligence. The detailed analysis reveals that ChatGPT offers a range of significant opportunities in educational settings by facilitating rapid access to up-to-date information and providing personalized learning experiences. Nevertheless, there is a critical need to address challenges, such as safeguarding academic integrity and ensuring the accuracy of the system-generated content. Additionally, emphasis is placed on developing adaptive assessment strategies that accurately reflect the contribution of tools like ChatGPT to the learning process. A limitation identified in existing research is the scant representation of the student perspective, highlighting the urgency to include and better comprehend their experiences and perceptions when utilizing this technology in educational environments. In summary, this study enriches understanding of the impacts, opportunities, and challenges of integrating ChatGPT into education. Its comprehensive analysis provides a robust foundation for future research endeavors that address artificial intelligence’s practical and ethical implementation in the educational domain."
Explainable Software Bot Contributions: Case Study of Automated Bug Fixes,M. Monperrus,10.1109/BotSE.2019.00010,2019,"In a software project, esp. in open-source, a contribution is a valuable piece of work made to the project: writing code, reporting bugs, translating, improving documentation, creating graphics, etc. We are now at the beginning of an exciting era where software bots will make contributions that are of similar nature than those by humans. Dry contributions, with no explanation, are often ignored or rejected, because the contribution is not understandable per se, because they are not put into a larger context, because they are not grounded on idioms shared by the core community of developers. We have been operating a program repair bot called Repairnator for 2 years and noticed the problem of ""dry patches"": a patch that does not say which bug it fixes, or that does not explain the effects of the patch on the system. We envision program repair systems that produce an ""explainable bug fix"": an integrated package of at least 1) a patch, 2) its explanation in natural or controlled language, and 3) a highlight of the behavioral difference with examples. In this paper, we generalize and suggest that software bot contributions must explainable, that they must be put into the context of the global software development conversation."
Analysis and illustration of the practical impact of Artificial Intelligence and Intelligent Personal Assistants on business processes in small- and medium-sized service enterprises,D. Hüsson; A. Holland; M. Fathi; R. Arteaga Sánchez,10.1109/SMC52423.2021.9659298,2021,"Artificial intelligence (AI) and intelligent personal assistants (IPAs) are becoming more and more important. This is no longer limited to private use but also becoming increasingly important in everyday business life. The identification of optimization potentials through the use of AI and IPAs is therefore relevant from both a theoretical and a practical point of view. This paper, therefore, identifies concrete use cases for relevant processes in small- and medium sized enterprises (SME) in the service industry and enhances them with AI and IPA capabilities. Based on a prototype, the use cases were presented to 10 experts who were interviewed regarding their usefulness and influencing factors. Subsequently, the results of the interviews were categorized and validated again by a quantitative survey within the expert panel. As a result, the use cases were evaluated with regard to the specific influencing factors and the potential for optimization was determined. The use cases were evaluated based on this data. It was shown that IPA features in particular are perceived as useful. On average, AI and IPA features have a cost savings potential of over 31%. This shows the importance of these features and the need to consider them when modeling modern business processes."
An adavanced Gesture Recognition Using Machine Learning,M. L. Hyder; S. H. Ahammad; K. Tarun; K. Saikumar; M. Prasad; V. S. Reddy,10.1109/SMARTGENCON56628.2022.10084168,2022,"A hand gesture recognition system is a good idea to work interaction with a computer system, so that we can make a machine that will take some kind of snaps and perform actions accordingly, the focus on gesture motion is to develop a system for better communication between humans and computers. This paper presents a systematic study of all last year's papers for 2016 to 2019. It displays every technique that was employed to recognize hand gestures. A thorough survey of the scientific literature on gestures movement that deals with touch-free computing machines in the adjacent surroundings. This method includes touchless gesture interaction. Around 50 papers were identified and analyzed of them, and from there only 20 papers I found the same concept of gesture motion. Most of the identified literature (62%) deals with the control of gesture motion. Another one offers interactive methods for various types of notions. In the last few years back touchless interaction with computer system approaches. Though few were examined systematically in the real world and the rest of the other placed a different priority on the topic. Further research is required to cope with the current limitations of touchless interaction. The main challenges for future research are the improvement and evaluation of usability and touchless human-computer interaction and the further development of hands-free interaction. The accuracy was 96.67%, recall 98.73%, F1 measure 98.23%, and throughput 97.82% had been attained which is a good improvement."
Is Arabic text categorization a solved task?,A. E. Kah; I. Zeroual,10.1109/ISCV54655.2022.9806076,2022,"The amount of Arabic content on the Internet is being proliferated day by day, scoring the highest growth among the top online languages during the last two decades. Arabic is the fourth most used language on the world wide web, making producing reliable data mining applications and information retrieval engines significantly challenging. Therefore, Arabic text categorization has gained the attention of different researchers from various fields. As a result, considerable research works have addressed Arabic text categorization. Some of these research works reported accuracy rates that were very close to 100%. Due to that, one can only think if Arabic text categorization is a solved task or is still under-studied? To answer this question, we screened 262 related papers based on eight former surveys and reviews, following the PRISMA-ScR guidelines. Then, we focused on top-ranked results that are over 95%. In this paper, we present the outcomes of our investigation by addressing several research questions regarding the datasets, the preprocessing techniques, the dimensionality reduction methods, and the classifiers that have been used."
A comprehensive Analysis on Diagnosis of Alzheimer Disease Using Generative Adversarial Network,D. Joon; R. Kumar; M. Gupta; A. J. Obaid,10.1109/ACCAI61061.2024.10602112,2024,"Chronic stress in daily life causes physiological changes, including melancholy, anxiety, and cognitive deterioration. AD, the most common type of dementia in older people, is characterized by gradual memory loss, disorientation, and reduced language and decision-making skills. The major research databases (including Scopus, Web of Science, and IEEE Explore) are used to search for articles on generative adversarial networks (GANs) to analyze AD. To identify similar articles, search engines employ keywords, like “Alzheimer’s,” “detection,” “Magnetic Resonance Imaging (MRI),” “GAN,” and so on. Following a comprehensive screening method, 532 abstracts are gathered from multiple databases, and 19 full-text articles are selected for review. The analysis focuses on articles that use preexisting AD image datasets, implementing Machine Learning (ML) and GAN. Further, Deep Convolutional GANs (DCGANs), Wasserstein GANs (WGANs), Conditional GANs (CGAN), and Auxiliary Classifier GANs (ACGANs) are being investigated. Performance metrics such as sensitivity, specificity, F1-score, Recall, Precision, AUC, loss functions, and accuracy are discussed. This study consistently shows that GANs excel over other ML algorithms in terms of diagnostic accuracy. However, difficulties still exist in integrating multi-modal imaging data and attaining generalization across different datasets. Finally, this thorough work sheds light on the use of GANs for the systematic and non-invasive detection of AD using neuroimage analysis."
Advancements in Arabic Text-to-Speech Systems: A 22-Year Literature Review,K. Chemnad; A. Othman,10.1109/ACCESS.2023.3260844,2023,"Although there are several speech synthesis models available for different languages tailored to specific domain requirements and applications, there is currently no readily available information on the latest trends in Arabic language speech synthesis. This can make it challenging for beginners to research and develop text-to-speech (TTS) systems for Arabic. To address this issue, this article provides a comprehensive overview of several scholars’ contributions to the field of Arabic TTS, along with an examination of the unique features of the Arabic language and the corresponding challenges in creating TTS systems. Reporting only on papers discussing Arabic TTS, this systematic review evaluated available literature published between 2000 and 2022. We conducted a systematic review of six databases using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines to identify studies that addressed Arabic Text-to-Speech systems. Of the 3719 articles identified, only 36 (0.96%) met our search criteria. Bibliometric analyses of these studies were conducted and reported. The results highlight the main types of speech synthesis techniques used in TTS systems: concatenative, formant, deep neural network (DNN), hybrid models, and multiagent. The corpora used to develop these systems, as well as the diacritization techniques incorporated, evaluation techniques, and the results of the performance of the systems are reported. Subjective evaluation using the mean opinion score is the most commonly applied method to measure the accuracy of systems. This study also identifies gaps in the literature and makes recommendations for future research directions."
Systematic Review on Interpretability in Computer-Aided Alcohol Use Disorder Diagnosis,N. Z. Janah; A. E. Permanasari; N. A. Setiawan,10.1109/ICWT62080.2024.10674675,2024,"Numerous studies have investigated the detection of AUD using EEG signals. However, the real-world implementation of diagnostic systems is hindered by users’ difficulty in understanding the rationale behind a decision. This study presents a systematic literature review based on PRISMA guidelines, utilizing journal and conference articles from 2019 to 2023 obtained from Scopus. Among 66 collected studies, feature engineering techniques dominated by EEG sub band decomposition processes, followed by time-domain feature extraction, EEG conversion to images, and brain connectivity features. With advancements in computation, comprehensive hybrid features and raw EEG data are emerging as inputs for classifiers. Some studies also use dimensionality reduction techniques such as statistical analysis, optimization methods, feature reduction methods, and manual selection. Moreover, while black box classifier models are widely used, intrinsically interpretable classifiers exhibit no significant differences in accuracy performance. Future work aims to develop AUD detection systems with enhanced interpretability while maintaining robust classification performance, utilizing features informed by experts’ domain knowledge to facilitate decision explanation."
Toward a Digital Twin for Arthroscopic Knee Surgery: A Systematic Review,Ø. Bjelland; B. Rasheed; H. G. Schaathun; M. D. Pedersen; M. Steinert; A. I. Hellevik; R. T. Bye,10.1109/ACCESS.2022.3170108,2022,"The use of digital twins to represent a product or process digitally is trending in many engineering disciplines. This term has also been recently introduced in the medical field. In arthroscopic surgery education, the paradigm shift from apprenticeship to simulation training has driven the need for better simulators, and the current focus is on improving simulators with respect to computational efficiency and system accuracy. However, expanding surgical simulations towards digital twins has not yet been explored. This paper introduces the digital twin concept for arthroscopic surgery, and explores its potential in light of the existing scientific literature. Thus, a systematic review was conducted to summarize and analyze the literature with respect to fast and robust design of an arthroscopic digital twin using patient-specific information, and methods for interactive surgical soft tissue simulation. The review was conducted using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) protocol with three reliable scientific search engines: IEEE Xplore, ScienceDirect and PubMed. Eighty papers were included in the review, and the extracted data included modeling methods, tissue types, constitutive behavior, computational efficiency or accuracy, hardware configuration, haptic device description, software tools, and system architectures. Considering the review, a novel macro-level conceptual arthroscopic digital twin system is presented, and the applicability of the review findings for the identified subsystems are discussed. The proposed system integrates patient-specific images, diagnostic data, intraoperative sensor data, and surgical practice as inputs, and conceptually enables surgical skills training, preoperative planning, and a database of virtual surgeries."
An Amplified COCOMO-II Based Cost Estimation Model in Global Software Development Context,J. A. Khan; S. U. R. Khan; T. A. Khan; I. U. R. Khan,10.1109/ACCESS.2021.3089870,2021,"Global Software Development (GSD) projects comprise several critical cost drivers that affect the overall project cost and budget overhead. Thus, there is a need to amplify the existing model in GSD context to reduce the risks associated with cost overhead. Motivated by this, the current work aims at amplifying the existing algorithmic model with GSD cost drivers to get efficient estimates in the context of GSD. To achieve the targeted research objective, current state-of-the-art cost estimation techniques and GSD models are reported. Furthermore, the current study has proposed a conceptual framework to amplify the algorithmic COCOMO-II model in the GSD domain to accommodate additional cost drivers empirically validated by a systematic review and industrial practitioners. The main phases of amplification include identifying cost drivers, categorizing cost drivers, forming metrics, assignment of values, and finally altering the base model equation. Moreover, the proposed conceptual model's effectiveness is validated through expert judgment, case studies, and Magnitude of Relative Estimates (MRE). The obtained estimates are efficient, quantified, and cover additional GSD aspects than the existing models; hence we could overcome the GSD project's overall risk by implementing the model. Finally, the results indicate that the model needs further calibration and validation."
"Strategic Integration of HR, Organizational Management, Big Data, IoT, and AI: A Comprehensive Framework for Future-Ready Enterprises",V. Chavan; Nagendra; M. Roshif U; A. R.; A. Joshi; D. Jha,10.1109/ICONSTEM60960.2024.10568704,2024,"This exploration paper proposes a comprehensive frame aimed at fostering unborn-ready enterprises through the strategic integration of Human coffers(HR), Organizational Management, Big Data, the Internet of Things (IoT), and Artificial Intelligence(AI). By synthesizing these critical factors, the frame seeks to optimize organizational effectiveness, enhance decision-making processes, and acclimatize proactively to evolving request dynamics. Through a methodical review of being literature and empirical substantiation, the paper delineates the interconnectedness of these rudiments and elucidates their collaborative impact on organizational performance and dexterity. likewise, it explores perpetration strategies and implicit challenges associated with espousing such an intertwined approach. This paper not only contributes to the theoretical understanding of strategic operation but also provides practical perceptivity for directors and directors seeking to navigate the complications of the contemporary business geography and place their associations for sustained success in a decreasingly digitized and competitive terrain."
"Technological Solutions for Sign Language Recognition: A Scoping Review of Research Trends, Challenges, and Opportunities",B. Joksimoski; E. Zdravevski; P. Lameski; I. M. Pires; F. J. Melero; T. P. Martinez; N. M. Garcia; M. Mihajlov; I. Chorbev; V. Trajkovik,10.1109/ACCESS.2022.3161440,2022,"Sign languages are critical in conveying meaning by the use of a visual-manual modality and are the primary means of communication of the deaf and hard of hearing with their family members and with the society. With the advances in computer graphics, computer vision, neural networks, and the introduction of new powerful hardware, the research into sign languages has shown a new potential. Novel technologies can help people learn, communicate, interpret, translate, visualize, document, and develop various sign languages and their related skills. This paper reviews the technological advancements applied in sign language recognition, visualization, and synthesis. We defined multiple research questions to identify the underlying technological drivers that strive to improve the challenges in this domain. This study is designed in accordance with the PRISMA methodology. We searched for articles published between 2010 and 2021 in multiple digital libraries (i.e., Elsevier, Springer, IEEE, PubMed, and MDPI). To automate the initial steps of PRISMA for identifying potentially relevant articles, duplicate removal and basic screening, we utilized a Natural Language Processing toolkit. Then, we performed a synthesis of the existing body of knowledge and identified the different studies that achieved significant advancements in sign language recognition, visualization, and synthesis. The identified trends based on analysis of almost 2000 papers clearly show that technology developments, especially in image processing and deep learning, are driving new applications and tools that improve the various performance metrics in these sign language-related task. Finally, we identified which techniques and devices contribute to such results and what are the common threads and gaps that would open new research directions in the field."
Safety Hazard Prediction System for Nuclear Power Construction Stage Based on Deep Learning,H. Yu,10.1109/ICEDCS64328.2024.00215,2024,"The construction of nuclear power projects spans the entire cycle from survey, civil engineering, installation to commissioning, and the various stages of construction activities are complex and varied. Faced with these challenges, the application of computer technology has become crucial. Through the efficient data processing capabilities of computers, massive amounts of data generated at each stage can be collected and analyzed in real-time, providing strong support for security management. This not only enhances the timeliness of hazard identification, but also promotes the precise implementation of risk prevention and control measures, providing a solid guarantee for the smooth progress of nuclear power engineering construction. Therefore, this article intends to conduct research on a risk warning system for nuclear power construction based on deep learning, in order to accurately predict safety hazards in the process of nuclear power construction and timely prevent and control them. By collecting and organizing typical safety accident case data in the domestic nuclear power construction process, a comprehensive and rich safety accident database will be established. This article takes the hidden danger database associated with the smart construction site system of a nuclear power project under construction as a specific example and analysis object, and automatically extracts features from massive data and uses advanced deep learning algorithms such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) to train and optimize samples, establishing an efficient and accurate prediction model. During the time periods of 8:00 to 9:00 and 17:15 and 17:30 on October 23, 2023, the predicted results of the model were consistent with the true labels, and the prediction confidence was high, indicating that the model had a high degree of confidence in the safety status at these times. The research results of this article will provide strong technical support for accident risk prevention and control in the process of nuclear power construction."
Predicting Students' Software Engineering Class Performance with Machine Learning and Pre-Class GitHub Metrics,J. Cui; F. Zhou; R. Zhang; R. Li; C. Liu; E. Gehringer,10.1109/FIE58773.2023.10343357,2023,"Research into predicting students' performance in computer science classes has been conducted globally for over five decades. Numerous metrics, including performance in prior courses, demographic information, and programming experience, have been used to predict success in computer science. Various analytical methods, such as linear regression, decision trees, ensemble methods, and even neural networks, have also been explored. In this study, we investigate whether pre-class GitHub contribution metrics, combined with machine learning techniques, can forecast student performance in a software engineering class. We address two research questions in this paper. Firstly, can pre-class GitHub contribution metrics predict students' performance? Secondly, which machine learning technique is most effective in predicting student performance? We collected data from 802 students over five years and 11 semesters, including pre-class GitHub contribution stats, students' exam grades, project grades, documentation grades, and review writing grades. Eight different machine learning methods were then tested to predict in-class performance using pre-class GitHub contributions. Our results indicate that exam performance can be relatively accurately predicted by machine learning methods. Ensemble methods such as Random Forest, AdaBoost, and XGBoost performed better than other methods. This suggests that pre-class GitHub contribution metrics can be a useful tool for predicting students' performance in software engineering classes, carrying significant implications for educators. This approach can help educators identify at-risk students at the earliest point in the class, enabling early intervention strategies to prevent failure. Our study uniquely utilizes pre-class GitHub contributions, providing a preliminary indication of a student's familiarity with the course material. While prior research has focused on using in-class data to predict student performance, our approach identifies struggling students from the very beginning. We believe this can provide the most beneficial support for students."
"Utilization of Data Governance as Support for Quality Assurance in the Systems Engineering Program at the Universidad Cooperativa de Colombia, Ibagué Espinal Campus",O. A. Diaz Triana; F. Gutierrez Portela; F. A. Reina; E. Catherine Oviedo Barbosa; G. R. Higuera,10.1109/AmITIC62658.2024.10747646,2024,"This article presents a systematic literature review on data governance in educational institutions, focusing on the case of Universidad Cooperativa de Colombia, Campus Ibagué Espinal. Using a structured methodology based on PRISMA guidelines, 43 articles were selected and analyzed to identify current trends, challenges, and opportunities in data governance. The review highlights the importance of data governance in supporting educational quality assurance and continuous improvement processes. Key findings include the evolution of governance frameworks, integration of emerging technologies like AI and blockchain, and the establishment of clear policies for data management and privacy protection. The study underscores the role of robust technological infrastructure and regulatory frameworks in enhancing data governance initiatives, ensuring data accuracy, reliability, and availability for informed decision-making. Proposed recommendations include a comprehensive framework for data management and governance tailored to educational accreditation processes, aimed at optimizing educational and administrative processes and fostering institutional excellence and student success."
Factors Affecting On-Time Delivery in Large-Scale Agile Software Development,E. Kula; E. Greuter; A. van Deursen; G. Gousios,10.1109/TSE.2021.3101192,2022,"Late delivery of software projects and cost overruns have been common problems in the software industry for decades. Both problems are manifestations of deficiencies in effort estimation during project planning. With software projects being complex socio-technical systems, a large pool of factors can affect effort estimation and on-time delivery. To identify the most relevant factors and their interactions affecting schedule deviations in large-scale agile software development, we conducted a mixed-methods case study at ING: two rounds of surveys revealed a multitude of organizational, people, process, project and technical factors which were then quantified and statistically modeled using software repository data from 185 teams. We find that factors such as requirements refinement, task dependencies, organizational alignment and organizational politics are perceived to have the greatest impact on on-time delivery, whereas proxy measures such as project size, number of dependencies, historical delivery performance and team familiarity can help explain a large degree of schedule deviations. We also discover hierarchical interactions among factors: organizational factors are perceived to interact with people factors, which in turn impact technical factors. We compose our findings in the form of a conceptual framework representing influential factors and their relationships to on-time delivery. Our results can help practitioners identify and manage delay risks in agile settings, can inform the design of automated tools to predict schedule overruns and can contribute towards the development of a relational theory of software project management."
Opportunities and Challenges from the Intersection of Engineering and the Digital Economy: A Mini Review,P. Onu; A. A. Adesoji; A. Emmanuel,10.1109/SEB4SDG60871.2024.10630211,2024,"The synergistic relationship between engineering and the digital economy is comprehensively analyzed in this exploratory study, providing an understanding of the intricate interplay that has emerged between these domains. Engineering principles actively shape the digital economy's transformative journey, redefining industries rather than simply adapting to them. The scope of the study is within the broader framework of technological advancement, emphasizing the mutually beneficial relationship between engineering and the digital economy. It acknowledges that technological advances, driven by fundamental engineering principles, are propelling the digital economy as a powerful force that reshapes industries worldwide. In addition to acknowledging this convergence, the study discusses the fundamental objective of our inquiry: to gain a comprehensive understanding of the opportunities and challenges that emerge at the juncture of engineering and the digital economy. It lays the foundation for a detailed examination, characterizing the inquiry as investigating symbiotic influences, potential advantages, and inherent complexities. Consequently, the study highlights the dynamic nature of the digital environment, underscoring the need for stakeholders in engineering and digital economies to evolve and adapt constantly."
A Threat Intelligence Event Extraction Conceptual Model for Cyber Threat Intelligence Feeds,J. H. Al-Yasiri; M. F. Bin Zolkipli; N. F. N. M. Farid; M. Alsamman; Z. A. Mohammed,10.1109/NETAPPS63333.2024.10823639,2024,"In response to the escalating cyber threats, the efficiency of Cyber Threat Intelligence (CTI) data collection has become paramount in ensuring robust cybersecurity. However, existing works encounter significant challenges in preprocessing large volumes of multilingual threat data, leading to inefficiencies in real-time threat analysis. This paper presents a systematic review of current techniques aimed at enhancing CTI data collection efficiency. Additionally, it proposes a conceptual model to further advance the effectiveness of threat intelligence feeds. Following the PRISMA guidelines, the review examines relevant studies from the Scopus database, highlighting the critical role of artificial intelligence (AI) and machine learning models in optimizing CTI data preprocessing. The findings underscore the importance of AI -driven methods, particularly supervised and unsupervised learning, in significantly improving the accuracy of threat detection and event extraction, thereby strengthening cybersecurity. Furthermore, the study identifies a gap in the existing research and introduces XBC conceptual model integrating XLM-RoBERTa, BiGRU, and CRF, specifically developed to address this gap. This paper contributes conceptually to the field by providing a detailed analysis of current CTI data collection techniques and introducing an innovative conceptual model to enhance future threat intelligence capabilities."
Performance analysis of Multipath Transport layer schedulers under 5G/B5G hybrid networks,P. S. Kumar; N. Fatima; P. Saxena,10.1109/COMSNETS53615.2022.9668483,2022,"In this paper, we present a comprehensive analysis and comparison of several multipath schedulers under 5G/B5G hybrid networks. Specifically, we have developed a physical testbed and integrated nine different schedulers including schedulers from both multipath TCP (MPTCP) and multipath QUIC (MPQUIC) transport layer protocols. Our results present the systematic comparison of the schedulers under different packet loss ratio, bandwidth, delay, etc. Finally, we also present results to quantify the impact of bandwidth outages and fluctuations on the performance of the multipath transport layer schedulers. Additionally, we also present a systematic literature review (SLR) on multipath transport layer schedulers using preferred reporting items for systematic reviews and meta-analyses (PRISMA) guidelines to gather and analyse the information on the current state-of-the-art. One of the main contributions of the SLR is to broadly classify the research in the multipath transport domain into different categories and identify the key findings of the current published literature."
"A Review of Deep Learning-Based Anomaly Detection Strategies in Industry 4.0 Focused on Application Fields, Sensing Equipment, and Algorithms",A. Liso; A. Cardellicchio; C. Patruno; M. Nitti; P. Ardino; E. Stella; V. Renò,10.1109/ACCESS.2024.3424488,2024,"Anomaly detection is a topic of interest in several areas, ranging from Industry 4.0 to Energy Management, Smart Agriculture, Cybersecurity, and Bioinformatics. In a wide sense, detecting anomalies implies finding samples generated within a process that differs from its standard data generation mechanisms. Identifying these samples is extremely important for a variety of reasons, depending on the specific application and scenario, ranging from the minimization of production costs to maintaining the required safety standards. As such, the increasing availability of wide networks of sensors that yield large amounts of data characterizing the processes under observation allowed the large adoption of deep learning techniques, which proved worthy of attention due to their capability of identifying anomalies with large precision, accuracy and reproducibility. Consequently, there is an extensive need to consolidate research results to provide a common framework to understand the topic and ensure a common foundation to establish future research trends. To respond to this need, this work systematically reviews the state of the art of anomaly detection in Industry 4.0, evaluating gaps in the current knowledge and proposing future directions of interest. To pursue this objective, three main dimensions have been considered: the scenario where the anomaly detection methodologies were applied, the sensing equipment used to gather data characterizing the underlying process, and the algorithm employed to properly interpret the phenomena. The study was conducted following the PRISMA protocol, which allowed the identification of a relevant selection of papers by extracting a meaningful dataset of 78 papers of interest. The analysis highlighted the diffusion of autoencoders in several configurations and application scenarios, highlighting their effectiveness and flexibility for anomaly detection."
Comparing Gradient Descent and Genetic Algorithm for Optimization In Regression Analysis For Handling Nonlinearity In Liver Cirrhosis: A Survey,H. S. Yadav; R. K. Singhal,10.1109/INOCON57975.2023.10101184,2023,"The development of a noninvasive diagnostic paradigm for digestive illnesses is the primary focus of the work being done in clinical research at the moment. We want to demonstrate the efficacy of both the BP-ANN algorithm and linear regression in the diagnosis of GI illnesses by contrasting their activation functions and data structures. PRISMA criteria were followed in the reporting of the systematic review that we conducted. We examined comparable papers in seven different online academic sources for the purpose of comparing the accuracy of diagnosis utilizing Our principal interests in BPANN and linear regression. Regardless of whether or not a given study met the inclusion requirements, we were able to acquire features, patient count, input/output marker, diagnostic accuracy, and results/conclusions related to comparison. Since just nine articles met our criteria, we chose to include them in our analysis. Our study is one of eight that found the BP-ANN model to be more effective than linear regression in proposing the course of disease using AUROC scores. One study found that linear regression was more effective than BPANN in spotting early signs of colorectal cancer. While both the BPANN method and the linear regression technique showed promise for fitting the diagnostic model, the former provided more convincing evidence of superior prediction accuracy when applied to a model of noninvasive diagnostics for gastrointestinal disorders. We compared the BP-ANN and linear regression activation functions and data formats to see which would work best for fitting the diagnostic model. The results revealed that BPANN was a complete recommendation method."
A Review E-Government Implementation: Its Impact on Public Value Creation and Citizen Perspectives,N. H. Setyawan; S. Sulistyo; R. Hartanto,10.1109/SIML61815.2024.10578171,2024,"The global expansion of e-government services reflects the rising demand for efficient government services among digital citizens. Governments must prioritize understanding public perspectives and values for effective citizen-centric e-government, involving all stakeholders for sustainable development and utilizing innovative technologies to overcome challenges. This systematic literature review (SLR) aims to explore citizen-centric e-government implementation worldwide, focusing on its impact on public value creation and citizen perception. By analyzing 69 papers from 2017 to 2023 using the PRISMA method, the review revealed diverse research landscapes and highlighted the importance of inclusive services. Key variables such as Information Quality, System Quality, and Trust in e-government significantly influenced citizen attitudes. Behavioral Intention to Use, Citizen Satisfaction, and Public Value emerged as prominent dependent variables. Notable countries like China, India, Nigeria, and Pakistan were extensively studied, employing various analytical techniques like Structural Equation Modeling (SEM) and thematic analysis. Frameworks such as UTAUT, TAM, and IS Success Model were commonly used, reflecting the multifaceted nature of e-government research and the need for comprehensive approaches."
Overview of Energy Harvesting and Sustainability in Smart Cities,R. Salama; C. Altrjman; F. Al-Turjman; S. P. Yadav; S. Vats,10.1109/AECE59614.2023.10428140,2023,"Sustainability is a key factor in the context of smart cities. In smart cities, promoting sustainable energy is one strategy for achieving sustainability and lowering greenhouse gas emissions. Different energy sources are needed to meet the need for affordable and dependable electrical energy, where the environmental impact typically outweighs the cost of generation. In order to increase sustainability in the electrical industry, smart cities are examined in this article. The methodological strategy for this involved conducting a systematic literature review utilizing the PRISMA procedure. 154 journal articles were examined in-depth for this review. Energy efficiency, renewable energy, and energy and urban planning were the themes that the results were organized into. The results of the study showed that: (a) there is a global academic publication landscape for research on smart cities and energy sustainability; (b) publications are unbalanced when comparing the results of studies on the energy use intensity of different continents to those on the energy sustain ability of smart cities; and (c) there is a strong focus on technology-related issues related to energy sustainability, efficiency, and renewables topics in the literature, but much less attention is given to these issues. Urban and energy authorities are informed by the insights produced, and scholars are given ideas for future research directions."
Synthetic CT Image Generation From CBCT: A Systematic Review,A. Altalib; S. McGregor; C. Li; A. Perelli,10.1109/TRPMS.2025.3533749,2025,"The generation of synthetic CT (sCT) images from cone-beam CT (CBCT) data using deep learning methodologies represents a significant advancement in radiation oncology. This systematic review, following PRISMA guidelines and using the PICO model, comprehensively evaluates the literature from 2014 to 2024 on the generation of sCT images for radiation therapy planning in oncology. A total of 35 relevant studies were identified and analyzed, revealing the prevalence of deep learning approaches in the generation of sCT. This review comprehensively covers synthetic CT generation based on CBCT and proton-based studies. Some of the commonly employed architectures explored are convolutional neural networks (CNNs), generative adversarial networks (GANs), transformers, and diffusion models. Evaluation metrics including mean absolute error (MAE), root mean square error (RMSE), peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) consistently demonstrate the comparability of sCT images with gold-standard planning CTs (pCT), indicating their potential to improve treatment precision and patient outcomes. Challenges such as field-of-view (FOV) disparities and integration into clinical workflows are discussed, along with recommendations for future research and standardization efforts. In general, the findings underscore the promising role of sCT-based approaches in personalized treatment planning and adaptive radiation therapy, with potential implications for improved oncology treatment delivery and patient care."
A Multidimensional Model of the New Work Environment in the Digital Age to Increase a Company’s Performance and Competitiveness,M. Rožman; D. Oreški; P. Tominc,10.1109/ACCESS.2023.3257104,2023,"The purpose of the paper is to develop a multidimensional model of the new work environment in the digital age to increase a company’s performance and competitiveness in VUCA (volatility, uncertainty, complexity, and ambiguity) business environment. The multidimensional model covers the implementation of an agile work environment through the prism of using artificial intelligence technology to increase company’s performance and competitiveness. Researched determined multidimensional aspects for successful implementation of work environment in the digital age are, therefore 1) drivers for shifting towards agility, 2) implementation of agile leadership, 3) implementation of an agile work environment, 4) implementation of AI technology in work environment, 5) company’s performance, 6) competitiveness. The main survey involved randomly selected 473 medium-sized and large companies in Slovenia. Structural equation modelling was used for statistical data analysis. The results show that drivers for shifting towards agility have a positive effect on implementation of agile leadership. Also, results show that implementation of agile leadership and implementation of AI technology in work environment have a positive effect on implementation of an agile work environment. Moreover, results show that implementation of an agile work environment has positive effect on company’s performance and competitiveness. The paper highlights the important multidimensional aspects of the successful implementation of an agile work environment to increase the company’s performance and competitiveness. Also, our results will contribute to the proper implementation of the work environment in the digital age and give owners or top managers a broad insight into the various aspects that must be considered in their business governance in today’s rapidly changing business environment."
A Review of Optical Text Recognition from Distorted Scene Image,O. O. Sumady; B. J. Antoni; R. Nasuta; Nurhasanah; E. Irwansyah,10.1109/ICORIS56080.2022.10031325,2022,"The growing number of images with text taken from a natural position increases the amount of text distortion. Some challenges come because of distortion, curvature, or blur which occur when images are taken from a natural position. Scene text recognition has made significant progress and improved in accuracy. However, issues arise from the nature of several images. This paper aims to review algorithms used for scene text recognition that focus on the accuracy and consistency of scene text recognition on various common datasets and compare them. In addition, to find the weakness and inconsistencies of various scene text recognition algorithms between different datasets. A PRISMA method flow diagram applies to conduct the review. The results show Convolutional Neural Network (CNN) is the most adopted approach to creating scene text recognition programs. The highest accuracy is the CA-FCN algorithm used for the SVT dataset. However, the consistency of algorithm performance varies from one dataset to another. Most algorithms struggled with the IC15 irregular or SVT regular dataset and performed best using the IC03 dataset."
Generative Adversarial Networks (GANs) for Image Augmentation in Farming: A Review,Z. U. Rahman; M. S. M. Asaari; H. Ibrahim; I. S. Z. Abidin; M. K. Ishak,10.1109/ACCESS.2024.3505989,2024,"Enhancing model performance in agricultural image analysis faces challenges due to limited datasets, biological variability, and uncontrolled environments. Deep learning models require large, realistic datasets, which are often difficult to obtain. Data augmentation, especially through Generative Adversarial Networks (GANs), has become essential in farming applications, generating synthetic images to improve model training and reduce the need for extensive image collection. This review explores various GAN approaches for image augmentation in farming, investigating their challenges and limitations. Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, 128 publications were analyzed to identify research trends and gaps in GAN applications within the farming industry. Key applications of GANs include plant classification, weed detection, animal detection and behavior recognition, animal health and disease analysis, plant disease detection, phenotyping, and fruit quality assessment. Persistent issues like limited training datasets, occlusion challenges, and imbalanced data hinder model performance across these applications. Recognizing these challenges is critical for enhancing the efficiency and effectiveness of farming operations. Finally, this review concludes with insights and future directions to foster progress in this field."
Practitioners' Expectations on Code Smell Detection,Z. Zhang; S. Yin; W. Wei; X. Ma; J. W. Keung; F. Li; W. Hu,10.1109/COMPSAC61105.2024.00175,2024,"Code smell detection can automatically identify code smells in software source code to help developers to improve code maintainability, readability, and overall code quality. Currently, a wide variety of code smell detection techniques/tools are proposed for practical use. However, it is unclear what practitioners expect for code smell detection tools and whether the existing research meets their needs. To fill the gap, we conduct an empirical study. We first interview 10 software development professionals and subsequently survey 310 software practitioners about their practices and expectations of code smell detection tools. In addition, we conduct an extensive literature review of code smell detection papers published in major publications from 2014 to 2024, and compare current research findings with practitioners' expectations. From this comparison, we highlight the direction in which researchers need to work to develop code smell detection techniques that are important to practitioners."
Self-Reported Activities of Android Developers,L. Pascarella; F. -X. Geiger; F. Palomba; D. Di Nucci; I. Malavolta; A. Bacchelli,,2018,"To gain a deeper empirical understanding of how developers work on Android apps, we investigate self-reported activities of Android developers and to what extent these activities can be classified with machine learning techniques. To this aim, we firstly create a taxonomy of self-reported activities coming from the manual analysis of 5,000 commit messages from 8,280 Android apps. Then, we study the frequency of each category of self-reported activities identified in the taxonomy, and investigate the feasibility of an automated classification approach. Our findings can inform be used by both practitioners and researchers to take informed decisions or support other software engineering activities."
Synthetic Data Generation via Generative Adversarial Networks in Healthcare: A Systematic Review of Image- and Signal-Based Studies,M. H. Akpinar; A. Sengur; M. Salvi; S. Seoni; O. Faust; H. Mir; F. Molinari; U. R. Acharya,10.1109/OJEMB.2024.3508472,2025,"Generative Adversarial Networks (GANs) have emerged as a powerful tool in artificial intelligence, particularly for unsupervised learning. This systematic review analyzes GAN applications in healthcare, focusing on image and signal-based studies across various clinical domains. Following Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guidelines, we reviewed 72 relevant journal articles. Our findings reveal that magnetic resonance imaging (MRI) and electrocardiogram (ECG) signal acquisition techniques were most utilized, with brain studies (22%), cardiology (18%), cancer (15%), ophthalmology (12%), and lung studies (10%) being the most researched areas. We discuss key GAN architectures, including cGAN (31%) and CycleGAN (18%), along with datasets, evaluation metrics, and performance outcomes. The review highlights promising data augmentation, anonymization, and multi-task learning results. We identify current limitations, such as the lack of standardized metrics and direct comparisons, and propose future directions, including the development of no-reference metrics, immersive simulation scenarios, and enhanced interpretability."
Emerging Trends in Software Engineering: Implications for Development and Efficiency,B. Chhatria; H. Dharua; S. K. Tarai; S. Panda,10.1109/ESIC64052.2025.10962758,2025,"This study explores recent advancements in software engineering, focusing on methods and practices that enhance development efficiency, security, and innovation. The originality of this research lies in its comprehensive analysis of the latest trends, such as Agile methodologies, and the integration of machine learning, which are revolutionizing the software development landscape. The primary objective is to assess the effectiveness of these trends in optimizing software development processes and improving end-user experiences. This research applies quantitative analysis, using time-series data and survey responses from software development teams to evaluate key performance indicators associated with each trend. The analysis, conducted using SPSS software, examines correlations between the adoption of modern methodologies and outcomes such as reduced development time, enhanced security, and overall software quality. Findings indicate that Agile and DevOps practices contribute significantly to faster deployment cycles, while machine learning integration aids in predictive analysis, improving error detection and reducing maintenance needs. The practical implications of this study emphasize the value of embracing emerging methodologies to foster innovation, enhance project success rates, and ensure robust software products. These insights aim to support software companies and developers in adopting strategic trends for sustainable and effective engineering solutions."
"Investigation, Detection and Prevention of Online Child Sexual Abuse Materials: A Comprehensive Survey",V. M. Ngo⊠; C. Thorpe; C. N. Dang; S. Mckeever,10.1109/RIVF55975.2022.10013853,2022,"Child sexual abuse inflicts lifelong devastating consequences for victims and is an ongoing social concern. In most countries, child sexual abuse material (CSAM) distribution is illegal. As a result, there are many research papers in the literature which proposed technologies to detect and investigate CSAM. In this survey, a comprehensive search of the peer-reviewed journal and conference paper databases (including preprints) is conducted to identify high-quality literature. We use the PRISMA methodology to refine our search space to 2,761 papers published by Springer, Elsevier, IEEE and ACM. After iterative reviews of title, abstract and full text for relevance to our topics, 43 papers are included for full review. Our paper provides a comprehensive synthesis of the tasks of the current research and how the papers use techniques and datasets to solve their tasks and evaluate their models. To the best of our knowledge, we are the first to focus exclusively on online CSAM detection and prevention with no geographic boundaries, and the first survey to review papers published after 2018. It can be used by researchers to identify gaps in knowledge and relevant publicly available datasets that may be useful for their research."
Empowering Data Science Teams: How Automation Frameworks Address Competency Gaps Across Project Lifecycles,M. Holtkemper; C. Beecks,10.1109/BigData62323.2024.10825556,2024,"In the fast-evolving field of data science, the combination of the right team competencies has a major impact on a successful project execution. These competencies, ranging from data acquisition to model deployment, are increasingly difficult to maintain due to widespread competency shortages. This puts data science projects at risk of delays, inefficiencies, and failure, as organizations struggle to find skilled professionals. Automation frameworks - software tools designed to automate repetitive or complex tasks - offer a solution to this challenge. While these frameworks provide benefits such as reducing manual labor and improving project efficiency, they have notable limitations, particularly in covering critical phases like business understanding and deployment. Additionally, training programs also struggle to fully address the competency gap due to time, cost and scalability constraints. This paper investigates how existing automation frameworks can fill these competency gaps within data science teams more effectively. Using the CRISP-DM model as an example of a structured process, this study first identifies tasks required in each phase. Then, it matches these tasks with relevant automation frameworks to assess the extent of automation possible. Finally, these tasks are mapped to the EDISON Data Science Competence Framework to highlight which competencies automation frameworks can address. The findings suggest that automation frameworks effectively bridge competency gaps, enabling teams to complete projects more efficiently and effectively where human expertise may be lacking. In this manner, our findings serve as a reference point for data scientists and practitioners alike."
Navigating the Landscape: An In-Depth Exploration of Modern Application Development Methodologies and Practices,M. Singh,10.1109/ICICET59348.2024.10616327,2024,"Modern software development is a dynamic landscape, continually shaped by evolving technologies, methodologies, and organizational demands. This research paper delves into the intricate web of modern application development methodologies and practices, aiming to provide a comprehensive understanding of the choices available to developers and organizations. The study investigates the Agile methodology, DevOps practices, continuous integration/continuous delivery (CI/CD), Microservices Architecture, and Serverless Computing, scrutinizing their principles, benefits, challenges, and real-world applications. The research employs a systematic literature review and comparative analysis to examine the strengths and weaknesses of each methodology. Insights derived from case studies and best practices highlight successful implementation strategies, fostering a nuanced understanding of the diverse approaches within the realm of modern application development. Our findings reveal that the Agile methodology, with its iterative and collaborative approach, has become a cornerstone for many development teams. DevOps emerges as a transformative force, promoting seamless integration between development and operations. CI/CD practices enable efficient delivery pipelines, while Microservices Architecture and Serverless Computing offer scalable and flexible solutions. The comparative analysis dissects these methodologies, offering a roadmap for developers and organizations to navigate the complexities and select the most suitable approach based on project requirements. Looking ahead, the paper explores future trends in modern application development, considering the potential impact of emerging technologies such as artificial intelligence and blockchain. The research identifies challenges and opportunities on the horizon, urging the software development community to stay vigilant and adaptive in the face of evolving demands. This research not only provides an in-depth exploration of modern application development methodologies but also offers practical insights to guide developers and organizations in making informed decisions. As technology continues to advance, the findings presented herein serve as a valuable resource for navigating the ever-changing landscape of application development."
The Effect of Virtual Reality and Augmented Reality on Managing Projects,D. M. E. Khatib; F. Al Falasi; H. A. Anani; W. Shurrab,10.1109/ICBATS57792.2023.10111112,2023,"This report highlights how digital disruption and transformation have affected project management. Amazon was selected as the organization, and project management was chosen as the industry. Primary research-based open-end interviews with thirty project managers and secondary research based on a review of past literature were conducted to perform this study. It was found that digital disruption has occurred in the project management industry, which has modified the behaviors, needs, and expectations of people in society, the market, and the industry. It was found that virtual reality (VR) and augmented reality (AR) has disrupted the project management industry. The AR caused a shift in the consumer (sponsor/client) wants in the project management industry. Customer derives the demand want from. Therefore the businesses and their competitors’ activities are modified to adapt the AR and VR-based technologies. AR and VR reduced the project managers’ efforts, time, and resources. Moreover, the metaverse is another technology that will cause significant digital disruption and digital transformation in the project management industry."
A Survey of Text Representation Methods and Their Genealogy,P. Siebers; C. Janiesch; P. Zschech,10.1109/ACCESS.2022.3205719,2022,"In recent years, with the advent of highly scalable artificial-neural-network-based text representation methods the field of natural language processing has seen unprecedented growth and sophistication. It has become possible to distill complex linguistic information of text into multidimensional dense numeric vectors with the use of the distributional hypothesis. As a consequence, text representation methods have been evolving at such a quick pace that the research community is struggling to retain knowledge of the methods and their interrelations. We contribute threefold to this lack of compilation, composition, and systematization by providing a survey of current approaches, by arranging them in a genealogy, and by conceptualizing a taxonomy of text representation methods to examine and explain the state-of-the-art. Our research is a valuable guide and reference for artificial intelligence researchers and practitioners interested in natural language processing applications such as recommender systems, chatbots, and sentiment analysis."
Prioritizing User Feedback from Twitter: A Survey Report,E. Guzman; M. Ibrahim; M. Glinz,10.1109/CSI-SE.2017.4,2017,"Twitter messages (tweets) contain important information for software and requirements evolution, such as feature requests, bug reports and feature shortcoming descriptions. For this reason, Twitter is an important source for crowd-based requirements engineering and software evolution. However, a manual analysis of this information is unfeasible due to the large number of tweets, its unstructured nature and varying quality. Therefore, automatic analysis techniques are needed for, e.g., summarizing, classifying and prioritizing tweets. In this work we present a survey with 84 software engineering practitioners and researchers that studies the tweet attributes that are most telling of tweet priority when performing software evolution tasks. We believe that our results can be used to implement mechanisms for prioritizing user feedback with social components. Thus, it can be helpful for enhancing crowd-based requirements engineering and software evolution."
A case study on machine learning model for code review expert system in software engineering,M. Madera; R. Tomoń,10.15439/2017F536,2017,"Code review is a key tool for quality assurance in software development. It is intended to find coding mistakes overlooked during development phase and lower risk of bugs in final product. In large and complex projects accurate code review is a challenging task. As code review depends on individual reviewer predisposition there is certain margin of source code changes that is not checked as it should. In this paper we propose machine learning approach for pointing project artifacts that are significantly at risk of failure. Planning and adjusting quality assurance (QA) activities could strongly benefit from accurate estimation of software areas endangered by defects. Extended code review could be directed there. The proposed approach has been evaluated for feasibility on large medical software project. Significant work was done to extract features from heterogeneous production data, leading to good predictive model. Our preliminary research results were considered worthy of implementation in the company where the research has been conducted, thus opening the opportunities for the continuation of the studies."
Computer Artificial Intelligence Technology in CAD Industrial Drafting and Design Platform System: A Review,Q. Yonghong; O. H. Hassan; S. Z. Abidin; A. B. M. Hashim,10.1109/ICDSCA59871.2023.10393041,2023,"This paper discusses how a 3D drafting system is developed to accommodate complex human-machine interface characteristics. An object-oriented multi-level structured test case design method is proposed. An Oracle-based distributed project management database is constructed. A new 3D CAD surface model search method is given. A 3D CAD surface model retrieval algorithm based on distance-curvature shape distribution is proposed. The method can effectively solve the collaboration problem of multiple parallel programs. The designed software platform is proved to be complete, scalable and open through practical applications. Finally, it can effectively achieve human-to-human collaboration and resource sharing."
Does pair programming work in a data science context? An initial case study,J. S. Saltz; I. Shamshurin,10.1109/BigData.2017.8258189,2017,"While pair programming has been studied extensively for software programmers, very little has been reported with respect to pair programming in a data science project. This paper reports on a case study evaluating the effectiveness of pair programming within a data science / big data context. Our findings show that pair programming can be useful for data science teams. In addition, while the driver role was similar to what has been described for software programmers, we note that the observer role had an expanded set of responsibilities, which we termed researcher activities. Further exploration is required to explore if these expanded roles are specific to data science pair programming."
Ground Harvesting and Gathering Robots in Agriculture: A Systematic and Technological Review,D. Huamanchahua; F. Soncco; A. Lugo-Figueroa; D. Chirinos-Castillo; B. A. Rojas,10.1109/COLCOM62950.2024.10720324,2024,"The study about the collectors’ robots started in 1960 due to the increasing population. The collector's robots are essential in food production, and the research of new technologies for harvesting robots has increased. This article focuses on a comprehensive review of the components’ applications that agricultural robots possess based on PRISMA methodology. The objective is to provide and inform researchers about the principal technologies and techniques used in different projects through a structured matrix. For the development of these articles, the information was obtained from databases such as ""ScienceDirect,"" ""IEEE Explore,"" ""Wiley Online Library,"" ""Springer Link,"" ""JSTAGE,"" and ""Scopus."" We have analyzed 36 articles that we’ve selected from 2020 to 2023. There has been a selection of 5 variables, and we will explain and describe their function and application in this kind of robot: the degree of freedom, sensors, and type of cameras, actuators, image recognition technique, and end-effector. We selected these variables because they are significant for interacting with the crops and detecting the robot’s objective. Finally, based on the analysis, the conclusion is presented about the challenges to be overcome in the five select variables."
"Precision Techniques and Agriculture 4.0 Technologies to Promote Sustainability in the Coffee Sector: State of the Art, Challenges and Future Trends",M. K. Sott; L. B. Furstenau; L. M. Kipper; F. D. Giraldo; J. R. López-Robles; M. J. Cobo; A. Zahid; Q. H. Abbasi; M. A. Imran,10.1109/ACCESS.2020.3016325,2020,"Precision Agriculture (PA) and Agriculture 4.0 (A4.0) have been widely discussed as a medium to address the challenges related to agricultural production. In this research, we present a Systematic Literature Review (SLR) supported by a Bibliometric Performance and Network Analysis (BPNA) of the use of A4.0 technologies and PA techniques in the coffee sector. To perform the SLR, 87 documents published since 2011 were extracted from the Scopus and Web of Science databases and processed through the Preferred Reporting Items for Systematic reviews and Meta-Analyzes (PRISMA) protocol. The BPNA was carried out to identify the strategic themes in the field of study. The results present 23 clusters with different levels of development and maturity. We also discovered and presented the thematic network structure of the most used A4.0 technologies in the coffee sector. Our findings shows that Internet of Things, Machine Learning and geostatistics are the most used technologies in the coffee sector, we also present the main challenges and trends related to technological adoption in coffee systems. We believe that the demonstrated results have the potential to be considered by researchers in future works and decision making related to the field of study."
A Meta-Analysis Survey on the Usage of Meta-Heuristic Algorithms for Feature Selection on High-Dimensional Datasets,L. Y. Yab; N. Wahid; R. A. Hamid,10.1109/ACCESS.2022.3221194,2022,"Feature selection (FS) using meta-heuristic algorithms on high-dimensional datasets (HDD) is becoming more prevalent due to the continuous advancement in data mining. However, the difficulty in identifying the threshold of features in a dataset to be categorised as HDD remains an issue due to the different schools of thought on this matter. Therefore, this survey intended to determine the threshold for a number of features to be HDD, and subsequently identify the trend or potential FS method for HDD and the most preferred meta-heuristic algorithms and classifiers for both wrapper-based and filter-based FS methods to analyse HDD. This study performed an extensive systematic literature review by implementing the PRISMA guidelines on 62 research articles that were published between 2016 to 2021. This survey proposed a novel grouping technique called literal grouping and data grouping (LGDG) to accurately group the chosen articles based on HDD. The LGDG method serves as a guide for other researchers who intend to perform FS research related to HDD. Literal grouping refers to searching for selected papers using specific keywords, like HDD in this case. While data grouping compares the number of features in datasets towards the threshold, which is set at 2,000 features by the majority. Based on the analyses of all the LGDG groupings, the filter-based FS method gained more attention in recent years with competent results no less than wrapper-based, especially on HDD. Besides that, Moth Flame Optimisation works well in filter-based methods, whereas Cuckoo Optimisation Algorithm works well in wrapper-based, while Whale Optimisation Algorithm works well in both FS methods. As for the classifier’s preferences, SVM, DT, and NB are preferred by the filter-based, while KNN is preferred by the wrapper-based method. It can be recommended that reviewing other aspects such as multi-objective FS on HDD and including more FS methods could be included in future studies as an extension to this survey."
Expanding the Number of Reviewers in Open-Source Projects by Recommending Appropriate Developers,A. Chueshev; J. Lawall; R. Bendraou; T. Ziadi,10.1109/ICSME46990.2020.00054,2020,"Code review is an important part of the development of any software project. Recently, many open source projects have begun practicing lightweight and tool-based code review (a.k.a modern code review) to make the process simpler and more efficient. However, those practices still require reviewers, of which there may not be sufficiently many to ensure timely decisions. In this paper, we propose a recommender-based approach to be used by open-source projects to increase the number of reviewers from among the appropriate developers. We first motivate our approach by an exploratory study of nine projects hosted on GitHub and Gerrit. Secondly, we build the recommender system itself, which, given a code change, initially searches for relevant reviewers based on similarities between the reviewing history and the files affected by the change, and then augments this set with developers who have a similar development history as these reviewers but have little or no relevant reviewing experience. To make these recommendations, we rely on collaborative filtering, and more precisely, on matrix factorization. Our evaluation shows that all nine projects could benefit from our system by using it both to get recommendations of previous reviewers and to expand their number from among the appropriate developers."
From Single to Multi-Disease: AI Techniques in Medical Diagnostics,Deepika; U. Garg,10.1109/ESIC64052.2025.10962774,2025,"The healthcare sector has witnessed a transformative rise in disease diagnosis due to the emergence of Artificial Intelligence (AI) and other related processes. AI is central to success in healthcare because of the outstanding techniques which are offered by Machine Learning (ML) and Deep Learning (DL). The optimal diagnosis of disease through these techniques requires medical imaging data like ultrasound, Magnetic Resonance Imaging (MRI), mammography, and many more. This research paper covers a detailed, AI-assisted, medical survey based on the techniques used in identifying various diseases including eye diseases, lung disorders, skin diseases, and neurological disorders inclusive of Alzheimer's and cancer. The study further presents the key highlights, strengths, and limitations of allied techniques along with their result analysis. Along with the single disease prediction, this research paper additionally analyses the techniques on Multi-Disease (MltD) prediction using PRISMA guidelines. Three specific scientific databases including Web of Science (WOS), Scopus, and PubMed are utilized to identify the related articles, and in total 321 articles were obtained. After a thorough screening process, 27 articles are selected for detailed analysis to provide valuable insights for future advancements in MltD prediction models in the healthcare system."
Development of smart Petroleum Projects Management Using Datamining Technology,A. A. Obaid,10.1109/ICONAT53423.2022.9725909,2022,"Project managements of oil and gas industry involves development of robust system that is able to integrate many skill professionals to work smoothly and together in the said project fields. This process is challenged by the ground reality as project manager is still have to look in many other tasks such as costing (budgets), contractors' relations, safety norms, office routines etc. in this paper, data mining i.e. (Delphi method) is used for knowledge enquiry for questionnaire survey data. The same is being established in order to derive a reliable project management system in oil and gas companies."
A Summary Of Using Reinforcement Learning Strategies For Treating Project And Production Management Problems,G. Koulinas; A. Xanthopoulos; A. Kiatipis; D. Koulouriotis,10.1109/ICDIM.2018.8847099,2018,"Recently, Reinforcement Learning (RL) strategies have attracted researchers' interest as a powerful approach for effective treating important problems in the field of production and project management. Generally, RL are autonomous machine learning algorithms that include a learning process that interacts with the problem, which is under study in order to search for good quality solutions in reasonable time. At each decision point of the algorithm, the current state of the problem is revised and decisions about the future of the searching strategy are taken. The objective of this work is to summarize, in brief, recently proposed studies using reinforcement learning strategies for solving project scheduling problems and production scheduling problems, as well. Based on the review, we suggest directions for future research about approaches that can be proved interesting in practice."
Transfer Learning of Human Activities Based on IMU Sensors: A Review,S. Ashry; S. Das; M. Rafiei; J. Baumbach; L. Baumbach,10.1109/JSEN.2024.3510097,2025,"This systematic review comprehensively scrutinizes transfer learning (TL) methods applied to human activity recognition (HAR) using inertial measurement unit (IMU) data. Our objective is to provide a comprehensive resource for researchers and developers by summarizing the existing activities, feature extractions, and TL techniques in the related studies. Moreover, we identify research gaps in these categories, allowing research endeavors to address these gaps in the future. Our methodology follows the structure of the preferred reporting items for systematic reviews and meta-analysis (PRISMA) statement by formulating precise research questions, establishing search queries, and specifying inclusion and exclusion criteria for study selection. Finally, we extracted and summarized the existing activities, feature extractions, and TL techniques utilized in the included studies. We analyzed 447 studies from PubMed, ACM, and Scopus datasets, of which we ultimately selected 33 pivotal studies that met our inclusion criteria. Overall, we found that TL has enhanced HAR performance by reusing pretrained models. However, it is important to carefully select relevant transfer information to avoid any potential adverse effects. We conclude that there is a lack of studies assessing specific activities, such as personal hygiene and elder care, using IMU sensors and TL. To improve signal representation, a combination of features based on activity nature is important. Aligning algorithms with activity nature is essential—simpler models like AlexNet are suitable for routine activities such as walking, while more complex models like DenseNet are better for intricate tasks like cleaning. Our review is a reference for advancing the understanding of TL’s potential of HAR using IMU data."
Privacy Analysis in Mobile Apps and Social Networks Using AI Techniques,D. Blanco-Aza; A. Robles-Gómez; R. Pastor-Vargas; L. Tobarra; P. Vidal-Balboa; M. Méndez-Suárez,10.1109/ICSC63108.2024.10895037,2024,"In the current landscape of mobile applications and social networks, privacy concerns have become paramount due to the extensive collection and processing of personal data. Therefore, this paper presents a comprehensive review of the state-of-the-art on automated privacy risk analysis in mobile applications and social networks. This review includes various methodologies, tools and frameworks that use ML and NLP systems to assess and ensure compliance with privacy regulations, such as the GDPR. Through a careful application of the PRISMA methodology, key studies have been systematically analyzed. Our findings reveal significant progress in the integration of automated techniques for assessing privacy risks."
Essboard: a collaborative tool for using Essence in software development,D. Quintanilla-Perez; A. Mauricio-Delgadillo; D. Mauricio-Sanchez,10.1109/ICSESS47205.2019.9040832,2019,"Knowing the current situation and goals of software development is an activity that takes into account the participation of the whole team in the evaluation of many aspects of it. The Essence standard provides a set of basic aspects that are presented in any software development, which helps achieve this goal. Currently, a set of tools has been developed to support Essence for that activity, however, they leave aside the participation and collaboration of the team, which is necessary to generate a common vision of the state and goals of a project. In this paper, we propose Essboard, a tool that takes a collaborative approach by putting in the forefront, the perspective of each member and the holistic vision of the team about the situation and progress of a project. We confirm that Essboard fulfills this goal through a Formal Technical Review (FTR) based on an Awareness Checklist, showing a noticeable improvement to the support of the collaboration in contrast to the existing Essence tools."
Financial Reporting and Decision-Making in Engineering Sector: A Bibliometric Review of Scopus Database,M. M. Albaz; T. M. Hashad; M. Khalifa,10.1109/DASA63652.2024.10836620,2024,"With a focus on documents indexed in the Scopus database, this bibliometric research explores the academic literature regarding financial reporting and decision-making in engineering sector firms. The study highlights major research areas, important works, authors, and methods used in the related literature. By examining a sizable dataset of relevant research (598 documents), this bibliometric provides researchers and policymakers with a valuable landscape of the current state of research in this area. Our main results are as follows; 1915 documents have been published in this area from 2014 to 2024, by adding some limitations, our final sample was 598 documents, the publication trend in this area seems to be positive with years. Moreover, the USA has the biggest number of published documents in this area."
CfgNet: A Framework for Tracking Equality-Based Configuration Dependencies Across a Software Project,S. Simon; N. Ruckel; N. Siegmund,10.1109/TSE.2023.3274349,2023,"Modern software development incorporates various technologies, such as containerization, CI/CD pipelines, and build tools, which have to be jointly configured to enable building, testing, deployment, and execution of software systems. The vast configuration space spans several different configuration artifacts with their own syntax and semantics, encoding hundreds of configuration options and their values. The interplay of these technologies requires some level of coordination, which is realized by matching configurations. That is, configuration options and their according values may depend on other options and values from entirely different technologies and artifacts. This creates non-obvious configuration dependencies that are hard to track. The missing awareness and overview of such configuration dependencies across diverse configuration artifacts, tools, and frameworks can lead to dependency conflicts and severe configuration errors. We propose CfgNet, a framework that models the configuration landscape of a software project as a configuration network in an extensible and artifact-independent way. This way, we enable the early detection of possible dependency violations and proactively prevent misconfigurations during software development and maintenance. In a literature study, we found that the most common form of dependencies is the equality of values of different options. Based on this result, we developed an equality-based linker to determine dependent options across different artifacts. To demonstrate the extensibility of our framework, we also implemented nine plugins for popular technologies, such as Maven and Docker. To evaluate our approach, we injected and violated five real-world configuration dependencies extracted from Stack Overflow, which we support with our technology plugins, in five subject systems. CfgNet found all injected dependency violations and four additional ones already present in these systems. Moreover, we applied CfgNet to the commit history of 50 repositories selected from GitHub and found dependency conflicts in about two thirds of these repositories. We manually inspected 883 conflicts, with about 89 % true positives, demonstrating the need to reliably track cross-technology configuration dependencies and prevent their misconfiguration."
Federated Learning in Manufacturing: A Systematic Review and Pathway to Industry 5.0,M. A. Bin Syed; Q. Rhaman; S. Sushil,10.1109/STI59863.2023.10464397,2023,"The aim of this research is to explore and evaluate the use of federated learning to tackle issues related to data privacy, data scarcity, data security, and anomaly detection in Industry 4.0 manufacturing settings and the transition towards industry 5.0. SCOPUS database has been used to systematically review existing literature on applying federated learning in Industry 4.0 manufacturing contexts. The identified literature is analyzed qualitatively using VOS viewer software and Microsoft Excel. Firstly, a bibliometric study following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines has been undertaken to provide insights into the progress of research on this topic. This analysis encompasses the examination of co-authorship patterns among countries as well as the analysis of keyword co-occurrence. Fur-thermore, a comprehensive analysis of the chosen articles (n=33) examines the specific research inquiries that will guide future research endeavors to bridge the existing gaps in knowledge and facilitate the implementation of federated learning in the context of Industry 4.0 manufacturing. The findings of this research demonstrate that the utilization of federated learning presents promising solutions for addressing challenges related to data isolation, data privacy, and efficient processing in the context of Industry 4.0 manufacturing. Furthermore, there is a growing number of emerging models that exhibit improved performance in these areas, with the aim of facilitating the implementation of Industry 5.0 in the manufacturing sector."
Measurement Models for Group-Peer Assessment of Project-Based Learning in Software Engineering,P. H. Vossen; S. Ajit,10.1109/ISCSET58624.2024.10808114,2024,"Over two decades we have been developing and testing alternative frameworks for Group-Peer-Assessment (GP A) in software engineering education. The focus lies on skills assessed through observation of students' behavior and/or examination of the results of project assignments. Assessment for such instructional types is not covered satisfactorily by traditional approaches. Assessment at the group as well as individual level must be able to cope with multiple quality criteria on scales of various types, and multiple assessors responsible for assessing distinct quality aspects. We offer two-parameter scoring models for GP A, by which individual student scores are derived from a group score and mutual peer ratings. The two parameters are: (1) a constraint on the spread of student scores and (2) the relative impact of peer ratings. GP A imposes no artificial restrictions on group size, type or number of quality criteria, or other context-specific aspects of GP A. We briefly describe our experience with GP A for software engineering courses at a university in the UK."
WoS Bibliometric-based Review for Security Testing of Android Applications using Malware Analysis,P. Kumar; S. Singh,10.1109/ICITIIT61487.2024.10580700,2024,"Android, the world’s predominant mobile operating system, witnesses a constant influx of new applications. Amid this surge, the realm of Android application security testing assumes paramount significance, as it entails scrutinizing application behavior to unveil latent vulnerabilities. Notably, the landscape of security testing for Android applications is currently undergoing remarkable evolution. this study ventures into this dynamic domain by conducting a comprehensive bibliometric analysis of Web of Science (WoS) articles spanning from 2019 to 2023, shedding light on the multidimensional facets of security testing through malware analysis. Executed within the RStudio environment, powered by the “bibliometrix” library, the analysis is fortified by the rigorous assessment of methodological quality through the AMSTAR and PRISMA checklists. The results encapsulate vital dimensions encompassing yearly publication trends, pivotal keywords, annual citation dynamics, source impact, malware analysis techniques, benchmark datasets, machine learning and emerging research gaps in the field of security testing of Android applications. The synthesis of these dimensions unveils a comprehensive portrait of the research landscape, accentuating eminent contributors, influential journals, and pioneering nations. Moreover, this inquiry identifies promising avenues for future exploration, ultimately propelling the enhancement of this critical field."
"IEEE Draft Standard for System, Software, and Hardware Verification and Validation",,,2024,"Verification and validation (V&V) processes are used to determine whether the development products of a given activity conform to the requirements of that activity and whether the product satisfies its intended use and user needs. V&V life cycle process requirements are specified for different integrity levels. The scope of V&V processes encompasses systems, software, and hardware, and it includes their interfaces. This standard applies to systems, software, and hardware being developed, maintained, or reused [legacy, commercial off-the-shelf (COTS), non-developmental items]. The term software also includes firmware and microcode, and each of the terms system, software, and hardware includes related information or documentation. V&V processes include the analysis, evaluation, review, inspection, assessment, and testing of product"
A Systematic Review on Semantic Role Labeling for Information Extraction in Low-Resource Data,A. D. P. Ariyanto; D. Purwitasari; C. Fatichah,10.1109/ACCESS.2024.3392370,2024,"Challenges in the big data phenomenon arise due to the existence of unstructured text data, which is very large, comes from various sources, has various formats, and contains much noise. The complexity of unstructured text data makes it difficult to extract useful information. Therefore, a process is needed to transform it into structured data to be processed further. The information Extraction (IE) process helps to extract relationships, entities, semantic roles, and events from unstructured text data by converting them into structured output. One of IE’s tasks is Semantic Role Labeling (SRL), which has a crucial function in identifying semantic roles in a sentence so that it can enrich the understanding of the text. However, much of SRL development focuses on high-resource data, especially in English. The limited development of SRL in specific low-resource languages or domains is a complex challenge. This research aims to conduct a systematic study on the development of SRL for low-resource data, both in low-resource language or domain-specific contexts. The review process was carried out systematically using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) model, and 54 quality papers were obtained from the filtering process (from 2018 to 2023). We review several essential points, including (1) datasets that are often used for SRL tasks and their labeling strategies for low-resource data, (2) methods that have currently been developed for SRL tasks and learning scenarios when dealing with low-resource data, (4) evaluation metrics, (5) application of SRL tasks. This review is complemented by a discussion of issues and potential solutions for developing SRL on low-resource data to help researchers develop SRL more effectively in dealing with the challenges faced with low-resource data."
"State-of-the-Art Review: Models and Algorithms for Optimal Power System Design, Stabilization, and Reliability Enhancement",S. Njabulo Zwane; B. Mendu; B. Baakanyang Monchusi,10.1109/ACCESS.2024.3510381,2024,"The stability and reliability of power systems are essential for effective operation and design. Models and algorithms are instrumental in bolstering stability, especially using Power System Stabilizers (PSS) and refined control methods. In this study, a state-of-the-art review of models and algorithms for optimal power system design, stabilization, and reliability enhancement is conducted. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) approach was utilized to systematically identify and refine the focus on the topic using the Scopus database, while VOSviewer assisted in analyzing trends. Noticeable aspect from the results includes the prevalence of metaheuristic algorithms like genetic algorithms, particle swarm optimization, cuckoo search, and various others. The results also revealed that these algorithms are utilized for tasks such as optimal power system design, substation placement, parameter tuning for power system stabilizers, and load forecasting. The trends analysis further shows a notable shift towards learning algorithms, indicating an increasing interest in data-driven approaches to improve system performance. This paper contributes by providing a comprehensive review of optimization and machine learning techniques, including genetic algorithms and metaheuristics, for enhancing power system stability and resilience. It focuses on advanced methodologies for stabilizer design, Phasor Measurement Units (PMU) placement, distributed generation integration, and power system performance optimization, supported by practical applications and case studies. Finally, this work suggests future research directions to address gaps in existing knowledge."
Assessing Soft Skills Development in Informatics Students Through Project-Based Learning and Agile Frameworks,V. M. Choque-Soto; V. D. Sosa-Jauregui,10.1109/ICACIT62963.2024.10788614,2024,"In the current dynamic and competitive job market, influenced by the rapid growth of Artificial Intelligence, Soft skills have become increasingly valuable. Similarly, today's informatics labor sector requires collaboration, ethics, and adaptability. However, traditional education often prioritizes technical skills over others. Thus, the present study aims to assess the development of soft skills in informatics students using the Project Based Learning (PBL) methodology within the context of Agile frameworks: Scrum and Kanban. A mixed-method study was designed using both quantitative and qualitative approaches. Student outcomes based on ICACIT accreditation criteria were utilized to quantitatively assess soft skills, focusing on indicators such as teamwork, communication, professionalism, and ethics. A perception survey was conducted to qualitatively gather data on students' experiences in developing soft skills. The results indicate a positive academic performance, with achievement levels exceeding 70%. Students also reported favorable perceptions of acquiring soft skills, and there was a significant correlation between quantitative and qualitative findings. Lastly, the assessment shows that implementing PBL within agile frameworks positively impacted informatics students, especially in communication and teamwork skills."
An applicative Industrial Metaverse experience in Semiconductor Industry,D. D. DONATO; G. D. PONT; L. NAVONI; S. MASSARINO; A. BERETTA,10.1109/EMR.2025.3529247,2025,"This article proposes a practical example of the application of the emerging concept of Industrial Metaverse in the semiconductor industry, and its impact on the role of humans and sustainability in the contemporary industrial context. It argues that the use of some technologies included in the Industrial Metaverse concept is bringing humans, after being marginalized by machines during the Industrial Revolution and by computers in the subsequent digital era, back to their centrality in the industrial landscape boosting their performance. Furthermore, positive impacts on sustainability have been noted, which however require further numerical investigation. On the other hand, the absence of consolidated academic metrics to evaluate those relations, calls for further research to fully understand the impact of the Industrial Metaverse on human resources and sustainability in a design and construction project and thereafter in operations, and to develop appropriate measures to monitor its effects, through a proposed model."
Inverse Problem for M/EEG Source Localization: A Review,S. Masoud Moosavi Basri; H. Al-Nashash; H. Mir,10.1109/JSEN.2024.3502917,2025,"This systematic review provides a comprehensive evaluation of current and emerging methodologies in magnetoencephalography/electroencephalography (M/EEG) source localization by addressing critical technological and methodological gaps. The review follows systematic review and meta-analysis (PRISMA) guidelines and meticulously examines a wide range of source localization methods, making it the most exhaustive evaluation in the field. It categorizes 28 distinct methods into five core groups: basic, hybrid, subspace-based, probabilistic, and machine learning-based, and offers an in-depth comparative analysis. Our findings reveal a pressing need for advancements in accuracy, spatial-temporal resolution, and computational efficiency. We identify the imperative for improved signal processing, advanced modeling, integration of machine learning, and AI, to enhance source localization accuracy. The review advocates for personalized, real-time applications in clinical settings and underscores the importance of multimodal neuroimaging studies for comprehensive brain activity insights. We recommend longitudinal, large-scale studies and open science practices for validating and generalizing findings. This review stands as a definitive guide for future research, aiming to propel M/EEG source localization to new heights of accuracy and clinical utility."
Python: An Automation Tool for Unlocking Innovation and Efficiency in the AEC Sector,A. M. Khan; W. Salah Alaloul; M. A. Musarat; M. Hassaan Farooq Khan,10.1109/ICDABI60145.2023.10629478,2023,"Python offers a lot of potential as an automation tool for the Architecture, Engineering, and Construction (AEC) industry, with opportunities to enhance decision-making, accelerate innovation, and simplify operations. The AEC industry challenges issues with efficiency, innovation, and sustainable development in the current data-driven world. Automation has proven a potent cure, revolutionizing many aspects of business. Python provides potential to enhance operational efficiency by minimizing AEC operations, enhancing decision-making, and exploiting its extensive libraries and flexibility. This research examines the adoption of automation technologies in the AEC industry, including computer-aided design (CAD), building information modelling (BIM), robotics, and machine learning, through a review of the literature, guidelines, and criteria that are assessed according to their effect. It exposes the deficiencies and problems, such as the lack of software system compatibility. Due to its versatility, considerable community support, and potential for connection with existing software systems, Python presents as a feasible automation solution. The findings reinforce the significance of continuing research and application in this sector by demonstrating the benefits of Python automation in encouraging collaboration, innovation, and a data-driven future for the AEC industry."
"Empowering Coding Collaboration with PaaS: A Study of Slackflow Messenger's Impact on Team Productivity with React, Redux, and Firebase",P. Parihar; R. Walia,10.1109/ICCSAI59793.2023.10421229,2023,"The influence of Slackflow Messenger, a platformas-a-service (PaaS) tool for coding collaboration, on team productivity is examined in this paper. The solution was created to enable smooth interaction and collaboration among software development teams utilizing current web technologies, such as React, Redux, and Firebase. The study's main objectives wereto assess the system's key capabilities and its potential to increase coding cooperation and team productivity. Analysis of use statistics, user surveys, and qualitative feedback gathering from software development teams were all part of the assessment process. The findings demonstrated that Slackflow Messenger's chat and message capabilities offered a productive method of communication and collaboration that enhanced project outcomes. The functionality and usability of the system were further improved by the incorporation of other technologies like version control and project management software. Overall, the research emphasizes the potential advantages of leveraging contemporary web technologies in constructing such solutions while highlighting the significance of collaboration tools and PaaS solutions in software development."
Agile and Touchless Automation in the software industry,G. Sriraman; S. Sehar; S. E,10.1109/ICACCS54159.2022.9785260,2022,"Software and systems are becoming the business drivers and core competencies for organisations than enabling function. Building effective and quality software systems is a crucial success factor for any organization. Technology and infrastructure capabilities have gone through a significant leap in the last few decades at one end, and the complexity of software and its expectations have become too high at another end of the spectrum. With these developments, it is essential to revisit the traditional way of developing, building, and operating software that leverages the advancements in technology and infrastructure developments, automation, agility, and industrialization practices. A survey was done to study existing industry practices, Organization's readiness in DevOps adoption, their pain points and how they aim to address the same. This study aims to provide deep insights into how the software industry is evolving so academicians understand the state of the industry and further the research for betterment."
An Evaluation of DL T Governance Models,S. Ntshangase; N. Ndhlovu; S. Myaka; O. Mahlasela; N. Siphambili; S. Mthethwa,10.1109/icABCD62167.2024.10645266,2024,"Distributed Ledger Technology (DL T) is a decentralised database architecture that allows multiple participants to have simultaneous access to a constantly updated digital ledger or record of information. This study presents a systematic literature review using the PRISMA framework to look at the DL T governance models. Six DL T governance models were identified: network, decentralized autonomous organisations, organisational, corporate, managerial, and operational. These models were then assessed based on how each is influenced by the four DL T governance dimensions, economical, political, technological, and social. Seven components of DL T governance were also considered during the evaluation such as stakeholders, participation, accountability, transparency, flexibility, enforcement, and decision-making. The results show that each governance model has a different level of influence from each dimension and a different level of consideration from key DL T governance components. The selection of which model to use depends on the requirements of each organisation and the users of the DL T system. Promoted results can assist organisations and researchers in selecting the best model that fits their requirements and prioritisation of dimensions and each component."
Decision Support System for Risk Assessment and Management Strategies in Distributed Software Development,A. Aslam; N. Ahmad; T. Saba; A. S. Almazyad; A. Rehman; A. Anjum; A. Khan,10.1109/ACCESS.2017.2757605,2017,"Risk management in distributed software development (DSD) is a well-researched area, providing different methods for assessing risks and suggesting control strategies. However, some of these methods are narrow in scope, only considering few risks, and are too complex to be used in practice whereas others provide many rules and guidelines which are often implicit. Moreover, the knowledge related to risks in DSD is scattered over different publications which make it difficult to find relevant information to be used in practice. This research aims to develop an automated decision support system to aid practitioners in assessing risks and deciding on suitable control strategies. In order to construct the knowledge base for the proposed decision support system, a systematic literature review (SLR) is conducted. Results of SLR are used to identify required questions, options and set of rules to implement our decision support system (DSS). In total 80 studies were identified from which 49 aspects, 53 questions, and a set of rules are extracted. DSS is evaluated through multiple case studies. The results indicate that the developed DSS supports decision-making process in risk assessment and selection of control strategy."
Making the Sourcing Decision of Software Maintenance and Information Technology,H. U. Rahman; M. Raza; P. Afsar; M. Khan; N. Iqbal; H. U. Khan,10.1109/ACCESS.2021.3051023,2021,"Outsourcing has been getting a significant growth for the last few years. Organizations tend to outsource Information Technology (IT), primarily to take advantage of the availability of qualified, trained and skilled workforce in low cost countries across the globe. Outsourcing of IT and software maintenance seem very promising, but a number of factors, risks, and challenges associated with the outsourcing process that make the sourcing decision very complicated. The present study aimed at gaining in-depth understanding of the three aspects of outsourcing, namely; perceived benefits of IT outsourcing, influencing factors of IT outsourcing and software maintenance offshoring. The findings of the current study will lead us to develop a sourcing framework for outsourcing decision as well as a decision support system for software maintenance. A systematic literature review is performed that presents perceived benefits of IT outsourcing, the influencing factors of IT outsourcing and software maintenance. Furthermore, the identified factors are analyzed based on their occurrences in literature as well as chi square test is performed to derive the significant differences amongst the factors based on decades. Similarly, critical success factors are derived both for IT outsourcing and software maintenance offshoring. Our article shows that how the critical success factors impact the IT as well the software maintenance in global delivery perspective. The findings of the current study will help the IT experts and decision makers in making suitable sourcing decisions."
Integrating Speech Input in Educational Immersive Virtual Reality Applications: A Systematic Review,N. Alghamdi; A. I. Cristea,10.1109/IS61756.2024.10705165,2024,"The topic of immersive virtual reality (IVR) in education has gained increasing attention in recent years, due to its potential to enhance learner outcomes and to mitigate learning costs. As we can capture a multitude of information from speech and generate valuable information from it, there has been an interest in exploring this source of data in such environments. Additionally, speech is being used in new and different ways in such environments. However, its specific usage in IVR-based education has not been reviewed yet. Thus, this systematic review seeks to examine, for the first time, the current state of research, specifically on using speech input - and, related to this, Natural Language Processing (NLP) - in educational IVR environments. We conducted a comprehensive search of the popular Web of Science and Scopus databases, to identify relevant papers. To properly reflect the state-of-the-art, English peer-reviewed articles published in the last 5 years (between 2020 - 2024), were included in the review, based on keywords search. 595 articles were identified and processed via the established PRISMA procedure, to exclude all duplicate or irrelevant papers, rendering 23 articles as relevant. For these, we identified the target educational subjects and the purpose of using speech as a data source. We also investigated speech recognition models and NLP models used. This systematic review provides evidence supporting the use of speech input as a valuable data source in educational IVR applications. We also propose the first, to the best of our knowledge, taxonomy for speech and NLP for IVR in education, as well as identify potential further research directions, all of which can help researchers and educators, when considering incorporating speech and NLP into educational IVR applications, to enrich the teaching and learning experience."
Transfer Learning-Based Methods for Prediction of Liver & Heart Diseases: A Review,S. C K; G. Sureshkumar,10.1109/ICCCNT56998.2023.10308322,2023,"To analyze and comprehend free text, machine learning, a subset of ""natural language processing,"" may be employed. It can be wielded in conjunction with medical data to enhance hospital triage processes, forecast patient outcomes, and flourish diagnostic algorithms that can spot early-stage chronic illnesses. As there is a greater volume of patient data to analyze in critical care and patient death prediction is widespread, these solicitations may be highly helpful. Transfer learning, a subset of ""machine learning,"" was flourished in addition to deep learning to address the issue of large datasets. Many different medicinal contexts have wielded it. This study aims to contribute a comprehensive analysis of the development and solicitation of transfer learning in ""Natural Language Processing"" (NLP) methodologies, with a focus on the analysis and forecasting of diseases, particularly heart and liver problems, using clinical pictures and free-text notes. It also examines how challenging it is for NLP techniques to comprehend clinical narratives and recognize features in medical images. The search parameters for ""PRISMA, or Preferred Reporting Items for Systematic Reviews and Meta-Analyses"", were implemented, and 31 papers were evaluated."
Wearable PPG Based BP Estimation Methods: A Systematic Review and Meta-Analysis,Z. Sastimoglu; S. Subramaniam; A. I. Faisal; W. Jiang; A. Ye; M. J. Deen,10.1109/JBHI.2024.3499834,2025,"This meta-analysis and systematic review, conducted in accordance with PRISMA guidelines, explores the efficacy of cuff-less blood pressure (BP) monitoring methods, particularly focusing on photoplethysmogram-based technologies. This comprehensive analysis carefully searched prominent databases such as MEDLINE, PubMed, AMED, Embase, and IEEE-Xplore, encompassing 25 studies with a collective participant pool of 21 142 individuals. The study primarily investigates the accuracy and practicality of continuous BP estimation devices and algorithms, aiming to assess their suitability for daily or long-term, as well as their applicability and usability across a broad population. The mean disparities were 4.14 mmHg for systolic blood pressure (SBP) and 2.79 mmHg for diastolic blood pressure (DBP), highlighting a close congruence with established measurement techniques. An in-depth analysis into specific methodologies reveals that Pulse Waveform Analysis (PWA) demonstrates a more favorable performance compared to Pulse Wave Velocity (PWV) for both SBP and DBP, although these differences are not statistically significant. The findings indicate a promising future for wearable devices in short-term BP monitoring scenarios. Both PWA and PWV methods in wearable formats have shown considerable potential as effective tools for BP assessment. However, the study underscores the need for further research, particularly targeting hypertensive populations, to validate the long-term effectiveness and reliability of these wearables. Finally, this investigation is crucial for establishing the role of wearables in ongoing, reliable BP monitoring, especially when considered in conjunction with other health monitoring technologies."
Towards a requirements engineering artefact model in the context of big data software development projects: Research in progress,D. Arruda; N. H. Madhavji,10.1109/BigData.2017.8258185,2017,"There is ample literature that suggests that the field of Big Data is growing rapidly. Also, there is emerging literature on the need to create end-user Big Data applications, as distinct from “data analytics” that typically employs machine learning algorithms to find value in large datasets for the stakeholder. A solid foundation for creating sound applications is a thorough understanding of domain and artefact models that embody artefact types and activities involved in a software project. This paper focuses on the Requirements Engineering (RE) aspect of a Big Data software project. Currently, there are no known RE artefact models to support RE process design and project understanding. To fill this void, this paper proposes a RE artefact model for Big Data end-user applications (BD-REAM). The paper also describes a method for creating the artefact model, including the basic elements and inter-relationships involved in the model."
Key technologies applied to the optimization of smart grid systems based on the Internet of Things: A Review,I. Núñez; E. Cano; C. Rovetto; K. Ojo-Gonzalez; A. Smolarz; J. J. Saldana-Barrios,10.1109/AmITIC55733.2022.9941270,2022,"This article describes an analysis of the key technologies currently applied to improve the quality, efficiency, safety and sustainability of Smart Grid systems and identifies the tools to optimize them and possible gaps in this area, considering the different energy sources, distributed generation, microgrids and energy consumption and production capacity. The research was conducted with a qualitative methodological approach, where the literature review was carried out with studies published from 2019 to 2022, in five (5) databases following the selection of studies recommended by the PRISMA guide. Of the five hundred and four (504) publications identified, ten (10) studies provided insight into the technological trends that are impacting this scenario, namely: Internet of Things, Big Data, Edge Computing, Artificial Intelligence and Blockchain. It is concluded that to obtain the best performance within Smart Grids, it is necessary to have the maximum synergy between these technologies, since this union will enable the application of advanced smart digital technology solutions to energy generation and distribution operations, thus allowing to conquer a new level of optimization."
"Roles, tasks and skills of the enterprise architect in the VUCA world",A. Ullrich; C. Bertheau; M. Wiedmann; E. Sultanow; T. Körppen; S. Bente,10.1109/EDOCW52865.2021.00057,2021,"For the last 20 years, enterprise architecture management (EAM) was primarily an instrument for harmonizing and consolidating IT landscapes and is lived as a transformation and governance discipline. It, however, is rather related to IT strategy than aligned to the actual corporate strategy and the work of the enterprise architect is characterized by tasks like prescribing, monitoring, documenting, and controlling. As digital transformation continues apace, companies are facing new challenges that lead to a volatile, uncertain, complex, and ambiguous (VUCA) world. To face these challenges, vision, understanding, clarity and agility allow to anticipative and implement necessary changes. This, of course, has implications for the role of the enterprise architect. S/he needs to start actively supporting innovation and taking more of an advisory role instead of just being driven by the current state of the enterprise architecture. This paper investigates the role of the enterprise architect in the VUCA world. Based on current literature and expert interviews, a survey was conducted among consultants who work as (or with) enterprise architects. Survey results include the evaluation of statements on current tasks of enterprise architects, their influence on projects and companies as well as future requirements on the roles of the enterprise architect. The results from the survey were synthesized with the findings from literature to derive the roles, tasks and skills of enterprise architect in the VUCA world."
Fusion of Computer Technology and Intelligent Logic Analysis Algorithm in Construction Engineering Cost Management,L. Xuan; J. Li,10.1109/ICSCDS53736.2022.9760789,2022,"This paper studies the integration of computer technology and intelligent logic analysis algorithms in construction project cost management. First, it analyzes its application in construction project cost management in detail: one is to speed up the pace of the calculation of construction project cost; the other is to be objective Fair, accurate, and real-time reflection of the construction project cost investment review system; third is to improve the accuracy of the calculation of the construction cost; fourth is to improve the efficiency of calculating the bill of quantities; the fifth is to speed up the pace of construction and bidding work; Facilitate the sharing of construction project cost information resources and other six aspects. The application of intelligent logic algorithm is studied, and the construction cost estimation based on artificial intelligence technology is studied."
Competency Model for Programming Courses in Information Technology Education (ITE) Programs from Industry Perspective: A Delphi Method,J. E. E. Goh; C. V. Mojado; H. J. T. Manaligod,10.1109/HNICEM57413.2022.10109471,2022,"This study aimed to develop a competency model for software development courses in ITE programs as perceived by the IT industry in the Philippines in the new normal. A review of previous studies revealed existing software engineering competency models being referenced by the IT industry along with the Commission on Higher Education (CHED) General Education Curriculum (GEC) being referenced by the academe. However, things drastically changed during this global pandemic which may have affected these current standards. Thus, this study aimed to determine the current competency needs in the IT industry in the new normal in relation to the programming courses being offered by the ITE programs in the Philippines; A mixed method three-round Delphi technique was used to solicit a unified expert opinion from the point of view of nine (9) IT industry experts in the Philippines. Twenty-three (23) technical skills and eleven (11) soft skills were extracted from online interviews using thematic analysis. Out of 23 technical skills, nineteen (19) have consensus. The findings produced a competency model for software development courses in the new normal consisting of must-have technical skills, game-changer technical skills, and must-have soft skills. The must-have technical skills include Web Development, Cyber Security, Cloud Computing, Agile Project Management, Mobile Development, and DevOps. The game changer technical skills include Data Science and Data Analytics, Artificial Intelligence and Machine Learning, the Internet of Things, and Research and Development. And the must-have soft skills include Communication skills, Results-Oriented, and Collaboration skills."
Language-Based Process Phase Detection in the Trauma Resuscitation,Y. Gu; X. Li; S. Chen; H. Li; R. A. Farneth; I. Marsic; R. S. Burd,10.1109/ICHI.2017.50,2017,"Process phase detection has been widely used in surgical process modeling (SPM) to track process progression. These studies mostly used video and embedded sensor data, but spoken language also provides rich semantic information directly related to process progression. We present a long-short term memory (LSTM) deep learning model to predict trauma resuscitation phases using verbal communication logs. We first use an LSTM to extract the sentence meaning representations, and then sequentially feed them into another LSTM to extract the mean-ing of a sentence group within a time window. This information is ultimately used for phase prediction. We used 24 manually-transcribed trauma resuscitation cases to train, and the remain-ing 6 cases to test our model. We achieved 79.12% accuracy, and showed performance advantages over existing visual-audio systems for critical phases of the process. In addition to language information, we evaluated a multimodal phase prediction structure that also uses audio input. We finally identified the challenges of substituting manual transcription with automatic speech recognition in trauma resuscitation."
Unveiling the Art of Software Testing Effort Estimation: An In-Depth Study of Current Techniques and their Analysis,V. Chahar; P. K. Bhatia,10.1109/ICACCTech61146.2023.00018,2023,"Software development heavily relies on the estimation of the software testing effort. It is, however, very infrequently discussed in the literature. This poses great difficulties in identifying the potential models, parameters, and datasets that should be included in the process of software testing effort estimation. Therefore, the two objectives of the paper are to analyze the existing test effort estimation techniques and identify the parameters that are effective in evaluating this estimation work. In the process, the researchers present a concise literature review of the latest methodologies and the important parameters that were evaluated by the researchers to justify their software testing effort estimation work. The authenticated articles published in the past 10 years are included to present survey outcomes and findings based on a number of inclusion and exclusion criteria. The paper proved to lay the foundation to guide future researchers in conducting research in the field of software testing effort estimation."
Automated Requirements Engineering in Agile Development: A Practitioners Survey,M. A. Umar; K. Lano,10.1109/ICECCME57830.2023.10253030,2023,"Requirements Engineering (RE) in the context of agile development is the process of discovering, analysing, validating, and managing software requirements through engaging with the stakeholders. RE activities are central to software development and its success. Advances in automation have been explored in requirements engineering activities and processes in the industry. However, the application of automated support is still limited in practice. In this study, we examine automated RE support in agile development through a qualitative analysis using industry experts. We have found that the application of automated RE support tools is primarily in the experimental stage, largely due to the limited practical implementation in industry settings. The current focus has been on research experiments, with relatively fewer instances of real-world industry application. Although experts are optimistic about its huge potential to reduce development efforts and costs if automation is harnessed. Therefore, we anticipate that these findings will be beneficial for practitioners responsible for developing RE tools for agile development, as well as for the researchers."
IsiXhosa in the Digital Age: Navigating Language Preservation and Innovation: A Systematic Scoping Review,A. Agbeyangi; N. Jere,10.1109/ACCESS.2024.3453773,2024,"Preserving and innovating indigenous languages is crucial for maintaining cultural heritage and facilitating community development in this digital age. Understanding the digital landscape of IsiXhosa language digitization efforts, however, remains underexplored. This systematic scoping review aims to map the existing literature on digitalization efforts for IsiXhosa language preservation and innovation. A scoping review was conducted, guided by PRISMA-ScR, on relevant literature pertaining to isiXhosa digitization, preservation, and innovations. Two databases were searched (Scopus and Web of Science), and additional articles were identified through a grey literature search (Google Scholar) to identify other relevant literature. Data were extracted in terms of title, year, country, language, digitization efforts, and summary of main findings and results related to isiXhosa language preservation and innovation. A total of 85 unique articles were included from 479 records, leading to the identification of five themes under the digitization efforts. Most studies were conducted in South Africa, accounting for about 78% of the total articles. As such, significant efforts were identified in grammar and morphology, speech recognition, and machine translation. There are unresolved gaps and challenges (such as large and high-quality datasets) that must be addressed. Nevertheless, the efforts demonstrate an increased acceptance of the significance of protecting and improving indigenous African languages, such as isiXhosa, in this era of digital technology."
Research on Credit Evaluation of Railway Construction Enterprises Based on Hierarchy Grey Model,X. Wang; H. Li; Y. Wei; S. Gong; X. Zhao; X. Lyu; B. Liu,10.1109/ICIBA62489.2024.10868329,2024,"As railway construction projects increasingly embrace refined management practices, the assessment of creditworthiness among railway construction enterprises has garnered heightened attention from industry regulators. Nonetheless, the practical evaluation process is beset with challenges, including inadequate evaluation index systems, subjective evaluation outcomes, and the absence of robust information technology tools to support the process. To address these issues, this study conducts research on the credit evaluation of railway construction enterprises, encompassing the development of an evaluation index system, the introduction of a credit evaluation methodology grounded in a hierarchy grey model, the design and establishment of a dedicated credit evaluation system for railway construction enterprises, and its subsequent implementation in real-world construction enterprises. The findings indicate that the proposed credit evaluation method is capable of automatically assessing the credit level of construction enterprises based on the scores of evaluation indicators, thereby offering valuable support for the bidding and management of construction projects."
Health–Related ICT Solutions of Smart Environments for Elderly–Systematic Review,P. Maresova; O. Krejcar; S. Barakovic; J. Barakovic Husic; P. Lameski; E. Zdravevski; I. Chorbev; V. Trajkovik,10.1109/ACCESS.2020.2981315,2020,"By improving the quality of life and extending the length of life, Western society is becoming an increasingly ageing population with a higher proportion of seniors. From another point of view, there is a critical shortage of care staff, both in hospitals and for in-home care. Thanks to new technology trends such as Smart Homes and Smart Furniture, there is an opportunity for increased support for seniors by utilizing new technologies. This paper presents the current trends and possibilities in applying smart information and communications technology (ICT) solutions for in-home care concerning diseases in old age. The paper consists of a systematic review according to the PRISMA methodology of the available literature in Web of Science, IEEE Xplore, PubMed, Springer, and the Espacenet patent database. Publications report the usage of some types of artificial intelligence and their implementation and non-intrusive sensing technologies. The patents review identified solutions with a focus on monitoring the state of older adults and mobility improvement. Existing ICT smart solutions must address the following issues: (1) ease-of-use; (2) invisibility and disuse that isolate older adults; (3) privacy and security; (4) affordability of technology in terms of cost; and (5) supporting elderly individuals to stay in their homes or move in different environments independently. There is a significant gap between a large number of scientific publications and commercial solutions. The existing products reflect the specifics of the diseases in a rather wider context instead of the fulfilment of exact needs. It is often stated that such devices can be used across diseases, but the direct connection and benefits for the disease is still rather weak. The challenge remains to tap the existing potential of a large number of innovative ideas on the market and improve the quality of life."
Software Development Methodologies: Analysis and Classification,D. A. Gurianov; K. S. Myshenkov; V. I. Terekhov,10.1109/REEPE57272.2023.10086852,2023,"This paper discusses software development life cycle models and development methodologies created on their basis. The place of methodologies and their role in the software development management process is determined, the differences and relationships between them are identified, as well as the degree of influence on the success of the project. A review of the criteria for comparing software development methodologies has been carried out. The existing methods of single-criteria and two-criteria classification of methodologies are analyzed, and their shortcomings are identified. New methods of single-criteria classification that eliminate the identified shortcomings are presented. A new multi-criteria classification based on four criteria has been developed, offering a more complete and broader hierarchical distribution of life cycle models and software development methodologies. Existing methods for selecting a software development methodology are considered, as well as an own method based on a retrospective analysis and machine learning methods presented. Improving methods of choosing a methodology is imperative to achieve the targets of Sustainable Development Goals (SDGs) Goal 9."
A Survey and Framework for Education in the Metaverse,J. Safari Bazargani; A. Sadeghi-Niaraki; S. -M. Choi,10.1109/ACCESS.2025.3543496,2025,"The rapid advancement of technology is reshaping education, and the metaverse stands at the forefront of this transformation. Beyond its roots in gaming and entertainment, the metaverse is poised to revolutionize how we learn by creating immersive, interactive, and personalized educational environments. However, despite its potential, a comprehensive framework for integrating the metaverse into education remains absent. This study conducts a systematic literature review to analyze and categorize existing research on education in the metaverse. The review follows a PRISMA-based methodology, utilizing Scopus, Web of Science, and PubMed databases for data collection. Studies were analyzed to identify trends, goals, and types of metaverse technologies explored. Building on this review, this paper proposes a Meta-education framework that addresses the interconnected roles of learners, instructors, and institutions in metaverse-driven education systems. The framework highlights how the metaverse enhances learner engagement, supports innovative teaching strategies, and enables institutional scalability. This study concludes that while the metaverse holds great potential to revolutionize education, significant challenges remain, including content development, instructor readiness, and integration of AI technologies. The proposed framework provides practical guidelines for future research and offers implications for educators, policymakers, and researchers aiming to create inclusive, sustainable, and transformative learning environments."
A Systematic Review on Fusion Techniques and Approaches Used in Applications,S. Jusoh; S. Almajali,10.1109/ACCESS.2020.2966400,2020,"Fusion technologies have rapidly evolved. These technologies are normally customized according to the needs of domains. Despite a large number of publications on intelligence fusion applications for various domains, they are scattered. The aim of this review is to present the state of the art for intelligence fusion applications within a specific domain. We identified three major domains for the purpose, namely robotics, military, and healthcare, during the initial process of the systematic review. These three domains are always in need of superior intelligence. Articles were searched mainly in IEEE Xplore. We limit the range of publications to the year 2014 to 2019, to focus on the most recent publications. We adopt the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) protocol to screen, filter and evaluate qualities of each retrieved article. As a result, we retrieved 675 articles at the initial stage of the search, we conducted screening and filtering process and reviewed 153 articles potential articles, and finally, we excluded 36 articles as they do not comply with our quality assessment criteria. Only 117 articles are included. The results of this study are a list of classified applications within the domains and a number of relevant techniques or approaches used in each classified application. The finding of this review showed that the most published works for the use of intelligence fusion are mainly applications in the robotics domain, where mostly used techniques are Kalman Filter and its variants. Outcomes of this study can be a guideline or an insight for researchers to further develop and implement in this field."
Wise Computing: Toward Endowing System Development with Proactive Wisdom,D. Harel; G. Katz; R. Marelly; A. Marron,10.1109/MC.2018.1451652,2018,"A broad, long-term research project is described, which will lead to the computer becoming an equal member of the system-development team, continuously making proactive contributions, akin to those expected from an experienced and knowledgeable customer or user, a conscientious QA engineer, a strict regulatory auditor, an engineering -team leader, or the organization’s CTO. The web extra at https://youtu.be/mmrv8ZACbpU describes the authors’ novel “wise computing” approach and demonstrates one possible application. The second web extra at https://youtu.be/DtpvMxMwYPM extends the discussion of the authors’ novel “wise computing” approach and presents a case study."
Recent Advances in Quantitative Gait Analysis Using Wearable Sensors: A Review,Y. Hutabarat; D. Owaki; M. Hayashibe,10.1109/JSEN.2021.3119658,2021,"The current gold standard for gait analysis involves performing the gait experiments in a laboratory environment with a constrained space. However, there is growing interest in using flexible, efficient, and inexpensive wearable sensors as tools to perform gait analysis. This review aimed to identify and summarize the current advances in wearable sensors for various aspects of gait analysis, such as the application of wearable gait analysis systems, sensor systems and their attachment locations, and the algorithms used for the analysis. The PRISMA guideline was adopted to find relevant studies from the period 2011 to 2020 from several scientific databases. A total of 76 articles were selected based on the inclusion and exclusion criteria. A wearable inertial measurement unit (IMU) attached to the lower limb region was found to be the most common approach for gait analysis. Temporal, spatial, and spatiotemporal features were the most common quantitative gait features extracted from the wearable sensors. The proposed frameworks showed varying performances, and an increased number of sensors did not necessarily improve the estimation performance metrics. A few studies have integrated various machine learning techniques for classification problems, correction algorithms, crosschecking functions, and scoring functions. Finally, this review paper discusses the challenges and future direction of the research on quantitative gait analysis."
Semantic Ontologies for Complex Healthcare Structures: A Scoping Review,A. Luschi; C. Petraccone; G. Fico; L. Pecchia; E. Iadanza,10.1109/ACCESS.2023.3248969,2023,"The healthcare environment is made up of highly complicated interactions between many technologies, activities, and people. Ensuring a solid communication between them is vital to ease the healthcare management. Semantic ontologies are knowledge representation tools that implement abstractions to fully describe a given topic in terms of subjects and relations. This scoping review aims to identify and analyse available ontologies which can depict all the available use-cases that describe the hospital environment in relation to the European project ODIN and its future expansion. The review has been conducted on the Scopus database on January 13th, 2023 using the PRISMA extensions for scoping reviews. Two reviewers screened 3,225 documents emerged from the database search. Further filtering led to a final set of 32 articles to be analysed for the results. A set of 34 ontologies extracted by the identified articles has been analysed and discussed as well. The results of this study will lead to the implementation of a common integrated ontology which could hold information about healthcare entities as well as their semantic relationships, strengthen data exchange and interconnections among people, devices and applications in an expanded scenario which include Internet of Things, robots and Artificial Intelligence."
The advancing role of digitalisation through the example of the Perlmutter project from the user side,Á. Csiszráaik-Kocsir; J. Varga,10.1109/SACI58269.2023.10158552,2023,"The events of recent years have shed new light on the crisis and change management practices of economic actors. It has been a truism that actors who are more receptive to change can be more successful and efficient than their peers in business markets. Perhaps the most significant changes in recent years have confirmed this even more. One only has to think of the impact of pandemic COVID-19, the energy crisis or the Russian-Ukrainian conflict. In a very short period of time, these events have brought about very significant changes and their impact has been largely negative for most economic actors. If it has not been sufficiently understood so far why it can be important to adapt to changes in a timely and appropriate way, or why good crisis management practices can be important, perhaps everyone will now. Meanwhile, other trends are shaping the global economy and will have an impact on the future state of the economy and society. Examples include sustainability (or the green transition) and digitalisation. Two global changes that will certainly have a long-term impact on society and business processes. No one can afford the luxury of ignoring these changes. What is more, the most competitive economic players are seeking to turn them to their advantage and to reap the benefits of sustainability or digitalisation. It is clear that our world has become faster and more complex than ever before. More and more things are changing around us, with ever more intense consequences. We need to recognise in time how we can respond to changing environmental conditions or circumstances. This is the subject of the present paper, which, after a brief literature review, draws on research findings to illustrate the importance and relevance of digitalisation."
Applications of Knowledge Technology in Construction Industry,P. Mesároš; T. Mandičák; M. Behún; J. Smetanková,10.1109/ICETA.2018.8572231,2018,"Knowledge technology represents new way and opportunity of efficiency and productivity in construction industry. Progressive tools and applications as Building Information Modelling, learning machine systems, Internet of Things and generally digitalization in construction presents way how to easier and more effective achieve goals of construction projects. Paper discusses the issue of applications and knowledge technology in construction industry. Today trend in construction field is increasing demands on technology in spite of facts, that today exploitation of information and communication technology generally is not satisfied. BIM technology is one of the most supported tool in construction industry, generally. Especially, that's true in design phase. But, there are more other applications that are in connection or not with BIM technology. All these reasons confirm the importance of discussing the issue of knowledge technology, knowledge management and applications in the field of construction. Digitalization in construction industry is key in implementation of progressive knowledge-based technology. The main aim of the paper is to analyze knowledge technology and application in construction industry from theoretical point of view. Survey should be brings overview of knowledge applications in construction industry."
A Review on White Matter Changes in Parkinson’s Disease Detected by Diffusion MRI,M. Vathanakumar; N. Ratnarajah,10.1109/MERCon63886.2024.10688457,2024,"Parkinson’s disease (PD), a progressive neurodegenerative disorder prevalent among the elderly, requires a deeper understanding of brain structural changes for improved diagnosis and treatment. Because of its clinical complexity, early identification of PD remains challenging, with diagnosis often taking up to 2.9 years from when a patient first begins to experience symptoms, with $80.6 \%$ accuracy. To address this, integrating computational neuroimaging techniques like diffusion MRI, as it reveals brain structural changes, particularly in white matter. This review focuses on using diffusion MRI to explore white matter alterations in PD, highlighting its role in identifying axonal abnormalities across different stages and subtypes of the disease. By examining major fiber bundles through tractography, analyzing brain networks using graph theory, and applying deep learning techniques for pattern recognition, we review widespread microstructural damage and structural disconnections in key white matter regions using the PRISMA model to examine relevant studies covering the period from 2013 to 2023. Unlike previous reviews, we comprehensively evaluate the white matter changes from both technical and clinical perspectives. These findings suggest paths for future research to enhance our understanding and improve prevention, early diagnosis, and treatment strategies for PD, ultimately advancing the clinical utility of diffusion MRI."
"Exploring the Potential of Metaverse Technology in Healthcare: Applications, Challenges, and Future Directions",H. Ullah; S. Manickam; M. Obaidat; S. U. A. Laghari; M. Uddin,10.1109/ACCESS.2023.3286696,2023,"In recent times, the emergence of the Metaverse has garnered worldwide attention as an innovative digital space that holds immense potential to provide a wide range of health services to medical professionals and patients. With increasing stress on healthcare systems, it has become crucial to explore the latest and cost-effective solutions that can provide fast and reliable medical services. The focus of this study, therefore, is to explore applications of metaverse in various health care systems and elaborate on how it can efficiently improve the clinical management of patients. Consequently, an in-depth assessment of the metaverse has been carried out, while covering its core fundamentals, key technologies, and diverse applications in healthcare and medicine, including but not limited to, emergency response learning, hands-on experience in anatomy learning, orthopaedics, paediatrics and so on. To carry out the study, we have used an exploratory approach to analyze qualitative data on healthcare metaverse services in our systematic review. Relevant articles from scientific databases such as Web of Science, Springer, Scopus, and IEEE have been identified, and the analysis has been conducted using the PRISMA reporting guideline to ensure transparent and comprehensive reporting. The results of the study suggest that the metaverse has the potential to transform healthcare systems by introducing novel methods for delivering healthcare services. Metaverse’s AR/VR technologies can enable remote medical consultations and training, benefiting patients and healthcare professionals. Additionally, patients can access health-related information and resources, empowering them to manage their health better and make more informed decisions."
Opportunities for Infrastructure PPP Projects in Time of COVID-19 - as a Resilience Strategy,S. Hasan; E. Elwakil; M. Hegab,10.1109/SusTech51236.2021.9467426,2021,"In December 2019, the pandemic of COVID -19 has hit the world without warning by the unprecedented scale and intensity to be declared officially as a pandemic in March 2020. Consequently, it has forced the countries to follow immediate strategies to alleviate this crisis's impacts. The lockdown and social distance were the most applied precautionary procedures. This situation has devastating consequences for both economic and social life. The construction sector is affected by this temporary paralysis, especially the public-private partnerships (PPP) infrastructure projects with a long-term contract and different stakeholders. Some earlier researches have analyzed the consequences and suggested options to alleviate the negative impacts, but potential positive risks are still spotting. Therefore, there is a need to follow proactive behavior to maximize the potentiality of opportunities. This research is exploring the positive risks for PPP infrastructure projects during COVID 19. This objective has been attained by a literature survey for allocated risks for PPP projects, factors affecting successful PPP projects, and the current practices as a response strategy to COVID -19. The research findings will contribute to building a known base of resilience plans for pandemic through two scenarios of negative and positive impacts to support PPP projects with leading practices. The revealed opportunities have been revealed based on exploring the common concepts between four main areas: 1) Positive Impacts of COVID-19, 2) Principles of resilient infrastructure, 3) Critical success factors of PPP projects, and 4) Current practices to respond to COVID-19. It was found that besides the tremendous burden on project stakeholders, users, the private corporate sector, and the public sector for the short and long term, some opportunities could be attained for PPP infrastructure projects. These revealed opportunities have been summarized as follows: 1) The need for implementing the resilience strategy and long term thinking, 2) Available time for maintenance work and updating operating systems for completed projects, 3) Priority to support and invest in innovative smart infrastructure projects that adopt artificial intelligence and smart systems that reduce human resource interferences and control the social distance, and 4) exploring the need to simplify or remedy the procurement processes of PPP. The concluded opportunities will help investors and public partners react positively during the pandemic and reduce the negative impacts and maximize positive impacts. This attitude will help PPP projects to survive in order to complete successfully after the crisis and integrate resilience into infrastructure projects."
A Survey of Software Clone Detection From Security Perspective,H. Zhang; K. Sakurai,10.1109/ACCESS.2021.3065872,2021,"For software engineering, if two code fragments are closely similar with minor modifications or even identical due to a copy-paste behavior, that is called software/code clone. Code clones can cause trouble in software maintenance and debugging process because identifying all copied compromised code fragments in other locations is time-consuming. Researchers have been working on code clone detection issues for a long time, and the discussion mainly focuses on software engineering management and system maintenance. Another considerable issue is that code cloning provides an easy way to attackers for malicious code injection. A thorough survey work of code clone identification/detection from the security perspective is indispensable for providing a comprehensive review of existing related works and proposing future potential research directions. This paper can satisfy above requirements. We review and introduce existing security-related works following three different classifications and various comparison criteria. We then discuss three further research directions, (i) deep learning-based code clone vulnerability detection, (ii) vulnerable code clone detection for 5G-Internet of Things devices, and (iii) real-time detection methods for more efficiently detecting clone attacks. These methods are more advanced and adaptive to technological development than current technologies, and still have enough research space for future studies."
Overcoming Barriers to Supply Chain Integration: Prioritization of Solutions via a Multicriteria Decision Support Method,F. H. L. Aguiar; J. K. Sagawa; L. Osiro,10.1109/ACCESS.2024.3482710,2024,"Supply Chain Integration (SCI) has an important role in influencing supply chain overall performance. Although scholars and practitioners have identified barriers and enablers to SCI, previous studies did not delve into the partners’ different perspectives regarding integration needs, or into specific solutions to overcome the barriers. This study aims to prioritize and rank supply chain integration barriers, to propose specific and practical solutions for integration, and to assess the power of each solution in mitigating the critical barriers. The perspective of experts from the focal company and from suppliers was analyzed separately. The Delphi method was employed to generate and select solutions for SCI, and the Hesitant Fuzzy Linguistic Terms Set (HFLTS) representation was utilized to deal with uncertainty and imprecision in the experts’ judgment. HFLTS was applied to the ranking of critical barriers and to the assessment of the solutions’ power. The results showed that there are two types of solutions for integration, referred to as “specific critical solutions” and “broadband solutions”, and revealed differences in the focal company’s and in the suppliers’ perspectives regarding the priority of these solutions. The study is unique, considering the simultaneous analysis of these two perspectives, the presentation of specific solutions to achieve integration, and the provision of clear guidelines for professionals regarding implementation priorities."
PaaS - Black or White: An Investigation into Software Development Model for Building Retail Industry SaaS,V. V. Hoang Pham; X. Liu; X. Zheng; M. Fu; S. V. Deshpande; W. Xia; R. Zhou; M. Abdelrazek,10.1109/ICSE-C.2017.57,2017,"One of the most important goals for Software Engineering is that end users or those people who understand software requirements but without too much programming experience can build their software products or prototypes easily. The recent success of cloud computing has made a big step towards this goal where Platform as a Service (PaaS) can provide general and comprehensive software development services within an integrated online environment for building Software as a Service (SaaS). However, currently, most PaaS are in a ""white-box"" which still requires significant learning efforts for software developers and lets alone inexperienced project managers or end users. Therefore, it is high time that we should comprehensively investigate the challenges for PaaS and provide a suitable development model. In this paper, we firstly identify and analyze the challenges for current White-PaaS through literature review. Afterwards, employing the retail industry as a typical application domain, a novel ""Black-Box"" PaaS framework is proposed which requires much less learning time and supports much more flexible and speedy SaaS design and development."
Construction of Substation Engineering Design Knowledge Graph Based on “Ontology Seven-step Method”,W. Jiang; Y. Wang; J. Hu; L. Guan; Z. Zhu,10.1109/CEEPE51765.2021.9475682,2021,"A great deal of substation engineering projects data and knowledge has been accumulated in the field of electric power survey and design. However, the lack of knowledge mining methods hinders its value creation. The knowledge management in the field hence needs a new mode. Knowledge graph has been widely adopted in some vertical industries recently. Since its schema-less nature, the knowledge graph could be constructed flexibly. It describes the complex relations between entities and concepts in the objective world. This study proposes the construction of substation engineering design domain knowledge graph. Ontology construction in this study is based on “ontology seven-step method” and in a top-down approach. Thematic clustering is considered to guide to construct ontology hierarchy manually. Concepts of substation engineering design are stored in the ontology layer, and entities from engineering projects information are in the instance layer. In this way, the structured knowledge can accelerate knowledge retrieving and assist in decision-making. Moreover, the tacit knowledge inferring can also promote knowledge innovation."
Quality Guidelines for Research Artifacts in Model-Driven Engineering,C. D. N. Damasceno; D. Strüber,10.1109/MODELS50736.2021.00036,2021,"Sharing research artifacts is known to help people to build upon existing knowledge, adopt novel contributions in practice, and increase the chances of papers receiving attention. In Model-Driven Engineering (MDE), openly providing research artifacts plays a key role, even more so as the community targets a broader use of AI techniques, which can only become feasible if large open datasets and confidence measures for their quality are available. However, the current lack of common discipline-specific guidelines for research data sharing opens the opportunity for misunderstandings about the true potential of research artifacts and subjective expectations regarding artifact quality. To address this issue, we introduce a set of guidelines for artifact sharing specifically tailored to MDE research. To design this guidelines set, we systematically analyzed general-purpose artifact sharing practices of major computer science venues and tailored them to the MDE domain. Subsequently, we conducted an online survey with 90 researchers and practitioners with expertise in MDE. We investigated our participants’ experiences in developing and sharing artifacts in MDE research and the challenges encountered while doing so. We then asked them to prioritize each of our guidelines as essential, desirable, or unnecessary. Finally, we asked them to evaluate our guidelines with respect to clarity, completeness, and relevance. In each of these dimensions, our guidelines were assessed positively by more than 92% of the participants. To foster the reproducibility and reusability of our results, we make the full set of generated artifacts available in an open repository at https://mdeartifacts.github.io/."
Database Availability Security: Handling Denial of Service (DoS) Attacks,J. Surianto; I. K. Wairooy; S. Wirananda; B. A. Makalew,10.1109/ICIMCIS63449.2024.10957544,2024,"In the data based digital age, there are risks of attacks that cause denial of service (DoS) or distributed denial of service (DDoS) which pose threats to the security and availability of databases especially with those who are affected by this which is very disturbing. This research aims at analysing the methods of detecting and preventing DoS and DDoS attacks, with more focus on the security of databases. This research identifies and evaluates various strategies employed to counter such threats to cloud and Internet of Things (IoT) systems from the literature review conducted using the PRISMA approach. The analyses show that machine learning algorithms are capable of enhancing the detection of DoS/DDoS attack's efficiency and accuracy, and the cloud and IoT security solutions, including complex load balancing and powerful firewalls, are crucial for preventing and mitigating such attacks. This study also shows that, AI should be included in the security plans and security measures should be adaptable and proactive. This work is useful for scholars, developers, and cybersecurity experts who are interested in enhancing measures to counter and prevent advanced cybercrimes and ensuring continued and reliable functionality in data centric settings."
Critical Factors Affecting Fast-Track Construction Management for Infrastructure Projects: Validated Scale Development and K-Means Clustering Algorithms Insights,J. A. Lising; D. L. Silva,10.1109/ICMSS61211.2024.00028,2024,"The fast-track project concept has become crucial in today’s competitive business landscape, driven by escalating market demands. This research aims to pinpoint success-determining factors in such projects, a quest initiated through an extensive literature review and further explored via structured interviews and questionnaires. This study unfolds in multiple phases, beginning with interviews of five experts with over 25 years of experience validating 10 critical factors: planning, scheduling, customer commitment, communication, risk management, technology adoption, project skills, material management, activity breakdown, and leadership. Subsequent phases involve quantitative approaches, including a survey with 125 respondents, predominantly Filipino professionals with substantial construction experience, and a follow-up phase with 200 Filipino respondents. These phases leverage principal component and confirmatory factor analysis, alongside machine learning techniques like K-means clustering, to analyze each success factor’s significance and perception variations. The analysis found high component loadings above 0.6 for critical success factors, confirmed by Cronbach’s Alpha values within the acceptable range of 0.6 to 0.8, but faced challenges in model fit, indicated by mixed results in Confirmatory Factor Analysis with communalities below 0.4 and inconsistent appropriate indices. The paper also looks into the application of the K-means clustering algorithm. Finally, it lays the groundwork for future research, emphasizing the need for diverse, experienced perspectives in understanding fast-track project dynamics."
A Pioneering Roadmap for ML-Driven Algorithmic Advancements in Electrical Networks,J. L. Cremer; A. Kelly; R. J. Bessa; M. Subasic; P. N. Papadopoulos; S. Young; A. Sagar; A. Marot,10.1109/ISGTEUROPE62998.2024.10863139,2024,"Advanced control, operation, and planning tools of electrical networks with ML are not straightforward. 110 experts were surveyed to show where and how ML algorithms could advance. This paper assesses this survey and research environment. Then, it develops an innovation roadmap that helps align our research community with a goal-oriented realisation of the opportunities that AI upholds. This paper finds that the R&D environment of system operators (and the surrounding research ecosystem) needs adaptation to enable faster developments with AI while maintaining high testing quality and safety. This roadmap serves system operators, academics, and labs advancing next-generation electrical network tools."
Indoor Positioning System: A Brief Review of Its Technologies and Signal-Filtering Techniques,F. Firmansyah; F. Rahma; K. D. Irianto; A. M. Shiddiqi,10.1109/SIML61815.2024.10578093,2024,"In recent years, the growing demand for location-based services has made positioning systems indispensable for mobile device users. Although the Global Positioning System (GPS) is widely used for outdoor positioning, its efficacy is significantly reduced due to obstructions that block or weaken the signal. To address this issue, Indoor Positioning Systems (IPS) offer a viable solution for indoor environments. This paper presents a comprehensive literature review of IPS from sources such as Science Direct and IEEE Xplore using the PRISMA flow diagram for systematic review. We explore the latest techniques in IPS, discuss its challenges, and examine its application in various domains to provide an updated overview of the current trends and challenges in the field. We highlight that trilateration and Kalman filtering are the predominant methods for location determination and filtering."
Active Learning in the Conditions of Slovak University Education,V. Pavliskova; F. Babic,10.1109/ICETA51985.2020.9379208,2020,"How do we know when our students are engaged? Or are they engaged at all? These questions are very important for improving the education process. If we are not able to find the right answer, the education will be only boring and demotivating obligation for both sides, teachers and students. In our paper we aim to investigate a potential of active learning approach in the conditions of Slovak university education oriented on the IT. For this purpose, we selected a study program called Business Informatics at the Faculty of Electrical Engineering and Informatics, Technical University of Kosice, This program educates IT graduates in line with current trends like data analytics or software products development. We applied suitable active learning strategies to the selected subjects and evaluated the results. In general, we can state that at the beginning it was very important to explain in details our motivation and benefits to the students. Based on them, the adoption on the students' side was faster and more positive. Equally important was regular feedback on the implemented activities. Finally, we can conclude that this improvement is not a one-off and short-term procedure."
"Recent Advances in Design, Sensing, and Autonomy of Biomimetic Robotic Fish: A Review",S. Yan; Z. Wu; J. Wang; Y. Feng; L. Yu; J. Yu; M. Tan,10.1109/TMECH.2024.3469953,2024,"The oceans process abundant resources necessary for human production and livelihood, yet the complex and variable marine environment has continued to limit human capability in detecting and utilizing these resources. Inspired by the excellent mobility of marine life, biomimetic robotic fish have emerged as a new type of autonomous underwater vehicle over the past three decades. In early reviews of robotic fish, researchers classified and discussed based on the fish species and their morphologies. The challenging problems and technological prospects of robotic fish are not revealed thoroughly due to countless species. To provide a comprehensive overview of the latest research advancements, we make a classification focusing on the integrated mechatronic designs of robotic fish. Specifically, eight hot topics concerning the propulsion mechanisms and structural designs of robotic fish have been broadly defined from literature screened with PRISMA process over the past decade, with representative seminal works in each topic being analyzed. Furthermore, this review also introduces the latest advances in underwater perception and autonomous technologies for biomimetic robotic fish, and provides an outlook on their future development. It is expected that by summarizing the latest achievements, researchers can be inspired with novel mechatronic designs to further promote the practicality research of robotic fish."
Automatic Detection of Ambiguous Software Requirements: An Insight,M. Q. Riaz; W. H. Butt; S. Rehman,10.1109/INFOMAN.2019.8714682,2019,"Requirements Engineering is one of the most important phases of the software development lifecycle. The success of the whole software project depends upon the quality of the requirements. But as we know that mostly the software requirements are stated and documented in the natural language. The requirements written in natural language can be ambiguous and inconsistent. These ambiguities and inconsistencies can lead to misinterpretations and wrong implementations in design and development phase. To address these issues a number of approaches, tools and techniques have been proposed for the automatic detection of natural language ambiguities form software requirement documents. However, to the best of our knowledge, there is very little work done to compare and analyze the differences between these tools and techniques. In this paper, we presented a state of art survey of the currently available tools and techniques for the automatic detection of natural language ambiguities from software requirements. We also focused on figuring out the popularity of different tools and techniques on the basis of citations. This research $\mathbf{will}$ help the practitioners and researchers to get the latest insights in the above-mentioned context."
"Recommendation System Issues, Approaches and Challenges Based on User Reviews",K. Benabbes; K. Housni; A. El Mezouary; A. Zellou,10.13052/jwe1540-9589.2143,2022,"With the ever-increasing volume of online information, recommender systems have been effective as a strategy to overcome information overload. They have a wide range of applications in many fields, including e-learning, e-commerce, e-government and scientific research. Recommender systems are search engines that are based on the user's browsing history to suggest a product that expresses their interests. Being usually in the form of textual comments and ratings, such reviews are a valuable source of information about users' perceptions. Recommender systems (RSs) apply various approaches to predict users' interest on information, products and services among a huge amount of available items. In this paper, we will describe the recommender system, discuss ongoing research in this field, and address the challenges, limitations and the techniques adopted. This paper also discusses how review texts are interpreted to solve some of the major problems with traditional recommendation techniques. To assess the value of a recommender system, qualitative evaluation measures are discussed as well in this research. Based on a series of selected articles published between 2008 and 2020, the study allowed us to conclude that the efficiency of RSs is strongly centered on the control of information context, the operated exploration algorithm, the method, and the type of processed data in addition to the information on users' trust."
"Knowledge Management in Construction Quality Management: Current State, Challenges, and Future Directions",X. Xiahou; G. Chen; Z. Li; X. Xu; Q. Li,10.1109/TEM.2025.3550354,2025,"Construction quality management (CQM), as one of the major activities in construction project management, relies heavily on knowledge. Unfortunately, the knowledge of CQM is diverse in format and scattered in different stakeholders within the whole construction processes. Therefore, knowledge management (KM) of CQM is underinvestigated. To offering a comprehensive view of KM in CQM, this article employed a mixed review method to critically review 87 related articles. The results indicate 1) building information modeling, ontology, and natural language processing are identified as critical technologies in KM, 2) expert system and decision support, structural health monitoring, and project management are the major application domains. This article conducts an in-depth analysis of the literature based on the three phases of quality control: pre-construction, in-construction, and post-construction. The results are discussed to critically assess the critical technologies in KM. A framework is proposed to guide the effective implementation of KM in CQM, alongside a discussion of the current challenges and opportunities. The article further identifies potential development directions for KM in CQM, including total quality management, digital twins, development of large language models, construction of “No-cost” KM platforms, uniform evaluation and standardization mechanisms, tacit knowledge capture, and confidentiality and security. A novel paradigm for knowledge-driven quality management decision-making is first introduced. This article offers a comprehensive perspective on the application of KM in CQM, which will significantly enhance the effectiveness of CQM implementation in the future."
A Review on Statistical Process Control in Healthcare: Data-Driven Monitoring Schemes,B. E. Pérez-Benítez; V. G. Tercero-Gómez; M. Khakifirooz,10.1109/ACCESS.2023.3282569,2023,"In recent years, the adoption of statistical process monitoring (SPM) techniques in healthcare has been successful. For instance, biosurveillance and biosignal monitoring have demonstrated direct benefits. As the latest reviews of the literature show, parametric SPM techniques have been implemented to evaluate the quality-of-service hospitals provide, track medical equipment, monitor safety markers, or assess the improvements made by quality projects. However, as shown in this research, world-trending topics in data science that include data-driven approaches integrated with SPM have not been reviewed. To bridge this gap and shed light on new research, a systematic review of scientific databases and a taxonomic literature analysis were performed. For the scientometric analysis, a set of bibliometric indicators were obtained to portray the performance of each subtopic, such as examining growth kinetics, identifying top authors, journals, countries and affiliations, as well as creating network maps of co-authorship and keyword co-occurrence. Additionally, the taxonomic analysis involved grouping proposals by methodological approach. Each approach was explained and discussed to identify the advantages, limitations, and challenges that researchers and practitioners may encounter. SPM researchers and practitioners require more flexibility in data-driven approaches to account for frequency unbalance, complexity, dimensionality problems, and speed. Those working in data-driven and computer-oriented areas can expand their toolbox by incorporating sequential approaches to enhance the power of their classifiers, assess risk, reduce misspecification, and adopt model-oriented mindsets."
Automated Testing for DevOps in GitHub Environment: A Comprehensive Analysis,H. H. H. A. Abdulla; F. A. Albalooshi,10.1109/ICAMIMIA60881.2023.10427702,2023,"This paper examines automated testing, a vital DevOps practice, within the GitHub development environment. It comprehensively covers various test automation types, including unit, integration, and security testing, analyzing their roles in continuous integration/deployment workflows. A particular focus is given to the advantages and challenges of implementing automated testing for DevOps on the GitHub platform. Through a detailed GitHub-based case study, the paper offers real-world insights into deploying test automation for a large project, highlighting successes and pitfalls. Emerging innovations like AI/ML in testing regarding their potential GitHub applications are also discussed. The goal is to synthesize current knowledge into a holistic overview of automated testing for DevOps on GitHub. It encompasses the significance of test automation, nuances of execution within GitHub, practical implementations, and future directions to advance automated testing in this ecosystem. The paper aims to provide technical depth and applied perspectives on this critical DevOps practice within a leading development environment."
Requirements Prioritization Techniques Review and Analysis,R. Qaddoura; A. Abu-Srhan; M. H. Qasem; A. Hudaib,10.1109/ICTCS.2017.55,2017,Requirements prioritization is considered as one of the most important activities in the requirement engineering process. This paper gives an overview of the requirements prioritization activities and techniques. It also presents how data mining and machine learning techniques have been used to prioritize the software project requirements. A comparison between these techniques is also presented.
Learning by Teaching: Professional Skills and New Technologies for University Education,M. -O. Pahl,10.1109/MCOM.001.1900248,2019,"Besides technical skills, industry increasingly demands professional skills. Teaching these at university is challenging and still rare. In computer science, new technologies and relevant tools emerge quickly. Keeping curricula up to date is challenging. We present a teaching methodology that teaches students the art of teaching. Students learn how to communicate technical content efficiently in lecture, self-learning, and teamwork settings. Students work with constructive alignment and active listening. In addition, they learn how to assess self-learning training materials systematically, and how to give constructive feedback. Finally, they access a technical topic from a new dimension by practically teaching it. During the course, the students train the described skills under guidance by developing new exercises. The article presents our course unit that teaches important professional skills and produces a continuous stream of new teaching material, which is integrated into class. The unit enables teaching relevant and new technologies early. The presented methodology has been continuously applied at the Technical University of Munich since 2013. Through 2019, more than 150 students have participated. More than 60 new exercises were created. A structured evaluation shows the power of the approach."
Prioritization-Based Taxonomy of Global Software Development Challenges: A FAHP Based Analysis,M. A. Akbar; A. Alsanad; S. Mahmood; A. Alothaim,10.1109/ACCESS.2021.3063116,2021,"Global Software Development (GSD) continues to receive interest from software industry due to potential economic benefits. Management of GSD projects is not straightforward due to involvement of different geographically distributed teams who collaborate to produce a software. The objective of this study is to prioritize the challenges faced by practitioners during management of a GSD project. A questionnaire survey was developed to collect feedback from GSD practitioners about relative importance of 20 challenges reported in literature. Next, the Fuzzy Analytical Hierarchy Process (FAHP) was used to rank the challenges associated with management of GSD projects. The study provides a prioritization-based taxonomy of challenges associated with management of GSD projects. We believe software organizations can use the taxonomy to better plan and manage GSD projects."
Video Game Selection Recommendation System,O. Veres; P. Ilchuk; O. Kots; Y. Veres,10.1109/CSIT65290.2024.10982670,2024,"This study aims to enhance the efficiency of the video game selection process for users through the implementation of advanced recommendation algorithms. The review the newest and most famous methods, means, algorithms and approaches to solving problems generation recommendations was done. Recommendation system was modeled with usage the UML diagram. Three algorithms have been implemented and analyzed the results of their work: collaborative ALS-based advisor, collaborative recommender systems utilizing EM and SVD, and content-driven recommenders. Collaborative ALS consultant showed itself the best. Developed information system can significantly improve users playing experience and make process of games selection more personalized and effective."
Recommending Good First Issues in GitHub OSS Projects,W. Xiao; H. He; W. Xu; X. Tan; J. Dong; M. Zhou,10.1145/3510003.3510196,2022,"Attracting and retaining newcomers is vital for the sustainability of an open-source software project. However, it is difficult for new-comers to locate suitable development tasks, while existing “Good First Issues” (GFI) in GitHub are often insufficient and inappropriate. In this paper, we propose RECGFI, an effective practical approach for the recommendation of good first issues to newcomers, which can be used to relieve maintainers' burden and help newcomers onboard. RECGFI models an issue with features from multiple dimensions (content, background, and dynamics) and uses an XGBoost classifier to generate its probability of being a GFI. To evaluate RECGFI, we collect 53,510 resolved issues among 100 GitHub projects and care-fully restore their historical states to build ground truth datasets. Our evaluation shows that RECGFI can achieve up to 0.853 AUC in the ground truth dataset and outperforms alternative models. Our interpretable analysis of the trained model further reveals in-teresting observations about GFI characteristics. Finally, we report latest issues (without GFI-signaling labels but recommended as GFI by our approach) to project maintainers among which 16 are confirmed as real GFIs and five have been resolved by a newcomer."
On Authority in Academia,R. Unnthorsson; C. P. Richter,10.1109/EAEEIE.2018.8534214,2018,"This paper describes and discusses the Icelandic experience of maintaining authority in University engineering courses. Professors are experiencing that students are intentionally and continuously challenging their authority; e.g. by ignoring instructions and carrying out tasks relying solely on their own understanding and talent (sometimes collectively). This makes teaching difficult and often frustrating. Several aspects of university teaching are addressed; e.g., different expectations of students and professors, different types of authority, challenges in building Professor-Student Relationships and what influence the relationship can have on the quality of teaching."
Exploring Requirement Engineering Challenges in Software Development: Insights from Global and Indonesian Landscape,M. I. Zul; S. M. Yasin; D. S. S. Sahid,10.1109/IConEEI64414.2024.10748069,2024,"Requirement Engineering (RE) is a critical component of software development, ensuring that software meets users' needs. This study aims to provide a comprehensive overview of the key challenges in applying RE globally and within the Indonesian context and also focuses on identifying their intersections. This PRISMA framework was used to conduct the literature study. 218 papers published between 2020 and 2024 were selected from leading digital libraries, including IEEE Xplore, ScienceDirect, SpringerLink, Scopus, and Google Scholar. Rigorous selection criteria and quality assessments were applied to ensure the relevance of the collected papers. The analysis identified nine major categories of RE challenges: quality, AI-based approaches, traceability and management, and human-centered approaches, which are the most widely discussed. The AI-based approach remains underrepresented in the Indonesian context, and security challenges have not been thoroughly explored. Understanding the interplay of these challenges can inform the development of more effective strategies to enhance RE practices in software development."
Factors Influencing the Design of Unbounded Rule-Based Expert Architecture for Selection of Software Development Methodologies,M. Vhutshilo; A. Kadyamatimba; N. M. Ochara; D. Tutani,10.1109/OI.2018.8535728,2018,"The extent of success of a given project can be increased by using an appropriate Project Management Methodology (PMM) that takes into account the specific characteristics of the project (such as complexity, size, budget, nature of risk, etc.). PMMs have evolved over the years to become more diverse, complex, with evolving and dynamic ICT platforms. Such PMMs have traditionally been used as frameworks to guide the project management process for decision makers (such as Project Managers, Project Owners and Project Teams). Therefore, the choice facing such decision makers in selecting an appropriate project methodology is daunting; apart from other considerations related to project characteristics such as budget, scope, schedule, performance and resource constraints. One of the vital stages of a successful software development project is selecting a good software development methodology that best suits that project. The aim of this research is to investigate the critical factors considered by project managers. These critical factors are then used as a foundation for an architecture for an unbounded rule-based expert system. A survey was conducted amongst project managers to determine the critical factors necessary for the selection of a software development methodology. From the findings of the study, the critical factors revolved around three constructs of Project Excellence Enablers, Excellent Project Management Practices, and Business Value Proposition factors. These constructs formed the basis of an unbounded rule-based architecture anchored on artificial intelligence principles."
A Review of Approaches to Detecting Software Design Patterns,J. Asaad; E. Avksentieva,10.23919/FRUCT61870.2024.10516345,2024,"Design patterns play a crucial role in modern software engineering, providing reusable solutions to common design challenges. Among the most influential collections of design patterns is the Gang of Four (GoF) patterns, which offer a timeless framework for addressing recurring design problems. This article investigates the enduring impact of GoF design patterns on software development practices, examining their utilization in contemporary software projects and frameworks. Additionally, this study conducts a thorough analysis of various design pattern detection approaches, evaluating their effectiveness and implications in real-world software development contexts. By combining theoretical frameworks with empirical studies, we aim to provide valuable insights into the role of design patterns in software engineering and offer guidance on selecting appropriate detection methods for software project."
The Role of Random Walk-Based Techniques in Enhancing Metaheuristic Optimization Algorithms—A Systematic and Comprehensive Review,A. M. Nassef; M. A. Abdelkareem; H. M. Maghrabie; A. Baroutaji,10.1109/ACCESS.2024.3466170,2024,"Metaheuristic algorithms (MHAs) occupy considerable attention among researchers because of their high performance and robustness in optimizing several engineering problems. Random walk (RW) techniques showed a significant role in improving the performance of these algorithms. Therefore, this paper aims to provide a systematic and comprehensive review of the role of three substantial random-walk (RW) strategies in enhancing the performance of MHAs. These strategies are the Gaussian, Levy Flight and Quantum random walks. The PRISMA methodology is applied through the articles obtained from four famous scientific databases. The study provides the integration mechanisms as well as the best controlling parameters’ values while integrating these RW strategies into Particle Swarm Optimization (PSO) to produce the Gaussian PSO (GPSO), Levy Flight PSO (LFPSO) and Quantum PSO (QPSO). An experimental study has been conducted to assess the performances of these algorithms in addition to the standard PSO on 23 unimodal, multimodal and fixed-dimension multimodal benchmark functions. Statistical measures have been calculated based on 30-run optimization processes. The comparisons showed that the QPSO, LFPSO, GPSO and PSO have successfully reached the optimal values of 23 standard benchmark functions with average percentages of 65%, 31%, 13% and 11%, respectively. Accordingly, the QPSO has gained the outstanding rank, especially for unimodal and multimodal functions followed by the LFPSO while the standard PSO comes in the last position preceded by the GPSO. From the results, it can be concluded that integrating random walk strategies into existing or new metaheuristic algorithms is capable of enhancing the optimization process and hence provides reliable results when applied to engineering applications."
Weighted Lexicon-based Sentiment Analysis for Women Career Traits in Information Technology,K. De Chastelain Finnigan; F. Anzum; J. Rokne; M. L. Gavrilova,10.1109/ICCICC57084.2022.10101520,2022,"In this paper, an unsupervised sentiment analysis model leveraging the Polarity Rank algorithm and sentence dependency graph is proposed to predict the sentiments of women in IT. The primary objective is to identify groups of individuals with common traits (community and job role) who had different sentiments, as well as explore the impact of Diversity Equity & Inclusion (DEI) programs on participants' career satisfaction. The model, based on a 7-point Likert scale, classified responses into Very Negative, Negative, Slightly Negative, Neutral, Slightly Positive, Positive, and Very Positive classes By delving into different demographics and different questions within the survey, it was found that DEI programs have a positive impact on career satisfaction as well as lessening the presence of Anger, Fear, and Sadness. Additionally, it was discovered that Anticipation was a dominant emotion in all responses. This paper provides a foundational look at the sentiments expressed by women in the IT industry."
"A Novel ICT Driven ESG Enabled Model for Productive, Efficient and Sustainable Development of Construction Industry",S. Pandey; G. T. Thampi,10.1109/ICICT60155.2024.10544826,2024,"Emerging ICTs (Information and Communication Technologies) have made immense growth in its computing capabilities and are being harnessed by many industries. On the other hand, the construction industry utilizes basic software solutions in its various activities and faces many challenges related to productivity, efficiency, safety, and ESG (Environment, Society, and Governance). By adopting emerging ICTs like drones, IoT, 5G, Cloud, AI/ML, etc. construction companies can enhance productivity, efficiency, and safety. EGS criteria can play a pivotal role in guiding the construction industry towards sustainable practices. Therefore, the integration of emerging ICTs with ESG practices would be a welcoming option in the construction industry. The paper's main objective is to propose an innovative ICT-driven and ESG-enabled model that can leverage emerging software solutions and digital devices with ESG practices. This integrated approach fosters a holistic & sustainable development paradigm in the construction industry by improving productivity, efficiency, social responsibility, governance practices, and environmental conservation throughout the construction life cycle. As per questionnaire survey findings, the majority of the respondents see numerous benefits for their companies with some adaptation challenges. The authors have validated the proposed model through interviews with professionals and a questionnaire survey. The survey findings highlight that 81 percent of industry professionals would like to embrace the proposed model for their enterprises to enhance productivity and efficiency with sustainability."
Comma: A Collaborative Medical Text Annotation Platform,H. Ma; X. Xu; X. Wang; Z. Guo; J. Li,10.1109/ICHI57859.2023.00097,2023,"High-quality annotated data plays an important role in data-driven or machine-learning-based medical studies. Large amounts of valuable data existed in free text records such as Electronic Health Records. Manual annotation is paramount for intelligent algorithm applications where plentiful structured data could be extracted. With the exponential growth of medical data, large-scale collaborative annotation is required where different annotation tools should be used. We investigated the existing annotation platform and discovered partial common problems that need to be solved, e.g., lack of integrated annotation workflow, absence of reasonable review and review-assisting process, frequent lexicon upload failure, prolonged auto annotation, unified export standards, etc. In response to the above requirements, we developed Comma, a collaborative medical annotation platform that allows English and Chinese annotation. Comma supports functions involving multiple ways with high success rates to upload documents, self-defined structured entity types, customized auto annotation, annotators capability assessment, various collaborative modes, statistics display, self-determined exportation, etc. It could provide a user-friendly interface, diverse functionalities, and fluent interactions for people demanding annotations. Comma is available online at http://comma.phoc.org.cn."
Intelligent Robots and Human-Robot Collaboration in the Construction Industry: A Review,H. -H. Wei; Y. Zhang; X. Sun; J. Chen; S. Li,10.26599/JIC.2023.9180002,2023,"The construction industry is a typical labor-intensive industry, which suffers from low productivity and labor shortage in the past decades. Recently, the developments in robotics and artificial intelligence technologies highlight the evolutionary reforming potential in the construction industry. An increasing number of robots are joining construction tasks and collaborating with human workers. This study reviews the major developments in intelligent robots and human-robot collaboration (HRC) in the construction industry. The technological foundations and fundamental concepts of construction robots and collaborative robots are reviewed, organized, and discussed to reveal that progress has been made. Based on a comprehensive review, the major challenges and future research directions of HRC have been proposed and examined. This study finally developed a comprehensive and in-depth discussion of the state-of-the-art implementation of robotics technologies in the construction industry and shed light on its path to future development."
Key Wearable Device Technologies Parameters for Innovative Healthcare Delivery in B5G Network: A Review,S. O. Ajakwe; C. I. Nwakanma; D. -S. Kim; J. -M. Lee,10.1109/ACCESS.2022.3173643,2022,"The future of healthcare relies heavily on the connection of humans to intelligent devices via communication networks for rapid medical response. Hence, the evaluation of the performance of smart wearable devices as veritable tools for prompt, pervasive, and proactive healthcare delivery to end-users in response to socio-economic dynamics is imperative especially as 5G unwinds and B5G emerges. Despite the boom in the wearable market and significant improvement in communication technologies, the translation of wearable data from clinical trials to valuable assets for practical medical application is burdened with varying challenges. This review provides an introspective analysis of the performance of unobtrusive wearable devices based on identified key performance indicators (KPIs) in relation to evolving generation networks in achieving innovative health care delivery. A total of 2751 articles pooled from 5 digital libraries were screened and 16 were selected for this review using PRISMA. The identified E-DISC wearable KPIs; energy efficiency, discretization, intelligence, secured network, and customizable standards are currently engrossed with both reliability and real-time issues that undermine its performance, perceptibility, and acceptability by end-users. The transformation of smart wearable devices’ data from clinical trials into intangible resources for medical application is the fulcrum of innovative healthcare actualization. Further insight on how the identified challenges can be streamlined for smooth device alignment and transition to the emerging B5G network and its eco-friendly environment is also discussed. It is hoped that this will serve as a rallying point for research direction in translating prospective wearable solutions into a valuable resource for actualizing p-health."
"Hybrid POF-VLC Systems: Recent Advances, Challenges, Opportunities, and Future Directions",R. Abdallah; M. Atef; N. Saeed,10.1109/OJCS.2025.3535663,2025,"Hybrid Polymer Optical Fiber and Visible Light Communication (POF-VLC) systems are emerging as a promising solution for high-speed, interference-free connectivity, especially in environments where traditional RF communication is constrained. This paper investigates key nonlinear impairments in POF-VLC systems, such as chromatic dispersion (CD), self-phase modulation (SPM), cross-phase modulation (XPM), four-wave mixing (FWM), and stimulated scattering, which severely degrade signal quality and limit transmission range. We review advanced modulation techniques like Orthogonal Frequency Division Multiplexing (OFDM) and Discrete Multitone Modulation (DMT), alongside traditional methods like Non-Return-to-Zero (NRZ) and On-Off Keying (OOK), evaluating their effectiveness in overcoming these challenges. Furthermore, the application of machine learning, particularly neural network-based equalizers like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), is highlighted for their potential to enhance signal quality and system performance. This review emphasizes the transformative role these advanced strategies can play in optimizing hybrid POF-VLC systems, paving the way for their integration into high-demand communication environments. Moreover, this paper presents several promising research directions, such as optimizing training algorithms, exploring deeper neural network architectures, and integrating POF-VLC systems with emerging technologies like beyond 5G, improving energy efficiency, and addressing scalability and complexity in real-time adaptive POF-VLC systems."
Evolution of Analytics in Product Management for Data-driven Feature Prioritization,S. De; I. Baroi,10.1109/PDGC56933.2022.10053237,2022,"Data is the building block for any business. From Product Development to managing the workforce, the reliance on data has powered systems to integrations with multiple data sources. The data-driven approach based on a mix of Master Data and Transactional data can affect a company’s success. This paper talks about Data-Driven Decision Making as a mindset and explores various means of analytics used by organizations to better fuel strategic and business decisions. It also explores the roles of Product Management in an organization and how analytics helps make conscious product strategy choices. A sample survey data highlights various Data Analysis techniques using SPSS Software. This study also delves into the specific issue of Autonomous Analytics, propelled by AI/ML principles, to give a data-driven parallel towards Industry 4.0 and the direction of a manufacturing environment. This paper provides a good study on how Analytics is helping current businesses and also talks about challenges for the future."
"Exploring Extended Reality (XR) in the Workplace - Applications, Challenges: Survey",K. T; M. V. Devi; S. SenthilPandi; M. Tharun,10.1109/RMKMATE59243.2023.10369447,2023,"In ordinary work life, extended reality (XR), which here collectively refers to virtual, augmented, and mixed (VR, AR, MR) reality, is becoming more prevalent. This essay offers a thorough analysis of academic works on XR that show modifications to the way labour is actually organized. We examine XR research's theoretical and methodological foundations as well as its application areas. The PRISMA statement was followed during the evaluation procedure. The primary application domains of XR were design, remote collaboration, and training. By modifying the perception of space, XR made it possible to overcome constraints imposed by time and space, safety, and resources. It is still rather uncommon to do research on XR applications in real-world workplace contexts, and it mostly focuses on three areas: teamwork, knowledge transfer evaluation, and working procedures. The most prevalent application of XR was virtual reality, albeit the hardware used varies depending on the situation. Collaboration, work habits, and knowledge transfer evaluation were the four XR research areas we defined, which roughly mirrored the application domains. Only a few recent studies used unique methods of data collection, such as videotaping participants' movement in virtual reality, in the articles we evaluated. We could not identify any XR-specific procedures in the articles we reviewed. For the time being, XR still has a lot of potential rather than definite general advantages in the workplace."
Hyperspectral Pansharpening: Review and Future Perspectives,M. Ciotola; G. Guarino; G. Vivone; J. Chanussot; A. Plaza; G. Scarpa,10.1109/IGARSS53475.2024.10640555,2024,"In this paper, a representative set of state-of-the-art methods for hyperspectral pansharpening, comprising both model- and deep learning-based ones, are reviewed and compared on four datasets from the PRISMA mission. The experimental analysis has been carried out using the most credited pansharpening quality indexes, complemented by a subjective visual inspection of sample results. The obtained outcomes have provided us a preview of the strengths and weaknesses of the latest solutions to the problem at hand, paving the way for future research lines from both the methodological and quality assessment perspectives."
The Impacts of Reward Structures on MTS-Based New Information Technology Product Development,J. C. -A. Tsai; X. Wu; J. J. Jiang,10.1109/TEM.2025.3550279,2025,"The literature on multiteam systems (MTSs) consistently emphasizes the need for multiple teams to collaborate in their efforts to accomplish overarching goals. Despite this consensus, management frequently encounters a distinct lack of coordination among teams engaged in new information technology product development (NITPD). The prevailing wisdom suggests that shared rewards can motivate intrateam members to prioritize shared responsibilities in pursuit of team-based goals, while individual rewards can motivate personal accomplishments. As research on MTS matures, in this article, we acknowledge a gap in our theoretical understanding of how better to motivate a group of teams within these systems. Our approach harnesses prevailing reward structures through a sequential mixed-method design, culminating in the establishment of an “MTS reward structures” framework. This framework integrates rewards focused on individual component teams with those targeting the MTS as a whole, taking into consideration the dynamic interplay with NITPD tasks. Qualitative findings suggest that task characteristics distinctly influence team motivation: information technology (IT) compatibility-building tasks appear to heighten perceived dependency and, thus, better motivate teams when system-based rewards are offered, while IT novelty-responding tasks seem to bolster perceived competence, resonating more with teams that receive component-team-based rewards. Our quantitative analysis, drawing from a multiwave and multisource survey involving 104 large-scale NITPDs, tests these hypothesized relationships. Results reveal that the nature of the task at hand profoundly magnifies the relationship between given reward structures and level of interteam coordination—an insight pivotal for managing and motivating teams in complex, NITPD contexts."
“Design-Led Agility: Unraveling the Influence of Design Thinking in the Agile Methodology Journey by User Experience Designers”,M. Chouki; M. Hofaidhllaoui; M. Kefi Ben Chehida; L. Giraud; M. Dabić,10.1109/TEM.2024.3486250,2024,"This article examines the impact of design thinking on the implementation of agile methods by user experience designers, with a focus on the roles of coordination and user testing as mediators. Data were collected from an online survey of 431 user experience designers in France between 2020 and 2022 and analyzed using structural equation modeling (partial least squares regressions). The results reveal a positive direct effect of research on conception and a positive direct effect of conception on user testing. In addition, a positive direct effect of user testing and coordination on the implementation of agile methods was observed. The research and conception dimensions of design thinking also exert a positive effect on agile methods through coordination. Finally, user testing fully mediates the relationship between conception and agile methods. The discussion addresses the theoretical and managerial implications of these findings and suggests directions for future research."
A Static Analysis Platform for Investigating Security Trends in Repositories,T. Sonnekalb; C. -T. Knaust; B. Gruner; C. -A. Brust; T. S. Heinze; L. v. Kurnatowski; A. Schreiber; P. Mäder,10.1109/SVM59160.2023.00005,2023,"Static analysis tools come in many forms and configurations, allowing them to handle various tasks in a (secure) development process: code style linting, bug/vulnerability detection, verification, etc., and adapt to the specific requirements of a software project, thus reducing the number of false positives.The wide range of configuration options poses a hurdle in their use for software developers, as the tools cannot be deployed out-of-the-box. However, static analysis tools only develop their full benefit if they are integrated into the software development workflow and used on regular. Vulnerability management should be integrated via version history to identify hotspots, for example.We present an analysis platform that integrates several static analysis tools that enable Git-based repositories to continuously monitor warnings across their version history. The framework is easily extensible with other tools and programming languages. We provide a visualization component in the form of a dashboard to display security trends and hotspots. Our tool can also be used to create a database of security alerts at a scale well-suited for machine learning applications such as bug or vulnerability detection."
Code Collaborate: Dissecting Team Dynamics in First-Semester Programming Students,S. Berrezueta-Guzman; P. Bassner; S. Wagner; S. Krusche,10.1109/ITHET61869.2024.10837659,2024,"Understanding collaboration patterns in introductory programming courses is essential, as teamwork is a critical skill in computer science. In professional environments, software development relies on effective teamwork, navigating diverse perspectives, and contributing to shared goals. This paper offers a comprehensive analysis of the factors influencing team efficiency and project success, providing actionable insights to enhance the effectiveness of collaborative programming education. By analyzing version control data, survey responses, and performance metrics, the study highlights the collaboration trends that emerge as first-semester students develop a 2D game project. Results indicate that students often slightly overestimate their contributions, with more engaged individuals more likely to acknowledge mistakes. Team performance shows no significant variation based on nationality or gender composition, though teams that disbanded frequently consisted of “lone wolves,” high-lighting collaboration challenges and the need for strengthened teamwork skills. Presentations closely reflected individual project contributions, with active students excelling in evaluative questioning and performing better on the final exam. Additionally, the complete absence of plagiarism underscores the effectiveness of proactive academic integrity measures, reinforcing honest collaboration in educational settings."
Learning through Formula Student Electric: Students and Staff Perspectives,N. Y. G. Lai; K. H. Wong; D. Halim; S. Mareta; L. Ran; H. Cheung,10.1109/TALE52509.2021.9678829,2021,"Formula Student is a competition held for students from different universities that mimic real-world performance motorsport racing such as Formula One. This study presents the perceptions of students and staff on the Formula Student Electric China (FSEC) team initiative in a Sino-foreign higher education institution in China. The literature showed that the FSEC and similar competitions aligned to the Problem Based Learning paradigm seek to broaden the students' knowledge and abilities. The study had explored the views and opinions of those involved, focusing on the benefits to the teaching and learning process. The qualitative paradigm was selected for this study of the process as it mainly focused on the human aspect, such as ideas, thoughts, feelings, and personal opinions about the FSEC. Data was collected through interviews with the students and staff, through process observations of the activities and a review of related activities documents and literature. A total of five key themes outlining the benefits of FSEC for students' learning and student experience were identified through thematic analysis. The Formula Student project creates an environment where the students are highly motivated to seek their knowledge through active learning and proactively engaged with their peers and related stakeholders, thus benefiting them with a rewarding educational experience. The findings will benefit future endeavors in planning for competition involvement and ensuring students' learning is also achieved as part of their university life experience."
Studying in the ‘Bazaar’: An Exploratory Study of Crowdsourced Learning in GitHub,Y. Lu; X. Mao; T. Wang; G. Yin; Z. Li; W. Wang,10.1109/ACCESS.2019.2915247,2019,"In recent years, GitHub, the most popular social coding site, has been increasingly employed for managing learning content, sharing knowledge, imparting experience, and requesting and contributing learning resources in a crowdsourced way. We thus refer to this type of e-learning practices in which people perform learning-related tasks with open calls in an online community to gain knowledge or skills in specific areas as crowdsourced learning (CL). To understand this emerging phenomenon in GitHub, we investigate the popularity of the learning projects and learners’ CL activities, first on the extracted GHTorrent learning projects and then on a selected sample of 105 popular learning projects. We then conduct an online survey of 301 learners to qualitatively understand their practices and perceptions of CL in GitHub. Our main findings reveal that the learners’ CL practices show some different characteristics from those of open source development, e.g., the learning projects have very few long-term contributors (less than 5% of all contributors). Moreover, although the learners benefit from conducting personalized learning with the high-quality content and contributions made by the community, they encounter challenges in maintaining initiatives to conduct continuous unsupervised learning and in ensuring the quality of content and contributions. Based on the findings, we discuss the reasons behind the growth of CL and provide implications on the CL platform design."
A Study on “Blockchain-Enabled Digital Twins: The Next Wave of Industrial Transformation with Future Research Challenges”,S. K. Dwivedi; M. S. Obaidat; M. Mittal; N. Ranjan; H. Tyagi; R. Amin; K. -F. Hsiao,10.1109/CITS61189.2024.10608017,2024,"A new era of creativity, efficiency, and optimisation has emerged across a number of industrial sectors with the introduction of digital twin technologies. To enhance their capabilities, digital twins are frequently combined with cutting-edge technologies like blockchain, artificial intelligence (AI), and the Internet of Things (IoT). This paper summarises and evaluates the present status of research and applications of blockchain enabled digital twins in the context of industrial sectors. This review highlights the benefits of blockchain based digital twins in the different industrial domains such as production manufacturing, product data management, construction project management and, healthcare. Furthermore, the paper addresses the challenges and future directions of digital twin adoption in industries, discussing interoperability, data security, and the need for standardized frameworks."
Technological Spotlights of Digital Transformation in Tertiary Education,T. -C. Truong; Q. B. Diep,10.1109/ACCESS.2023.3270340,2023,"In the current globalization trend, digital transformation is an indispensable requirement that affects all aspects of life. Within the education domain, it creates opportunities for tertiary education institutions to replace traditional teaching, learning, research, and operation methods with more innovative, creative, and cost-effective methods. This article aims to examine recent technological advancements to promote the transformation in tertiary education. Our research methodology consists of two main activities: 1) identifying relevant literature on the use of technology to promote transformation in tertiary education by adopting PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines; 2) conducting a thematic analysis based on the findings of the literature review; to explore the relationships between them in order to better understand the link between technological trends and digital transformation in tertiary education. The findings indicate that the current technology trends that are being concerned and deployed in the educational environment are Artificial intelligence, the Internet of Things, blockchain technology, and other relevant platforms and technologies such as Social networks, Mobile platforms, Big data analytics, Cloud computing, Robotic Process Automation, Virtual reality and Augmented reality and Additive manufacturing. The article also discusses the adjustment of tertiary education in the process of digital transformation with the support of technology and points out the shortcomings as well as research directions in the coming time."
Contextualizing Introductory App Development Course for First-Year Engineering Students,Z. Liang,10.1109/TALE54877.2022.00022,2022,"This paper presents an experience report of an introductory app development course that uses a context-based approach to teach app development to first-year engineering students in university. This course uses MIT App Inventor as the development environment to teach how to make apps in a variety of contexts (e.g., quiz app, painting app, speech translator app). Pre-course survey showed that many students were motivated to learn app development because they considered it a useful skill in real life. App development attitudes survey shows that a small portion of students had a fallacy of app development and view it as disconnected to the real world. This justifies the importance of contextualizing the learning to help students develop a correct mental model of app development. The quantitative and qualitative results of the course assessment questionnaire demonstrated the effectiveness of the learning design. Overall, this course received positive appraisal from students, and animation, machine learning/AI, and location sensing were perceived as the most attractive and useful contexts."
Collaborative Filtering Techniques for Predicting Web Service QoS Values in Static and Dynamic Environments: A Systematic and Thorough Analysis,G. Khababa; S. Bessou; F. Seghir; N. H. Harun; A. S. Almazyad; P. Jangir; A. W. Mohamed,10.1109/ACCESS.2025.3550284,2025,"In recent years, the rapid growth of Web Services (WSs) has led to a proliferation of functionally similar options, making Quality of Service (QoS) a crucial factor for users in selecting the most suitable services. Predicting QoS values and recommending optimal services remain challenging, particularly in dynamic environments. This study systematically reviews QoS prediction for web services, focusing on Collaborative Filtering (CF) techniques. Following PRISMA guidelines, 512 studies were initially identified from databases like IEEE Xplore, ACM Digital Library, and Google Scholar, using keywords such as “collaborative filtering,” “web services,” and “QoS prediction.” After rigorous screening, 146 studies underwent a full-text review. Key insights were gathered on algorithms, evaluation metrics, datasets, and performance outcomes, with a focus on CF methods and advancements in hybrid and context-aware models. Despite progress, challenges in dynamic WS environments persist, highlighting the need for adaptive and real-time prediction approaches."
Emerging Technologies for V2X Communication and Vehicular Edge Computing in the 6G era: Challenges and Opportunities for Sustainable IoV,M. Georgiades; M. S. Poullas,10.1109/DCOSS-IoT58021.2023.00108,2023,"This paper presents a comprehensive review of emerging technologies in V2X communication and Vehicular Edge Computing (VEC) for sustainable Internet of Vehicles (IoV) in the 6G era. The paper employs the PRISMA method to systematically identify, screen, select, synthesize, and report relevant literature. The objective is to identify and discuss key technologies, assess challenges and opportunities, analyze potential impacts on energy efficiency and environmental sustainability, and provide recommendations for future research directions. Challenges analysed include both technical and non-technical challenges and oppportunities include both societal benefits as well as industry opportunities. Finally and based on these findings, future research directions are proposed for striving towards sustainable IoV."
