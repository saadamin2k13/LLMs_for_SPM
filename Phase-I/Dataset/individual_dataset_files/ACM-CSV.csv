title,authors,doi,year,abstract
A New Strategy for Online Secondary-Path Modeling of Narrowband Active Noise Control,"Ma, Yaping and Xiao, Yegui and Yaping Ma and Yegui Xiao",10.1109/TASLP.2016.2633799,2017,"Online secondary-path modeling SPM based on the use of random auxiliary noise is a very promising and attractive technique for implementing active noise control ANC. In this paper, a new strategy is proposed for the online SPM of narrowband ANC NANC. A bandpass filter bank consisting of IIR notch filters is applied to the residual noise to separate the uncancelled sinusoidal noise from other broadband noises due to the injected auxiliary noise and an additive noise. The extracted sinusoidal noise is used simultaneously to scale the auxiliary noise, to clean the desired signal for the SPM subsystem, and to update the controller. The benefits of this strategy are twofold. It can effectively reduce the contribution of injected auxiliary noise to the residual noise power. Meanwhile, it can also considerably raise the convergence rate of overall system as well as the accuracy of online SPM. Preliminary analysis is also provided to support the proposed strategy. Extensive simulations with synthetic/real secondary paths and noises are performed to demonstrate the superiority of the proposed SPM strategy over the conventional techniques."
Robust &lt;italic&gt;Q&lt;/italic&gt;-Gradient Subband Adaptive Filter for Nonlinear Active Noise Control,"Yin, Kai-Li and Pu, Yi-Fei and Lu, Lu",10.1109/TASLP.2021.3102193,2021,"Active noise control (ANC) is gaining attention for attenuating noise from a remote location. Considering the problem of nonlinear active noise control (NLANC) at a virtual location, a robust filtered-s subband adaptive filtering algorithm based on the &lt;inline-formula&gt;&lt;tex-math notation=""LaTeX""&gt;$q$&lt;/tex-math&gt;&lt;/inline-formula&gt;-gradient maximum correntropy criterion (RFsSAF-qMCC) is proposed in this paper. The proposed RFsSAF-qMCC algorithm develops the functional link artificial neural network (FLANN)-SAF structure as the controller, and embeds the MCC with the concept of &lt;italic&gt;q&lt;/italic&gt;-gradient, thereby improving the convergence speed in the impulsive environment. To solve the trade-off between fast convergence and low noise residue caused by the fixed &lt;italic&gt;q&lt;/italic&gt;-gradient, a variable &lt;italic&gt;q&lt;/italic&gt;-gradient algorithm, termed as RFsSAF-vqMCC, is further developed. As an additional contribution, the convergence behavior of the proposed RFsSAF-qMCC and RFsSAF-vqMCC algorithms is analyzed. Simulation results corroborate the effectiveness of the proposed algorithms as compared to state-of-the-art algorithms."
Spatial Active Noise Control in Rooms Using Higher Order Sources,"Zhang, Junqing and Zhang, Wen and Zhang, Jihui Aimee and Abhayapala, Thushara Dheemantha and Zhang, Lijun",10.1109/TASLP.2021.3126936,2021,"All spatial active noise control (ANC) systems, when deployed in typical room environments, have time-varying acoustic channels between the secondary sources and the error microphones. The conventional online secondary path modeling techniques, which introduce additive auxiliary random noise to estimate the secondary paths, become challenging especially in a multichannel setup. In this work, we propose to use higher-order variable-directivity sound sources as secondary sources for spatial ANC, in which both the interior residual noise field within the control region and exterior sound field due to secondary source radiation are jointly controlled. The aim of controlling the exterior sound field is to minimize room reverberation generated by the secondary sources so that the secondary paths in the proposed algorithm can be approximated as free-field propagation and thus can be pre-calibrated. The system is implemented in an adaptive manner to track noise variations. The results show that the proposed method can effectively cancel spatial noise field and control exterior sound field at an acceptable low level in time-varying room environments."
Systematic Review of Machine Learning Approaches for Detecting Developmental Stuttering,"Barrett, Liam and Hu, Junchao and Howell, Peter",10.1109/TASLP.2022.3155295,2022,"A systematic review of the literature on statistical and machine learning schemes for identifying symptoms of developmental stuttering from audio recordings is reported. Twenty-seven papers met the quality standards that were set. Comparison of results across studies was not possible because training and testing data, model architecture and feature inputs varied across studies. The limitations that were identified for comparison across studies included: no indication of application for the work, data were selected for training and testing models in ways that could lead to biases, studies used different datasets and attempted to locate different symptom types, feature inputs were reported in different ways and there was no standard way of reporting performance statistics. Recommendations were made about how these problems can be addressed in future work on this topic."
SinTechSVS: A Singing Technique Controllable Singing Voice Synthesis System,"Zhao, Junchuan and Chetwin, Low Qi Hong and Wang, Ye",10.1109/TASLP.2024.3394769,2024,"The precise control of singing techniques is of utmost importance in achieving emotionally expressive vocal performances. To bridge the gap between current Singing Voice Synthesis (SVS) systems and human singers, our paper focuses on developing an SVS system that allows for control over singing techniques. In this paper, we introduce SinTechSVS, a singing technique controllable SVS system composed of a singing technique annotator, a singing technique controllable synthesizer, and a singing technique recommender. Our approach leverages transfer learning for efficient singing technique annotation and adapts the DiffSinger framework with additional style encoders and an attention-based singing technique local score (STLS) module to enhance singing technique controllability. We also propose a Seq2Seq singing technique recommender for the new task of Singing Technique Recommendation (STR). Experimental results demonstrate that SinTechSVS significantly improves the quality and expressiveness of synthesized vocal performances, with comparable general synthesis capabilities to state-of-the-art SVS systems and enhanced control over singing techniques, as evidenced by objective and subjective evaluations. To the best of our knowledge, SinTechSVS is the first SVS capable of controlling singing techniques."
Multi-Agent Deep Learning for the Detection of Multiple Speech Steganography Methods,"Sun, Congcong and Tian, Hui and Tian, Peng and Li, Haizhou and Qian, Zhenxing",10.1109/TASLP.2024.3407607,2024,"The ability to detect multiple steganographic methods in speech streams is an important prerequisite for steganalysis methods to move from theory to practical application, but it is also a challenging problem. To address this challenge, we propose a novel steganalysis method based on multi-agent deep learning, which can effectively detect multiple steganography methods in speech streams. Our method utilizes multiple agents to learn the features of multiple sub-training datasets separately and then fuses the information of each agent through the weight parameter aggregation mechanism to obtain the final weight parameter of the steganalysis model. Experimental results show that our proposed method outperforms the state-of-art steganalysis methods. In particular, for low embedding rates, the presented method increases average detection accuracy by about 9&lt;inline-formula&gt;&lt;tex-math notation=""LaTeX""&gt;$%$&lt;/tex-math&gt;&lt;/inline-formula&gt;."
A New Hybrid Active Noise Control System With Input-Power-Controlled Online Secondary-Path Modeling,"Wang, Zijie and Xiao, Yegui and Ma, Yaping and Ma, Liying and Khorasani, Khashayar",10.1109/TASLP.2024.3414271,2024,"A new hybrid active noise control (ANC, HANC) system is proposed in this paper that is equipped with an input-power-controlled online secondary-path modeling (OSPM) subsystem. An FIR linear prediction filter (LPF) is newly included that takes the FIR supporting filter (SF) error as its desired signal and separates the remaining target narrowband noise component from all the other broadband noise components. Placed right after the LPF is the OSPM subsystem. The SF and LPF output signals, namely the target broadband and narrowband components that remain in the residual error are not only used to update the feedforward and feedback ANC (FFANC, FBANC) subcontroller, respectively, but are also adopted to control the power of the OSPM-exclusive auxiliary white Gaussian noise (AWGN) to pursue a trade-off between the OSPM quality and the AWGN contribution to the residual error. The OSPM error is utilized to simultaneously update not only the OSPM subsystem but also the SF and the LPF. Due to inclusion of the LPF, the adverse coupling effects among the FFANC, the FBANC and the OSPM is reduced substantially, leaving a possibility for improving the HANC overall convergence and noise reduction performance (NRP). Furthermore, preliminary steady-state analysis of the LPF is also conducted to reveal its properties and effectiveness. Extensive simulations with both synthetical and real settings are provided and conducted to verify that the proposed HANC system is superior to existing solutions."
Integrating Imaging Genomic Data in the Quest for Biomarkers of Schizophrenia Disease,"Deng, Su-Ping and Hu, Wenxing and Calhoun, Vince D. and Wang, Yu-Ping",10.1109/TCBB.2017.2748944,2018,"It's increasingly important but difficult to determine potential biomarkers of schizophrenia SCZ disease, owing to the complex pathophysiology of this disease. In this study, a network-fusion based framework was proposed to identify genetic biomarkers of the SCZ disease. A three-step feature selection was applied to single nucleotide polymorphisms SNPs, DNA methylation, and functional magnetic resonance imaging fMRI data to select important features, which were then used to construct two gene networks in different states for the SNPs and DNA methylation data, respectively. Two health networks one is for SNP data and the other is for DNA methylation data were combined into one health network from which health minimum spanning trees MSTs were extracted. Two disease networks also followed the same procedures. Those genes with significant changes were determined as SCZ biomarkers by comparing MSTs in two different states and they were finally validated from five aspects. The effectiveness of the proposed discovery framework was also demonstrated by comparing with other network-based discovery methods. In summary, our approach provides a general framework for discovering gene biomarkers of the complex diseases by integrating imaging genomic data, which can be applied to the diagnosis of the complex diseases in the future."
Identifying Candidate Genetic Associations with MRI-Derived AD-Related ROI via Tree-Guided Sparse Learning,"Hao, Xiaoke and Yao, Xiaohui and Risacher, Shannon L. and Saykin, Andrew J. and Yu, Jintai and Wang, Huifu and Tan, Lan and Shen, Li and Zhang, Daoqiang",10.1109/TCBB.2018.2833487,2019,"Imaging genetics has attracted significant interests in recent studies. Traditional work has focused on mass-univariate statistical approaches that identify important single nucleotide polymorphisms (SNPs) associated with quantitative traits (QTs) of brain structure or function. More recently, to address the problem of multiple comparison and weak detection, multivariate analysis methods such as the least absolute shrinkage and selection operator (Lasso) are often used to select the most relevant SNPs associated with QTs. However, one problem of Lasso, as well as many other feature selection methods for imaging genetics, is that some useful prior information, e.g., the hierarchical structure among SNPs, are rarely used for designing a more powerful model. In this paper, we propose to identify the associations between candidate genetic features (i.e., SNPs) and magnetic resonance imaging (MRI)-derived measures using a tree-guided sparse learning (TGSL) method. The advantage of our method is that it explicitly models the complex hierarchical structure among the SNPs in the objective function for feature selection. Specifically, motivated by the biological knowledge, the hierarchical structures involving gene groups and linkage disequilibrium (LD) blocks as well as individual SNPs are imposed as a tree-guided regularization term in our TGSL model. Experimental studies on simulation data and the Alzheimer's Disease Neuroimaging Initiative (ADNI) data show that our method not only achieves better predictions than competing methods on the MRI-derived measures of AD-related region of interests (ROIs) (i.e., hippocampus, parahippocampal gyrus, and precuneus), but also identifies sparse SNP patterns at the block level to better guide the biological interpretation."
Use of Electronic Health Data for Disease Prediction: A Comprehensive Literature Review,"Hossain, Md. Ekramul and Khan, Arif and Moni, Mohammad Ali and Uddin, Shahadat",10.1109/TCBB.2019.2937862,2021,"Disease prediction has the potential to benefit stakeholders such as the government and health insurance companies. It can identify patients at risk of disease or health conditions. Clinicians can then take appropriate measures to avoid or minimize the risk and in turn, improve quality of care and avoid potential hospital admissions. Due to the recent advancement of tools and techniques for data analytics, disease risk prediction can leverage large amounts of semantic information, such as demographics, clinical diagnosis and measurements, health behaviours, laboratory results, prescriptions and care utilisation. In this regard, electronic health data can be a potential choice for developing disease prediction models. A significant number of such disease prediction models have been proposed in the literature over time utilizing large-scale electronic health databases, different methods, and healthcare variables. The goal of this comprehensive literature review was to discuss different risk prediction models that have been proposed based on electronic health data. Search terms were designed to find relevant research articles that utilized electronic health data to predict disease risks. Online scholarly databases were searched to retrieve results, which were then reviewed and compared in terms of the method used, disease type, and prediction accuracy. This paper provides a comprehensive review of the use of electronic health data for risk prediction models. A comparison of the results from different techniques for three frequently modelled diseases using electronic health data was also discussed in this study. In addition, the advantages and disadvantages of different risk prediction models, as well as their performance, were presented. Electronic health data have been widely used for disease prediction. A few modelling approaches show very high accuracy in predicting different diseases using such data. These modelling approaches have been used to inform the clinical decision process to achieve better outcomes."
Predicting the Prognosis of MCI Patients Using Longitudinal MRI Data,"Er, Fusun and Goularas, Dionysis",10.1109/TCBB.2020.3017872,2020,"The aim of this study is to develop a computer-aided diagnosis system with a deep-learning approach for distinguishing “Mild Cognitive Impairment (MCI) due to Alzheimer's Disease (AD)” patients among a list of MCI patients. In this system we are using the power of longitudinal data extracted from magnetic resonance (MR). For this work, a total of 294 MCI patients were selected from the ADNI database. Among them, 125 patients developed AD during their follow-up and the rest remained stable. The proposed computer-aided diagnosis system (CAD) attempts to identify brain regions that are significant for the prediction of developing AD. The longitudinal data were constructed using a 3D Jacobian-based method aiming to track the brain differences between two consecutive follow-ups. The proposed CAD system distinguishes MCI patients who developed AD from those who remained stable with an accuracy of 87.2 percent. Moreover, it does not depend on data acquired by invasive methods or cognitive tests. This work demonstrates that the use of data in different time periods contains information that is beneficial for prognosis prediction purposes that outperform similar methods and are slightly inferior only to those systems that use invasive methods or neuropsychological tests."
A Review of Fusion Methods for Omics and Imaging Data,"Huang, Weixian and Tan, Kaiwen and Zhang, Ziye and Hu, Jinlong and Dong, Shoubin",10.1109/TCBB.2022.3143900,2022,"The development of omics data and biomedical images has greatly advanced the progress of precision medicine in diagnosis, treatment, and prognosis. The fusion of omics and imaging data, i.e., omics-imaging fusion, offers a new strategy for understanding complex diseases. However, due to a variety of issues such as the limited number of samples, high dimensionality of features, and heterogeneity of different data types, efficiently learning complementary or associated discriminative fusion information from omics and imaging data remains a challenge. Recently, numerous machine learning methods have been proposed to alleviate these problems. In this review, from the perspective of fusion levels and fusion methods, we first provide an overview of preprocessing and feature extraction methods for omics and imaging data, and comprehensively analyze and summarize the basic forms and variations of commonly used and newly emerging fusion methods, along with their advantages, disadvantages and the applicable scope. We then describe public datasets and compare experimental results of various fusion methods on the ADNI and TCGA datasets. Finally, we discuss future prospects and highlight remaining challenges in the field."
PARCEL: Physics-Based Unsupervised Contrastive Representation Learning for Multi-Coil MR Imaging,"Wang, Shanshan and Wu, Ruoyou and Li, Cheng and Zou, Juan and Zhang, Ziyao and Liu, Qiegen and Xi, Yan and Zheng, Hairong",10.1109/TCBB.2022.3213669,2022,"With the successful application of deep learning to magnetic resonance (MR) imaging, parallel imaging techniques based on neural networks have attracted wide attention. However, in the absence of high-quality, fully sampled datasets for training, the performance of these methods is limited. And the interpretability of models is not strong enough. To tackle this issue, this paper proposes a Physics-bAsed unsupeRvised Contrastive rEpresentation Learning (PARCEL) method to speed up parallel MR imaging. Specifically, PARCEL has a parallel framework to contrastively learn two branches of model-based unrolling networks from augmented undersampled multi-coil k-space data. A sophisticated co-training loss with three essential components has been designed to guide the two networks in capturing the inherent features and representations for MR images. And the final MR image is reconstructed with the trained contrastive networks. PARCEL was evaluated on two vivo datasets and compared to five state-of-the-art methods. The results show that PARCEL is able to learn essential representations for accurate MR reconstruction without relying on fully sampled datasets. The code will be made available at &lt;uri&gt;https://github.com/ternencewu123/PARCEL&lt;/uri&gt;."
Genomic Machine Learning Meta-regression: Insights on Associations of Study Features With Reported Model Performance,"Barnett, Eric J. and Onete, Daniel G. and Salekin, Asif and Faraone, Stephen V.",10.1109/TCBB.2023.3343808,2023,"Many studies have been conducted with the goal of correctly predicting diagnostic status of a disorder using the combination of genomic data and machine learning. It is often hard to judge which components of a study led to better results and whether better reported results represent a true improvement or an uncorrected bias inflating performance. We extracted information about the methods used and other differentiating features in genomic machine learning models. We used these features in linear regressions predicting model performance. We tested for univariate and multivariate associations as well as interactions between features. Of the models reviewed, 46% used feature selection methods that can lead to data leakage. Across our models, the number of hyperparameter optimizations reported, data leakage due to feature selection, model type, and modeling an autoimmune disorder were significantly associated with an increase in reported model performance. We found a significant, negative interaction between data leakage and training size. Our results suggest that methods susceptible to data leakage are prevalent among genomic machine learning research, resulting in inflated reported performance. Best practice guidelines that promote the avoidance and recognition of data leakage may help the field avoid biased results."
Analysis of Cancer-Associated Mutations of POLB Using Machine Learning and Bioinformatics,"Alkhanbouli, Razan and Al-Aamri, Amira and Maalouf, Maher and Taha, Kamal and Henschel, Andreas and Homouz, Dirar",10.1109/TCBB.2024.3395777,2024,"DNA damage is a critical factor in the onset and progression of cancer. When DNA is damaged, the number of genetic mutations increases, making it necessary to activate DNA repair mechanisms. A crucial factor in the base excision repair process, which helps maintain the stability of the genome, is an enzyme called DNA polymerase &lt;inline-formula&gt;&lt;tex-math notation=""LaTeX""&gt;$boldsymbol{beta}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=""maalouf-ieq1-3395777.gif""/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; (Pol&lt;inline-formula&gt;&lt;tex-math notation=""LaTeX""&gt;$boldsymbol{beta}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href=""maalouf-ieq2-3395777.gif""/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;) encoded by the POLB gene. It plays a vital role in the repair of damaged DNA. Additionally, variations known as Single Nucleotide Polymorphisms (SNPs) in the POLB gene can potentially affect the ability to repair DNA. This study uses bioinformatics tools that extract important features from SNPs to construct a feature matrix, which is then used in combination with machine learning algorithms to predict the likelihood of developing cancer associated with a specific mutation. Eight different machine learning algorithms were used to investigate the relationship between POLB gene variations and their potential role in cancer onset. This study not only highlights the complex link between POLB gene SNPs and cancer, but also underscores the effectiveness of machine learning approaches in genomic studies, paving the way for advanced predictive models in genetic and cancer research."
Activity-Oriented Production Promotion Utility Maximization in Metaverse Social Networks,"Ni, Peikun and Zhu, Jianming and Wang, Guoqing",10.1109/TNET.2023.3309624,2023,"The continuous development of network media technology has driven the unceasing change in the online social environment, from PC social to mobile social, which are currently experiencing a new change: Metaverse social. The iteration of the social environment gives impetus to the uninterrupted upgrading of social patterns, which leads to the ceaseless innovation of the product promotion model. In this article, motivated by the characteristics of the Metaverse social, we devise virtual activity-oriented product promotion tactics. We propose an activity-oriented promotion utility maximization problem and tackle it systematically. We demonstrate the complexity and inapproximability of the problem. A continuity approximate concave relaxation method is devised to optimize the set function with a supermodularity ratio. We develop an approximate projected subgradient procedure to obtain the solution with an approximate factor guarantee. Experiments on two types (traditional social network and Metaverse social network) of real-world datasets verify the feasibility and scalability of our algorithm and model, and the research results have guiding significance for the online promotion of products."
CASE Tool Support for Variability Management in Software Product Lines,"Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz",10.1145/3034827,2017,"Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies."
A Survey on Financial Applications of Metaheuristics,"Soler-Dominguez, Amparo and Juan, Angel A. and Kizys, Renatas",10.1145/3054133,2017,"Modern heuristics or metaheuristics are optimization algorithms that have been increasingly used during the last decades to support complex decision-making in a number of fields, such as logistics and transportation, telecommunication networks, bioinformatics, finance, and the like. The continuous increase in computing power, together with advancements in metaheuristics frameworks and parallelization strategies, are empowering these types of algorithms as one of the best alternatives to solve rich and real-life combinatorial optimization problems that arise in a number of financial and banking activities. This article reviews some of the works related to the use of metaheuristics in solving both classical and emergent problems in the finance arena. A non-exhaustive list of examples includes rich portfolio optimization, index tracking, enhanced indexation, credit risk, stock investments, financial project scheduling, option pricing, feature selection, bankruptcy and financial distress prediction, and credit risk assessment. This article also discusses some open opportunities for researchers in the field, and forecast the evolution of metaheuristics to include real-life uncertainty conditions into the optimization problems being considered."
“Hands On” Visual Recognition for Visually Impaired Users,"Sosa-Garc\'{\i}a, Joan and Odone, Francesca",10.1145/3060056,2017,"Blind or visually impaired (BVI) individuals are capable of identifying an object in their hands by combining the available visual cues (if available) with manipulation. It is harder for them to associate the object with a specific brand, a model, or a type. Starting from this observation, we propose a collaborative system designed to deliver visual feedback automatically and to help the user filling this semantic gap. Our visual recognition module is implemented by means of an image retrieval procedure that provides real-time feedback, performs the computation locally on the device, and is scalable to new categories and instances. We carry out a thorough experimental analysis of the visual recognition module, which includes a comparative analysis with the state of the art. We also present two different system implementations that we test with the help of BVI users to evaluate the technical soundness, the usability, and the effectiveness of the proposed concept."
Visual Classification of Furniture Styles,"Hu, Zhenhen and Wen, Yonggang and Liu, Luoqi and Jiang, Jianguo and Hong, Richang and Wang, Meng and Yan, Shuicheng",10.1145/3065951,2017,"Furniture style describes the discriminative appearance characteristics of furniture. It plays an important role in real-world indoor decoration. In this article, we explore the furniture style features and study the problem of furniture style classification. Differing from traditional object classification, furniture style classification aims at classifying different furniture in terms of the “style” that describes its appearance (e.g., American style, Gothic style, Rococo style, etc.) rather than the “kind” that is more related to its functional structure (e.g., bed, desk, etc.). To pursue efficient furniture style features, we construct a novel dataset of furniture styles that contains 16 common style categories and implement three strategies with respect to two categories of classification, that is, handcrafted classification and learning-based classification. First, we follow the typical image classification pipeline to extract the handcrafted features and train the classifier by support vector machine. Then we use the convolutional neural network to extract learning-based features from training images. To obtain comprehensive furniture style features, we finally combine the handcrafted image classification pipeline and the learning-based network. We experimentally evaluate the performances of handcrafted features and learning-based features of each strategy, and the results show the superiority of learning-based features and also the comprehensiveness of handcrafted features."
Visual attribute transfer through deep image analogy,"Liao, Jing and Yao, Yuan and Yuan, Lu and Hua, Gang and Kang, Sing Bing",10.1145/3072959.3073683,2017,"We propose a new technique for visual attribute transfer across images that may have very different appearance but have perceptually similar semantic structure. By visual attribute transfer, we mean transfer of visual information (such as color, tone, texture, and style) from one image to another. For example, one image could be that of a painting or a sketch while the other is a photo of a real scene, and both depict the same type of scene.Our technique finds semantically-meaningful dense correspondences between two input images. To accomplish this, it adapts the notion of ""image analogy"" [Hertzmann et al. 2001] with features extracted from a Deep Convolutional Neutral Network for matching; we call our technique deep image analogy. A coarse-to-fine strategy is used to compute the nearest-neighbor field for generating the results. We validate the effectiveness of our proposed method in a variety of cases, including style/texture transfer, color/style swap, sketch/painting to photo, and time lapse."
Data Science: A Comprehensive Overview,"Cao, Longbing",10.1145/3076253,2017,"The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics."
Bridging the Chasm: A Survey of Software Engineering Practice in Scientific Programming,"Storer, Tim",10.1145/3084225,2017,"The use of software is pervasive in all fields of science. Associated software development efforts may be very large, long lived, and complex, requiring the commitment of significant resources. However, several authors have argued that the “gap” or “chasm” between software engineering and scientific programming is a serious risk to the production of reliable scientific results, as demonstrated in a number of case studies. This article reviews the research that addresses the gap, exploring how both software engineering and research practice may need to evolve to accommodate the use of software in science."
4th International Workshop on Conducting Empirical Studies in Industry (CESI 2016): Post-workshop Report,"Duarte, Carlos Henrique C. and Jedlitschka, Andreas and Bener, Ayse",10.1145/3089649.3089655,2017,"Few would deny today the importance of empirical studies in the field of Software Engineering. An increasing number of studies are being conducted involving the software industry, but, while literature abounds on idealistic empirical procedures, relatively little is known about the dynamics and complexity of conducting empirical studies in the software industry. How research results are put into action in industrial settings and how much cross company learning takes place through replication of empirical studies in different contexts? What are the impediments when attempting to follow prescriptive procedures in the organizational setting and how to best handle them? These drivers underly the organization of the fourth in a series of workshops, CESI 2016, held on 17th May, 2016 at ICSE 2016. This report summarizes the workshop details and the proceedings of the day."
Software Vulnerability Analysis and Discovery Using Machine-Learning and Data-Mining Techniques: A Survey,"Ghaffarian, Seyed Mohammad and Shahriari, Hamid Reza",10.1145/3092566,2017,"Software security vulnerabilities are one of the critical issues in the realm of computer security. Due to their potential high severity impacts, many different approaches have been proposed in the past decades to mitigate the damages of software vulnerabilities. Machine-learning and data-mining techniques are also among the many approaches to address this issue. In this article, we provide an extensive review of the many different works in the field of software vulnerability analysis and discovery that utilize machine-learning and data-mining techniques. We review different categories of works in this domain, discuss both advantages and shortcomings, and point out challenges and some uncharted territories in the field."
Tightening Contention Delays While Scheduling Parallel Applications on Multi-core Architectures,"Rouxel, Benjamin and Derrien, Steven and Puaut, Isabelle",10.1145/3126496,2017,"Multi-core systems are increasingly interesting candidates for executing parallel real-time applications, in avionic, space or automotive industries, as they provide both computing capabilities and power efficiency. However, ensuring that timing constraints are met on such platforms is challenging, because some hardware resources are shared between cores.Assuming worst-case contentions when analyzing the schedulability of applications may result in systems mistakenly declared unschedulable, although the worst-case level of contentions can never occur in practice. In this paper, we present two contention-aware scheduling strategies that produce a time-triggered schedule of the application’s tasks. Based on knowledge of the application’s structure, our scheduling strategies precisely estimate the effective contentions, in order to minimize the overall makespan of the schedule. An Integer Linear Programming (ILP) solution of the scheduling problem is presented, as well as a heuristic solution that generates schedules very close to ones of the ILP (5% longer on average), with a much lower time complexity. Our heuristic improves by 19% the overall makespan of the resulting schedules compared to a worst-case contention baseline."
PrivacyCheck: Automatic Summarization of Privacy Policies Using Data Mining,"Zaeem, Razieh Nokhbeh and German, Rachel L. and Barber, K. Suzanne",10.1145/3127519,2018,"Prior research shows that only a tiny percentage of users actually read the online privacy policies they implicitly agree to while using a website. Prior research also suggests that users ignore privacy policies because these policies are lengthy and, on average, require 2 years of college education to comprehend. We propose a novel technique that tackles this problem by automatically extracting summaries of online privacy policies. We use data mining models to analyze the text of privacy policies and answer 10 basic questions concerning the privacy and security of user data, what information is gathered from them, and how this information is used. In order to train the data mining models, we thoroughly study privacy policies of 400 companies (considering 10% of all listings on NYSE, Nasdaq, and AMEX stock markets) across industries. Our free Chrome browser extension, PrivacyCheck, utilizes the data mining models to summarize any HTML page that contains a privacy policy. PrivacyCheck stands out from currently available counterparts because it is readily applicable on any online privacy policy. Cross-validation results show that PrivacyCheck summaries are accurate 40% to 73% of the time. Over 400 independent Chrome users are currently using PrivacyCheck."
Detecting Emerging Activity-Based Working Traits through Wearable Technology,"Montanari, Alessandro and Mascolo, Cecilia and Sailer, Kerstin and Nawaz, Sarfraz",10.1145/3130951,2017,"A recent trend in corporate real-estate is Activity-Based Working (ABW). The ABW concept removes designated desks but offers different work settings designed to support typical work activities. In this context there is still a need for objective data to understand the implications of these design decisions. We aim to contribute by using automated data collection to study how ABW’s principles impact office usage and dynamics.To this aim we analyse team dynamics and employees’ tie strength in relation to space usage and organisational hierarchy using data collected with wearable devices in a company adopting ABW principles. Our findings show that the office fosters interactions across team boundaries and among the lower levels of the hierarchy suggesting a strong lateral communication. Employees also tend to have low space exploration on a daily basis which is instead more prevalent during an average week and strong social clusters seem to be resisting the ABW principles of space dynamics. With the availability of two additional data sets about social encounters in traditional offices we highlight traits emerging from the application of ABW’s principles. In particular, we observe how the absence of designated desks might be responsible for more rapid dynamics inside the office.In more general terms, this work opens the door to new and scalable technology-based methodologies to study dynamic office usage and social interactions."
Understanding software process improvement in global software development: a theoretical framework of human factors,"Khan, Arif Ali and Keung, Jacky and Hussain, Shahid and Niazi, Mahmood and Tamimy, Muhammad Manzoor Ilahi",10.1145/3131080.3131081,2017,"Presently, most of the software development organizations are adopting the phenomena of Global Software Development (GSD), mainly because of the significant return on investment it produces. However, GSD is a complex phenomenon and there are many challenges associated with it, especially that related to Software Process Improvement (SPI). The aim of this work is to identify humans' related success factors and barriers that could impact the SPI process in GSD organizations and proposed a theoretical framework of the factors in relation to SPI implementation. We have adopted the Systematic Literature Review (SLR) method in order to investigate the success factors and barriers. Using the SLR approach, total ten success factors and eight barriers were identified. The paper also reported the Critical Success Factors (CSFs) and Critical Barriers (CBs) for SPI implementation following the criteria of the factors having a frequency ≥ 50% as critical. Our results reveal that five out of ten factors are critical for SPI program. Moreover, total three barriers were ranked as the most critical barriers. Based on the analysis of the identified factors, we have presented a theoretical framework that has highlighted an association between the identified factors and the implementation of the SPI program in GSD environment."
A Unified Framework for Multi-Modal Isolated Gesture Recognition,"Duan, Jiali and Wan, Jun and Zhou, Shuai and Guo, Xiaoyuan and Li, Stan Z.",10.1145/3131343,2018,"In this article, we focus on isolated gesture recognition and explore different modalities by involving RGB stream, depth stream, and saliency stream for inspection. Our goal is to push the boundary of this realm even further by proposing a unified framework that exploits the advantages of multi-modality fusion. Specifically, a spatial-temporal network architecture based on consensus-voting has been proposed to explicitly model the long-term structure of the video sequence and to reduce estimation variance when confronted with comprehensive inter-class variations. In addition, a three-dimensional depth-saliency convolutional network is aggregated in parallel to capture subtle motion characteristics. Extensive experiments are done to analyze the performance of each component and our proposed approach achieves the best results on two public benchmarks, ChaLearn IsoGD and RGBD-HuDaAct, outperforming the closest competitor by a margin of over 10% and 15%, respectively. Our project and codes will be released at https://davidsonic.github.io/index/acm_tomm_2017.html."
Abridging source code,"Yuan, Binhang and Murali, Vijayaraghavan and Jermaine, Christopher",10.1145/3133882,2017,"In this paper, we consider the problem of source code abridgment, where the goal is to remove statements from a source code in order to display the source code in a small space, while at the same time leaving the ``important'' parts of the source code intact, so that an engineer can read the code and quickly understand purpose of the code. To this end, we develop an algorithm that looks at a number of examples, human-created source code abridgments, and learns how to remove lines from the code in order to mimic the human abridger. The learning algorithm takes into account syntactic features of the code, as well as semantic features such as control flow and data dependencies. Through a comprehensive user study, we show that the abridgments that our system produces can decrease the time that a user must look at code in order to understand its functionality, as well as increase the accuracy of the assessment, while displaying the code in a greatly reduced area."
Why Users Do Not Want to Write Together When They Are Writing Together: Users' Rationales for Today's Collaborative Writing Practices,"Wang, Dakuo and Tan, Haodan and Lu, Tun",10.1145/3134742,2017,"This study builds upon the 30-years HCI research of collaborative writing and focuses on users' experience of writing together in today's context. By interviewing 30 participants from both academia and industry, the paper examines how people write together using today's commercially available systems. The analysis focuses on the new co-editing capabilities (e.g., track changes) that are integrated into commercial tools and thus adopted by users widely in the last decade. These capabilities enable new ways of working together (e.g., directly edit the content at the character level at the same time), but users reported reluctance to fully commit to these new working styles. We thus systematically analyze users' rationales of why they do not want to write together while they are writing together with other. We argue that the development of collaborative writing tools is far from finished and these findings provide insights for the design of technology, and suggest future directions for research."
A Scalable Methodology to Guide Student Teams Executing Computing Projects,"Saltz, Jeffrey S. and Heckman, Robert R.",10.1145/3145477,2018,"This article reports on a sequential mixed-methods research study, which compared different approaches on how to guide students through a semester-long data science project. Four different methodologies, ranging from a traditional “just assign some intermediate milestones” to other more Agile methodologies, were first compared via a controlled experiment. The results of this initial experiment showed that the project methodology used made a significant difference in student outcomes. Surprisingly, the Agile Kanban approach was found to be much more effective than the Agile Scrum methodology. Based on these initial results, in the second semester, we focused on use of the Kanban methodology. The findings in the second, more qualitative phase, confirmed the methodology's usefulness and scalability. A key issue when using the scrum methodology was that the students had a very difficult time estimating what could be completed in each of their two-week sprints. The Kanban board, which visually shows and limits work-in-progress, was found to be an effective way for students to communicate with each other as well as with their instructor. In addition, Agile Kanban also streamlined the work required for instructors to efficiently provide guidance to student teams and to understand each team's status. In summary, a scalable Kanban methodology, which can be applied to a wide variety of student computing projects, was found to an effective methodology to guide and manage student projects, improving student outcomes and minimizing instructor workload."
A Centralized Platform of Open Government Data as Support to Applications in the Smart Cities Context,"Vieira, Ianegitz and Alvaro, Alexandre",10.1145/3149485.3149512,2018,"Purpose - This article aims to present the design, implementation and validation of an Open Government Data Platform.Design/methodology/approach - The development of the work took place in 4 steps: survey of the state of the art of literature; design of the open government data platform; implementation and testing of the platform; and, finally, experimental validation with a group of students of a Brazilian university.Findings - Through the validations of the platform can be noted advantages with respect to the productivity gain for the development of solutions, in the context of Smart Cities, using the proposed platform.Research limitations/implications - The experiment was developed in a controlled manner in the context of a Brazilian university. In addition, there is a need to capture more data from other town hall to store on the platform.Originality/value - The centralized storage of open government data is a tool that enables the decision-making of public managers as well as the beginning of the transformation of the present cities to a smart city."
Decentralized blockchain-based electronic marketplaces,"Subramanian, Hemang",10.1145/3158333,2017,"In a decentralized marketplace, buyers and sellers transact directly, without manipulation by intermediary platforms."
Spatially Coherent Feature Learning for Pose-Invariant Facial Expression Recognition,"Zhang, Feifei and Mao, Qirong and Shen, Xiangjun and Zhan, Yongzhao and Dong, Ming",10.1145/3176646,2018,"Feature learning has enjoyed much attention and achieved good performance in recent studies of image processing. Unlike the required training conditions often assumed there, far less labeled data is available for training emotion classification systems. In addition, current feature learning is typically performed on an entire face image without considering the dependency between features. These approaches ignore the fact that faces are structured and the neighboring features are dependent. Thus, the learned features lack the power to describe visually coherent facial images. Our method is therefore designed with the goal of simplifying the problem domain by removing expression-irrelevant factors from the input images, with a key region-based mechanism, which is an effort to reduce the amount of data required to effectively train the feature-learning methods. Meanwhile, we can construct geometric constraints between the key regions and its detected positions. To this end, we introduce a Spatially Coherent featurelearning method for Pose-invariant Facial Expression Recognition (SC-PFER). In our model, we first perform face frontalization through a 3D pose-normalization technique, which could normalize poses while preserving the identity information through synthesizing frontal faces for facial images with arbitrary views. Subsequently, we select a sequence of key regions around 51 key points in the synthetic frontal face images for efficient unsupervised feature learning. Finally, we introduce a linkage structure over the learning-based features and the corresponding geometry information of each key region to encode the dependencies of the regions. Our method, on the whole, does not require training multiple models for each specific pose and avoids separating training and parameter tuning for each pose. The proposed framework has been evaluated on two benchmark databases, BU-3DFE and SFEW, for pose-invariant Facial Expression Recognition (FER). The experimental results demonstrate that our algorithm outperforms current state-of-the-art FER methods. Specifically, our model achieves an improvement of 1.72% and 1.11% FER accuracy, on average, on BU-3DFE and SFEW, respectively."
Prioritizing lingering bugs,"Akbarinasaji, Shirin",10.1145/3178315.3178326,2018,"As the software projects become more complex, the release deci-sion is made without resolving all the bugs in the issue tracking system. Accumulation of the bugs in the bug repository is similar to nancial obligation as we borrow time and resources to engage in another activity rather than resolving the bugs. Deferring the bug in the next release may have some consequences. Therefore, the decision whether to resolve the bug in the current release or postponing it to the next release is a crucial decision. In this proposal, we study the deferred bugs (lingering bugs) against the non-deferred bugs (regular bugs). Our aim is to develop the pre-dictive model which can predict whether the bug would linger or not. Additionally, we are interested in measuring of the linger-ing bug in terms of principal (standard time it takes to x them) and risk of liability (impact). We propose to use reinforcement learning for prioritization of lingering bugs with respect to their impact."
How Far We Have Progressed in the Journey? An Examination of Cross-Project Defect Prediction,"Zhou, Yuming and Yang, Yibiao and Lu, Hongmin and Chen, Lin and Li, Yanhui and Zhao, Yangyang and Qian, Junyan and Xu, Baowen",10.1145/3183339,2018,"Background. Recent years have seen an increasing interest in cross-project defect prediction (CPDP), which aims to apply defect prediction models built on source projects to a target project. Currently, a variety of (complex) CPDP models have been proposed with a promising prediction performance.Problem. Most, if not all, of the existing CPDP models are not compared against those simple module size models that are easy to implement and have shown a good performance in defect prediction in the literature.Objective. We aim to investigate how far we have really progressed in the journey by comparing the performance in defect prediction between the existing CPDP models and simple module size models.Method. We first use module size in the target project to build two simple defect prediction models, ManualDown and ManualUp, which do not require any training data from source projects. ManualDown considers a larger module as more defect-prone, while ManualUp considers a smaller module as more defect-prone. Then, we take the following measures to ensure a fair comparison on the performance in defect prediction between the existing CPDP models and the simple module size models: using the same publicly available data sets, using the same performance indicators, and using the prediction performance reported in the original cross-project defect prediction studies.Result. The simple module size models have a prediction performance comparable or even superior to most of the existing CPDP models in the literature, including many newly proposed models.Conclusion. The results caution us that, if the prediction performance is the goal, the real progress in CPDP is not being achieved as it might have been envisaged. We hence recommend that future studies should include ManualDown/ManualUp as the baseline models for comparison when developing new CPDP models to predict defects in a complete target project."
Measuring Software Process: A Systematic Mapping Study,"Meidan, Ayman and Garc\'{\i}a-Garc\'{\i}a, Juli\'{a}n A. and Ramos, Isabel and Escalona, Mar\'{\i}a Jos\'{e}",10.1145/3186888,2018,"Context: Measurement is essential to reach predictable performance and high capability processes. It provides support for better understanding, evaluation, management, and control of the development process and project, as well as the resulting product. It also enables organizations to improve and predict its process's performance, which places organizations in better positions to make appropriate decisions. Objective: This study aims to understand the measurement of the software development process, to identify studies, create a classification scheme based on the identified studies, and then to map such studies into the scheme to answer the research questions. Method: Systematic mapping is the selected research methodology for this study. Results: A total of 462 studies are included and classified into four topics with respect to their focus and into three groups based on the publishing date. Five abstractions and 64 attributes were identified, 25 methods/models and 17 contexts were distinguished. Conclusion: capability and performance were the most measured process attributes, while effort and performance were the most measured project attributes. Goal Question Metric and Capability Maturity Model Integration were the main methods and models used in the studies, whereas agile/lean development and small/medium-size enterprise were the most frequently identified research contexts."
Designing for human-machine collaboration: smart hearing aids as wearable technologies,"Kennedy, Krista",10.1145/3188387.3188391,2018,"This study examines design aspects that shape human/machine collaboration between wearers of smart hearing aids and their networked aids. The Starkey Halo hearing aid and the TruLink iPhone app that facilitates real-time adjustments by the wearer offer a case study in designing for this sort of collaboration and for the wearer's rhetorical management of disability disclosure in social contexts. Through close textual analysis of the company's promotional materials for patient and professional audiences as well as interface analysis and autoethnography, I examine the ways that close integration between the wearer, onboard algorithms and hardware, and geolocative telemetry shape everyday interactions in multiple hearing situations. Reliance on ubiquitous, familiar hardware such as smart phones and intuitive interface design can drive patient comfort and adoption rates of these complex technologies that influence cognitive health, social connectedness, and crucial information access."
A Survey of Attention Management Systems in Ubiquitous Computing Environments,"Anderson, Christoph and H\""{u}bener, Isabel and Seipp, Ann-Kathrin and Ohly, Sandra and David, Klaus and Pejovic, Veljko",10.1145/3214261,2018,"Today's information and communication devices provide always-on connectivity, instant access to an endless repository of information, and represent the most direct point of contact to almost any person in the world. Despite these advantages, devices such as smartphones or personal computers lead to the phenomenon of attention fragmentation, continuously interrupting individuals' activities and tasks with notifications. Attention management systems aim to provide active support in such scenarios, managing interruptions, for example, by postponing notifications to opportune moments for information delivery. In this article, we review attention management system research with a particular focus on ubiquitous computing environments. We first examine cognitive theories of attention and extract guidelines for practical attention management systems. Mathematical models of human attention are at the core of these systems, and in this article, we review sensing and machine learning techniques that make such models possible. We then discuss design challenges towards the implementation of such systems, and finally, we investigate future directions in this area, paving the way for new approaches and systems supporting users in their attention management."
"Engagement in HCI: Conception, Theory and Measurement","Doherty, Kevin and Doherty, Gavin",10.1145/3234149,2018,"Engaging users is a priority for designers of products and services of every kind. The need to understand users’ experiences has motivated a focus on user engagement across computer science. However, to date, there has been limited review of how Human--Computer Interaction and computer science research interprets and employs the concept. Questions persist concerning its conception, abstraction, and measurement. This article presents a systematic review of engagement spanning a corpus of 351 articles and 102 definitions. We map the current state of engagement research, including the diverse interpretation, theory, and measurement of the concept. We describe the ecology of engagement and strategies for the design of engaging experiences, discuss the value of the concept and its relationship to other terms, and present a set of guidelines and opportunities for future research."
Person-Job Fit: Adapting the Right Talent for the Right Job with Joint Representation Learning,"Zhu, Chen and Zhu, Hengshu and Xiong, Hui and Ma, Chao and Xie, Fang and Ding, Pengliang and Li, Pan",10.1145/3234465,2018,"Person-Job Fit is the process of matching the right talent for the right job by identifying talent competencies that are required for the job. While many qualitative efforts have been made in related fields, it still lacks quantitative ways of measuring talent competencies as well as the job’s talent requirements. To this end, in this article, we propose a novel end-to-end data-driven model based on a Convolutional Neural Network (CNN), namely, the Person-Job Fit Neural Network (PJFNN), for matching a talent qualification to the requirements of a job. To be specific, PJFNN is a bipartite neural network that can effectively learn the joint representation of Person-Job fitness from historical job applications. In particular, due to the design of a hierarchical representation structure, PJFNN can not only estimate whether a candidate fits a job but also identify which specific requirement items in the job posting are satisfied by the candidate by measuring the distances between corresponding latent representations. Finally, the extensive experiments on a large-scale real-world dataset clearly validate the performance of PJFNN in terms of Person-Job Fit prediction. Also, we provide effective data visualization to show some job and talent benchmark insights obtained by PJFNN."
Corp to Cloud: Google’s Virtual Desktops: How Google moved its virtual desktops to the cloud,"Fata, Matt and Arida, Philippe-Joseph and Hahn, Patrick and Beyer, Betsy",10.1145/3236386.3264508,2018,"Over one-fourth of Googlers use internal, data-center-hosted virtual desktops. This on-premises offering sits in the corporate network and allows users to develop code, access internal resources, and use GUI tools remotely from anywhere in the world. Among its most notable features, a virtual desktop instance can be sized according to the task at hand, has persistent user storage, and can be moved between corporate data centers to follow traveling Googlers. Until recently, our virtual desktops were hosted on commercially available hardware on Google’s corporate network using a homegrown open-source virtual cluster-management system called Ganeti. Today, this substantial and Google-critical workload runs on GCP (Google Compute Platform). This article discusses the reasons for the move to GCP, and how the migration was accomplished."
"A system for acquiring, processing, and rendering panoramic light field stills for virtual reality","Overbeck, Ryan S. and Erickson, Daniel and Evangelakos, Daniel and Pharr, Matt and Debevec, Paul",10.1145/3272127.3275031,2018,"We present a system for acquiring, processing, and rendering panoramic light field still photography for display in Virtual Reality (VR). We acquire spherical light field datasets with two novel light field camera rigs designed for portable and efficient light field acquisition. We introduce a novel real-time light field reconstruction algorithm that uses a per-view geometry and a disk-based blending field. We also demonstrate how to use a light field prefiltering operation to project from a high-quality offline reconstruction model into our real-time model while suppressing artifacts. We introduce a practical approach for compressing light fields by modifying the VP9 video codec to provide high quality compression with real-time, random access decompression.We combine these components into a complete light field system offering convenient acquisition, compact file size, and high-quality rendering while generating stereo views at 90Hz on commodity VR hardware. Using our system, we built a freely available light field experience application called Welcome to Light Fields featuring a library of panoramic light field stills for consumer VR which has been downloaded over 15,000 times."
"Trust in Data Science: Collaboration, Translation, and Accountability in Corporate Data Science Projects","Passi, Samir and Jackson, Steven J.",10.1145/3274405,2018,"The trustworthiness of data science systems in applied and real-world settings emerges from the resolution of specific tensions through situated, pragmatic, and ongoing forms of work. Drawing on research in CSCW, critical data studies, and history and sociology of science, and six months of immersive ethnographic fieldwork with a corporate data science team, we describe four common tensions in applied data science work: (un)equivocal numbers, (counter)intuitive knowledge, (in)credible data, and (in)scrutable models. We show how organizational actors establish and re-negotiate trust under messy and uncertain analytic conditions through practices of skepticism, assessment, and credibility. Highlighting the collaborative and heterogeneous nature of real-world data science, we show how the management of trust in applied corporate data science settings depends not only on pre-processing and quantification, but also on negotiation and translation. We conclude by discussing the implications of our findings for data science research and practice, both within and beyond CSCW."
"See No Evil, Hear No Evil: Audio-Visual-Textual Cyberbullying Detection","Soni, Devin and Singh, Vivek K.",10.1145/3274433,2018,"Emerging multimedia communication apps are allowing for more natural communication and richer user engagement. At the same time, they can be abused to engage in cyberbullying, which can cause significant psychological harm to those affected. Thus, with the growth in multimodal communication platforms, there is an urgent need to devise multimodal methods for cyberbullying detection and prevention. However, there are no existing approaches that use automated audio and video analysis to complement textual analysis. Based on the analysis of a human-labeled cyberbullying data-set of Vine ""media sessions' (six-second videos, with audio, and corresponding text comments), we report that: 1) multiple audio and visual features are significantly associated with the occurrence of cyberbullying, and 2) audio and video features complement textual features for more accurate and earlier cyberbullying detection. These results pave the way for more effective cyberbullying detection in emerging multimodal (audio, visual, virtual reality) social interaction spaces."
Making Sense of Group Chat through Collaborative Tagging and Summarization,"Zhang, Amy X. and Cranshaw, Justin",10.1145/3274465,2018,"While group chat is becoming increasingly popular for team collaboration, these systems generate long streams of unstructured back-and-forth discussion that are difficult to comprehend. In this work, we investigate ways to enrich the representation of chat conversations, using techniques such as tagging and summarization, to enable users to better make sense of chat. Through needfinding interviews with 15 active group chat users, who were shown mock-up alternative chat designs, we found the importance of structured representations, including signals such as discourse acts. We then developed Tilda, a prototype system that enables people to collaboratively enrich their chat conversation while conversing. From lab evaluations, we examined the ease of marking up chat using Tilda as well as the effectiveness of Tilda-enabled summaries for getting an overview. From a field deployment, we found that teams actively engaged with Tilda both for marking up their chat as well as catching up on chat."
Simulating Experiential Learning in Professional Online Courses,"Gasson, Susan and Waters, James",10.1145/3290768.3290774,2018,"Few studies have explored how to simulate the experiential learning of professional knowledge domains in online graduate courses. This paper addresses how to provide the type of deep, socially situated learning that is achieved in professional practice by employing peer and vicarious learning in discussion forums. We start by describing what we have learned about scaffolding peer learning environments from prior studies. We distinguish vicarious learning, which exposes and sensitizes students to situated knowledge based on observation of peer discussions, from peer interactions which engage students actively in co-constructing knowledge about professional practice. Then we present findings from a 10-week graduate course in Information Systems Project Management. Our findings present evidence for vicarious learning from observing peer interactions, and of interactive peer learning that not only demonstrates the co-construction of knowledge across learners but also uncovers a collaborative process based on a sense of community identity. We end with a substantive theory of social cognition in community learning based on our findings. The intention is to provide a framework for the evaluation of collaborative knowledge construction, so that courses may support deep, experiential learning in graduate online professional education."
Software Effort Interval Prediction via Bayesian Inference and Synthetic Bootstrap Resampling,"Song, Liyan and Minku, Leandro L. and Yao, Xin",10.1145/3295700,2019,"Software effort estimation (SEE) usually suffers from inherent uncertainty arising from predictive model limitations and data noise. Relying on point estimation only may ignore the uncertain factors and lead project managers (PMs) to wrong decision making. Prediction intervals (PIs) with confidence levels (CLs) present a more reasonable representation of reality, potentially helping PMs to make better-informed decisions and enable more flexibility in these decisions. However, existing methods for PIs either have strong limitations or are unable to provide informative PIs. To develop a “better” effort predictor, we propose a novel PI estimator called Synthetic Bootstrap ensemble of Relevance Vector Machines (SynB-RVM) that adopts Bootstrap resampling to produce multiple RVM models based on modified training bags whose replicated data projects are replaced by their synthetic counterparts. We then provide three ways to assemble those RVM models into a final probabilistic effort predictor, from which PIs with CLs can be generated. When used as a point estimator, SynB-RVM can either significantly outperform or have similar performance compared with other investigated methods. When used as an uncertain predictor, SynB-RVM can achieve significantly narrower PIs compared to its base learner RVM. Its hit rates and relative widths are no worse than the other compared methods that can provide uncertain estimation."
Comparing and Combining Interaction Data and Eye-tracking Data for the Real-time Prediction of User Cognitive Abilities in Visualization Tasks,"Conati, Cristina and Lall\'{e}, S\'{e}bastien and Rahman, Md Abed and Toker, Dereck",10.1145/3301400,2020,"Previous work has shown that some user cognitive abilities relevant for processing information visualizations can be predicted from eye-tracking data. Performing this type of user modeling is important for devising visualizations that can detect a user's abilities and adapt accordingly during the interaction. In this article, we extend previous user modeling work by investigating for the first time interaction data as an alternative source to predict cognitive abilities during visualization processing when it is not feasible to collect eye-tracking data. We present an extensive comparison of user models based solely on eye-tracking data, on interaction data, as well as on a combination of the two. Although we found that eye-tracking data generate the most accurate predictions, results show that interaction data can still outperform a majority-class baseline, meaning that adaptation for interactive visualizations could be enabled even when it is not feasible to perform eye tracking, using solely interaction data. Furthermore, we found that interaction data can predict several cognitive abilities with better accuracy at the very beginning of the task than eye-tracking data, which are valuable for delivering adaptation early in the task. We also extend previous work by examining the value of multimodal classifiers combining interaction data and eye-tracking data, with promising results for some of our target user cognitive abilities. Next, we contribute to previous work by extending the type of visualizations considered and the set of cognitive abilities that can be predicted from either eye-tracking data and interaction data. Finally, we evaluate how noise in gaze data impacts prediction accuracy and find that retaining rather noisy gaze datapoints can yield equal or even better predictions than discarding them, a novel and important contribution for devising adaptive visualizations in real settings where eye-tracking data are typically noisier than in laboratory settings."
"Insight Into Insiders and IT: A Survey of Insider Threat Taxonomies, Analysis, Modeling, and Countermeasures","Homoliak, Ivan and Toffalini, Flavio and Guarnizo, Juan and Elovici, Yuval and Ochoa, Mart\'{\i}n",10.1145/3303771,2019,"Insider threats are one of today’s most challenging cybersecurity issues that are not well addressed by commonly employed security solutions. In this work, we propose structural taxonomy and novel categorization of research that contribute to the organization and disambiguation of insider threat incidents and the defense solutions used against them. The objective of our categorization is to systematize knowledge in insider threat research while using an existing grounded theory method for rigorous literature review. The proposed categorization depicts the workflow among particular categories that include incidents and datasets, analysis of incidents, simulations, and defense solutions. Special attention is paid to the definitions and taxonomies of the insider threat; we present a structural taxonomy of insider threat incidents that is based on existing taxonomies and the 5W1H questions of the information gathering problem. Our survey will enhance researchers’ efforts in the domain of insider threat because it provides (1) a novel structural taxonomy that contributes to orthogonal classification of incidents and defining the scope of defense solutions employed against them, (2) an overview on publicly available datasets that can be used to test new detection solutions against other works, (3) references of existing case studies and frameworks modeling insiders’ behaviors for the purpose of reviewing defense solutions or extending their coverage, and (4) a discussion of existing trends and further research directions that can be used for reasoning in the insider threat domain."
Information architects: what they do and how to become one,"Swope, Amber",10.1145/3309578.3309582,2019,"Every organization relies on information to communicate with prospects and customers - blog posts, articles, whitepapers, user manuals, web portals, videos, tweets, social media posts, moderated forums, and more. This means that many people are creating content and are delivering it in multiple ways. To meet our users' needs, we need information architecture (IA) to provide the framework for developing and delivering this information.Although most content creators do not think of themselves as information architects, many of them perform tasks that are information architecture responsibilities. If you decide what information gets created and delivered, identify keywords to support findability, or organize the hierarchy for a table of contents, you are performing IA tasks. To learn who was performing these tasks and how they ended up with this role, I conducted a survey. This article presents my analysis of the results based upon my experience and relevant industry sources."
"Digital Heritage as a Scholarly Field—Topics, Researchers, and Perspectives from a Bibliometric Point of View","M\""{u}nster, Sander",10.1145/3310012,2019,"Digital heritage comprises a broad variety of approaches and topics and involves researchers from multiple disciplines. Against this background, this article presents a four-stage investigation on standards, publications, disciplinary cultures, as well as scholars in the field of digital heritage and particularly tangible objects as monuments and sites, carried out in 2016 and 2017. It includes results of (1) the inquiry of nearly 4,000 publications from major conferences, (2) a workshop-based survey involving 44 researchers, (3) 15 qualitative interviews, as well as (4) two online surveys with 1,000 and 700 participants, respectively. As an overall finding, the community is driven by researchers from European countries, especially Italy, with a background in humanities. Cross-national co-authorships are promoted by cultural and spatial closeness and—probably due to funding policy—EU membership. A discourse is primarily driven by technologies, and the most common keywords refer to the technologies used. Most prominent research areas are data acquisition and management, visualization, and analysis. Recent topics are, for instance, unmanned airborne vehicle (UAV)-based 3D surveying technologies, augmented and virtual reality visualization, metadata and paradata standards for documentation, and virtual museums. Since a lack of money is named as the biggest obstacle nowadays, competency and human resources are most frequently named as demand. An epistemic culture in the scholarly field of digital heritage is closer to engineering than to humanities. Moreover, conference series are most relevant for a scientific discourse, and especially EU projects set pace as most important research endeavors."
Automated program repair,"Le Goues, Claire and Pradel, Michael and Roychoudhury, Abhik",10.1145/3318162,2019,Automated program repair can relieve programmers from the burden of manually fixing the ever-increasing number of programming mistakes.
Experience: Quality Benchmarking of Datasets Used in Software Effort Estimation,"Bosu, Michael F. and Macdonell, Stephen G.",10.1145/3328746,2019,"Data is a cornerstone of empirical software engineering (ESE) research and practice. Data underpin numerous process and project management activities, including the estimation of development effort and the prediction of the likely location and severity of defects in code. Serious questions have been raised, however, over the quality of the data used in ESE. Data quality problems caused by noise, outliers, and incompleteness have been noted as being especially prevalent. Other quality issues, although also potentially important, have received less attention. In this study, we assess the quality of 13 datasets that have been used extensively in research on software effort estimation. The quality issues considered in this article draw on a taxonomy that we published previously based on a systematic mapping of data quality issues in ESE. Our contributions are as follows: (1) an evaluation of the “fitness for purpose” of these commonly used datasets and (2) an assessment of the utility of the taxonomy in terms of dataset benchmarking. We also propose a template that could be used to both improve the ESE data collection/submission process and to evaluate other such datasets, contributing to enhanced awareness of data quality issues in the ESE community and, in time, the availability and use of higher-quality datasets."
Broadening the Conceptualization of Theory in the Information Systems Discipline: A Meta-Theory Approach,"Niederman, Fred and March, Salvatore T.",10.1145/3330472.3330476,2019,"The traditional and most prevalent view of theory in the information systems (IS) discipline is conceptualized in what has been termed ?variance theory,"" or more properly ?variance meta-theory,"" where a meta-theory is a means by which to conceptualize the types of constructs and relationships used to develop a specific theory (instance). Variance meta-theory conceptualizes the constructs and relationships in a theory strictly as properties of entities, interrelated in a static (statistical) correlational manner. Theories formulated using variance meta-theory generally seek to explain or predict immutable inferred causal relationships. This view of theory is limiting and constraining in an applied field like IS where phenomena of interest are complex, constantly changing, and reliant on proactive human actions, as well as underlying forces. As new technologies are developed, users adapt and learn, take actions, and respond to results. We argue that alternative meta-theories more readily address this wider range of IS research questions. We examine three such meta-theories ? network, process, and co-evolution, comparing and contrasting their underlying conceptualizations, entities, relationships, and methodologies. We argue further that theory (instances) formulated in the language of each of these meta-theories can be used as part of a broad scientific process of proposing, testing, and reevaluating theory to develop increasingly nuanced understandings of IS phenomena, ultimately resulting in the accumulation of knowledge that is relevant to both theory and practice."
Urban Human Mobility: Data-Driven Modeling and Prediction,"Wang, Jinzhong and Kong, Xiangjie and Xia, Feng and Sun, Lijun",10.1145/3331651.3331653,2019,"Human mobility is a multidisciplinary field of physics and computer science and has drawn a lot of attentions in recent years. Some representative models and prediction approaches have been proposed for modeling and predicting human mobility. However, multi-source heterogeneous data from handheld terminals, GPS, and social media, provides a new driving force for exploring urban human mobility patterns from a quantitative and microscopic perspective. The studies of human mobility modeling and prediction play a vital role in a series of applications such as urban planning, epidemic control, location-based services, and intelligent transportation management. In this survey, we review human mobility models based on a human-centric angle in a datadriven context. Specifically, we characterize human mobility patterns from individual, collective, and hybrid levels. Meanwhile, we survey human mobility prediction methods from four aspects and then describe recent development respectively. Finally, we discuss some open issues that provide a helpful reference for researchers' future direction. This review not only lays a solid foundation for beginners who want to acquire a quick understanding of human mobility but also provides helpful information for researchers on how to develop a unified human mobility model."
Modeling and Analyzing Incremental Natures of Developing Software,"Jiang, Jian-Min and Hong, Zhong and Chen, Yangyang",10.1145/3333535,2019,"The basic premise of iterative and evolutionary project management is that a project is divided into early, frequent, and short duration delivery steps. Each step attempts to deliver some real value to stakeholders. The increment size and iteration length usually depend on profitability, finance, deadline, and so on, rather than the functionality of a developing system. It is difficult to guarantee the correctness in every iteration step. In this article, we propose a method of ensuring the correctness of iterative design in terms of deadlock-freedom of the behavior of software. The method first obtains the correct (deadlock-free) atomic subsystems of a system using a decomposition approach. In the iterative development process, the method then requires that one atomic subsystem or the composition of multiple atomic subsystems should be regarded as one increment. Every increment is naturally correct and can be completely independently developed, independently deployed, and independently maintained. The currently released system in each iteration step is naturally guaranteed to be correct. It is not necessary for developers to consider the composition of the increment and the previously released system may cause flaws and errors. We also discuss the approach for ensuring correctness when design modifications are made in an iteration step. Finally, we explore the automatic decomposition of a system into multiple atomic subsystems and present the corresponding algorithm. A case demonstrates these results."
Predicting Users’ Movie Preference and Rating Behavior from Personality and Values,"Khan, Euna Mehnaz and Mukta, Md. Saddam Hossain and Ali, Mohammed Eunus and Mahmud, Jalal",10.1145/3338244,2020,"In this article, we propose novel techniques to predict a user’s movie genre preference and rating behavior from her psycholinguistic attributes obtained from the social media interactions. The motivation of this work comes from various psychological studies that demonstrate that psychological attributes such as personality and values can influence one’s decision or choice in real life. In this work, we integrate user interactions in Twitter and IMDb to derive interesting relations between human psychological attributes and their movie preferences. In particular, we first predict a user’s movie genre preferences from the personality and value scores of the user derived from her tweets. Second, we also develop models to predict user movie rating behavior from her tweets in Twitter and movie genre and storyline preferences from IMDb. We further strengthen the movie rating model by incorporating the user reviews. In the above models, we investigate the role of personality and values independently and combinedly while predicting movie genre preferences and movie rating behaviors. We find that our combined models significantly improve the accuracy than that of a single model that is built by using personality or values independently. We also compare our technique with the traditional movie genre and rating prediction techniques. The experimental results show that our models are effective in recommending movies to users."
Machine Learning Techniques for the Diagnosis of Alzheimer’s Disease: A Review,"Tanveer, M. and Richhariya, B. and Khan, R. U. and Rashid, A. H. and Khanna, P. and Prasad, M. and Lin, C. T.",10.1145/3344998,2020,"Alzheimer’s disease is an incurable neurodegenerative disease primarily affecting the elderly population. Efficient automated techniques are needed for early diagnosis of Alzheimer’s. Many novel approaches are proposed by researchers for classification of Alzheimer’s disease. However, to develop more efficient learning techniques, better understanding of the work done on Alzheimer’s is needed. Here, we provide a review on 165 papers from 2005 to 2019, using various feature extraction and machine learning techniques. The machine learning techniques are surveyed under three main categories: support vector machine (SVM), artificial neural network (ANN), and deep learning (DL) and ensemble methods. We present a detailed review on these three approaches for Alzheimer’s with possible future directions."
StressMon: Scalable Detection of Perceived Stress and Depression Using Passive Sensing of Changes in Work Routines and Group Interactions,"Zakaria, Camellia and Balan, Rajesh and Lee, Youngki",10.1145/3359139,2019,"Stress and depression are a common affliction in all walks of life. When left unmanaged, stress can inhibit productivity or cause depression. Depression can occur independently of stress. There has been a sharp rise in mobile health initiatives to monitor stress and depression. However, these initiatives usually require users to install dedicated apps or multiple sensors, making such solutions hard to scale. Moreover, they emphasise sensing individual factors and overlook social interactions, which plays a significant role in influencing stress and depression while being a part of a social system. We present StressMon, a stress and depression detection system that leverages single-attribute location data, passively sensed from the WiFi infrastructure. Using the location data, it extracts a detailed set of movement, and physical group interaction pattern features without requiring explicit user actions or software installation on client devices. These features are used in two different machine learning models to detect stress and depression. To validate StressMon, we conducted three different longitudinal studies at a university with different groups of students, totalling up to 108 participants. Our evaluation demonstrated StressMon detecting severely stressed students with a 96.01% True Positive Rate (TPR), an 80.76% True Negative Rate (TNR), and a 0.97 area under the ROC curve (AUC) score (a score of 1 indicates a perfect binary classifier) using a 6-day prediction window. In addition, StressMon was able to detect depression at 91.21% TPR, 66.71% TNR, and 0.88 AUC using a 15-day window. We end by discussing how StressMon can expand CSCW research, especially in areas involving collaborative practices for mental health management."
How Data Scientists Use Computational Notebooks for Real-Time Collaboration,"Wang, April Yi and Mittal, Anant and Brooks, Christopher and Oney, Steve",10.1145/3359141,2019,"Effective collaboration in data science can leverage domain expertise from each team member and thus improve the quality and efficiency of the work. Computational notebooks give data scientists a convenient interactive solution for sharing and keeping track of the data exploration process through a combination of code, narrative text, visualizations, and other rich media. In this paper, we report how synchronous editing in computational notebooks changes the way data scientists work together compared to working on individual notebooks. We first conducted a formative survey with 195 data scientists to understand their past experience with collaboration in the context of data science. Next, we carried out an observational study of 24 data scientists working in pairs remotely to solve a typical data science predictive modeling problem, working on either notebooks supported by synchronous groupware or individual notebooks in a collaborative setting. The study showed that working on the synchronous notebooks improves collaboration by creating a shared context, encouraging more exploration, and reducing communication costs. However, the current synchronous editing features may lead to unbalanced participation and activity interference without strategic coordination. The synchronous notebooks may also amplify the tension between quick exploration and clear explanations. Building on these findings, we propose several design implications aimed at better supporting collaborative editing in computational notebooks, and thus improving efficiency in teamwork among data scientists."
"Socio-technical Affordances for Stigmergic Coordination Implemented in MIDST, a Tool for Data-Science Teams","Crowston, Kevin and Saltz, Jeff S. and Rezgui, Amira and Hegde, Yatish and You, Sangseok",10.1145/3359219,2019,"We present a conceptual framework for socio-technical affordances for stigmergic coordination, that is, coordination supported by a shared work product. Based on research on free/libre open source software development, we theorize that stigmergic coordination depends on three sets of socio-technical affordances: the visibility and combinability of the work, along with defined genres of work contributions. As a demonstration of the utility of the developed framework, we use it as the basis for the design and implementation of a system, MIDST, that supports these affordances and that we thus expect to support stigmergic coordination. We describe an initial assessment of the impact of the tool on the work of project teams of three to six data-science students that suggests that the tool was useful but also in need of further development. We conclude with plans for future research and an assessment of theory-driven system design."
"Who is the ""Human"" in Human-Centered Machine Learning: The Case of Predicting Mental Health from Social Media","Chancellor, Stevie and Baumer, Eric P. S. and De Choudhury, Munmun",10.1145/3359249,2019,"""Human-centered machine learning"" (HCML) combines human insights and domain expertise with data-driven predictions to answer societal questions. This area's inherent interdisciplinarity causes tensions in the obligations researchers have to the humans whose data they use. This paper studies how scientific papers represent human research subjects in HCML. Using mental health status prediction on social media as a case study, we conduct thematic discourse analysis on 55 papers to examine these representations. We identify five discourses that weave a complex narrative of who the human subject is in this research: Disorder/Patient, Social Media, Scientific, Data/Machine Learning, and Person. We show how these five discourses create paradoxical subject and object representations of the human, which may inadvertently risk dehumanization. We also discuss the tensions and impacts of interdisciplinary research; the risks of this work to scientific rigor, online communities, and mental health; and guidelines for stronger HCML research in this nascent area."
Joining Together Online: The Trajectory of CSCW Scholarship on Group Formation,"Harris, Alexa M. and G\'{o}mez-Zar\'{a}, Diego and DeChurch, Leslie A. and Contractor, Noshir S.",10.1145/3359250,2019,"The field of Computer Supported Cooperative Work (CSCW) has an enduring interest in studying and designing technologies that bring people together in partnerships, teams, crowds, communities, and other collectives. As the technologies enabling group formation have evolved, so too have the guiding questions pursued by CSCW scholars. This review outlines the trajectory of scholarship on group formation with an eye towards the most pressing future questions in this area. To understand how CSCW researchers have studied technology-enabled group formation, we systematically review articles published at CSCW from 1992 to 2018. Exploring more than 2,000 potentially relevant works, we identified 35 focused on technologies and group formation. Content coding and thematic analysis revealed four periods and six themes in the study of online group formation. These themes include: group composition, self-presentation, assembly mechanisms, recruitment, organizing structures, and group culture. Quo vadis? Based on our review, we offer recommendations for the next generation of CSCW scholarship seeking to understand and enable collectives joining together online."
Seamless Parametrization with Arbitrary Cones for Arbitrary Genus,"Campen, Marcel and Shen, Hanxiao and Zhou, Jiaran and Zorin, Denis",10.1145/3360511,2019,"Seamless global parametrization of surfaces is a key operation in geometry processing, e.g., for high-quality quad mesh generation. A common approach is to prescribe the parametric domain structure, in particular, the locations of parametrization singularities (cones), and solve a non-convex optimization problem minimizing a distortion measure, with local injectivity imposed through either constraints or barrier terms. In both cases, an initial valid parametrization is essential to serve as a feasible starting point for obtaining an optimized solution. While convexified versions of the constraints eliminate this initialization requirement, they narrow the range of solutions, causing some problem instances that actually do have a solution to become infeasible.We demonstrate that for arbitrary given sets of topologically admissible parametric cones with prescribed curvature, a global seamless parametrization always exists (with the exception of one well-known case). Importantly, our proof is constructive and directly leads to a general algorithm for computing such parametrizations. Most distinctively, this algorithm is bootstrapped with a convex optimization problem (solving for a conformal map), in tandem with a simple linear equation system (determining a seamless modification of this map). This initial map can then serve as a valid starting point and be optimized for low distortion using existing injectivity preserving methods."
How Data ScientistsWork Together With Domain Experts in Scientific Collaborations: To Find The Right Answer Or To Ask The Right Question?,"Mao, Yaoli and Wang, Dakuo and Muller, Michael and Varshney, Kush R. and Baldini, Ioana and Dugan, Casey and Mojsilovi\'{c}, Aleksandra",10.1145/3361118,2019,"In recent years there has been an increasing trend in which data scientists and domain experts work together to tackle complex scientific questions. However, such collaborations often face challenges. In this paper, we aim to decipher this collaboration complexity through a semi-structured interview study with 22 interviewees from teams of bio-medical scientists collaborating with data scientists. In the analysis, we adopt the Olsons' four-dimensions framework proposed in Distance Matters to code interview transcripts. Our findings suggest that besides the glitches in the collaboration readiness, technology readiness, and coupling of work dimensions, the tensions that exist in the common ground building process influence the collaboration outcomes, and then persist in the actual collaboration process. In contrast to prior works' general account of building a high level of common ground, the breakdowns of content common ground together with the strengthen of process common ground in this process is more beneficial for scientific discovery. We discuss why that is and what the design suggestions are, and conclude the paper with future directions and limitations."
A Comprehensive Review of the Fireworks Algorithm,"Li, Junzhi and Tan, Ying",10.1145/3362788,2019,"The fireworks algorithm, which is inspired from the phenomenon of fireworks explosion, is a special kind of swarm intelligence algorithm proposed in 2010. Since then, it has been attracting more and more research interest and has been widely employed in many real-world problems due to its unique search manner and high efficiency. In this article, we present a comprehensive review of its advances and applications. We begin with an introduction to the original fireworks algorithm. Then we review its algorithmic research work for single objective and multi-objective optimization problems. After that, we present the theoretical analyses of the fireworks algorithm. Finally, we give a brief overview of its applications and implementations. Hopefully, this article could provide a useful road map for researchers and practitioners who are interested in this algorithm and inspire new ideas for its further development."
Humanistic communication in information centric workplaces,"Ranade, Nupoor and Swarts, Jason",10.1145/3363790.3363792,2022,"Professional writers adapt their skills to suit expanded professional roles that involve production and management of information, but preparation through mere skill-based training is problematic because that communication work is messy in ways that are not addressable through simple skills training. We must understand how skills ""influence and shape the discursive activities surrounding their use"" (Selber, 1994). This paper reports the results of a study of people trained in humanities disciplines like communication, English, writing studies, technical communication, etc., on how they have found means to employ their training in their workplace and keep what is humanistic about writing and communicating at the foreground of their interactions with information technologies. Instead of focusing on technology alone, this research encourages a unified approach to preparing students for the workplace."
Designing an AI Health Coach and Studying Its Utility in Promoting Regular Aerobic Exercise,"Mohan, Shiwali and Venkatakrishnan, Anusha and Hartzler, Andrea L.",10.1145/3366501,2020,"Our research aims to develop interactive, social agents that can coach people to learn new tasks, skills, and habits. In this article, we focus on coaching sedentary, overweight individuals (i.e., “trainees”) to exercise regularly. We employ adaptive goal setting in which the intelligent health coach generates, tracks, and revises personalized exercise goals for a trainee. The goals become incrementally more difficult as the trainee progresses through the training program. Our approach is model-based—the coach maintains a parameterized model of the trainee’s aerobic capability that drives its expectation of the trainee’s performance. The model is continually revised based on trainee-coach interactions. The coach is embodied in a smartphone application, NutriWalking, which serves as a medium for coach-trainee interaction. We adopt a task-centric evaluation approach for studying the utility of the proposed algorithm in promoting regular aerobic exercise. We show that our approach can adapt the trainee program not only to several trainees with different capabilities but also to how a trainee’s capability improves as they begin to exercise more. Experts rate the goals selected by the coach better than other plausible goals, demonstrating that our approach is consistent with clinical recommendations. Further, in a 6-week observational study with sedentary participants, we show that the proposed approach helps increase exercise volume performed each week."
The Landscape of Exascale Research: A Data-Driven Literature Analysis,"Heldens, Stijn and Hijma, Pieter and Werkhoven, Ben Van and Maassen, Jason and Belloum, Adam S. Z. and Van Nieuwpoort, Rob V.",10.1145/3372390,2020,"The next generation of supercomputers will break the exascale barrier. Soon we will have systems capable of at least one quintillion (billion billion) floating-point operations per second (1018 FLOPS). Tremendous amounts of work have been invested into identifying and overcoming the challenges of the exascale era. In this work, we present an overview of these efforts and provide insight into the important trends, developments, and exciting research opportunities in exascale computing. We use a three-stage approach in which we (1) discuss various exascale landmark studies, (2) use data-driven techniques to analyze the large collection of related literature, and (3) discuss eight research areas in depth based on influential articles. Overall, we observe that great advancements have been made in tackling the two primary exascale challenges: energy efficiency and fault tolerance. However, as we look forward, we still foresee two major concerns: the lack of suitable programming tools and the growing gap between processor performance and data bandwidth (i.e., memory, storage, networks). Although we will certainly reach exascale soon, without additional research, these issues could potentially limit the applicability of exascale computing."
Report from the 1st Int. Workshop on Education through Advanced Software Engineering and Artificial Intelligence (EASEAI '19),"Vanderose, Beno\^{\i}t and Frenay, Beno\^{\i}t and Henry, Julie and Devroey, Xavier",10.1145/3375572.3375579,2020,"In the past years, everyday life has been profoundly transformed by the development and widespread of digital technologies. Gen- eral, as well as specialized audiences, have to face an ever-increasing amount of knowledge and learn new abilities. This rst edition of the EASEAI workshop tried to address that challenge by look- ing at software engineering, education, and arti cial intelligence research elds to explore how they can be combined. Speci cally, we brought together researchers, teachers, and practitioners who use advanced software engineering tools and arti cial intelligence techniques in education. And researchers and teachers in edu- cation science who address the problem of improving the aware- ness regarding digital technologies through a transgenerational and transdisciplinary range of students."
SEADS: Scalable and Cost-effective Dynamic Dependence Analysis of Distributed Systems via Reinforcement Learning,"Fu, Xiaoqin and Cai, Haipeng and Li, Wen and Li, Li",10.1145/3379345,2021,"Distributed software systems are increasingly developed and deployed today. Many of these systems are supposed to run continuously. Given their critical roles in our society and daily lives, assuring the quality of distributed systems is crucial. Analyzing runtime program dependencies has long been a fundamental technique underlying numerous tool support for software quality assurance. Yet conventional approaches to dynamic dependence analysis face severe scalability barriers when they are applied to real-world distributed systems, due to the unbounded executions to be analyzed in addition to common efficiency challenges suffered by dynamic analysis in general.In this article, we present SEADS, a distributed, online, and cost-effective dynamic dependence analysis framework that aims at scaling the analysis to real-world distributed systems. The analysis itself is distributed to exploit the distributed computing resources (e.g., a cluster) of the system under analysis; it works online to overcome the problem with unbounded execution traces while running continuously with the system being analyzed to provide timely querying of analysis results (i.e., runtime dependence set of any given query). Most importantly, given a user-specified time budget, the analysis automatically adjusts itself to better cost-effectiveness tradeoffs (than otherwise) while respecting the budget by changing various analysis parameters according to the time being spent by the dependence analysis. At the core of the automatic adjustment is our application of a reinforcement learning method for the decision making—deciding which configuration to adjust to according to the current configuration and its associated analysis cost with respect to the user budget. We have implemented SEADS for Java and applied it to eight real-world distributed systems with continuous executions. Our empirical results revealed the efficiency and scalability advantages of our framework over a conventional dynamic analysis, at least for dynamic dependence computation at method level. While we demonstrate it in the context of dynamic dependence analysis in this article, the methodology for achieving and maintaining scalability and greater cost-effectiveness against continuously running systems is more broadly applicable to other dynamic analyses."
Mining Career Paths from Large Resume Databases: Evidence from IT Professionals,"Lappas, Theodoros",10.1145/3379984,2020,"The emergence of online professional platforms, such as LinkedIn and Indeed, has led to unprecedented volumes of rich resume data that have revolutionized the study of careers. One of the most prevalent problems in this space is the extraction of prototype career paths from a workforce. Previous research has consistently relied on a two-step approach to tackle this problem. The first step computes the pairwise distances between all the career sequences in the database. The second step uses the distance matrix to create clusters, with each cluster representing a different prototype path. As we demonstrate in this work, this approach faces two significant challenges when applied on large resume databases. First, the overwhelming diversity of job titles in the modern workforce prevents the accurate evaluation of distance between career sequences. Second, the clustering step of the standard approach leads to highly heterogeneous clusters, due to its inability to handle categorical sequences and sensitivity to outliers. This leads to non-representative centroids and spurious prototype paths that do not accurately represent the actual groups in the workforce. Our work addresses these two challenges and has practical implications for the numerous researchers and practitioners working on the analysis of career data across domains."
How Enterprises Adopt Agile Forms of Organizational Design: A Multiple-Case Study,"Gerster, Daniel and Dremel, Christian and Brenner, Walter and Kelker, Prashant",10.1145/3380799.3380807,2020,"The question of how to increase speed and flexibility in times of digital disruption is essential to almost any company. While previous research mainly addresses agility in the context of information systems development, as form for organizing startups or ""born digital"" companies, little knowledge exists about the adoption of agile practices and structures at established enterprises. With an exploratory study of fifteen global cases, we aim at examining how established enterprises adopt and scale agile forms of organizational design. We found that (1) agile forms of organizational design are currently adopted by enterprises at large scale, (2) agile forms of organizational design are adopted not only by IT, but successively also by business units and in contexts outside information systems development, and (3) while Spotify's organization serves as a widespread template for a fully agile unit, enterprises adapt and fine-tune this template according to their needs and scale. We identified three additional models for fully agile forms of organizational design where a fully agile unit with cross-product support is the most frequently observed model."
Predicting Brain Functional Connectivity Using Mobile Sensing,"Obuchi, Mikio and Huckins, Jeremy F. and Wang, Weichen and daSilva, Alex and Rogers, Courtney and Murphy, Eilis and Hedlund, Elin and Holtzheimer, Paul and Mirjafari, Shayan and Campbell, Andrew",10.1145/3381001,2020,"Brain circuit functioning and connectivity between specific regions allow us to learn, remember, recognize and think as humans. In this paper, we ask the question if mobile sensing from phones can predict brain functional connectivity. We study the brain resting-state functional connectivity (RSFC) between the ventromedial prefrontal cortex (vmPFC) and the amygdala, which has been shown by neuroscientists to be associated with mental illness such as anxiety and depression. We discuss initial results and insights from the NeuroSence study, an exploratory study of 105 first year college students using neuroimaging and mobile sensing across one semester. We observe correlations between several behavioral features from students' mobile phones and connectivity between vmPFC and amygdala, including conversation duration (r=0.365, p&lt;0.001), sleep onset time (r=0.299, p&lt;0.001) and the number of phone unlocks (r=0.253, p=0.029). We use a support vector classifier and 10-fold cross validation and show that we can classify whether students have higher (i.e., stronger) or lower (i.e., weaker) vmPFC-amygdala RSFC purely based on mobile sensing data with an F1 score of 0.793. To the best of our knowledge, this is the first paper to report that resting-state brain functional connectivity can be predicted using passive sensing data from mobile phones."
Regional E-governance Development Index for Developing Nations,"Gupta, Rajan and Muttoo, Sunil Kumar and Pal, Saibal Kumar",10.1145/3386163,2020,"E-governance has proven to be instrumental in the expansion and evolution of how governments interact with and deliver services to their citizens. The United Nations (UN) E-Governance Development Index (EGDI) is the most widely used metric for assessment of e-governance development; however, this metric is not appropriate for assessment at the regional level, especially for developing nations. Therefore, the authors have studied various factors in the context of developing nations, such as the Online Availability and Performance Index, Telecommunications Index, Human Capital Index, E-governance-related Infrastructure Index, and E-governance Performance Index, with the aim of analyzing the success and implementation rate of e-governance activities across the different regions of a developing nation like India. The results showed that the UN's EGDI is not suitable for assessment at a regional level and that adding new components to the model helps to achieve better results for around 30% of the regions under study. The rankings, which were calculated through the new model and compared against other standard indices, obtained good correlations, proving the validity of the new model. India, as a developing nation, was the region selected for the experimental work. Central governments, state governments, investors, stakeholders, and government consultants can obtain benefits through this research."
Evolution of Emacs Lisp,"Monnier, Stefan and Sperber, Michael",10.1145/3386324,2020,"While Emacs proponents largely agree that it is the world’s greatest text editor, it is almost as much a Lisp machine disguised as an editor. Indeed, one of its chief appeals is that it is programmable via its own programming language. Emacs Lisp is a Lisp in the classic tradition. In this article, we present the history of this language over its more than 30 years of evolution. Its core has remained remarkably stable since its inception in 1985, in large part to preserve compatibility with the many third-party packages providing a multitude of extensions. Still, Emacs Lisp has evolved and continues to do so. Important aspects of Emacs Lisp have been shaped by concrete requirements of the editor it supports as well as implementation constraints. These requirements led to the choice of a Lisp dialect as Emacs’s language in the first place, specifically its simplicity and dynamic nature: Loading additional Emacs packages or changing the ones in place occurs frequently, and having to restart the editor in order to re-compile or re-link the code would be unacceptable. Fulfilling this requirement in a more static language would have been difficult at best. One of Lisp’s chief characteristics is its malleability through its uniform syntax and the use of macros. This has allowed the language to evolve much more rapidly and substantively than the evolution of its core would suggest, by letting Emacs packages provide new surface syntax alongside new functions. In particular, Emacs Lisp can be customized to look much like Common Lisp, and additional packages provide multiple-dispatch object systems, legible regular expressions, programmable pattern-matching constructs, generalized variables, and more. Still, the core has also evolved, albeit slowly. Most notably, it acquired support for lexical scoping. The timeline of Emacs Lisp development is closely tied to the projects and people who have shaped it over the years: We document Emacs Lisp history through its predecessors, Mocklisp and MacLisp, its early development up to the “Emacs schism” and the fork of Lucid Emacs, the development of XEmacs, and the subsequent rennaissance of Emacs development."
Voice in Human–Agent Interaction: A Survey,"Seaborn, Katie and Miyake, Norihisa P. and Pennefather, Peter and Otake-Matsuura, Mihoko",10.1145/3386867,2021,"Social robots, conversational agents, voice assistants, and other embodied AI are increasingly a feature of everyday life. What connects these various types of intelligent agents is their ability to interact with people through voice. Voice is becoming an essential modality of embodiment, communication, and interaction between computer-based agents and end-users. This survey presents a meta-synthesis on agent voice in the design and experience of agents from a human-centered perspective: voice-based human–agent interaction (vHAI). Findings emphasize the social role of voice in HAI as well as circumscribe a relationship between agent voice and body, corresponding to human models of social psychology and cognition. Additionally, changes in perceptions of and reactions to agent voice over time reveals a generational shift coinciding with the commercial proliferation of mobile voice assistants. The main contributions of this work are a vHAI classification framework for voice across various agent forms, contexts, and user groups, a critical analysis grounded in key theories, and an identification of future directions for the oncoming wave of vocal machines."
Unveiling Elite Developers’ Activities in Open Source Projects,"Wang, Zhendong and Feng, Yang and Wang, Yi and Jones, James A. and Redmiles, David",10.1145/3387111,2020,"Open source developers, particularly the elite developers who own the administrative privileges for a project, maintain a diverse portfolio of contributing activities. They not only commit source code but also exert significant efforts on other communicative, organizational, and supportive activities. However, almost all prior research focuses on specific activities and fails to analyze elite developers’ activities in a comprehensive way. To bridge this gap, we conduct an empirical study with fine-grained event data from 20 large open source projects hosted on GITHUB. We investigate elite developers’ contributing activities and their impacts on project outcomes. Our analyses reveal three key findings: (1) elite developers participate in a variety of activities, of which technical contributions (e.g., coding) only account for a small proportion; (2) as the project grows, elite developers tend to put more effort into supportive and communicative activities and less effort into coding; and (3) elite developers’ efforts in nontechnical activities are negatively correlated with the project’s outcomes in terms of productivity and quality in general, except for a positive correlation with the bug fix rate (a quality indicator). These results provide an integrated view of elite developers’ activities and can inform an individual’s decision making about effort allocation, which could lead to improved project outcomes. The results also provide implications for supporting these elite developers."
"How do Data Science Workers Collaborate? Roles, Workflows, and Tools","Zhang, Amy X. and Muller, Michael and Wang, Dakuo",10.1145/3392826,2020,"Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions."
Are information systems and computer science overlapping more and more?,"Gutierrez-Cardenas, Juan",10.1145/3396367,2020,
Multi-core Devices for Safety-critical Systems: A Survey,"Cerrolaza, Jon Perez and Obermaisser, Roman and Abella, Jaume and Cazorla, Francisco J. and Gr\""{u}ttner, Kim and Agirre, Irune and Ahmadian, Hamidreza and Allende, Imanol",10.1145/3398665,2020,"Multi-core devices are envisioned to support the development of next-generation safety-critical systems, enabling the on-chip integration of functions of different criticality. This integration provides multiple system-level potential benefits such as cost, size, power, and weight reduction. However, safety certification becomes a challenge and several fundamental safety technical requirements must be addressed, such as temporal and spatial independence, reliability, and diagnostic coverage. This survey provides a categorization and overview at different device abstraction levels (nanoscale, component, and device) of selected key research contributions that support the compliance with these fundamental safety requirements."
Wearable Physical Activity Tracking Systems for Older Adults—A Systematic Review,"Vargemidis, Dimitri and Gerling, Kathrin and Spiel, Katta and Abeele, Vero Vanden and Geurts, Luc",10.1145/3402523,2020,"Physical activity (PA) positively impacts the quality of life of older adults, with technology as a promising factor in maintaining motivation. Within Computer Science and Engineering, research investigates how to track PA of older adults for various purposes. We present a systematic review of 204 papers and discuss wearable tracking systems according to their purpose, technological context, and target audience, as well as design and evaluation processes with particular attention to the meaningful involvement of older adults. Our results show that most systems focus on supervising older adults in the context of disease and frailty management. Only few systems focus on supporting older adults by promoting rehabilitation and respecting agency of older adults via self-monitoring PA, or encouraging PA to maintain healthy levels of activity. Moreover, systems are often narrowly limited to walking, although older adults may enjoy a broader range of activities. Likewise, the involvement of older adults in design processes is scarce, and their experience with a given technology is rarely considered relevant for evaluation. In sum, we contribute an overview of wearable technology for tracking older adults’ PA, contextualize our findings within recommendations provided by Sports and Rehabilitation Science, and illustrate opportunities for future work."
A Survey of Multilingual Neural Machine Translation,"Dabre, Raj and Chu, Chenhui and Kunchukuttan, Anoop",10.1145/3406095,2020,"We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in recent years. MNMT has been useful in improving translation quality as a result of translation knowledge transfer (transfer learning). MNMT is more promising and interesting than its statistical machine translation counterpart, because end-to-end modeling and distributed representations open new avenues for research on machine translation. Many approaches have been proposed to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and, hence, deserve further exploration. In this article, we present an in-depth survey of existing literature on MNMT. We first categorize various approaches based on their central use-case and then further categorize them based on resource scenarios, underlying modeling principles, core-issues, and challenges. Wherever possible, we address the strengths and weaknesses of several techniques by comparing them with each other. We also discuss the future directions for MNMT. This article is aimed towards both beginners and experts in NMT. We hope this article will serve as a starting point as well as a source of new ideas for researchers and engineers interested in MNMT."
Mastering Variation in Human Studies: The Role of Aggregation,"Siegmund, Janet and Peitek, Norman and Apel, Sven and Siegmund, Norbert",10.1145/3406544,2021,"The human factor is prevalent in empirical software engineering research. However, human studies often do not use the full potential of analysis methods by combining analysis of individual tasks and participants with an analysis that aggregates results over tasks and/or participants. This may hide interesting insights of tasks and participants and may lead to false conclusions by overrating or underrating single-task or participant performance. We show that studying multiple levels of aggregation of individual tasks and participants allows researchers to have both insights from individual variations as well as generalized, reliable conclusions based on aggregated data. Our literature survey revealed that most human studies perform either a fully aggregated analysis or an analysis of individual tasks. To show that there is important, non-trivial variation when including human participants, we reanalyze 12 published empirical studies, thereby changing the conclusions or making them more nuanced. Moreover, we demonstrate the effects of different aggregation levels by answering a novel research question on published sets of fMRI data. We show that when more data are aggregated, the results become more accurate. This proposed technique can help researchers to find a sweet spot in the tradeoff between cost of a study and reliability of conclusions."
Uncertainty-wise Requirements Prioritization with Search,"Zhang, Huihui and Zhang, Man and Yue, Tao and Ali, Shaukat and Li, Yan",10.1145/3408301,2021,"Requirements review is an effective technique to ensure the quality of requirements in practice, especially in safety-critical domains (e.g., avionics systems, automotive systems). In such contexts, a typical requirements review process often prioritizes requirements, due to limited time and monetary budget, by, for instance, prioritizing requirements with higher implementation cost earlier in the review process. However, such a requirement implementation cost is typically estimated by stakeholders who often lack knowledge about (future) requirements implementation scenarios, which leads to uncertainty in cost overrun. In this article, we explicitly consider such uncertainty (quantified as cost overrun probability) when prioritizing requirements based on the assumption that a requirement with higher importance, a higher number of dependencies to other requirements, and higher implementation cost will be reviewed with the higher priority. Motivated by this, we formulate four objectives for uncertainty-wise requirements prioritization: maximizing the importance of requirements, requirements dependencies, the implementation cost of requirements, and cost overrun probability. These four objectives are integrated as part of our search-based uncertainty-wise requirements prioritization approach with tool support, named as URP. We evaluated six Multi-Objective Search Algorithms (MOSAs) (i.e., NSGA-II, NSGA-III, MOCell, SPEA2, IBEA, and PAES) together with Random Search (RS) using three real-world datasets (i.e., the RALIC, Word, and ReleasePlanner datasets) and 19 synthetic optimization problems. Results show that all the selected MOSAs can solve the requirements prioritization problem with significantly better performance than RS. Among them, IBEA was over 40% better than RS in terms of permutation effectiveness for the first 10% of prioritized requirements in the prioritization sequence of all three datasets. In addition, IBEA achieved the best performance in terms of the convergence of solutions, and NSGA-III performed the best when considering both the convergence and diversity of nondominated solutions."
Big Data Systems: A Software Engineering Perspective,"Davoudian, Ali and Liu, Mengchi",10.1145/3408314,2020,"Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby massive amounts of heterogeneous data are gathered from multiple sources, managed, analyzed (in batch, stream or hybrid fashion), and served to end-users and external applications. Such systems pose specific challenges in all phases of software development lifecycle and might become very complex by evolving data, technologies, and target value over time. Consequently, many organizations and enterprises have found it difficult to adopt BDSs. In this article, we provide insight into three major activities of software engineering in the context of BDSs as well as the choices made to tackle them regarding state-of-the-art research and industry efforts. These activities include the engineering of requirements, designing and constructing software to meet the specified requirements, and software/data quality assurance. We also disclose some open challenges of developing effective BDSs, which need attention from both researchers and practitioners."
Federated Learning in a Medical Context: A Systematic Literature Review,"Pfitzner, Bjarne and Steckhan, Nico and Arnrich, Bert",10.1145/3412357,2021,"Data privacy is a very important issue. Especially in fields like medicine, it is paramount to abide by the existing privacy regulations to preserve patients’ anonymity. However, data is required for research and training machine learning models that could help gain insight into complex correlations or personalised treatments that may otherwise stay undiscovered. Those models generally scale with the amount of data available, but the current situation often prohibits building large databases across sites. So it would be beneficial to be able to combine similar or related data from different sites all over the world while still preserving data privacy. Federated learning has been proposed as a solution for this, because it relies on the sharing of machine learning models, instead of the raw data itself. That means private data never leaves the site or device it was collected on. Federated learning is an emerging research area, and many domains have been identified for the application of those methods. This systematic literature review provides an extensive look at the concept of and research into federated learning and its applicability for confidential healthcare datasets."
SAFER: Development and Evaluation of an IoT Device Risk Assessment Framework in a Multinational Organization,"Oser, Pascal and Feger, Sebastian and Wo\'{z}niak, Pawe\l{} W. and Karolus, Jakob and Spagnuelo, Dayana and Gupta, Akash and L\""{u}ders, Stefan and Schmidt, Albrecht and Kargl, Frank",10.1145/3414173,2020,"Users of Internet of Things (IoT) devices are often unaware of their security risks and cannot sufficiently factor security considerations into their device selection. This puts networks, infrastructure and users at risk. We developed and evaluated SAFER, an IoT device risk assessment framework designed to improve users' ability to assess the security of connected devices. We deployed SAFER in a large multinational organization that permits use of private devices. To evaluate the framework, we conducted a mixed-method study with 20 employees. Our findings suggest that SAFER increases users' awareness of security issues. It provides valuable advice and impacts device selection. Based on our findings, we discuss implications for the design of device risk assessment tools, with particular regard to the relationship between risk communication and user perceptions of device complexity."
Continuous curve textures,"Tu, Peihan and Wei, Li-Yi and Yatani, Koji and Igarashi, Takeo and Zwicker, Matthias",10.1145/3414685.3417780,2020,"Repetitive patterns are ubiquitous in natural and human-made objects, and can be created with a variety of tools and methods. Manual authoring provides unmatched degree of freedom and control, but can require significant artistic expertise and manual labor. Computational methods can automate parts of the manual creation process, but are mainly tailored for discrete pixels or elements instead of more general continuous structures. We propose an example-based method to synthesize continuous curve patterns from exemplars. Our main idea is to extend prior sample-based discrete element synthesis methods to consider not only sample positions (geometry) but also their connections (topology). Since continuous structures can exhibit higher complexity than discrete elements, we also propose robust, hierarchical synthesis to enhance output quality. Our algorithm can generate a variety of continuous curve patterns fully automatically. For further quality improvement and customization, we also present an autocomplete user interface to facilitate interactive creation and iterative editing. We evaluate our methods and interface via different patterns, ablation studies, and comparisons with alternative methods."
"MixTAPE: Mixed-initiative Team Action Plan Creation Through Semi-structured Notes, Automatic Task Generation, and Task Classification","Rahman, Sajjadur and Siangliulue, Pao and Marcus, Adam",10.1145/3415240,2020,"Checklists and action plans are a proven mechanism for project-based collaboration. Synthesizing project-specific plans is challenging, as project managers must consider multiple sources of information, from structured surveys to semi-structured conversations with stakeholders. In a needfinding study with project managers, we identified challenges in creating action plans for teams. We built MixTAPE, a mixed-initiative system that addressed these challenges with three components: a semi-structured note-taking interface for capturing stakeholder conversations, a plan generator for automatically combining multi-source information into action plans, and classification models for assigning and prioritizing action items. We evaluated MixTAPE in an observational study of 32 website design projects. Compared to a previously unstructured process, MixTAPE generated 1.45X as many tasks that are more consistent, while reducing the plan creation time by 33.70%. Through interviews and surveys, we found that participants rate MixTAPE highly across several measures. Based on our findings, we discuss the implications and opportunities for mixed-initiative action plan creation."
A Taxonomy of Team-Assembly Systems: Understanding How People Use Technologies to Form Teams,"G\'{o}mez-Zar\'{a}, Diego and DeChurch, Leslie A. and Contractor, Noshir S.",10.1145/3415252,2020,"The emergence of team-assembly technologies has brought with it new challenges in designing and implementing socio-technical systems. Our understanding of how systems shape the team-assembly processes is still limited. How do systems enable users to find teammates? How do users make decisions when using these systems? And what factors explain the characteristics of the teams assembled? Building on existing literature from CSCW, computer science, and management science, we propose a taxonomy to characterize how systems influence team assembly. This taxonomy argues that two dimensions determine how systems shape team assembly: (i) users? agency, to what extent the system enables its users to exercise their agency, and (ii) users? participation, how many users the system allows to participate in the team-formation process. The intersection of these two dimensions manifest four types of teams enabled by systems: self-assembled teams, staffed teams, optimized teams, and augmented teams. We characterize each one of these types of teams, considering their qualities, advantages, and challenges. To contextualize these types of teams, we map the current literature of team-assembly systems using a scoping literature review. Lastly, we discuss ways through which these two dimensions alter users' behavior, team diversity, and team composition. This paper provides theoretical implications and research questions for future systems that reconfigure the organization of people into teams."
Summary of the 1st ICSSP-ICGSE Joint Event,"Tell, Paolo and Raffo, David and Huang, Liguo and Steinmacher, Igor and Britto, Ricardo and T\""{u}z\""{u}n, Eray and Clarke, Paul",10.1145/3417564.3417576,2021,"Having the common objective of bringing together researchers and industry practitioners to share their research findings, experiences, and new ideas as well as sharing topics of interest, the organizing committees of the 14th International Conference on Software and System Processes (ICSSP) and the 15th International Conference on Global Software Engineering (ICGSE) ceased the opportunity to explore the idea of bringing together the two communities once it was clear that the International Conference on Software Engineering and all its co-located events had to be redesigned as online events."
Using Social Media for Mental Health Surveillance: A Review,"Skaik, Ruba and Inkpen, Diana",10.1145/3422824,2020,"Data on social media contain a wealth of user information. Big data research of social media data may also support standard surveillance approaches and provide decision-makers with usable information. These data can be analyzed using Natural Language Processing (NLP) and Machine Learning (ML) techniques to detect signs of mental disorders that need attention, such as depression and suicide ideation. This article presents the recent trends and tools that are used in this field, the different means for data collection, and the current applications of ML and NLP in the surveillance of public mental health. We highlight the best practices and the challenges. Furthermore, we discuss the current gaps that need to be addressed and resolved."
Delivering Unemployment Assistance in Times of Crisis: Scalable Cloud Solutions Can Keep Essential Government Programs Running and Supporting Those in Need,"Angell, Mintaka and Gold, Samantha and Howison, Mark and Kidd, Victoria and Molitor, Daniel and Burns, Casey and Johnson, Chris and Kahn, Matthew and Venzke, Stuart and Deneault, Sandra and Doweiko, Donald and Dziembowski, Stephen and Kumar, Bijay and O'donnell, Patrick and Patel, Chirag and Reidl, Andrew and Tardiff, Brian and Hastings, Justine S. and Jensen, Scott and Pellegrino, Angelika and Roberts, Amelia and Sarathy, Rahul",10.1145/3428125,2020,"The COVID-19 public health emergency caused widespread economic shutdown and unemployment. The resulting surge in Unemployment Insurance claims threatened to overwhelm the legacy systems state workforce agencies rely on to collect, process, and pay claims. In Rhode Island, we developed a scalable cloud solution to collect Pandemic Unemployment Assistance claims as part of a new program created under the Coronavirus Aid, Relief and Economic Security Act to extend unemployment benefits to independent contractors and gig-economy workers not covered by traditional Unemployment Insurance. Our new system was developed, tested, and deployed within 10 days following the passage of the Coronavirus Aid, Relief and Economic Security Act, making Rhode Island the first state in the nation to collect, validate, and pay Pandemic Unemployment Assistance claims. A cloud-enhanced interactive voice response system was deployed a week later to handle the corresponding surge in weekly certifications for continuing unemployment benefits. Cloud solutions can augment legacy systems by offloading processes that are more efficiently handled in modern scalable systems, reserving the limited resources of legacy systems for what they were originally designed. This agile use of combined technologies allowed Rhode Island to deliver timely Pandemic Unemployment Assistance benefits with an estimated cost savings of $502,000 (representing a 411% return on investment)."
QuestionComb: A Gamification Approach for the Visual Explanation of Linguistic Phenomena through Interactive Labeling,"Sevastjanova, Rita and Jentner, Wolfgang and Sperrle, Fabian and Kehlbeck, Rebecca and Bernard, J\""{u}rgen and El-assady, Mennatallah",10.1145/3429448,2021,"Linguistic insight in the form of high-level relationships and rules in text builds the basis of our understanding of language. However, the data-driven generation of such structures often lacks labeled resources that can be used as training data for supervised machine learning. The creation of such ground-truth data is a time-consuming process that often requires domain expertise to resolve text ambiguities and characterize linguistic phenomena. Furthermore, the creation and refinement of machine learning models is often challenging for linguists as the models are often complex, in-transparent, and difficult to understand. To tackle these challenges, we present a visual analytics technique for interactive data labeling that applies concepts from gamification and explainable Artificial Intelligence (XAI) to support complex classification tasks. The visual-interactive labeling interface promotes the creation of effective training data. Visual explanations of learned rules unveil the decisions of the machine learning model and support iterative and interactive optimization. The gamification-inspired design guides the user through the labeling process and provides feedback on the model performance. As an instance of the proposed technique, we present QuestionComb, a workspace tailored to the task of question classification (i.e., in information-seeking vs. non-information-seeking questions). Our evaluation studies confirm that gamification concepts are beneficial to engage users through continuous feedback, offering an effective visual analytics technique when combined with active learning and XAI."
ComFeel: Productivity is a Matter of the Senses Too,"Constantinides, Marios and \v{S}\'{c}epanovi\'{c}, Sanja and Quercia, Daniele and Li, Hongwei and Sassi, Ugo and Eggleston, Michael",10.1145/3432234,2020,"Indoor environmental quality has been found to impact employees' productivity in the long run, yet it is unclear its meeting-level impact in the short term. We studied the relationship between sensorial pleasantness of a meeting's room and the meeting's productivity. By administering a 28-item questionnaire to 363 online participants, we indeed found that three factors captured 62% of people's experience of meetings: (a) productivity; (b) psychological safety; and (c) room pleasantness. To measure room pleasantness, we developed and deployed ComFeel, an indoor environmental sensing infrastructure, which captures light, temperature, and gas resistance readings through miniaturized and unobtrusive devices we built and named 'Geckos'. Across 29 real-world meetings, using ComFeel, we collected 1373 minutes of readings. For each of these meetings, we also collected whether each participant felt the meeting to have been productive, the setting to be psychologically safe, and the meeting room to be pleasant. As one expects, we found that, on average, the probability of a meeting being productive increased by 35% for each standard deviation increase in the psychological safety participants experienced. Importantly, that probability increased by as much as 25% for each increase in room pleasantness, confirming the significant short-term impact of the indoor environment on meetings' productivity."
A Systematic Review on Hadith Authentication and Classification Methods,"Binbeshr, Farid and Kamsin, Amirrudin and Mohammed, Manal",10.1145/3434236,2021,"Background: A hadith refers to sayings, actions, and characteristics of the Prophet Muhammad peace be upon him. The authenticity of hadiths is crucial, because they constitute the source of legislation for Muslims with the Holy Quran. Classifying hadiths into groups is a matter of importance as well, to make them easy to search and recognize.Objective: To report the results of a systematic review concerning hadith authentication and classification methods.Data sources: Original articles found in ACM, IEEE Xplore, ScienceDirect, Scopus, Web of Science, Springer Link, and Wiley Online Library.Study selection criteria: Only original articles written in English and dealing with hadith authentication and classification. Reviews, editorial, letters, grey literature, and restricted or incomplete articles are excluded.Data extraction: Two authors were assigned to extract data using a predefined data extraction form to answer research questions and assess studies quality.Results: A total of 27 studies were included in this review. There are 14 studies in authentication and 13 studies in classification. Most of the selected studies (17 of 27) were published in conferences, while the others (10 of 27) were published in scientific journals. Research in the area of hadith authentication and classification has received more attention in recent years (2016–2019).Conclusions: Hadith authentication methods are classified into machine learning, rule-based, and a hybrid of rule-based and machine learning and rule-based and statistical methods. Hadith classification methods are classified into machine learning and rule-based. All classification studies used Matn, while the majority of authentication studies used isnad. As a dataset source, Sahih Al-Bukhari was used by most studies. None of the used datasets is publicly available as a benchmark dataset, either in hadith authentication or classification. Recall and Precision are the most frequent evaluation metrics used by the selected studies."
Six reasons why virtual reality is a game-changing computing and communication platform for organizations,"Torro, Osku and Jalo, Henri and Pirkkalainen, Henri",10.1145/3440868,2021,"Beyond the pandemic, organizations need to recognize what digital assets, interactions, and communication processes reap the most benefits from virtual reality."
Predicting Influential Users in Online Social Network Groups,"De Salve, Andrea and Mori, Paolo and Guidi, Barbara and Ricci, Laura and Pietro, Roberto Di",10.1145/3441447,2021,"The widespread adoption of Online Social Networks (OSNs), the ever-increasing amount of information produced by their users, and the corresponding capacity to influence markets, politics, and society, have led both industrial and academic researchers to focus on how such systems could be influenced. While previous work has mainly focused on measuring current influential users, contents, or pages on the overall OSNs, the problem of predicting influencers in OSNs has remained relatively unexplored from a research perspective. Indeed, one of the main characteristics of OSNs is the ability of users to create different groups types, as well as to join groups defined by other users, in order to share information and opinions. In this article, we formulate the Influencers Prediction problem in the context of groups created in OSNs, and we define a general framework and an effective methodology to predict which users will be able to influence the behavior of the other ones in a future time period, based on historical interactions that occurred within the group. Our contribution, while rooted in solid rationale and established analytical tools, is also supported by an extensive experimental campaign. We investigate the accuracy of the predictions collecting data concerning the interactions among about 800,000 users from 18 Facebook groups belonging to different categories (i.e., News, Education, Sport, Entertainment, and Work). The achieved results show the quality and viability of our approach. For instance, we are able to predict, on average, for each group, around a third of what an ex-post analysis will show being the 10 most influential members of that group. While our contribution is interesting on its own and—to the best of our knowledge—unique, it is worth noticing that it also paves the way for further research in this field."
Software Testing Effort Estimation and Related Problems: A Systematic Literature Review,"Bluemke, Ilona and Malanowska, Agnieszka",10.1145/3442694,2021,"Although testing effort estimation is a very important task in software project management, it is rarely described in the literature. There are many difficulties in finding any useful methods or tools for this purpose. Solutions to many other problems related to testing effort calculation are published much more often. There is also no research focusing on both testing effort estimation and all related areas of software engineering. To fill this gap, we performed a systematic literature review on both questions. Although our primary objective was to find some tools or implementable metods for test effort estimation, we have quickly discovered many other interesting topics related to the main one. The main contribution of this work is the presentation of the testing effort estimation task in a very wide context, indicating the relations with other research fields. This systematic literature review presents a detailed overview of testing effort estimation task, including challenges and approaches to automating it and the solutions proposed in the literature. It also exhaustively investigates related research topics, classifying publications that can be found in connection to the testing effort according to seven criteria formulated on the basis of our research questions. We present here both synthesis of our finding and the deep analysis of the stated research problems."
Design Space Optimization of Shared Memory Architecture in Accelerator-rich Systems,"Sinha, Mitali and Harsha, Gade Sri and Bhattacharyya, Pramit and Deb, Sujay",10.1145/3446001,2021,"Shared memory architectures, as opposed to private-only memories, provide a viable alternative to meet the ever-increasing memory requirements of multi-accelerator systems to achieve high performance under stringent area and energy constraints. However, an impulsive memory sharing degrades performance due to network contention and latency to access shared memory. We propose the Accelerator Shared Memory (ASM) framework to provide an optimal private/shared memory configuration and shared data allocation under a system’s resource and network constraints. Evaluations show ASM provides up to 34.35% and 31.34% improvement in performance and energy, respectively, over baseline systems."
Humanized Recommender Systems: State-of-the-art and Research Issues,"Tran, Thi Ngoc Trang and Felfernig, Alexander and Tintarev, Nava",10.1145/3446906,2021,"Psychological factors such as personality, emotions, social connections, and decision biases can significantly affect the outcome of a decision process. These factors are also prevalent in the existing literature related to the inclusion of psychological aspects in recommender system development. Personality and emotions of users have strong connections with their interests and decision-making behavior. Hence, integrating these factors into recommender systems can help to better predict users’ item preferences and increase the satisfaction with recommended items. In scenarios where decisions are made by groups (e.g., selecting a tourism destination to visit with friends), group composition and social connections among group members can affect the outcome of a group decision. Decision biases often occur in a recommendation process, since users usually apply heuristics when making a decision. These biases can result in low-quality decisions. In this article, we provide a rigorous review of existing research on the influence of the mentioned psychological factors on recommender systems. These factors are not only considered in single-user recommendation scenarios but, importantly, also in group recommendation ones, where groups of users are involved in a decision-making process. We include working examples to provide a deeper understanding of how to take into account these factors in recommendation processes. The provided examples go beyond single-user recommendation scenarios by also considering specific aspects of group recommendation settings."
Syntactic Pattern Recognition in Computer Vision: A Systematic Review,"Astolfi, Gilberto and Rezende, F\'{a}bio Prestes Cesar and Porto, Jo\~{a}o Vitor De Andrade and Matsubara, Edson Takashi and Pistori, Hemerson",10.1145/3447241,2021,"Using techniques derived from the syntactic methods for visual pattern recognition is not new and was much explored in the area called syntactical or structural pattern recognition. Syntactic methods have been useful because they are intuitively simple to understand and have transparent, interpretable, and elegant representations. Their capacity to represent patterns in a semantic, hierarchical, compositional, spatial, and temporal way have made them very popular in the research community. In this article, we try to give an overview of how syntactic methods have been employed for computer vision tasks. We conduct a systematic literature review to survey the most relevant studies that use syntactic methods for pattern recognition tasks in images and videos. Our search returned 597 papers, of which 71 papers were selected for analysis. The results indicated that in most of the studies surveyed, the syntactic methods were used as a high-level structure that makes the hierarchical or semantic relationship among objects or actions to perform the most diverse tasks."
How Far Have We Progressed in Identifying Self-admitted Technical Debts? A Comprehensive Empirical Study,"Guo, Zhaoqiang and Liu, Shiran and Liu, Jinping and Li, Yanhui and Chen, Lin and Lu, Hongmin and Zhou, Yuming",10.1145/3447247,2021,"Background. Self-admitted technical debt (SATD) is a special kind of technical debt that is intentionally introduced and remarked by code comments. Those technical debts reduce the quality of software and increase the cost of subsequent software maintenance. Therefore, it is necessary to find out and resolve these debts in time. Recently, many automatic approaches have been proposed to identify SATD. Problem. Popular IDEs support a number of predefined task annotation tags for indicating SATD in comments, which have been used in many projects. However, such clear prior knowledge is neglected by existing SATD identification approaches when identifying SATD. Objective. We aim to investigate how far we have really progressed in the field of SATD identification by comparing existing approaches with a simple approach that leverages the predefined task tags to identify SATD. Method. We first propose a simple heuristic approach that fuzzily Matches task Annotation Tags (MAT) in comments to identify SATD. In nature, MAT is an unsupervised approach, which does not need any data to train a prediction model and has a good understandability. Then, we examine the real progress in SATD identification by comparing MAT against existing approaches. Result. The experimental results reveal that: (1) MAT has a similar or even superior performance for SATD identification compared with existing approaches, regardless of whether non-effort-aware or effort-aware evaluation indicators are considered; (2) the SATDs (or non-SATDs) correctly identified by existing approaches are highly overlapped with those identified by MAT; and (3) supervised approaches misclassify many SATDs marked with task tags as non-SATDs, which can be easily corrected by their combinations with MAT. Conclusion. It appears that the problem of SATD identification has been (unintentionally) complicated by our community, i.e., the real progress in SATD comments identification is not being achieved as it might have been envisaged. We hence suggest that, when many task tags are used in the comments of a target project, future SATD identification studies should use MAT as an easy-to-implement baseline to demonstrate the usefulness of any newly proposed approach."
Self-evaluation Interventions: Impact on Self-efficacy and Performance in Introductory Programming,"Lishinski, Alex and Yadav, Aman",10.1145/3447378,2021,"Research has repeatedly shown self-efficacy to be associated with course outcomes in CS and across other fields. CS education research has documented this and has developed CS-specific self-efficacy measurement instruments, but to date there have been only a few studies examining interventions intended to improve students’ self-efficacy in CS, and several types of self-efficacy interventions suggested by previous research remain to be tested in CS. This study attempts to address this lack of research by reporting on the results of a trial intervention intended to improve students’ self-efficacy in an introductory programming course. Students were recruited to complete a self-evaluation task, which previous research has suggested could have a beneficial impact on self-efficacy, which should in turn have a beneficial impact on course performance. Participating students’ course outcomes and self-efficacy were compared with those of the students who did not complete the self-evaluation task, using propensity score weighting adjustments to control for differences between the groups on entering characteristics and prior values of self-efficacy and course outcomes. We found that, whereas there was only marginal evidence for the self-evaluation intervention having a direct effect on self-efficacy, students who completed the self-evaluation task had significantly higher project scores during the weeks they were asked to complete it, compared to the students who did not participate. These findings suggest that there are potential benefits to incorporating self-evaluation tasks into introductory CS courses, although perhaps not by virtue of directly influencing self-efficacy."
"A GDPR-compliant Ecosystem for Speech Recognition with Transfer, Federated, and Evolutionary Learning","Jiang, Di and Tan, Conghui and Peng, Jinhua and Chen, Chaotao and Wu, Xueyang and Zhao, Weiwei and Song, Yuanfeng and Tong, Yongxin and Liu, Chang and Xu, Qian and Yang, Qiang and Deng, Li",10.1145/3447687,2021,"Automatic Speech Recognition (ASR) is playing a vital role in a wide range of real-world applications. However, Commercial ASR solutions are typically “one-size-fits-all” products and clients are inevitably faced with the risk of severe performance degradation in field test. Meanwhile, with new data regulations such as the European Union’s General Data Protection Regulation (GDPR) coming into force, ASR vendors, which traditionally utilize the speech training data in a centralized approach, are becoming increasingly helpless to solve this problem, since accessing clients’ speech data is prohibited. Here, we show that by seamlessly integrating three machine learning paradigms (i.e., Transfer learning, Federated learning, and Evolutionary learning (TFE)), we can successfully build a win-win ecosystem for ASR clients and vendors and solve all the aforementioned problems plaguing them. Through large-scale quantitative experiments, we show that with TFE, the clients can enjoy far better ASR solutions than the “one-size-fits-all” counterpart, and the vendors can exploit the abundance of clients’ data to effectively refine their own ASR products."
Software Engineering in Australasia,"Licorish, Sherlock A. and Treude, Christoph and Grundy, John and Blincoe, Kelly and MacDonell, Stephen and Tantithamthavorn, Chakkrit and Li, Li and Schneider, Jean-Guy",10.1145/3448992.3448995,2021,"Six months ago an important call was made for researchers globally to provide insights into the way Software Engineering is done in their region. Heeding this call, we hereby outline the position Software Engineering in Australasia (New Zealand and Australia). This article first considers the software development methods, practices and tools that are popular in the Australasian software engineering community. We then briefly review the particular strengths of software engineering researchers in Australasia. Finally, we make an open call for collaborators by reflecting on our current position and identifying future opportunities."
Emerging Software Engineering Research Networks in (East) Africa,"Bainomugisha, Engineer and Hebig, Regina and R. V. Chaudron, Michel",10.1145/3448992.3448996,2021,Software engineering (SE) researchers and research networks from emerging communities are often not visible in already established Software Engineering venues for a multitude of reasons. This limits the opportunities and mutual bene ts that can arise from collaborations between global and emerging Software Engineer- ing networks. This article focuses on a rst attempt to provide a map of the African software engineering research community with focus on the networks of two big East African Universities. We hope that this very initial mapping e ort will help to raise aware- ness in the international community about the variety of software engineering research in Africa. We formulate some suggestions for making our academic Software Engineering community more inclusive.
Where Responsible AI meets Reality: Practitioner Perspectives on Enablers for Shifting Organizational Practices,"Rakova, Bogdana and Yang, Jingying and Cramer, Henriette and Chowdhury, Rumman",10.1145/3449081,2021,"Large and ever-evolving technology companies continue to invest more time and resources to incorporate responsible Artificial Intelligence (AI) into production-ready systems to increase algorithmic accountability. This paper examines and seeks to offer a framework for analyzing how organizational culture and structure impact the effectiveness of responsible AI initiatives in practice. We present the results of semi-structured qualitative interviews with practitioners working in industry, investigating common challenges, ethical tensions, and effective enablers for responsible AI initiatives. Focusing on major companies developing or utilizing AI, we have mapped what organizational structures currently support or hinder responsible AI initiatives, what aspirational future processes and structures would best enable effective initiatives, and what key elements comprise the transition from current work practices to the aspirational future."
Modular Politics: Toward a Governance Layer for Online Communities,"Schneider, Nathan and De Filippi, Primavera and Frey, Seth and Tan, Joshua Z. and Zhang, Amy X.",10.1145/3449090,2021,"Governance in online communities is an increasingly high-stakes challenge, and yet many basic features of offline governance legacies-juries, political parties, term limits, and formal debates, to name a few-are not in the feature-sets of the software most community platforms use. Drawing on the paradigm of Institutional Analysis and Development, this paper proposes a strategy for addressing this lapse by specifying basic features of a generalizable paradigm for online governance called Modular Politics. Whereas classical governance typologies tend to present a choice among wholesale ideologies, such as democracy or oligarchy, Modular Politics would enable platform operators and their users to build bottom-up governance processes from computational components that are modular and composable, highly versatile in their expressiveness, portable from one context to another, and interoperable across platforms. This kind of approach could implement pre-digital governance systems as well as accelerate innovation in uniquely digital techniques. As diverse communities share and connect their components and data, governance could occur through a ubiquitous network layer. To that end, this paper proposes the development of an open standard for networked governance."
"Evaluating MIDST, A System to Support Stigmergic Team Coordination","Crowston, Kevin and Saltz, Jeffery and Sitaula, Niraj and Hegde, Yatish",10.1145/3449110,2021,"Data science teams working on a shared analysis face coordination problems such as dividing up the work to be done, monitoring performance and integrating the pieces. Research on distributed software development teams has raised the potential of stigmergic coordination, that is, coordination through a shared work product in place of explicit communication. The MIDST system was developed to support stigmergic coordination by making individual contributions to a shared work product visible, legible and combinable. In this paper, we present initial studies of a total of 40 student teams (24 using MIDST) that shows that teams that used MIDST did experience the intended system affordances to support their work, did seem to coordinate at least in part stigmergically and performed better on an assigned project."
Detection of Social Identification in Workgroups from a Passively-sensed WiFi Infrastructure,"Zakaria, Camellia and Lee, Youngki and Balan, Rajesh",10.1145/3449145,2021,"Social identification: how much individuals psychologically associate themselves with a group has been posited as an essential construct to measure individual and group dynamics. Studies have shown that individuals who identify very differently from their workgroup provide critical cues to the lack of social support or work overloads. However, measuring identification is typically achieved through time-consuming and privacy-invasive surveys. We hypothesize that the extremities in-group norm affects individuals' behaviors, thus more likely to give rise to negative appraisals. As a more convenient and less-invasive technique, we propose a method to predict individuals who are increasingly different in identifying themselves with their working peers using mobility data passively sensed from the WiFi infrastructure. To test our hypothesis, we collected WiFi data of 62 college students over a whole semester. Students provided regular self-reports on their identification towards a workgroup as ground truth. We analyze the contrasts between groups' mobility patterns and build a classification model to determine students who identify very differently from their workgroup. The classifier achieves approximately 80% True Positive Rate (TPR), 73% True negative rate (TNR), and 78% Accuracy (ACC). Such a mechanism can help distinguish students who are more likely to struggle with negative workgroup appraisals and enable interventions to improve their overall team experience."
Problematic Machine Behavior: A Systematic Literature Review of Algorithm Audits,"Bandy, Jack",10.1145/3449148,2021,"While algorithm audits are growing rapidly in commonality and public importance, relatively little scholarly work has gone toward synthesizing prior work and strategizing future research in the area. This systematic literature review aims to do just that, following PRISMA guidelines in a review of over 500 English articles that yielded 62 algorithm audit studies. The studies are synthesized and organized primarily by behavior (discrimination, distortion, exploitation, and misjudgement), with codes also provided for domain (e.g. search, vision, advertising, etc.), organization (e.g. Google, Facebook, Amazon, etc.), and audit method (e.g. sock puppet, direct scrape, crowdsourcing, etc.). The review shows how previous audit studies have exposed public-facing algorithms exhibiting problematic behavior, such as search algorithms culpable of distortion and advertising algorithms culpable of discrimination. Based on the studies reviewed, it also suggests some behaviors (e.g. discrimination on the basis of intersectional identities), domains (e.g. advertising algorithms), methods (e.g. code auditing), and organizations (e.g. Twitter, TikTok, LinkedIn) that call for future audit attention. The paper concludes by offering the common ingredients of successful audits, and discussing algorithm auditing in the context of broader research working toward algorithmic justice."
Supporting Serendipity: Opportunities and Challenges for Human-AI Collaboration in Qualitative Analysis,"Jiang, Jialun Aaron and Wade, Kandrea and Fiesler, Casey and Brubaker, Jed R.",10.1145/3449168,2021,"Qualitative inductive methods are widely used in CSCW and HCI research for their ability to generatively discover deep and contextualized insights, but these inherently manual and human-resource-intensive processes are often infeasible for analyzing large corpora. Researchers have been increasingly interested in ways to apply qualitative methods to ""big"" data problems, hoping to achieve more generalizable results from larger amounts of data while preserving the depth and richness of qualitative methods. In this paper, we describe a study of qualitative researchers' work practices and their challenges, with an eye towards whether this is an appropriate domain for human-AI collaboration and what successful collaborations might entail. Our findings characterize participants' diverse methodological practices and nuanced collaboration dynamics, and identify areas where they might benefit from AI-based tools. While participants highlight the messiness and uncertainty of qualitative inductive analysis, they still want full agency over the process and believe that AI should not interfere. Our study provides a deep investigation of task delegability in human-AI collaboration in the context of qualitative analysis, and offers directions for the design of AI assistance that honor serendipity, human agency, and ambiguity."
"Logging On, Reaching Out, and Getting By: A Review of Self-reported Psychosocial Impacts of Online Peer Support for People Impacted by Cancer","Allison, Kimberley R. and Patterson, Pandora and Guilbert, Daniel and Noke, Melissa and Husson, Olga",10.1145/3449169,2021,"For individuals impacted by their own or a family member's cancer, connecting with other people in similar situations can be an invaluable source of informational and emotional support. Online spaces provide opportunities for peer support that may be more accessible, given the medical and logistical restrictions on face-to-face socialisation associated with cancer and treatment. However, little is known about the impacts of online peer support. This systematic review surveys the literature on psychosocial impacts of online peer support for people impacted by cancer, integrating research from psychology, health, communications, informatics and social computing disciplines. The reviewed papers and interventions vary widely in the type of online peer support provided, who this support was intended for, and how outcomes were evaluated. Quantitative evidence suggests that online peer support may improve psychosocial wellbeing, particularly anxiety and stress, although this appears to depend on how individuals engage and interact with others. Qualitative findings suggest clear value in connecting and sharing experiences with those in similar situations, benefits which may not be well captured quantitatively. For individuals who share experiences, express emotions and feel understood and accepted by others, online peer spaces may be a valuable and viable source of support. However, this require strategic community design and management to build an active and sustainable group dynamic which can effectively and safely support people impacted by cancer."
"I Want to, but First I Need to: Understanding Crowdworkers' Career Goals, Challenges, and Tensions","Rivera, Veronica A. and Lee, David T.",10.1145/3449224,2021,"Career development is vital for ensuring a happy and productive workforce, and for maintaining relevance in a rapidly changing economy shaped by technological progress. Yet career development is largely ignored in crowdwork. Crowdwork platforms like Amazon Mechanical Turk (AMT) do not support crowdworkers in reskilling and changing careers. In this paper, we study the career goals of AMT workers and the challenges they face in trying to transition out of crowdwork and into high-skill jobs offline or into specialized freelance work. We performed a qualitative study in which we surveyed 20 AMT workers and interviewed 6 of them about their career goals, how they are currently pursuing them, and the challenges they have faced. We found that crowdworkers aspire to transition out of AMT but face challenges due to lack of career guidance, and limited time and financial resources. Drawing on literature in career studies and organization science, we discuss how crowdworkers' challenges are further aggravated by the enviornment on AMT, and provide implications for future research and design that may better support crowdworkers in making a career change."
A Systematic Literature Review on Federated Machine Learning: From a Software Engineering Perspective,"Lo, Sin Kit and Lu, Qinghua and Wang, Chen and Paik, Hye-Young and Zhu, Liming",10.1145/3450288,2021,"Federated learning is an emerging machine learning paradigm where clients train models locally and formulate a global model based on the local model updates. To identify the state-of-the-art in federated learning and explore how to develop federated learning systems, we perform a systematic literature review from a software engineering perspective, based on 231 primary studies. Our data synthesis covers the lifecycle of federated learning system development that includes background understanding, requirement analysis, architecture design, implementation, and evaluation. We highlight and summarise the findings from the results and identify future trends to encourage researchers to advance their current work."
A Survey on Stream-Based Recommender Systems,"Al-Ghossein, Marie and Abdessalem, Talel and BARR\'{E}, Anthony",10.1145/3453443,2021,"Recommender Systems (RS) have proven to be effective tools to help users overcome information overload, and significant advances have been made in the field over the past two decades. Although addressing the recommendation problem required first a formulation that could be easily studied and evaluated, there currently exists a gap between research contributions and industrial applications where RS are actually deployed. In particular, most RS are meant to function in batch: they rely on a large static dataset and build a recommendation model that is only periodically updated. This functioning introduces several limitations in various settings, leading to considering more realistic settings where RS learn from continuous streams of interactions. Such RS are framed as Stream-Based Recommender Systems (SBRS).In this article, we review SBRS, underline their relation with time-aware RS and online adaptive learning, and present and categorize existing work that tackle the corresponding problem and its multiple facets. We discuss the methodologies used to evaluate SBRS and the adapted datasets that can be used, and finally we outline open challenges in the area."
Understanding Software-2.0: A Study of Machine Learning Library Usage and Evolution,"Dilhara, Malinda and Ketkar, Ameya and Dig, Danny",10.1145/3453478,2021,"Enabled by a rich ecosystem of Machine Learning (ML) libraries, programming using learned models, i.e., Software-2.0, has gained substantial adoption. However, we do not know what challenges developers encounter when they use ML libraries. With this knowledge gap, researchers miss opportunities to contribute to new research directions, tool builders do not invest resources where automation is most needed, library designers cannot make informed decisions when releasing ML library versions, and developers fail to use common practices when using ML libraries.We present the first large-scale quantitative and qualitative empirical study to shed light on how developers in Software-2.0 use ML libraries, and how this evolution affects their code. Particularly, using static analysis we perform a longitudinal study of 3,340 top-rated open-source projects with 46,110 contributors. To further understand the challenges of ML library evolution, we survey 109 developers who introduce and evolve ML libraries. Using this rich dataset we reveal several novel findings.Among others, we found an increasing trend of using ML libraries: The ratio of new Python projects that use ML libraries increased from 2% in 2013 to 50% in 2018. We identify several usage patterns including the following: (i) 36% of the projects use multiple ML libraries to implement various stages of the ML workflows, (ii) developers update ML libraries more often than the traditional libraries, (iii) strict upgrades are the most popular for ML libraries among other update kinds, (iv) ML library updates often result in cascading library updates, and (v) ML libraries are often downgraded (22.04% of cases). We also observed unique challenges when evolving and maintaining Software-2.0 such as (i) binary incompatibility of trained ML models and (ii) benchmarking ML models. Finally, we present actionable implications of our findings for researchers, tool builders, developers, educators, library vendors, and hardware vendors."
Software development in disruptive times,"Varaj\~{a}o, Jo\~{a}o",10.1145/3453932,2021,"Creating a software solution with fast decision capability, agile project management, and extreme low-code technology."
The SPACE of Developer Productivity: There's more to it than you think.,"Forsgren, Nicole and Storey, Margaret-Anne and Maddila, Chandra and Zimmermann, Thomas and Houck, Brian and Butler, Jenna",10.1145/3454122.3454124,2021,"Developer productivity is about more than an individual's activity levels or the efficiency of the engineering systems relied on to ship software, and it cannot be measured by a single metric or dimension. The SPACE framework captures different dimensions of productivity, and here we demonstrate how this framework can be used to understand productivity in practice and why using it will help teams better understand developer productivity and create better measures to inform their work and teams."
"Software Development in Disruptive Times: Creating a software solution with fast decision capability, agile project management, and extreme low-code technology","Varaj\~{a}o, Jo\~{a}o",10.1145/3454122.3458743,2021,"In this project, the challenge was to ""deploy software faster than the coronavirus spread."" In a project with such peculiar characteristics, several factors can influence success, but some clearly stand out: top management support, agility, understanding and commitment of the project team, and the technology used. Conventional development approaches and technologies would simply not be able to meet the requirements promptly."
Temporal Relation Extraction in Clinical Texts: A Systematic Review,"Gumiel, Yohan Bonescki and Silva e Oliveira, Lucas Emanuel and Claveau, Vincent and Grabar, Natalia and Paraiso, Emerson Cabrera and Moro, Claudia and Carvalho, Deborah Ribeiro",10.1145/3462475,2021,"Unstructured data in electronic health records, represented by clinical texts, are a vast source of healthcare information because they describe a patient's journey, including clinical findings, procedures, and information about the continuity of care. The publication of several studies on temporal relation extraction from clinical texts during the last decade and the realization of multiple shared tasks highlight the importance of this research theme. Therefore, we propose a review of temporal relation extraction in clinical texts. We analyzed 105 articles and verified that relations between events and document creation time, a coarse temporality type, were addressed with traditional machine learning–based models with few recent initiatives to push the state-of-the-art with deep learning–based models. For temporal relations between entities (event and temporal expressions) in the document, factors such as dataset imbalance because of candidate pair generation and task complexity directly affect the system's performance. The state-of-the-art resides on attention-based models, with contextualized word representations being fine-tuned for temporal relation extraction. However, further experiments and advances in the research topic are required until real-time clinical domain applications are released. Furthermore, most of the publications mainly reside on the same dataset, hindering the need for new annotation projects that provide datasets for different medical specialties, clinical text types, and even languages."
Skills Gaps in the Industry: Opinions of Embedded Software Practitioners,"Akdur, Deniz",10.1145/3463340,2021,"Many practitioners in the software-intensive embedded industry often face difficulties after beginning their careers due to misalignment of the skills learned at the university with what is required in the workplace. Companies spend crucial resources to train personnel whose academic backgrounds are not only based on “computing disciplines” but also on non-computing ones. Analyzing the gap between the software industry and academia is important for three reasons: (1) for employers, hiring properly trained practitioners allows them to spend less time in training them while incorporating them more efficiently into the workforce; (2) for practitioners, knowing the most important skillset is helpful to increase their chance of employability; and (3) for academia, understanding the necessary skillset is critical to making curriculum changes. To achieve these objectives, we conducted a survey that yielded responses from 659 software professionals working worldwide in different roles. In this study, we only included the responses of 393 embedded software practitioners whose undergraduate degree was completed in Turkey, working in 10 countries. This article sheds light on the most important skills in the embedded software industry by presenting various cross-factor analyses. Understanding the coverage of these skills in the curriculum (mostly in Turkish universities) helps bridge the gaps, which can and should be achieved through more Industry Academia Collaborations (IACs)."
Evaluation of Software Architectures under Uncertainty: A Systematic Literature Review,"Sobhy, Dalia and Bahsoon, Rami and Minku, Leandro and Kazman, Rick",10.1145/3464305,2021,"Context: Evaluating software architectures in uncertain environments raises new challenges, which require continuous approaches. We define continuous evaluation as multiple evaluations of the software architecture that begins at the early stages of the development and is periodically and repeatedly performed throughout the lifetime of the software system. Numerous approaches have been developed for continuous evaluation; to handle dynamics and uncertainties at run-time, over the past years, these approaches are still very few, limited, and lack maturity. Objective: This review surveys efforts on architecture evaluation and provides a unified terminology and perspective on the subject. Method: We conducted a systematic literature review to identify and analyse architecture evaluation approaches for uncertainty including continuous and non-continuous, covering work published between 1990–2020. We examined each approach and provided a classification framework for this field. We present an analysis of the results and provide insights regarding open challenges. Major results and conclusions: The survey reveals that most of the existing architecture evaluation approaches typically lack an explicit linkage between design-time and run-time. Additionally, there is a general lack of systematic approaches on how continuous architecture evaluation can be realised or conducted. To remedy this lack, we present a set of necessary requirements for continuous evaluation and describe some examples."
Context-aware Retrieval-based Deep Commit Message Generation,"Wang, Haoye and Xia, Xin and Lo, David and He, Qiang and Wang, Xinyu and Grundy, John",10.1145/3464689,2021,"Commit messages recorded in version control systems contain valuable information for software development, maintenance, and comprehension. Unfortunately, developers often commit code with empty or poor quality commit messages. To address this issue, several studies have proposed approaches to generate commit messages from commit diffs. Recent studies make use of neural machine translation algorithms to try and translate git diffs into commit messages and have achieved some promising results. However, these learning-based methods tend to generate high-frequency words but ignore low-frequency ones. In addition, they suffer from exposure bias issues, which leads to a gap between training phase and testing phase.In this article, we propose CoRec to address the above two limitations. Specifically, we first train a context-aware encoder-decoder model that randomly selects the previous output of the decoder or the embedding vector of a ground truth word as context to make the model gradually aware of previous alignment choices. Given a diff for testing, the trained model is reused to retrieve the most similar diff from the training set. Finally, we use the retrieval diff to guide the probability distribution for the final generated vocabulary. Our method combines the advantages of both information retrieval and neural machine translation. We evaluate CoRec on a dataset from Liu et&nbsp;al. and a large-scale dataset crawled from 10K popular Java repositories in Github. Our experimental results show that CoRec significantly outperforms the state-of-the-art method NNGen by 19% on average in terms of BLEU."
The Agile Success Model: A Mixed-methods Study of a Large-scale Agile Transformation,"Russo, Daniel",10.1145/3464938,2021,"Organizations are increasingly adopting Agile frameworks for their internal software development. Cost reduction, rapid deployment, requirements and mental model alignment are typical reasons for an Agile transformation. This article presents an in-depth field study of a large-scale Agile transformation in a mission-critical environment, where stakeholders’ commitment was a critical success factor. The goal of such a transformation was to implement mission-oriented features, reducing costs and time to operate in critical scenarios. The project lasted several years and involved over 40 professionals. We report how a hierarchical and plan-driven organization exploited Agile methods to develop a Command &amp; Control (C2) system. Accordingly, we first abstract our experience, inducing a success model of general use for other comparable organizations by performing a post-mortem study. The goal of the inductive research process was to identify critical success factors and their relations. Finally, we validated and generalized our model through Partial Least Squares - Structural Equation Modelling, surveying 200 software engineers involved in similar projects. We conclude the article with data-driven recommendations concerning the management of Agile projects."
Automating App Review Response Generation Based on Contextual Knowledge,"Gao, Cuiyun and Zhou, Wenjie and Xia, Xin and Lo, David and Xie, Qi and Lyu, Michael R.",10.1145/3464969,2021,"User experience of mobile apps is an essential ingredient that can influence the user base and app revenue. To ensure good user experience and assist app development, several prior studies resort to analysis of app reviews, a type of repository that directly reflects user opinions about the apps. Accurately responding to the app reviews is one of the ways to relieve user concerns and thus improve user experience. However, the response quality of the existing method relies on the pre-extracted features from other tools, including manually labelled keywords and predicted review sentiment, which may hinder the generalizability and flexibility of the method. In this article, we propose a novel neural network approach, named CoRe, with the contextual knowledge naturally incorporated and without involving external tools. Specifically, CoRe integrates two types of contextual knowledge in the training corpus, including official app descriptions from app store and responses of the retrieved semantically similar reviews, for enhancing the relevance and accuracy of the generated review responses. Experiments on practical review data show that CoRe can outperform the state-of-the-art method by 12.36% in terms of BLEU-4, an accuracy metric that is widely used to evaluate text generation systems."
A Comprehensive Review and a Taxonomy Proposal of Team Formation Problems,"Ju\'{a}rez, Julio and Santos, Cipriano (Pano) and Brizuela, Carlos A.",10.1145/3465399,2021,"With a growing interest in high-performing work teams and how to form them, a new computational challenge, denominated Team Formation Problem (TFP), has emerged. After almost two decades of research on this problem, many works continue to raise particular conceptions of what a TFP is. Any new practitioner, unfamiliar with the problem, may be hindered in discerning what is essential and what is particular in each proposal. Until now, there was a lack of a document serving as a guide, synthesizing and framing what has been done to date. In this review, we mainly introduce two things: (1) a taxonomy proposal for the TFPs and (2) the description of the main components of a TFP. Additionally, we present and discuss some applications, complexity, and resolution methods. Finally, we conclude by describing some perspectives on this topic for future studies, where we give some insight into open problems and research opportunities. The main goal of this review is to facilitate the generalization of research, identify knowledge gaps, and support the development of a theory for the TFP."
A Survey on Session-based Recommender Systems,"Wang, Shoujin and Cao, Longbing and Wang, Yan and Sheng, Quan Z. and Orgun, Mehmet A. and Lian, Defu",10.1145/3465401,2021,"Recommender systems (RSs) have been playing an increasingly important role for informed consumption, services, and decision-making in the overloaded information era and digitized economy. In recent years, session-based recommender systems (SBRSs) have emerged as a new paradigm of RSs. Different from other RSs such as content-based RSs and collaborative filtering-based RSs that usually model long-term yet static user preferences, SBRSs aim to capture short-term but dynamic user preferences to provide more timely and accurate recommendations sensitive to the evolution of their session contexts. Although SBRSs have been intensively studied, neither unified problem statements for SBRSs nor in-depth elaboration of SBRS characteristics and challenges are available. It is also unclear to what extent SBRS challenges have been addressed and what the overall research landscape of SBRSs is. This comprehensive review of SBRSs addresses the above aspects by exploring in depth the SBRS entities (e.g., sessions), behaviours (e.g., users’ clicks on items), and their properties (e.g., session length). We propose a general problem statement of SBRSs, summarize the diversified data characteristics and challenges of SBRSs, and define a taxonomy to categorize the representative SBRS research. Finally, we discuss new research opportunities in this exciting and vibrant area."
The Impact of Dormant Defects on Defect Prediction: A Study of 19 Apache Projects,"Falessi, Davide and Ahluwalia, Aalok and Penta, Massimiliano DI",10.1145/3467895,2021,"Defect prediction models can be beneficial to prioritize testing, analysis, or code review activities, and has been the subject of a substantial effort in academia, and some applications in industrial contexts. A necessary precondition when creating a defect prediction model is the availability of defect data from the history of projects. If this data is noisy, the resulting defect prediction model could result to be unreliable. One of the causes of noise for defect datasets is the presence of “dormant defects,” i.e., of defects discovered several releases after their introduction. This can cause a class to be labeled as defect-free while it is not, and is, therefore “snoring.” In this article, we investigate the impact of snoring on classifiers' accuracy and the effectiveness of a possible countermeasure, i.e., dropping too recent data from a training set. We analyze the accuracy of 15 machine learning defect prediction classifiers, on data from more than 4,000 defects and 600 releases of 19 open source projects from the Apache ecosystem. Our results show that on average across projects (i) the presence of dormant defects decreases the recall of defect prediction classifiers, and (ii) removing from the training set the classes that in the last release are labeled as not defective significantly improves the accuracy of the classifiers. In summary, this article provides insights on how to create defects datasets by mitigating the negative effect of dormant defects on defect prediction."
MICOSE4aPS: Industrially Applicable Maturity Metric to Improve Systematic Reuse of Control Software,"Vogel-Heuser, Birgit and Neumann, Eva-Maria and Fischer, Juliane",10.1145/3467896,2021,"automated Production Systems (aPS) are highly complex, mechatronic systems that usually have to operate reliably for many decades. Standardization and reuse of control software modules is a core prerequisite to achieve the required system quality in increasingly shorter development cycles. However, industrial case studies in aPS show that many aPS companies still struggle with strategically reusing software. This paper proposes a metric-based approach to objectively measure the maturity of industrial IEC 61131-based control software in aPS (MICOSE4aPS) to identify potential weaknesses and quality issues hampering systematic reuse. Module developers in the machine and plant manufacturing industry can directly benefit as the metric calculation is integrated into the software engineering workflow. An in-depth industrial evaluation in a top-ranked machine manufacturing company in food packaging and an expert evaluation with different companies confirmed the benefit of efficiently managing the quality of control software."
A journal for interdisciplinary data science education,"Hazzan, Orit and Mike, Koby",10.1145/3469281,2021,"The Communications website, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts.twitterFollow us on Twitter at http://twitter.com/blogCACMhttp://cacm.acm.org/blogs/blog-cacmOrit Hazzan and Koby Mike on the need for a journal to cover data science education exclusively."
Social Robots for the Care of Persons with Dementia: A Systematic Review,"Ghafurian, Moojan and Hoey, Jesse and Dautenhahn, Kerstin",10.1145/3469653,2021,"Intelligent assistive robots can enhance the quality of life of people with dementia and their caregivers. They can increase the independence of older adults, reduce tensions between a person with dementia and their caregiver, and increase social engagement. This article provides a review of assistive robots designed for and evaluated by persons with dementia. Assistive robots that only increased mobility or brain-computer interfaces were excluded. Google Scholar, IEEE Digital Library, PubMed, and ACM Digital Library were searched. A final set of 53 articles covering research in 16 different countries are reviewed. Assistive robots are categorized into five different applications and evaluated for their effectiveness, as well as the robots’ social and emotional capabilities. Our findings show that robots used in the context of therapy or for increasing engagement received the most attention in the literature, whereas the robots that assist by providing health guidance or help with an activity of daily living received relatively limited attention. PARO was the most commonly used robot in dementia care studies. The effectiveness of each assistive robot and the outcome of the studies are discussed, and particularly, the social/emotional capabilities of each assistive robot are summarized. Gaps in the research literature are identified and we provide directions for future work."
"A Comprehensive Survey of Attacks without Physical Access Targeting Hardware Vulnerabilities in IoT/IIoT Devices, and Their Detection Mechanisms","Polychronou, Nikolaos-Foivos and Thevenon, Pierre-Henri and Puys, Maxime and Beroulle, Vincent",10.1145/3471936,2021,"With the advances in the field of the Internet of Things (IoT) and Industrial IoT (IIoT), these devices are increasingly used in daily life or industry. To reduce costs related to the time required to develop these devices, security features are usually not considered. This situation creates a major security concern. Many solutions have been proposed to protect IoT/IIoT against various attacks, most of which are based on attacks involving physical access. However, a new class of attacks has emerged targeting hardware vulnerabilities in the micro-architecture that do not require physical access. We present attacks based on micro-architectural hardware vulnerabilities and the side effects they produce in the system. In addition, we present security mechanisms that can be implemented to address some of these attacks. Most of the security mechanisms target a small set of attack vectors or a single specific attack vector. As many attack vectors exist, solutions must be found to protect against a wide variety of threats. This survey aims to inform designers about the side effects related to attacks and detection mechanisms that have been described in the literature. For this purpose, we present two tables listing and classifying the side effects and detection mechanisms based on the given criteria."
Hiring CS Graduates: What We Learned from Employers,"Stepanova, Anna and Weaver, Alexis and Lahey, Joanna and Alexander, Gerianne and Hammond, Tracy",10.1145/3474623,2021,"Computer science (CS) majors are in high demand and account for a large part of national computer and information technology job market applicants. Employment in this sector is projected to grow 12% between 2018 and 2028, which is faster than the average of all other occupations. Published data are available on traditional non-computer science-specific hiring processes. However, the hiring process for CS majors may be different. It is critical to have up-to-date information on questions such as “what positions are in high demand for CS majors?,” “what is a typical hiring process?,” and “what do employers say they look for when hiring CS graduates?” This article discusses the analysis of a survey of 218 recruiters hiring CS graduates in the United States. We used Atlas.ti to analyze qualitative survey data and report the results on what positions are in the highest demand, the hiring process, and the resume review process. Our study revealed that a software developer was the most common job the recruiters were looking to fill. We found that the hiring process steps for CS graduates are generally aligned with traditional hiring steps, with an additional emphasis on technical and coding tests. Recruiters reported that their hiring choices were based on reviewing resume’s experience, GPA, and projects sections. The results provide insights into the hiring process, decision making, resume analysis, and some discrepancies between current undergraduate CS program outcomes and employers’ expectations."
"To Be or Not to Be Stuck, or Is It a Continuum? A Systematic Literature Review on the Concept of Being Stuck in Games","Drey, Tobias and Fischbach, Fabian and Jansen, Pascal and Frommel, Julian and Rietzler, Michael and Rukzio, Enrico",10.1145/3474656,2021,"Players can get stuck in video games, which impedes their process to their goal and results in unfavorable outcomes like negative emotions, impediments of flow, and obstacles for learning. Currently, it is not easily possible to assess if a player is stuck, as no widely accepted definition of ""being stuck"" in games exists. We conducted 13 expert interviews and a systematic literature review with 104 relevant papers selected from 4022 candidates. We present a definition of ""being stuck"" that conceptualizes the state as a continuum and contextualize it within related concepts. Our stuck continuum can be applied to regulate the player's stuck level. We propose a taxonomy of measures that are useful for the detection of the level of stuckness and discuss the effectiveness of countermeasures. Our stuck concept is crucial for game developers creating an optimal player experience in games."
Transparency in Measurement Reporting: A Systematic Literature Review of CHI PLAY,"Aeschbach, Lena Fanya and Perrig, Sebastian A.C. and Weder, Lorena and Opwis, Klaus and Br\""{u}hlmann, Florian",10.1145/3474660,2021,"Measuring theoretical concepts, so-called constructs, is a central challenge of Player Experience research. Building on recent work in HCI and psychology, we conducted a systematic literature review to study the transparency of measurement reporting. We accessed the ACM Digital Library to analyze all 48 full papers published at CHI PLAY 2020, of those, 24 papers used self-report measurements and were included in the full review. We assessed specifically, whether researchers reported What, How and Why they measured. We found that researchers matched their measures to the construct under study and that administrative details, such as number of points on a Likert-type scale, were frequently reported. However, definitions of the constructs to be measured and justifications for selecting a particular scale were sparse. Lack of transparency in these areas threaten the validity of singular studies, but further compromise the building of theories and accumulation of research knowledge in meta-analytic work. This work is limited to only assessing the current transparency of measurement reporting at CHI PLAY 2020, however we argue this constitutes a fair foundation to assess potential pitfalls. To address these pitfalls, we propose a prescriptive model of a measurement selection process, which aids researchers to systematically define their constructs, specify operationalizations, and justify why these measures were chosen. Future research employing this model should contribute to more transparency in measurement reporting. The research was funded through internal resources. All materials are available on https://osf.io/4xz2v/."
Biofeedback Methods in Entertainment Video Games: A Review of Physiological Interaction Techniques,"Navarro, Diego and Sundstedt, Veronica and Garro, Valeria",10.1145/3474695,2021,"The area of biofeedback interaction has grown over recent years, thanks to the release of more affordable and reliable sensor technology, and the accessibility offered by modern game development tools. This article presents a systematic literature review focusing on how different biofeedback interaction methods have been used for entertainment purposes in video games, between 2008 and 2020. It divides previous contributions in terms of a proposed interaction classification criteria and five different biofeedback methods (with a sixth category combining them): electroencephalography, electrocardiography, eye tracking, electrodermal activity, electromyography, and multi-modal interaction. The review describes the properties, sensor technologies, and the type of data gathered for every included biofeedback method, and presents their respective interaction techniques. It summarizes a set of opportunities and challenges for each included method, based on the results from previous work, and discusses these findings. It also analyzes how these interaction techniques are distributed between different common game genres. The review is beneficial for people interested in biofeedback methods and their potential use for novel interaction techniques in future video games."
"Orienting, Framing, Bridging, Magic, and Counseling: How Data Scientists Navigate the Outer Loop of Client Collaborations in Industry and Academia","Kross, Sean and Guo, Philip",10.1145/3476052,2021,"Data scientists often collaborate with clients to analyze data to meet a client's needs. What does the end-to-end workflow of a data scientist's collaboration with clients look like throughout the lifetime of a project? To investigate this question, we interviewed ten data scientists (5 female, 4 male, 1 non-binary) in diverse roles across industry and academia. We discovered that they work with clients in a six-stage outer-loop workflow, which involves 1) laying groundwork by building trust before a project begins, 2) orienting to the constraints of the client's environment, 3) collaboratively framing the problem, 4) bridging the gap between data science and domain expertise, 5) the inner loop of technical data analysis work, 6) counseling to help clients emotionally cope with analysis results. This novel outer-loop workflow contributes to CSCW by expanding the notion of what collaboration means in data science beyond the widely-known inner-loop technical workflow stages of acquiring, cleaning, analyzing, modeling, and visualizing data. We conclude by discussing the implications of our findings for data science education, parallels to design work, and unmet needs for tool development."
A Survey on Uncertainty Estimation in Deep Learning Classification Systems from a Bayesian Perspective,"Mena, Jos\'{e} and Pujol, Oriol and Vitri\`{a}, Jordi",10.1145/3477140,2021,"Decision-making based on machine learning systems, especially when this decision-making can affect human lives, is a subject of maximum interest in the Machine Learning community. It is, therefore, necessary to equip these systems with a means of estimating uncertainty in the predictions they emit in order to help practitioners make more informed decisions. In the present work, we introduce the topic of uncertainty estimation, and we analyze the peculiarities of such estimation when applied to classification systems. We analyze different methods that have been designed to provide classification systems based on deep learning with mechanisms for measuring the uncertainty of their predictions. We will take a look at how this uncertainty can be modeled and measured using different approaches, as well as practical considerations of different applications of uncertainty. Moreover, we review some of the properties that should be borne in mind when developing such metrics. All in all, the present survey aims at providing a pragmatic overview of the estimation of uncertainty in classification systems that can be very useful for both academic research and deep learning practitioners."
On the Reproducibility and Replicability of Deep Learning in Software Engineering,"Liu, Chao and Gao, Cuiyun and Xia, Xin and Lo, David and Grundy, John and Yang, Xiaohu",10.1145/3477535,2021,"Context: Deep learning (DL) techniques have gained significant popularity among software engineering (SE) researchers in recent years. This is because they can often solve many SE challenges without enormous manual feature engineering effort and complex domain knowledge.Objective: Although many DL studies have reported substantial advantages over other state-of-the-art models on effectiveness, they often ignore two factors: (1) reproducibility—whether the reported experimental results can be obtained by other researchers using authors’ artifacts (i.e., source code and datasets) with the same experimental setup; and (2) replicability—whether the reported experimental result can be obtained by other researchers using their re-implemented artifacts with a different experimental setup. We observed that DL studies commonly overlook these two factors and declare them as minor threats or leave them for future work. This is mainly due to high model complexity with many manually set parameters and the time-consuming optimization process, unlike classical supervised machine learning (ML) methods (e.g., random forest). This study aims to investigate the urgency and importance of reproducibility and replicability for DL studies on SE tasks.Method: In this study, we conducted a literature review on 147 DL studies recently published in 20 SE venues and 20 AI (Artificial Intelligence) venues to investigate these issues. We also re-ran four representative DL models in SE to investigate important factors that may strongly affect the reproducibility and replicability of a study.Results: Our statistics show the urgency of investigating these two factors in SE, where only 10.2% of the studies investigate any research question to show that their models can address at least one issue of replicability and/or reproducibility. More than 62.6% of the studies do not even share high-quality source code or complete data to support the reproducibility of their complex models. Meanwhile, our experimental results show the importance of reproducibility and replicability, where the reported performance of a DL model could not be reproduced for an unstable optimization process. Replicability could be substantially compromised if the model training is not convergent, or if performance is sensitive to the size of vocabulary and testing data.Conclusion: It is urgent for the SE community to provide a long-lasting link to a high-quality reproduction package, enhance DL-based solution stability and convergence, and avoid performance sensitivity on different sampled data."
"The ""Shut the f**k up"" Phenomenon: Characterizing Incivility in Open Source Code Review Discussions","Ferreira, Isabella and Cheng, Jinghui and Adams, Bram",10.1145/3479497,2021,"Code review is an important quality assurance activity for software development. Code review discussions among developers and maintainers can be heated and sometimes involve personal attacks and unnecessary disrespectful comments, demonstrating, therefore, incivility. Although incivility in public discussions has received increasing attention from researchers in different domains, the knowledge about the characteristics, causes, and consequences of uncivil communication is still very limited in the context of software development, and more specifically, code review. To address this gap in the literature, we leverage the mature social construct of incivility as a lens to understand confrontational conflicts in open source code review discussions. For that, we conducted a qualitative analysis on 1,545 emails from the Linux Kernel Mailing List (LKML) that were associated with rejected changes. We found that more than half (66.66%) of the non-technical emails included uncivil features. Particularly, frustration, name calling, and impatience are the most frequent features in uncivil emails. We also found that there are civil alternatives to address arguments, while uncivil comments can potentially be made by any people when discussing any topic. Finally, we identified various causes and consequences of such uncivil communication. Our work serves as the first study about the phenomenon of in(civility) in open source software development, paving the road for a new field of research about collaboration and communication in the context of software engineering activities."
A Method to Analyze Multiple Social Identities in Twitter Bios,"Pathak, Arjunil and Madani, Navid and Joseph, Kenneth",10.1145/3479502,2021,"Twitter users signal social identity in their profile descriptions, or bios, in a number of important but complex ways that are not well-captured by existing characterizations of how identity is expressed in language. Better ways of defining and measuring these expressions may therefore be useful both in understanding how social identity is expressed in text, and how the self is presented on Twitter. To this end, the present work makes three contributions. First, using qualitative methods, we identify and define the concept of a personal identifier, which is more representative of the ways in which identity is signaled in Twitter bios. Second, we propose a method to extract all personal identifiers expressed in a given bio. Finally, we present a series of validation analyses that explore the strengths and limitations of our proposed method. Our work opens up exciting new opportunities at the intersection between the social psychological study of social identity and the study of how we compose the self through markers of identity on Twitter and in social media more generally."
On the Congruence Between Online Social Content and Future IT Skill Demand,"Mahdavimoghaddam, Jalehsadat and Krishnaswamy, Niranjan and Bagheri, Ebrahim",10.1145/3479511,2021,"The speed of digital transformation has resulted in new challenges for job seekers to become lifelong learners and to develop new skills faster than before. In this paper, our main objective is to examine how online content can serve as indicators for changes to the Information Technology (IT) industry and its in-demand skills. To study this relationship, we collect Reddit posts to represent social media content and job postings to reflect the IT industry based on which we explore possible correlations between them. Further, we propose a methodology to quantitatively estimate the predictive power of social media content for future in-demand skills. Our results show that the frequency of skill-related conversations on Reddit correlates with the popularity of skills in job posting data. Additionally, our findings indicate that the number of social posts dedicated to a specific skill can be a strong indicator for future job requirements. This is an important finding because identifying what skills the labor force should acquire will assist job seekers to plan their lifelong learning objectives to (a) maximize their employability, (b) continuously update their skills to remain in demand, and (c) be informed and actively engaged in defining knowledge trends, rather than reactively becoming informed of the latest information."
On the State of Reporting in Crowdsourcing Experiments and a Checklist to Aid Current Practices,"Ram\'{\i}rez, Jorge and Sayin, Burcu and Baez, Marcos and Casati, Fabio and Cernuzzi, Luca and Benatallah, Boualem and Demartini, Gianluca",10.1145/3479531,2021,"Crowdsourcing is being increasingly adopted as a platform to run studies with human subjects. Running a crowdsourcing experiment involves several choices and strategies to successfully port an experimental design into an otherwise uncontrolled research environment, e.g., sampling crowd workers, mapping experimental conditions to micro-tasks, or ensure quality contributions. While several guidelines inform researchers in these choices, guidance of how and what to report from crowdsourcing experiments has been largely overlooked. If under-reported, implementation choices constitute variability sources that can affect the experiment's reproducibility and prevent a fair assessment of research outcomes. In this paper, we examine the current state of reporting of crowdsourcing experiments and offer guidance to address associated reporting issues. We start by identifying sensible implementation choices, relying on existing literature and interviews with experts, to then extensively analyze the reporting of 171 crowdsourcing experiments. Informed by this process, we propose a checklist for reporting crowdsourcing experiments."
Enabling Collaborative Data Science Development with the Ballet Framework,"Smith, Micah J. and Cito, J\""{u}rgen and Lu, Kelvin and Veeramachaneni, Kalyan",10.1145/3479575,2021,"While the open-source software development model has led to successful large-scale collaborations in building software systems, data science projects are frequently developed by individuals or small teams. We describe challenges to scaling data science collaborations and present a conceptual framework and ML programming model to address them. We instantiate these ideas in Ballet, the first lightweight framework for collaborative, open-source data science through a focus on feature engineering, and an accompanying cloud-based development environment. Using our framework, collaborators incrementally propose feature definitions to a repository which are each subjected to software and ML performance validation and can be automatically merged into an executable feature engineering pipeline. We leverage Ballet to conduct a case study analysis of an income prediction problem with 27 collaborators, and discuss implications for future designers of collaborative projects."
Designing Technologies to Support Parent-Child Relationships: A Review of Current Findings and Suggestions for Future Directions,"Shin, Ji Youn and Rheu, Minjin and Huh-Yoo, Jina and Peng, Wei",10.1145/3479585,2021,"Diverse fields, including CSCW, Communication, and Human Development studies, have investigated how technologies can better support parent-child relationships. While these studies are scattered across literature, little effort has been made to synthesize the findings. We conducted a review of studies that examined the factors associated with parent-child relationships that are mediated by technologies. Specifically, we synthesized previous studies based on children's age groups and different family contexts, including cohabitation. From a total of 12,942 search results from two databases, and 32 results from the hand-searching process, we conducted a full-text review of 190 articles and identified 19 suitable studies. An additional search during the revision cycle resulted in 6 more full-text reviews and 1 additional study being included in the data analysis. We analyzed challenges and facilitators in designing CSCW systems supporting parent-child relationships for families living together or apart and families with children of different developmental stages. Findings showed two common challenges, which should be addressed in technology designed to support parent-child relationships: discrepancies in expected communication between parents and child(ren) and the complex emotions of parents toward parenting caused by their busy schedules. Challenges specific to families who are living apart included consequences from being physically distant and having limited access to communication resources. The following factors commonly helped facilitate parent-child relationships: (1) reciprocity norms of the family (2) reinforcement of transparency, affection, and trust, (3) a physical proxy of each other through an object or interface design, (4) accessibility, the sophistication level of technology, and communication resources, (5) enjoyable, age-appropriate shared content among parents and children, and (6) situational awareness and routine as ways to increase parent-child relationships. Media richness and synchronicity in system design and privacy preservation without interruption facilitated parent-child relationships of families living apart. Based on the findings, we discuss opportunities for technological innovation for physically co-located families and the importance of considering children's age and developmental stages in designing technology for parent-child relationships."
The Philosopher's Corner: The Role of Theory in Information Systems Research,"Fink, Lior",10.1145/3481629.3481636,2021,"Theory has become critical in information systems (IS) research. Generally, empirical papers cannot be accepted for publication in IS journals unless the hypotheses or findings are grounded in theory. In this paper, I argue that IS has taken the role of theory too far, up to a point in which insisting on theory to explain observation may be costly to the advancement of IS research. It is suggested that the logic of discovery in scientific inquiry has been completely abandoned in favor of a logic of justification; that important accidental discoveries demonstrate the merit of discovery-oriented research; that the recent emergence of data science amplifies the value of discovery to theory development; and that the increasing length of empirical articles, partially as a consequence of the increasing importance of theory, may shift the limited attention of the community away from the phenomena of interest. Finally, a possible remedy for the overreliance on theory is offered to allow the IS community to open a door to discovery-oriented research that may yield long-term benefits and even strengthen its theoretical foundations."
A Survey of AIOps Methods for Failure Management,"Notaro, Paolo and Cardoso, Jorge and Gerndt, Michael",10.1145/3483424,2021,"Modern society is increasingly moving toward complex and distributed computing systems. The increase in scale and complexity of these systems challenges O&amp;M teams that perform daily monitoring and repair operations, in contrast with the increasing demand for reliability and scalability of modern applications. For this reason, the study of automated and intelligent monitoring systems has recently sparked much interest across applied IT industry and academia. Artificial Intelligence for IT Operations (AIOps) has been proposed to tackle modern IT administration challenges thanks to Machine Learning, AI, and Big Data. However, AIOps as a research topic is still largely unstructured and unexplored, due to missing conventions in categorizing contributions for their data requirements, target goals, and components. In this work, we focus on AIOps for Failure Management (FM), characterizing and describing 5 different categories and 14 subcategories of contributions, based on their time intervention window and the target problem being solved. We review 100 FM solutions, focusing on applicability requirements and the quantitative results achieved, to facilitate an effective application of AIOps solutions. Finally, we discuss current development problems in the areas covered by AIOps and delineate possible future trends for AI-based failure management."
Neural Network–Based Financial Volatility Forecasting: A Systematic Review,"Ge, Wenbo and Lalbakhsh, Pooia and Isai, Leigh and Lenskiy, Artem and Suominen, Hanna",10.1145/3483596,2022,"Volatility forecasting is an important aspect of finance as it dictates many decisions of market players. A snapshot of state-of-the-art neural network–based financial volatility forecasting was generated by examining 35 studies, published after 2015. Several issues were identified, such as the inability for easy and meaningful comparisons, and the large gap between modern machine learning models and those applied to volatility forecasting. A shared task was proposed to evaluate state-of-the-art models, and several promising ways to bridge the gap were suggested. Finally, adequate background was provided to serve as an introduction to the field of neural network volatility forecasting."
Solver-based gradual type migration,"Phipps-Costin, Luna and Anderson, Carolyn Jane and Greenberg, Michael and Guha, Arjun",10.1145/3485488,2021,"Gradually typed languages allow programmers to mix statically and dynamically typed code, enabling them to incrementally reap the benefits of static typing as they add type annotations to their code. However, this type migration process is typically a manual effort with limited tool support. This paper examines the problem of automated type migration: given a dynamic program, infer additional or improved type annotations. Existing type migration algorithms prioritize different goals, such as maximizing type precision, maintaining compatibility with unmigrated code, and preserving the semantics of the original program. We argue that the type migration problem involves fundamental compromises: optimizing for a single goal often comes at the expense of others. Ideally, a type migration tool would flexibly accommodate a range of user priorities. We present TypeWhich, a new approach to automated type migration for the gradually-typed lambda calculus with some extensions. Unlike prior work, which relies on custom solvers, TypeWhich produces constraints for an off-the-shelf MaxSMT solver. This allows us to easily express objectives, such as minimizing the number of necessary syntactic coercions, and constraining the type of the migration to be compatible with unmigrated code. We present the first comprehensive evaluation of GTLC type migration algorithms, and compare TypeWhich to four other tools from the literature. Our evaluation uses prior benchmarks, and a new set of ""challenge problems."" Moreover, we design a new evaluation methodology that highlights the subtleties of gradual type migration. In addition, we apply TypeWhich to a suite of benchmarks for Grift, a programming language based on the GTLC. TypeWhich is able to reconstruct all human-written annotations on all but one program."
OSS Effort Estimation Using Software Features Similarity and Developer Activity-Based Metrics,"Kapur, Ritu and Sodhi, Balwinder",10.1145/3485819,2022,"Software development effort estimation (SDEE) generally involves leveraging the information about the effort spent in developing similar software in the past. Most organizations do not have access to sufficient and reliable forms of such data from past projects. As such, the existing SDEE methods suffer from low usage and accuracy.We propose an efficient SDEE method for open source software, which provides accurate and fast effort estimates. The significant contributions of our article are (i) novel SDEE software metrics derived from developer activity information of various software repositories, (ii) an SDEE dataset comprising the SDEE metrics’ values derived from approximately 13,000 GitHub repositories from 150 different software categories, and (iii) an effort estimation tool based on SDEE metrics and a software description similarity model. Our software description similarity model is basically a machine learning model trained using the PVA on the software product descriptions of GitHub repositories. Given the software description of a newly envisioned software, our tool yields an effort estimate for developing it.Our method achieves the highest standardized accuracy score of 87.26% (with Cliff’s δ = 0.88 at 99.999% confidence level) and 42.7% with the automatically transformed linear baseline model. Our software artifacts are available at https://doi.org/10.5281/zenodo.5095723."
Software Engineering for AI-Based Systems: A Survey,"Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Bogner, Justus and Franch, Xavier and Oriol, Marc and Siebert, Julien and Trendowicz, Adam and Vollmer, Anna Maria and Wagner, Stefan",10.1145/3487043,2022,"AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula."
"Vulnerability Exposure Driven Intelligence in Smart, Circular Cities","Jarvis, Paul-David and Damianou, Amalia and Ciobanu, Cosmin and Katos, Vasilis",10.1145/3487059,2022,"In this article, we study the vulnerability management dimension in smart city initiatives. As many cities across the globe invest a considerable amount of effort, resources and budget to modernise their infrastructure by deploying a series of technologies such as 5G, Software Defined Networks, and IoT, we conduct an empirical analysis of their current exposure to existing vulnerabilities. We use an updated vulnerability dataset that is further enriched by quantitative research data from independent studies evaluating the maturity and accomplishments of cities in their journey to become smart. We particularly focus on cities that aspire to implement a (data-driven) Circular Economy agenda that we consider to potentially yield the highest risk from a vulnerabilities exposure perspective. Findings show that although a smarter city is attributed with a higher vulnerability exposure, investments on technology and human capital moderate this exposure in a way that it can be reduced."
A Systematic Review on Osmotic Computing,"Neha, Benazir and Panda, Sanjaya Kumar and Sahu, Pradip Kumar and Sahoo, Kshira Sagar and Gandomi, Amir H.",10.1145/3488247,2022,"Osmotic computing in association with related computing paradigms (cloud, fog, and edge) emerges as a promising solution for handling bulk of security-critical as well as latency-sensitive data generated by the digital devices. It is a growing research domain that studies deployment, migration, and optimization of applications in the form of microservices across cloud/edge infrastructure. It presents dynamically tailored microservices in technology-centric environments by exploiting edge and cloud platforms. Osmotic computing promotes digital transformation and furnishes benefits to transportation, smart cities, education, and healthcare. In this article, we present a comprehensive analysis of osmotic computing through a systematic literature review approach. To ensure high-quality review, we conduct an advanced search on numerous digital libraries to extracting related studies. The advanced search strategy identifies 99 studies, from which 29 relevant studies are selected for a thorough review. We present a summary of applications in osmotic computing build on their key features. On the basis of the observations, we outline the research challenges for the applications in this research field. Finally, we discuss the security issues resolved and unresolved in osmotic computing."
Hierarchical Scheduling of an SDF/L Graph onto Multiple Processors,"Oldja, Mari-Liis and Kim, Jangryul and Jeong, Dowhan and Ha, Soonhoi",10.1145/3489469,2021,"Although dataflow models are known to thrive at exploiting task-level parallelism of an application, it is difficult to exploit the parallelism of data, represented well with loop structures, since these structures are not explicitly specified in existing dataflow models. SDF/L model overcomes this shortcoming by specifying the loop structures explicitly in a hierarchical fashion. We introduce a scheduling technique of an application represented by the SDF/L model onto heterogeneous processors. In the proposed method, we explore the mapping of tasks using an evolutionary meta-heuristic and schedule hierarchically in a bottom-up fashion, creating parallel loop schedules at lower levels first and then re-using them when constructing the schedule at a higher level. The efficiency of the proposed scheduling methodology is verified with benchmark examples and randomly generated SDF/L graphs."
Personalized and Explainable Employee Training Course Recommendations: A Bayesian Variational Approach,"Wang, Chao and Zhu, Hengshu and Wang, Peng and Zhu, Chen and Zhang, Xi and Chen, Enhong and Xiong, Hui",10.1145/3490476,2021,"As a major component of strategic talent management, learning and development (L&amp;D) aims at improving the individual and organization performances through planning tailored training for employees to increase and improve their skills and knowledge. While many companies have developed the learning management systems (LMSs) for facilitating the online training of employees, a long-standing important issue is how to achieve personalized training recommendations with the consideration of their needs for future career development. To this end, in this article, we present a focused study on the explainable personalized online course recommender system for enhancing employee training and development. Specifically, we first propose a novel end-to-end hierarchical framework, namely Demand-aware Collaborative Bayesian Variational Network (DCBVN), to jointly model both the employees’ current competencies and their career development preferences in an explainable way. In DCBVN, we first extract the latent interpretable representations of the employees’ competencies from their skill profiles with autoencoding variational inference based topic modeling. Then, we develop an effective demand recognition mechanism for learning the personal demands of career development for employees. In particular, all the above processes are integrated into a unified Bayesian inference view for obtaining both accurate and explainable recommendations. Furthermore, for handling the employees with sparse or missing skill profiles, we develop an improved version of DCBVN, called the Demand-aware Collaborative Competency Attentive Network (DCCAN) framework, by considering the connectivity among employees. In DCCAN, we first build two employee competency graphs from learning and working aspects. Then, we design a graph-attentive network and a multi-head integration mechanism to infer one’s competency information from her neighborhood employees. Finally, we can generate explainable recommendation results based on the competency representations. Extensive experimental results on real-world data clearly demonstrate the effectiveness and the interpretability of both of our frameworks, as well as their robustness on sparse and cold-start scenarios."
A Comprehensive Report on Machine Learning-based Early Detection of Alzheimer's Disease using Multi-modal Neuroimaging Data,"Sharma, Shallu and Mandal, Pravat Kumar",10.1145/3492865,2022,"Alzheimer's Disease (AD) is a devastating neurodegenerative brain disorder with no cure. An early identification helps patients with AD sustain a normal living. We have outlined machine learning (ML) methodologies with different schemes of feature extraction to synergize complementary and correlated characteristics of data acquired from multiple modalities of neuroimaging. A variety of feature selection, scaling, and fusion methodologies along with confronted challenges are elaborated for designing an ML-based AD diagnosis system. Additionally, thematic analysis has been provided to compare the ML workflow for possible diagnostic solutions. This comprehensive report adds value to the further advancement of computer-aided early diagnosis system based on multi-modal neuroimaging data from patients with AD."
Do Developers Really Know How to Use Git Commands? A Large-scale Study Using Stack Overflow,"Yang, Wenhua and Zhang, Chong and Pan, Minxue and Xu, Chang and Zhou, Yu and Huang, Zhiqiu",10.1145/3494518,2022,"Git, a cross-platform and open source distributed version control tool, provides strong support for non-linear development and is capable of handling everything from small to large projects with speed and efficiency. It has become an indispensable tool for millions of software developers and is the de facto standard of version control in software development nowadays. However, despite its widespread use, developers still frequently face difficulties when using various Git commands to manage projects and collaborate. To better help developers use Git, it is necessary to understand the issues and difficulties that they may encounter when using Git. Unfortunately, this problem has not yet been comprehensively studied. To fill this knowledge gap, in this article, we conduct a large-scale study on Stack Overflow, a popular Q&amp;A forum for developers. We extracted and analyzed 80,370 relevant questions from Stack Overflow, and reported the increasing popularity of the Git command questions. By analyzing the questions, we identified the Git commands that are frequently asked and those that are associated with difficult questions on Stack Overflow to help understand the difficulties developers may encounter when using Git commands. In addition, we conducted a survey to understand how developers learn Git commands in practice, showing that self-learning is the primary learning approach. These findings provide a range of actionable implications for researchers, educators, and developers."
Industry–Academia Research Collaboration and Knowledge Co-creation: Patterns and Anti-patterns,"Marijan, Dusica and Sen, Sagar",10.1145/3494519,2022,"Increasing the impact of software engineering research in the software industry and the society at large has long been a concern of high priority for the software engineering community. The problem of two cultures, research conducted in a vacuum (disconnected from the real world), or misaligned time horizons are just some of the many complex challenges standing in the way of successful industry–academia collaborations. This article reports on the experience of research collaboration and knowledge co-creation between industry and academia in software engineering as a way to bridge the research–practice collaboration gap. Our experience spans 14 years of collaboration between researchers in software engineering and the European and Norwegian software and IT industry. Using the participant observation and interview methods, we have collected and afterwards analyzed an extensive record of qualitative data. Drawing upon the findings made and the experience gained, we provide a set of 14 patterns and 14 anti-patterns for industry–academia collaborations, aimed to support other researchers and practitioners in establishing and running research collaboration projects in software engineering."
Designing PairBuddy—A Conversational Agent for Pair Programming,"Robe, Peter and Kuttal, Sandeep Kaur",10.1145/3498326,2022,"From automated customer support to virtual assistants, conversational agents have transformed everyday interactions, yet despite phenomenal progress, no agent exists for programming tasks. To understand the design space of such an agent, we prototyped PairBuddy—an interactive pair programming partner—based on research from conversational agents, software engineering, education, human-robot interactions, psychology, and artificial intelligence. We iterated PairBuddy’s design using a series of Wizard-of-Oz studies. Our pilot study of six programmers showed promising results and provided insights toward PairBuddy’s interface design. Our second study of 14 programmers was positively praised across all skill levels. PairBuddy’s active application of soft skills—adaptability, motivation, and social presence—as a navigator increased participants’ confidence and trust, while its technical skills—code contributions, just-in-time feedback, and creativity support—as a driver helped participants realize their own solutions. PairBuddy takes the first step towards an Alexa-like programming partner."
Report from the 3rd Int. Workshop on Education through Advanced Software Engineering and Artificial Intelligence (EASEAI '21),"Henry, Julie and Praphamontripong, Upsorn and Serban, Camelia and Vescan, Andreea",10.1145/3502771.350279,2022,"With the development and widespread of digital technologies, ev- eryday life has been profoundly transformed. The general public, as well as specialized audiences, have to face an ever-increasing amount of knowledge and learn new abilities. The EASEAI work- shop series addresses that challenge by looking at software en- gineering, education, and arti cial intelligence research elds to explore how they can be combined. Speci cally, this workshop brings together researchers, teachers, and practitioners who use advanced software engineering tools and arti cial intelligence tech- niques in the education eld and through a transgenerational and transdisciplinary range of students to discuss the current state of the art and practices, and establish new future directions. More information at https://easeai.github.io."
Accessibility in Software Practice: A Practitioner’s Perspective,"Bi, Tingting and Xia, Xin and Lo, David and Grundy, John and Zimmermann, Thomas and Ford, Denae",10.1145/3503508,2022,"Being able to access software in daily life is vital for everyone, and thus accessibility is a fundamental challenge for software development. However, given the number of accessibility issues reported by many users, e.g., in app reviews, it is not clear if accessibility is widely integrated into current software projects and how software projects address accessibility issues. In this article, we report a study of the critical challenges and benefits of incorporating accessibility into software development and design. We applied a mixed qualitative and quantitative approach for gathering data from 15 interviews and 365 survey respondents from 26 countries across five continents to understand how practitioners perceive accessibility development and design in practice. We got 44 statements grouped into eight topics on accessibility from practitioners’ viewpoints and different software development stages. Our statistical analysis reveals substantial gaps between groups, e.g., practitioners have Direct vs. Indirect accessibility relevant work experience when they reviewed the summarized statements. These gaps might hinder the quality of accessibility development and design, and we use our findings to establish a set of guidelines to help practitioners be aware of accessibility challenges and benefit factors. We suggest development teams put accessibility as a first-class consideration throughout the software development process, and we also propose some remedies to resolve the gaps between groups and to highlight key future research directions to incorporate accessibility into software design and development."
A Primer on the Conditional Mediation Analysis in PLS-SEM,"Cheah, Jun-Hwa and Nitzl, Christian and Rold\'{a}n, Jos\'{e} L. and Cepeda-Carrion, Gabriel and Gudergan, Siegfried P.",10.1145/3505639.3505645,2022,"Conditional mediation (CoMe) analysis integrates mediation and moderation analyses to examine and test hypotheses about how mediated relationships vary as a function of context, boundaries, or individual differences. Although CoMe analysis can be a crucial element of empirical studies that seek to advance theory in information systems, applications of such analysis are scarce, in general, and in partial least squares structural equation modeling (PLS-SEM), in particular. This paper clarifies conceptual fundamentals of and develops guidelines for CoMe analysis within the PLS-SEM context. Furthermore, the paper outlines the illustrative use of CoMe analysis in PLS-SEM and presents detailed step-by-step procedures to do so in the PLS-SEM setting. Overall, this paper provides researchers and practitioners with the required knowledge to properly carry out, report, and interpret CoMe analysis in PLS-SEM."
Metaphoraction: Support Gesture-based Interaction Design with Metaphorical Meanings,"Sun, Zhida and Wang, Sitong and Liu, Chengzhong and Ma, Xiaojuan",10.1145/3511892,2022,"Previous user experience research emphasizes meaning in interaction design beyond conventional interactive gestures. However, existing exemplars that successfully reify abstract meanings through interactions are usually case-specific, and it is currently unclear how to systematically create or extend meanings for general gesture-based interactions. We present Metaphoraction, a creativity support tool that formulates design ideas for gesture-based interactions to show metaphorical meanings with four interconnected components: gesture, action, object, and meaning. To represent the interaction design ideas with these four components, Metaphoraction links interactive gestures to actions based on the similarity of appearances, movements, and experiences; relates actions to objects by applying the immediate association; bridges objects and meanings by leveraging the metaphor TARGET-SOURCE mappings. We build a dataset containing 588,770 unique design idea candidates through surveying related research and conducting two crowdsourced studies to support meaningful gesture-based interaction design ideation. Five design experts validate that Metaphoraction can effectively support creativity and productivity during the ideation process. The paper concludes by presenting insights into meaningful gesture-based interaction design and discussing potential future uses of the tool."
Eliciting Best Practices for Collaboration with Computational Notebooks,"Quaranta, Luigi and Calefato, Fabio and Lanubile, Filippo",10.1145/3512934,2022,"Despite the widespread adoption of computational notebooks, little is known about best practices for their usage in collaborative contexts. In this paper, we fill this gap by eliciting a catalog of best practices for collaborative data science with computational notebooks. With this aim, we first look for best practices through a multivocal literature review. Then, we conduct interviews with professional data scientists to assess their awareness of these best practices. Finally, we assess the adoption of best practices through the analysis of 1,380 Jupyter notebooks retrieved from the Kaggle platform. Findings reveal that experts are mostly aware of the best practices and tend to adopt them in their daily work. Nonetheless, they do not consistently follow all the recommendations as, depending on specific contexts, some are deemed unfeasible or counterproductive due to the lack of proper tool support. As such, we envision the design of notebook solutions that allow data scientists not to have to prioritize exploration and rapid prototyping over writing code of quality."
Group Chat Ecology in Enterprise Instant Messaging: How Employees Collaborate Through Multi-User Chat Channels on Slack,"Wang, Dakuo and Wang, Haoyu and Yu, Mo and Ashktorab, Zahra and Tan, Ming",10.1145/3512941,2022,"Despite the long history of studying instant messaging usage, we know very little about how today's people participate in group chat channels and interact with others inside a real-world organization. In this short paper, we aim to update the existing knowledge on how group chat is used in the context of today's organizations. The knowledge is particularly important for the new norm of remote works under the COVID-19 pandemic. We have the privilege of collecting two valuable datasets: a total of 4,300 group chat channels in Slack from an R&amp;D department in a multinational IT company; and a total of 117 groups' performance data. Through qualitative coding of 100 randomly sampled group channels from the 4,300 channels dataset, we identified and reported 9 categories such as Project channels, IT-Support channels, and Event channels. We further defined a feature metric with 21 meta-features (and their derived features) without looking at the message content to depict the group communication style for these group chat channels, with which we successfully trained a machine learning model that can automatically classify a given group channel into one of the 9 categories. In addition to the descriptive data analysis, we illustrated how these communication metrics can be used to analyze team performance. We cross-referenced 117 project teams and their team-based Slack channels and identified 57 teams that appeared in both datasets, then we built a regression model to reveal the relationship between these group communication styles and the project team performance. This work contributes an updated empirical understanding of human-human communication practices within the enterprise setting, and suggests design opportunities for the future of human-AI communication experience."
Home-Life and Work Rhythm Diversity in Distributed Teamwork: A Study with Information Workers during the COVID-19 Pandemic,"Breideband, Thomas and Talkad Sukumar, Poorna and Mark, Gloria and Caruso, Megan and D'Mello, Sidney and Striegel, Aaron D.",10.1145/3512942,2022,"During the COVID-19 pandemic, millions of previously co-located information workers had to work from home, a trend expected to become much more commonplace in the future. We interviewed 53 information workers from 17 U.S. teams to understand how this unique extended work-from-home setting influenced teamwork and how they adapted to it. Using a grounded theory approach, we discovered that extended remote work highlighted diversity in team members' home-lives and daily work rhythms. Whereas these types of diversity played only marginal roles for teams in the co-located office, they had a more tangible impact in the work-from-home setting, from coordination delays and interruptions to conflicts related to workload fairness, miscommunication, and trust. Importantly, workers reported that their teams adapted to these challenges by setting explicit norms and standards for online communication and asynchronous collaboration and by promoting general social and situational awareness. We discuss computer-supported designs to help teams manage these latent diversities in an extended remote teamwork setting."
Interpersonal Conflicts During Code Review: Developers' Experience and Practices,"Wurzel Gon\c{c}alves, Pavl\'{\i}na and \c{C}alikli, G\""{u}l and Bacchelli, Alberto",10.1145/3512945,2022,"Code review consists of manual inspection, discussion, and judgment of source code by developers other than the code's author. Due to discussions around competing ideas and group decision-making processes, interpersonal conflicts during code reviews are expected. This study systematically investigates how developers perceive code review conflicts and addresses interpersonal conflicts during code reviews as a theoretical construct. Through the thematic analysis of interviews conducted with 22 developers, we confirm that conflicts during code reviews are commonplace, anticipated and seen as normal by developers. Even though conflicts do happen and carry a negative impact for the review, conflicts-if resolved constructively-can also create value and bring improvement. Moreover, the analysis provided insights on how strongly conflicts during code review and its context (i.e., code, developer, team, organization) are intertwined. Finally, there are aspects specific to code review conflicts that call for the research and application of customized conflict resolution and management techniques, some of which are discussed in this paper. Preprint: https://arxiv.org/abs/2201.05425 Data and material: https://doi.org/10.5281/zenodo.5848794"
The Invisible Labor of Access in Academic Writing Practices: A Case Analysis with Dyslexic Adults,"Wang, Emily Q. and Piper, Anne Marie",10.1145/3512967,2022,"Writing is the currency of academia. Although technology-mediated writing has been studied extensively in CSCW, we know little about how writing practices unfold with disabled people, such as dyslexic writers whose neurodivergence shapes how they process language. Our qualitative analysis reveals how dyslexic professionals simultaneously identify how editing tools break down on academic language; develop workarounds that re-appropriate other tools as language sources; cultivate ad-hoc collaborations to compensate for technology's limitations; and navigate culturally ingrained ableist expectations for writing. We discuss how dyslexic writers' experiences with shouldering invisible work to participate in academic writing processes indicates that current tools and services do not support their needs. We then draw on our findings to inform design opportunities to make writing processes more accessible through changes to writing tools, institutional services, and peer review practices."
"User Experience Research in the Work Context: Maps, Gaps and Agenda","Simsek Caglar, Pinar and Roto, Virpi and Vainio, Teija",10.1145/3512979,2022,"User experience (UX) research has been criticized for focusing on leisure contexts and overlooking the work contexts. Moreover, researchers have been drawing attention to how UX at work differs from UX in leisure contexts, requiring development of domain specific knowledge. Inspired by these discussions, our motivation is to see the current state of UX at work research. Based on the systematic review of 52 papers, our results revealed that UX at work research is still immature. It is run by dichotomies; the conceptualizations such as the definition and the dimensions of UX at work are ambiguous; the variety of the studied work domains are limited and do not contribute to the understanding of UX at work; and the consideration of contextual factors are arbitrary and coincidental. To advance this important field of research, we indicate the research gaps and suggest a research agenda introducing areas for future research."
Knowledge Accumulation in Design Science Research: Ways to Foster Scientific Progress,"Reining, Stefan and Ahlemann, Frederik and Mueller, Benjamin and Thakurta, Rahul",10.1145/3514097.3514100,2022,"Design science research (DSR) is increasingly prominent in the information systems discipline. However, despite a lively discourse, and methodical and conceptual advances notwithstanding, DSR contributions are rarely cumulative and mostly remain isolated. This lack of cumulative research impedes scientific progress and practical impact. Our goal in this paper is to help establish a cumulative tradition in DSR by proposing (1) a general, consensus-oriented, evolutionary process model of scientific progress based on a synthesis of different streams in the philosophy of science, (2) quality criteria for DSR contributions corresponding to the individual steps in the process model, (3) actions that the IS community as a whole can take to facilitate adherence to these quality criteria. We expect that our work can pave the way towards developing a more cumulative research tradition in DSR and thereby foster genuine scientific progress in the field."
"An Agile New Research Framework for Hybrid Human-AI Teaming: Trust, Transparency, and Transferability","Caldwell, Sabrina and Sweetser, Penny and O’Donnell, Nicholas and Knight, Matthew J. and Aitchison, Matthew and Gedeon, Tom and Johnson, Daniel and Brereton, Margot and Gallagher, Marcus and Conroy, David",10.1145/3514257,2022,"We propose a new research framework by which the nascent discipline of human-AI teaming can be explored within experimental environments in preparation for transferal to real-world contexts. We examine the existing literature and unanswered research questions through the lens of an Agile approach to construct our proposed framework. Our framework aims to provide a structure for understanding the macro features of this research landscape, supporting holistic research into the acceptability of human-AI teaming to human team members and the affordances of AI team members. The framework has the potential to enhance decision-making and performance of hybrid human-AI teams. Further, our framework proposes the application of Agile methodology for research management and knowledge discovery. We propose a transferability pathway for hybrid teaming to be initially tested in a safe environment, such as a real-time strategy video game, with elements of lessons learned that can be transferred to real-world situations."
Automated Analysis of Blood Smear Images for Leukemia Detection: A Comprehensive Review,"Mittal, Ajay and Dhalla, Sabrina and Gupta, Savita and Gupta, Aastha",10.1145/3514495,2022,"Leukemia, the malignancy of blood-forming tissues, becomes fatal if not detected in the early stages. It is detected through a blood smear test that involves the morphological analysis of the stained blood slide. The manual microscopic examination of slides is tedious, time-consuming, error-prone, and subject to inter-observer and intra-observer bias. Several computerized methods to automate this task have been developed to alleviate these problems during the past few years. However, no exclusive comprehensive review of these methods has been presented to date. Such a review shall be highly beneficial for novice readers interested in pursuing research in this domain. This article fills the void by presenting a comprehensive review of 149 papers detailing the methods used to analyze blood smear images and detect leukemia. The primary focus of the review is on presenting the underlying techniques used and their reported performance, along with their merits and demerits. It also enumerates the research issues that have been satisfactorily solved and open challenges still existing in the domain."
Revisiting Reflection in HCI: Four Design Resources for Technologies that Support Reflection,"Bentvelzen, Marit and Wo\'{z}niak, Pawe\l{} W. and Herbes, Pia S.F. and Stefanidi, Evropi and Niess, Jasmin",10.1145/3517233,2022,"Reflection is a commonly addressed design goal in commercial systems and in Human-Computer Interaction (HCI) research. Yet, it is still unclear what tools are at the disposal of designers who want to build systems that support reflection. Understanding the design space of reflection support systems and the interaction techniques that can foster reflection is necessary to enable building technologies that contribute to the users' well-being. In order to gain additional insight into how interactive artefacts foster reflection, we investigated past research prototypes and reflection-supporting smartphone applications (apps). Through a structured literature review and an analysis of app reviews, we constructed four design resources for reflection: temporal perspective, conversation, comparison and discovery. We also identified design patterns in past digital artefacts that implement the resources. Our work constitutes intermediate-level knowledge that is intended to inspire future technologies that better support reflection."
A Review on Source Code Documentation,"Rai, Sawan and Belwal, Ramesh Chandra and Gupta, Atul",10.1145/3519312,2022,Context: Coding is an incremental activity where a developer may need to understand a code before making suitable changes in the code. Code documentation is considered one of the best practices in software development but requires significant efforts from developers. Recent advances in natural language processing and machine learning have provided enough motivation to devise automated approaches for source code documentation at multiple levels.Objective: The review aims to study current code documentation practices and analyze the existing literature to provide a perspective on their preparedness to address the stated problem and the challenges that lie ahead.Methodology: We provide a detailed account of the literature in the area of automated source code documentation at different levels and critically analyze the effectiveness of the proposed approaches. This also allows us to infer gaps and challenges to address the problem at different levels.Findings: (1) The research community focused on method-level summarization. (2) Deep learning has dominated the past five years of this research field. (3) Researchers are regularly proposing bigger corpora for source code documentation. (4) Java and Python are the widely used programming languages as corpus. (5) Bilingual Evaluation Understudy is the most favored evaluation metric for the research persons.
Defining a Knowledge Graph Development Process Through a Systematic Review,"Tama\v{s}auskaitundefined, Gytundefined and Groth, Paul",10.1145/3522586,2023,"Knowledge graphs are widely used in industry and studied within the academic community. However, the models applied in the development of knowledge graphs vary. Analysing and providing a synthesis of the commonly used approaches to knowledge graph development would provide researchers and practitioners a better understanding of the overall process and methods involved. Hence, this article aims at defining the overall process of knowledge graph development and its key constituent steps. For this purpose, a systematic review and a conceptual analysis of the literature was conducted. The resulting process was compared to case studies to evaluate its applicability. The proposed process suggests a unified approach and provides guidance for both researchers and practitioners when constructing and managing knowledge graphs."
Multi-document Summarization via Deep Learning Techniques: A Survey,"Ma, Congbo and Zhang, Wei Emma and Guo, Mingyu and Wang, Hu and Sheng, Quan Z.",10.1145/3529754,2022,"Multi-document summarization (MDS) is an effective tool for information aggregation that generates an informative and concise summary from a cluster of topic-related documents. Our survey, the first of its kind, systematically overviews the recent deep-learning-based MDS models. We propose a novel taxonomy to summarize the design strategies of neural networks and conduct a comprehensive summary of the state of the art. We highlight the differences between various objective functions that are rarely discussed in the existing literature. Finally, we propose several future directions pertaining to this new and exciting field."
A Survey on Data-driven Software Vulnerability Assessment and Prioritization,"Le, Triet H. M. and Chen, Huaming and Babar, M. Ali",10.1145/3529757,2022,"Software Vulnerabilities (SVs) are increasing in complexity and scale, posing great security risks to many software systems. Given the limited resources in practice, SV assessment and prioritization help practitioners devise optimal SV mitigation plans based on various SV characteristics. The surges in SV data sources and data-driven techniques such as Machine Learning and Deep Learning have taken SV assessment and prioritization to the next level. Our survey provides a taxonomy of the past research efforts and highlights the best practices for data-driven SV assessment and prioritization. We also discuss the current limitations and propose potential solutions to address such issues."
A Systematic Survey on Android API Usage for Data-driven Analytics with Smartphones,"Lee, Hansoo and Park, Joonyoung and Lee, Uichin",10.1145/3530814,2022,"Recent industrial and academic research has focused on data-driven analytics with smartphones by collecting user interaction, context, and device systems data through Application Programming Interfaces (APIs) and sensors. The Android operating system provides various APIs to collect such mobile usage and sensor data for third-party developers. Usage Statistics API (US API) and Accessibility Service API (AS API) are representative Android APIs for collecting app usage data and are used for various research purposes, as they can collect fine-grained interaction data (e.g., app usage history and user interaction type). Furthermore, other sensor APIs help to collect a user’s context and device state data, along with AS/US APIs. This review investigates mobile usage and sensor data-driven research using AS/US APIs by categorizing the research purposes and the data types. In this article, the surveyed studies are classified as follows: five themes and 21 subthemes and a four-layer hierarchical data classification structure. This allows us to identify a data usage trend and derive insight into data collection according to research purposes. Several limitations and future research directions of mobile usage and sensor data-driven analytics research are discussed, including the impact of changes in the Android API versions on research, the privacy and data quality issues, and the mitigation of reproducibility risks with standardized data typology."
Generating Quality Threat Intelligence Leveraging OSINT and a Cyber Threat Unified Taxonomy,"Martins, Cl\'{a}udio and Medeiros, Ib\'{e}ria",10.1145/3530977,2022,"Today’s threats use multiple means of propagation, such as social engineering, email, and application vulnerabilities, and often operate in different phases, such as single device compromise, lateral network movement, and data exfiltration. These complex threats rely on advanced persistent threats supported by well-advanced tactics for appearing unknown to traditional security defenses. As organizations realize that attacks are increasing in size and complexity, cyber threat intelligence (TI) is growing in popularity and use. This trend followed the evolution of advanced persistent threats, as they require a different level of response that is more specific to the organization. TI can be obtained via many formats, with open-source intelligence one of the most common, and using threat intelligence platforms (TIPs) that aid organizations to consume, produce, and share TI. TIPs have multiple advantages that enable organizations to quickly bootstrap the core processes of collecting, analyzing, and sharing threat-related information. However, current TIPs have some limitations that prevent their mass adoption. This article proposes AECCP, a platform that addresses some of the TIPs limitations. AECCP improves quality TI by classifying it accordingly a single unified taxonomy, removing the information with low value, enriching it with valuable information from open-source intelligence sources, and aggregating it for complementing information associated with the same threat. AECCP was validated and evaluated with three datasets of events and compared with two other platforms, showing that it can generate quality TI automatically and help security analysts analyze security incidents in less time."
Time Series Prediction Using Deep Learning Methods in Healthcare,"Morid, Mohammad Amin and Sheng, Olivia R. Liu and Dunbar, Joseph",10.1145/3531326,2023,"Traditional machine learning methods face unique challenges when applied to healthcare predictive analytics. The high-dimensional nature of healthcare data necessitates labor-intensive and time-consuming processes when selecting an appropriate set of features for each new task. Furthermore, machine learning methods depend heavily on feature engineering to capture the sequential nature of patient data, oftentimes failing to adequately leverage the temporal patterns of medical events and their dependencies. In contrast, recent deep learning (DL) methods have shown promising performance for various healthcare prediction tasks by specifically addressing the high-dimensional and temporal challenges of medical data. DL techniques excel at learning useful representations of medical concepts and patient clinical data as well as their nonlinear interactions from high-dimensional raw or minimally processed healthcare data.In this article, we systematically reviewed research works that focused on advancing deep neural networks to leverage patient structured time series data for healthcare prediction tasks. To identify relevant studies, we searched MEDLINE, IEEE, Scopus, and ACM Digital Library for relevant publications through November 4, 2021. Overall, we found that researchers have contributed to deep time series prediction literature in 10 identifiable research streams: DL models, missing value handling, addressing temporal irregularity, patient representation, static data inclusion, attention mechanisms, interpretation, incorporation of medical ontologies, learning strategies, and scalability. This study summarizes research insights from these literature streams, identifies several critical research gaps, and suggests future research opportunities for DL applications using patient time series data."
US-Rule: Discovering Utility-driven Sequential Rules,"Huang, Gengsen and Gan, Wensheng and Weng, Jian and Yu, Philip S.",10.1145/3532613,2023,"Utility-driven mining is an important task in data science and has many applications in real life. High-utility sequential pattern mining (HUSPM) is one kind of utility-driven mining. It aims at discovering all sequential patterns with high utility. However, the existing algorithms of HUSPM can not provide a relatively accurate probability to deal with some scenarios for prediction or recommendation. High-utility sequential rule mining (HUSRM) is proposed to discover all sequential rules with high utility and high confidence. There is only one algorithm proposed for HUSRM, which is not efficient enough. In this article, we propose a faster algorithm called US-Rule, to efficiently mine high-utility sequential rules. It utilizes the rule estimated utility co-occurrence pruning strategy (REUCP) to avoid meaningless computations. Moreover, to improve its efficiency on dense and long sequence datasets, four tighter upper bounds (LEEU, REEU, LERSU, and RERSU) and corresponding pruning strategies (LEEUP, REEUP, LERSUP, and RERSUP) are designed. US-Rule also proposes the rule estimated utility recomputing pruning strategy (REURP) to deal with sparse datasets. Finally, a large number of experiments on different datasets compared to the state-of-the-art algorithm demonstrate that US-Rule can achieve better performance in terms of execution time, memory consumption, and scalability."
Systematic Review of Comparative Studies of the Impact of Realism in Immersive Virtual Experiences,"Gon\c{c}alves, Guilherme and Coelho, Hugo and Monteiro, Pedro and Melo, Miguel and Bessa, Maximino",10.1145/3533377,2022,"The adoption of immersive virtual experiences (IVEs) opened new research lines where the impact of realism is being studied, allowing developers to focus resources on realism factors proven to improve the user experience the most. We analyzed papers that compared different levels of realism and evaluated their impact on user experience. Exploratorily, we also synthesized the realism terms used by authors. From 1,300 initial documents, 79 met the eligibility criteria. Overall, most of the studies reported that higher realism has a positive impact on user experience. These data allow a better understanding of realism in IVEs, guiding future R&amp;D."
Challenges in Deploying Machine Learning: A Survey of Case Studies,"Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.",10.1145/3533378,2022,"In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries, and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow, we show that practitioners face issues at each stage of the deployment process. The goal of this article is to lay out a research agenda to explore approaches addressing these challenges."
A Survey on Empirical Security Analysis of Access-control Systems: A Real-world Perspective,"Parkinson, Simon and Khan, Saad",10.1145/3533703,2022,"There any many different access-control systems, yet a commonality is that they provide flexible mechanisms to enforce different access levels. Their importance in organisations to adequately restrict resources, coupled with their use in a dynamic environment, mandates the need to routinely perform policy analysis. The aim of performing analysis is often to identify potential problematic permissions, which have the potential to be exploited and could result in data theft and unintended modification. There is a vast body of published literature on analysing access-control systems, yet as performing analysis has a strong end-user motivation and is grounded in security challenges faced in real-world systems, it is important to understand how research is developing, what are the common themes of interest, and to identify key challenges that should be addressed in future work. To the best of the authors’ knowledge, no survey has been performed to gain an understanding of empirical access-control analysis, focussing on how techniques are evaluated and how they align to the needs of real-world analysis tasks. This article provides a systematic literature review, identifying and summarising key works. Key findings are identified and discussed as areas of future work."
A Design Space for Human Sensor and Actuator Focused In-Vehicle Interaction Based on a Systematic Literature Review,"Jansen, Pascal and Colley, Mark and Rukzio, Enrico",10.1145/3534617,2022,"Automotive user interfaces constantly change due to increasing automation, novel features, additional applications, and user demands. While in-vehicle interaction can utilize numerous promising modalities, no existing overview includes an extensive set of human sensors and actuators and interaction locations throughout the vehicle interior. We conducted a systematic literature review of 327 publications leading to a design space for in-vehicle interaction that outlines existing and lack of work regarding input and output modalities, locations, and multimodal interaction. To investigate user acceptance of possible modalities and locations inferred from existing work and gaps unveiled in our design space, we conducted an online study (N=48). The study revealed users' general acceptance of novel modalities (e.g., brain or thermal activity) and interaction with locations other than the front (e.g., seat or table). Our work helps practitioners evaluate key design decisions, exploit trends, and explore new areas in the domain of in-vehicle interaction."
Human Movement Datasets: An Interdisciplinary Scoping Review,"Olugbade, Temitayo and Bie\'{n}kiewicz, Marta and Barbareschi, Giulia and D’amato, Vincenzo and Oneto, Luca and Camurri, Antonio and Holloway, Catherine and Bj\""{o}rkman, M\r{a}rten and Keller, Peter and Clayton, Martin and Williams, Amanda C De C and Gold, Nicolas and Becchio, Cristina and Bardy, Beno\^{\i}t and Bianchi-Berthouze, Nadia",10.1145/3534970,2022,"Movement dataset reviews exist but are limited in coverage, both in terms of size and research discipline. While topic-specific reviews clearly have their merit, it is critical to have a comprehensive overview based on a systematic survey across disciplines. This enables higher visibility of datasets available to the research communities and can foster interdisciplinary collaborations. We present a catalogue of 704 open datasets described by 10 variables that can be valuable to researchers searching for secondary data: name and reference, creation purpose, data type, annotations, source, population groups, ordinal size of people captured simultaneously, URL, motion capture sensor, and funders. The catalogue is available in the supplementary materials. We provide an analysis of the datasets and further review them under the themes of human diversity, ecological validity, and data recorded. The resulting 12-dimension framework can guide researchers in planning the creation of open movement datasets. This work has been the interdisciplinary effort of researchers across affective computing, clinical psychology, disability innovation, ethnomusicology, human-computer interaction, machine learning, music cognition, music computing, and movement neuroscience."
NHS Number Open Source Software: Implications for Digital Health Regulation and Development,"Thimbleby, Harold",10.1145/3538382,2022,"A national example of open source digital healthcare is critiqued. The code for implementing numeric patient identifiers is surprisingly na\""{\i}ve and bug-ridden, despite patient identifiers being computationally trivial and a critical component of reliable healthcare. The issues raised are shown to be widespread, long term, and apparently unrecognized. Problems are traced back to inadequacies in the relevant standards, and, at every stage, regulation through to development, inadequate Software Engineering input. An important finding is that the relevant healthcare standards are inconsistent and written without sufficient rigor to be at all constructive for implementing digital systems. The widely recognized problems of interoperability may be traced back to diverse (and buggy) interpretations of vague standards."
"Social Network Analysis: A Survey on Measure, Structure, Language Information Analysis, Privacy, and Applications","Singh, Shashank Sheshar and Srivastava, Vishal and Kumar, Ajay and Tiwari, Shailendra and Singh, Dilbag and Lee, Heung-No",10.1145/3539732,2023,"The rapid growth in popularity of online social networks provides new opportunities in computer science, sociology, math, information studies, biology, business, and more. Social network analysis (SNA) is a paramount technique supporting understanding social relationships and networks. Accordingly, certain studies and reviews have been presented focusing on information dissemination, influence analysis, link prediction, and more. However, the ultimate aim is for social network background knowledge and analysis to solve real-world social network problems. SNA still has several research challenges in this context, including users’ privacy in online social networks. Inspired by these facts, we have presented a survey on social network analysis techniques, visualization, structure, privacy, and applications. This detailed study has started with the basics of network representation, structure, and measures. Our primary focus is on SNA applications with state-of-the-art techniques. We further provide a comparative analysis of recent developments on SNA problems in the sequel. The privacy preservation with SNA is also surveyed. In the end, research challenges and future directions are discussed to suggest to researchers a starting point for their research."
Prosocial Norm Emergence in Multi-agent Systems,"Mashayekhi, Mehdi and Ajmeri, Nirav and List, George F. and Singh, Munindar P.",10.1145/3540202,2022,"Multi-agent systems provide a basis for developing systems of autonomous entities and thus find application in a variety of domains. We consider a setting where not only the member agents are adaptive but also the multi-agent system viewed as an entity in its own right is adaptive. Specifically, the social structure of a multi-agent system can be reflected in the social norms among its members. It is well recognized that the norms that arise in society are not always beneficial to its members. We focus on prosocial norms, which help achieve positive outcomes for society and often provide guidance to agents to act in a manner that takes into account the welfare of others.Specifically, we propose Cha, a framework for the emergence of prosocial norms. Unlike previous norm emergence approaches, Cha supports continual change to a system (agents may enter and leave) and dynamism (norms may change when the environment changes). Importantly, Cha agents incorporate prosocial decision-making based on inequity aversion theory, reflecting an intuition of guilt arising from being antisocial. In this manner, Cha brings together two important themes in prosociality: decision-making by individuals and fairness of system-level outcomes. We demonstrate via simulation that Cha can improve aggregate societal gains and fairness of outcomes."
Aide-m\'{e}moire: Improving a Project’s Collective Memory via Pull Request–Issue Links,"P\^{a}r\c{t}achi, Profir-Petru and White, David R. and Barr, Earl T.",10.1145/3542937,2023,"Links between pull request and the issues they address document and accelerate the development of a software project but are often omitted. We present a new tool, Aide-m\'{e}moire, to suggest such links when a developer submits a pull request or closes an issue, smoothly integrating into existing workflows. In contrast to previous state-of-the-art approaches that repair related commit histories, Aide-m\'{e}moire is designed for continuous, real-time, and long-term use, employing Mondrian forest to adapt over a project’s lifetime and continuously improve traceability. Aide-m\'{e}moire is tailored for two specific instances of the general traceability problem—namely, commit to issue and pull request to issue links, with a focus on the latter—and exploits data inherent to these two problems to outperform tools for general purpose link recovery. Our approach is online, language-agnostic, and scalable. We evaluate over a corpus of 213 projects and six programming languages, achieving a mean average precision of 0.95. Adopting Aide-m\'{e}moire is both efficient and effective: A programmer need only evaluate a single suggested link 94% of the time, and 16% of all discovered links were originally missed by developers."
Nudge: Accelerating Overdue Pull Requests toward Completion,"Maddila, Chandra and Upadrasta, Sai Surya and Bansal, Chetan and Nagappan, Nachiappan and Gousios, Georgios and van Deursen, Arie",10.1145/3544791,2023,"Pull requests are a key part of the collaborative software development and code review process today. However, pull requests can also slow down the software development process when the reviewer(s) or the author do not actively engage with the pull request. In this work, we design an end-to-end service, Nudge, for accelerating overdue pull requests toward completion by reminding the author or the reviewer(s) to engage with their overdue pull requests. First, we use models based on effort estimation and machine learning to predict the completion time for a given pull request. Second, we use activity detection to filter out pull requests that may be overdue but for which sufficient action is taking place nonetheless. Last, we use actor identification to understand who the blocker of the pull request is and nudge the appropriate actor (author or reviewer(s)). The&nbsp;key novelty of Nudge is that it succeeds in reducing pull request resolution time, while ensuring that developers perceive the notifications sent as useful, at the scale of thousands of repositories. In a randomized trial on 147 repositories in use at Microsoft, Nudge was able to reduce pull request resolution time by 60% for 8,500 pull requests, when compared to overdue pull requests for which Nudge did not send a notification. Furthermore, developers receiving Nudge notifications resolved 73% of these notifications as positive. We&nbsp;observed similar results when scaling up the deployment of Nudge to 8,000 repositories at Microsoft, for which Nudge sent 210,000 notifications during a full year. This demonstrates Nudge’s ability to scale to thousands of repositories. Last, our qualitative analysis of a selection of Nudge notifications indicates areas for future research, such as taking dependencies among pull requests and developer availability into account."
A Systematic Evaluation of Solutions for the Final 100m Challenge of Highly Automated Vehicles,"Colley, Mark and Wankm\""{u}ller, Bastian and Rukzio, Enrico",10.1145/3546713,2022,"Automated vehicles will change the interaction with the user drastically. While freeing the user of the driving task for most of the journey, the ""final 100 meters problem'', directing the vehicle to the final parking spot, could require human intervention. Therefore, we present a classification of interaction concepts for automated vehicles based on modality and interaction mode. In a subsequent Virtual Reality study (N=16), we evaluated sixteen interaction concepts. We found that the medially abstracted interaction mode was consistently rated most usable over all modalities (joystick, speech, gaze, gesture, and tablet). While the steering wheel was still preferred, our findings indicate that other interaction concepts are usable if the steering wheel were unavailable."
Towards Implicit Interaction in Highly Automated Vehicles - A Systematic Literature Review,"Stampf, Annika and Colley, Mark and Rukzio, Enrico",10.1145/3546726,2022,"The inclusion of in-vehicle sensors and increased intention and state recognition capabilities enable implicit in-vehicle interaction. Starting from a systematic literature review (SLR) on implicit in-vehicle interaction, which resulted in 82 publications, we investigated state and intention recognition methods based on (1) their used modalities, (2) their underlying level of automation, and (3) their considered interaction focus. Our SLR revealed a research gap addressing implicit interaction in highly automated vehicles (HAVs). Therefore, we discussed how the requirements for implicit state and intention recognition methods and interaction based on them are changing in HAVs. With this, open questions and opportunities for further research in this area were identified."
A System for Automated Industrial Test Laboratory Scheduling,"Danzinger, Philipp and Geibinger, Tobias and Janneau, David and Mischek, Florian and Musliu, Nysret and Poschalko, Christian",10.1145/3546871,2023,"Automated scheduling solutions are tremendously important for the efficient operation of industrial laboratories. The Test Laboratory Scheduling Problem (TLSP) is an extension of the well-known Resource Constrained Project Scheduling Problem (RCPSP) and captures the specific requirements of such laboratories. In addition to several new scheduling constraints, it features a grouping phase, where the jobs to be scheduled are assembled from smaller units. In this work, we introduce an innovative scheduling system that allows the efficient and flexible generation of schedules for TLSP. It features a new Constraint Programming model that covers both the grouping and the scheduling aspect, as well as a hybrid Very Large Neighborhood Search that internally uses the CP model. Our experimental results on generated and real-world benchmark instances show that good results can be obtained even compared to settings which have a good grouping already provided, including several new best known solutions for these instances. Our algorithms for TLSP have been successfully implemented in a real-world industrial test laboratory. We provide a detailed description of the deployed system as well as additional useful soft constraints supported by the solvers and general lessons learned. This includes a discussion of the choice of soft constraint weights, with an analysis on the impact and relation of different objectives to each other. Our experiments show that some soft constraints complement each other well, while others require explicit trade-offs via their relative weights."
Eye-tracking Technologies in Mobile Devices Using Edge Computing: A Systematic Review,"Gunawardena, Nishan and Ginige, Jeewani Anupama and Javadi, Bahman",10.1145/3546938,2022,"Eye-tracking provides invaluable insight into the cognitive activities underlying a wide range of human behaviours. Identifying cognitive activities provides valuable perceptions of human learning patterns and signs of cognitive diseases like Alzheimer’s, Parkinson’s, and autism. Also, mobile devices have changed the way that we experience daily life and become a pervasive part. This systematic review provides a detailed analysis of mobile device eye-tracking technology reported in 36 studies published in high-ranked scientific journals from 2010 to 2020 (September), along with several reports from grey literature. The review provides in-depth analysis on algorithms, additional apparatus, calibration methods, computational systems, and metrics applied to measure the performance of the proposed solutions. Also, the review presents a comprehensive classification of mobile device eye-tracking applications used across various domains such as healthcare, education, road safety, news, and human authentication. We have outlined the shortcomings identified in the literature and the limitations of the current mobile device eye-tracking technologies, such as using the front-facing mobile camera. Further, we have proposed an edge computing driven eye-tracking solution to achieve the real-time eye-tracking experience. Based on the findings, the article outlines various research gaps and future opportunities that are expected to be of significant value for improving the work in the eye-tracking domain."
Suboptimal Comments in Java Projects: From Independent Comment Changes to Commenting Practices,"Wang, Chao and He, Hao and Pal, Uma and Marinov, Darko and Zhou, Minghui",10.1145/3546949,2023,"High-quality source code comments are valuable for software development and maintenance, however, code often contains low-quality comments or lacks them altogether. We name such source code comments as suboptimal comments. Such suboptimal comments create challenges in code comprehension and maintenance. Despite substantial research on low-quality source code comments, empirical knowledge about commenting practices that produce suboptimal comments and reasons that lead to suboptimal comments are lacking. We help bridge this knowledge gap by investigating (1)&nbsp;independent comment changes (ICCs)—comment changes committed independently of code changes—which likely address suboptimal comments, (2)&nbsp;commenting guidelines, and (3)&nbsp;comment-checking tools and comment-generating tools, which are often employed to help commenting practice—especially to prevent suboptimal comments. We collect 24M+ comment changes from 4,392 open-source GitHub Java repositories and find that ICCs widely exist. The ICC ratio—proportion of ICCs among all comment changes—is ~15.5%, with 98.7% of the repositories having ICC. Our thematic analysis of 3,533 randomly sampled ICCs provides a three-dimensional taxonomy for what is changed (four comment categories and 13 subcategories), how it changed (six commenting activity categories), and what factors are associated with the change (three factors). We investigate 600 repositories to understand the prevalence, content, impact, and violations of commenting guidelines. We find that only 15.5% of the 600 sampled repositories have any commenting guidelines. We provide the first taxonomy for elements in commenting guidelines: where and what to comment are particularly important. The repositories without such guidelines have a statistically significantly higher ICC ratio, indicating the negative impact of the lack of commenting guidelines. However, commenting guidelines are not strictly followed: 85.5% of checked repositories have violations. We also systematically study how developers use two kinds of tools, comment-checking tools and comment-generating tools, in the 4,392 repositories. We find that the use of Javadoc tool is negatively correlated with the ICC ratio, while the use of Checkstyle has no statistically significant correlation; the use of comment-generating tools leads to a higher ICC ratio. To conclude, we reveal issues and challenges in current commenting practice, which help understand how suboptimal comments are introduced. We propose potential research directions on comment location prediction, comment generation, and comment quality assessment; suggest how developers can formulate commenting guidelines and enforce rules with tools; and recommend how to enhance current comment-checking and comment-generating tools."
Quantum Software Components and Platforms: Overview and Quality Assessment,"Serrano, Manuel A. and Cruz-Lemus, Jos\'{e} A. and Perez-Castillo, Ricardo and Piattini, Mario",10.1145/3548679,2022,"Quantum computing is the latest revolution in computing and will probably come to be seen as an advance as important as the steam engine or the information society. In the last few decades, our understanding of quantum computers has expanded and multiple efforts have been made to create languages, libraries, tools, and environments to facilitate their programming. Nonetheless, quantum computers are complex systems at the bottom of a stack of layers that programmers need to understand. Hence, efforts towards creating quantum programming languages and computing environments that can abstract low-level technology details have become crucial steps to achieve a useful quantum computing technology. However, most of these environments still lack many of the features that would be desirable, such as those outlined in The Talavera Manifesto for Quantum Software Engineering and Programming. For advancing quantum computing, we will need to develop quantum software engineering techniques and tools to ensure the feasibility of this new type of quantum software. To contribute to this goal, this paper provides a review of the main quantum software components and platforms. We also propose a set of quality requirements for the development of quantum software platforms and the conduct of their quality assessment."
GPU Devices for Safety-Critical Systems: A Survey,"Perez-Cerrolaza, Jon and Abella, Jaume and Kosmidis, Leonidas and Calderon, Alejandro J. and Cazorla, Francisco and Flores, Jose Luis",10.1145/3549526,2022,"&nbsp;Graphics Processing Unit (GPU) devices and their associated software programming languages and frameworks can deliver the computing performance required to facilitate the development of next-generation high-performance safety-critical systems such as autonomous driving systems. However, the integration of complex, parallel, and computationally demanding software functions with different safety-criticality levels on GPU devices with shared hardware resources contributes to several safety certification challenges. This survey categorizes and provides an overview of research contributions that address GPU devices’ random hardware failures, systematic failures, and independence of execution."
ONP-Miner: One-off Negative Sequential Pattern Mining,"Wu, Youxi and Chen, Mingjie and Li, Yan and Liu, Jing and Li, Zhao and Li, Jinyan and Wu, Xindong",10.1145/3549940,2023,"Negative sequential pattern mining (SPM) is an important SPM research topic. Unlike positive SPM, negative SPM can discover events that should have occurred but have not occurred, and it can be used for financial risk management and fraud detection. However, existing methods generally ignore the repetitions of the pattern and do not consider gap constraints, which can lead to mining results containing a large number of patterns that users are not interested in. To solve this problem, this article discovers frequent one-off negative sequential patterns (ONPs). This problem has the following two characteristics. First, the support is calculated under the one-off condition, which means that any character in the sequence can only be used once at most. Second, the gap constraint can be given by the user. To efficiently mine patterns, this article proposes the ONP-Miner algorithm, which employs depth-first and backtracking strategies to calculate the support. Therefore, ONP-Miner can effectively avoid creating redundant nodes and parent-child relationships. Moreover, to effectively reduce the number of candidate patterns, ONP-Miner uses pattern join and pruning strategies to generate and further prune the candidate patterns, respectively. Experimental results show that ONP-Miner not only improves the mining efficiency but also has better mining performance than the state-of-the-art algorithms. More importantly, ONP mining can find more interesting patterns in traffic volume data to predict future traffic."
"Platform-mediated Markets, Online Freelance Workers and Deconstructed Identities","Munoz, Isabel and Dunn, Michael and Sawyer, Steve and Michaels, Emily",10.1145/3555092,2022,"We advance the concept of deconstructed identity to explain how online workers' identities are being reshaped, diminished and controlled by digital labor platforms. We focus on online freelance workers and contribute to contemporary conceptualizations regarding worker's self-presentation. The empirical basis for our analysis and theorizing build from two rounds of a longitudinal panel study of online freelance workers and their interactions with online labor platforms. Findings illuminate how online freelancer's identity presentation is constrained by the structuring of their profile, the ratings and client feedback, the algorithms used by the digital platform, and platform's terms of use. Data demonstrate that workers' profiles are focused on skills, reflecting the realities of competing for work in under-regulated labor markets. Study participants report the centrality of client and platform ratings of their work, and the need to manage client feedback and ratings as a core part of their online identity presentation. These findings suggest that, far from a subjective and personal story, a freelancer's identity on a digital labor platform is better understood as a standardized depiction of skills, ratings, and metrics controlled by platform algorithms. Coupled with use policies and evolving platform designs, this platform control creates what seems to be a form of indentured servitude. We further note online freelancers both recognize this control and resist their deconstructed identity."
Exploring the Utility of Social Content for Understanding Future In-Demand Skills,"Mahdavimoghaddam, Jalehsadat and Bahuguna, Ayush and Bagheri, Ebrahim",10.1145/3555114,2022,"Rapid technological innovations, especially in the information technology space, demand the workforce to be vigilant by acquiring new skills to remain relevant and employable. The workforce needs to be engaged in a continuous lifelong learning process by educating themselves about skills that will be in demand in the future. To do so, it is essential for students, job seekers, and even recruiters to know which skills will be in demand in the future and to invest time and resources in developing these skills. On this basis, the main objective of this paper is to investigate whether social content can offer insight into potential future in-demand skills in the IT job market. Based on the analysis of social content from Reddit and job posting data from Dice and Monster websites, we find that social content related to job skills is a strong indicator for future in-demand skills. We further find that specific social content associated with recruitment-related topics are stronger indicators of future skills. Our findings encourage learners and job seekers to pay close attention to online social content to strategically plan new skills and maximize their employability."
The Craft and Coordination of Data Curation: Complicating Workflow Views of Data Science,"Thomer, Andrea K. and Akmon, Dharma and York, Jeremy J. and Tyler, Allison R. B. and Polasek, Faye and Lafia, Sara and Hemphill, Libby and Yakel, Elizabeth",10.1145/3555139,2022,"Data curation is the process of making a dataset fit-for-use and archivable. It is critical to data-intensive science because it makes complex data pipelines possible, studies reproducible, and data reusable. Yet the complexities of the hands-on, technical, and intellectual work of data curation is frequently overlooked or downplayed. Obscuring the work of data curation not only renders the labor and contributions of data curators invisible but also hides the impact that curators' work has on the later usability, reliability, and reproducibility of data. To better understand the work and impact of data curation, we conducted a close examination of data curation at a large social science data repository, the Inter-university Consortium for Political and Social Research (ICPSR). We asked: What does curatorial work entail at ICPSR, and what work is more or less visible to different stakeholders and in different contexts? And, how is that curatorial work coordinated across the organization? We triangulated accounts of data curation from interviews and records of curation in Jira tickets to develop a rich and detailed account of curatorial work. While we identified numerous curatorial actions performed by ICPSR curators, we also found that curators rely on a number of craft practices to perform their jobs. The reality of their work practices defies the rote sequence of events implied by many life cycle or workflow models. Further, we show that craft practices are needed to enact data curation best practices and standards. The craft that goes into data curation is often invisible to end users, but it is well recognized by ICPSR curators and their supervisors. Explicitly acknowledging and supporting data curators as craftspeople is important in creating sustainable and successful curatorial infrastructures."
Human and Technological Infrastructures of Fact-checking,"Juneja, Prerna and Mitra, Tanushree",10.1145/3555143,2022,"Increasing demands for fact-checking have led to a growing interest in developing systems and tools to automate the fact-checking process. However, such systems are limited in practice because their system design often does not take into account how fact-checking is done in the real world and ignores the insights and needs of various stakeholder groups core to the fact-checking process. This paper unpacks the fact-checking process by revealing the infrastructures---both human and technological---that support and shape fact-checking work. We interviewed 26 participants belonging to 16 fact-checking teams and organizations with representation from 4 continents. Through these interviews, we describe the human infrastructure of fact-checking by identifying and presenting, in-depth, the roles of six primary stakeholder groups, 1) Editors, 2) External fact-checkers, 3) In-house fact-checkers, 4) Investigators and researchers, 5) Social media managers, and 6) Advocators. Our findings highlight that the fact-checking process is a collaborative effort among various stakeholder groups and associated technological and informational infrastructures. By rendering visibility to the infrastructures, we reveal how fact-checking has evolved to include both short-term claims centric and long-term advocacy centric fact-checking. Our work also identifies key social and technical needs and challenges faced by each stakeholder group. Based on our findings, we suggest that improving the quality of fact-checking requires systematic changes in the civic, informational, and technological contexts."
A Systematic Review of Interaction Design Strategies for Group Recommendation Systems,"Alvarado, Oscar and Htun, Nyi Nyi and Jin, Yucheng and Verbert, Katrien",10.1145/3555161,2022,"Systems involving artificial intelligence (AI) are protagonists in many everyday activities. Moreover, designers are increasingly implementing these systems for groups of users in various social and cooperative domains. Unfortunately, research on personalized recommendation systems often reports negative experiences due to a lack of diversity, control, or transparency. Providing a meta-analysis of the interaction design strategies for group recommendation systems (GRS) offers designers and practitioners a departure to address these issues and imagine new interaction possibilities for this context. Therefore, we systematically reviewed the ACM, IEEE, and Scopus digital libraries to identify GRS interface designs, resulting in a final corpus of 142 academic papers. After a systematic coding process, we used descriptive statistics and thematic analysis to uncover the current state of the art regarding interaction design strategies for GRS in six areas: (1) application domains; (2) devices chosen to implement the systems; (3) prototype fidelity; (4) strategies for profile transparency, justification, control, and diversity; (5) strategies for group formation and final group consensus; and, (6) evaluation methods applied in user studies during the design process. Based on our findings, we present an exhaustive typology of interaction design strategies for GRS and a set of research opportunities to foster human-centered interfaces for personalized recommendations in cooperative and social computing contexts."
Counting to be Counted: Anganwadi Workers and Digital Infrastructures of Ambivalent Care,"S P, Azhagu Meena and Vaghela, Palashi and Pal, Joyojeet",10.1145/3555177,2022,"Data collection on the population is a key mode of public health management in the Global South. This information is seen as a means to improve health metrics through welfare programs. In this study, we examine the changes brought about by an ICT-based Real-Time Monitoring System to the infrastructure of a welfare program and the nature of work of Anganwadi workers in India. Anganwadi workers, traditionally serving as daycare providers and community health workers, are increasingly being asked to serve primarily as data collectors for the new digital system. We ask the question 'cui bono?' to this system by drawing attention to the precarity of Anganwadi workers whose care-work is standardized through this app for 'efficient' monitoring by the Indian state but remains contingent on their relationship with the local community and ability to mobilize resources on the ground. Using auto-ethnographic and interview methods, we find that Anganwadi workers are caught between conflicting demands of state bureaucracy and the situated nature of their care work resulting in forms of ambivalent care. We find that the real-time monitoring apps intended to collect data for efficient delivery of state services end up serving the state's need for performing care through data rhetorics produced at the expense of the professional and personal well-being of the workers, and arguably the communities they serve."
Interrogating Data Work as a Community of Practice,"Rothschild, Annabel and Meng, Amanda and DiSalvo, Carl and Johnson, Britney and Shapiro, Ben Rydal and DiSalvo, Betsy",10.1145/3555198,2022,"We apply Lave &amp; Wenger's construct of a community of practice to identify and position members of the data work community of practice, focusing on members on the periphery who have received less attention - as compared to full practitioners (e.g., data scientists). Reporting on results of interviews with 19 civic workers who perform data work as their main task, we identify an atypical relationship between subject-domain experts (such as our interviewees) and full members of the data work community. Our interviewees may have less computational skill in data work, but they have extensive and varied practices to engage in data contextualization that data scientists and other full community members could learn from. In identifying the attributes of data workers on the periphery, we also hope to call attention to the challenges they face in performing data work in low resources institutions (e.g., governmental, non-profit). Our findings contribute to the larger conversations in human-centered data science about who performs data work and how they go about it, in order to addresses questions of power, fairness, and bias in data-intensive systems."
"Research Data Management Commitment Drivers: An Analysis of Practices, Training, Policies, Infrastructure, and Motivation in Global Agricultural Science","Feger, Sebastian S. and Pertiwi, Cininta and Bonaiuti, Enrico",10.1145/3555213,2022,"Scientists largely acknowledge the value of research data management (RDM) to enable reproducibility and reuse. But, RDM practices are not sufficiently rewarded within the traditional academic reputation economy. Recent work showed that emerging RDM tools can offer new incentives and rewards. But, the design of such platforms and scientists' commitment to RDM is contingent on additional factors, including policies, training, and several types of personal motivation. To date, studies focused on investigating single or few of those RDM components within a given environment. In contrast, we conducted three studies within a global agricultural science organization, to provide a more complete account of RDM commitment drivers: one survey study (n = 23) and two qualitative explorations of regulatory frameworks (n = 17), as well as motivation, infrastructure, and training components (n = 13). Based on the sum of findings, we contribute to the triangulation of a recent RDM commitment evolution model. In particular, we find that strong support and suitable tools help develop RDM commitment, while policy conflicts, unclear data standards, and multi-platform sharing, lead to unexpected negotiation processes. We expect that these findings will help to better understand RDM commitment drivers, refine the RDM commitment evolution model, and benefit its application in science."
ColorCook: Augmenting Color Design for Dashboarding with Domain-Associated Palettes,"Shi, Yang and Chen, Siji and Liu, Pei and Long, Jiang and Cao, Nan",10.1145/3555534,2022,"Visualization dashboards serve as an information presentation that uses a tiled layout of key metrics visualized in charts for collaborative decision-making. Existing work has developed tools and techniques for computational color design. Much of these efforts have focused on selecting effective color palettes for independent charts while few attempts have been made to support the expressive color design of multiple coordinated charts in dashboards. In this work, we describe ColorCook, an interactive system that helps design expressive and effective dashboard colorings using domain-associated palettes. ColorCook employs an integrated color workflow for dashboarding, consisting of color selection, assignment, and adjustment. We evaluated ColorCook through a crowdsourcing experiment and a user study. The results of our evaluation indicated that ColorCook is useful for effective and expressive color design."
Platform-Mediated Food Delivery Work: A Review for CSCW,"Kusk, Kalle and Nouwens, Midas",10.1145/3555645,2022,"CSCW holds a valuable set of notions and tools for future granular inquiry into the work practice on food delivery platforms and pertinent questions from the field of food delivery resonate with research agendas within CSCW. This argument is based on a synthetic literature review that identifies characteristics of food delivery work and connects them to previous scholarship within CSCW. With few exceptions, the food delivery workers in the papers included are typically young, male and of immigrant background. The work practice the papers describe typically relies on algorithmic management, flexible scheduling, pay-per-delivery, and while food delivery workers work solitarily, workers repeatedly organize in both formal and informal groups. Finally, the review shows that while these characteristics are common, the local environment where the work is done has a significant impact on the workers' practice, resulting in meaningful differences between places and platforms that should be considered. The discussion brings forth salient issues and outlines two alleys of future work."
Android Source Code Vulnerability Detection: A Systematic Literature Review,"Senanayake, Janaka and Kalutarage, Harsha and Al-Kadri, Mhd Omar and Petrovski, Andrei and Piras, Luca",10.1145/3556974,2023,"The use of mobile devices is rising daily in this technological era. A continuous and increasing number of mobile applications are constantly offered on mobile marketplaces to fulfil the needs of smartphone users. Many Android applications do not address the security aspects appropriately. This is often due to a lack of automated mechanisms to identify, test, and fix source code vulnerabilities at the early stages of design and development. Therefore, the need to fix such issues at the initial stages rather than providing updates and patches to the published applications is widely recognized. Researchers have proposed several methods to improve the security of applications by detecting source code vulnerabilities and malicious codes. This Systematic Literature Review (SLR) focuses on Android application analysis and source code vulnerability detection methods and tools by critically evaluating 118 carefully selected technical studies published between 2016 and 2022. It highlights the advantages, disadvantages, applicability of the proposed techniques, and potential improvements of those studies. Both Machine Learning (ML)-based methods and conventional methods related to vulnerability detection are discussed while focusing more on ML-based methods, since many recent studies conducted experiments with ML. Therefore, this article aims to enable researchers to acquire in-depth knowledge in secure mobile application development while minimizing the vulnerabilities by applying ML methods. Furthermore, researchers can use the discussions and findings of this SLR to identify potential future research and development directions."
A Survey of User Perspectives on Security and Privacy in a Home Networking Environment,"Pattnaik, Nandita and Li, Shujun and Nurse, Jason R. C.",10.1145/3558095,2023,"The security and privacy of smart home systems, particularly from a home user’s perspective, have been a very active research area in recent years. However, via a meta-review of 52 review papers covering related topics (published between 2000 and 2021), this article shows a lack of a more recent literature review on user perspectives of smart home security and privacy since the 2010s. This identified gap motivated us to conduct a systematic literature review (SLR) covering 126 relevant research papers published from 2010 to 2021. Our SLR led to the discovery of a number of important areas where further research is needed; these include holistic methods that consider a more diverse and heterogeneous range of home devices, interactions between multiple home users, complicated data flow between multiple home devices and home users, some less studied demographic factors, and advanced conceptual frameworks. Based on these findings, we recommended key future research directions, e.g., research for a better understanding of security and privacy aspects in different multi-device and multi-user contexts, and a more comprehensive ontology on the security and privacy of the smart home covering varying types of home devices and behaviors of different types of home users."
Methods for Analyzing Medical-Order Sequence Variants in Sequential Pattern Mining for Electronic Medical Record Systems,"Le, Hieu Hanh and Yamada, Tatsuhiro and Honda, Yuichi and Sakamoto, Takatoshi and Matsuo, Ryosuke and Yamazaki, Tomoyoshi and Araki, Kenji and Yokota, Haruo",10.1145/3561825,2023,"Electronic medical record systems have been adopted by many large hospitals worldwide, enabling the recorded data to be analyzed by various computer-based techniques to gain a better understanding of hospital-based disease treatments. Among such techniques, sequential pattern mining, already widely used for data mining and knowledge discovery in other application domains, has shown great potential for discovering frequent patterns in sequences of disease treatments. However, studies have yet to evaluate the use of medical-order sequence variants, where a “frequent pattern” can include some limited variations to the pattern, or have considered the factors that lead to these variants. Such a study would be meaningful for medical tasks such as improving the quality of a particular treatment method, comparing treatments with multiple hospitals, recommending the best-suited treatment for each patient, and optimizing the running costs in hospitals. This article proposes methods for evaluating medical-order sequence variants and understanding variant factors based on a statistical approach. We consider the safety and efficiency of sequences and related information about the variants, such as gender, age, and test results from hospitals. Our proposal has been demonstrated as effective by experimentally evaluating an electronic medical record system’s real dataset and obtaining feedback from medical workers. The experimental results indicate that the medical treatment history and specimen test results after hospitalization are significant in identifying the factors that lead to variants."
Video Game Bad Smells: What They Are and How Developers Perceive Them,"Nardone, Vittoria and Muse, Biruk and Abidi, Mouna and Khomh, Foutse and Di Penta, Massimiliano",10.1145/3563214,2023,"Video games represent a substantial and increasing share of the software market. However, their development is particularly challenging as it requires multi-faceted knowledge, which is not consolidated in computer science education yet. This article aims at defining a catalog of bad smells related to video game development. To achieve this goal, we mined discussions on general-purpose and video game-specific forums. After querying such a forum, we adopted an open coding strategy on a statistically significant sample of 572 discussions, stratified over different forums. As a result, we obtained a catalog of 28 bad smells, organized into five categories, covering problems related to game design and logic, physics, animation, rendering, or multiplayer. Then, we assessed the perceived relevance of such bad smells by surveying 76 game development professionals. The survey respondents agreed with the identified bad smells but also provided us with further insights about the discussed smells. Upon reporting results, we discuss bad smell examples, their consequences, as well as possible mitigation/fixing strategies and trade-offs to be pursued by developers. The catalog can be used not only as a guideline for developers and educators but also can pave the way toward better automated tool support for video game developers."
Conversational Agents in Therapeutic Interventions for Neurodevelopmental Disorders: A Survey,"Catania, Fabio and Spitale, Micol and Garzotto, Franca",10.1145/3564269,2023,"Neurodevelopmental Disorders (NDD) are a group of conditions with onset in the developmental period characterized by deficits in the cognitive and social areas. Conversational agents have been increasingly explored to support therapeutic interventions for people with NDD. This survey provides a structured view of the crucial design features of these systems, the types of therapeutic goals they address, and the empirical methods adopted for their evaluation. From this analysis, we elaborate a set of recommendations and highlight the gaps left unsolved in the state of the art, upon which we ground a research agenda on conversational agents for NDD."
Code Search: A Survey of Techniques for Finding Code,"Di Grazia, Luca and Pradel, Michael",10.1145/3565971,2023,"The immense amounts of source code provide ample challenges and opportunities during software development. To handle the size of code bases, developers commonly search for code, e.g., when trying to find where a particular feature is implemented or when looking for code examples to reuse. To support developers in finding relevant code, various code search engines have been proposed. This article surveys 30 years of research on code search, giving a comprehensive overview of challenges and techniques that address them. We discuss the kinds of queries that code search engines support, how to preprocess and expand queries, different techniques for indexing and retrieving code, and ways to rank and prune search results. Moreover, we describe empirical studies of code search in practice. Based on the discussion of prior work, we conclude the article with an outline of challenges and opportunities to be addressed in the future."
A Systematic Survey of Just-in-Time Software Defect Prediction,"Zhao, Yunhua and Damevski, Kostadin and Chen, Hui",10.1145/3567550,2023,"Recent years have experienced sustained focus in research on software defect prediction that aims to predict the likelihood of software defects. Moreover, with the increased interest in continuous deployment, a variant of software defect prediction called Just-in-Time Software Defect Prediction (JIT-SDP) focuses on predicting whether each incremental software change is defective. JIT-SDP is unique in that it consists of two interconnected data streams, one consisting of the arrivals of software changes stemming from design and implementation, and the other the (defective or clean) labels of software changes resulting from quality assurance processes.We present a systematic survey of 67 JIT-SDP studies with the objective to help researchers advance the state of the art in JIT-SDP and to help practitioners become familiar with recent progress. We summarize best practices in each phase of the JIT-SDP workflow, carry out a meta-analysis of prior studies, and suggest future research directions. Our meta-analysis of JIT-SDP studies indicates, among other findings, that the predictive performance correlates with change defect ratio, suggesting that JIT-SDP is most performant in projects that experience relatively high defect ratios. Future research directions for JIT-SDP include situating each technique into its application domain, reliability-aware JIT-SDP, and user-centered JIT-SDP."
The AI Tech-Stack Model,"Tsaih, Rua-Huan and Chang, Hsin-Lu and Hsu, Chih-Chun and Yen, David C.",10.1145/3568026,2023,Management and technology challenges of AI-enabled application projects.
Emerging Technologies in K–12 Education: A Future HCI Research Agenda,"Van Mechelen, Maarten and Smith, Rachel Charlotte and Schaper, Marie-Monique and Tamashiro, Mariana and Bilstrup, Karl-Emil and Lunding, Mille and Graves Petersen, Marianne and Sejer Iversen, Ole",10.1145/3569897,2023,"This systematic mapping review sheds light on how emerging technologies have been introduced and taught in various K–12 learning settings, particularly with regard to artificial intelligence (AI), machine learning (ML), the internet of things (IoT), augmented reality (AR), and virtual reality (VR). These technologies are rapidly being integrated into children's everyday lives, but their functions and implications are rarely understood due to their complex and distributed nature. The review provides a rigorous overview of the state of the art based on 107 records published across the fields of human-computer interaction, learning sciences, computing education, and child–computer interaction between 2010 and 2020.&nbsp;The findings show the urgent need on a global scale for inter- and transdisciplinary research that can integrate these dispersed contributions into a more coherent field of research and practice. The article presents nine discussion points for developing a shared agenda to mature the field. Based on the HCI community's expertise in human-centred approaches to technology and aspects of learning, we argue that the community is ideally positioned to take a leading role in the realisation of this future research agenda."
What Is the Intended Usage Context of This Model? An Exploratory Study of Pre-Trained Models on Various Model Repositories,"Gong, Lina and Zhang, Jingxuan and Wei, Mingqiang and Zhang, Haoxiang and Huang, Zhiqiu",10.1145/3569934,2023,"There is a trend of researchers and practitioners to directly apply pre-trained models to solve their specific tasks. For example, researchers in software engineering (SE) have successfully exploited the pre-trained language models to automatically generate the source code and comments. However, there are domain gaps in different benchmark datasets. These data-driven (or machine learning based) models trained on one benchmark dataset may not operate smoothly on other benchmarks. Thus, the reuse of pre-trained models introduces large costs and additional problems of checking whether arbitrary pre-trained models are suitable for the task-specific reuse or not. To our knowledge, software engineers can leverage code contracts to maximize the reuse of existing software components or software services. Similar to the software reuse in the SE field, reuse SE could be extended to the area of pre-trained model reuse. Therefore, according to the model card’s and FactSheet’s guidance for suppliers of pre-trained models on what information they should be published, we propose model contracts including the pre- and post-conditions of pre-trained models to enable better model reuse. Furthermore, many non-trivial yet challenging issues have not been fully investigated, although many pre-trained models are readily available on the model repositories. Based on our model contract, we conduct an exploratory study of 1908 pre-trained models on six mainstream model repositories (i.e., the TensorFlow Hub, PyTorch Hub, Model Zoo, Wolfram Neural Net Repository, Nvidia, and Hugging Face) to investigate the gap between necessary pre- and post-condition information and actual specifications. Our results clearly show that (1) the model repositories tend to provide confusing information of the pre-trained models, especially the information about the task’s type, model, training set, and (2) the model repositories cannot provide all of our proposed pre/post-condition information, especially the intended use, limitation, performance, and quantitative analysis. On the basis of our new findings, we suggest that (1) the developers of model repositories shall provide some necessary options (e.g., the training dataset, model algorithm, and performance measures) for each of pre/post-conditions of pre-trained models in each task type, (2) future researchers and practitioners provide more efficient metrics to recommend suitable pre-trained model, and (3) the suppliers of pre-trained models should report their pre-trained models in strict accordance with our proposed pre/post-condition and report their models according to the characteristics of each condition that has been reported in the model repositories."
Scheduling of Resource Allocation Systems with Timed Petri Nets: A Survey,"Huang, Bo and Zhou, Mengchu and Lu, Xiaoyu Sean and Abusorrah, Abdullah",10.1145/3570326,2023,"Resource allocation systems (RASs) belong to a kind of discrete event system commonly seen in the industry. In such systems, available resources are allocated to concurrently running processes to optimize some performance criteria. Search strategies in the reachability graph (RG) of a timed Petri net (PN) attracted much attention in the past decades to cope with RAS scheduling problems (RSPs), since PNs are very suitable to model and analyze RASs and their RGs fully reflect systems’ behavior. However, there has been no existing related survey and review paper till now. In this work, we present a tutorial and comprehensive literature survey of RG-based RSP methods. Many state-of-the-art RG-based RAS scheduling strategies are reviewed and summarized. First, we present a framework of RSPs and classify RSPs and their PNs in terms of resource usage and net structures. The differences and relations among the PNs are also given. Then, we introduce timed PN construction methods for RSPs and scheduling objectives and search strategies for RG-based RSPs. Next, we summarize different heuristic functions adopted in a frequently used A* search to solve RG-based RSPs. Finally, we discuss some important future research directions and open issues."
AI-Based Intrusion Detection Systems for In-Vehicle Networks: A Survey,"Rajapaksha, Sampath and Kalutarage, Harsha and Al-Kadri, M. Omar and Petrovski, Andrei and Madzudzo, Garikayi and Cheah, Madeline",10.1145/3570954,2023,"The Controller Area Network (CAN) is the most widely used in-vehicle communication protocol, which still lacks the implementation of suitable security mechanisms such as message authentication and encryption. This makes the CAN bus vulnerable to numerous cyber attacks. Various Intrusion Detection Systems (IDSs) have been developed to detect these attacks. However, the high generalization capabilities of Artificial Intelligence (AI) make AI-based IDS an excellent countermeasure against automotive cyber attacks. This article surveys AI-based in-vehicle IDS from 2016 to 2022 (August) with a novel taxonomy. It reviews the detection techniques, attack types, features, and benchmark datasets. Furthermore, the article discusses the security of AI models, necessary steps to develop AI-based IDSs in the CAN bus, identifies the limitations of existing proposals, and gives recommendations for future research directions."
A Systematic Survey of General Sparse Matrix-matrix Multiplication,"Gao, Jianhua and Ji, Weixing and Chang, Fangli and Han, Shiyu and Wei, Bingxin and Liu, Zeming and Wang, Yizhuo",10.1145/3571157,2023,"General Sparse Matrix-Matrix Multiplication (SpGEMM) has attracted much attention from researchers in graph analyzing, scientific computing, and deep learning. Many optimization techniques have been developed for different applications and computing architectures over the past decades. The objective of this article is to provide a structured and comprehensive overview of the researches on SpGEMM. Existing researches have been grouped into different categories based on target architectures and design choices. Covered topics include typical applications, compression formats, general formulations, key problems and techniques, architecture-oriented optimizations, and programming models. The rationales of different algorithms are analyzed and summarized. This survey sufficiently reveals the latest progress of SpGEMM research to 2021. Moreover, a thorough performance comparison of existing implementations is presented. Based on our findings, we highlight future research directions, which encourage better design and implementations in later studies."
Achieving Digital Wellbeing Through Digital Self-control Tools: A Systematic Review and Meta-analysis,"Roffarello, Alberto Monge and De Russis, Luigi",10.1145/3571810,2023,"Public media and researchers in different areas have recently focused on perhaps unexpected problems that derive from an excessive and frequent use of technology, giving rise to a new kind of psychological “digital” wellbeing. Such a novel and pressing topic has fostered, both in the academia and in the industry, the emergence of a variety of digital self-control tools allowing users to self-regulate their technology use through interventions like timers and lock-out mechanisms. While these emerging technologies for behavior change hold great promise to support people’s digital wellbeing, we still have a limited understanding of their real effectiveness, as well as of how to best design and evaluate them. Aiming to guide future research in this important domain, this article presents a systematic review and a meta-analysis of current work on tools for digital self-control. We surface motivations, strategies, design choices, and challenges that characterize the design, development, and evaluation of digital self-control tools. Furthermore, we estimate their overall effect size on reducing (unwanted) technology use through a meta-analysis. By discussing our findings, we provide insights on how to (i) overcome a limited perspective that exclusively focuses on technology overuse and self-monitoring tools, (ii) evaluate digital self-control tools through long-term studies and standardized measures, and (iii) bring ethics in the digital wellbeing discourse and deal with the business model of contemporary tech companies."
Machine Learning for Software Engineering: A Tertiary Study,"Kotti, Zoe and Galanopoulou, Rafaila and Spinellis, Diomidis",10.1145/3572905,2023,"Machine learning (ML) techniques increase the effectiveness of software engineering (SE) lifecycle activities. We systematically collected, quality-assessed, summarized, and categorized 83 reviews in ML for SE published between 2009 and 2022, covering 6,117 primary studies. The SE areas most tackled with ML are software quality and testing, while human-centered areas appear more challenging for ML. We propose a number of ML for SE research challenges and actions, including conducting further empirical validation and industrial studies on ML, reconsidering deficient SE methods, documenting and automating data collection and pipeline processes, reexamining how industrial practitioners distribute their proprietary data, and implementing incremental ML approaches."
Computer Vision-based Analysis of Buildings and Built Environments: A Systematic Review of Current Approaches,"Starzy\'{n}ska-Grze\'{s}, Ma\l{}gorzata B. and Roussel, Robin and Jacoby, Sam and Asadipour, Ali",10.1145/3578552,2023,"Analysing 88 sources published from 2011 to 2021, this article presents a first systematic review of the computer vision-based analysis of buildings and the built environment. Its aim is to assess the potential of this research for architectural studies and the implications of a shift to a cross-disciplinarity approach between architecture and computer science for research problems, aims, processes, and applications. To this end, the types of algorithms and data sources used in the reviewed studies are discussed in respect to architectural applications such as a building classification, detail classification, qualitative environmental analysis, building condition survey, and building value estimation. Based on this, current research gaps and trends are identified, with two main research aims emerging. First, studies that use or optimise computer vision methods to automate time-consuming, labour-intensive, or complex tasks when analysing architectural image data. Second, work that explores the methodological benefits of machine learning approaches to overcome limitations of conventional analysis to investigate new questions about the built environment by finding patterns and relationships among visual, statistical, and qualitative data. The growing body of research offers new methods to architectural and design studies, with the article identifying future challenges and directions of research."
Does SoC Hardware Development Become Agile by Saying So: A Literature Review and Mapping Study,"Rautakoura, Antti and H\""{a}m\""{a}l\""{a}inen, Timo",10.1145/3578554,2023,"The success of agile development methods in software development has raised interest in System-on-Chip (SoC) design, which involves high architectural and development process complexity under time and project management pressure. This article discovers the current state of agile hardware development with the questions (1) how well literature covers the SoC development process, (2) what agile methods and practices are applied or (3) what proposals are made to increase the agility, and (4) what is the impact for the SoC community. To answer the questions, a mapping study and literature review were performed. Seven hundred thirty papers were first studied, and eventually, after a rigorous filtering process, 25 papers were thoroughly analyzed. The results show that the popular agile SW development methods are applied in 5 cases, ideas adapted from the agile Hardware manifesto in 9 cases, and 11 cases do not define the Agile HW development method. Most of the papers address shorter development time by better methodologies and tools that indirectly shape the SoC development toward agility. The focus of agile hardware development is mostly on the SoC artifacts and methodological improvements have not been quantified. However, the literature indicates a significant impact on many academic chip prototypes. The challenges are better understood and the interest in agile methods is clearly increasing. The methodological gaps in the prevalent situation encourage further research and more accurate reporting of the development in addition to the SoC artifacts."
Malayalam Natural Language Processing: Challenges in Building a Phrase-Based Statistical Machine Translation System,"Sebastian, Mary Priya and G., Santhosh Kumar",10.1145/3579163,2023,"Statistical Machine Translation (SMT) is a preferred Machine Translation approach to convert the text in a specific language into another by automatically learning translations using a parallel corpus. SMT has been successful in producing quality translations in many foreign languages, but there are only a few works attempted in South Indian languages. The article discusses on experiments conducted with SMT for Malayalam language and analyzes how the methods defined for SMT in foreign languages affect a Dravidian language, Malayalam. The baseline SMT model does not work for Malayalam due to its unique characteristics like agglutinative nature and morphological richness. Hence, the challenge is to identify where precisely the SMT model has to be modified such that it adapts the challenges of the language peculiarity into the baseline model and give better translations for English to Malayalam translation. The alignments between English and Malayalam sentence pairs, subjected to the training process in SMT, plays a crucial role in producing quality output translation. Therefore, this work focuses on improving the translation model of SMT by refining the alignments between English–Malayalam sentence pairs. The phrase alignment algorithms align the verb and noun phrases in the sentence pairs and develop a new set of alignments for the English–Malayalam sentence pairs. These alignment sets refine the alignments formed from Giza++ produced as a result of EM training algorithm. The improved Phrase-Based SMT model trained using these refined alignments resulted in better translation quality, as indicated by the AER and BLUE scores."
Beyond Transactional Democracy: A Study of Civic Tech in Canada,"McCord, Curtis W. and Becker, Christoph",10.1145/3579462,2023,"Technologies are increasingly enrolled in projects to involve civilians in the work of policy-making, often under the label of 'civic technology'. But conventional forms of participation through transactions such as voting provide limited opportunities for engagement. In response, some civic tech groups organize around issues of shared concern to explore new forms of democratic technologies. How does their work affect the relationship between publics and public servants?This paper explores how a Civic Tech Toronto creates a platform for civic engagement through the maintenance of an autonomous community for civic engagement and participation that is casual, social, nonpartisan, experimental, and flexible. Based on two years of action research, including community organizing, interviews, and observations, this paper shows how this grassroots civic tech group creates a civic platform that places a diverse range of participants in contact with the work of public servants, helping to build capacities and relationships that prepare both publics and public servants for the work of participatory democracy.The case shows that understanding civic tech requires a lens beyond the mere analysis or production of technical artifacts. As a practice for making technologies that is social and participatory, civic tech creates alternative modes of technology development and opportunities for experimentation and learning, and it can reconfigure the roles of democratic participants."
Infrastructures for Virtual Volunteering at Online Music Festivals,"Benford, Steve and Manninen, Kadja and Martindale, Sarah and Hazzard, Adrian and Martinez Avila, Juan Pablo and Tennent, Paul and Spence, Jocelyn and Castle-Green, Teresa and Brundell, Pat and Barnard, Pepita and Darzentas, Dimitrios Paris",10.1145/3579498,2023,"Volunteering benefits recipients, volunteers, communities, and society, while digital technologies establish new opportunities for virtual volunteering. We describe how volunteers transitioned the UK's long-established Oxjam grassroots music festival online in response to the COVID pandemic, delivering a local pilot before scaling up nationwide. We adopt an infrastructural perspective to reveal how two teams of volunteers defined a flexible festival format, knitted together diverse technologies into a technical platform, and operated this to deliver the festival. We highlight the need for teams of volunteers to orchestrate both audience and performer trajectories through festivals. We argue for deliberately designing in volunteer labour rather than automating it out by translating traditional roles online while defining new digital ones. We propose to make these roles rewarding through a more social volunteer experience, including privileged backstage access. We highlight the challenges of using social media for such events, including complying with algorithmic policing of rights."
Reviewing Interventions to Address Misinformation: The Need to Expand Our Vision Beyond an Individualistic Focus,"Aghajari, Zhila and Baumer, Eric P. S. and DiFranzo, Dominic",10.1145/3579520,2023,"Prior work has identified a variety of factors that drive the way people identify and respond to misinformation. Such factors include confirmation bias, perceived credibility of the information source, individual media literacy, social norms, and others. This paper reviews the interventions designed to address misinformation and examines how various underlying mechanisms of response to misinformation are operationalized and implemented in the reviewed interventions. Key findings show that most prior work to address misinformation heavily focuses on individual pieces of misinformation and the actions individuals take in response to those individual pieces. These individualistic approaches, we argue, overlook the other drivers of responses to misinformation, such as individuals' prior beliefs and the social contexts in which misinformation is encountered. Additionally, the analysis shows that an individualistic focus on misinformation draws attention away from the systemic nature and consequences of misinformation. This paper argues that to overcome the limitation of individualistic approaches to addressing misinformation, future interventions need to expand their scope beyond individualistic approaches. As one way to do so, it discusses leveraging the impacts of community factors that impact the spread and impacts of misinformation. The paper concludes by using social norms as an example to illustrate how a focus on community factors might work in practice."
Seeing Like a Toolkit: How Toolkits Envision the Work of AI Ethics,"Wong, Richmond Y. and Madaio, Michael A. and Merrill, Nick",10.1145/3579621,2023,"Numerous toolkits have been developed to support ethical AI development. However, toolkits, like all tools, encode assumptions in their design about what work should be done and how. In this paper, we conduct a qualitative analysis of 27 AI ethics toolkits to critically examine how the work of ethics is imagined and how it is supported by these toolkits. Specifically, we examine the discourses toolkits rely on when talking about ethical issues, who they imagine should do the work of ethics, and how they envision the work practices involved in addressing ethics. Among the toolkits, we identify a mismatch between the imagined work of ethics and the support the toolkits provide for doing that work. In particular, we identify a lack of guidance around how to navigate labor, organizational, and institutional power dynamics as they relate to performing ethical work. We use these omissions to chart future work for researchers and designers of AI ethics toolkits."
C-suite Leadership of Digital Government,"Kristensen, Kenneth and Andersen, Kim Normann",10.1145/3580000,2023,"Despite decades of digitalization in day-to-day government operations and in the governance of the public sector, there is a major research gap in understanding the nature of digital government leadership (DGL) and the diversity in how top managers are leading the digital transition and transformation of government. Based on a structured literature review and in-depth inductive analysis of previous research within the domains of e-government, information systems, and public administration research, this article explores how the C-suite level of government is leading the digitalization. In the article, we propose a definition of DGL and a leadership framework to capture the nexus and direction of leadership. Also, the article proposes distinct leadership roles and actions to forward digital government."
Cancer Prognosis and Diagnosis Methods Based on Ensemble Learning,"Zolfaghari, Behrouz and Mirsadeghi, Leila and Bibak, Khodakhast and Kavousi, Kaveh",10.1145/3580218,2023,"Ensemble methods try to improve performance via integrating different kinds of input data, features, or learning algorithms. In addition to other areas, they are finding their applications in cancer prognosis and diagnosis. However, in this area, the research community is lagging behind the technology. A systematic review along with a taxonomy on ensemble methods used in cancer prognosis and diagnosis can pave the way for the research community to keep pace with the technology and even lead trend. In this article, we first present an overview on existing relevant surveys and highlight their shortcomings, which raise the need for a new survey focusing on Ensemble Classifiers (ECs) used for the diagnosis and prognosis of different cancer types. Then, we exhaustively review the existing methods, including the traditional ones as well as those based on deep learning. The review leads to a taxonomy as well as the identification of the best-studied cancer types, the best ensemble methods used for the related purposes, the prevailing input data types, the most common decision-making strategies, and the common evaluating methodologies. Moreover, we establish future directions for researchers interested in following existing research trends or working on less-studied aspects of the area."
A Survey on Measuring Cognitive Workload in Human-Computer Interaction,"Kosch, Thomas and Karolus, Jakob and Zagermann, Johannes and Reiterer, Harald and Schmidt, Albrecht and Wo\'{z}niak, Pawe\l{} W.",10.1145/3582272,2023,"The ever-increasing number of computing devices around us results in more and more systems competing for our attention, making cognitive workload a crucial factor for the user experience of human-computer interfaces. Research in Human-Computer Interaction (HCI) has used various metrics to determine users’ mental demands. However, there needs to be a systematic way to choose an appropriate and effective measure for cognitive workload in experimental setups, posing a challenge to their reproducibility. We present a literature survey of past and current metrics for cognitive workload used throughout HCI literature to address this challenge. By initially exploring what cognitive workload resembles in the HCI context, we derive a categorization supporting researchers and practitioners in selecting cognitive workload metrics for system design and evaluation. We conclude with three following research gaps: (1) defining and interpreting cognitive workload in HCI, (2) the hidden cost of the NASA-TLX, and (3) HCI research as a catalyst for workload-aware systems, highlighting that HCI research has to deepen and conceptualize the understanding of cognitive workload in the context of interactive computing systems."
"A Review on Tools, Mechanics, Benefits, and Challenges of Gamified Software Testing","Fulcini, Tommaso and Coppola, Riccardo and Ardito, Luca and Torchiano, Marco",10.1145/3582273,2023,"Gamification is an established practice in Software Engineering to increase effectiveness and engagement in many practices. This manuscript provides a characterization of the application of gamification to the Software Testing area. Such practice in fact reportedly suffers from low engagement by both personnel in industrial contexts and learners in educational contexts. Our goal is to identify the application areas and utilized gamified techniques and mechanics, the provided benefits and drawbacks, as well as the open challenges in the field. To this purpose, we conducted a Multivocal Literature Review to identify white and grey literature sources addressing gamified software testing.We analyzed 73 contributions and summarized the most common gamified mechanics, concepts, tools, and domains where they are mostly applied. We conclude that gamification in software testing is mostly applied to the test creation phase with simple white-box unit or mutation testing tools and is mostly used to foster good behaviors by promoting the testers’ accomplishment. Key research areas and main challenges in the field are: careful design of tailored gamified mechanics for specific testing techniques; the need for technological improvements to enable crowdsourcing, cooperation, and concurrency; the necessity for empirical and large-scale evaluation of the benefits delivered by gamification mechanics."
Clinician-Facing AI in the Wild: Taking Stock of the Sociotechnical Challenges and Opportunities for HCI,"Zaj\k{a}c, Hubert D. and Li, Dana and Dai, Xiang and Carlsen, Jonathan F. and Kensing, Finn and Andersen, Tariq O.",10.1145/3582430,2023,"Artificial Intelligence (AI) in medical applications holds great promise. However, the use of Machine Learning-based (ML) systems in clinical practice is still minimal. It is uniquely difficult to introduce clinician-facing ML-based systems in practice, which has been recognised in HCI and related fields. Recent publications have begun to address the sociotechnical challenges of designing, developing, and successfully deploying clinician-facing ML-based systems. We conducted a qualitative systematic review and provided answers to the question: “How can HCI researchers and practitioners contribute to the successful realisation of ML in medical practice?” We reviewed 25 eligible papers that investigated the real-world clinical implications of concrete clinician-facing ML-based systems. The main contributions of this systematic review are: (1) an overview of the technical aspects of ML innovation and their consequences for HCI researchers and practitioners; (2) a description of the different roles that ML-based systems can take in clinical settings; (3) a conceptualisation of the main activities of medical ML innovation processes; (4) identification of five sociotechnical interdependencies that emerge from medical ML innovation; and (5) implications for HCI researchers and practitioners on how to mitigate the sociotechnical challenges of medical ML innovation."
"Code-line-level Bugginess Identification: How Far have We Come, and How Far have We Yet to Go?","Guo, Zhaoqiang and Liu, Shiran and Liu, Xutong and Lai, Wei and Ma, Mingliang and Zhang, Xu and Ni, Chao and Yang, Yibiao and Li, Yanhui and Chen, Lin and Zhou, Guoqiang and Zhou, Yuming",10.1145/3582572,2023,"Background. Code-line-level bugginess identification (CLBI) is a vital technique that can facilitate developers to identify buggy lines without expending a large amount of human effort. Most of the existing studies tried to mine the characteristics of source codes to train supervised prediction models, which have been reported to be able to discriminate buggy code lines amongst others in a target program.Problem. However, several simple and clear code characteristics, such as complexity of code lines, have been disregarded in the current literature. Such characteristics can be acquired and applied easily in an unsupervised way to conduct more accurate CLBI, which also can decrease the application cost of existing CLBI approaches by a large margin.Objective. We aim at investigating the status quo in the field of CLBI from the perspective of (1) how far we have really come in the literature, and (2) how far we have yet to go in the industry, by analyzing the performance of state-of-the-art (SOTA) CLBI approaches and tools, respectively.Method. We propose a simple heuristic baseline solution GLANCE (aiminG at controL- ANd ComplEx-statements) with three implementations (i.e., GLANCE-MD, GLANCE-EA, and GLANCE-LR). GLANCE is a two-stage CLBI framework: first, use a simple model to predict the potentially defective files; second, leverage simple code characteristics to identify buggy code lines in the predicted defective files. We use GLANCE as the baseline to investigate the effectiveness of the SOTA CLBI approaches, including natural language processing (NLP) based, model interpretation techniques (MIT) based, and popular static analysis tools (SAT).Result. Based on 19 open-source projects with 142 different releases, the experimental results show that GLANCE framework has a prediction performance comparable or even superior to the existing SOTA CLBI approaches and tools in terms of 8 different performance indicators.Conclusion. The results caution us that, if the identification performance is the goal, the real progress in CLBI is not being achieved as it might have been envisaged in the literature and there is still a long way to go to really promote the effectiveness of static analysis tools in industry. In addition, we suggest using GLANCE as a baseline in future studies to demonstrate the usefulness of any newly proposed CLBI approach."
Estimating Software Functional Size via Machine Learning,"Lavazza, Luigi and Locoro, Angela and Liu, Geng and Meli, Roberto",10.1145/3582575,2023,"Measuring software functional size via standard Function Points Analysis (FPA) requires the availability of fully specified requirements and specific competencies. Most of the time, the need to measure software functional size occurs well in advance with respect to these ideal conditions, under the lack of complete information or skilled experts. To work around the constraints of the official measurement process, several estimation methods&nbsp;for FPA have been proposed and are commonly used. Among these, the International Function Points User Group (IFPUG) has adopted the “High-level FPA” method (also known as the NESMA method). This method avoids weighting each data and transaction function by using fixed weights instead. Applying High-level FPA, or similar estimation methods, is faster and easier than carrying out the official measurement process but inevitably yields an approximation in the measures. In this article, we contribute to the problem of estimating software functional size measures by using machine learning. To the best of our knowledge, machine learning methods were never applied to the early estimation of software functional size. Our goal is to understand whether machine learning techniques yield estimates of FPA measures that are more accurate than those obtained with High-level FPA or similar methods. An empirical study on a large dataset of functional size predictors was carried out to train and test three of the most popular and robust machine learning methods, namely Random Forests, Support Vector Regression&nbsp;, and Neural Networks. A systematic experimental phase, with cycles of dataset filtering and splitting, parameter tuning, and model training and validation, is presented. The estimation accuracy of the obtained models was then evaluated and compared to that of fixed-weight models (e.g., High-level FPA) and linear regression models, also using a second dataset as the test set. We found that Support Vector Regression yields quite accurate estimation models. However, the obtained level of accuracy does not appear significantly better with respect to High-level FPA or to models built via ordinary least squares regression. Noticeably, fairly good accuracy levels were obtained by models that do not even require discerning among different types of transactions and data."
A New Sustainable Model for Aligning Industry Requirements and University Programs,"Hol, Ana and Richardson, Joan and McGovern, James and Hamilton, Margaret",10.1145/3583086,2023,
Assessing the Early Bird Heuristic (for Predicting Project Quality),"C., Shrikanth N. and Menzies, Tim",10.1145/3583565,2023,"Before researchers rush to reason across all available data or try complex methods, perhaps it is prudent to first check for simpler alternatives. Specifically, if the historical data has the most information in some small region, then perhaps a model learned from that region would suffice for the rest of the project.To support this claim, we offer a case study with 240 projects, where we find that the information in those projects “clumps” towards the earliest parts of the project. A quality prediction model learned from just the first 150 commits works as well, or better than state-of-the-art alternatives. Using just this “early bird” data, we can build models very quickly and very early in the project life cycle. Moreover, using this early bird method, we have shown that a simple model (with just a few features) generalizes to hundreds of projects.Based on this experience, we doubt that prior work on generalizing quality models may have needlessly complicated an inherently simple process. Further, prior work that focused on later-life cycle data needs to be revisited, since their conclusions were drawn from relatively uninformative regions.Replication note: All our data and scripts are available here: https://github.com/snaraya7/early-bird."
Content-based and Knowledge-enriched Representations for Classification Across Modalities: A Survey,"Pittaras, Nikiforos and Giannakopoulos, George and Stamatopoulos, Panagiotis and Karkaletsis, Vangelis",10.1145/3583682,2023,"This survey documents representation approaches for classification across different modalities, from purely content-based methods to techniques utilizing external sources of structured knowledge. We present studies related to three paradigms used for representation, namely (a) low-level template-matching methods, (b) aggregation-based approaches, and (c) deep representation learning systems. We then describe existing resources of structure knowledge and elaborate on the need for enriching representations with such information. Approaches that utilize knowledge resources are presented next, organized with respect to how external information is exploited, i.e., (a) input enrichment and modification, (b) knowledge-based refinement and (c) end-to-end knowledge-aware systems. We subsequently provide a high-level discussion to summarize and compare strengths/weaknesses of the representation/enrichment paradigms proposed, and conclude the survey with an overview of relevant research findings and possible directions for future work."
Building Knowledge through Action: Considerations for Machine Learning in the Workplace,"Lindley, Si\^{a}n E. and Wilkins, Denise J.",10.1145/3584947,2023,"Innovations in machine learning are enabling organisational knowledge bases to be automatically generated from working people's activities. The potential for these to shift the ways in which knowledge is produced and shared raises questions about what types of knowledge might be inferred from working people's actions, how these can be used to support work, and what the broader ramifications of this might be. This article draws on findings from studies of (i) collaborative actions, and (ii) knowledge actions, to explore how these actions might (i) inform automatically generated knowledge bases, and (ii) be better supported through technological innovation. We triangulate findings to develop a framework of actions that are performed as part of everyday work, and use this to explore how mining those actions could result in knowledge being explicitly and implicitly contributed to a knowledge base. We draw on these possibilities to highlight implications and considerations for responsible design."
Data Practices and Data Stewardship,"Wong, Janis",10.1145/3589133,2023,"This forum is dedicated to exploring the notion of meaningfulness in design processes, taking the perspectives of community groups, nongovernmental organizations, and those who are marginalized in society as starting points. Authors will reflect conceptually and methodologically on practical engagements. --- Rosanna Bellini and Angelika Strohmayer, Editors"
"Conceptual Modeling: Topics, Themes, and Technology Trends","Storey, Veda C. and Lukyanenko, Roman and Castellanos, Arturo",10.1145/3589338,2023,"Conceptual modeling is an important part of information systems development and use that involves identifying and representing relevant aspects of reality. Although the past decades have experienced continuous digitalization of services and products that impact business and society, conceptual modeling efforts are still required to support new technologies as they emerge. This paper surveys research on conceptual modeling over the past five decades and shows how its topics and trends continue to evolve to accommodate emerging technologies, while remaining grounded in basic constructs. We survey over 5,300 papers that address conceptual modeling topics from the 1970s to the present, which are collected from 35 multidisciplinary journals and conferences, and use them as the basis from which to analyze the progression of conceptual modeling. The important role that conceptual modeling should play in our evolving digital world is discussed, and future research directions proposed."
RadarSense: Accurate Recognition of Mid-air Hand Gestures with Radar Sensing and Few Training Examples,"Slu\""{y}ters, Arthur and Lambot, S\'{e}bastien and Vanderdonckt, Jean and Vatavu, Radu-Daniel",10.1145/3589645,2023,"Microwave radars bring many benefits to mid-air gesture sensing due to their large field of view and independence from environmental conditions, such as ambient light and occlusion. However, radar signals are highly dimensional and usually require complex deep learning approaches. To understand this landscape, we report results from a systematic literature review of (N=118) scientific papers on radar sensing, unveiling a large variety of radar technology of different operating frequencies and bandwidths and antenna configurations but also various gesture recognition techniques. Although highly accurate, these techniques require a large amount of training data that depend on the type of radar. Therefore, the training results cannot be easily transferred to other radars. To address this aspect, we introduce a new gesture recognition pipeline that implements advanced full-wave electromagnetic modeling and inversion to retrieve physical characteristics of gestures that are radar independent, i.e., independent of the source, antennas, and radar-hand interactions. Inversion of radar signals further reduces the size of the dataset by several orders of magnitude, while preserving the essential information. This approach is compatible with conventional gesture recognizers, such as those based on template matching, which only need a few training examples to deliver high recognition accuracy rates. To evaluate our gesture recognition pipeline, we conducted user-dependent and user-independent evaluations on a dataset of 16 gesture types collected with the Walabot, a low-cost off-the-shelf array radar. We contrast these results with those obtained for the same gesture types collected with an ultra-wideband radar made of a vector network analyzer with a single horn antenna and with a computer vision sensor, respectively. Based on our findings, we suggest some design implications to support future development in radar-based gesture recognition."
Detecting Mental Distresses Using Social Behavior Analysis in the Context of COVID-19: A Survey,"Dhelim, Sahraoui and Chen, Liming and Das, Sajal K. and Ning, Huansheng and Nugent, Chris and Leavey, Gerard and Pesch, Dirk and Bantry-White, Eleanor and Burns, Devin",10.1145/3589784,2023,"Online social media provides a channel for monitoring people’s social behaviors from which to infer and detect their mental distresses. During the COVID-19 pandemic, online social networks were increasingly used to express opinions, views, and moods due to the restrictions on physical activities and in-person meetings, leading to a significant amount of diverse user-generated social media content. This offers a unique opportunity to examine how COVID-19 changed global behaviors regarding its ramifications on mental well-being. In this article, we surveyed the literature on social media analysis for the detection of mental distress, with a special emphasis on the studies published since the COVID-19 outbreak. We analyze relevant research and its characteristics and propose new approaches to organizing the large amount of studies arising from this emerging research area, thus drawing new views, insights, and knowledge for interested communities. Specifically, we first classify the studies in terms of feature extraction types, language usage patterns, aesthetic preferences, and online behaviors. We then explored various methods (including machine learning and deep learning techniques) for detecting mental health problems. Building upon the in-depth review, we present our findings and discuss future research directions and niche areas in detecting mental health problems using social media data. We also elaborate on the challenges of this fast-growing research area, such as technical issues in deploying such systems at scale as well as privacy and ethical concerns."
Study on Logistic Service Management of Colleges and Universities Based on Data Mining Algorithms,"Zhang, Zhicheng",10.1145/3590961,2023,"Construction of a large logistics service (LS) that can adapt to the new situation is necessary for improving the self-development capability of university logistics in the reform process of socialization, and the measures are as follows: with support from the government sector, to create an external environment; with resource integration as a goal, to create an organizational structure; with market mechanism as a promoter, to the Independent college is a significant innovation of the higher education system, whose method of operation achieves the partnership between resources and social forces in higher education. There are several references in the text for further logistic reform in universities via data mining (DM) algorithms concerning logistic entities and autonomous colleges, which examine the market features and interaction between them. The logistics service data mining (LS-DM) approach plays a critical role in advancing logistic management science while boosting the economy's overall benefits when used with other measures. As a result of the rapid popularization of higher education, new features and models place an even greater demand on logistics management in colleges and universities. Refined management must be advocated and implemented in the new scenario. To apply refined management, you must alter your management philosophy, fine-tune your rules and regulations, enhance performance capabilities, and put mechanisms for monitoring and assessing progress. As a result, logistics management can be continuously improved, students and teachers receive better and more gratifying services, and the scientific growth of colleges and universities may be laid solidly.. The proposed LS-DM system with logistics service, data mining, and machine learning model demonstrates simulation outcomes with an accuracy of 89.7% and a precision of 87.8%, which is greater than the accuracy and precision exhibited by the existing models."
A Digital Collaborative Platform for the Silver Economy: Functionalities Required by Stakeholders in a Multinational Baltic Sea Region Project,"Butt, Sidra Azmat and Suran, Shweta and Pappel, Ingrid and Sm\ae{}rup, Michael and Krimmer, Robert and Draheim, Dirk",10.1145/3592618,2023,"This article develops functionalities for a digital collaborative platform, called the Digital Silver Hub, which aims to serve as an ecosystem for the quadruple helix actors (private sector, public sector, academic institutions, and senior citizens) to participate in knowledge exchange, collaboration, and co-creation of innovative technological solutions to facilitate the elderly population. In service of that, we have conducted 30 interviews from the partner heads of an EU-funded project as well as quadruple helix actors from each region in the Baltic Sea Region (Estonia, Latvia, Finland, Lithuania, Denmark, and St. Petersburg) to deeply understand the functionalities that are needed to be offered by the platform. A deductive thematic analysis has been conducted to analyse these functionalities in terms of collective intelligence (CI) components (namely staffing, processes, goals, and motivation) based on a most recent generic CI model. The functionalities were further evaluated by experts working in the field of science and technology in the silver economy. Overall, this article offers insights into the functionalities that are required for a digital collaborative platform to support the elderly population and facilitate co-creation among quadruple helix actors, as well as provides a foundation for future work on designing and implementing such a platform."
Global Manager: A Serious Game to Raise Awareness of the Challenges of Being a Project Manager in Global Software Development,"Vizca\'{\i}no, Aurora and Garc\'{\i}a, F\'{e}lix O. and Men\'{e}ndez, V\'{\i}ctor Hugo and Manjavacas, Antonio and M\'{a}rquez, Rub\'{e}n and Molina, Marta",10.1145/3592620,2023,"Project managers tend to confront multiple challenges in Global Software Development (GSD), including misunderstandings about the project requirements, complex estimations of costs, risks, and efforts, along with increases in task allocation and a lack of coordination. “Soft skills” play a fundamental role in solving these challenges, as they are the human or social abilities that facilitate the resolution of conflicts and involve dealing with people and managing projects in their most social and creative dimension. However, developing the soft skills that allow project managers to lead in an effective manner is not an easy task, especially in global or distributed environments, where coordination and control problems are commonplace. One suitable mechanism with which to develop these soft skills is that of serious games, which focus on learning by doing and allow users to simulate real scenarios. The authors of this article are aware of the usefulness of this type of teaching method and have, therefore, developed the serious game described herein. This serious game is denominated as Global Manager and is focused on helping both students and novice project managers to become aware of the factors that may affect GSD projects. Global Manager was evaluated by means of a quasi-experiment based on a pretest-posttest design, which was used to assess whether the game can increase the players’ awareness and knowledge of the factors that a project manager should consider in GSD projects. The results obtained after evaluating the game were quite satisfactory, as the game taught the players to differentiate between global and distributed software projects and to experience situations that may and do occur in these settings. Furthermore, Global Manager helped improve players' perceptions of the importance of different influencing factors in GSD."
A Quality Model-based Approach for Measuring User Interface Aesthetics with Grace,"Zen, Mathieu and Burny, Nicolas and Vanderdonckt, Jean",10.1145/3593224,2023,"User interface aesthetics, a particular sub-characteristic of the ISO 25010 software quality model, is correlated to the perceived or actual usability of a graphical user interface, its user experience, and trust. While many measures, such as balance, symmetry, proportion, alignment, regularity, and simplicity, can be computed, no consensus exists today on which measure to adopt, which formula to compute for each measure, and which interpretation to give for each computed formula. To accommodate these variations and to make the assessment explicit and interpretable, we present a quality model-based approach for measuring aesthetics that defines a quality function in terms of the formula of the measures, their weights, their composition, and their overall computation. This quality model is transformed into a configuration used by GRACE, a web application developed for this purpose. When the measures, their formula, their weights, or the quality function change, the quality model changes, which is re-computed to compare it with any other model, thus making the measurement process explicit and interpretable. We apply this approach to a large dataset ""Lab in the Wild'' to investigate the correlations between aesthetic measures and perceived visual complexity and colorfulness. We discuss limitations through threats to validity."
Data Owner Benefit-Driven Design of People Analytics,"Zander, Patrik and Zieglmeier, Valentin",10.1145/3593225,2023,"With increasingly digitalized workplaces, the potential for sophisticated analyses of employee data rises. This increases the relevance of people analytics (PA), which are tools for the behavioral analysis of employees. Despite this potential, the successful usage of PA is hindered by employee concerns. Especially in Europe, where the GDPR or equivalent laws apply, employee consent is required before data can be processed in PA. Therefore, PA can only provide relevant insights if employees are willing to share their data. One potential way of achieving this is the use of appeal strategies. In the design of PA, the core strategy that can be used is the inclusion of data owner benefits, such as automated feedback, that are given to employees in exchange for sharing their own data. In this paper, we examine benefits as an appeal strategy and develop four design principles for the inclusion of benefits in PA. Then, we describe an exemplary set of analyses and benefits, demonstrating how our principles may be put into practice. Based on this exemplary implementation, we describe and discuss the results of a user study (n = 46) among employees in the EU and UK. Our study investigates the factors that foster or hinder employees' consent to sharing their data with PA. Then, we introduce our data owner benefits and analyze whether they can positively influence this consent decision. Our introduced data owner benefits were, contrary to our expectations, not suited to motivate our participants to consent to sharing their data. We therefore analyze how participants judge the benefits. Participants generally appreciate having them, confirming the value of including data owner benefits when designing PA. Some of our introduced benefits negatively influenced participants' sharing decision, though, meaning that careful consideration of potential risks is required when conceptualizing them."
EEG-Based Brain-Computer Interactions in Immersive Virtual and Augmented Reality: A Systematic Review,"Nwagu, Chukwuemeka and AlSlaity, Alaa and Orji, Rita",10.1145/3593226,2023,"Brain-computer interactions allow humans to passively or actively control computer systems using their brain activity. For more than a decade now, these interactions have been implemented and evaluated in immersive virtual environments where they prompt novel means of human interaction with systems. In this paper, we present a systematic review of 76 studies published over the last 10 years that develop and evaluate immersive virtual reality or augmented reality systems with electroencephalography-based interactions. The aim of the review is to summarize and highlight trends in technology design, research methods, current practices, techniques used in systems of this kind, challenges and opportunities that present direction for future research in this area. Our analysis uncovers useful insights, limitations, and highlights of the trends, innovations, and usability and technical challenges at the intersection of brain-computer interfaces and immersive technologies, as well as recommendations for future research."
Developing a Multimodal Classroom Engagement Analysis Dashboard for Higher-Education,"Sabuncuoglu, Alpay and Sezgin, T. Metin",10.1145/3593240,2023,"Developing learning analytics dashboards (LADs) is a growing research interest as online learning tools have become more accessible in K-12 and higher education settings. This paper reports our multimodal classroom engagement data analysis and dashboard design process and the resulting engagement dashboard. Our work stems from the importance of monitoring classroom engagement, which refers to students' active physical and cognitive involvement in learning that influences their motivation and success in a given course. To monitor this vital facade of learning, we developed an engagement dashboard using an iterative and user-centered process. We first created a multimodal machine learning model that utilizes face and pose features obtained from recent deep learning models. Then, we created a dashboard where users can view their engagement over time and discover their learning/teaching patterns. Finally, we conducted user studies with undergraduate and graduate-level participants to obtain feedback on our dashboard design. Our paper makes three contributions by (1) presenting a student-centric, open-source dashboard, (2) demonstrating a baseline architecture for engagement analysis using our open-access data, and (3) presenting user insights and design takeaways to inspire future LADs. We expect our research to guide the development of tools for novice teacher education, student self-evaluation, and engagement evaluation in crowded classrooms."
Automatic Core-Developer Identification on GitHub: A Validation Study,"Bock, Thomas and Alznauer, Nils and Joblin, Mitchell and Apel, Sven",10.1145/3593803,2023,"Many open-source software projects are self-organized and do not maintain official lists with information on developer roles. So, knowing which developers take core and maintainer roles is, despite being relevant, often tacit knowledge. We propose a method to automatically identify core developers based on role permissions of privileged events triggered in GitHub issues and pull requests. In an empirical study on 25/GitHub projects, (1) we validate the set of automatically identified core developers with a sample of project-reported developer lists, and (2) we use our set of identified core developers to assess the accuracy of state-of-the-art unsupervised developer classification methods. Our results indicate that the set of core developers, which we extracted from privileged issue events, is sound and the accuracy of state-of-the-art unsupervised classification methods depends mainly on the data source (commit data versus issue data) rather than the network-construction method (directed versus undirected, etc.). In perspective, our results shall guide research and practice to choose appropriate unsupervised classification methods, and our method can help create reliable ground-truth data for training supervised classification methods."
Real Time Fiducial Marker Localisation System with Full 6 DOF Pose Estimation,"Ulrich, Ji\v{r}\'{\i} and Blaha, Jan and Alsayed, Ahmad and Rou\v{c}ek, Tom\'{a}\v{s} and Arvin, Farshad and Krajn\'{\i}k, Tom\'{a}\v{s}",10.1145/3594264.3594266,2023,"The ability to reliably determine its own position, as well as the position of surrounding objects, is crucial for any autonomous robot. While this can be achieved with a certain degree of reliability, augmenting the environment with artificial markers that make these tasks easier is often practical. This applies especially to the evaluation of robotic experiments, which often require exact ground truth data containing the positions of the robots. This paper proposes a new method for estimating the position and orientation of circular fiducial markers in 3D space. Simulated and real experiments show that our method achieved three times lower localisation error than the method it derived from. The experiments also indicate that our method outperforms state-of-the-art systems in terms of orientation estimation precision while maintaining similar or better accuracy in position estimation. Moreover, our method is computationally efficient, allowing it to detect and localise several markers in a fraction of the time required by the state-of-the-art fiducial markers. Furthermore, the presented method requires only an off-the-shelf camera and printed tags, can be quickly set up and works in natural light conditions outdoors. These properties make it a viable alternative to expensive high-end localisation systems."
Urdu Speech Emotion Recognition: A Systematic Literature Review,"Taj, Soonh and Mujtaba, Ghulam and Daudpota, Sher Muhammad and Mughal, Muhammad Hussain",10.1145/3595377,2023,"Research on Speech Emotion Recognition is becoming more mature day by day, and a lot of research is being carried out on Speech Emotion Recognition in resource-rich languages like English, German, French, and Chinese. Urdu is among the top 10 languages spoken worldwide. Despite its importance, few studies have worked on Urdu Speech emotion as Urdu is recognized as a resource-poor language. The Urdu language lacks publicly available datasets, and for this reason, few researchers have worked on Urdu Speech Emotion Recognition. To the best of our knowledge, no review has been found on Urdu Speech Emotion recognition. This study is the first systematic literature review on Urdu Speech Emotion Recognition, and the primary goal of this study is to provide a detailed analysis of the literature on Urdu Speech Emotion Recognition which includes the datasets, features, pre-processing, approaches, performance metrics, and validation methods used for Urdu Speech Emotion Recognition. This study also highlights the challenges and future directions for Urdu Speech Emotion Recognition."
Exploring the Potential of Cyber Manufacturing System in the Digital Age,"Ahmed, Usman and Lin, Jerry Chun-Wei and Srivastava, Gautam",10.1145/3596602,2023,"Cyber-manufacturing Systems (CMS) have been growing in popularity, transitioning from conventional manufacturing to an innovative paradigm that emphasizes innovation, automation, better customer service, and intelligent systems. A new manufacturing model can improve efficiency and productivity, and provide better customer service and response times. In addition, it may revolutionize the way products are produced, from design to completion. Therefore, it is likely that this new manufacturing model will become increasingly popular. By building new technologies on top of existing CMS, these systems will ensure that data exchange and integration between decentralized systems are reliable and secure. Recently published case studies from industry and the literature support this claim; some challenges remain to be overcome. In general, the use of CMS can revolutionize the manufacturing industry. This study comprehensively analyzes these systems and their potential applications and implications. An overview of the field is then given and various aspects of CMS are also explored with more details. A taxonomy of the most common and current approaches to CMS is presented, including networked cyber-manufacturing systems, distributed cyber-manufacturing systems, cloud-based cyber-manufacturing systems, and cyber-physical systems (CPS). Furthermore, our survey identifies several popular open-source software and datasets and discusses how these resources can reduce barriers to CMS research. In addition, we identify several important issues and research opportunities associated with CMS, including better integration between hardware and software, improved security and privacy protocols, communication protocols, and improved data management systems. In summary, this paper presents a comprehensive overview of current technology and valuable insights are provided for the potential impact of CMS on society and industry."
CodeEditor: Learning to Edit Source Code with Pre-trained Models,"Li, Jia and Li, Ge and Li, Zhuo and Jin, Zhi and Hu, Xing and Zhang, Kechi and Fu, Zhiyi",10.1145/3597207,2023,"Developers often perform repetitive code editing activities (up to 70%) for various reasons (e.g., code refactoring) during software development. Many deep learning (DL) models have been proposed to automate code editing by learning from the code editing history. Among DL-based models, pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for automatic code editing.In this article, we propose a novel pre-training task specialized in code editing and present an effective pre-trained code editing model named CodeEditor. Compared to previous code infilling tasks, our pre-training task further improves the performance and generalization ability of code editing models. Specifically, we collect lots of real-world code snippets as the ground truth and use a powerful generator to rewrite them into mutated versions. Then, we pre-train our CodeEditor to edit mutated versions into the corresponding ground truth, to learn edit patterns. We conduct experiments on four code editing datasets and evaluate the pre-trained CodeEditor in three settings (i.e., fine-tuning, few-shot, and zero-shot). (1) In the fine-tuning setting, we train the pre-trained CodeEditor with four datasets and evaluate it on the test data. CodeEditor outperforms the SOTA baselines by 15%, 25.5%, 9.4%, and 26.6% on four datasets. (2) In the few-shot setting, we train the pre-trained CodeEditor with limited data and evaluate it on the test data. CodeEditor substantially performs better than all baselines, even outperforming baselines that are fine-tuned with all data. (3) In the zero-shot setting, we evaluate the pre-trained CodeEditor on the test data without training. CodeEditor correctly edits 1,113 programs, while the SOTA baselines cannot work. The results show that the superiority of our pre-training task and the pre-trained CodeEditor is more effective in automatic code editing."
An Empirical Study on GitHub Pull Requests’ Reactions,"Batoun, Mohamed Amine and Yung, Ka Lai and Tian, Yuan and Sayagh, Mohammed",10.1145/3597208,2023,"The pull request mechanism is commonly used to propose source code modifications and get feedback from the community before merging them into a software repository. On GitHub, practitioners can provide feedback on a pull request by either commenting on the pull request or simply reacting to it using a set of pre-defined GitHub reactions, i.e., “Thumbs-up”, “Laugh”, “Hooray”, “Heart”, “Rocket”, “Thumbs-down”, “Confused”, and “Eyes”. While a large number of prior studies investigated how to improve different software engineering activities (e.g., code review and integration) by investigating the feedback on pull requests, they focused only on pull requests’ comments as a source of feedback. However, the GitHub reactions, according to our preliminary study, contain feedback that is not manifested within the comments of pull requests. In fact, our preliminary analysis of six popular projects shows that a median of 100% of the practitioners who reacted to a pull request did not leave any comment suggesting that reactions can be a unique source of feedback to further improve the code review and integration process.To help future studies better leverage reactions as a feedback mechanism, we conduct an empirical study to understand the usage of GitHub reactions and understand their promises and limitations. We investigate in this article how reactions are used, when and who use them on what types of pull requests, and for what purposes. Our study considers a quantitative analysis on a set of 380 k reactions on 63 k pull requests of six popular open-source projects on GitHub and three qualitative analyses on a total number of 989 reactions from the same six projects. We find that the most common used GitHub reactions are the positive ones (i.e., “Thumbs-up”, “Hooray”, “Heart”, “Rocket”, and “Laugh”). We observe that reactors use positive reactions to express positive attitude (e.g., approval, appreciation, and excitement) on the proposed changes in pull requests. A median of just 1.95% of the used reactions are negative ones, which are used by reactors who disagree with the proposed changes for six reasons, such as feature modifications that might have more downsides than upsides or the use of the wrong approach to address certain problems. Most (a median of 78.40%) reactions on a pull request come before the closing of the corresponding pull requests. Interestingly, we observe that non-contributors (i.e., outsiders who potentially are the “end-users” of the software) are also active on reacting to pull requests. On top of that, we observe that core contributors, peripheral contributors, casual contributors and outsiders have different behaviors when reacting to pull requests. For instance, most core contributors react in the early stages of a pull request, while peripheral contributors, casual contributors and outsiders react around the closing time or, in some cases, after a pull request is merged. Contributors tend to react to the pull request’s source code, while outsiders are more concerned about the impact of the pull request on the end-user experience. Our findings shed light on common patterns of GitHub reactions usage on pull requests and provide taxonomies about the intention of reactors, which can inspire future studies better leverage pull requests’ reactions."
Evolving Software: Combining Online Learning with Mutation-Based Stochastic Search,"Renzullo, Joseph and Weimer, Westley and Forrest, Stephanie",10.1145/3597617,2023,"Evolutionary algorithms and related mutation-based methods have been used in software engineering, with recent emphasis on the problem of repairing bugs. In this work, programs are typically not synthesized from a random start. Instead, existing solutions—which may be flawed or inefficient—are taken as starting points, with the evolutionary process searching for useful improvements. This approach, however, introduces a challenge for the search algorithm: what is the optimal number of neutral mutations that should be combined? Too much is likely to introduce errors and break the program while too little hampers the search process, inducing the classic tradeoff between exploration and exploitation.In the context of software improvement, this work considers MWRepair, an algorithm for enhancing mutation-based searches, which uses online learning to optimize the tradeoff between exploration and exploitation. The aggressiveness parameter governs how many individual mutations should be applied simultaneously to an individual between fitness evaluations. MWRepair is evaluated in the context of automated program repair problems, where the goal is repairing software bugs with minimal human involvement. The article analyzes the search space for automated program repair induced by neutral mutations, finding that the greatest probability of finding successful repairs often occurs when many neutral mutations are applied to the original program. Moreover, repair probability follows a characteristic, unimodal distribution. MWRepair uses online learning to leverage this property, finding both rare and multi-edit repairs to defects in the popular Defects4J benchmark set of buggy Java programs."
HUSP-SP: Faster Utility Mining on Sequence Data,"Zhang, Chunkai and Yang, Yuting and Du, Zilin and Gan, Wensheng and Yu, Philip S.",10.1145/3597935,2023,"High-utility sequential pattern mining (HUSPM) has emerged as an important topic due to its wide application and considerable popularity. However, due to the combinatorial explosion of the search space when the HUSPM problem encounters a low-utility threshold or large-scale data, it may be time-consuming and memory-costly to address the HUSPM problem. Several algorithms have been proposed for addressing this problem, but they still cost a lot in terms of running time and memory usage. In this article, to further solve this problem efficiently, we design a compact structure called sequence projection (seqPro) and propose an efficient algorithm, namely, discovering high-utility sequential patterns with the seqPro structure (HUSP-SP). HUSP-SP utilizes the compact seq-array to store the necessary information in a sequence database. The seqPro structure is designed to efficiently calculate candidate patterns’ utilities and upper-bound values. Furthermore, a new upper bound on utility, namely, tighter reduced sequence utility and two pruning strategies in search space, are utilized to improve the mining performance of HUSP-SP. Experimental results on both synthetic and real-life datasets show that HUSP-SP can significantly outperform the state-of-the-art algorithms in terms of running time, memory usage, search space pruning efficiency, and scalability."
An Accurate Identifier Renaming Prediction and Suggestion Approach,"Zhang, Jingxuan and Luo, Junpeng and Liang, Jiahui and Gong, Lina and Huang, Zhiqiu",10.1145/3603109,2023,"Identifiers play an important role in helping developers analyze and comprehend source code. However, many identifiers exist that are inconsistent with the corresponding code conventions or semantic functions, leading to flawed identifiers. Hence, identifiers need to be renamed regularly. Even though researchers have proposed several approaches to identify identifiers that need renaming and further suggest correct identifiers for them, these approaches only focus on a single or a limited number of granularities of identifiers without universally considering all the granularities and suggest a series of sub-tokens for composing identifiers without completely generating new identifiers. In this article, we propose a novel identifier renaming prediction and suggestion approach. Specifically, given a set of training source code, we first extract all the identifiers in multiple granularities. Then, we design and extract five groups of features from identifiers to capture inherent properties of identifiers themselves and the relationships between identifiers and code conventions, as well as other related code entities, enclosing files, and change history. By parsing the change history of identifiers, we can figure out whether specific identifiers have been renamed or not. These identifier features and their renaming history are used to train a Random Forest classifier, which can be further used to predict whether a given new identifier needs to be renamed or not. Subsequently, for the identifiers that need renaming, we extract all the related code entities and their renaming change history. Based on the intuition that identifiers are co-evolved as their relevant code entities with similar patterns and renaming sequences, we could suggest and recommend a series of new identifiers for those identifiers. We conduct extensive experiments to validate our approach in both the Java projects and the Android projects. Experimental results demonstrate that our approach could identify identifiers that need renaming with an average F-measure of more than 89%, which outperforms the state-of-the-art approach by 8.30% in the Java projects and 21.38% in the Android projects. In addition, our approach achieves a Hit@10 of 48.58% and 40.97% in the Java and Android projects in suggesting correct identifiers and outperforms the state-of-the-art approach by 29.62% and 15.75%, respectively."
Dependency Update Strategies and Package Characteristics,"Javan Jafari, Abbas and Costa, Diego Elias and Shihab, Emad and Abdalkareem, Rabe",10.1145/3603110,2023,"Managing project dependencies is a key maintenance issue in software development. Developers need to choose an update strategy that allows them to receive important updates and fixes while protecting them from breaking changes. Semantic Versioning was proposed to address this dilemma, but many have opted for more restrictive or permissive alternatives. This empirical study explores the association between package characteristics and the dependency update strategy selected by its dependents to understand how developers select and change their update strategies. We study over 112,000 Node Package Manager (npm) packages and use 19 characteristics to build a prediction model that identifies the common dependency update strategy for each package. Our model achieves a minimum improvement of 72% over the baselines and is much better aligned with community decisions than the npm default strategy. We investigate how different package characteristics can influence the predicted update strategy and find that dependent count, age, and release status to be the highest influencing features. We complement the work with qualitative analyses of 160 packages to investigate the evolution of update strategies. While the common update strategy remains consistent for many packages, certain events such as the release of the 1.0.0 version or breaking changes influence the selected update strategy over time."
Deep Learning-based Human Pose Estimation: A Survey,"Zheng, Ce and Wu, Wenhan and Chen, Chen and Yang, Taojiannan and Zhu, Sijie and Shen, Ju and Kehtarnavaz, Nasser and Shah, Mubarak",10.1145/3603618,2023,"Human pose estimation aims to locate the human body parts and build human body representation (e.g., body skeleton) from input data such as images and videos. It has drawn increasing attention during the past decade and has been utilized in a wide range of applications including human-computer interaction, motion analysis, augmented reality, and virtual reality. Although the recently developed deep learning-based solutions have achieved high performance in human pose estimation, there still remain challenges due to insufficient training data, depth ambiguities, and occlusion. The goal of this survey article is to provide a comprehensive review of recent deep learning-based solutions for both 2D and 3D pose estimation via a systematic analysis and comparison of these solutions based on their input data and inference procedures. More than 260 research papers since 2014 are covered in this survey. Furthermore, 2D and 3D human pose estimation datasets and evaluation metrics are included. Quantitative performance comparisons of the reviewed methods on popular datasets are summarized and discussed. Finally, the challenges involved, applications, and future research directions are concluded. A regularly updated project page is provided: ."
Big Code Search: A Bibliography,"Kim, Kisub and Ghatpande, Sankalp and Kim, Dongsun and Zhou, Xin and Liu, Kui and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques and Le Traon, Yves",10.1145/3604905,2023,"Code search is an essential task in software development. Developers often search the internet and other code databases for necessary source code snippets to ease the development efforts. Code search techniques also help learn programming as novice programmers or students can quickly retrieve (hopefully good) examples already used in actual software projects. Given the recurrence of the code search activity in software development, there is an increasing interest in the research community. To improve the code search experience, the research community suggests many code search tools and techniques. These tools and techniques leverage several different ideas and claim a better code search performance. However, it is still challenging to illustrate a comprehensive view of the field considering that existing studies generally explore narrow and limited subsets of used components. This study aims to devise a grounded approach to understanding the procedure for code search and build an operational taxonomy capturing the critical facets of code search techniques. Additionally, we investigate evaluation methods, benchmarks, and datasets used in the field of code search."
A Systematic Review of Automated Query Reformulations in Source Code Search,"Rahman, Mohammad Masudur and Roy, Chanchal K.",10.1145/3607179,2023,"Fixing software bugs and adding new features are two of the major maintenance tasks. Software bugs and features are reported as change requests. Developers consult these requests and often choose a few keywords from them as an ad hoc query. Then they execute the query with a search engine to find the exact locations within software code that need to be changed. Unfortunately, even experienced developers often fail to choose appropriate queries, which leads to costly trials and errors during a code search. Over the years, many studies have attempted to reformulate the ad hoc queries from developers to support them. In this systematic literature review, we carefully select 70 primary studies on query reformulations from 2,970 candidate studies, perform an in-depth qualitative analysis (e.g., Grounded Theory), and then answer seven research questions with major findings. First, to date, eight major methodologies (e.g., term weighting, term co-occurrence analysis, thesaurus lookup) have been adopted to reformulate queries. Second, the existing studies suffer from several major limitations (e.g., lack of generalizability, the vocabulary mismatch problem, subjective bias) that might prevent their wide adoption. Finally, we discuss the best practices and future opportunities to advance the state of research in search query reformulations."
What Constitutes the Deployment and Runtime Configuration System? An Empirical Study on OpenStack Projects,"Bessghaier, Narjes and Sayagh, Mohammed and Ouni, Ali and Mkaouer, Mohamed Wiem",10.1145/3607186,2023,"Modern software systems are designed to be deployed in different configured environments (e.g., permissions, virtual resources, network connections) and adapted at runtime to different situations (e.g., memory limits, enabling/disabling features, database credentials). Such a configuration during the deployment and runtime of a software system is implemented via a set of configuration files, which together constitute what we refer to as a “configuration system.” Recent research efforts investigated the evolution and maintenance of configuration files. However, they merely focused on a limited part of the configuration system (e.g., specific infrastructure configuration files or Dockerfiles), and their results do not generalize to the whole configuration system. To cope with such a limitation, we aim to better capture and understand what files constitute a configuration system. To do so, we leverage an open card sort technique to qualitatively study 1,756 configuration files from OpenStack, a large and widely studied open source software ecosystem. Our investigation reveals the existence of nine types of configuration files, which cover the creation of the infrastructure on top of which OpenStack will be deployed, along with other types of configuration files used to customize OpenStack after its deployment. These configuration files are interconnected while being used at different deployment stages. For instance, we observe specific configuration files used during the deployment stage to create other configuration files that are used in the runtime stage. We also observe that identifying and classifying these types of files is not straightforward, as five out of the nine types can be written in similar programming languages (e.g., Python and Bash) as regular source code files. We also found that the same file extensions (e.g., Yaml) can be used for different configuration types, making it difficult to identify and classify configuration files. Thus, we first leverage a machine learning model to identify configuration from non-configuration files, which achieved a median area under the curve (AUC) of 0.91, a median Brier score of 0.12, a median precision of 0.86, and a median recall of 0.83. Thereafter, we leverage a multi-class classification model to classify configuration files based on the nine configuration types. Our multi-class classification model achieved a median weighted AUC of 0.92, a median Brier score of 0.04, a median weighted precision of 0.84, and a median weighted recall of 0.82. Our analysis also shows that with only 100 labeled configuration and non-configuration files, our model reached a median AUC higher than 0.69. Furthermore, our configuration model requires a minimum of 100 configuration files to reach a median weighted AUC higher than 0.75."
Metaphors in Voice User Interfaces: A Slippery Fish,"Desai, Smit and Twidale, Michael",10.1145/3609326,2023,"We explore a range of different metaphors used for Voice User Interfaces (VUIs) by designers, end-users, manufacturers, and researchers using a novel framework derived from semi-structured interviews and a literature review. We focus less on the well-established idea of metaphors as a way for interface designers to help novice users learn how to interact with novel technology, and more on other ways metaphors can be used. We find that metaphors people use are contextually fluid, can change with the mode of conversation, and can reveal differences in how people perceive VUIs compared to other devices. Not all metaphors are helpful, and some may be offensive. Analyzing this broader class of metaphors can help understand, perhaps even predict problems. Metaphor analysis can be a low-cost tool to inspire design creativity and facilitate complex discussions about sociotechnical issues, enabling us to spot potential opportunities and problems in the situated use of technologies."
How Do Users Experience Moderation?: A Systematic Literature Review,"Ma, Renkai and You, Yue and Gui, Xinning and Kou, Yubo",10.1145/3610069,2023,"Researchers across various fields have investigated how users experience moderation through different perspectives and methodologies. At present, there is a pressing need of synthesizing and extracting key insights from prior literature to formulate a systematic understanding of what constitutes a moderation experience and to explore how such understanding could further inform moderation-related research and practices. To answer this question, we conducted a systematic literature review (SLR) by analyzing 42 empirical studies related to moderation experiences and published between January 2016 and March 2022. We describe these studies' characteristics and how they characterize users' moderation experiences. We further identify five primary perspectives that prior researchers use to conceptualize moderation experiences. These findings suggest an expansive scope of research interests in understanding moderation experiences and considering moderated users as an important stakeholder group to reflect on current moderation design but also pertain to the dominance of the punitive, solutionist logic in moderation and ample implications for future moderation research, design, and practice."
Knowing Unknown Teammates: Exploring Anonymity and Explanations in a Teammate Information-Sharing Recommender System,"Musick, Geoff and Gilman, Elizabeth S. and Duan, Wen and McNeese, Nathan J. and Knijnenburg, Bart and O'Neill, Thomas",10.1145/3610075,2023,"A growing organizational trend is to utilize ad-hoc team formation which allows for teams to intentionally form based on the member skills required to accomplish a specific task. Due to the unfamiliar nature of these teams, teammates are often limited by their understanding of one another (e.g., teammate preferences, tendencies, attitudes) which limits the team's functioning and efficiency. This study conceptualizes and investigates the use of a teammate information-sharing recommender system which selectively shares interpersonal recommendations between unfamiliar teammates (e.g., ""Your voice may be overshadowed by this teammate when making decisions..."") to promote teammate understanding. Through a mixed-methods approach involving 105 participants working on actual unfamiliar teams, this study explores how presentation elements such as anonymity and explanations influence system perceptions and how anonymity influences team outcomes. Results indicate that anonymizing recommendations was associated with worse team measures, particularly team satisfaction and team cohesion. Qualitative results shed light on why team members perceived privacy concerns and team benefits associated with using the system. We contribute to CSCW through a better understanding of how to support unfamiliar teams, the conceptualization and empirical investigation of a novel teammate information-sharing recommender system, and foundational design recommendations associated with such a system."
"""Why do you need 400 photographs of 400 different Lockheed Constellation?"": Value Expressions by Contributors and Users of Wikimedia Commons","Yu, Yihan and McDonald, David W.",10.1145/3610094,2023,"Understanding the values that collaborators bring to a collaboration is important for the design of new systems. In collaborative systems understanding differing values could help design solutions to mitigate conflicts and more effectively coordinate collaboration. We review prior studies of Commons-Based Peer Production (CBPP) identifying four common value dimensions previously noted as present in CBPP: usage value, social value, ideological value, and monetary value. We use this synthetic framework to analyze a dataset of 32 interviews with contributors to Wikimedia Commons and editors of Wikipedia who use Commons resources. Our analysis supports the prior values categories while expanding how some dimensions are expressed by participants. We also highlight four additional value dimensions that were not previously identified in CBPP: cultural heritage value, rarity value, aesthetic value, and administrative value. We discuss the implications of our findings for the design of collaborative systems."
"Co-creating a Transdisciplinary Map of Technology-mediated Harms, Risks and Vulnerabilities: Challenges, Ambivalences and Opportunities","Dom\'{\i}nguez Hern\'{a}ndez, Andr\'{e}s and Ramokapane, Kopo M. and Das Chowdhury, Partha and Michalec, Ola and Johnstone, Emily and Godwin, Emily and Cork, Alicia G. and Rashid, Awais",10.1145/3610179,2023,"The phrase ""online harms'' has emerged in recent years out of a growing political willingness to address the ethical and social issues associated with the use of the Internet and digital technology at large. The broad landscape that surrounds online harms gathers a multitude of disciplinary, sectoral and organizational efforts while raising myriad challenges and opportunities for the crossing entrenched boundaries. In this paper we draw lessons from a journey of co-creating a transdisciplinary knowledge infrastructure within a large research initiative animated by the online harms agenda. We begin with a reflection of the implications of mapping, taxonomizing and constructing knowledge infrastructures and a brief review of how online harm and adjacent themes have been theorized and classified in the literature to date. Grounded on our own experience of co-creating a map of online harms, we then argue that the map---and the process of mapping---perform three mutually constitutive functions, acting simultaneously as method, medium and provocation. We draw lessons from how an open-ended approach to mapping, despite not guaranteeing consensus, can foster productive debate and collaboration in ethically and politically fraught areas of research. We end with a call for CSCW research to surface and engage with the multiple temporalities, social lives and political sensibilities of knowledge infrastructures."
"Sensing Wellbeing in the Workplace, Why and For Whom? Envisioning Impacts with Organizational Stakeholders","Kawakami, Anna and Chowdhary, Shreya and Iqbal, Shamsi T. and Liao, Q. Vera and Olteanu, Alexandra and Suh, Jina and Saha, Koustuv",10.1145/3610207,2023,"With the heightened digitization of the workplace, alongside the rise of remote and hybrid work prompted by the pandemic, there is growing corporate interest in using passive sensing technologies for workplace wellbeing. Existing research on these technologies often focus on understanding or improving interactions between an individual user and the technology. Workplace settings can, however, introduce a range of complexities that challenge the potential impact and in-practice desirability of wellbeing sensing technologies. Today, there is an inadequate empirical understanding of how everyday workers---including those who are impacted by, and impact the deployment of workplace technologies--envision its broader socio-ecological impacts. In this study, we conduct storyboard-driven interviews with 33 participants across three stakeholder groups: organizational governors, AI builders, and worker data subjects. Overall, our findings surface how workers envisioned wellbeing sensing technologies may lead to cascading impacts on their broader organizational culture, interpersonal relationships with colleagues, and individual day-to-day lives. Participants anticipated harms arising from ambiguity and misalignment around scaled notions of ""worker wellbeing,'' underlying technical limitations to workplace-situated sensing, and assumptions regarding how social structures and relationships may shape the impacts and use of these technologies. Based on our findings, we discuss implications for designing worker-centered data-driven wellbeing technologies."
Understanding Design Collaboration Between Designers and Artificial Intelligence: A Systematic Literature Review,"Shi, Yang and Gao, Tian and Jiao, Xiaohan and Cao, Nan",10.1145/3610217,2023,"Recent interest in design through the artificial intelligence (AI) lens is rapidly increasing. Designers, as a special user group interacting with AI, have received more attention in the Human-Computer Interaction community. Prior work has discussed emerging challenges that persist in designing for AI. However, few systematic reviews focus on AI for design to understand how designers and AI can augment each other's complementary strengths in design collaboration. In this work, we conducted a landscape analysis of AI for design, via a systematic literature review of 93 papers. The analysis first provides a bird's eye view of overall patterns in this area. The analysis also reveals three themes interpreted from the paper corpus associated with AI for design, including AI assisting designers, designers assisting AI, and characterizing designer-AI collaboration. We discuss the implications of our findings and suggested methodological proposals to guide HCI toward research and practices that center on collaborative creativity."
SelfSeg: A Self-supervised Sub-word Segmentation Method for Neural Machine Translation,"Song, Haiyue and Dabre, Raj and Chu, Chenhui and Kurohashi, Sadao and Sumita, Eiichiro",10.1145/3610611,2023,"Sub-word segmentation is an essential pre-processing step for Neural Machine Translation (NMT). Existing work has shown that neural sub-word segmenters are better than Byte-Pair Encoding (BPE), however, they are inefficient, as they require parallel corpora, days to train, and hours to decode. This article introduces SelfSeg, a self-supervised neural sub-word segmentation method that is much faster to train/decode and requires only monolingual dictionaries instead of parallel corpora. SelfSeg takes as input a word in the form of a partially masked character sequence, optimizes the word generation probability, and generates the segmentation with the maximum posterior probability, which is calculated using a dynamic programming algorithm. The training time of SelfSeg depends on word frequencies, and we explore several word frequency normalization strategies to accelerate the training phase. Additionally, we propose a regularization mechanism that allows the segmenter to generate various segmentations for one word. To show the effectiveness of our approach, we conduct MT experiments in low-, middle-, and high-resource scenarios, where we compare the performance of using different segmentation methods. The experimental results demonstrate that, on the low-resource ALT dataset, our method achieves more than 1.2 BLEU score improvement compared with BPE and SentencePiece, and a 1.1 score improvement over Dynamic Programming Encoding (DPE) and Vocabulary Learning via Optimal Transport (VOLT), on average. The regularization method achieves approximately a 4.3 BLEU score improvement over BPE and a 1.2 BLEU score improvement over BPE-dropout, the regularized version of BPE. We also observed significant improvements on IWSLT15 Vi→En, WMT16 Ro→En, and WMT15 Fi→En datasets and competitive results on the WMT14 De→En and WMT14 Fr→En datasets. Furthermore, our method is 17.8\texttimes{} faster during training and up to 36.8\texttimes{} faster during decoding in a high-resource scenario compared to DPE. We provide extensive analysis, including why monolingual word-level data is enough to train SelfSeg."
Extensions of Fuzzy Cognitive Maps: A Systematic Review,"Schuerkamp, Ryan and Giabbanelli, Philippe J.",10.1145/3610771,2023,"Fuzzy Cognitive Maps (FCMs) are widely used to simulate complex systems. However, they cannot handle nonlinear relationships or time delays/lags, nor can they fully represent uncertain information, which prompted the development of extended FCMs. The latest review covered extensions up to 2010. We search for extensions from 2011 to March 2023 and assess their motivations, features, operationalizations, use cases, reproducibility, and evaluation to support modelers in reusing existing solutions. We reviewed 26 extensions and found a paucity of extensions addressing multiple limitations, and none of the extensions provided code, hindering modelers in reusing existing extensions while suggesting future work."
Playing with Emotions: A Systematic Review Examining Emotions and Emotion Regulation in Esports Performance,"Beres, Nicole A. and Klarkowski, Madison and Mandryk, Regan L.",10.1145/3611041,2023,"The massive growth of esports has vitalized the need to study human performance in competitive video gaming. The pressure of competitive play elicits a range of emotional experiences, which can affect players during and beyond a gaming session. In this work, we review the state of the literature concerning the role emotions play in esports performance as well as highlight coping strategies players use to regulate emotions during competitive play. We review the findings of N=32 peer-reviewed articles pertaining to emotions and esports, finding that the emotional experiences elicited by competitive play affect esports performance. In response, players attempt to regulate their emotions to maintain performance; however, efforts to do so vary, as they currently lack effective coping strategies. Lastly, we review the potential of technical interventions in esports training for improving emotion regulation among players. Our findings support knowledge development in esports, and present avenues towards promoting the emotional wellbeing of competitive gamers."
Let’s Play Together through Channels: Understanding the Practices and Experience of Danmaku Participation Game Players in China,"Wang, PiaoHong and Lu, Zhicong",10.1145/3611059,2023,"Live streaming is becoming increasingly popular in recent years, as most channels prioritize the delivery of engaging content to their viewers. Among various live streaming channels, Danmaku participation game (DPG) has emerged in China as a mixture of live streaming and online gaming, offering an immersive gaming experience to players. Although prior research has explored audience participation games (APGs) in North America and Europe, it primarily focuses on discussing prototypes and lacks observation of players in natural settings. Little is known about how players perceive DPGs and their player experience. To fill the research gap, we observed a series of DPG channels and conducted an interview-based study to gain insights into the practices and experiences of DPG players. Our work reveals that DPGs can effectively synergize live streaming and online games, amplifying both player engagement and a profound sense of accomplishment to players."
The Business Impact of Inner Source and How to Quantify It,"Buchner, Stefan and Riehle, Dirk",10.1145/3611648,2023,"Inner source software development is the practice of using open source practices for firm-internal software development. Practitioner reports have shown that inner source can increase flexibility and reduce costs. Despite the potential benefits of inner source, there has been little research on its impact on businesses and their processes. To address this gap, we conducted a systematic literature review that identified which business processes are affected by inner source development, particularly within the accounting and management domain. Our review revealed the need for new dedicated community building processes within companies. In addition, we examined computational tools and techniques that can be used to measure inner source development. We found that existing tools and techniques are insufficiently suitable to manage inner source processes. Based on this, we propose research topics for future work on quantifying inner source."
Artificial Intelligence Applied to Software Testing: A Tertiary Study,"Amalfitano, Domenico and Faralli, Stefano and Hauck, Jean Carlo Rossa and Matalonga, Santiago and Distante, Damiano",10.1145/3616372,2023,"Context: Artificial intelligence (AI) methods and models have extensively been applied to support different phases of the software development lifecycle, including software testing (ST). Several secondary studies investigated the interplay between AI and ST but restricted the scope of the research to specific domains or sub-domains within either area.Objective: This research aims to explore the overall contribution of AI to ST, while identifying the most popular applications and potential paths for future research directions.Method: We executed a tertiary study following well-established guidelines for conducting systematic literature mappings in software engineering and for answering nine research questions.Results: We identified and analyzed 20 relevant secondary studies. The analysis was performed by drawing from well-recognized AI and ST taxonomies and mapping the selected studies according to them. The resulting mapping and discussions provide extensive and detailed information on the interplay between AI and ST.Conclusion: The application of AI to support ST is a well-consolidated and growing interest research topic. The mapping resulting from our study can be used by researchers to identify opportunities for future research, and by practitioners looking for evidence-based information on which AI-supported technology to possibly adopt in their testing processes."
Acrobats and Safety Nets: Problematizing Large-Scale Agile Software Development,"Rolland, Knut H. and Fitzgerald, Brian and Dings\o{}yr, Torgeir and Stol, Klaas-Jan",10.1145/3617169,2023,"Agile development methods have become a standard in the software industry, including in large-scale projects. These methods share a set of underlying assumptions that distinguish them from more traditional plan-driven approaches. In this article, we adopt Alvesson and Sandberg's problematization approach to challenge three key assumptions that are prevalent in the large-scale agile literature: (1) agile and plan-driven methods are mutually exclusive; (2) self-managing and hierarchically organized teams are mutually exclusive; and (3) agile methods can scale through simple linear composition. Using a longitudinal case study of large-scale agile development, we describe a series of trigger events and episodes whereby the agile approach was tailored to address the needs of the large-scale development context, which was very much at odds with these fundamental assumptions. We develop a set of new underlying assumptions which suggest that agile and plan-driven practices are mutually enabling and necessary for coordination and scaling in large-scale agile projects. We develop nine propositions for large-scale agile projects based on these new alternative underlying assumptions. Finally, we summarize our theoretical contribution in a generic process model of continuously adjusting agile and plan-driven practices in order to accommodate process challenges in large-scale agile projects."
Blueprints: Systematizing Behavior Change Designs—The Case of Social Comparison Theory,"De Vries, Roelof A. J. and Lemke, Mailin and Ludden, Geke D. S.",10.1145/3617364,2023,"To improve people’s lives, human-computer interaction researchers are increasingly designing technological solutions based on behavior change theory, such as social comparison theory (SCT). However, how researchers operationalize such a theory as a design remains largely unclear. One way to clarify this methodological step is to clearly state which functional elements of a design are aimed at operationalizing a specific behavior change theory construct to evaluate if such aims were successful. In this article, we investigate how the operationalization of functional elements of theories and designs can be more easily conveyed. First, we present a scoping review of the literature to determine the state of operationalizations of SCT as behavior change designs. Second, we introduce a new tool to facilitate the operationalization process. We term the tool blueprints. A blueprint explicates essential functional elements of a behavior change theory by describing it in relation to necessary and sufficient building blocks incorporated in a design. We describe the process of developing a blueprint for SCT. Last, we illustrate how the blueprint can be used during the design refinement and reflection process."
Joint UV Optimization and Texture Baking,"Knodt, Julian and Pan, Zherong and Wu, Kui and Gao, Xifeng",10.1145/3617683,2023,"Level of detail has been widely used in interactive computer graphics. In current industrial 3D modeling pipelines, artists rely on commercial software to generate highly detailed models with UV maps and then bake textures for low-poly counterparts. In these pipelines, each step is performed separately, leading to unsatisfactory visual appearances for low polygon count models. Moreover, existing texture baking techniques assume the low-poly mesh has a small geometric difference from the high-poly, which is often not true in practice, especially with extremely low poly count models.To alleviate the visual discrepancy of the low-poly mesh, we propose to jointly optimize UV mappings during texture baking, allowing for low-poly models to faithfully replicate the appearance of the high-poly even with large geometric differences. We formulate the optimization within a differentiable rendering framework, allowing the automatic adjustment of texture regions to encode appearance information. To compensate for view parallax when two meshes have large geometric differences, we introduce a spherical harmonic parallax mapping, which uses spherical harmonic functions to modulate per-texel UV coordinates based on the view direction. We evaluate the effectiveness and robustness of our approach on a dataset composed of online downloaded models, with varying complexities and geometric discrepancies. Our method achieves superior quality over state-of-the-art techniques and commercial solutions."
Bolstering the Persistence of Black Students in Undergraduate Computer Science Programs: A Systematic Mapping Study,"Belle, Alvine B. and Sutherland, Callum and Adesina, Opeyemi O. and Kpodjedo, S\`{e}gla and Ojong, Nathanael and Cole, Lisa",10.1145/3617896,2023,"Background: People who are racialized, gendered, or otherwise minoritized are underrepresented in computing professions in North America. This is reflected in undergraduate computer science (CS) programs, in which students from marginalized backgrounds continue to experience inequities that do not typically affect White cis-men. This is especially true for Black students in general, and Black women in particular, whose experience of systemic, anti-Black racism compromises their ability to persist and thrive in CS education contexts.Objectives: This systematic mapping study endeavours to (1) determine the quantity of existing non-deficit-based studies concerned with the persistence of Black students in undergraduate CS; (2) summarize the findings and recommendations in those studies; and (3) identify areas in which additional studies may be required. We aim to accomplish these objectives by way of two research questions: (RQ1) What factors are associated with Black students’ persistence in undergraduate CS programs?; and (RQ2) What recommendations have been made to further bolster Black students’ persistence in undergraduate CS education programs?Methods: This systematic mapping study was conducted in accordance with PRISMA 2020 and SEGRESS guidelines. Studies were identified by conducting keyword searches in seven databases. Inclusion and exclusion criteria were designed to capture studies illuminating persistence factors for Black students in undergraduate CS programs. To ensure the completeness of our search results, we engaged in snowballing and an expert-based search to identify additional studies of interest. Finally, data were collected from each study to address the research questions outlined above.Results: Using the methods outlined above, we identified 16 empirical studies, including qualitative, quantitative, and mixed-methods studies informed by a range of theoretical frameworks. Based on data collected from the primary studies in our sample, we identified 13 persistence factors across four categories: (I) social capital, networking, &amp; support; (II) career &amp; professional development; (III) pedagogical &amp; programmatic interventions; and (IV) exposure &amp; access. This data-collection process also yielded 26 recommendations across six stakeholder groups: (i) researchers; (ii) colleges and universities; (iii) the computing industry; (iv) K-12 systems and schools; (v) governments; and (vi) parents.Conclusion: This systematic mapping study resulted in the identification of numerous persistence factors for Black students in CS. Crucially, however, these persistence factors allow Black students to persist, but not thrive, in CS. Accordingly, we contend that more needs to be done to address the systemic inequities faced by Black people in general, and Black women in particular, in computing programs and professions. As evidenced by the relatively small number of primary studies captured by this systematic mapping study, there exists an urgent need for additional, asset-based empirical studies involving Black students in CS. In addition to foregrounding the intersectional experiences of Black women in CS, future studies should attend to the currently understudied experiences of Black men."
A Review of Stability in Topic Modeling: Metrics for Assessing and Techniques for Improving Stability,"Hosseiny Marani, Amin and Baumer, Eric P. S.",10.1145/3623269,2023,"Topic modeling includes a variety of machine learning techniques for identifying latent themes in a corpus of documents. Generating an exact solution (i.e., finding global optimum) is often computationally intractable. Various optimization techniques (e.g., Variational Bayes or Gibbs Sampling) are employed to generate topic solutions approximately by finding local optima. Such an approximation often begins with a random initialization, which leads to different results with different initializations. The term “stability” refers to a topic model’s ability to produce solutions that are partially or completely identical across multiple runs with different random initializations. Although a variety of work has been done analyzing, measuring, or improving stability, no single paper has provided a thorough review of different stability metrics nor of various techniques that improved the stability of a topic model. This paper fills that gap and provides a systematic review of different approaches to measure stability and of various techniques that are intended to improve stability. It also describes differences and similarities between stability measures and other metrics (e.g., generality, coherence). Finally, the paper discusses the importance of analyzing both stability and quality metrics to assess and to compare topic models."
Automatic Quality Assessment of Wikipedia Articles—A Systematic Literature Review,"Mo\'{a}s, Pedro Miguel and Lopes, Carla Teixeira",10.1145/3625286,2023,"Wikipedia is the world’s largest online encyclopedia, but maintaining article quality through collaboration is challenging. Wikipedia designed a quality scale, but with such a manual assessment process, many articles remain unassessed. We review existing methods for automatically measuring the quality of Wikipedia articles, identifying and comparing machine learning algorithms, article features, quality metrics, and used datasets, examining 149distinct studies, and exploring commonalities and gaps in them. The literature is extensive, and the approaches follow past technological trends. However, machine learning is still not widely used by Wikipedia, and we hope that our analysis helps future researchers change that reality."
"A Joint Study of the Challenges, Opportunities, and Roadmap of MLOps and AIOps: A Systematic Survey","Diaz-de-Arcaya, Josu and Torre-Bastida, Ana I. and Z\'{a}rate, Gorka and Mi\~{n}\'{o}n, Ra\'{u}l and Almeida, Aitor",10.1145/3625289,2023,"Data science projects represent a greater challenge than software engineering for organizations pursuing their adoption. The diverse stakeholders involved emphasize the need for a collaborative culture in organizations. This article aims to offer joint insights into the role of MLOps and AIOps methodologies for raising the success of data science projects in various fields, ranging from pure research to more traditional industries. We analyze the open issues, opportunities, and future trends organizations face when implementing MLOps and AIOps. Then, the frameworks and architectures that promote these paradigms are presented, as are the different fields in which they are being utilized. This systematic review was conducted using an automated procedure that identified 44,903 records, which were filtered down to 93 studies. These articles are meant to better clarify the problem at hand and highlight the future areas in both research and industry in which MLOPs and AIOps are thriving. Our findings indicate that AIOps flourish in challenging circumstances like those presented by 5G and 6G technologies, whereas MLOps is more prevalent in traditional industrial environments. The use of AIOps in certain stages of the ML lifecycle, such as deployment, remains underrepresented in scientific literature."
ALL: Supporting Experiential Accessibility Education and Inclusive Software Development,"Shi, Weishi and Moses, Heather and Yu, Qi and Malachowsky, Samuel and Krutz, Daniel E.",10.1145/3625292,2023,"Creating accessible software is imperative for making software inclusive for all users.Unfortunately, the topic of accessibility is frequently excluded from computing education, leading to scenarios where students are unaware of either how to develop accessible software or see the need to create it. To address this challenge, we have created a set of educational labs that are systematically designed to not only inform students about fundamental topics in producing accessible software but also demonstrate its importance. Over the previous year, these labs were included in several Computer Science 2 offerings at the Rochester Institute of Technology, comprising a total of 500&nbsp;student participants. This article discusses instructional observations from these offerings, some of which include the following: (i) many of the research findings from previous efforts remain true with the larger, more diverse evaluation; (ii) our created material and format reduced students’ belief that creating accessible software was difficult in relation to the baseline,; (iii) we observed that our created material and format benefited student opinion that creating accessible software is important, and (iv) computing majors may not be uniformly impacted by experiential educational accessibility material. The educational labs are publicly available on the project website (https://all.rit.edu)."
Poracle: Testing Patches under Preservation Conditions to Combat the Overfitting Problem of Program Repair,"Ismayilzada, Elkhan and Rahman, Md Mazba Ur and Kim, Dongsun and Yi, Jooyong",10.1145/3625293,2023,"To date, the users of test-driven program repair tools suffer from the overfitting problem; a generated patch may pass all available tests without being correct. In the existing work, users are treated as merely passive consumers of the tests. However, what if they are willing to modify the test to better assess the patches obtained from a repair tool? In this work, we propose a novel semi-automatic patch-classification methodology named Poracle. Our key contributions are three-fold. First, we design a novel lightweight specification method that reuses the existing test. Specifically, the users extend the existing failing test with a preservation condition—the condition under which the patched and pre-patched versions should produce the same output. Second, we develop a fuzzer that performs differential fuzzing with a test containing a preservation condition. Once we find an input that satisfies a specified preservation condition but produces different outputs between the patched and pre-patched versions, we classify the patch as incorrect with high confidence. We show that our approach is more effective than the four state-of-the-art patch classification approaches. Last, we show through a user study that the users find our semi-automatic patch assessment method more effective and preferable than the manual assessment."
RoboWorld: Verification of Robotic Systems with Environment in the Loop,"Baxter, James and Carvalho, Gustavo and Cavalcanti, Ana and J\'{u}nior, Francisco Rodrigues",10.1145/3625563,2023,"A robot affects and is affected by its environment, so that typically its behaviour depends on properties of that environment. For verification, we need to formalise those properties. Modelling the environment is very challenging, if not impossible, but we can capture assumptions. Here, we present RoboWorld, a domain-specific controlled natural language with a process algebraic semantics that can be used to define (a)&nbsp;operational requirements, and (b)&nbsp;environment interactions of a robot. RoboWorld is part of the RoboStar framework for verification of robotic systems. In this article, we define RoboWorld’s syntax and hybrid semantics, and illustrate its use for capturing operational requirements, for automatic test generation, and for proof. We also present a tool that supports the writing of RoboWorld documents. Since RoboWorld is a controlled natural language, it complements the other RoboStar notations in being accessible to roboticists, while at the same time benefitting from a formal semantics to support rigorous verification&nbsp;(via testing and proof)."
Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering,"Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Whittle, Jon and Zowghi, Didar and Jacquet, Aurelie",10.1145/3626234,2024,"Responsible Artificial Intelligence (RAI) is widely considered as one of the greatest scientific challenges of our time and is key to increase the adoption of Artificial Intelligence (AI). Recently, a number of AI ethics principles frameworks have been published. However, without further guidance on best practices, practitioners are left with nothing much beyond truisms. In addition, significant efforts have been placed at algorithm level rather than system level, mainly focusing on a subset of mathematics-amenable ethical principles, such as fairness. Nevertheless, ethical issues can arise at any step of the development lifecycle, cutting across many AI and non-AI components of systems beyond AI algorithms and models. To operationalize RAI from a system perspective, in this article, we present an RAI Pattern Catalogue based on the results of a multivocal literature review. Rather than staying at the principle or algorithm level, we focus on patterns that AI system stakeholders can undertake in practice to ensure that the developed AI systems are responsible throughout the entire governance and engineering lifecycle. The RAI Pattern Catalogue classifies the patterns into three groups: multi-level governance patterns, trustworthy process patterns, and RAI-by-design product patterns. These patterns provide systematic and actionable guidance for stakeholders to implement RAI."
From Digital Media to Empathic Spaces: A Systematic Review of Empathy Research in Extended Reality Environments,"Paananen, Ville and Kiarostami, Mohammad Sina and Lik-Hang, Lee and Braud, Tristan and Hosio, Simo",10.1145/3626518,2023,"Recent advances in extended reality (XR) technologies have enabled new and increasingly realistic empathy tools and experiences. In XR, all interactions take place in different spatial contexts, all with different features, affordances, and constraints. We present a systematic literature survey of recent work on empathy in XR. As a result, we contribute a research roadmap with three future opportunities and six open questions in XR-enabled empathy research across both physical and virtual spaces."
DESPP-DETR: A Dense Connection Efficient Spatial Pooling DEtection TRansformer for Vehicle Detection,"P., Krishnendhu S. and Mohandas, Prabu",10.1145/3628426,2023,"Real-time vehicle detection is a challenging and vital task in intelligent transportation systems. The key requirements for a vehicle detection model are speed and accuracy. However, existing real-time vehicle detection models often sacrifice one of these qualities in favor of the other. This trade-off makes them unfit for real-time deployment, where both speed and accuracy are equally important. Additionally, occlusion, which refers to the obstruction or partial covering of vehicles, further complicates detection and affects the system’s accuracy. In this study, we propose DESPP-DETR, a one-stage detection network for real-time vehicle detection. It is based on bipartite matching and a transformer encoder-decoder architecture, with the addition of a dense connection block and enhanced spatial pyramid pooling. The presence of dense connection block strengthens feature extraction. The enhanced spatial pyramid pooling eliminates the fixed-size constraint and increases the network’s learning capacity. When compared to existing models, DESPP-DETR achieves greater accuracy in real-time vehicle detection. On the MS COCO 2017 dataset, the proposed model achieves an improved mean average precision (mAP) of 75.53%, making it a promising solution for intelligent transportation systems."
A Systematic Analysis of Problems in Open Collaborative Data Engineering,"Heltweg, Philip and Riehle, Dirk",10.1145/3629040,2023,"Collaborative workflows are common in open-source software development. They reduce individual costs and improve the quality of work results. Open data shares many characteristics with open-source software, as it can be used, modified, and redistributed by anyone, for free. However, in contrast to open-source software engineering, collaborative data engineering on open data lacks a shared understanding of processes, methods, and tools. This article presents a systematic literature review of collaboration processes, methods, and tools in data engineering as performed by open data users. An additional interview study with practitioners confirms and enhances the findings and strengthens the resulting insights. We find an ecosystem with heterogeneous participants and no standardized processes, methods, and tools. Participants face a variety of technical and social challenges during their work. Our work provides a structured overview of collaboration systems in open collaborative data engineering, enabling further research. Additionally, we contribute preliminary guidelines for successful open collaborative data engineering projects and recommendations to increase its adoption for open data ecosystems."
A Survey on Conflict Detection in IoT-based Smart Homes,"Huang, Bing and Chaki, Dipankar and Bouguettaya, Athman and Lam, Kwok-Yan",10.1145/3629517,2023,"As the adoption of IoT-based smart homes continues to grow, the importance of addressing potential conflicts becomes increasingly vital for ensuring seamless functionality and user satisfaction. In this survey, we introduce a novel conflict taxonomy, complete with formal definitions of each conflict type that may arise within the smart home environment. We design an advanced conflict model to effectively categorize these conflicts, setting the stage for our in-depth review of recent research in the field. By employing our proposed model, we systematically classify conflicts and present a comprehensive overview of cutting-edge conflict detection approaches. This extensive analysis allows us to highlight similarities, clarify significant differences, and uncover prevailing trends in conflict detection techniques. In conclusion, we shed light on open issues and suggest promising avenues for future research to foster accelerated development and deployment of IoT-based smart homes, ultimately enhancing their overall performance and user experience."
Learning from Very Little Data: On the Value of Landscape Analysis for Predicting Software Project Health,"Lustosa, Andre and Menzies, Tim",10.1145/3630252,2024,"When data is scarce, software analytics can make many mistakes. For example, consider learning predictors for open source project health (e.g., the number of closed pull requests in 12 months time). The training data for this task may be very small (e.g., 5 years of data, collected every month means just 60 rows of training data). The models generated from such tiny datasets can make many prediction errors.Those errors can be tamed by a landscape analysis that selects better learner control parameters. Our niSNEAK tool (a)&nbsp;clusters the data to find the general landscape of the hyperparameters, then (b)&nbsp;explores a few representatives from each part of that landscape. niSNEAK is both faster and more effective than prior state-of-the-art hyperparameter optimization algorithms (e.g., FLASH, HYPEROPT, OPTUNA).The configurations found by niSNEAK have far less error than other methods. For example, for project health indicators such as C = number of commits, I = number of closed issues, and R = number of closed pull requests, niSNEAK’s 12-month prediction errors are {I=0%, R=33%&nbsp;C=47%}, whereas other methods have far larger errors of {I=61%,R=119%&nbsp;C=149%}. We conjecture that niSNEAK works so well since it finds the most informative regions of the hyperparameters, then jumps to those regions. Other methods (that do not reflect over the landscape) can waste time exploring less informative options.Based on the preceding, we recommend landscape analytics (e.g., niSNEAK) especially when learning from very small datasets. This article only explores the application of niSNEAK to project health. That said, we see nothing in principle that prevents the application of this technique to a wider range of problems.To assist other researchers in repeating, improving, or even refuting our results, all our scripts and data are available on GitHub at https://github.com/zxcv123456qwe/niSneak."
Effectiveness of Video-based Training for Face-to-face Communication Skills of Software Engineers: Evidence from a Three-year Study,"Mitrovic, Antonija and Galster, Matthias and Malinen, Sanna and Holland, Jay and Musa, Ja'afaru and Mohammadhassan, Negar and Lumapas, Raul Vincent",10.1145/3631532,2023,"Objectives. Communication skills are crucial for effective software development teams, but those skills are difficult to teach. The goal of our project is to evaluate the effectiveness of teaching face-to-face communication skills using AVW-Space, a platform for video-based learning that provides personalized nudges to support student's engagement during video watching.Participants. The participants in our study are second-year software engineering students. The study was conducted over three years, with students enrolled in a semester-long project course.Study Method. We performed a quasi-experimental study over three years to teach face-to-face communication using AVW-Space, a video-based learning platform. We present the instance of AVW-Space we developed to teach face-to-face communication. Participants watched and commented on 10 videos and later commented on the recording of their own team meeting. In 2020, the participants (n = 50) did not receive nudges, and we use the data collected that year as control. In 2021 (n = 49) and 2022 (n = 48), nudges were provided adaptively to encourage students to write more and higher-quality comments.Findings. The findings from the study show the effectiveness of nudges. We found significant differences in engagement when nudges were provided. Furthermore, there is a causal effect of nudges on the interaction time, the total number of comments written, and the number of high-quality comments, as well as on learning. Finally, participants exposed to nudges reported higher perceived learning.Conclusions. Our research shows the effect of nudges on student engagement and learning while using the instance of AVW-Space for teaching face-to-face communication skills. Future work will explore other soft skills, as well as providing explanations for the decisions made by AVW-Space."
A Survey of Learning-based Automated Program Repair,"Zhang, Quanjun and Fang, Chunrong and Ma, Yuxiang and Sun, Weisong and Chen, Zhenyu",10.1145/3631974,2023,"Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance.In this article, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our article can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at the repository: ."
Vision Transformer Inspired Automated Vulnerability Repair,"Fu, Michael and Nguyen, Van and Tantithamthavorn, Chakkrit and Phung, Dinh and Le, Trung",10.1145/3632746,2024,"Recently, automated vulnerability repair approaches have been widely adopted to combat increasing software security issues. In particular, transformer-based encoder-decoder models achieve competitive results. Whereas vulnerable programs may only consist of a few vulnerable code areas that need repair, existing AVR approaches lack a mechanism guiding their model to pay more attention to vulnerable code areas during repair generation. In this article, we propose a novel vulnerability repair framework inspired by the Vision Transformer based approaches for object detection in the computer vision domain. Similar to the object queries used to locate objects in object detection in computer vision, we introduce and leverage vulnerability queries (VQs) to locate vulnerable code areas and then suggest their repairs. In particular, we leverage the cross-attention mechanism to achieve the cross-match between VQs and their corresponding vulnerable code areas. To strengthen our cross-match and generate more accurate vulnerability repairs, we propose to learn a novel vulnerability mask (VM) and integrate it into decoders’ cross-attention, which makes our VQs pay more attention to vulnerable code areas during repair generation. In addition, we incorporate our VM into encoders’ self-attention to learn embeddings that emphasize the vulnerable areas of a program. Through an extensive evaluation using the real-world 5,417 vulnerabilities, our approach outperforms all of the automated vulnerability repair baseline methods by 2.68% to 32.33%. Additionally, our analysis of the cross-attention map of our approach confirms the design rationale of our VM and its effectiveness. Finally, our survey study with 71 software practitioners highlights the significance and usefulness of AI-generated vulnerability repairs in the realm of software security. The training code and pre-trained models are available at https://github.com/awsm-research/VQM."
Application of Smart Insoles for Recognition of Activities of Daily Living: A Systematic Review,"D’arco, Luigi and Mccalmont, Graham and Wang, Haiying and Zheng, Huiru",10.1145/3633785,2024,"Recent years have witnessed the increasing literature on using smart insoles in health and well-being, and yet, their capability of daily living activity recognition has not been reviewed. This paper addressed this need and provided a systematic review of smart insole-based systems in the recognition of Activities of Daily Living (ADLs). The review followed the PRISMA guidelines, assessing the sensing elements used, the participants involved, the activities recognised, and the algorithms employed. The findings demonstrate the feasibility of using smart insoles for recognising ADLs, showing their high performance in recognising ambulation and physical activities involving the lower body, ranging from 70% to 99.8% of Accuracy, with 13 studies over 95%. The preferred solutions have been those including machine learning. A lack of existing publicly available datasets has been identified, and the majority of the studies were conducted in controlled environments. Furthermore, no studies assessed the impact of different sampling frequencies during data collection, and a trade-off between comfort and performance has been identified between the solutions. In conclusion, real-life applications were investigated showing the benefits of smart insoles over other solutions and placing more emphasis on the capabilities of smart insoles."
Learning-based Relaxation of Completeness Requirements for Data Entry Forms,"Belgacem, Hichem and Li, Xiaochen and Bianculli, Domenico and Briand, Lionel",10.1145/3635708,2024,"Data entry forms use completeness requirements to specify the fields that are required or optional to fill for collecting necessary information from different types of users. However, because of the evolving nature of software, some required fields may not be applicable for certain types of users anymore. Nevertheless, they may still be incorrectly marked as required in the form; we call such fields obsolete required fields. Since obsolete required fields usually have “not-null” validation checks before submitting the form, users have to enter meaningless values in such fields to complete the form submission. These meaningless values threaten the quality of the filled data and could negatively affect stakeholders or learning-based tools that use the data. To avoid users filling meaningless values, existing techniques usually rely on manually written rules to identify the obsolete required fields and relax their completeness requirements. However, these techniques are ineffective and costly.In this article, we propose LACQUER, a learning-based automated approach for relaxing the completeness requirements of data entry forms. LACQUER builds Bayesian Network models to automatically learn conditions under which users had to fill meaningless values. To improve its learning ability, LACQUER identifies the cases where a required field is only applicable for a small group of users and uses SMOTE, an oversampling technique, to generate more instances on such fields for effectively mining dependencies on them. During the data entry session, LACQUER predicts the completeness requirement of a target based on the already filled fields and their conditional dependencies in the trained model.Our experimental results show that LACQUER can accurately relax the completeness requirements of required fields in data entry forms with precision values ranging between 0.76 and 0.90 on different datasets. LACQUER can prevent users from filling 20% to 64% of meaningless values, with negative predictive values (i.e., the ability to correctly predict a field as “optional”) between 0.72 and 0.91. Furthermore, LACQUER is efficient; it takes at most 839 ms to predict the completeness requirement of an instance."
Variable Autonomy through Responsible Robotics: Design Guidelines and Research Agenda,"Reinmund, Tyler and Salvini, Pericle and Kunze, Lars and Jirotka, Marina and Winfield, Alan F. T.",10.1145/3636432,2024,"Physically embodied artificial agents, or robots, are being incorporated into various practical and social contexts, from self-driving cars for personal transportation to assistive robotics in social care. To enable these systems to better perform under changing conditions, designers have proposed to endow robots with varying degrees of autonomous capabilities and the capacity to move between them—an approach known as variable autonomy. Researchers are beginning to understand how robots with fixed autonomous capabilities influence a person’s sense of autonomy, social relations, and, as a result, notions of responsibility; however, addressing these topics in scenarios where robot autonomy dynamically changes is underexplored. To establish a research agenda for variable autonomy that emphasises the responsible design and use of robotics, we conduct a developmental review. Based on a sample of 42 papers, we provide a synthesised definition of variable autonomy to connect currently disjointed research efforts, detail research approaches in variable autonomy to strengthen the empirical basis for subsequent work, characterise the dimensions of variable autonomy, and present design guidelines for variable autonomy research based on responsible robotics."
Brave New GES World: A Systematic Literature Review of Gestures and Referents in Gesture Elicitation Studies,"Villarreal-Narvaez, Santiago and Slu\""{y}ters, Arthur and Vanderdonckt, Jean and Vatavu, Radu-Daniel",10.1145/3636458,2024,"How do we determine highly effective and intuitive gesture sets for interactive systems tailored to end users’ preferences? A substantial body of knowledge is available on this topic, among which gesture elicitation studies stand out distinctively. In these studies, end users are invited to propose gestures for specific referents, which are the functions to control for an interactive system. The vast majority of gesture elicitation studies conclude with a consensus gesture set identified following a process of consensus or agreement analysis. However, the information about specific gesture sets determined for specific applications is scattered across a wide landscape of disconnected scientific publications, which poses challenges to researchers and practitioners to effectively harness this body of knowledge. To address this challenge, we conducted a systematic literature review and examined a corpus of N= 267 studies encompassing a total of 187,265 gestures elicited from 6,659 participants for 4,106 referents. To understand similarities in users’ gesture preferences within this extensive dataset, we analyzed a sample of 2,304 gestures extracted from the studies identified in our literature review. Our approach consisted of (i) identifying the context of use represented by end users, devices, platforms, and gesture sensing technology; (ii) categorizing the referents; (iii) classifying the gestures elicited for those referents; and (iv) cataloging the gestures based on their representation and implementation modalities. Drawing from the findings of this review, we propose guidelines for conducting future end-user gesture elicitation studies."
Automated Grading and Feedback Tools for Programming Education: A Systematic Review,"Messer, Marcus and Brown, Neil C. C. and K\""{o}lling, Michael and Shi, Miaojing",10.1145/3636515,2024,"We conducted a systematic literature review on automated grading and feedback tools for programming education. We analysed 121 research papers from 2017 to 2021 inclusive and categorised them based on skills assessed, approach, language paradigm, degree of automation, and evaluation techniques. Most papers assess the correctness of assignments in object-oriented languages. Typically, these tools use a dynamic technique, primarily unit testing, to provide grades and feedback to the students or static analysis techniques to compare a submission with a reference solution or with a set of correct student submissions. However, these techniques’ feedback is often limited to whether the unit tests have passed or failed, the expected and actual output, or how they differ from the reference solution. Furthermore, few tools assess the maintainability, readability, or documentation of the source code, with most using static analysis techniques, such as code quality metrics, in conjunction with grading correctness. Additionally, we found that most tools offered fully automated assessment to allow for near-instantaneous feedback and multiple resubmissions, which can increase student satisfaction and provide them with more opportunities to succeed. In terms of techniques used to evaluate the tools’ performance, most papers primarily use student surveys or compare the automatic assessment tools to grades or feedback provided by human graders. However, because the evaluation dataset is frequently unavailable, it is more difficult to reproduce results and compare tools to a collection of common assignments."
"A Systematic Review of Fairness, Accountability, Transparency, and Ethics in Information Retrieval","Bernard, Nolwenn and Balog, Krisztian",10.1145/3637211,2025,"We live in an information society that strongly relies on information retrieval systems, such as search engines and conversational assistants. Consequently, the trustworthiness of these systems is of critical importance and has attracted significant research attention in recent years. In this work, we perform a systematic literature review of the field of fairness, accountability, transparency, and ethics in information retrieval. In particular, we investigate the definitions, approaches, and evaluation methodologies proposed to build trustworthy information retrieval systems. This review reveals the lack of standard definitions, arguably due to the multi-dimensional nature of the different notions. In terms of approaches, most of the work focuses on building either a fair or a transparent information retrieval system. As for evaluation, fairness is often assessed by means of automatic evaluation, while accountability and transparency are most commonly evaluated using audits and user studies. Based on the surveyed literature, we develop taxonomies of requirements for the different notions, and further use these taxonomies to propose practical definitions to quantify the degree to which an information retrieval system satisfies a given notion. Finally, we discuss challenges that have yet to be solved for information retrieval systems to be trustworthy."
PACE: A Program Analysis Framework for Continuous Performance Prediction,"Biringa, Chidera and Kul, G\""{o}khan",10.1145/3637230,2024,"Software development teams establish elaborate continuous integration pipelines containing automated test cases to accelerate the development process of software. Automated tests help to verify the correctness of code modifications decreasing the response time to changing requirements. However, when the software teams do not track the performance impact of pending modifications, they may need to spend considerable time refactoring existing code. This article presents PACE, a program analysis framework that provides continuous feedback on the performance impact of pending code updates. We design performance microbenchmarks by mapping the execution time of functional test cases given a code update. We map microbenchmarks to code stylometry features and feed them to predictors for performance predictions. Our experiments achieved significant performance in predicting code performance, outperforming current state-of-the-art by 75% on neural-represented code stylometry features."
Characterizing Developers' Linguistic Behaviors in Open Source Development across Their Social Statuses,"Han, Yisi and Wang, Zhendong and Feng, Yang and Zhao, Zhihong and Wang, Yi",10.1145/3637306,2024,"Open Source Software (OSS) development has attracted numerous developers. As a typical complex sociotechnical system, an OSS project often forms a hierarchical social structure where a few developers are elite while the rest are non-elite. Differences in social status may result in distinct language use behaviors in interpersonal communication. Characterizing such behaviors is critical for supporting efficient and effective communication among developers with different social statuses. This study empirically compared elite and non-elite developers' language behaviors in their communication. We compiled a corpus of - 216,000 discourses collected from 20 large projects on GitHub. We investigated the linguistic differences in three aspects, namely, linguistic styles and characters, main concerns, and sentence patterns. Our findings reveal that elite and non-elite developers showed different linguistic patterns and had different concerns in their discourses. Their discourses also reflect the variation of the main focuses in the development process. Furthermore, elite and non-elite developers exhibited noticeable patterns in their linguistic behaviors in accordance with their roles and corresponding divisions of labor in the production process, no matter which semantic contexts. These findings provide implications for supporting communication that crosses social statuses in OSS development."
"""It Felt Like Having a Second Mind"": Investigating Human-AI Co-creativity in Prewriting with Large Language Models","Wan, Qian and Hu, Siying and Zhang, Yu and Wang, Piaohong and Wen, Bo and Lu, Zhicong",10.1145/3637361,2024,"Prewriting is the process of discovering and developing ideas before writing a first draft, which requires divergent thinking and often implies unstructured strategies such as diagramming, outlining, free-writing, etc. Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting. The preferred collaborative role and initiative of LLMs during such a creative process is also unclear. To investigate human-LLM collaboration patterns and dynamics during prewriting, we conducted a three-session qualitative study with 15 participants in two creative tasks: story writing and slogan writing. The findings indicated that during collaborative prewriting, there appears to be a three-stage iterative Human-AI Co-creativity process that includes Ideation, Illumination, and Implementation stages. This collaborative process champions the human in a dominant role, in addition to mixed and shifting levels of initiative that exist between humans and LLMs. This research also reports on collaboration breakdowns that occur during this process, user perceptions of using existing LLMs during Human-AI Co-creativity, and discusses design implications to support this co-creativity process."
Making Data Work Count,"Chandhiramowuli, Srravya and Taylor, Alex S. and Heitlinger, Sara and Wang, Ding",10.1145/3637367,2024,"In this paper, we examine the work of data annotation. Specifically, we focus on the role of counting or quantification in organising annotation work. Based on an ethnographic study of data annotation in two outsourcing centres in India, we observe that counting practices and its associated logics are an integral part of day-to-day annotation activities. In particular, we call attention to the presumption of total countability observed in annotation - the notion that everything, from tasks, datasets and deliverables, to workers, work time, quality and performance, can be managed by applying the logics of counting. To examine this, we draw on sociological and socio-technical scholarship on quantification and develop the lens of a 'regime of counting' that makes explicit the specific counts, practices, actors and structures that underpin the pervasive counting in annotation. We find that within the AI supply chain and data work, counting regimes aid the assertion of authority by the AI clients (also called requesters) over annotation processes, constituting them as reductive, standardised, and homogenous. We illustrate how this has implications for i) how annotation work and workers get valued, ii) the role human discretion plays in annotation, and iii) broader efforts to introduce accountable and more just practices in AI. Through these implications, we illustrate the limits of operating within the logic of total countability. Instead, we argue for a view of counting as partial - located in distinct geographies, shaped by specific interests and accountable in only limited ways. This, we propose, sets the stage for a fundamentally different orientation to counting and what counts in data annotation."
When Workers Want to Say No: A View into Critical Consciousness and Workplace Democracy in Data Work,"DiSalvo, Carl and Rothschild, Annabel and Schenck, Lara L. and Shapiro, Ben Rydal and DiSalvo, Betsy",10.1145/3637433,2024,"In this paper, we describe and reflect upon the development of critical consciousness and workplace democracy within an experimental workplace called DataWorks. Through DataWorks, we hire adults from communities historically minoritized in computing education and data careers, and train them in entry-level data skills developed through work on client projects. In this process, workers gain a range of skills. Some of these skills are technical, such as programming for data analysis; some are managerial, such as scoping and bidding projects; others are social, perhaps even political, such as the ability to say ""No"" to projects. In what follows, we describe a workshop series developed to build the workers' critical literacy and consciousness about their data work, specifically regarding the use of data in machine learning systems. After that, we describe a data project the workers questioned and resisted because they determined the work to be harmful. In that process, they demonstrated and enacted a critical consciousness towards data and machine learning. Reflecting on this enactment of data-focused critical consciousness, we identify themes that characterize a democratic workplace, describe the work of designing for organizational action and institutional relations, and discuss how worker and researcher positionality affects this work. In doing so, we argue for enabling workers to resist and refuse harmful data work and challenge the standard power structures of academic research and data work."
"Explainable AI for Medical Data: Current Methods, Limitations, and Future Directions","Hossain, Md Imran and Zamzmi, Ghada and Mouton, Peter R. and Salekin, Md Sirajus and Sun, Yu and Goldgof, Dmitry",10.1145/3637487,2025,"With the power of parallel processing, large datasets, and fast computational resources, deep neural networks (DNNs) have outperformed highly trained and experienced human experts in medical applications. However, the large global community of healthcare professionals, many of whom routinely face potentially life-or-death outcomes with complex medicolegal consequences, have yet to embrace this powerful technology. The major problem is that most current AI solutions function as a metaphorical black-box positioned between input data and output decisions without a rigorous explanation for their internal processes. With the goal of enhancing trust and improving acceptance of artificial intelligence– (AI) based technology in clinical medicine, there is a large and growing effort to address this challenge using eXplainable AI (XAI), a set of techniques, strategies, and algorithms with an explicit focus on explaining the “hows and whys” of DNNs. Here, we provide a comprehensive review of the state-of-the-art XAI techniques concerning healthcare applications and discuss current challenges and future directions. We emphasize the strengths and limitations of each category, including image, tabular, and textual explanations, and explore a range of evaluation metrics for assessing the effectiveness of XAI solutions. Finally, we highlight promising opportunities for XAI research to enhance the acceptance of DNNs by the healthcare community."
Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains,"Cheng, Yu and Chen, Jieshan and Huang, Qing and Xing, Zhenchang and Xu, Xiwei and Lu, Qinghua",10.1145/3638247,2024,"The emergence of foundation models, such as large language models (LLMs) GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities across various domains. People can now use natural language (i.e., prompts) to communicate with AI to perform tasks. While people can use foundation models through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the underlying models, is not a production tool for building reusable AI services. APIs like LangChain allow for LLM-based application development but require substantial programming knowledge, thus posing a barrier. To mitigate this, we systematically review, summarise, refine and extend the concept of AI chain by incorporating the best principles and practices that have been accumulated in software engineering for decades into AI chain engineering, to systematize AI chain engineering methodology. We also develop a no-code integrated development environment, , which embodies these AI chain engineering principles and patterns naturally in the process of building AI chains, thereby improving the performance and quality of AI chains. With Prompt Sapper, AI chain engineers can compose prompt-based AI services on top of foundation models through chat-based requirement analysis and visual programming. Our user study evaluated and demonstrated the efficiency and correctness of Prompt Sapper."
"Security for Machine Learning-based Software Systems: A Survey of Threats, Practices, and Challenges","Chen, Huaming and Babar, M. Ali",10.1145/3638531,2024,"The rapid development of Machine Learning (ML) has demonstrated superior performance in many areas, such as computer vision and video and speech recognition. It has now been increasingly leveraged in software systems to automate the core tasks. However, how to securely develop the machine learning-based modern software systems (MLBSS) remains a big challenge, for which the insufficient consideration will largely limit its application in safety-critical domains. One concern is that the present MLBSS development tends to be rushed, and the latent vulnerabilities and privacy issues exposed to external users and attackers will be largely neglected and hard to be identified. Additionally, machine learning-based software systems exhibit different liabilities towards novel vulnerabilities at different development stages from requirement analysis to system maintenance, due to its inherent limitations from the model and data and the external adversary capabilities. The successful generation of such intelligent systems will thus solicit dedicated efforts jointly from different research areas, i.e., software engineering, system security, and machine learning. Most of the recent works regarding the security issues for ML have a strong focus on the data and models, which has brought adversarial attacks into consideration. In this work, we consider that security for machine learning-based software systems may arise from inherent system defects or external adversarial attacks, and the secure development practices should be taken throughout the whole lifecycle. While machine learning has become a new threat domain for existing software engineering practices, there is no such review work covering the topic. Overall, we present a holistic review regarding the security for MLBSS, which covers a systematic understanding from a structure review of three distinct aspects in terms of security threats. Moreover, it provides a thorough state-of-the-practice for MLBSS secure development. Finally, we summarize the literature for system security assurance and motivate the future research directions with open challenges. We anticipate this work provides sufficient discussion and novel insights to incorporate system security engineering for future exploration."
Survey on Recommender Systems for Biomedical Items in Life and Health Sciences,"Pato, Matilde and Barros, M\'{a}rcia and Couto, Francisco M.",10.1145/3639047,2024,"The generation of biomedical data is of such magnitude that its retrieval and analysis have posed several challenges. A survey of recommender system (RS) approaches in biomedical fields is provided in this analysis, along with a discussion of existing challenges related to large-scale biomedical information retrieval systems. We collect original studies, identify entities and models, and discuss how knowledge graphs (KGs) can improve results. As a result, most of the papers used model-based collaborative filtering algorithms, most of the available datasets did not follow the standard format &lt;&nbsp;user, item, rating&nbsp;&gt;, and regarding qualitative evaluations of RSs use mainly classification metrics. Finally, we have assembled and coded a unique dataset of 60 papers — Sur-RS4BioT, available for download at DOI:10.34740/kaggle/ds/2346894"
A Realist Review of Undergraduate Student Attitudes towards Ethical Interventions in Technical Computing Courses,"Padiyath, Aadarsh",10.1145/3639572,2024,"As computing educators begin to recognize that their students need strong ethical foundations, there is a growing interest to integrate meaningful ethics education into undergraduate computing curricula. To achieve this, it is crucial to understand how students respond to ethical interventions in the classroom. This review examines the acceptance of ethical interventions in undergraduate computing courses, using the realist synthesis method to identify and refine underlying theories of student acceptance, and refine them through available studies. Four theories were identified in a synthesis of 13 reports, providing insight into what may improve student attitudes towards ethical interventions in which contexts and under which circumstances. The findings of this realist review offer guidance to intervention designers, researchers, and educators seeking to meaningfully engage students with ethics in computing education."
"""That comes with a huge career cost:"" Understanding Collaborative Ideation Experiences of Disabled Professionals","Das, Maitraye and Stangl, Abigale and Findlater, Leah",10.1145/3641018,2024,"Collaborative ideation plays a vital role in driving creativity and innovation across various professional and educational contexts. This study investigates the experiences of disabled individuals within the collaborative ideation process, specifically examining their utilization of digital whiteboarding tools. Through interviews with 19 professionals and academics with disabilities, alongside a thematic analysis of online forum posts for two popular digital whiteboarding platforms (Miro and Figma), we delve into the access barriers encountered by disabled individuals and the strategies they employ to create access in collaborative ideation. Our findings illuminate the multifaceted nature of access barriers, encompassing issues such as inaccessible visual features, technology-induced discomfort, unstructured nature of freeform content, and complex communication setups. Furthermore, we uncover the intricate dynamics involved in negotiating diverse access needs and conflicts within teams involving people with different disabilities. Through this analysis, we highlight tensions around proficiency with inaccessible technologies stemming from ableist standards of professional success and discuss the implications of our findings for the design of accessible collaborative ideation systems."
Computer Science Education Facing Unconventional Odds: Case Studies from the Arab World,"Aly, Sherif G. and Echihabi, Karima and Eldawlatly, Seif and Shuaib, Khaled and Tekli, Joe",10.1145/3643036,2024,
An Empirical Analysis of Issue Templates Usage in Large-Scale Projects on GitHub,"S\""{u}l\""{u}n, Emre and Sa\c{c}ak\c{c}\i{}, Metehan and T\""{u}z\""{u}n, Eray",10.1145/3643673,2024,"GitHub Issues is a widely used issue tracking tool in open-source software projects. Originally designed with broad flexibility, its lack of standardization led to incomplete issue reports, impeding software development and maintenance efficiency. To counteract this, GitHub introduced issue templates in 2016, which rapidly became popular. Our study assesses the current use and evolution of these templates in large-scale open-source projects and their impact on issue tracking metrics, including resolution time, number of reopens, and number of issue comments. Employing a comprehensive analysis of 350 templates from 100 projects, we also evaluated over 1.9 million issues for template conformity and impact. Additionally, we solicited insights from open-source software maintainers through a survey. Our findings highlight issue templates’ extensive usage in 99 of the 100 surveyed projects, with a growing preference for YAML-based templates, a more structured template variant. Projects with a template exhibited markedly reduced resolution time (381.02 days to 103.18&nbsp;days) and reduced issue comment count (4.95 to 4.32) compared to those without. The use of YAML-based templates further significantly decreased resolution time, the number of reopenings, and the discussion extent. Thus, our research underscores issue templates’ positive impact on large-scale open-source projects, offering recommendations for improved effectiveness."
Digital Transformation of Tax Administration and Compliance: A Systematic Literature Review on E-Invoicing and Prefilled Returns,"Hesami, Siamand and Jenkins, Hatice and Jenkins, Glenn P.",10.1145/3643687,2024,"This article systematically reviews the impact of electronic invoicing and prefilling tax returns on tax administration and compliance, adhering to the PRISMA guidelines. Our comprehensive literature search and analysis reveal that these technologies are crucial in reducing tax compliance and administration costs. The results indicate significant benefits, including reduced financial stress for firms, especially during development phases, and enhanced efficiency in tax administration processes. The study highlights how e-invoicing and prefilling systems simplify and improve the tracking of taxation, leading to increased efficiency in tax practices globally. This research contributes to understanding the transformative effects of digital technologies in taxation, demonstrating their potential in streamlining tax compliance and administrative procedures."
JIT-Smart: A Multi-task Learning Framework for Just-in-Time Defect Prediction and Localization,"Chen, Xiangping and Xu, Furen and Huang, Yuan and Zhang, Neng and Zheng, Zibin",10.1145/3643727,2024,"Just-in-time defect prediction (JIT-DP) is used to predict the defect-proneness of a commit and just-in-time defect localization (JIT-DL) is used to locate the exact buggy positions (defective lines) in a commit. Recently, various JIT-DP and JIT-DL techniques have been proposed, while most of them use a post-mortem way (e.g., code entropy, attention weight, LIME) to achieve the JIT-DL goal based on the prediction results in JIT-DP. These methods do not utilize the label information of the defective code lines during model building. In this paper, we propose a unified model JIT-Smart, which makes the training process of just-in-time defect prediction and localization tasks a mutually reinforcing multi-task learning process. Specifically, we design a novel defect localization network (DLN), which explicitly introduces the label information of defective code lines for supervised learning in JIT-DL with considering the class imbalance issue. To further investigate the accuracy and cost-effectiveness of JIT-Smart, we compare JIT-Smart with 7 state-of-the-art baselines under 5 commit-level and 5 line-level evaluation metrics in JIT-DP and JIT-DL. The results demonstrate that JIT-Smart is statistically better than all the state-of-the-art baselines in JIT-DP and JIT-DL. In JIT-DP, at the median value, JIT-Smart achieves F1-Score of 0.475, AUC of 0.886, Recall@20%Effort of 0.823, Effort@20%Recall of 0.01 and Popt of 0.942 and improves the baselines by 19.89%-702.74%, 1.23%-31.34%, 9.44%-33.16%, 21.6%-53.82% and 1.94%-34.89%, respectively . In JIT-DL, at the median value, JIT-Smart achieves Top-5 Accuracy of 0.539 and Top-10 Accuracy of 0.396, Recall@20%Effortline of 0.726, Effort@20%Recallline of 0.087 and IFAline of 0.098 and improves the baselines by 101.83%-178.35%, 101.01%-277.31%, 257.88%-404.63%, 71.91%-74.31% and 99.11%-99.41%, respectively. Statistical analysis shows that our JIT-Smart performs more stably than the best-performing model. Besides, JIT-Smart also achieves the best performance compared with the state-of-the-art baselines in cross-project evaluation."
Understanding Developers’ Discussions and Perceptions on Non-functional Requirements: The Case of the Spring Ecosystem,"Oliveira, Anderson and Correia, Jo\~{a}o and Assun\c{c}\~{a}o, Wesley K. G. and Pereira, Juliana Alves and de Mello, Rafael and Coutinho, Daniel and Barbosa, Caio and Lib\'{o}rio, Paulo and Garcia, Alessandro",10.1145/3643750,2024,"Non-Functional Requirements (NFRs) should be defined in the early stages of the software development process, driving developers to make important design decisions. Neglecting NFRs may lead developers to create systems that are difficult to maintain and do not meet users expectations. Despite their importance, the discussion of NFRs is often ad-hoc and scattered through multiple sources, limiting developers' awareness of NFRs. In that scenario, Pull Request (PR) discussions provide a centralized platform for comprehensive NFR discussions. However, existing studies do not explore this important source of information in open-source software development, which developers widely use to discuss software requirements. In this study, we report an investigation of NFR discussions in PRs of repositories of the Spring ecosystem. We collected, manually curated, and analyzed PR discussions addressing four categories of NFRs: maintainability, security, performance, and robustness. We observed that discussions surrounding these PRs tend to address the introduction of a code change or explain some anomaly regarding a particular NFR. Also, we found that more than 77% of the discussions related to NFRs are triggered in the PR title and/or description, indicating that developers are often provided with information regarding NFRs straightway. To gain additional knowledge from these NFR discussions, our study also analyzed the characteristics and activities of developers who actually discuss and fix NFR issues. In particular, we performed an in-depth analysis of 63 developers that stood out in collaborating with the mapped PRs. To complement this analysis, we conducted a survey with 44 developers to gather their perceptions on NFR discussions. By observing how developers approach NFRs and participate in discussions, we documented the best practices and strategies newcomers can adopt to address NFRs effectively. We also provided a curated dataset of 1,533 PR discussions classified with NFR presence."
"Rocks Coding, Not Development: A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks","Wang, Wei and Ning, Huilong and Zhang, Gaowei and Liu, Libo and Wang, Yi",10.1145/3643758,2024,"Recently, large language models (LLM) based generative AI has been gaining momentum for their impressive high-quality performances in multiple domains, particularly after the release of the ChatGPT. Many believe that they have the potential to perform general-purpose problem-solving in software development and replace human software developers. Nevertheless, there are in a lack of serious investigation into the capability of these LLM techniques in fulfilling software development tasks. In a controlled 2 \texttimes{} 2 between-subject experiment with 109 participants, we examined whether and to what degree working with ChatGPT was helpful in the coding task and typical software development task and how people work with ChatGPT. We found that while ChatGPT performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good. We also observed the interactions between participants and ChatGPT and found the relations between the interactions and the outcomes. Our study thus provides first-hand insights into using ChatGPT to fulfill software engineering tasks with real-world developers and motivates the need for novel interaction mechanisms that help developers effectively work with large language models to achieve desired outcomes."
Computer Science Undergraduate Programs in Australia,"Hamilton, Margaret and Hol, Ana and Richardson, Joan and McGovern, James",10.1145/3644816,2024,
Who’s in Charge Here? A Survey on Trustworthy AI in Variable Autonomy Robotic Systems,"Methnani, Leila and Chiou, Manolis and Dignum, Virginia and Theodorou, Andreas",10.1145/3645090,2024,"This article surveys the Variable Autonomy (VA) robotics literature that considers two contributory elements to Trustworthy AI: transparency and explainability. These elements should play a crucial role when designing and adopting robotic systems, especially in VA where poor or untimely adjustments of the system’s level of autonomy can lead to errors, control conflicts, user frustration, and ultimate disuse of the system. Despite this need, transparency and explainability is, to the best of our knowledge, mostly overlooked in VA robotics literature or is not considered explicitly. In this article, we aim to present and examine the most recent contributions to the VA literature concerning transparency and explainability. In addition, we propose a way of thinking about VA by breaking these two concepts down based on: the mission of the human-robot team; who the stakeholder is; what needs to be made transparent or explained; why they need it; and how it can be achieved. Last, we provide insights and propose ways to move VA research forward. Our goal with this article is to raise awareness and inter-community discussions among the Trustworthy AI and the VA robotics communities."
On Trust Recommendations in the Social Internet of Things – A Survey,"Becherer, Marius and Hussain, Omar Khadeer and Zhang, Yu and den Hartog, Frank and Chang, Elizabeth",10.1145/3645100,2024,"The novel paradigm Social Internet of Things (SIoT) improves the network navigability, identifies suitable service providers, and addresses scalability concerns. Ensuring trustworthy collaborations among devices is a key aspect in SIoT and can be realized through trust recommendations. However, the outcome of trust recommendations depends on multiple factors related to the context-dependent nature of SIoT and practical constraints brought by the devices and networks embedded in the SIoT. While the existing literature has proposed numerous trust recommendation models to assess the trustworthiness of devices in various scenarios, researchers have not sufficiently examined the required features for trust recommendations in the SIoT. Consequently, trust recommendation models may inaccurately assess the true risk of device interactions. In this literature survey, we investigate the context-dependent features and recommendation methods used for the SIoT using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology. We propose a novel taxonomy to categorize trust recommendation models according to their input features and design. Our findings reveal limited attention is given to the context-dependent features, constraints of the information environment, and limited inference capabilities that impede more precise trust recommendations. Finally, we present the research gaps and outline future directions to enable trustworthy inter-domain operations within the SIoT."
From Pixels to Play: Opportunities and Challenges of a Diverse and Democratized Games Industry,"Long, Sebastian and Denisova, Alena and Mirza-Babaei, Pejman",10.1145/3647646,2024,"This forum features game-practitioner perspectives on the interaction design process, techniques, and evaluation involved in creating playful experiences. We focus on how technology advancement, infrastructure, and constraints shape the player experience."
Fuzzers for Stateful Systems: Survey and Research Directions,"Daniele, Cristian and Andarzian, Seyed Behnam and Poll, Erik",10.1145/3648468,2024,"Fuzzing is a very effective testing methodology to find bugs. In a nutshell, a fuzzer sends many slightly malformed messages to the software under test, hoping for crashes or incorrect system behaviour. The methodology is relatively simple, although applications that keep internal states are challenging to fuzz. The research community has responded to this challenge by developing fuzzers tailored to stateful systems, but a clear understanding of the variety of strategies is still missing. In this paper, we present the first taxonomy of fuzzers for stateful systems and provide a systematic comparison and classification of these fuzzers."
Non-invasive Techniques for Muscle Fatigue Monitoring: A Comprehensive Survey,"Li, Na and Zhou, Rui and Krishna, Bharath and Pradhan, Ashirbad and Lee, Hyowon and He, Jiayuan and Jiang, Ning",10.1145/3648679,2024,"Muscle fatigue represents a complex physiological and psychological phenomenon that impairs physical performance and increases the risks of injury. It is important to continuously monitor fatigue levels for early detection and management of fatigue. The detection and classification of muscle fatigue also provide important information in human-computer interactions (HMI), sports injuries and performance, ergonomics, and prosthetic control. With this purpose in mind, this review first provides an overview of the mechanisms of muscle fatigue and its biomarkers and further enumerates various non-invasive techniques commonly used for muscle fatigue monitoring and detection in the literature, including electromyogram (EMG), which records the muscle electrical activity during muscle contractions, mechanomyogram (MMG), which records vibration signals of muscle fibers, near-infrared spectroscopy (NIRS), which measures the amount of oxygen in the muscle, ultrasound (US), which records signals of muscle deformation during muscle contractions. This review also introduces the principle and mechanism, parameters used for fatigue detection, application in fatigue detection, and advantages and disadvantages of each technology in detail. To conclude, the limitations/challenges that need to be addressed for future research in this area are presented."
SoK: Analyzing Privacy and Security of Healthcare Data from the User Perspective,"Tazi, Faiza and Nandakumar, Archana and Dykstra, Josiah and Rajivan, Prashanth and Das, Sanchari",10.1145/3650116,2024,"Interactions in healthcare, by necessity, involve sharing sensitive information to achieve high-quality patient outcomes. Therefore, sensitive data must be carefully protected. This article explores existing privacy and security research conducted in the context of healthcare organizations. We conducted a systematic literature review of N=1,553 articles that examine the security and privacy of healthcare data and focus on 80 articles addressing human factors. Key findings show that much of the healthcare security and privacy research is focused on technology (44.11%, 712 articles), with a lack of emphasis on the human element (4.96%, 80 articles). In the subset of user studies, we find that patients and the general public express concerns about privacy and security with technologies like electronic health records (EHRs). Furthermore, our analysis shows that healthcare professionals often have low awareness of risks related to data security. Additionally, our analysis revealed that most research focuses narrowly on large hospitals, neglecting private practices and the unique challenges they face. We conclude by identifying research gaps and providing potential solutions to enable robust data security for sensitive patient data."
Extended Reality (XR) Toward Building Immersive Solutions: The Key to Unlocking Industry 4.0,"Alhakamy, A’aeshah",10.1145/3652595,2024,"When developing XR applications for Industry 4.0, it is important to consider the integration of visual displays, hardware components, and multimodal interaction techniques that are compatible with the entire system. The potential use of multimodal interactions in industrial applications has been recognized as a significant factor in enhancing humans’ ability to perform tasks and make informed decisions. To offer a comprehensive analysis of the current advancements in industrial XR, this review presents a structured tutorial that provides answers to the following research questions: (R.Q.1) What are the similarities and differences between XR technologies, including augmented reality (AR), mixed reality (MR), Augmented Virtuality (AV), and virtual reality (VR) under Industry 4.0 consideration? (R.Q.2) What types of visual displays and hardware devices are needed to present XR for Industry 4.0? (R.Q.3) How did the multimodal interaction in XR perceive and relate to Industry 4.0? (R.Q.4) How have modern adaptations of XR technologies dealt with the theme of Industry 4.0? (R.Q.5) How can XR technologies in Industry 4.0 develop their services and usages to be more solution-inclusive? This review showcases various instances that demonstrate XR’s potential to transform how humans interact with the physical world in Industry 4.0. These advancements can increase productivity, reduce costs, and enhance safety."
"Mobile Near-infrared Sensing—A Systematic Review on Devices, Data, Modeling, and Applications","Jiang, Weiwei and Goncalves, Jorge and Kostakos, Vassilis",10.1145/3652596,2024,"Mobile near-infrared sensing is becoming an increasingly important method in many research and industrial areas. To help consolidate progress in this area, we use the PRISMA guidelines to conduct a systematic review of mobile near-infrared sensing, including (1) existing prototypes and commercial products, (2) data collection techniques, (3) machine learning methods, and (4) relevant application areas. Our work measures historical and current trends and identifies current challenges and future directions for this emerging topic."
Design Tensions in Online Freelancing Platforms: Using Speculative Participatory Design to Support Freelancers' Relationships with Clients,"Huang, Jessica and Ma, Ning F. and Rivera, Veronica A. and Somani, Tabreek and Lee, Patrick Yung Kang and Mcgrenere, Joanna and Yoon, Dongwook",10.1145/3653700,2024,"This paper explores the design challenges that arise in supporting online freelancers to navigate relationships with clients. Prior studies have shown that current platform designs can lead to worker precarity in freelancer-client relationships, such as power imbalances, information asymmetry, and labor abuse. To envision alternative designs that empower workers in managing their relationships with clients, we engaged 22 Upwork freelancers in participatory speculative design activities. Through this co-design process, we identified design tensions that constrain design options as a result of conflicting values and priorities that could only be balanced and compromised rather than completely resolved. Six design tensions were identified in the context of designing for four different phases of freelancing. We observed three patterns in these tensions: 1) the freelancers' need for client involvement in their tasks and career growth, which conflicted with their skepticism that clients had sufficient incentives to be involved; 2) that there was often no viable balancing option for some tensions, but they could be addressed through changes in the platform's incentive structure; and 3) some tensions occurred not only between freelancers and clients, but also within the freelancer community. We present three approaches for addressing these design tensions and discuss how this research can support more equitable and healthy freelancer-client relationships."
AI-Powered Reminders for Collaborative Tasks: Experiences and Futures,"Morrison, Katelyn and Iqbal, Shamsi T. and Horvitz, Eric",10.1145/3653701,2024,"Email continues to serve as a central medium for managing collaborations. While unstructured email messaging is lightweight and conducive to coordination, it is easy to overlook commitments and requests for collaborations that are embedded in the text of free-flowing communications. Twenty-one years ago, Bellotti et al. proposed TaskMaster with the goal of redesigning the email interface to have explicit task management capabilities. Recently, AI-based task recognition and reminder services have been introduced in major email systems as one approach to managing asynchronous collaborations. While these services have been provided to millions of people around the world, there is little understanding of how people interact with and benefit from them. We explore knowledge workers' experiences with Microsoft's Viva Daily Briefing Email to better understand how AI-powered reminders can support asynchronous collaborations. Through semi-structured interviews and surveys, we shed light on how AI-powered reminders are incorporated into workflows to support asynchronous collaborations. We identify what knowledge workers prefer AI-powered reminders to remind them about and how they would like to interact with these reminders. Using mixed methods and a self-assessment methodology, we investigate the relationship between information workers' work styles and the perceived value of the Viva Daily Briefing Email to identify users who are more likely to benefit from AI-powered reminders for asynchronous collaborations. We conclude by discussing the experiences and futures of AI-powered reminders for collaborative tasks and asynchronous collaborations."
"Tinker, Tailor, Configure, Customize: The Articulation Work of Contextualizing an AI Fairness Checklist","Madaio, Michael A. and Chen, Jingya and Wallach, Hanna and Wortman Vaughan, Jennifer",10.1145/3653705,2024,"Many responsible AI resources, such as toolkits, playbooks, and checklists, have been developed to support AI practitioners in identifying, measuring, and mitigating potential fairness-related harms. These resources are often designed to be general purpose in order to be applicable to a variety of use cases, domains, and deployment contexts. However, this can lead to decontextualization, where such resources lack the level of relevance or specificity needed to use them. To understand how AI practitioners might contextualize one such resource, an AI fairness checklist, for their particular use cases, domains, and deployment contexts, we conducted a retrospective contextual inquiry with 13 AI practitioners from seven organizations. We identify how contextualizing this checklist introduces new forms of work for AI practitioners and other stakeholders, as well as opening up new sites for negotiation and contestation of values in AI. We also identify how the contextualization process may help AI practitioners develop a shared language around AI fairness, and we identify tensions related to ownership over this process that suggest larger issues of accountability in responsible AI work."
On the Way to SBOMs: Investigating Design Issues and Solutions in Practice,"Bi, Tingting and Xia, Boming and Xing, Zhenchang and Lu, Qinghua and Zhu, Liming",10.1145/3654442,2024,"The increase of software supply chain threats has underscored the necessity for robust security mechanisms, among which the Software Bill of Materials (SBOM) stands out as a promising solution. SBOMs, by providing a machine-readable inventory of software composition details, play a crucial role in enhancing transparency and traceability within software supply chains. This empirical study delves into the practical challenges and solutions associated with the adoption of SBOMs through an analysis of 4,786 GitHub discussions across 510 SBOM-related projects. Through repository mining and analysis, this research delineates key topics, challenges, and solutions intrinsic to the effective utilization of SBOMs. Furthermore, we shed light on commonly used tools and frameworks for SBOM generation, exploring their respective strengths and limitations. This study underscores a set of findings, for example, there are four phases of the SBOM life cycle, and each phase has a set of SBOM development activities and issues; in addition, this study emphasizes the role SBOM play in ensuring resilient software development practices and the imperative of their widespread adoption and integration to bolster supply chain security. The insights of our study provide vital input for future work and practical advancements in this topic."
Early and Realistic Exploitability Prediction of Just-Disclosed Software Vulnerabilities: How Reliable Can It Be?,"Iannone, Emanuele and Sellitto, Giulia and Iaccarino, Emanuele and Ferrucci, Filomena and De Lucia, Andrea and Palomba, Fabio",10.1145/3654443,2024,"With the rate of discovered and disclosed vulnerabilities escalating, researchers have been experimenting with machine learning to predict whether a vulnerability will be exploited. Existing solutions leverage information unavailable when a CVE is created, making them unsuitable just after the disclosure. This paper experiments with early exploitability prediction models driven exclusively by the initial CVE record, i.e., the original description and the linked online discussions. Leveraging NVD and Exploit Database, we evaluate 72 prediction models trained using six traditional machine learning classifiers, four feature representation schemas, and three data balancing algorithms. We also experiment with five pre-trained large language models (LLMs). The models leverage seven different corpora made by combining three data sources, i.e., CVE description, Security Focus, and BugTraq. The models are evaluated in a realistic, time-aware fashion by removing the training and test instances that cannot be labeled “neutral”&nbsp;with sufficient confidence. The validation reveals that CVE descriptions and Security Focus discussions are the best data to train on. Pre-trained LLMs do not show the expected performance, requiring further pre-training in the security domain. We distill new research directions, identify possible room for improvement, and envision automated systems assisting security experts in assessing the exploitability."
Where Are the Values? A Systematic Literature Review on News Recommender Systems,"Bauer, Christine and Bagchi, Chandni and Hundogan, Olusanmi A. and van Es, Karin",10.1145/3654805,2024,"In the recommender systems field, it is increasingly recognized that focusing on accuracy measures is limiting and misguided. Unsurprisingly, in recent years, the field has witnessed more interest in the research of values “beyond accuracy.” This trend is particularly pronounced in the news domain where recommender systems perform parts of the editorial function, required to uphold journalistic values of news organizations. In the literature, various values and approaches have been proposed and evaluated. This article reviews the current state of the proposed news recommender systems (NRS). We perform a systematic literature review, analyzing 183 papers. The primary aim is to study the development, scope, and focus of value-aware NRS over time. In contrast to previous surveys, we are particularly interested in identifying the range of values discussed and evaluated in the context of NRS and embrace an interdisciplinary view. We identified a total of 40 values, categorized into five value groups. Most research on value-aware NRS has taken an algorithmic approach, whereas conceptual discussions are comparably scarce. Often, algorithms are evaluated by accuracy-based metrics, but the values are not evaluated with respective measures. Overall, our work identifies research gaps concerning values that have not received much attention. Values need to be targeted on a more fine-grained and specific level."
Exploring the Profile of University Assessments Flagged as Containing AI-Generated Material,"Gooch, Daniel and Waugh, Kevin and Richards, Mike and Slaymaker, Mark and Woodthorpe, John",10.1145/3656478,2024,
Balancing Methodological Openness and Control in TPC-UX Pedagogy,"Hunter, Paul Thompson",10.1145/3658422.3658426,2025,"This experience report describes a six-week unit at the intersection of technical and professional communication and user experience design (TPC-UX). Drawing on the work of Patricia Sullivan and Thomas Kent, it argues for a paralogic hermeneutic approach to TPC-UX pedagogy and illustrates how the Double Diamond design process can be used to scaffold assignments and create methodological balance. It also describes how commonplace TPC assignments---such as the technology tutorial---can be reframed according to user experience methods. Details about readings and deliverables are included."
An Experience Report on the Opportunities and Challenges of a Community-Engaged User Experience (CEUX) Pedagogy for a Masters-Level Course,"Patcha, Sidouane and Read, Sarah",10.1145/3658422.3658428,2025,"In this experience report, we share our approach to a Community-Engaged User Experience (CEUX) (Lee et al., 2023) pedagogy for a graduate-level technical writing research methods course in a traditional English department at Portland State University. We narrate the institutional context and history of the course and two sections of the course with different community partners: the Spring 2022 collaboration with the Oregon Health Authority (OHA) focused on the state's COVID-19 response websites and the Spring 2023 collaboration with the Oregon Health &amp; Science University (OHSU) focused on OHSU's main website and its Spanish and Russian microsites. We discuss the opportunities and challenges of each instance of the course and of our variation of a ""one-to-many"" model for CEUX."
Cultivating Empathic Engineering Design through UX Pedagogy: Challenges and Insights,"Rea, Ashley and Balghare, Akshata",10.1145/3658422.3658429,2025,"This article reports on a study about design thinking pedagogy in technical communication courses taken by engineering students. The study suggests that design thinking pedagogy can foster engineering students' empathy for users, particularly their ability to recognize the feelings, knowledge, and perspectives of others. However, its findings reinforce the difficulty faced when encouraging students' societal-level empathy and the limitations of empathy. While engineering students may struggle to transfer user empathy to courses in their major, this study found that engineering students believe design thinking has relevance to their future careers. This article offers teaching strategies and project ideas for technical and professional communication instructors to facilitate students' ability to transfer user empathy to their disciplines."
"Review of ""Engineering Words: Communicating Clearly in the Workplace by Sharon Burton and Bonni Graham Gonzalez,"" Burton, S., &amp; Gonzalez, B. G. (2023). Engineering words: Communicating clearly in the workplace. XML Press.","Wang, Hua",10.1145/3658438.3658444,2025,"Engineering Words: Communicating Clearly in the Workplace by Sharon Burton and Bonni Graham Gonzalez emphasizes that effective communication is essential for engineers to succeed in their careers. The book argues that technical brilliance alone is insufficient, and engineers must also convey their ideas clearly and persuasively to various audiences in business environments. It provides practical guidelines and techniques for various forms of communication that engineers commonly use. These include writing clear business documents, creating compelling presentations, and understanding the business context in which engineering communication occurs. Additionally, the authors integrate concepts of human cognition to explain how people process information and how this understanding can be applied to improve communication. They argue that mastering communication can significantly enhance an engineer's career prospects and influence."
Qualitative Approaches to Voice UX,"Seaborn, Katie and Urakami, Jaqueline and Pennefather, Peter and Miyake, Norihisa",10.1145/3658666,2024,"Voice is a natural mode of expression offered by modern computer-based systems. Qualitative perspectives on voice-based user experiences (voice UX) offer rich descriptions of complex interactions that numbers alone cannot fully represent. We conducted a systematic review of the literature on qualitative approaches to voice UX, capturing the nature of this body of work in a systematic map and offering a qualitative synthesis of findings. We highlight the benefits of qualitative methods for voice UX research, identify opportunities for increasing rigour in methods and outcomes, and distill patterns of experience across a diversity of devices and modes of qualitative praxis."
A Meta-Analysis of Vulnerability and Trust in Human–Robot Interaction,"Mckenna, Peter E. and Ahmad, Muneeb I. and Maisva, Tafadzwa and Nesset, Birthe and Lohan, Katrin and Hastie, Helen",10.1145/3658897,2024,"In human–robot interaction studies, trust is often defined as a process whereby a trustor makes themselves vulnerable to a trustee. The role of vulnerability however is often overlooked in this process but could play an important role in the gaining and maintenance of trust between users and robots. To better understand how vulnerability affects human–robot trust, we first reviewed the literature to create a conceptual model of vulnerability with four vulnerability categories. We then performed a meta-analysis, first to check the overall contribution of the variables included on trust. The results showed that overall, the variables investigated in our sample of studies have a positive impact on trust. We then conducted two multilevel moderator analysis to assess the effect of vulnerability on trust, including: (1) an intercept model that considers the relationship between our vulnerability categories and (2) a non-intercept model that treats each vulnerability category as an independent predictor. Only model 2 was significant, suggesting that to build trust effectively, research should focus on improving robot performance in situations where the users are unsure how reliable the robot will be. As our vulnerability variable is derived from studies of human–robot interaction and researcher reflections about the different risks involved, we relate our findings to these domains and make suggestions for future research avenues."
Topology-aware Federated Learning in Edge Computing: A Comprehensive Survey,"Wu, Jiajun and Dong, Fan and Leung, Henry and Zhu, Zhuangdi and Zhou, Jiayu and Drew, Steve",10.1145/3659205,2024,"The ultra-low latency requirements of 5G/6G applications and privacy constraints call for distributed machine learning systems to be deployed at the edge. With its simple yet effective approach, federated learning (FL) is a natural solution for massive user-owned devices in edge computing with distributed and private training data. FL methods based on FedAvg typically follow a naive star topology, ignoring the heterogeneity and hierarchy of the volatile edge computing architectures and topologies in reality. Several other network topologies exist and can address the limitations and bottlenecks of the star topology. This motivates us to survey network topology-related FL solutions. In this paper, we conduct a comprehensive survey of the existing FL works focusing on network topologies. After a brief overview of FL and edge computing networks, we discuss various edge network topologies as well as their advantages and disadvantages. Lastly, we discuss the remaining challenges and future works for applying FL to topology-specific edge networks."
Deceived by Immersion: A Systematic Analysis of Deceptive Design in Extended Reality,"Hadan, Hilda and Choong, Lydia and Zhang-Kennedy, Leah and Nacke, Lennart E.",10.1145/3659945,2024,"The well-established deceptive design literature has focused on conventional user interfaces. With the rise of extended reality (XR), understanding deceptive design’s unique manifestations in this immersive domain is crucial. However, existing research lacks a full, cross-disciplinary analysis that analyzes how XR technologies enable new forms of deceptive design. Our study reviews the literature on deceptive design in XR environments. We use thematic synthesis to identify key themes. We found that XR’s immersive capabilities and extensive data collection enable subtle and powerful manipulation strategies. We identified eight themes outlining these strategies and discussed existing countermeasures. Our findings show the unique risks of deceptive design in XR, highlighting implications for researchers, designers, and policymakers. We propose future research directions that explore unintentional deceptive design, data-driven manipulation solutions, user education, and the link between ethical design and policy regulations."
Load Balanced PIM-Based Graph Processing,"Zhao, Xiang and Chen, Song and Kang, Yi",10.1145/3659951,2024,"Graph processing is widely used for many modern applications, such as social networks, recommendation systems, and knowledge graphs. However, processing large-scale graphs on traditional Von Neumann architectures is challenging due to the irregular graph data and memory-bound graph algorithms. Processing-in-memory (PIM) architecture has emerged as a promising approach for accelerating graph processing by enabling computation to be performed directly on memory. Despite having many processing units and high local memory bandwidth, PIM often suffers from insufficient global communication bandwidth and high synchronization overhead due to load imbalance. This article proposes GraphB, a novel PIM-based graph processing system, to address all these issues. From the algorithm perspective, we propose a degree-aware graph partitioning algorithm that can generate balanced partitioning at a low cost. From the architecture perspective, we introduce tile buffers incorporated with an on-chip 2D-Mesh, which provides high bandwidth for inter-node data transfer. Dataflow in GraphB is designed to enable computation–communication overlap and dynamic load balancing. In a PyMTL3-based cycle-accurate simulator with five real-world graphs and three common algorithms, GraphB achieves an average 2.2\texttimes{} and maximum 2.8\texttimes{} speedup compared to the SOTA PIM-based graph processing system GraphQ."
Extending Jupyter with Multi-Paradigm Editors,"Weber, Thomas and Ehe, Janina and Mayer, Sven",10.1145/3660247,2024,"Computational notebooks like the Jupyter programming environment have been popular, particularly for developing data-driven applications. One of its main benefits is that it easily supports different programming languages with exchangeable kernels. Thus, it makes the user interface of computational notebooks broadly accessible. While their literate programming paradigm has advantages, we can use this infrastructure to make other paradigms similarly easily and broadly accessible to developers. In our work, we demonstrate how the Jupyter infrastructure can be utilized with different interfaces for different programming paradigms, enabling even greater flexibility for programmers and making it easier for them to adopt different paradigms when they are most suitable. We present a prototype that adds graphical programming and a multi-paradigm editor on top of the Jupyter system. The multi-paradigm editor seamlessly combines the added graphical programming with the familiar notebook interface side-by-side, which can further help developers switch between programming paradigms when desired. A subsequent user evaluation demonstrates the benefits not only of alternate interfaces and paradigms but also of the flexibility of seamlessly switching between them. Finally, we discuss some of the challenges in implementing these systems and how these can enhance the software development process in the future."
Do Words Have Power? Understanding and Fostering Civility in Code Review Discussion,"Rahman, Md Shamimur and Codabux, Zadia and Roy, Chanchal K.",10.1145/3660780,2024,"Modern Code Review (MCR) is an integral part of the software development process where developers improve product quality through collaborative discussions. Unfortunately, these discussions can sometimes become heated by the presence of inappropriate behaviors such as personal attacks, insults, disrespectful comments, and derogatory conduct, often referred to as incivility. While researchers have extensively explored such incivility in various public domains, our understanding of its causes, consequences, and courses of action remains limited within the professional context of software development, specifically within code review discussions. To bridge this gap, our study draws upon the experience of 171 professional software developers representing diverse development practices across different geographical regions. Our findings reveal that more than half of these developers (56.72%) have encountered instances of workplace incivility, and a substantial portion of that group (83.70%) reported experiencing such incidents at least once a month. We also identified 
 
various causes, positive and negative consequences, and potential courses of action for uncivil communication. Moreover, to address the negative aspects of incivility, we propose a model for promoting civility that detects uncivil comments during communication and provides alternative civil suggestions while preserving the original comments’ semantics, enabling developers to engage in respectful and constructive discussions. An in-depth analysis of 2K uncivil review comments using eight different evaluation metrics and a manual evaluation suggested that our proposed approach could generate civil alternatives significantly compared to the state-of-the-art politeness and detoxification models. Moreover, a survey involving 36 developers who used our civility model reported its effectiveness in enhancing online development interactions, fostering better relationships, increasing contributor involvement, and expediting development processes. Our research is a pioneer in generating civil alternatives for uncivil discussions in software development, opening new avenues for research in collaboration and communication within the software engineering context."
Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice,"Khojah, Ranim and Mohamad, Mazen and Leitner, Philipp and de Oliveira Neto, Francisco Gomes",10.1145/3660788,2024,"Large Language Models (LLMs) are frequently discussed in academia and the general public as support tools for virtually any use case that relies on the production of text, including software engineering. Currently, there is much debate, but little empirical evidence, regarding the practical usefulness of LLM-based tools such as ChatGPT for engineers in industry. We conduct an observational study of 24 professional software engineers who have been using ChatGPT over a period of one week in their jobs, and qualitatively analyse their dialogues with the chatbot as well as their overall experience (as captured by an exit survey). We find that rather than expecting ChatGPT to generate ready-to-use software artifacts (e.g., code), practitioners more often use ChatGPT to receive guidance on how to solve their tasks or learn about a topic in more abstract terms. We also propose a theoretical framework for how the (i) purpose of the interaction, (ii) internal factors (e.g., the user's personality), and (iii) external factors (e.g., company policy) together shape the experience (in terms of perceived usefulness and trust). We envision that our framework can be used by future research to further the academic discussion on LLM usage by software engineering practitioners, and to serve as a reference point for the design of future empirical LLM research in this domain."
Learning to Detect and Localize Multilingual Bugs,"Yang, Haoran and Nong, Yu and Zhang, Tao and Luo, Xiapu and Cai, Haipeng",10.1145/3660804,2024,"Increasing studies have shown bugs in multi-language software as a critical loophole in modern software quality assurance, especially those induced by language interactions (i.e., multilingual bugs). Yet existing tool support for bug detection/localization remains largely limited to single-language software, despite the long-standing prevalence of multi-language systems in various real-world software domains. Extant static/dynamic analysis and deep learning (DL) based approaches all face major challenges in addressing multilingual bugs. In this paper, we present xLoc, a DL-based technique/tool for detecting and localizing multilingual bugs. Motivated by results of our bug-characteristics study on top locations of multilingual bugs, xLoc first learns the general knowledge relevant to differentiating various multilingual control-flow structures. This is achieved by pre-training a Transformer model with customized position encoding against novel objectives. Then, xLoc learns task-specific knowledge for the task of multilingual bug detection/localization, through another new position encoding scheme (based on cross-language API vicinity) that allows for the model to attend particularly to control-flow constructs that bear most multilingual bugs during fine-tuning. We have implemented xLoc for Python-C software and curated a dataset of 3,770 buggy and 15,884 non-buggy Python-C samples, which enabled our extensive evaluation of xLoc against two state-of-the-art baselines: fine-tuned CodeT5 and zero-shot ChatGPT. Our results show that xLoc achieved 94.98% F1 and 87.24%@Top-1 accuracy, which are significantly (up to 162.88% and 511.75%) higher than the baselines. Ablation studies further confirmed significant contributions of each of the novel design elements in xLoc. With respective bug-location characteristics and labeled bug datasets for fine-tuning, our design may be applied to other language combinations beyond Python-C."
Mining Action Rules for Defect Reduction Planning,"Oueslati, Khouloud and Laberge, Gabriel and Lamothe, Maxime and Khomh, Foutse",10.1145/3660809,2024,"Defect reduction planning plays a vital role in enhancing software quality and minimizing software maintenance costs. By training a black box machine learning model and “explaining” its predictions, explainable AI for software engineering aims to identify the code characteristics that impact maintenance risks. However, post-hoc explanations do not always faithfully reflect what the original model computes. In this paper, we introduce CounterACT, a Counterfactual ACTion rule mining approach that can generate defect reduction plans without black-box models. By leveraging action rules, CounterACT provides a course of action that can be considered as a counterfactual explanation for the class (e.g., buggy or not buggy) assigned to a piece of code. We compare the effectiveness of CounterACT with the original action rule mining algorithm and six established defect reduction approaches on 9 software projects. Our evaluation is based on (a) overlap scores between proposed code changes and actual developer modifications; (b) improvement scores in future releases; and (c) the precision, recall, and F1-score of the plans. Our results show that, compared to competing approaches, CounterACT’s explainable plans achieve higher overlap scores at the release level (median 95%) and commit level (median 85.97%), and they offer better trade-off between precision and recall (median F1-score 88.12%). Finally, we venture beyond planning and explore leveraging Large Language models (LLM) for generating code edits from our generated plans. Our results show that suggested LLM code edits supported by our plans are actionable and are more likely to pass relevant test cases than vanilla LLM code recommendations."
A Weak Supervision-Based Approach to Improve Chatbots for Code Repositories,"Farhour, Farbod and Abdellatif, Ahmad and Mansour, Essam and Shihab, Emad",10.1145/3660812,2024,"Software chatbots are growing in popularity and have been increasingly used in software projects due to their benefits in saving time, cost, and effort. At the core of every chatbot is a Natural Language Understanding (NLU) component that enables chatbots to comprehend the users' queries. Prior work shows that chatbot practitioners face challenges in training the NLUs because the labeled training data is scarce. Consequently, practitioners resort to user queries to enhance chatbot performance. They annotate these queries and use them for NLU training. However, such training is done manually and prohibitively expensive.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Therefore, we propose AlphaBot to automate the query annotation process for SE chatbots. Specifically, we leverage weak supervision to label users' queries posted to a software repository-based chatbot. To evaluate the impact of using AlphaBot on the NLU's performance, we conducted a case study using a dataset that comprises 749 queries and 52 intents. The results show that using AlphaBot improves the NLU's performance in terms of F1-score, with improvements ranging from 0.96% to 35%. Furthermore, our results show that applying more labeling functions improves the NLU's classification of users' queries. Our work enables practitioners to focus on their chatbots' core functionalities rather than annotating users' queries."
Significant Productivity Gains through Programming with Large Language Models,"Weber, Thomas and Brandmaier, Maximilian and Schmidt, Albrecht and Mayer, Sven",10.1145/3661145,2024,"Large language models like GPT and Codex drastically alter many daily tasks, including programming, where they can rapidly generate code from natural language or informal specifications. Thus, they will change what it means to be a programmer and how programmers act during software development. This work explores how AI assistance for code generation impacts productivity. In our user study (N=24), we asked programmers to complete Python programming tasks supported by a) an auto-complete interface using GitHub Copilot, b) a conversational system using GPT-3, and c) traditionally with just the web browser. Aside from significantly increasing productivity metrics, participants displayed distinctive usage patterns and strategies, highlighting that the form of presentation and interaction affects how users engage with these systems. Our findings emphasize the benefits of AI-assisted coding and highlight the different design challenges for these systems."
A Meta-Study of Software-Change Intentions,"Kr\""{u}ger, Jacob and Li, Yi and Lossev, Kirill and Zhu, Chenguang and Chechik, Marsha and Berger, Thorsten and Rubin, Julia",10.1145/3661484,2024,"Every software system undergoes changes, for example, to add new features, fix bugs, or refactor code. The importance of understanding software changes has been widely recognized, resulting in various techniques and studies, for instance, on change-impact analysis or classifying developers’ activities. Since changes are triggered by developers’ intentions—something they plan or want to change in the system—many researchers have studied intentions behind changes. While there appears to be a consensus among software-engineering researchers and practitioners that knowing the intentions behind software changes is important, it is not clear how developers can actually benefit from this knowledge. In fact, there is no consolidated, recent overview of the state of the art on software-change intentions (SCIs) and their relevance for software engineering. We present a meta-study of 122 publications, which we used to derive a categorization of SCIs and to discuss motivations, evidence, and techniques relating to SCIs. Unfortunately, we found that individual pieces of research are often disconnected from each other, because a common understanding is missing. Similarly, some publications showcase the potential of knowing SCIs, but more substantial research to understand the practical benefits of knowing SCIs is needed. Our contributions can help researchers and practitioners improve their understanding of SCIs and how SCIs can aid software engineering tasks."
A Survey of Graph Neural Networks for Social Recommender Systems,"Sharma, Kartik and Lee, Yeon-Chang and Nambi, Sivagami and Salian, Aditya and Shah, Shlok and Kim, Sang-Wook and Kumar, Srijan",10.1145/3661821,2024,"Social recommender systems (SocialRS) simultaneously leverage the user-to-item interactions as well as the user-to-user social relations for the task of generating item recommendations to users. Additionally exploiting social relations is clearly effective in understanding users’ tastes due to the effects of homophily and social influence. For this reason, SocialRS has increasingly attracted attention. In particular, with the advance of graph neural networks (GNN), many GNN-based SocialRS methods have been developed recently. Therefore, we conduct a comprehensive and systematic review of the literature on GNN-based SocialRS.In this survey, we first identify 84 papers on GNN-based SocialRS after annotating 2,151 papers by following the PRISMA framework (preferred reporting items for systematic reviews and meta-analyses). Then, we comprehensively review them in terms of their inputs and architectures to propose a novel taxonomy: (1) input taxonomy includes five groups of input type notations and seven groups of input representation notations; (2) architecture taxonomy includes eight groups of GNN encoder notations, two groups of decoder notations, and 12 groups of loss function notations. We classify the GNN-based SocialRS methods into several categories as per the taxonomy and describe their details. Furthermore, we summarize benchmark datasets and metrics widely used to evaluate the GNN-based SocialRS methods. Finally, we conclude this survey by presenting some future research directions. GitHub repository with the curated list of papers are available at"
A Systematic Literature Review on Reasons and Approaches for Accurate Effort Estimations in Agile,"Pasuksmit, Jirat and Thongtanunam, Patanamon and Karunasekera, Shanika",10.1145/3663365,2024,"Background: Accurate effort estimation is crucial for planning in Agile iterative development. Agile estimation generally relies on consensus-based methods like planning poker, which require less time and information than other formal methods (e.g., COSMIC) but are prone to inaccuracies. Understanding the common reasons for inaccurate estimations and how proposed approaches can assist practitioners is essential. However, prior systematic literature reviews (SLR) only focus on the estimation practices (e.g., References&nbsp;[26, 127]) and the effort estimation approaches (e.g., Reference&nbsp;[6]). Aim: We aim at identifing themes of reasons for inaccurate estimations and classify approaches to improve effort estimation. Method: We conducted an SLR and identified the key themes and a taxonomy. Results: The reasons for inaccurate estimation are related to information quality, team, estimation practice, project management, and business influences. The effort estimation approaches were the most investigated in the literature, while only a few aim to support the effort estimation process. Yet, few automated approaches are at risk of data leakage and indirect validation scenarios. Recommendations: Practitioners should enhance the quality of information for effort estimation, potentially by adopting an automated approach. Future research should aim at improving the information quality, while avoiding data leakage and indirect validation scenarios."
A Formal Explainer for Just-In-Time Defect Predictions,"Yu, Jinqiang and Fu, Michael and Ignatiev, Alexey and Tantithamthavorn, Chakkrit and Stuckey, Peter",10.1145/3664809,2024,"Just-in-Tim e (JIT) defect prediction has been proposed to help teams prioritize the limited resources on the most risky commits (or pull requests), yet it remains largely a black box, whose predictions are not explainable or actionable to practitioners. Thus, prior studies have applied various model-agnostic techniques to explain the predictions of JIT models. Yet, explanations generated from existing model-agnostic techniques are still not formally sound, robust, and actionable. In this article, we propose FoX, a Formal eXplainer for JIT Defect Prediction, which builds on formal reasoning about the behavior of JIT defect prediction models and hence is able to provide provably correct explanations, which are additionally guaranteed to be minimal. Our experimental results show that FoX &nbsp;is able to efficiently generate provably correct, robust, and actionable explanations, while existing model-agnostic techniques cannot. Our survey study with 54 software practitioners provides valuable insights into the usefulness and trustworthiness of our FoX &nbsp;approach; 86% of participants agreed that our approach is useful, while 74% of participants found it trustworthy. Thus, this article serves as an important stepping stone towards trustable explanations for JIT models to help domain experts and practitioners better understand why a commit is predicted as defective and what to do to mitigate the risk."
What Makes a Good TODO Comment?,"Wang, Haoye and Gao, Zhipeng and Bi, Tingting and Grundy, John and Wang, Xinyu and Wu, Minghui and Yang, Xiaohu",10.1145/3664811,2024,"Software development is a collaborative process that involves various interactions among individuals and teams. TODO comments in source code play a critical role in managing and coordinating diverse tasks during this process. However, this study finds that a large proportion of open-source project TODO comments are left unresolved or take a long time to be resolved. About 46.7% of TODO comments in open-source repositories are of low-quality (e.g., TODOs that are ambiguous, lack information, or are useless to developers). This highlights the need for better TODO practices. In this study, we investigate four aspects regarding the quality of TODO comments in open-source projects: (1) the prevalence of low-quality TODO comments; (2) the key characteristics of high-quality TODO comments; (3) how are TODO comments of different quality managed in practice; and (4) the feasibility of automatically assessing TODO comment quality. Examining 2,863 TODO comments from Top100 GitHub Java repositories, we propose criteria to identify high-quality TODO comments and provide insights into their optimal composition. We discuss the lifecycle of TODO comments with varying quality. To assist developers, we construct deep learning-based methods that show promising performance in identifying the quality of TODO comments, potentially enhancing development efficiency and code quality."
Intent and Extent: Computer Science Concepts and Practices in Integrated Computing,"Margulieux, Lauren E. and Liao, Yin-Chan and Anderson, Erin and Parker, Miranda C. and Calandra, Brendan D.",10.1145/3664825,2024,"Integrated computing curricula combine learning objectives in computing with those in another discipline, like literacy, math, or science, to give all students experience with computing, typically before they must decide whether to take standalone CS courses. One goal of integrated computing curricula is to provide an accessible path to an introductory computing course by introducing computing concepts and practices in required courses. This study analyzed integrated computing curricula to determine which CS practices and concepts are taught, how extensively the curricula are taught, and, by extension, how they might prepare students for later computing courses. The authors conducted a content analysis to examine primary and lower secondary (i.e., K-8) curricula that are taught in non-CS classrooms, have explicit CS learning objectives (i.e., CS+X), and that took 5+ hours to complete. Lesson plans, descriptions, and resources were scored based on frameworks developed from the K-12 CS Framework, including programming concepts, non-programming CS concepts, and CS practices. The results found that curricula most extensively taught introductory concepts and practices, such as sequences, and rarely taught more advanced content, such as conditionals. Students who engage with most of these curricula would have no experience working with fundamental concepts, like variables, operators, data collection or storage, or abstraction in the context of a program. While this focus might be appropriate for integrated curricula, it has implications for the prior knowledge that students should be expected to have when starting standalone computing courses."
The First Principles: Setting the Context for a Safe and Secure Metaverse,"Gupta, Ankur and Sawhney, Sahil and Kompella, Kashyap",10.1145/3665495,2024,"The metaverse delivered through converged and amalgamated technologies holds promise. No wonder technology heavyweights, large corporates, research organizations and businesses cutting across industry verticals are racing to put in place a metaverse-first strategy. The bets on consumers rapidly migrating from traditional social networks and collaborative applications to more immersive digital experiences have been placed. However, the transition is not expected to be seamless. Privacy, safety and security concerns abound in the early versions of the metaverse. Increased regulatory oversight and diverse national laws threaten to derail the hype around the metaverse. It is increasingly clear that the final iteration of the metaverse will need to assuage the concerns of individual users while addressing complex legal and regulatory requirements. Thus, a multi-perspective approach needs to be adopted to help set the agenda for the evolution of the metaverse. This research paper examines the different aspects and challenges which the future metaverse will need to address. A set of “first principles” are formulated, which if implemented will lead to the development of an equitable, inclusive, safe and secure metaverse."
Understanding Informatics in Continuing Vocational Education and Training Data in Germany,"D\""{o}rpinghaus, Jens and Binnewitt, Johanna and Samray, David and Hein, Kristine",10.1145/3665932,2024,"Objectives. The purpose of this study is to reveal the importance of informatics in continuing vocational education in Germany. The labour market is a field with diverse data structures and multiple applications, for example connecting jobseekers and trainings or jobs. The labour market heavily relies on vocational education and training and advanced vocational qualification to meet challenges, e.g., digitalization. Study Methods. Since continuing vocational education and training (CVET) is a structurally important lever for the digital transformation of work, this article presents a methodological procedure for content analysis that provides information about the significance of computer science in unregulated continuing education offerings and in formal continuing education regulations. Findings. The question of the extent to which continuing education programs include informaticss topics is investigated, assuming that they can be found in continuing education as cross-cutting topics in a wide variety of thematic contexts. Our results indicating the need for training in computing education. At the same time, computing education offers the highest share of unregulated CVET programs. This could reflect the fact that training and further education regulations in Germany are designed open to technology. Conclusions. We present a novel and unique approach to analyze the importance of informatics and digitalization in CVET advertisements and official regulations for the same."
“Are you feeling sick?” – A systematic literature review of cybersickness in virtual reality,"Biswas, Nilotpal and Mukherjee, Anamitra and Bhattacharya, Samit",10.1145/3670008,2024,"Cybersickness (CS), also known as visually induced motion sickness (VIMS), is a condition that can affect individuals when they interact with virtual reality (VR) technology. This condition is characterized by symptoms such as nausea, dizziness, headaches, eye fatigue, and so on, and can be caused by a variety of factors. Finding a feasible solution to reduce the impact of CS is extremely important as it will greatly enhance the overall user experience and make VR more appealing to a wider range of people. We have carefully compiled a list of 223 highly pertinent studies to review the current state of research on the most essential aspects of CS. We have provided a novel taxonomy that encapsulates various aspects of CS measurement techniques found in the literature. We have proposed a set of CS mitigation guidelines for both developers and users. We have also discussed various CS-inducing factors and provided a taxonomy that tries to capture the same. Overall, our work provides a comprehensive overview of the current state of research in CS with a particular emphasis on different measurement techniques and CS mitigation strategies, identifies research gaps in the literature, and provides recommendations for future research in the field."
ZTA-IoT: A Novel Architecture for Zero-Trust in IoT Systems and an Ensuing Usage Control Model,"Ameer, Safwa and Praharaj, Lopamudra and Sandhu, Ravi and Bhatt, Smriti and Gupta, Maanak",10.1145/3671147,2024,"Recently, several researchers motivated the need to integrate Zero Trust (ZT) principles when designing and implementing authentication and authorization systems for IoT. An integrated Zero Trust IoT system comprises the network infrastructure (physical and virtual) and operational policies in place for IoT as a product of a ZT architecture plan. This article proposes a novel Zero Trust architecture for IoT systems called ZTA-IoT. Additionally, based on different types of interactions between various layers and components in this architecture, we present ZTA-IoT-ACF, an access control framework that recognizes different interactions that need to be controlled in IoT systems. Within this framework, the article then refines its focus to object-level interactions, i.e., interactions where the target resource is a device (equivalently a thing) or an information file generated or stored by a device. Building on the recently proposed Zero Trust score-based authorization framework (ZT-SAF), we develop the object-level Zero Trust score-based authorization framework for IoT systems, denoted as ZTA-IoT-OL-SAF, to govern access requests in this context. With this machinery in place, we finally develop a novel usage control model for users-to-objects and devices-to-objects interactions, denoted as UCON  (_{IoT}) . We give formal definitions, illustrative use cases, and a proof-of-concept implementation of UCON (_{IoT}) . This article is a first step toward establishing a rigorous formally defined score-based access control framework for Zero Trust IoT systems."
HFOSS Education,"Hislop, Gregory W. and Ellis, Heidi J. C.",10.1145/3672203.3672210,2024,"Open source software has become part of the mainstream of software development, with the adoption of open source by a large majority of business and governmental organizations. Results of recent surveys [1] indicate that 95% of responding organizations use open source in mission critical software applications and plan to increase spending on open source activities in the coming years."
The Landscape of User-centered Misinformation Interventions - A Systematic Literature Review,"Hartwig, Katrin and Doell, Frederic and Reuter, Christian",10.1145/3674724,2024,"Misinformation is one of the key challenges facing society today. User-centered misinformation interventions as digital countermeasures that exert a direct influence on users represent a promising means to deal with the large amounts of information available. While an extensive body of research on this topic exists, researchers are confronted with a diverse research landscape spanning multiple disciplines. This review systematizes the landscape of user-centered misinformation interventions to facilitate knowledge transfer, identify trends, and enable informed decision-making. Over 6,000 scholarly publications were screened, and a systematic literature review (N=172) was conducted. A taxonomy was derived regarding intervention design (e.g., labels, showing indicators of misinformation, corrections, removal, or visibility reduction of content), user interaction (active or passive), and timing (e.g., pre or post exposure to misinformation or on request of the user). We provide a structured overview of approaches across multiple disciplines and derive six overarching challenges for future research regarding transferability of approaches to (1) novel platforms and (2) emerging video- and image-based misinformation, the sensible combination of automated mechanisms with (3) human experts and (4) user-centered feedback to facilitate comprehensibility, (5) encouraging media literacy without misinformation exposure, and (6) adequately addressing particularly vulnerable users such as older people or adolescents."
Building Guardrails in AI Systems with Threat Modeling,"Dev, Jayati and Akhuseyinoglu, Nuray Baltaci and Kayas, Golam and Rashidi, Bahman and Garg, Vaibhav",10.1145/3674845,2025,"Much like cars, AI technologies must undergo rigorous testing to ensure their safety and reliability. However, just as a 16-wheel truck’s brakes are different from that of a standard hatchback, AI models too may need distinct analyses based on their risk, size, application domain, and other factors. Prior research has attempted to do this, by identifying areas of concern for AI/ML applications and tools needed to simulate the effect of adversarial actors. However, currently, a variety of frameworks exist which poses challenges due to inconsistent terminology, focus, complexity, and interoperability issues, hindering effective threat discovery. In this article, we present a meta-analysis of 14 AI threat modeling frameworks, providing a streamlined set of questions for AI/ML threat analysis. We then review this library, incorporating feedback from 10 experts to refine the questions. This refined set of questions allow practitioners to seamlessly integrate threat analysis for comprehensive manual evaluation of a wide range of AI/ML applications."
Augmented Reality on the Move: A Systematic Literature Review for Vulnerable Road Users,"Stefanidi, Helen and Tatzgern, Markus and Meschtscherjakov, Alexander",10.1145/3676490,2024,"Due to the continuous improvement of Augmented Reality (AR) head-mounted displays (HMDs), these devices are bound to be increasingly integrated into our daily routines. So far, a major focus of AR research has been on indoor usage and deployment. However, since seamlessly supporting users in their activities while being on-the-move in various outdoor contexts becomes increasingly important, there is a need to investigate the current state-of-the-art of AR technologies while people are in motion outdoors. Therefore, we conducted a systematic literature review of pertinent HCI publications, specifically looking into applications concerning vulnerable road users. We identify the contexts in which such technologies have been researched, prevailing challenges in the field, and applied methodological approaches. Our findings show that most contributions address pedestrians, a shift towards HMDs, and a prevalence of lab studies due to technology limitations. Based on our findings, we discuss trends, existing gaps and opportunities for future research."
Evolution-Aware Constraint Derivation Approach for Software Remodularization,"Meng, Fanyi and Wang, Ying and Chong, Chun Yong and Yu, Hai and Zhu, Zhiliang",10.1145/3676960,2024,"Existing software clustering techniques tend to ignore prior knowledge from domain experts, leading to results (suggested big-bang remodularization actions) that cannot be acceptable to developers. Incorporating domain experts knowledge or constraints during clustering ensures the obtained modularization aligns with developers’ perspectives, enhancing software quality. However, manual review by knowledgeable domain experts for constraint generation is time-consuming and labor-intensive. In this article, we propose an evolution-aware constraint derivation approach, Escort, which automatically derives clustering constraints based on the evolutionary history from the analyzed software. Specifically, Escort can serve as an alternative approach to derive implicit and explicit constraints in situations where domain experts are absent. In the subsequent constrained clustering process, Escort can be considered as a framework to help supplement and enhance various unconstrained clustering techniques to improve their accuracy and reliability. We evaluate Escort based on both quantitative and qualitative analysis. In quantitative validation, Escort, using generated clustering constraints, outperforms seven classic unconstrained clustering techniques. Qualitatively, a survey with developers from five IT companies indicates that 89% agree with Escort’s clustering constraints. We also evaluate the utility of refactoring suggestions from our constrained clustering approach, with 54% acknowledged by project developers, either implemented or planned for future releases."
Discussing the Protagonist Role of Students in Game-Based Learning,"Weixelbraun, Petra F. and G\""{o}bl, Barbara and Steinb\""{o}ck, Matthias and Duvivi\'{e}, Mirjam and Kayali, Fares",10.1145/3677065,2024,"In the ongoing process of the digital transformation of society, corresponding competencies are required from children more than ever. The concept of computational empowerment (CE) promotes confident and reflective engagement with digital technology, which often falls short in formal education. This work outlines how both entertainment and serious games can be used to support the cause of CE and what opportunities they offer for more empowering learning in school settings. The use of digital games appears to be a promising way to connect with the lifeworld of young people and thus offering a low-threshold and differentiated starting point. In this context, the focus should be placed on a stronger establishment of the protagonist role to empower students to engage with digital technologies in depth.In the Serious Game Changers project, students created their own educational modules on specific Sustainable Development Goals (SDGs) using digital games and creating new artifacts as learning objects through remixing game elements. In this project, students should be empowered to become active participants by designing with the help of commercial and serious games. Thus giving them their own voice in future (formal) educational settings and promoting future skills in order to make them self-confident 'protagonists' of their surroundings."
How To Tame a Toxic Player? A Systematic Literature Review on Intervention Systems for Toxic Behaviors in Online Video Games,"Wijkstra, Michel and Rogers, Katja and Mandryk, Regan L. and Veltkamp, Remco C. and Frommel, Julian",10.1145/3677080,2024,"Toxic behavior is known to cause harm in online games. Players regularly experience negative, hateful, or inappropriate behavior. Interventions, such as banning players or chat message filtering, can help combat toxicity but are not widely available or even comprehensively studied regarding their approaches and evaluations. We conducted a systematic literature review that provides insights into the current state of interventions literature, outlining their strengths and shortcomings. We identified 36 interventions and qualitatively analyzed their approaches. We describe the types of toxicity being addressed, the entities through which they act, the methods used by intervention systems, and how they are evaluated. Our results provide guidance for future interventions, outlining a design space based on known systems. Furthermore, our findings highlight gaps in the literature, e.g., a sparsity of empirical evaluations, and underexplored areas in the design space, enabling researchers to explore novel directions for future interventions."
"""Wow another fake game from YouTube ad"": Unpacking Fake Games Through a Mixed-Methods Investigation","Moradzadeh, Sam and Kou, Yubo",10.1145/3677115,2024,"Mobile games have become highly popular and profitable. While much work has been done to understand deceptive patterns of games and some unethical practices they apply, little is known about fake games, an emergent phenomenon in mobile gaming. To answer this question, we conducted two studies: a walkthrough method to characterize fake games, and a thematic analysis of user reviews to gain understanding from the user perspective. We found five types of misalignments that render a game fake and identified four primary facets of player experience with fake games. These misalignments act as realization points in the users' decision-making to define games as being fake. We discuss the fakeness of fake games, how the formation of an ecosystem helps with the circulation of fakeness, as well as challenges to governing fake games. Lastly, we propose implications for research and design on how to mitigate and identify fake games."
A Comprehensive Analysis of Explainable AI for Malware Hunting,"Saqib, Mohd and Mahdavifar, Samaneh and Fung, Benjamin C. M. and Charland, Philippe",10.1145/3677374,2024,"In the past decade, the number of malware variants has increased rapidly. Many researchers have proposed to detect malware using intelligent techniques, such as Machine Learning (ML) and Deep Learning (DL), which have high accuracy and precision. These methods, however, suffer from being opaque in the decision-making process. Therefore, we need Artificial Intelligence (AI)-based models to be explainable, interpretable, and transparent to be reliable and trustworthy. In this survey, we reviewed articles related to Explainable AI (XAI) and their application to the significant scope of malware detection. The article encompasses a comprehensive examination of various XAI algorithms employed in malware analysis. Moreover, we have addressed the characteristics, challenges, and requirements in malware analysis that cannot be accommodated by standard XAI methods. We discussed that even though Explainable Malware Detection (EMD) models provide explainability, they make an AI-based model more vulnerable to adversarial attacks. We also propose a framework that assigns a level of explainability to each XAI malware analysis model, based on the security features involved in each method. In summary, the proposed project focuses on combining XAI and malware analysis to apply XAI models for scrutinizing the opaque nature of AI systems and their applications to malware analysis."
Converging Measures and an Emergent Model: A Meta-Analysis of Human-Machine Trust Questionnaires,"Razin, Yosef S. and Feigh, Karen M.",10.1145/3677614,2024,"Trust is crucial for technological acceptance, continued usage, and teamwork. However, human-robot trust, and human-machine trust more generally, suffer from terminological disagreement and construct proliferation. By comparing, mapping, and analyzing well-constructed trust survey instruments, this work uncovers a consensus structure of trust in human–machine interaction. To do so, we identify the most frequently cited and best-validated human-machine and human-robot trust questionnaires as well as the best-established factors that form the dimensions and antecedents of such trust. To reduce both confusion and construct proliferation, we provide a detailed mapping of terminology between questionnaires. Furthermore, we perform a meta-analysis of the regression models which emerged from the experiments that employed multi-factorial survey instruments. Based on this meta-analysis, we provide the most complete, experimentally validated model of human-machine and human-robot trust to date. This convergent model establishes an integrated framework for future research. It determines the current boundaries of trust measurement and where further investigation and validation are necessary. We close by discussing how to choose an appropriate trust survey instrument and how to design for trust. By identifying the internal workings of trust, a more complete basis for measuring trust is developed that is widely applicable."
A Disruptive Research Playbook for Studying Disruptive Innovations,"Storey, Margaret-Anne and Russo, Daniel and Novielli, Nicole and Kobayashi, Takashi and Wang, Dong",10.1145/3678172,2024,"As researchers today, we are witnessing a fundamental change in our technologically-enabled world due to the advent and diffusion of highly disruptive technologies such as generative Artificial Intelligence (AI), Augmented Reality (AR) and Virtual Reality (VR). In particular, software engineering has been profoundly affected by the transformative power of disruptive innovations for decades, with a significant impact of technical advancements on social dynamics due to its socio-technical nature. In this article, we reflect on the importance of formulating and addressing research problems in software engineering through a socio-technical lens, thus ensuring a holistic understanding of the complex phenomena in this field. We propose a research playbook with the aim of providing a guide to formulate compelling and socially relevant research questions and to identify the appropriate research strategies for empirical investigations, with an eye on the long-term implications of technologies or their use. We showcase how to apply the research playbook. Firstly, we show how it can be used retrospectively to reflect on a prior disruptive technology, Stack Overflow, and its impact on software development. Secondly, we show how it can be used to question the impact of two current disruptive technologies: AI and AR/VR. Finally, we introduce a specialized GPT model to support the researcher in framing future investigations. We conclude by discussing the broader implications of adopting the playbook for both researchers and practitioners in software engineering and beyond."
Spatial Computing Opportunities in Biomedical Decision Support: The Atlas-EHR Vision,"Farhadloo, Majid and Sharma, Arun and Shekhar, Shashi and Markovic, Svetomir",10.1145/3679201,2024,"We consider the problem of reducing the time that healthcare professionals need to understand the patient’s medical history through the next generation of biomedical decision support. This problem is societally important because it has the potential to improve healthcare quality and patient outcomes. However, navigating electronic health records (EHR) is challenging due to high patient-doctor ratios, potentially long medical histories, urgency of treatment for some medical conditions, and patient variability. The current EHR systems provide only a longitudinal view of patient medical history, which is time-consuming to browse, and doctors often need to engage nurses, residents, and others for initial analysis. To overcome this limitation, we envision an alternative spatial representation of patient histories (e.g., electronic health records) and other biomedical data in the form of Atlas-EHR. Just like Google Maps, which allows a global, national, regional, and local view, Atlas-EHR can start with an overview of the patient’s anatomy and history before drilling down to spatially anatomical subsystems, their individual components, or subcomponents. Atlas-EHR presents a compelling opportunity for spatial computing since healthcare is almost a fifth of the US economy. However, traditional spatial computing designed for geographic use cases (e.g., navigation, land survey, and mapping) faces many hurdles in the biomedical domain. This article presents several open research questions under this theme in five broad areas of spatial computing."
Data Farming the Parameters of Simulation-Optimization Solvers,"Shashaani, Sara and Eckman, David and Sanchez, Susan",10.1145/3680282,2024,"The performance of a simulation-optimization algorithm, a.k.a. a solver, depends on its parameter settings. Much of the research to date has focused on how a solver’s parameters affect its convergence and other asymptotic behavior. While these results are important for providing a theoretical understanding of a solver, they can be of limited utility to a user who must set up and run the solver on a particular problem. When running a solver in practice, good finite-time performance is paramount. In this article, we explore the relationship between a solver’s parameter settings and its finite-time performance by adopting a data farming approach. The approach involves conducting and analyzing the outputs of a designed experiment wherein the factors are the solver’s parameters and the responses are assorted performance metrics measuring the solver’s speed and solution quality over time. We demonstrate this approach with a study of the ASTRO-DF solver when solving a stochastic activity network problem and an inventory control problem. Through these examples, we show that how some of the solver’s parameters are set greatly affects its ability to achieve rapid, reliable progress and gain insights into the solver’s inner workings. We discuss the implications of using this framework for tuning solver parameters, as well as for addressing related questions of interest to solver specialists and generalists."
MULTICR: Predicting Merged and Abandoned Code Changes in Modern Code Review Using Multi-Objective Search,"Chouchen, Moataz and Ouni, Ali and Mkaouer, Mohamed Wiem",10.1145/3680472,2024,"Modern Code Review (MCR) is an essential process in software development to ensure high-quality code. However, developers often spend considerable time reviewing code changes before being merged into the main code base. Previous studies attempted to predict whether a code change was going to be merged or abandoned soon after it was submitted to improve the code review process. However, these approaches require complex cost-sensitive learning, which makes their adoption challenging since it is difficult for developers to understand the main factors behind the models’ predictions. To address this issue, we introduce in this article, MULTICR, a multi-objective search-based approach that uses Multi-Objective Genetic Programming (MOGP) to learn early code review prediction models as IF-THEN rules. MULTICR evolves predictive models while maximizing the accuracy of both merged and abandoned classes, eliminating the need for misclassification cost estimation. To evaluate MULTICR, we conducted an empirical study on 146,612 code reviews from Eclipse, LibreOffice, and Gerrithub. The obtained results show that MULTICR outperforms existing baselines in terms of Matthew Correlation Coefficient (MCC) and F1 scores while learning less complex models compared to decision trees. Our experiments also showed how MULTICR allows identifying the main factors related to abandoned code reviews as well as their associated thresholds, making it a promising approach for early code review prediction with notable performance and inter-operability. Additionally, we qualitatively evaluate MULTICR by conducting a user study through semi-structured interviews involving 10 practitioners from different organizations. The obtained results indicate that 90% of the participants find that MULTICR is useful and can help them to improve the code review process. Additionally, the learned IF-THEN rules of MULTICR are transparent."
"AI-Assisted Diagnosing, Monitoring and Treatment of Mental Disorders: A Survey","Muetunda, Faustino and Sabry, Soumaya and Jamil, M. Luqman and Pais, Sebasti\~{a}o and Dias, Ga\""{e}l and Cordeiro, Jo\~{a}o",10.1145/3681794,2024,"Globally, one in seven people has some kind of mental or substance use disorder that affects their thinking, feelings and behaviour in everyday life. People with mental health disorders can continue their normal lives with proper treatment and support. Mental well-being is vital for physical health. The use of AI in mental health areas has grown exponentially in the last decade. However, mental disorders are still complex to diagnose due to similar and common symptoms for numerous mental illnesses, with a minute difference. Intelligent systems can help us identify mental diseases precisely, which is a critical step in diagnosing. Using these systems efficiently can improve the treatment and rapid recovery of patients. We survey different artificial intelligence systems used in mental healthcare, such as mobile applications, machine learning and deep learning methods, and multi-modal systems and draw comparisons from recent developments and related challenges. Also, we discuss types of mental disorders and how these different techniques can support the therapist in diagnosing, monitoring, and treating patients with mental disorders."
A Systematic Review of Digital Twin Technology for Home Care,"Zafar, Raja Omman and Rybarczyk, Yves and Borg, Johan",10.1145/3681797,2024,"The concept of digital twin has captured significant attention in recent years, and its potential application within the domain of home care has been explored in several studies. This review endeavors to provide a comprehensive overview of digital twin technology and its applications in the realm of home care, delineating the key attributes and challenges entailed in their implementation. A systematic search was conducted across five databases, namely ACM digital library, IEEE Xplore, PubMed, Scopus, and Web of Science. Findings from forty-five included articles were categorized employing a systematic approach, highlighting the technology's deployment in remote older adults’ care monitoring, health issue prediction and personalized treatment planning. Furthermore, this review identified the challenges of integrating digital twins into the home care sector. Despite recognition of its potential, there is a distinct lack in the literature of in-depth studies specifically exploring the implementation of digital twin technology in home care, highlighting the need for further research."
Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning,"Liu, Xiao-Yang and Zhu, Rongyi and Zha, Daochen and Gao, Jiechao and Zhong, Shan and White, Matt and Qiu, Meikang",10.1145/3682068,2025,"The surge in interest and application of large language models (LLMs) has sparked a drive to fine-tune these models to suit specific applications, such as finance and medical science. However, concerns regarding data privacy have emerged, especially when multiple stakeholders aim to collaboratively enhance LLMs using sensitive data. In this scenario, federated learning becomes a natural choice, allowing decentralized fine-tuning without exposing raw data to central servers. Motivated by this, we investigate how data privacy can be ensured in LLM fine-tuning through practical federated learning approaches, enabling secure contributions from multiple parties to enhance LLMs. Yet, challenges arise: (1) despite avoiding raw data exposure, there is a risk of inferring sensitive information from model outputs, and (2) federated learning for LLMs incurs notable communication overhead. To address these challenges, this article introduces DP-LoRA, a novel federated learning algorithm tailored for LLMs. DP-LoRA preserves data privacy by employing a Gaussian mechanism that adds noise in weight updates, maintaining individual data privacy while facilitating collaborative model training. Moreover, DP-LoRA optimizes communication efficiency via low-rank adaptation, minimizing the transmission of updated weights during distributed training. The experimental results across medical, financial, and general datasets using various LLMs demonstrate that DP-LoRA effectively ensures strict privacy constraints while minimizing communication overhead."
A Review on the Use of Physiological Signals for Assessing Postoperative Pain,"Pais, Daniela and Br\'{a}s, Susana and Sebasti\~{a}o, Raquel",10.1145/3685674,2024,"The assessment of pain intensity after surgery is important for guiding pain management. Due to the limitations of current pain evaluation tools, there is increasing interest in using objective methods to assess pain in clinical settings. This literature review aims to provide an overview of physiological methods for postoperative pain evaluation. The Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) guidelines were followed to perform a literature search on the Scopus database from 2002 to March 2024. Sixty-one studies were included in this review investigating parameters derived from six physiological signals, including Electrocardiogram (ECG), Photoplethysmogram (PPG), Skin Conductance (SC), Electroencephalogram (EEG), Electromyogram (EMG), and pupil size and reflexes. Parameters extracted from ECG, PPG, and SC signals were the most commonly studied. While there is evidence to support the use of physiological parameters as measures for postoperative pain evaluation, further research is needed to establish the reliability and generalizability of these parameters to develop an indicator that can be consistently applied across various clinical settings. For this purpose, this review describes current research findings and identifies limitations and directions for future work that should be considered in upcoming research on postoperative pain assessment."
"“The Devil You Know”: Barriers and Opportunities for Co-Designing Microclimate Sensors, A Case Study of Manoomi","Greenlee, Eric and Rothrock, Blaine and Kim, Hyeonwook and Zegura, Ellen and Hester, Josiah",10.1145/3685695,2024,"Current environmental challenges have profound local consequences and often benefit from the collection of fine-grained microclimate data. Advances in wireless sensor networks and the Internet of Things have led to technologies nominally suited to support remote sensing; however, in practice long-running deployments of in-field environmental sensors are rare. Field conditions are often remote and culturally sensitive, with limited power, Internet, transportation, and human infrastructure; advances in device technology alone will not suffice. We ask how communities, Internet of Things researchers, government, and other interested parties can work together to co-design useful, low burden, sustainability-focused infrastructure. Toward this end, we conducted 11 semi-structured interviews with 13 experts who use or rely on environmental sensing technology. To complement our interview data, we engaged in three months of participant observation while immersed in organizations specifically working toward manoomin (wild rice) conservation. We make two primary contributions. First, we confirm and enrich a five-stage model, the microclimate sensor lifecycle, focusing on desired features and persistent challenges. Second, we outline a space for co-design of microclimate sensors with emphasis on the cost of experience, the generally unaddressed issue of technical usability in the messy field, and the opportunity for community engagement to improve technical design and outcomes. Furthermore, we discuss future design opportunities, recommendations, and challenges in the microclimate sensor design, deployment, and sustainability space."
COVID-19 Modeling: A Review,"Cao, Longbing and Liu, Qing",10.1145/3686150,2024,"The SARS-CoV-2 viruses and their triggered COVID-19 pandemic have fundamentally reshaped the world in almost every aspect, their evolution and influences remain. While over a million of literature have been produced on these unprecedented, overwhelming global disaster, one critical question is open: How has COVID-19 been quantified globally? This further inspires many other questions: What COVID-19 problems have been modeled? How have modeling methods in areas such as epidemiology, artificial intelligence (AI), data science, machine learning, deep learning, mathematics and social science played their roles in characterizing COVID-19? Where are the gaps and issues of these COVID-19 modeling studies? What are the lessons for quantifying future disasters? Answering these questions involves the analysis of a very broad spectrum of literature across different disciplines and domains. Distinguishing from specific efforts, this review takes the first attempt to generate a systematic, structured and contrastive landscape and taxonomy of global COVID-19 modeling. First, the surveyed problems span over a full range of COVID-19, including epidemic transmission processes, case identification and tracing, infection diagnosis and medical treatments, non-pharmaceutical interventions and their influence, drug and vaccine development, psychological, economic and social influence and impact, and misinformation, and so on. Second, the reviewed modeling methods traverse all relevant disciplines, from statistic modeling to epidemic modeling, medical analysis, biomedical analysis, AI, deep and machine learning, analytics, and simulation. Critical analyses further identify significant issues and gaps, for example, simple techniques and similar problems have been overwhelmingly addressed everywhere, while intrinsic and foundational issues and deep insights have been overlooked. The review discloses significant opportunities for more deeply, effectively and uniquely quantifying COVID-19-like global disasters from their intrinsic working mechanisms, interactions and dynamics."
Articulation Work and Tinkering for Fairness in Machine Learning,"Fahimi, Miriam and Russo, Mayra and Scott, Kristen M. and Vidal, Maria-Esther and Berendt, Bettina and Kinder-Kurlanda, Katharina",10.1145/3686973,2024,"The field of fair AI aims to counter biased algorithms through computational modelling. However, it faces increasing criticism for perpetuating the use of overly technical and reductionist methods. As a result, novel approaches appear in the field to address more socially-oriented and interdisciplinary (SOI) perspectives on fair AI. In this paper, we take this dynamic as the starting point to study the tension between computer science (CS) and SOI research. By drawing on STS and CSCW theory, we position fair AI research as a matter of 'organizational alignment': what makes research 'doable' is the successful alignment of three levels of work organization (the social world, the laboratory, and the experiment). Based on qualitative interviews with CS researchers, we analyze the tasks, resources, and actors required for doable research in the case of fair AI. We find that CS researchers engage with SOI research to some extent, but organizational conditions, articulation work, and ambiguities of the social world constrain the doability of SOI research for them. Based on our findings, we identify and discuss problems for aligning CS and SOI as fair AI continues to evolve."
An Empirical Study on Social Anxiety in a Virtual Environment through Mediating Variables and Multiple Sensor Data,"Kim, Eunji and Jin, Seungwan and Han, Kyungsik",10.1145/3686977,2024,"Social anxiety disorder is a psychological condition characterized by excessive nervousness in social situations, such as interpersonal interactions. Exposure therapy has shown benefits in its treatment, and virtual reality (VR) technology has gained much attention for reducing physical and psychological distance and providing additional quantitative evidence from the data generated by standard VR devices (e.g., head-mounted display). Clinical psychology studies have highlighted the importance of mediating variables of social anxiety; however, existing VR-based social anxiety studies have neglected such variables with respect to user experience and data analysis in the context of VR, although these variables could provide insights into the design and use of VR for the treatment of social anxiety. In this study, we focused on two representative mediating variables of social anxiety: (1) the gap between self-presentation motivation and expectancy, and (2) self-focused attention. We used sensor data (e.g., head movement, eye movement, eye gaze, and psychological signals) to investigate the impact of these variables on users' anxiety responses in VR. We developed &lt;u&gt;VR&lt;/u&gt;-based &lt;u&gt;S&lt;/u&gt;ocial Anxiety Support &lt;u&gt;T&lt;/u&gt;ool (VRST) that reflects the theoretical design elements of effective anxiety provocation. Based on the results of a user study with 30 participants, we confirmed that the mediating variables were associated with social anxiety in the VR environment. We also found that the mediating variables were associated with eye gaze, eye pupil, head movement, and body temperature. Our study results provide researchers, designers, and practitioners with empirical evidence and implications for the use of VR technology and sensor data in the mental health context."
"Forming Shared Interest Pods: Barriers to Self-Assembly of Interest-Based Small Groups, and Dynamics of Retaining and Giving up Control to Find Collective Fit","Palea, Dustin and Guo, Ana and Nair, Atira and Anderson, Ryan and Charagulla, Nisha and Su, Norman Makoto and Lee, David T.",10.1145/3686984,2024,"We explore the challenges individuals face when seeking like-minded partners to pursue shared interests in small groups, specifically outside the workplace, classroom, or other externally organized contexts. Conducting nine 90-minute semi-structured interviews involving a hypothetical small group formation process, we found four compounding challenges that make it difficult to form and maintain stable groups. In contrast to other kinds of small groups discussed in the literature, the unique ""combination"" of these challenges bring into focus a class of interest-based small groups we term ""shared interest pods"": self-assembled small groups united by shared interests, whose members derive benefits from sustained, synchronous interactions that hinge on the level of fit among them. The need to discover and maintain collective fit, the considerable commitment required for regular interactions, their small size, and the lack of external enforcement, makes shared interest pods fragile and unstable, and an information-rich site for studying dynamics of self-organized consensus-seeking in underexplored areas of Lee and Paine's Model of Coordinated Action, and for considering the tension between supporting personal agency through organic group formation and optimizing stability through algorithmic assignments. A central finding is how dynamics of maintaining and giving up control co-exist in shared interest pods in their search for collective fit. People seek to maintain control over certain aspects of finding fit (e.g. in determining group membership), but are simultaneously willing to give up control to organizers or algorithms under certain conditions (e.g. when balancing desired quality with social and ethical considerations). We discuss design implications for supporting the formation of shared interest pods, and more broadly, for exploring self-organized consensus-seeking in other societal collective decision-making contexts."
"""A Lot of Moving Parts"": A Case Study of Open-Source Hardware Design Collaboration in the Thingiverse Community","Cheng, Kathy and Zhou, Shurui and Olechowski, Alison",10.1145/3687008,2024,"Open-source is a decentralized and collaborative method of development that encourages open contribution from an extensive and undefined network of individuals. Although commonly associated with software development (OSS), the open-source model extends to hardware development, forming the basis of open-source hardware development (OSH). Compared to OSS, OSH is relatively nascent, lacking adequate tooling support from existing platforms and best practices for efficient collaboration. Taking a necessary step towards improving OSH collaboration, we conduct a detailed case study of DrawBot, a successful OSH project that remarkably fostered a long-term collaboration on Thingiverse - a platform not explicitly intended for complex collaborative design. Through analyzing comment threads and design changes over the course of the project, we found how collaboration occurred, the challenges faced, and how the DrawBot community managed to overcome these obstacles. Beyond offering a detailed account of collaboration practices and challenges, our work contributes best practices, design implications, and practical implications for OSH project maintainers, platform builders, and researchers, respectively. With these insights and our publicly available dataset, we aim to foster more effective and efficient collaborative design in OSH projects."
"""Come to us first"": Centering Community Organizations in Artificial Intelligence for Social Good Partnerships","Lin, Hongjin and Karusala, Naveena and Okolo, Chinasa T. and D'Ignazio, Catherine and Gajos, Krzysztof Z.",10.1145/3687009,2024,"Artificial Intelligence for Social Good (AI4SG) has emerged as a growing body of research and practice exploring the potential of AI technologies to tackle social issues. This area emphasizes interdisciplinary partnerships with community organizations, such as non-profits and government agencies. However, amidst excitement about new advances in AI and their potential impact, the needs, expectations, and aspirations of these community organizations--and whether they are being met--are not well understood. Understanding these factors is important to ensure that the considerable efforts by AI teams and community organizations can actually achieve the positive social impact they strive for. Drawing on the Data Feminism framework, we explored the perspectives of community organization members on their partnerships with AI teams through 16 semi-structured interviews. Our study highlights the pervasive influence of funding agendas and the optimism surrounding AI's potential. Despite the significant intellectual contributions and labor provided by community organization members, their goals were frequently sidelined in favor of other stakeholders, including AI teams. While many community organization members expected tangible project deployment, only two out of 14 projects we studied reached the deployment stage. However, community organization members sustained their belief in the potential of the projects, still seeing diminished goals as valuable. To enhance the efficacy of future collaborations, our participants shared their aspirations for success, calling for co-leadership starting from the early stages of projects. We propose data co-liberation as a grounding principle for approaching AI4SG moving forward, positing that community organizations' co-leadership is essential for fostering more effective, sustainable, and ethical development of AI."
"""Ultimately We're Together"": Understanding New Parents' Experiences of Co-parenting","Lin, Ya-Fang and Li, Na and Huang, Wan-Hsuan and Ecsedy, Karen and Feinberg, Mark E. and Teti, Douglas and Carroll, John M.",10.1145/3687018,2024,"Positive co-parenting is critical for parenting and child outcomes, especially for new parents, who suffer from an increase in conflicts and decreased marital relationships. Investigating the practices, strategies, and challenges of new parents' co-parenting, including collaborating, supporting, and relating to each other, can help better understand this vulnerable parenting period. Furthermore, this understanding can bring insights to inform technology design for supporting co-parenting. In this paper, we present an overview of related research on parents' collaboration, support, and relationships. We then report findings from an exploratory semi-structured interview study with 11 pairs of new parents who are interested in co-parenting skills and unpack their helpful co-parenting strategies, tools, and challenges when applying those strategies. We conclude with a discussion of the design implications aiming to support new parents' positive co-parenting in their day-to-day lives and an observation that couples' co-parenting is a case of coproduction."
Reimagining Meaningful Data Work through Citizen Science,"Boone, Ashley and Rothschild, Annabel and Koo, Xander and Pfohl, Grace and Sheehan, Alyssa and DiSalvo, Betsy and Le Dantec, Christopher A and DiSalvo, Carl",10.1145/3687049,2024,"Data work is often completed by crowdworkers, who are routinely dehumanized, disempowered, and sidelined. We turn to citizen science to reimagine data work, highlighting collaborative relationships between citizen science project managers and volunteers. Though citizen science and traditional crowd work entail similar forms of data work, such as classifying or transcribing large data sets, citizen science relies on volunteer contributions rather than paid data work. We detail the work citizen science project managers did to shape volunteer experiences: aligning science goals, minimizing barriers to participation, engaging communities, communicating with volunteers, providing training and education, rewarding contributions, and reflecting on volunteer work. These management strategies created opportunities for meaningful work by cultivating intrinsic motivation and fostering collaborative work relationships but ultimately limited participation to specific data-related tasks. We recommend management tactics and task design strategies for creating meaningful work for ""invisible collar"" workers, an understudied class of labor in CSCW."
ICT-facilitated Health Interventions for Indigenous Communities: A Critical Literature Review,"Vigil-Hayes, Morgan and Panguluri, Lakshmi and Dececco, Harry and Hossain, Md Nazmul and Collier, Ann and Joseph, Darold and Amresh, Ashish",10.1145/3687133,2024,"Despite significant cultural strengths and knowledge, Indigenous people around the world experience substantial health inequities due to the historic and ongoing impacts of settler colonialism. As information and communication technologies (ICTs) are increasingly used as part of health interventions to help bridge equity gaps, it is important to characterize and critically evaluate how ICT-facilitated health interventions are designed for and used by Indigenous people. This critical literature review queried articles from three archives focused on health and technology with the goal of identifying cross-cutting challenges and opportunities for ICT-facilitated health interventions in Indigenous communities. Importantly, we use the lens of decolonization to understand important issues that impact Indigenous sovereignty, including the incorporation of Indigenous Knowledge and engagement with data sovereignty."
Assessing IT Project Success: Perception vs. Reality: We would not be in the digital age if it were not for the recurrent success of IT projects.,"Varaj\~{a}o, Jo\~{a}o and Trigo, Ant\'{o}nio",10.1145/3687999,2024,"This study has significant implications for practice, research, and education by providing new insights into IT project success. It expands the body of knowledge on project management by reporting project success (and not exclusively project management success), grounded in several objective criteria such as deliverables usage by the client in the post-project stage, hiring of project-related support/maintenance services by the client, contracting of new projects by the client, and vendor recommendation by the client to potential clients. Researchers can find a set of criteria they can use when studying and reporting the success of IT projects, thus expanding the current perspective on evaluation and contributing to more accurate conclusions. For practitioners, this study provides a rich set of criteria that can be used for evaluating their projects, as well as strong evidence of the importance of considering not only project execution, but also post-project outcomes and impacts in the evaluation."
A Survey on Deep Learning for Design and Generation of Virtual Architecture,"Wang, Anqi and Dong, Jiahua and Lee, Lik-Hang and Shen, Jiachuan and Hui, Pan",10.1145/3688569,2024,"Three-dimensional (3D) shape generation techniques leveraging deep learning have garnered significant interest from both computer vision and architectural design communities, promising to enrich the content in the virtual environment. However, research on virtual architectural design remains limited, particularly regarding designer-AI collaboration and deep learning-assisted design. In our survey, we reviewed 149 related articles (81.2% of articles published between 2019 and 2023) covering architectural design, 3D shape techniques, and virtual environments. Through scrutinizing the literature, we first identify the principles of virtual architecture and illuminate its current production challenges, including datasets, multimodality, design intuition, and generative frameworks. We then introduce the latest approaches to designing and generating virtual buildings leveraging 3D shape generation and summarize four characteristics of various approaches to virtual architecture. Based on our analysis, we expound on four research agendas, including agency, communication, user consideration, and integrating tools. Additionally, we highlight four important enablers of ubiquitous interaction with immersive systems in deep learning-assisted architectural generation. Our work contributes to fostering understanding between designers and deep learning techniques, broadening access to designer-AI collaboration. We advocate for interdisciplinary efforts to address this timely research topic, facilitating content designing and generation in the virtual environment."
An Exploratory Study on Machine Learning Model Management,"Latendresse, Jasmine and Abedu, Samuel and Abdellatif, Ahmad and Shihab, Emad",10.1145/3688841,2024,"Effective model management is crucial for ensuring performance and reliability in Machine Learning (ML) systems, given the dynamic nature of data and operational environments. However, standard practices are lacking, often resulting in ad hoc approaches. To address this, our research provides a clear definition of ML model management activities, processes, and techniques. Analyzing 227 ML repositories, we propose a taxonomy of 16 model management activities and identify 12 unique challenges. We find that 57.9% of the identified activities belong to the maintenance category, with activities like refactoring (20.5%) and documentation (18.3%) dominating. Our findings also reveal significant challenges in documentation maintenance (15.3%) and bug management (14.9%), emphasizing the need for robust versioning tools and practices in the ML pipeline. Additionally, we conducted a survey that underscores a shift toward automation, particularly in data, model, and documentation versioning, as key to managing ML models effectively. Our contributions include a detailed taxonomy of model management activities, a mapping of challenges to these activities, practitioner-informed solutions for challenge mitigation, and a publicly available dataset of model management activities and challenges. This work aims to equip ML developers with knowledge and best practices essential for the robust management of ML models."
"Multi-Class Imbalanced Data Handling with Concept Drift in Fog Computing: A Taxonomy, Review, and Future Directions","Sharief, Farhana and Ijaz, Humaira and Shojafar, Mohammad and Naeem, Muhammad Asif",10.1145/3689627,2024,"A network of actual physical objects or “IoT components” linked to the internet and equipped with sensors, electronics, software, and network connectivity is known as the Internet of Things (IoT). This ability of the IoT components to gather and share data is made possible by this network connectivity. Many IoT devices are currently operating, which generate a lot of data. When these IoT devices started collecting data, the cloud was the only place to analyze, filter, pre-process, and aggregate it. However, when it comes to IoT, the cloud has restrictions regarding latency and a more centralized method of distributing programs. A new form of computing called Fog computing has been proposed to address the shortcomings of current cloud computing. In an IoT context, sensors regularly communicate signal information, and edge devices process the data obtained from these sensors using Fog computing. The sensors’ internal or external problems, security breaches, or the integration of heterogeneous equipment contribute to the imbalanced data, i.e., comparatively speaking, one class has more instances than the other. As a result of this data, the pattern extraction is imbalanced. Recent attempts have concentrated heavily on binary-class imbalanced concerns with exactly two classes. However, the classification of multi-class imbalanced data is an issue that needs to be fixed in Fog computing, even if it is widespread in other fields, including text categorization, human activity detection, and medical diagnosis. The study intends to deal with this problem. It presents a systematic, thorough, and in-depth comparative analysis of several binary-class and multi-class imbalanced data handling strategies for batch and streaming data in IoT networks and Fog computing. There are five major objectives in this study. First, reviewing the Fog computing concept. Second, outlining the optimization metric used in Fog computing. Third, focusing on binary and multi-class batch data handling for IoT networks and Fog computing. Fourth, reviewing and comparing the current imbalanced data handling methodologies for multi-class data streams. Fifth, explaining how to cope with the concept drift, including novel and recurring classes, targeted optimization measures, and evaluation tools. Finally, the best performance metrics and tools for concept drift, binary-class (batch and stream) data, and multi-class (batch and stream) data are highlighted."
Decoding Debugging Instruction: A Systematic Literature Review of Debugging Interventions,"Yang, Stephanie and Baird, Miles and O’Rourke, Eleanor and Brennan, Karen and Schneider, Bertrand",10.1145/3690652,2024,"Students learning computer science frequently struggle with debugging errors in their code. These struggles can have significant downstream effects—negatively influencing how students assess their programming ability and contributing to their decision to drop out of CS courses. However, debugging instruction is often an overlooked topic, and instructors report feeling unaware of effective approaches to teach debugging. Within the literature, research on the topic is sporadic, and though there are rigorous and insightful studies to be found, there is a need to synthesize instructional approaches for debugging. In this article, we review research from 2010 to 2022 on debugging interventions. We summarize the common pedagogical approaches for learning and categorize how these target specific cognitive and non-cognitive debugging skills, such as self-efficacy and emotion regulation. We also present a summary of assessment methods and their outcomes in order to discuss intervention efficacy and directions for further research. Our sample displays a diverse variety of debugging interventions and pedagogical approaches, ranging from games to unplugged activities. An evaluation of article results also presents encouraging findings, revealing several interventions that improved debugging accuracy and learning. Still, we notice gaps in interventions addressing non-cognitive debugging skills and observe limited success in guiding students toward adopting systematic debugging strategies. The review concludes with a discussion of future directions and implications for researchers and instructors in the field."
A Survey on Stability of Learning with Limited Labelled Data and its Sensitivity to the Effects of Randomness,"Pecher, Branislav and Srba, Ivan and Bielikova, Maria",10.1145/3691339,2024,"Learning with limited labelled data, such as prompting, in-context learning, fine-tuning, meta-learning, or few-shot learning, aims to effectively train a model using only a small amount of labelled samples. However, these approaches have been observed to be excessively sensitive to the effects of uncontrolled randomness caused by non-determinism in the training process. The randomness negatively affects the stability of the models, leading to large variances in results across training runs. When such sensitivity is disregarded, it can unintentionally, but unfortunately also intentionally, create an imaginary perception of research progress. Recently, this area started to attract research attention and the number of relevant studies is continuously growing. In this survey, we provide a comprehensive overview of 415 papers addressing the effects of randomness on the stability of learning with limited labelled data. We distinguish between four main tasks addressed in the papers (investigate/evaluate, determine, mitigate, benchmark/compare/report randomness effects), providing findings for each one. Furthermore, we identify and discuss seven challenges and open problems together with possible directions to facilitate further research. The ultimate goal of this survey is to emphasise the importance of this growing research area, which so far has not received an appropriate level of attention, and reveal impactful directions for future research."
"A Survey on the Role of Crowds in Combating Online Misinformation: Annotators, Evaluators, and Creators","He, Bing and Hu, Yibo and Lee, Yeon-Chang and Oh, Soyoung and Verma, Gaurav and Kumar, Srijan",10.1145/3694980,2024,"Online misinformation poses a global risk with significant real-world consequences. To combat misinformation, current research relies on professionals like journalists and fact-checkers for annotating and debunking false information while also developing automated machine learning methods for detecting misinformation. Complementary to these approaches, recent research has increasingly concentrated on utilizing the power of ordinary social media users, a.k.a. “the crowd,” who act as eyes-on-the-ground proactively questioning and countering misinformation. Notably, recent studies show that 96% of counter-misinformation responses originate from them. Acknowledging their prominent role, we present the first systematic and comprehensive survey of research papers that actively leverage the crowds to combat misinformation.In this survey, we first identify 88 papers related to crowd-based efforts,1 following a meticulous annotation process adhering to the PRISMA framework (preferred reporting items for systematic reviews and meta-analyses). We then present key statistics related to misinformation, counter-misinformation, and crowd input in different formats and topics. Upon holistic analysis of the papers, we introduce a novel taxonomy of the roles played by the crowds in combating misinformation: (i) crowds as annotators who actively identify misinformation; (ii) crowds as evaluators who assess counter-misinformation effectiveness; (iii) crowds as creators who create counter-misinformation. This taxonomy explores the crowd’s capabilities in misinformation detection, identifies the prerequisites for effective counter-misinformation, and analyzes crowd-generated counter-misinformation. In each assigned role, we conduct a detailed analysis to categorize the specific utilization of the crowd. Particularly, we delve into (i) distinguishing individual, collaborative, and machine-assisted labeling for annotators; (ii) analyzing the effectiveness of counter-misinformation through surveys, interviews, and in-lab experiments for evaluators; and (iii) characterizing creation patterns and creator profiles for creators. Finally, we conclude this survey by outlining potential avenues for future research in this field."
Large Language Models for Software Engineering: A Systematic Literature Review,"Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu",10.1145/3695988,2024,"Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a Systematic Literature Review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We selected and analyzed 395 research articles from January 2017 to January 2024 to answer four key Research Questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, pre-processing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and highlighting promising areas for future study. Our artifacts are publicly available at ."
Cognitive Urban Planning Enhancing Decision-Making in Local Governments.,"Meza, Jaime and Jimenez-Pacheco, Pedro and Vaca-Cardenas, Leticia and Munoz, Emanuel",10.1145/3696008,2024,"According to the United Nations (UN), a “continuous, participatory, and inclusive urban planning process should be the starting point and framework for improving population living conditions,” and this research emerges from examining trends in smart cities. It explores the impact of collective intelligence, geographical information systems, and cognitive systems as a way of supporting the decision-making of local governments (municipalities) to face the challenges announced by the UN in the Urban Agenda 2017. This methodology employs an exploratory approach using the participatory action research method. The model was tested in three municipalities in Ecuador using a technological and collaborative platform called Cognitive Urban Planning (PUC). The results highlighted improvements in the decision-making process in urban planning, since the interactive participation processes through the PUC allowed for the suggestion of a group of strategies for an emerging neighborhood plan developed with the citizens working together in the municipality in real-time. In conclusion, the effects of the decision-making plan could be applied to improve and enhance community participation through city co-creation, supporting the decision-making process of municipalities in a way that promotes inclusive urban planning and stakeholders' awareness, as the recommended plans are supported by the desires and needs raised by citizens."
"Advanced Persistent Threat Attack Detection Systems: A Review of Approaches, Challenges, and Trends","Buchta, Robin and Gkoktsis, George and Heine, Felix and Kleiner, Carsten",10.1145/3696014,2024,"Advanced persistent threat (APT) attacks present a significant challenge for any organization, as they are difficult to detect due to their elusive nature and characteristics. In this article, we conduct a comprehensive literature review to investigate the various APT attack detection systems and approaches and classify them based on their threat model and detection method. Our findings reveal common obstacles in APT attack detection, such as correctly attributing anomalous behavior to APT attack activities, limited availability of public datasets and inadequate evaluation methods, challenges with detection procedures, and misinterpretation of requirements. Based on our findings, we propose a reference architecture to enhance the comparability of existing systems and provide a framework for classifying detection systems. In addition, we look in detail at the problems encountered in current evaluations and other scientific gaps, such as a neglected consideration of integrating the systems into existing security architectures and their adaptability and durability. While no one-size-fits-all solution exists for APT attack detection, this review shows that graph-based approaches hold promising potential. However, further research is required for real-world usability, considering the systems’ adaptability and explainability."
Meta-Review on Brain-Computer Interface (BCI) in the Metaverse,"Gholizadeh HamlAbadi, Kamran and Laamarti, Fedwa and El Saddik, Abdulmotaleb",10.1145/3696109,2024,"This article presents a comprehensive meta-review of the intersection between Brain-Computer Interface (BCI) technologies and the Metaverse, emphasizing the enhancement of immersive experiences through VR, AR, MR, XR, Digital Twin, and haptic interfaces. The study classifies BCI devices into wearable and non-wearable categories, with a focus on their applications in robotics. It explores BCI user feedback mechanisms and their impact on medical and non-medical settings, including personalized rehabilitation and immersive gaming. The review introduces two frameworks for leveraging the Metaverse to navigate multisensory integration between BCI and assistive devices. Applications such as VR therapies for stroke patients and neuro-responsive multiplayer gaming environments showcase the potential of BCIs to enhance Metaverse interactions. To the best of our knowledge, this is the first meta-review on the integration of BCI and the Metaverse, identifying key challenges and research gaps, and serves as a foundational reference for future research and development in this interdisciplinary field."
An Expectation-Maximization framework for Personalized Itinerary Recommendation with POI Categories and Must-see POIs,"Panagiotakis, Costas and Daskalaki, Evangelia and Papadakis, Harris and Fragopoulou, Paraskevi",10.1145/3696114,2024,"In this article, we introduce a novel deterministic method based on Expectation Maximization (EM) to solve the rather complex problem of designing a tourist trip or Personalized Itinerary Recommendation (PIR). PIR objective is to recommend a personalized tour consisting of successive Points of Interest (POIs), which maximizes user satisfaction and respects user time-frame constraints. On top of that, the POIs are divided into categories, in order for travelers to be able to set limits on the maximum (and minimum) number of POIs that belong to one category and are included in the itinerary. In the proposed framework, emphasis is given on the POIs sequence selection, which exploits the customized POI recommendations offered by a recommender system. Additionally, the proposed methodology with POIs categories is able to solve the TourMustSee problem, so that the tour includes a set of POIs that must be visited. The proposed system has been successfully incorporated into a mobile app, offering a complete tourist trip design. The high performance, resilience, and computational efficiency of the proposed framework are demonstrated by experimental findings and comparisons to existing approaches on numerous synthetic and real datasets."
"A Systematic Review of Fast, Scalable, and Efficient Hardware Implementations of Elliptic Curve Cryptography for Blockchain","Ifrim, Rares and Loghin, Dumitrel and Popescu, Decebal",10.1145/3696422,2024,"Blockchain technology entered the enterprise domain under the name of permissioned blockchains and hybrid or verifiable database systems, as they provide a distributed solution that allows multiple distrusting parties to share common information. One drawback of these systems is the overhead added by the cryptographic functions which impacts the throughput in terms of transactions per second and increases the latency of transaction processing. Many of the cryptographic functions and protocols used in blockchains are based on Elliptic Curve Cryptography (ECC). Unfortunately, ECC operations such as modulo inverse or scalar point multiplication have considerable latency which causes the slowdown of the entire system. In such situations, reconfigurable computing architectures, such as FPGAs, can be used to offload these tasks to overcome the performance loss. This survey analyzes the current state-of-the-art designs and implementations of ECC from a hardware perspective. We use a PRISMA-based approach to filter recent publications and to reduce their number from over 16,000 to only 43 highly relevant designs. In the end, we show that very few designs are able to fulfill all three properties of high performance, scalability, and efficiency."
"A Systematic Review on Fostering Appropriate Trust in Human-AI Interaction: Trends, Opportunities and Challenges","Mehrotra, Siddharth and Degachi, Chadha and Vereschak, Oleksandra and Jonker, Catholijn M. and Tielman, Myrthe L.",10.1145/3696449,2024,"Appropriate trust in Artificial Intelligence (AI) systems has rapidly become an important area of focus for both researchers and practitioners. Various approaches have been used to achieve it, such as confidence scores, explanations, trustworthiness cues, and uncertainty communication. However, a comprehensive understanding of the field is lacking due to the diversity of perspectives arising from various backgrounds that influence it and the lack of a single definition for appropriate trust. To investigate this topic, this article presents a systematic review to identify current practices in building appropriate trust, different ways to measure it, types of tasks used, and potential challenges associated with it. We also propose a Belief, Intentions, and Actions mapping to study commonalities and differences in the concepts related to appropriate trust by (a) describing the existing disagreements on defining appropriate trust, and (b) providing an overview of the concepts and definitions related to appropriate trust in AI from the existing literature. Finally, the challenges identified in studying appropriate trust are discussed, and observations are summarized as current trends, potential gaps, and research opportunities for future work. Overall, the article provides insights into the complex concept of appropriate trust in human-AI interaction and presents research opportunities to advance our understanding on this topic."
"Evolving Paradigms in Automated Program Repair: Taxonomy, Challenges, and Opportunities","Huang, Kai and Xu, Zhengzi and Yang, Su and Sun, Hongyu and Li, Xuejun and Yan, Zheng and Zhang, Yuqing",10.1145/3696450,2024,"With the rapid development and large-scale popularity of program software, modern society increasingly relies on software systems. However, the problems exposed by software have also come to the fore. The software bug has become an important factor troubling developers. In this context, Automated Program Repair (APR) techniques have emerged, aiming to automatically fix software bug problems and reduce manual debugging work. In particular, benefiting from the advances in deep learning, numerous learning-based APR techniques have emerged in recent years, which also bring new opportunities for APR research. To give researchers a quick overview of APR techniques’ complete development and future opportunities, we review the evolution of APR techniques and discuss in depth the latest advances in APR research. In this article, the development of APR techniques is introduced in terms of four different patch generation schemes: search-based, constraint-based, template-based, and learning-based. Moreover, we propose a uniform set of criteria to review and compare each APR tool and then discuss the current state of APR development. Finally, we analyze current challenges and future directions, especially highlighting the critical opportunities that large language models bring to APR research."
Cross Project Defect Prediction using Dropout Regularized Deep Learning and Unique Matched Metrics,"Bal, Pravas Ranjan and Kumar, Sandeep",10.1145/3698109,2025,"The primary goal of software defect prediction (SDP) is to predict the software defects for a specific software using historical data or data from past releases of software projects. The existing state of arts on SDP primarily discusses two prediction scenarios: Within-project Defect Prediction (WPDP) and Cross-project Defect Prediction (CPDP). The prediction model belongs to the WPDP scenario, which means that the model is trained and tested on different parts of the same dataset or trained on the dataset belonging to the previous version of the same project. While in the CPDP scenario, training and testing occur on different software project datasets. Due to the unavailability of historical datasets or prior releases of software defect datasets, CPDP is more useful in real-life scenarios. So, CPDP analysis is a very challenging issue in the SDP domain. Sometimes, machine learning (ML) models perform poorly due to inadequate training in the CPDP scenario. To support better CPDP performance, we must carefully build an ML model focusing on lower training error and overfitting issues. To address these issues, we have proposed a cross-project data preprocessing method to correlate the metrics of different project datasets, namely, Unique Selection of Matched Metrics (USMM), using the KS test and Hungarian method. To further improve the CPDP performance, we have also used the dropout regularized deep learning (DRDL) model. We have deployed 34 software defect datasets to validate the DRDL model and USMM method. The experimental results demonstrate that the DRDL model using the USMM method (DRDL-USMM) is a promising model to enhance the prediction accuracy, and an improvement in the range of 3.3% to 8.5% as compared to the existing works in the CPDP scenario has been found."
Passive Stylus Tracking: A Systematic Literature Review,"Burnah, Tavish M. and Imtiaz, Md. Athar and Guesgen, Hans Werner and Rudolph, George L. and Blagojevic, Rachel",10.1145/3698144,2024,"Passive stylus systems offer a simple and cost-effective solution for digital input, compatible with a wide range of surfaces and devices. This study reviews the domain of passive stylus tracking on passive surfaces, a topic previously underexplored in existing literature. We answer four key research questions: what type of systems exist in this domain, what methods do they use for tracking styli, how accurate are they, and what are their limitations?
 
 
 
 
 
 
 
A systematic literature review resulted in 24 papers describing passive stylus systems. Their methods primarily fall into four categories: monocular cameras with image processing, multiple camera systems with image processing, machine learning systems using high-speed cameras or motion capture hardware, and radio frequency signal-based systems with signal processing. 
 
 
 
 
 
 
 
We found the system with the highest accuracy used a single monocular camera. In many systems, markers such as retroreflective spheres, tape, or fiducial markers were used to enhance the feature matching. We have also found stagnation and in some cases, regression in the precision and reliability of these systems over time. The limitations in these systems include the lack of varied stylus form factor support, the restriction to specific camera positions and angles, and the requirement of expensive hardware. Given these findings, we discuss the important characteristics and features of passive stylus systems and propose ways forward in this field."
Zooming In: A Review of Designing for Photo Taking in Human-Computer Interaction and Future Prospects,"Wysoki\'{n}ska, Aleksandra and Str\""{o}mel, Konstantin R. and Wo\'{z}niak, Pawe\l{} W.",10.1145/3698150,2024,"Photography has been pivotal in culture for decades and its importance has increased with the rise of digital technology. However, the exploration of picture taking within the Human-Computer Interaction (HCI) community does not seem to match its cultural and technological significance. Recognizing this discrepancy, we sought to understand areas of interest in photography as a conscious creative process. To address this issue, we performed a systematic literature review using the PRISMA methodology. From our research, we identified 62 pertinent papers spanning from 2005 to 2022. Our examination revealed six primary dimensions, further classified into study type, design goal, photo-taking style, device type, interaction style, and number of users. In-depth analysis showed the dominant role of exploratory and functional research, the balance between qualitative and quantitative methods, and a strong focus on smartphone cameras. Our review has highlighted significant gaps in the existing literature, offering valuable insights for future research on photo taking."
A Systematic Review of Privacy Policy Literature,"Javed, Yousra and Sajid, Ayesha",10.1145/3698393,2024,"An organization’s privacy policy states how it collects, stores, processes, and shares its users’ personal information. The growing number of data protection laws and regulations, as well as the numerous sectors where the organizations are collecting user information, has led to the investigation of privacy policies with regards to their accessibility, readability, completeness, comparison with organization’s actual data practices, use of machine learning/natural language processing for automated analysis, and comprehension/perception/concerns of end-users via summarization/visualization tools and user studies. However, there is limited work on systematically reviewing the existing research on this topic. We address this gap by conducting a systematic review of the existing privacy policy literature. To this end, we compiled and analyzed 202 papers (published till 31st December, 2023) that investigated privacy policies. Our work advances the field of privacy policies by summarizing the analysis techniques that have been used to study them, the data protection laws/regulations explored, and the sectors to which these policies pertain. We provide actionable insights for organizations to achieve better end-user privacy."
Artificial Intelligence to Support the Training and Assessment of Professionals: A Systematic Literature Review,"Albaladejo-Gonz\'{a}lez, Mariano and Ruip\'{e}rez-Valiente, Jos\'{e} A. and Gmez M\'{a}rmol, F\'{e}lix",10.1145/3699712,2024,"Advances in Artificial Intelligence (AI) and sensors are significantly impacting multiple areas, including education and workplaces. Following the PRISMA methodology, this review explores the current status of using AI to support the training and assessment of professionals. We examined 83 research papers, analyzing (1) the targeted professionals, (2) the skills assessed, (3) the AI algorithms utilized, (4) the data and devices employed, (5) data fusion techniques utilized, (6) the architecture of the proposed platforms, (7) the management of ethics and privacy, and (8) validations of the proposals. The review highlights a trend in evaluating healthcare professionals (especially surgeons) motivated by the critical role of hands-on training in these professions. Besides, the review reveals that data fusion techniques and certain technologies, like transfer learning and explainable AI, are not widely utilized despite their huge potential. Finally, the review underscores that most proposals remain within the research domain, lacking the integration and maturity needed for sustained use in real-world environments. Therefore, most of the proposals are not currently available to support the training of professionals. The insights of this review can guide researchers aiming to improve the training of professionals and, consequently, their education."
Self-supervised Learning for Accelerometer-based Human Activity Recognition: A Survey,"Logacjov, Aleksej",10.1145/3699767,2024,"Self-supervised learning (SSL) has emerged as a promising alternative to purely supervised learning, since it can learn from labeled and unlabeled data using a pre-train-then-fine-tune strategy, achieving state-of-the-art performances across many research areas. The field of accelerometer-based human activity recognition (HAR) can benefit from SSL since unlabeled data can be collected cost-efficiently due to the ubiquitous nature of sensors embedded in smart devices, which is in contrast to labeled data, that require a costly annotation process. Motivated by the success of SSL and the lack of surveys on SSL for HAR, this survey comprehensively examines 52 SSL methods applied to HAR, and categorizes them into four SSL paradigms based on pre-training objectives. We discuss SSL strategies, evaluation protocols, and utilized datasets. We highlight limitations in current methodologies, including little large-scale pre-training, the absence of foundation models, as well as the scarcity of systematic domain shift experiments and domain knowledge utilization. Notably, the diversity in evaluation protocols across papers poses a considerable challenge when comparing methods. Future directions outlined in this survey include the development of an SSL framework for HAR to enable standardized benchmarking and large-scale pre-training, along with integrating domain knowledge to enhance model performance."
SIGecom Job Market Candidate Profiles 2023,"Gkatzelis, Vasilis and Hartline, Jason",10.1145/3699804.3699806,2024,This is the eighth annual collection of profiles of the junior faculty job market candidates of the SIGecom community. The thirty candidates for 2023 are listed alphabetically and indexed by research areas that define the interests of the community. The candidates can be contacted individually or via the moderated mailing list ecom-candidates2023@acm.org.
A Survey on IoT Programming Platforms: A Business-Domain Experts Perspective,"Hannou, Fatma-Zohra and Lefran\c{c}ois, Maxime and Jouvelot, Pierre and Charpenay, Victor and Zimmermann, Antoine",10.1145/3699954,2024,"The vast growth and digitalization potential offered by the Internet of Things (IoT) is hindered by substantial barriers in accessibility, interoperability, and complexity, mainly affecting small organizations and non-technical entities. This survey article provides a detailed overview of the landscape of IoT programming platforms, focusing specifically on the development support they offer for varying end user profiles, ranging from developers with IoT expertise to business experts willing to take advantage of IoT solutions to automate their organization processes. The survey examines a range of IoT platforms, classified according to their programming approach between general-purpose programming solutions, model-driven programming, mashups, and end-user programming. Necessary IoT and programming backgrounds are described to empower non-technical readers with a comprehensive field summary. In addition, the article compares the features of the most representative platforms and provides decision insights and guidelines to support end users in selecting appropriate IoT platforms for their use cases. This work contributes to narrowing the knowledge gap between IoT specialists and end users, breaking accessibility barriers and further promoting the integration of IoT technologies in various domains.1"
Systematic Review of Social Robots for Health and Wellbeing: A Personal Healthcare Journey Lens,"Ghafurian, Moojan and Chandra, Shruti and Hutchinson, Rebecca and Lim, Angelica and Baliyan, Ishan and Rhim, Jimin and Gupta, Garima and Aroyo, Alexander M. and Rasouli, Samira and Dautenhahn, Kerstin",10.1145/3700446,2024,"Social robots have great potential in supporting individuals’ physical and mental health/wellbeing. While they have been increasingly evaluated in some domains, such as with children with autism, their evaluation has not been as extensive in other areas. We present a systematic review of domains in which social robots have been evaluated specifically in health/wellbeing contexts. We ask which robots have been evaluated, who the participants were, and how participants interacted with the robots. PRISMA guidelines for systematic reviews were followed. Articles with children as participants, using a purely robotic device, and in languages other than English were excluded. A total of 9,362 peer-reviewed articles (up to February 2021) from ACM DL, IEEE Xplore, Scopus, PubMed, and PsychInfo were identified. After applying the inclusion/exclusion criteria 443 articles were included in the review. The majority of studies were conducted at care centers while studies in hospitals/clinics have seen relatively limited attention. In many cases, the social robots were not programmed for specific health-related tasks, limiting their application. We also discuss robots used in real-world settings and propose a “Personal healthcare journey,” which includes different stages of one’s life which could benefit from a social robot, with the goal of increasing long-term adoption of social robots for supporting health/wellbeing."
A Systematic Literature Review on Multi-Robot Task Allocation,"K A, Athira and J, Divya Udayan and Subramaniam, Umashankar",10.1145/3700591,2024,"Muti-Robot system is gaining attention and is one of the critical areas of research when it comes to robotics. Coordination among multiple robots and how different tasks are allocated to different system agents are being studied. The objective of this Systematic Literature Review (SLR) is to provide insights on the recent advancement in Multi-Robot Task Allocation (MRTA) problems emphasizing promising approaches for task allocation. In this study, we collected scientific papers from five different databases for MRTA. We outline the different approaches for task allocation algorithms, classifying them according to the methods, and emphasizing recent advances. In addition, we discuss the function of uncertainty in task allocation and typical coordination techniques utilized in task allocation to identify gaps in the literature and suggest the most promising ones."
A Human–Security Robot Interaction Literature Review,"Ye, Xin and Robert, Lionel P.",10.1145/3700888,2024,"As advances in robotics continue, security robots are increasingly integrated into public and private security, enhancing protection in locations such as streets, parks, and shopping malls. To be effective, security robots must interact with civilians and security personnel, underscoring the need to enhance our knowledge of their interactions with humans. To investigate this issue, the authors systematically reviewed 47 studies on human interaction with security robots, covering 2003 to 2023. Papers in this domain have significantly increased over the last 7 years. The article provides three contributions. First, it comprehensively summarizes existing literature on human interaction with security robots. Second, it employs the Human–Robot Integrative Framework (HRIF) to categorize this literature into three main thrusts: human, robot, and context. The framework is leveraged to derive insights into the methodologies, tasks, predictors, and outcomes studied. Last, the article synthesizes and discusses the findings from the reviewed literature, identifying avenues for future research in this domain."
Learning to Work From Home: How Novice and Experienced Software Professionals Compare Online and In-person Collaboration,"Tang, Ying and Ziv, Hadar and Patil, Sameer",10.1145/3701199,2025,"Industry-sponsored capstone experiences that involve developing software solutions create valuable opportunities for students to engage in teamwork and learn important career skills just prior to joining the software workforce. The collaboration and communication in capstone projects, which typically happen in person, were forced to move online during the COVID-19 pandemic. We leveraged the transition to conduct a two-stage mixed-methods study that compared the in-person and online capstone experiences of students (novice software developers) and project sponsors (experienced industry professionals) to understand how novices engage in online collaboration. We uncovered that novices find online collaboration more challenging compared to experienced professionals. The challenges faced by the novices and the misalignment with experienced professionals point to a number of avenues for training novice software professionals on effective engagement in geographically distributed collaboration, which is increasingly the norm in the software industry."
What Knowledge Do We Produce from Social Media Data and How?,"Alvarado Garcia, Adriana and Yang, Tianling and Miceli, Milagros",10.1145/3701216,2025,"HCI and CSCW research that uses social media data to make inferences about individuals and communities has proliferated in the last decade. Previous studies have elaborated on methodological concerns and challenges and examined the assumptions and values underlying knowledge production through quantification and data. We expand this line of research by making visible and explicit the conventions and practices that establish, sustain, and reinforce current discourses in social media research. We conducted a Critical Discourse Analysis on 84 research papers published between 2010 and 2023 in CHI, CSCW, and GROUP that combine social media data and computational methods. Our findings show that plenty of this work legitimizes social media data as a valid source of information by centering its public availability, unobtrusiveness, and volume. Furthermore, to justify computational techniques, these papers prioritize computational expediency over data and method appropriateness. We argue that these embedded strategies may result in a methodological and epistemological distance between researchers and the studied communities, impacting problem framing, data collection, and finding application. With this work, we join the voices that have advocated for increased reflexivity in HCI and CSCW communities to scrutinize knowledge production and the role of researchers as knowledge producers."
Trained without My Consent: Detecting Code Inclusion in Language Models Trained on Code,"Majdinasab, Vahid and Nikanjam, Amin and Khomh, Foutse",10.1145/3702980,2025,"Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers’ codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To address this challenge, we propose a new approach, TraWiC; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an LLM’s training dataset. We extract syntactic and semantic identifiers unique to each program to train a classifier for detecting code inclusion. In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an LLM. In comparison, the prevalent clone detection tool NiCad is only capable of detecting 47.64%. In addition to its remarkable performance, TraWiC has low resource overhead in contrast to pairwise clone detection that is conducted during the auditing process of tools like CodeWhisperer reference tracker, across thousands of code snippets."
Toward Better Comprehension of Breaking Changes in the NPM Ecosystem,"Kong, Dezhen and Liu, Jiakun and Bao, Lingfeng and Lo, David",10.1145/3702991,2025,"Code evolution is prevalent in software ecosystems, which can provide many benefits, such as new features, bug fixes, security patches, while still introducing breaking changes that make downstream projects fail to work. Breaking changes cause a lot of effort to both downstream and upstream developers: downstream developers need to adapt to breaking changes and upstream developers are responsible for identifying and documenting them. In the NPM ecosystem, characterized by frequent code changes and a high tolerance for making breaking changes, the effort is larger.For better comprehension of breaking changes in the NPM ecosystem and to enhance breaking change detection tools, we conduct a large-scale empirical study to investigate breaking changes in the NPM ecosystem. We construct a dataset of explicitly documented breaking changes from 381 popular NPM projects. We find that 95.4% of the detected breaking changes can be covered by developers’ documentation, and 19% of the breaking changes cannot be detected by regression testing. Then in the process of investigating source code of our collected breaking changes, we yield a taxonomy of JavaScript- and TypeScript-specific syntactic breaking changes and a taxonomy of major types of behavioral breaking changes. Additionally, we investigate the reasons why developers make breaking changes in NPM and find three major reasons, i.e., to reduce code redundancy, to improve identifier names, and to improve API design, and each category contains several sub-items.We provide actionable implications for future research, e.g., automatic naming and renaming techniques should be applied in JavaScript projects to improve identifier names, future research can try to detect more types of behavioral breaking changes. By presenting the implications, we also discuss the weakness of automatic renaming and breaking change detection approaches, such as the lack of support for public identifiers and various types of breaking changes."
Multimodal Sentiment Analysis for the Malay Language: New Corpus using CNN-based Framework,"Taylor, Serena and Fauzi, Fariza",10.1145/3703445,2024,"Sentiment analysis in the Malay language has traditionally focused on text-based data. Malay is the native language of Malaysia and other surrounding countries. While text-based sentiment analysis has shown good performance, it often lacks accuracy due to the absence of the speaker's affective state and intentions, which can lead to misinterpretations. Multimodal sentiment analysis addresses these shortcomings and has demonstrated improved performance in various prediction tasks. Unfortunately, there has been little research in this area for the Malay language, due to a lack of corpus and baseline studies. This paper introduces a new Malay Multimodal Sentiment Corpus, ‘MyMSC’, with annotations at both the multimodal and unimodal text levels. It contains 1208 segments covering political and social topics. The corpus development processes are described in detail, along with the necessary guidelines and considerations. This paper proposes a CNN-based framework with a late fusion method as the baseline model. Experiments with the proposed model demonstrate that the multimodal approach (F1 score = 0.77) outperforms the unimodal approach (F1 score = 0.68), validating the contribution of multimodality to classification performance. The differences between the two types of annotations and their impact are further elaborated. The full corpus is available at ."
"A Systematic Literature Review on Blockchain-based Smart Contracts: Platforms, Applications, and Challenges","Singh, Renu and Gupta, Ashlesha and Mittal, Poonam",10.1145/3704741,2024,"Blockchain technology has rapidly emerged with a multitude of applications, among which smart contracts have garnered considerable attention. Smart contracts represent a promising solution for streamlining trade and business transactions between untrusted parties without intermediaries. These self-executing pieces of code automatically execute predefined actions when specific conditions are met. Despite the growing enthusiasm for blockchain and smart contracts, researchers believe this powerful combination has not yet reached its full potential. Hence, a systematic study is conducted to explore the various facets of blockchain-based smart contracts comprehensively. The research follows the PRISMA framework and employs two primary approaches: bibliometric analysis and systematic literature review. The process was initiated by formulating targeted search queries within the Scopus database, identifying a total of 1,949 publications spanning from January 2019 to August 2023. Subsequently, a bibliometric analysis was conducted on these publications using VOSViewer and Biblioshiny. Further, the full text of these publications was meticulously screened to isolate those with a significant focus on smart contracts. This led to the identification of 48 publications, each offering unique insights into various smart contract applications. Upon further examination, it was observed that the majority of these publications held rankings within the China Computer Federation, which refers to the qualitative research work in this domain. The study concludes with the current state of blockchain-based smart contracts, their platforms, applications, and challenges and reveals the substantial potential in handling tasks with predefined conditions and security requirements."
"Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering: A Multivocal Literature Review","Lambiase, Stefano and Catolino, Gemma and Palomba, Fabio and Ferrucci, Filomena",10.1145/3704806,2024,"Bots are software systems designed to support users by automating specific processes, tasks, or activities. When these systems implement a conversational component to interact with users, they are also known as conversational agents or chatbots. Bots—particularly in their conversation-oriented version and AI-powered—have seen increased adoption over time for software development and engineering purposes. Despite their exciting potential, which has been further enhanced by the advent of Generative AI and Large Language Models, bots still face challenges in terms of development and integration into the development cycle, as practitioners report that bots can add difficulties rather than provide improvements. In this work, we aim to provide a taxonomy for characterizing bots, as well as a series of challenges for their adoption in software engineering, accompanied by potential mitigation strategies. To achieve our objectives, we conducted a multivocal literature review, examining both research and practitioner literature. Through such an approach, we hope to contribute to both researchers and practitioners by providing (i) a series of future research directions to pursue, (ii) a list of strategies to adopt for improving the use of bots for software engineering purposes, and (iii) fostering technology and knowledge transfer from the research field to practice—one of the primary goals of multivocal literature reviews."
MLTL Multi-type: A Typed Logic for Cyber-Physical Systems,"Hariharan, Gokul and Kempa, Brian and Wongpiromsarn, Tichakorn and Jones, Phillip and Rozier, Kristin",10.1145/3704809,2024,"Modern cyber-physical systems-of-systems (CPSoS) operate in complex systems-of-systems that must seamlessly work together to control safety- or mission-critical functions. Linear Temporal Logic (LTL) and Mission-time Linear Temporal logic (MLTL) intuitively express CPSoS requirements for automated system verification and validation. However, both LTL and MLTL presume that all signals populating the variables in a formula are sampled over the same rate and type (e.g., time or distance), and agree on a standard “time” step. Formal verification of cyber-physical systems-of-systems needs validate-able requirements expressed over (sub-)system signals of different types, such as signals sampled at different timescales, distances, or levels of abstraction, expressed in the same formula. Previous works developed more expressive logics to account for types (e.g., timescales) by sacrificing the intuitive simplicity of LTL. However, a legible direct one-to-one correspondence between a verbal and formal specification will ease validation, reduce bugs, increase productivity, and linearize the workflow from a project’s conception to actualization. Validation includes both transparency for human interpretation, and tractability for automated reasoning, as CPSoS often run on resource-limited embedded systems. To address these challenges, we introduced Mission-time Linear Temporal Logic Multi-type (Hariharan et&nbsp;al., Numerical Software Verification Workshop, 2022), a logic building on MLTL. MLTLM enables writing formal requirements over finite input signals (e.g., sensor signals and local computations) of different types, while maintaining the same simplicity as LTL and MLTL. Furthermore, MLTLM maintains a direct correspondence between a verbal requirement and its corresponding formal specification. Additionally, reasoning a formal specification in the intended type (e.g., hourly for an hourly rate, and per second for a seconds rate) will use significantly less memory in resource-constrained hardware. This article extends the previous work with (1) many illustrated examples on types (e.g., time and space) expressed in the same specification, (2) proofs omitted for space in the workshop version, (3) proofs of succinctness of MLTLM compared to MLTL, and (4) a minimal translation to MLTL of optimal length."
Context-based Transfer Learning for Structuring Fault Localization and Program Repair Automation,"Zhang, Lehuan and Guo, Shikai and Guo, Yi and Li, Hui and Chai, Yu and Chen, Rong and Li, Xiaochen and Jiang, He",10.1145/3705302,2025,"Automated software debugging plays a crucial role in aiding software developers to swiftly identify and attempt to rectify faults, thereby significantly reducing developers’ workload. Previous researches have predominantly relied on simplistic semantic deep learning or statistical analysis methods to locate faulty statements in diverse projects. However, code repositories often consist of lengthy sequences with long-distance dependencies, posing challenges for accurately modeling fault localization using these methods. In addition, the lack of joint reasoning among various faults prevents existing models from deeply capturing fault information. To address these challenges, we propose a method named CodeHealer to achieve accurate fault localization and program repair. CodeHealer comprises three components: a Deep Semantic Information Extraction Component that effectively extracts deep semantic features from suspicious code statements using classifiers based on Joint-attention mechanisms; a Suspicious Statement Ranking Component that combines various fault localization features and employs multilayer perceptrons to derive multidimensional vectors of suspicion values; and a Fault Repair Component that, based on ranked suspicious statements generated by fault localization, adopts a top-down approach using multiple classifiers based on Co-teaching mechanisms to select repair templates and generate patches. The experimental results indicate that when applied to fault localization, CodeHealer outperforms the best baseline method with improvements of 11.4%, 2.7%, and 1.6% on Top-1/3/5 metrics, respectively. It also reduces the MFR and MAR by 9.8% and 2.1%, where lower values denote better fault localization effectiveness. Additionally, in automated software debugging, CodeHealer fixes an additional 6 faults compared to the current best method, totaling 53 faults repaired."
Meta-Review of Wearable Devices for Healthcare in the Metaverse,"Vahdati, Monireh (Monica) and Laamarti, Fedwa and Saddik, Abdulmotaleb El",10.1145/3705320,2024,"In recent years, there has been a growing interest in leveraging the metaverse to enhance community engagement and healthcare. This paper provides a comprehensive examination of wearable devices and sensors utilized within immersive environments to improve well-being and healthcare outcomes. We categorize the healthcare application domains that employ wearable devices and identify commonly used devices and sensors based on a thorough review of the literature. Our study offers a detailed summary of these applications, highlighting their potential to enhance overall quality of life through remote monitoring, rehabilitation, and chronic disease management. Furthermore, we address existing research gaps and challenges in this field, offering insights for future research directions. This meta-review emphasizes the need for further exploration in the rapidly evolving domain of wearable healthcare technologies within the metaverse, presenting an overview of the current state of wearable devices in healthcare and underscoring their significance in advancing healthcare delivery and outcomes."
An Architectural Viewpoint for Benefit-Cost-Risk-Aware Decision-Making in Self-Adaptive Systems,"Weyns, Danny and Hezavehi, Sara M. and Avgeriou, Paris and Calinescu, Radu and Mirandola, Raffaela and Perez-Palacin, Diego",10.1145/3705612,2025,"Self-adaptation equips a software system with a feedback loop that resolves uncertainties during operation and adapts the system to deal with them when necessary. Most self-adaptation approaches today use decision-making mechanisms that select for execution the adaptation option with the best-estimated benefit expressed as a set of adaptation goals. A few approaches also consider the estimated (one-off) cost of executing the candidate adaptation options. We argue that besides benefit and cost, decision-making in self-adaptive systems should also consider the estimated risk the system or its users would be exposed to if an adaptation option were selected for execution. Balancing all three concerns when evaluating the options for adaptation to mitigate uncertainty is essential for satisfying stakeholders’ concerns and ensuring the safety and public acceptance of self-adaptive systems. In this article, we present a reference model for decision-making in self-adaptation that considers the estimated benefit, cost, and risk as core concerns of each adaptation option. Leveraging this model, we then present an ISO/IEC/IEEE 42010 compatible architectural viewpoint that aims at supporting software architects responsible for designing robust decision-making mechanisms for self-adaptive systems. We demonstrate the applicability, usefulness, and understandability of the viewpoint through a case study where participants with experience in the engineering of self-adaptive systems performed a set of design tasks in DeltaIoT, an Internet-of-Things exemplar for research on self-adaptive systems."
SoK: Access Control Policy Generation from High-level Natural Language Requirements,"Jayasundara, Sakuna Harinda and Gamagedara Arachchilage, Nalin Asanka and Russello, Giovanni",10.1145/3706057,2024,"Administrator-centered access control failures can cause data breaches, putting organizations at risk of financial loss and reputation damage. Existing graphical policy configuration tools and automated policy generation frameworks attempt to help administrators configure and generate access control policies by avoiding such failures. However, graphical policy configuration tools are prone to human errors, making them unusable. On the other hand, automated policy generation frameworks are prone to erroneous predictions, making them unreliable. Therefore, to find ways to improve their usability and reliability, we conducted a Systematic Literature Review analyzing 49 publications. The thematic analysis of the publications revealed that graphical policy configuration tools are developed to write and visualize policies manually. Moreover, automated policy generation frameworks are developed using machine learning (ML) and natural language processing (NLP) techniques to automatically generate access control policies from high-level requirement specifications. Despite their utility in the access control domain, limitations of these tools, such as the lack of flexibility, and limitations of frameworks, such as the lack of domain adaptation, negatively affect their usability and reliability, respectively. Our study offers recommendations to address these limitations through real-world applications and recent advancements in the NLP domain, paving the way for future research."
A Systematic Review of Trust Assessments in Human–Robot Interaction,"Campagna, Giulio and Rehm, Matthias",10.1145/3706123,2025,"The integration of robots into daily life has increased significantly, spanning applications from social-care to industrial settings with collaborative robots. Ensuring a safe, secure environment and equitable workload distribution in human-robot collaborations is crucial. Trust is a key factor in these environments, essential for enhancing collaboration and achieving tasks while maintaining safety. Under-trusting robots can hinder productivity, while over-trusting them can lead to accidents. A comprehensive literature review of 100 publications from 2003 to 2023 analyzes trust and its influencing factors in industrial and social-care contexts. Findings reveal that in industrial settings, robot-related factors are more influential, whereas in social-care, human and environmental factors play a significant role. Furthermore, the review delves into gauging trust through observable behavior, while also exploring various trust evaluation methodologies. Results show that trust can be gauged through human behaviors, physical cues, and physiological measurements. Concerning trust evaluation methodologies, traditional questionnaires have limitations, opening new opportunities for machine learning and sensor-based approaches to real-time trust evaluation, as trust is a dynamic cognitive value that evolves over time. Notably, 97% of the reviewed articles were published in the last decade, underscoring a growing interest in human–robot interaction and trust within the scientific community."
"Object Selection and Manipulation in VR Headsets: Research Challenges, Solutions, and Success Measurements","Yu, Difeng and Dingler, Tilman and Velloso, Eduardo and Goncalves, Jorge",10.1145/3706417,2024,"Object selection and manipulation are the foundation of VR interactions. With the rapid development of VR technology and the field of virtual object selection and manipulation, the literature demands a structured understanding of the core research challenges and a critical reflection of the current practices. To provide such understanding and reflections, we systematically reviewed 106 papers. We identified classic and emerging topics, categorized existing solutions, and evaluated how success was measured in these publications. Based on our analysis, we discuss future research directions and propose a framework for developing and determining appropriate solutions for different application scenarios."
What Could Possibly Go Wrong: Undesirable Patterns in Collective Development,"Evtikhiev, Mikhail and Koshchenko, Ekaterina and Kovalenko, Vladimir",10.1145/3707451,2025,"Software development, often perceived as a technical endeavor, is fundamentally a social activity requiring collaboration among team members. Acknowledging this, the software development community has devised strategies to address possible collaboration-related shortcomings. Various studies have attempted to capture the social dynamics within software engineering. These studies developed methods to identify numerous teamwork issues and proposed various approaches to address them. However, there is a need for a comprehensive bottom-up exploration from practitioner’s perceptions to common patterns. This article introduces the concept of undesirable patterns in collective development, referring to potential teamwork problems that may escalate if unaddressed. Through 38 in-depth exploratory interviews, we identify and classify 42 patterns, revealing their origins and consequences. To the best of our knowledge, some patterns, like Teamwork pipeline bottleneck, were never reported before. Subsequent surveys, 436 and 968 participants each, explore the significance and frequency of the undesirable patterns and evaluate potential tools and features to manage these patterns. The study contributes a nuanced understanding of undesirable patterns, evaluating their impact and proposing pragmatic tools and features for industrial application. The findings provide a valuable foundation for further in-depth studies and the development of tools to enhance collaborative software engineering practices."
Refining Code-Line-Level Bugginess Identification: Getting the Best of Both Worlds by Fusing Syntactic and Semantic Features,"Zhou, Yufei and Tang, Haihua and Zhu, Longtao and Ding, Hao and Qian, Junyan",10.1145/3707456,2025,"Background: Code-line-level bugginess identification (CLBI) is an important area within software quality assurance, aiming to pinpoint potential buggy source code lines in a given software product. Recently, two concurrent approaches, GLANCE and DeepLineDP, have showcased impressive performance by respectively leveraging syntactic and semantic features compared with the existing state-of-the-art (SOTA) approaches in this field. Problem: Yet, the literature lacks a thorough investigation that fuses these two types of features to enhance CLBI. Such fusion holds the promise of significantly improving the efficacy of identifying defective lines. Objective: We aim to advance CLBI by fusing syntactic and semantic features, thereby harnessing their respective strengths. Method: We propose to build a CLBI approach, booSting DeePLineDP wIth syntaCtic fEatures (SPLICE), by fusing syntactic features from GLANCE and semantic features from DeepLineDP. SPLICE comprises three variants—SPLICE-S, SPLICE-G, and SPLICE-F—each utilizing a unique line-level sorting approach. We make a comprehensive comparison with the existing SOTA approaches using six performance metrics. Result: Through an analysis of nine open source projects, our experimental results reveal that SPLICE is competitive with current SOTA CLBI approaches. Notably, SPLICE-F demonstrates superiority over all SOTA CLBI approaches, including GLANCE and DeepLineDP, across all six metrics, indicating a substantial improvement. Conclusion: This discovery underscores the critical importance of future CLBI research in fusing syntactic and semantic features to construct more effective bugginess identification approaches. It is worth noting that the analysis was conducted within the context of Java programs, which highlights the potential for exploring similar methods in other programming languages in future research."
Training and Support Practices and Insights in the Sustainability of mHealth Implementations in Sub-Saharan Africa,"Babirye, Diana and Anderson, Richard",10.1145/3707638,2025,"We conducted an interview-based study to understand the training and support practices and insights that influence sustainable implementations of mHealth projects of varying mHealth technologies. The qualitative assessment was inspired by the Consolidated Framework for Implementation Research adapted with the recommended questions for guiding the implementation of training programs. Participants in the study (N = 16) were both international and in-country researchers managing projects in sub-Saharan Africa. We chose to focus on training and support, as it is often the interface between international and in-country components of these projects. Participants agreed on the central role of training in the implementation of mHealth projects and are following best practices. There was often a division in the responsibility of training between international and in-country researchers reflecting the distinction between project management and local implementation adaptions. One of the main findings is the importance of individualized continuous retraining to make deployments robust with respect to changing conditions with technology and personnel, an aspect that has been managed a bit differently with varying mHealth technologies as presented in this article."
Intelligent Generation of Graphical Game Assets: A Conceptual Framework and Systematic Review of the State of the Art,"Fukaya, Kaisei and Daylamani-Zad, Damon and Agius, Harry",10.1145/3708499,2025,"Procedural content generation (PCG) can be applied to a wide variety of tasks in games, from narratives, levels, and sounds to trees and weapons. A large amount of game content is composed of graphical assets, such as clouds, buildings, or vegetation, that do not require gameplay function considerations. There is also a breadth of literature examining the procedural generation of such elements for purposes outside of games. The body of research, focused on specific methods for generating specific assets, provides a narrow view of the available possibilities. Hence, it is difficult to have a clear picture of all approaches and possibilities, with no guide for interested parties to discover possible methods and approaches for their needs and no facility to guide them through each technique or approach to map out the process of using them. Therefore, a systematic literature review has been conducted, yielding 239 accepted papers. This article explores state-of-the-art approaches to graphical asset generation, examining research from a wide range of applications, inside and outside of games. Informed by the literature, a conceptual framework has been derived to address the aforementioned gaps."
Automatic Programming: Large Language Models and Beyond,"Lyu, Michael R. and Ray, Baishakhi and Roychoudhury, Abhik and Tan, Shin Hwei and Thongtanunam, Patanamon",10.1145/3708519,2024,"Automatic programming has seen increasing popularity due to the emergence of tools like GitHub Copilot which rely on Large Language Models (LLMs). At the same time, automatically generated code faces challenges during deployment due to concerns around quality and trust. In this article, we study automated coding in a general sense and study the concerns around code quality, security and related issues of programmer responsibility. These are key issues for organizations while deciding on the usage of automatically generated code. We discuss how advances in software engineering such as program repair and analysis can enable automatic programming. We conclude with a forward looking view, focusing on the programming environment of the near future, where programmers may need to switch to different roles to fully utilize the power of automatic programming. Automated repair of automatically generated programs from LLMs, can help produce higher assurance code from LLMs, along with evidence of assurance."
A road-map to Readily Available Early Validation &amp; Verification of System Behaviour in Model-Based Systems Engineering using Software Engineering Best Practices,"Cederbladh, Johan and Cicchetti, Antonio and Jongeling, Robbert",10.1145/3708520,2024,"In this article we discuss how we can facilitate the growing need for early validation and verification (V&amp;V) of system behaviour in Model-Based Systems Engineering (MBSyE). Several aspects, such as reducing cost and time to market, push companies towards integration of V&amp;V methods earlier in development to support effective decision-making. One foundational methodology seeing increased attention in industry is the use of MBSyE, which brings benefits of models with well-defined syntax and semantics to support V&amp;V activities, rather than relying on natural language text documentation. Despite their promise, industrial adoption of these practices is still challenging.This article presents a vision for readily available early V&amp;V. We present a summary of the literature on early V&amp;V in MBSyE and position existing challenges regarding potential solutions and future investigations towards this vision. We elaborate our vision by means of challenges with a specific emphasis on early V&amp;V of system behaviour. We identify three specific challenge areas: Creating and managing Models, Organisational systems engineering aspects, and early V&amp;V Methods. Finally, we outline a road-map to address these categories of challenges, in which we propose the transfer of established best practices from the software engineering domain to support emerging technologies in the systems engineering domain."
A Systematic Literature Review of Multi-Label Learning in Software Engineering,"H\""{a}m\""{a}l\""{a}inen, Joonas and Das, Teerath and Mikkonen, Tommi",10.1145/3708532,2024,"In this paper, we provide the first systematic literature review of the intersection of two research areas, Multi-Label Learning (MLL) and Software Engineering (SE). We refer to this intersection as MLL4SE. In recent years, MLL problems have increased in many applications and research areas because real-world datasets often have a multi-label nature. For multi-label data, simplifying the assumption of traditional classification approaches that an instance can only be associated with one class only leads to worse accuracy. Thus, a better match of methods and assumptions about the data is required. We identified 50 primary studies in our systematic literature review in the MLL4SE domain. Based on this review, we identified six main SE application domains where MLL has been applied. These domains include Software Requirement Engineering, Issue Tracking and Management, Community and Knowledge Management, API Usage and Management, Code Quality and Maintenance, and Mobile Application Development. We summarized the methods used and the data nature of the MLL4SE applications. Moreover, we separately provide taxonomies of future work directions from machine learning and software engineering perspectives. In general, we highlight current trends, research gaps, and shortcomings."
Adaptive Intention Learning for Session-Based Recommendation,"Zhang, Qingbo and Yang, Xiaochun and Chen, Hao and Wang, Bin and Sun, Zhu and Zhou, Xiangmin",10.1145/3709004,2025,"In recent years, session-based recommender systems (SRSs) have emerged as a significant research focus within the recommendation field. Capturing user intentions to infer user interest accordingly has proven to be effective in enhancing the accuracy of SRSs. However, existing techniques assume that all sessions have the same number of intentions or that the items in one category belonging to the same session reflect the same intention. In real applications, such as e-commerce, sessions may have different numbers of intentions, and the same type of items in a session may correspond to different intentions. As a result, existing techniques cannot guarantee high-quality user interest prediction. In this article, we propose a novel Adaptive Intention Learning Network (AILN) to capture an adaptive number of intentions for each session, thereby enhancing the accuracy of user interest inference. Specifically, we design an intention evaluation network (IEN) to evaluate whether a subsequence of a session corresponds to a valid intention, and an intention generation network (IGN) to learn the representation of a valid intention. By checking each subsequence of a session, IEN and IGN enable the incremental learning of a session-specific intention hierarchy (IH) to store valid intentions of the session. To reduce the cost of building the IH, we propose a pruning strategy that exploits the intention validity to avoid unnecessary evaluation. The representative intentions are selected from IH and input into a designed interest predictor to infer the user interest. Experimental results on two real-world datasets demonstrate the superiority of our proposed AILN."
Enhancing Brain Disease Diagnosis with XAI: A Review of Recent Studies,"Bibi, Nighat and Courtney, Jane and McGuinness, Kevin",10.1145/3709152,2025,"The area of eXplainable Artificial Intelligence (XAI) has shown remarkable progress in the past few years, with the aim of enhancing the transparency and interpretability of the Machine Learning (ML) and Deep Learning (DL) models. This review article presents an in-depth review of the current state-of-the-art XAI techniques applied to the diagnosis of brain diseases. The challenges encountered by traditional ML and DL models within this domain are thoroughly examined, emphasizing the pivotal role of XAI in providing the transparency and interpretability of these models. Furthermore, this article presents a comprehensive survey of the XAI methodologies used for making diagnoses of various brain disorders. Recent studies utilizing XAI for diagnosing a range of brain illnesses, including Alzheimer, brain tumors, dementia, Parkinson, multiple sclerosis, autism, epilepsy, and stroke, are critically reviewed. Finally, the limitations inherent in current XAI techniques are discussed, along with prospective avenues for future research. The key goal of this study is to provide researchers with a roadmap that shows the potential of XAI techniques in improving the interpretability and transparency of DL and ML algorithms for the diagnosis of brain diseases, while also delineating the challenges that require concerted research efforts."
From Today’s Code to Tomorrow’s Symphony: The AI Transformation of Developer’s Routine by 2030,"Qiu, Ketai and Puccinelli, Niccol\`{o} and Ciniselli, Matteo and Di Grazia, Luca",10.1145/3709353,2024,"In the rapidly evolving landscape of software engineering, the integration of Artificial Intelligence (AI) into the Software Development Life-Cycle (SDLC) heralds a transformative era for developers. Recently, we have assisted to a pivotal shift towards AI-assisted programming, exemplified by tools like GitHub Copilot and OpenAI’s ChatGPT, which have become a crucial element for coding, debugging, and software design. In this paper we provide a comparative analysis between the current state of AI-assisted programming in 2024 and our projections for 2030, by exploring how AI advancements are set to enhance the implementation phase, fundamentally altering developers’ roles from manual coders to orchestrators of AI-driven development ecosystems. We envision HyperAssistant, an augmented AI tool that offers comprehensive support to 2030 developers, addressing current limitations in mental health support, fault detection, code optimization, team interaction, and skill development. We emphasize AI as a complementary force, augmenting developers’ capabilities rather than replacing them, leading to the creation of sophisticated, reliable, and secure software solutions. Our vision seeks to anticipate the evolution of programming practices, challenges, and future directions, shaping a new paradigm where developers and AI collaborate more closely, promising a significant leap in SE efficiency, security and creativity."
From Triumph to Uncertainty: The Journey of Software Engineering in the AI Era,"Mastropaolo, Antonio and Escobar-Vel\'{a}squez, Camilo and Linares-V\'{a}squez, Mario",10.1145/3709360,2024,"Over the last ten years, the realm of Artificial Intelligence (AI) has experienced an explosion of revolutionary breakthroughs, transforming what seemed like a far-off dream into a reality that is now deeply embedded in our everyday lives. AI’s widespread impact is revolutionizing virtually all aspects of human life, and software engineering (SE) is no exception. As we explore this changing landscape, we are faced with questions about what the future holds for SE and how AI will reshape the roles, duties, and methodologies within the field. The introduction of these groundbreaking technologies highlights the inevitable shift towards a new paradigm, suggesting a future where AI’s capabilities may redefine the boundaries of SE, potentially even more than human input.In this paper, we aim at outlining the key elements that, based on our expertise, are vital for the smooth integration of AI into SE, all while preserving the intrinsic human creativity that has been the driving force behind the field. First, we provide a brief description of SE and AI evolution. Afterward, we delve into the intricate interplay between AI-driven automation and human innovation, exploring how these two components can work together to advance SE practices to new methods and standards."
Towards Equitable Community-Industry Collaborations: Understanding the Experiences of Nonprofits' Collaborations with Tech Companies,"Erete, Sheena and Corbett, Eric and Smith-Walker, Natasha and Cunningham, Jay L. and Gatz, Erin and Park, Tina and Perry, Tam and Wilcox, Lauren and Denton, Remi",10.1145/3710918,2025,"Community-based partnerships are essential to creating inclusive and equitable technologies and design practices. Though recent scholarship in HCI focuses on equitable design practices, there is less focus on understanding the experiences of community-based nonprofit organizations (CBOs) when partnering with technology companies. In this paper, we focus on understanding the perspectives of CBOs by answering the following research question: What are the experiences of CBOs that have collaborated with technology companies? Through a series of design workshops with 18 participants who work at community-based nonprofits that have collaborated with technology firms, we identified four elements of community-industry collaborations that collectively shape the overall experience: divergences in cultural and organizational norms, ''setting the table,'' project relationship dynamics, and affective qualities. We conclude by discussing the power structures that impact community-industry collaboration and suggest reflective practices to guide equitable collaborations between CBOs and tech companies."
"AURA: Amplifying Understanding, Resilience, and Awareness for Responsible AI Content Work","Zhang, Alice Qian and Amores, Judith and Shen, Hong and Czerwinski, Mary and Gray, Mary L. and Suh, Jina",10.1145/3710931,2025,"Behind the scenes of maintaining the safety of technology products from harmful and illegal digital content lies unrecognized human labor. The recent rise in the use of generative AI technologies and the accelerating demands to meet responsible AI (RAI) aims necessitates an increased focus on the labor behind such efforts in the age of AI. This study investigates the nature and challenges of content work that supports RAI efforts, or ""RAI content work,"" that spans content moderation, data labeling, and red teaming -- through the lived experiences of content workers. We conduct a formative survey and semi-structured interview studies to develop a conceptualization of RAI content work and a subsequent framework of recommendations for providing holistic support for content workers. We validate our recommendations through a series of workshops with content workers and derive considerations for and examples of implementing such recommendations. We discuss how our framework may guide future innovation to support the well-being and professional development of the RAI content workforce."
Black Older Adults' Perception of Using Voice Assistants to Enact a Medical Recovery Curriculum,"Green, Andrea and Polite, Gabrielle and Hung, Isabelle and Fessele, Kristen L. and Billington, Sarah L. and Landay, James A. and Cuadra, Andrea",10.1145/3710937,2025,"The use of interactive voice assistants (IVAs) in healthcare provides an avenue to address diverse health needs, such as gaps in the medical recovery period for older adult patients who have recently experienced serious illness. By using a voice-assisted medical recovery curriculum, discharged patients can receive ongoing support as they recover. However, there exist significant medical and technology disparities among older adults, particularly among Black older adults. We recruited 26 Black older adults to participate in the design process of an IVA-enacted medical recovery curriculum by providing feedback during the early stages of design. Lack of cultural relevancy, accountability, privacy concerns, and stigmas associated with aging and disability made participants reluctant to engage with the technology unless in a position of extreme need. This study underscored the need for Black cultural representation, whether it regarded the IVA's accent, the types of media featured, or race-specific medical advice, and the need for strategies to address participants' concerns and stigmas. Participants saw the value in the curriculum for those who did not have caregivers and deliberated about the trade-offs the technology presented. We discuss tensions surrounding inclusion and representation and conclude by showing how we enacted the lessons from this study in future design plans."
Customizing Generated Signs and Voices of AI Avatars: Deaf-Centric Mixed-Reality Design for Deaf-Hearing Communication,"Chen, Si and Cheng, Haocong and Su, Suzy and Patterson, Stephanie and Kushalnagar, Raja and Huang, Yun and Wang, Qi",10.1145/3710953,2025,"This study investigates innovative interaction designs for communication and collaborative learning between learners of mixed hearing and signing abilities, leveraging advancements in mixed reality technologies like Apple Vision Pro and generative AI for animated avatars. Adopting a participatory design approach, we engaged 15 d/Deaf and hard of hearing (DHH) students to brainstorm ideas for an AI avatar with interpreting ability (sign language to English and English to sign language) that would facilitate their face-to-face communication with hearing peers. Participants envisioned the AI avatars to address some issues with human interpreters, such as lack of availability, and provide affordable options to expensive personalized interpreting services. Our findings indicate a range of preferences for integrating the AI avatars with actual human figures of both DHH and hearing communication partners. The participants highlighted the importance of having control over customizing the AI avatar, such as AI-generated signs, voices, facial expressions, and their synchronization for enhanced emotional display in communication. Based on our findings, we propose a suite of design recommendations that balance respecting sign language norms with adherence to hearing social norms. Our study offers insights into improving the authenticity of generative AI in scenarios involving specific and sometimes unfamiliar social norms."
LATA: A Pilot Study on LLM-Assisted Thematic Analysis of Online Social Network Data Generation Experiences,"Wang, Qile and Erqsous, Moath and Barner, Kenneth E. and Mauriello, Matthew Louis",10.1145/3711022,2025,"Large Language Models (LLMs) have gained attention in research and industry, aiming to streamline processes and enhance text analysis performance. Thematic Analysis (TA), a prevalent qualitative method for analyzing interview content, often requires at least two human experts to review and analyze data. This study demonstrates the feasibility of LLM-Assisted Thematic Analysis (LATA) using GPT-4 and Gemini. Specifically, we conducted semi-structured interviews with 14 researchers to gather insights on their experiences generating and analyzing Online Social Network (OSN) communications datasets. Following Braun and Clarke's six-phase TA framework with an inductive approach, we initially analyzed our interview transcripts with human experts. Subsequently, we iteratively designed prompts to guide LLMs through a similar process. We compare and discuss the manually analyzed outcomes with responses generated by LLMs and achieve a cosine similarity score up to 0.76, demonstrating a promising prospect for LATA. Additionally, the study delves into researchers' experiences navigating the complexities of collecting and analyzing OSN data, offering recommendations for future research and application designers."
RequestAtlas: Supporting the Slow and Iterative Process of Requesting Public Records,"Warren, Rachel B. and Pickoff-White, Lisa and Parameswaran, Aditya G. and Salehi, Niloufar",10.1145/3711056,2025,"Public records requests are a central mechanism for government transparency. In practice, they are slow, complex processes that require analyzing large amounts of messy, unstructured data. In this paper, we introduce RequestAtlas, a system that helps investigative journalists review large quantities of unstructured data that result from submitting many public records requests. RequestAtlas was developed through a year-long participatory design collaboration with the California Reporting Project (CRP), a journalistic collective researching police use of force and police misconduct in California. RequestAtlas helps journalists evaluate the results of public records requests for completeness and negotiate with agencies for additional information. RequestAtlas has had significant real-world impact. It has been deployed for more than a year to identify missing data in response to public records requests and to facilitate negotiation with public records request officers. Through the process of designing and observing the use of RequestAtlas, we explore the technical challenges associated with the public records request process and the design needs of investigative journalists more generally. We argue that public records requests represent an instance of an adversarial technical relationship in which two entities engage in a prolonged, iterative, often adversarial exchange of information. Technologists can support information-gathering efforts within these adversarial technical relationships by building flexible local solutions that help both entities account for the state of the ongoing information exchange. Additionally, we offer insights on ways to design applications that can assist investigative journalists in the inevitably significant data cleaning phase of processing large documents while supporting journalistic norms of verification and human review. Finally, we reflect on the ways that this participatory design process, despite its success, lays bare some of the limitations inherent in the public records request process and in the ''request and respond'' model of transparency more generally."
Systematic Literature Review of Using Virtual Reality as a Social Platform in HCI Community,"Wei, Xiaoying and Jin, Xiaofu and Lin Kan, Ge and Yan, Yukang and Fan, Mingming",10.1145/3711078,2025,"Virtual reality (VR) is increasingly used as a social platform for users to interact and build connections with one another in an immersive virtual environment. Reflecting on the empirical progress in this area of study, a comprehensive review of how VR could be used to support social interaction is required to consolidate existing practices and identify research gaps to inspire future studies. In this work, we conducted a systematic review of 94 publications in the HCI field to examine how VR is designed and evaluated for social purposes. We found that VR influences social interaction through self-representation, interpersonal interactions, and interaction environments. We summarized four positive effects of using VR for socializing, which are relaxation, engagement, intimacy, and accessibility, and showed that it could also negatively affect user social experiences by intensifying harassment experiences and amplifying privacy concerns. We introduce an evaluation framework that outlines the key aspects of social experience: intrapersonal, interpersonal, and interaction experiences. According to the results, we uncover several research gaps and propose future directions for designing and developing VR to enhance social experience."
A Systematic Literature Review on Equity and Technology in HCI and Fairness: Navigating the Complexities and Nuances of Equity Research,"Kim, Seyun and Bai, Yuanchen and Zhu, Haiyi and Eslami, Motahhare",10.1145/3711079,2025,"Equity is crucial to the ethical implications in technology development. However, implementing equity in practice comes with complexities and nuances. In response, the research community, especially the human-computer interaction (HCI) and Fairness community, has endeavored to integrate equity into technology design, addressing issues of societal inequities. With such increasing efforts, it is yet unclear why and how researchers discuss equity and its integration into technology, what research has been conducted, and what gaps need to be addressed. We conducted a systematic literature review on equity and technology, collecting and analyzing 202 papers published in HCI and Fairness-focused venues. Amidst the substantial growth of relevant publications within the past four years, we deliver three main contributions: (1) we elaborate a comprehensive understanding researchers' motivations for studying equity and technology, (2) we illustrate the different equity definitions and frameworks utilized to discuss equity, (3) we characterize the key themes addressing interventions as well as tensions and trade-offs when advancing and integrating equity to technology. Based on our findings, we elaborate an equity framework for researchers who seek to address existing gaps and advance equity in technology."
Who is to Blame: A Comprehensive Review of Challenges and Opportunities in Designer-Developer Collaboration,"Zhang, Shutong and Zhang, Tianyu and Cheng, Jinghui and Zhou, Shurui",10.1145/3711105,2025,"Software development relies on effective collaboration between Software Development Engineers (SDEs) and User eXperience Designers (UXDs) to create software products of high quality and usability. While this collaboration issue has been explored over the past decades, anecdotal evidence continues to indicate the existence of challenges in their collaborative efforts. To understand this gap, we first conducted a systematic literature review (SLR) of 45 papers published since 2004, uncovering three key collaboration challenges and two main categories of potential best practices. We then analyzed designer and developer forums and discussions from one open-source software repository to assess how the challenges and practices manifest in the status quo. Our findings have broad applicability for collaboration in software development, extending beyond the partnership between SDEs and UXDs. The suggested best practices and interventions also act as a reference for future research, assisting in the development of dedicated collaboration tools for SDEs and UXDs."
LLM-Powered Static Binary Taint Analysis,"Liu, Puzhuo and Sun, Chengnian and Zheng, Yaowen and Feng, Xuan and Qin, Chuan and Wang, Yuncheng and Xu, Zhenyang and Li, Zhi and Di, Peng and Jiang, Yu and Sun, Limin",10.1145/3711816,2025,"This article proposes LATTE, the first static binary taint analysis that is powered by a large language model (LLM). LATTE is superior to the state of the art (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully automated while prior static binary taint analyzers need rely on human expertise to manually customize taint propagation rules and vulnerability inspection rules. Second, LATTE is significantly effective in vulnerability detection, demonstrated by our comprehensive evaluations. For example, LATTE has found 37 new bugs in real-world firmware, which the baselines failed to find. Moreover, 10 of them have been assigned CVE numbers. Lastly, LATTE incurs remarkably low engineering cost, making it a cost-efficient and scalable solution for security researchers and practitioners. We strongly believe that LATTE opens up a new direction to harness the recent advance in LLMs to improve vulnerability analysis for binary programs."
How are We Detecting Inconsistent Method Names? An Empirical Study from Code Review Perspective,"Kim, Kisub and Zhou, Xin and Kim, Dongsun and Lawall, Julia and Liu, Kui and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques and Lee, Jaekwon and Lo, David",10.1145/3711901,2025,"Proper naming of methods can make program code easier to understand, and thus enhance software maintainability. Yet, developers may use inconsistent names due to poor communication or a lack of familiarity with conventions within the software development lifecycle. To address this issue, much research effort has been invested into building automatic tools that can check for method name inconsistency and recommend consistent names. However, existing datasets generally do not provide precise details about why a method name was deemed improper and required to be changed. Such information can give useful hints on how to improve the recommendation of adequate method names. Accordingly, we construct a sample method-naming benchmark, ReName4J, by matching name changes with code reviews. We then present an empirical study on how state-of-the-art techniques perform in detecting or recommending consistent and inconsistent method names based on ReName4J. The main purpose of the study is to reveal a different perspective based on reviewed names rather than proposing a complete benchmark. We find that the existing techniques underperform on our review-driven benchmark, both in inconsistent checking and the recommendation. We further identify potential biases in the evaluation of existing techniques, which future research should consider thoroughly."
How Can Haptic Feedback Assist People with Blind and Low Vision (BLV): A Systematic Literature Review,"Jiang, Chutian and Kuang, Emily and Fan, Mingming",10.1145/3711931,2025,"People who are blind and low vision (BLV) encounter numerous challenges in their daily lives and work. To support them, various haptic assistive tools have been developed. Despite these advancements, the effective utilization of these tools—including the optimal haptic feedback and on-body stimulation positions for different tasks along with their limitations—remains poorly understood. Recognizing these gaps, we conducted a systematic literature review spanning two decades (2004–2024) to evaluate the development of haptic assistive tools within the HCI community. Our findings reveal that these tools are primarily used for understanding graphical information, providing guidance and navigation, and facilitating education and training, among other life and work tasks. We identified three main limitations: hardware limitations, functionality limitations, and UX and evaluation methods limitations. Based on these insights, we discuss potential research avenues and offer suggestions for enhancing the effectiveness of future haptic assistive technologies."
Quantum Software Engineering: Roadmap and Challenges Ahead,"Murillo, Juan M. and Garcia-Alonso, Jose and Moguel, Enrique and Barzen, Johanna and Leymann, Frank and Ali, Shaukat and Yue, Tao and Arcaini, Paolo and P\'{e}rez-Castillo, Ricardo and Garc\'{\i}a Rodr\'{\i}guez de Guzm\'{a}n, Ignacio and Piattini, Mario and Ruiz-Cort\'{e}s, Antonio and Brogi, Antonio and Zhao, Jianjun and Miranskyy, Andriy and Wimmer, Manuel",10.1145/3712002,2025,"As quantum computers advance, the complexity of the software they can execute increases as well. To ensure this software is efficient, maintainable, reusable, and cost-effective —key qualities of any industry-grade software— mature software engineering practices must be applied throughout its design, development, and operation. However, the significant differences between classical and quantum software make it challenging to directly apply classical software engineering methods to quantum systems. This challenge has led to the emergence of Quantum Software Engineering as a distinct field within the broader software engineering landscape. In this work, a group of active researchers analyse in depth the current state of quantum software engineering research. From this analysis, the key areas of quantum software engineering are identified and explored in order to determine the most relevant open challenges that should be addressed in the next years. These challenges help identify necessary breakthroughs and future research directions for advancing Quantum Software Engineering."
"LLM-Based Multi-Agent Systems for Software Engineering: Literature Review, Vision and the Road Ahead","He, Junda and Treude, Christoph and Lo, David",10.1145/3712003,2025,"Integrating Large Language Models (LLMs) into autonomous agents marks a significant shift in the research landscape by offering cognitive abilities that are competitive with human planning and reasoning. This paper explores the transformative potential of integrating Large Language Models into Multi-Agent (LMA) systems for addressing complex challenges in software engineering (SE). By leveraging the collaborative and specialized abilities of multiple agents, LMA systems enable autonomous problem-solving, improve robustness, and provide scalable solutions for managing the complexity of real-world software projects. In this paper, we conduct a systematic review of recent primary studies to map the current landscape of LMA applications across various stages of the software development lifecycle (SDLC). To illustrate current capabilities and limitations, we perform two case studies to demonstrate the effectiveness of state-of-the-art LMA frameworks. Additionally, we identify critical research gaps and propose a comprehensive research agenda focused on enhancing individual agent capabilities and optimizing agent synergy. Our work outlines a forward-looking vision for developing fully autonomous, scalable, and trustworthy LMA systems, laying the foundation for the evolution of Software Engineering 2.0."
The Current Challenges of Software Engineering in the Era of Large Language Models,"Gao, Cuiyun and Hu, Xing and Gao, Shan and Xia, Xin and Jin, Zhi",10.1145/3712005,2025,"With the advent of large language models (LLMs) in the artificial intelligence (AI) area, the field of software engineering (SE) has also witnessed a paradigm shift. These models, by leveraging the power of deep learning and massive amounts of data, have demonstrated an unprecedented capacity to understand, generate, and operate programming languages. They can assist developers in completing a broad spectrum of software development activities, encompassing software design, automated programming, and maintenance, which potentially reduces huge human efforts. Integrating LLMs within the SE landscape (LLM4SE) has become a burgeoning trend, necessitating exploring this emergent landscape’s challenges and opportunities.The paper aims at revisiting the software development life cycle (SDLC) under LLMs, and highlighting challenges and opportunities of the new paradigm. The paper first summarizes the overall process of LLM4SE, and then elaborates on the current challenges based on a through discussion. The discussion was held among more than 20 participants from academia and industry, specializing in fields such as software engineering and artificial intelligence. Specifically, we achieve 26 key challenges from seven aspects, including software requirement &amp; design, coding assistance, testing code generation, code review, code maintenance, software vulnerability management, and data, training, and evaluation. We hope the achieved challenges would benefit future research in the LLM4SE field."
AI for DevSecOps: A Landscape and Future Opportunities,"Fu, Michael and Pasuksmit, Jirat and Tantithamthavorn, Chakkrit",10.1145/3712190,2025,"DevOps has emerged as one of the most rapidly evolving software development paradigms. With the growing concerns surrounding security in software systems, the DevSecOps paradigm has gained prominence, urging practitioners to incorporate security practices seamlessly into the DevOps workflow. However, integrating security into the DevOps workflow can impact agility and impede delivery speed. Recently, the advancement of artificial intelligence (AI) has revolutionized automation in various software domains, including software security. AI-driven security approaches, particularly those leveraging machine learning or deep learning, hold promise in automating security workflows. They have the potential to reduce manual efforts and can be incorporated into DevOps practices to support consistent delivery speed while aligning with the principles of the DevSecOps paradigm. This article seeks to contribute to the critical intersection of AI and DevSecOps by presenting a comprehensive landscape of AI-driven security techniques applicable to DevOps and identifying avenues for enhancing security, trust, and efficiency in software development processes. We analyzed 99 research papers spanning from 2017 to 2023. Specifically, we address two key research questions (RQs). In RQ1, we identified 12 security tasks associated with the DevSecOps process and reviewed existing AI-driven security approaches, the problems they addressed, and the 65 benchmarks used to evaluate those approaches. Drawing insights from our findings, in RQ2, we discussed state-of-the-art AI-driven security approaches, highlighted 15 challenges in existing research, and proposed 15 corresponding avenues for future opportunities."
"Uncovering Community Smells in Machine Learning-Enabled Systems: Causes, Effects, and Mitigation Strategies","Annunziata, Giusy and Lambiase, Stefano and Tamburri, Damian A. and van den Heuvel, Willem-Jan and Palomba, Fabio and Catolino, Gemma and Ferrucci, Filomena and De Lucia, Andrea",10.1145/3712198,2025,"Successful software development hinges on effective communication and collaboration, which are significantly influenced by human and social dynamics. Poor management of these elements can lead to the emergence of ‘community smells’, i.e., negative patterns in socio-technical interactions that gradually accumulate as ‘social debt’. This issue is particularly pertinent in machine learning-enabled systems, where diverse actors such as data engineers and software engineers interact at various levels. The unique collaboration context of these systems presents an ideal setting to investigate community smells and their impact on development communities. This paper addresses a gap in the literature by identifying the types, causes, effects, and potential mitigation strategies of community smells in machine learning-enabled systems. Using Partial Least Squares Structural Equation Modeling (PLS-SEM), we developed hypotheses based on existing literature and interviews, and conducted a questionnaire-based study to collect data. Our analysis resulted in the construction and validation of five models that represent the causes, effects, and strategies for five specific community smells. These models can help practitioners identify and address community smells within their organizations, while also providing valuable insights for future research on the socio-technical aspects of machine learning-enabled system communities."
A Responsible and Inclusive Technology Framework for Attending to Business-to-Business Contexts,"Becerra Sandoval, Juana Catalina and Figueredo de santana, Vagner and Berger, Sara and Thomas Quigley, Lauren and Hobson, Stacy",10.1145/3712310,2025,"Technology development practices in industry are often primarily focused on business results, which risks creating unbalanced power relations between corporate interests and the needs or concerns of people who are affected by technology implementation and use. These practices, and their associated cultural norms, may result in uses of technology that have direct, indirect, short-term, and even long-term negative effects on groups of people and/or the environment. This is especially critical in B2B (business-to-business) settings due to the potential for responsibility gaps to emerge in such contexts where technologies are sold to one or more third-party company obfuscating downstream impacts. This article contributes a formative framework—the Responsible and Inclusive Technology Framework—that orients critical reflection around the social contexts of technology creation and use; the power dynamics between self, business, and societal stakeholders; the impacts of technology on various communities across past, present, and future dimensions; and the practical decisions that imbue technological artifacts with cultural values. The framework and its components were iteratively developed based on observations of 10 internal exploratory workshops conducted with a total of 49 participants across the company. We expect that the use of the Responsible and Inclusive Technology framework, especially in B2B industry settings, will serve as a catalyst for more intentional and socially grounded practices, thus bridging the responsibility and principles-to-practice gap."
Research on WebAssembly Runtimes: A Survey,"Zhang, Yixuan and Liu, Mugeng and Wang, Haoyu and Ma, Yun and Huang, Gang and Liu, Xuanzhe",10.1145/3714465,2025,"WebAssembly (abbreviated as Wasm) was initially introduced for the Web and quickly extended its reach into various domains beyond the Web. To create Wasm applications, developers can compile high-level programming languages into Wasm binaries or manually write the textual format of Wasm and translate it into Wasm binaries by the toolchain. Regardless of whether it is utilized within or outside the Web, the execution of Wasm binaries is supported by the Wasm runtime. Such a runtime provides a secure, memory-efficient, and sandboxed execution environment to execute Wasm binaries. This paper provides a comprehensive survey of research on Wasm runtimes with 103 collected research papers related to Wasm runtimes following the traditional systematic literature review process. It characterizes existing studies from two different angles, including the internal research of Wasm runtimes (Wasm runtime design, testing, and analysis) and the external research (applying Wasm runtimes to various domains). This paper also proposes future research directions about Wasm runtimes."
Secured Network Architectures Based on Blockchain Technologies: A Systematic Review,"Kim, Song-Kyoo and Vong, Hou Cheng",10.1145/3715000,2025,"Blockchain applications have emerged in recent decades, among which blockchain secured-networks serve as a prevalent application. This article discusses the potential of networks secured by blockchain technology to enhance various domains and provides a structured view of the current landscape of blockchain applications, capturing the practical applications and potential of blockchain technology. Following a background overview, a comprehensive survey analysis of the latest advancements in blockchain-secured networks is presented, comprising six application fields: Vehicular Ad-Hoc, Health Care, Smart Home, Unmanned Aerial Vehicle (UAV), Internet of Things (IoT), and Industrial IoT Networks. An in-depth discussion of these key research topics within blockchain-secured networks is provided, enhancing understanding of their application and influence across multiple disciplines. The study overall conveys the versatility of blockchain-secured networks, highlighting their immense potential for application in various practical fields. By providing a comprehensive understanding of the current trends in blockchain application, this articles enables readers to navigate information effectively and identify key areas of interest, facilitating further exploration and research opportunities in the field of blockchain technology."
The Future of AI-Driven Software Engineering,"Terragni, Valerio and Vella, Annie and Roop, Partha and Blincoe, Kelly",10.1145/3715003,2025,"A paradigm shift is underway in Software Engineering, with AI systems such as LLMs playing an increasingly important role in boosting software development productivity. This trend is anticipated to persist. In the next years, we expect a growing symbiotic partnership between human software developers and AI. The Software Engineering research community cannot afford to overlook this trend; we must address the key research challenges posed by the integration of AI into the software development process. In this paper, we present our vision of the future of software development in an AI-driven world and explore the key challenges that our research community should address to realize this vision."
"The Good, the Bad, and the Monstrous: Predicting Highly Change-Prone Source Code Methods at Their Inception","Chowdhury, Shaiful",10.1145/3715006,2025,"The cost of software maintenance often surpasses the initial development expenses, making it a significant concern for the software industry. A key strategy for alleviating future maintenance burdens is the early prediction and identification of change-prone code components, which allows for timely optimizations. While prior research has largely concentrated on predicting change-prone files and classes—an approach less favored by practitioners—this paper shifts focus to predicting highly change-prone methods, aligning with the preferences of both practitioners and researchers. We analyzed 774,051 source code methods from 49 prominent open-source Java projects. Our findings reveal that approximately 80% of changes are concentrated in just 20% of the methods, demonstrating the Pareto 80/20 principle. Moreover, this subset of methods is responsible for the majority of the identified bugs in these projects. After establishing their critical role in mitigating software maintenance costs, our study shows that machine learning models can effectively identify these highly change-prone methods from their inception. Additionally, we conducted a thorough manual analysis to uncover common patterns (or concepts) among the more difficult-to-predict methods. These insights can help future research develop new features and enhance prediction accuracy."
An Empirical Study on Challenges for LLM Application Developers,"Chen, Xiang and Gao, Chaoyang and Chen, Chunyang and Zhang, Guangbei and Liu, Yong",10.1145/3715007,2025,"In recent years, large language models (LLMs) have seen rapid advancements, significantly impacting various fields such as computer vision, natural language processing, and software engineering. These LLMs, exemplified by OpenAI’s ChatGPT, have revolutionized the way we approach language understanding and generation tasks. However, in contrast to traditional software development practices, LLM development introduces new challenges for AI developers in design, implementation, and deployment. These challenges span different areas (such as prompts, APIs, and plugins), requiring developers to navigate unique methodologies and considerations specific to LLM application development.Despite the profound influence of LLMs, to the best of our knowledge, these challenges have not been thoroughly investigated in previous empirical studies. To fill this gap, we present the first comprehensive study on understanding the challenges faced by LLM developers. Specifically, we crawl and analyze 29,057 relevant questions from a popular OpenAI developer forum. We first examine their popularity and difficulty. After manually analyzing 2,364 sampled questions, we construct a taxonomy of challenges faced by LLM developers. Based on this taxonomy, we summarize a set of findings and actionable implications for LLM-related stakeholders, including developers and providers (especially the OpenAI organization)."
Software Engineering by and for Humans in an AI Era,"Abrah\~{a}o, Silvia and Grundy, John and Pezz\`{e}, Mauro and Storey, Margaret-Anne and Andrew Tamburri, Damian",10.1145/3715111,2025,"The landscape of software engineering is undergoing a transformative shift driven by advancements in machine learning, artificial intelligence (AI), and autonomous systems. This roadmap paper explores how these technologies are reshaping the field, positioning humans not only as end users but also as critical components within expansive software ecosystems. We examine the challenges and opportunities arising from this human-centered paradigm, including ethical considerations, fairness, and the intricate interplay between technical and human factors. By recognizing humans at the heart of the software lifecycle —spanning professional engineers, end users, and end-user developers —we emphasize the importance of inclusivity, human-aligned workflows, and the seamless integration of AI-augmented socio-technical systems. As software systems evolve to become more intelligent and human-centric, software engineering practices must adapt to this new reality. This paper provides a comprehensive examination of this transformation, outlining current trends, key challenges, and opportunities that define the emerging research and practice landscape, and envisioning a future where software engineering and AI work synergistically to place humans at the core of the ecosystem."
Conversational Voice User Interfaces Supporting Individuals with Down Syndrome: A Literature Review,"Cibrian, Franceli L. and Valdez, Concepci\'{o}n and Min, Lauren and Genaro Motti, Vivian",10.1145/3715160,2025,"Conversational Voice User Interfaces (CVUIs) are widely used in commercial applications such as personal assistants. CVUIs are beneficial for most users as they enable interaction through speech and natural language. However, recent studies indicate that underrepresented user groups, such as individuals with speech impairments and specifically those with Down syndrome, face challenges in using voice commands to control CVUIs. The anatomical and physiological differences affecting the voice, speech, fluency, and prosody of users with Down syndrome hinder their experience with CVUIs. This article presents the results of 43 papers related to the use of voice user interfaces supporting individuals with Down syndrome, showing that (1) the most used form factor for voice assistants are displays with or without speakers; (2) although most of the papers related to speech-to-text technologies are about datasets aimed at supporting speech recognition, most of them are created involving a small sample size; and (3) there is high interest in including individuals with Down syndrome in evaluation phases mainly to address their communication skills, therapy, and general assistance. We discuss the design and evaluation stages, as well as how to make CVUIs more accessible and inclusive for individuals with Down syndrome in particular and neurodiverse users in general."
Beyond Dependencies: The Role of Copy-Based Reuse in Open Source Software Development,"Jahanshahi, Mahmoud and Reid, David and Mockus, Audris",10.1145/3715907,2025,"In Open Source Software, resources of any project are open for reuse by introducing dependencies or copying the resource itself. In contrast to dependency-based reuse, the infrastructure to systematically support copy-based reuse appears to be entirely missing. Our aim is to enable future research and tool development to increase efficiency and reduce the risks of copy-based reuse. We seek a better understanding of such reuse by measuring its prevalence and identifying factors affecting the propensity to reuse. To identify reused artifacts and trace their origins, our method exploits World of Code infrastructure. We begin with a set of theory-derived factors related to the propensity to reuse, sample instances of different reuse types, and survey developers to better understand their intentions. Our results indicate that copy-based reuse is common, with many developers being aware of it when writing code. The propensity for a file to be reused varies greatly among languages and between source code and binary files, consistently decreasing over time. Files introduced by popular projects are more likely to be reused, but at least half of reused resources originate from “small” and “medium” projects. Developers had various reasons for reuse but were generally positive about using a package manager."
Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning,"Susnjak, Teo and Hwang, Peter and Reyes, Napoleon and Barczak, Andre L. C. and McIntosh, Timothy and Ranathunga, Surangika",10.1145/3715964,2025,"This research pioneers the use of fine-tuned Large Language Models (LLMs) to automate Systematic Literature Reviews (SLRs), presenting a significant and novel contribution in integrating AI to enhance academic research methodologies. Our study employed advanced fine-tuning methodologies on open sourced LLMs, applying textual data mining techniques to automate the knowledge discovery and synthesis phases of an SLR process, thus demonstrating a practical and efficient approach for extracting and analyzing high-quality information from large academic datasets. The results maintained high fidelity in factual accuracy in LLM responses, and were validated through the replication of an existing PRISMA-conforming SLR. Our research proposed solutions for mitigating LLM hallucination and proposed mechanisms for tracking LLM responses to their sources of information, thus demonstrating how this approach can meet the rigorous demands of scholarly research. The findings ultimately confirmed the potential of fine-tuned LLMs in streamlining various labor-intensive processes of conducting literature reviews. As a scalable proof-of-concept, this study highlights the broad applicability of our approach across multiple research domains. The potential demonstrated here advocates for updates to PRISMA reporting guidelines, incorporating AI-driven processes to ensure methodological transparency and reliability in future SLRs. This study broadens the appeal of AI-enhanced tools across various academic and research fields, demonstrating how to conduct comprehensive and accurate literature reviews with more efficiency in the face of ever-increasing volumes of academic studies while maintaining high standards."
The Effects of Information and Incentive Interventions on the Adoption of Algorithms in Human Resources: An Experimental Study,"Ochmann, Jessica and Zilker, Sandra and Michels, Leonard and Tiefenbeck, Verena and Matzner, Martin and Laumer, Sven",10.1145/3715966.3715970,2025,"The economic potential of human resources (HR) algorithms requires organizations to ensure their adoption, but HR employees are often reluctant to use them. One driver of this reluctance may be employees' overconfidence in their own judgments and the sensitive nature of HR decisions, which directly affect human beings. Interventions that aim to increase technology adoption can help organizations overcome this issue. Drawing on the theoretical lens of information and incentive interventions, we conducted a survey-based incentive-compatible online experiment with 276 participants to determine the effects of interventions on the adoption of algorithms in an organizational HR context. We distinguish between adoption in the short and long runs. Our study, which contributes to research on the use of algorithms in HR and induced technology adoption, has practical implications for HR leaders who seek to increase HR employees' adoption of algorithms in completing HR-related tasks."
"HCI, Disability, and Sport: A Literature Review","Strobel, Lukas and Gerling, Kathrin",10.1145/3716136,2025,"Equitable access to sport for disabled people remains challenging, and technology is often viewed as a way of addressing barriers. However, little is known about how disability is approached in such research and the purpose of sport that is afforded to disabled people. We address this issue in a review of 60 publications in the field of Human-Computer Interaction. We leverage Template Analysis in combination with Mueller and Young’s lenses on virtues of sport to also explore the experiential side of sports technology for disabled people. Our results are threefold: (1) We show that disability shifts the intended purpose of sports technology away from leisure to health, and that technologies such as exergames are viewed as an opportunity to replace real-world sport to address barriers and increase motivation. (2) We highlight that in(ter)dependence plays a strong role in technology development, but that disabled people are not extensively involved in research. (3) We show that virtues beyond health as per Mueller and Young do apply to existing work, but that value frameworks need to be re-worked in the context of disability, placing a stronger emphasis on sport as leisure, and the enriching role that technology can play."
Deep Learning Based Image Aesthetic Quality Assessment- A Review,"Daryanavard Chounchenani, Maedeh and Shahbahrami, Asadollah and Hassanpour, Reza and Gaydadjiev, Georgi",10.1145/3716820,2025,"Image Aesthetic Quality Assessment (IAQA) spans applications such as the fashion industry, AI-generated content, product design, and e-commerce. Recent deep learning advancements have been employed to evaluate image aesthetic quality. A few surveys have been conducted on IAQA models; however, details of recent deep learning models and challenges have not been fully mentioned. This article aims to fill these gaps by providing a review of deep learning IAQA over the past decade, based on input, process, and output phases. Methodologies for deep learning–based IAQA can be categorized into general and task-specific approaches, depending on the type and diversity of input images. The processing phase involves considerations related to network architecture, learning structures, and feature extraction methods. The output phase generates results such as scoring, distribution, attributes, and description. Despite achieving a maximum accuracy of 91.5%, further improvements in deep learning models are still required. Our study highlights several challenges, including adapting models for task-specific methodology, accounting for environmental factors influencing aesthetics, the lack of substantial datasets with appropriate labels, imbalanced data, preserving image aspect ratio and integrity in network architecture design, and the need for explainable AI to understand the causative factors behind aesthetic judgments."
An empirical study on vulnerability disclosure management of open source software systems,"Liu, Shuhan and Zhou, Jiayuan and Hu, Xing and Cogo, Filipe Roseiro and Xia, Xin and Yang, Xiaohu",10.1145/3716822,2025,"Vulnerability disclosure is critical for ensuring the security and reliability of open source software (OSS). However, in practice, many vulnerabilities are reported and discussed on public platforms before being formally disclosed, posing significant risks to vulnerability management. Inadequate vulnerability disclosure can expose users to security threats and severely impact the stability and reliability of software systems. For example, prior work shows that over 21% of CVEs are publicly discussed before a patch is released. Despite its importance, we still lack clarity on the vulnerability disclosure practices adopted by open source communities and the preferences of practitioners regarding vulnerability management. To fill this gap, we analyzed the vulnerability disclosure practices of 8,073 OSS projects spanning from 2017 to 2023. We then conducted an empirical study by surveying practitioners about their preferences and recommendations in vulnerability disclosure management. Finally, we compared the survey results with the actual vulnerability practice observed within the OSS projects. Our results show that while over 80% of practitioners support Coordinated Vulnerability Disclosure (CVD), only 55% of vulnerabilities conform to CVD in practice. Although only 20% of practitioners advocate discussions before disclosure, 42% of vulnerabilities are discussed in issue reports before their disclosure. This study reveals the vulnerability management practices in OSS, provides valuable guidance to OSS owners, and highlights potential directions to improve the security of OSS platforms."
ATE-FS: An Average Treatment Effect-Based Feature Selection Technique for Software Fault Prediction,"Mangal, Akshat and Rathore, Santosh Singh",10.1145/3716857,2025,"In software development, software fault prediction (SFP) models aim to identify code sections with a high likelihood of faults before the testing process. SFP models achieve this by analyzing data about the structural properties of the software’s previous versions. Consequently, the accuracy and interpretation of SFP models depend heavily on the chosen software metrics and how well they correlate with patterns of fault occurrence. Previous research has explored improving SFP model performance through feature selection (metric selection). Yet inconsistencies in conclusions arose due to the presence of inconsistent and correlated software metrics. Relying solely on correlations between metrics and faults makes it difficult for developers to take actionable steps, as the causal relationships remain unclear. To address this challenge, this work investigates the use of Causal Inference (CI) methods to understand the causal relationships between software project characteristics, development practices, and the fault-proneness of code sections. We propose a CI-based technique called Average Treatment Effect for Feature Selection (ATE-FS). This technique leverages the CI concept to quantify the cause-and-effect relationships between software metrics and fault-proneness. ATE-FS utilizes Average Treatment Effect (ATE) features to identify code metrics that are most suitable for building SFP models. These ATE features capture the causal impact of a metric on fault-proneness. Through an experimental analysis involving twenty-seven SFP datasets, we validate the performance of ATE-FS. We further compare its performance with other state-of-the-art feature selection techniques. The results demonstrate that ATE-FS achieves a significant performance for fault prediction. Additionally, ATE-FS improved consistency in feature selection across diverse SFP datasets."
Modeling Infectious Disease Epidemics in Mass Religious Gatherings: A Systematic Review,"Alshammari, Sultanah and Ba-Aoum, Mohammed and Alganmi, Nofe and Showail, Ahmad",10.1145/3716869,2025,"Like other global mass gatherings, religious pilgrimages, such as Hajj, Arba’een, and the Hindu festival Kumbh Mela, attract millions of pilgrims to gather at specific holy sites on specific dates. During disease pandemics, mass gatherings can become super spreader events, causing exponential growth of infections in multiple regions. Epidemic modeling approaches can be valuable tools for studying the impact of mass gatherings on global health during disease outbreaks. To assess the use of epidemic models at religious pilgrimages, we compile published studies that proposed epidemic models at mass religious gatherings. A review of existing epidemic models at various religious gatherings highlights the role of epidemic modeling approaches in assessing the implications of religious pilgrimages on disease pandemics. All the articles surveyed showed a link between hosting religious gatherings and an increase in the number of cases of the simulated epidemic. In addition, we found that the SEIR mathematical model was the most common type developed with variations in some of the retrieved papers. The results reported in these studies motivate further investigation of the role of epidemic modeling and simulation in estimating the size and geographic scale of infections while hosting religious gatherings. Finally, we believe that this survey article draws attention to the application of epidemic models in the advanced planning of recurrent religious pilgrimages, as it is not feasible to cancel, suspend, or reallocate these pilgrimages. These epidemic models can provide a baseline for policymakers to determine which control measures should be implemented and when."
Don't Touch the Power Line - A Proof-of-Concept for Aligned LLM-Based Assistance Systems to Support the Maintenance in the Electricity Distribution System,"Kaltenpoth, Sascha and M\""{u}ller, Oliver",10.1145/3717413.3717415,2025,"As a possible solution to the demographic change and the resulting knowledge loss due to retirements in the Energy sector, this study aimed to develop a generic pipeline to implement and evaluate proof-of-concepts (PoCs) for LLM-based assistance systems in new domains. Our pipeline contains an LLM-based data generation strategy based on documents, a retrieval-augmented generation (RAG) architecture utilizing prompting techniques on existing German LLMs, and an LLM-based automatic evaluation strategy. We leverage our pipeline to evaluate five LLMs using data from a German DSO. We found that the Llama3 and the Mistral model are appropriately aligned for the task. We plan to pilot the RAG architecture in the DSO's infrastructure for future research and continuously research improvements using the generated human demonstrations."
Generative AI in Fashion: Overview,"Shi, Wenda and Wong, Waikeung and Zou, Xingxing",10.1145/3718098,2025,"Generative Artificial Intelligence (GenAI) has recently gained immense popularity by offering various applications for generating high-quality and aesthetically pleasing content of image, 3D, and video data format. The innovative GenAI solutions have shifted paradigms across various design-related industries, particularly fashion. In this paper, we explore the incorporation of GenAI into fashion-related tasks and applications. Our examination encompasses a thorough review of more than 470 research papers and an in-depth analysis of over 300 applications, focusing on their contributions to the field. These contributions are identified as 13 tasks within four categories: multi-modal fashion understanding, and fashion synthesis of image, 3D, and dynamic (video and animatable 3D) formats We delve into these methods, recognizing their potential to propel future endeavours toward achieving state-of-the-art (SOTA) performance. Furthermore, we present a comprehensive overview of 53 publicly available datasets suitable for training and benchmarking fashion-centric models, accompanied by the relevant evaluation metrics. Finally, we review real-world applications, unveiling existing challenges and future directions. With comprehensive investigation and in-depth analysis, this paper is targeted to serve as a useful resource for understanding the current landscape of GenAI in fashion, paving the way for future innovations in this dynamic field. Papers discussed in this paper, along with public code and datasets links are available at: ."
Green Federated Learning: A New Era of Green Aware AI,"Thakur, Dipanwita and Guzzo, Antonella and Fortino, Giancarlo and Piccialli, Francesco",10.1145/3718363,2025,"The development of AI applications, especially in large-scale wireless networks, is growing exponentially, alongside the size and complexity of the architectures used. Particularly, machine learning is acknowledged as one of today’s most energy-intensive computational applications, posing a significant challenge to the environmental sustainability of next-generation intelligent systems. Achieving environmental sustainability entails ensuring that every AI algorithm is designed with sustainability in mind, integrating green considerations from the architectural phase onwards. Recently, Federated Learning (FL), with its distributed nature, presents new opportunities to address this need. Hence, it is imperative to elucidate the potential and challenges stemming from recent FL advancements and their implications for sustainability. Moreover, it is crucial to furnish researchers, stakeholders, and interested parties with a roadmap to navigate and understand existing efforts and gaps in green-aware AI algorithms. This survey primarily aims to achieve this objective by identifying and analyzing over a hundred FL works and assessing their contributions to green-aware artificial intelligence for sustainable environments, with a specific focus on IoT research. It delves into current issues in green federated learning from an energy-efficient standpoint, discussing potential challenges and future prospects for green IoT application research."
Artificial Intelligence for Software Engineering: The Journey so far and the Road ahead,"Ahmed, Iftekhar and Aleti, Aldeida and Cai, Haipeng and Chatzigeorgiou, Alexander and He, Pinjia and Hu, Xing and Pezz\`{e}, Mauro and Poshyvanyk, Denys and Xia, Xin",10.1145/3719006,2025,"Artificial intelligence and recent advances in deep learning architectures, including transformer networks and large language models, change the way people think and act to solve problems. Software engineering, as an increasingly complex process to design, develop, test, deploy, and maintain large-scale software systems for solving real-world challenges, is profoundly affected by many revolutionary artificial intelligence tools in general, and machine learning in particular. In this roadmap for artificial intelligence in software engineering, we highlight the recent deep impact of artificial intelligence on software engineering by discussing successful stories of applications of artificial intelligence to classic and new software development challenges. We identify the new challenges that the software engineering community has to address in the coming years to successfully apply artificial intelligence in software engineering, and we share our research roadmap towards the effective use of artificial intelligence in the software engineering profession, while still protecting fundamental human values.We spotlight three main areas that challenge the research in software engineering: the use of generative artificial intelligence and large language models for engineering large software systems, the need of large and unbiased datasets and benchmarks for training and evaluating deep learning and large language models for software engineering, and the need of a new code of digital ethics to apply artificial intelligence in software engineering."
"Adopting Emerging Technologies in Digital Government: A Multi-Case Analysis of Drivers, Enablers, and Challenges in Saudi Arabia","Alshahrani, Abdulaziz",10.1145/3719297,2025,"The adoption of emerging technologies is reshaping government operations. These technologies enhance efficiency and service delivery. This study explores the key drivers, enablers, and challenges influencing the adoption of these technologies in six government sectors. A qualitative multi-case study methodology was employed, combining interviews with project managers and a thematic analysis of project reports, government publications, and policy documents. The key findings highlight strategic alignment with Saudi Vision 2030 goals, operational efficiency demands, and enhanced accessibility as primary adoption drivers. Regulatory frameworks, leadership commitment, and infrastructure readiness emerged as critical enablers. Conversely, barriers to adoption include technical complexities in legacy system integration, interoperability challenges, financial limitations, and organizational resistance to change. This research contributes to the discourse on digital transformation in the government sector by identifying context-specific success factors and systemic obstacles. It proposes evidence-based recommendations for policymakers, emphasizing the need for adaptive governance models, phased infrastructure modernization, and stakeholder capacity-building initiatives. The contribution aims to inform sustainable technology adoption strategies in alignment with national development agendas."
QED in Context: An Observation Study of Proof Assistant Users,"Shi, Jessica and Torczon, Cassia and Goldstein, Harrison and Pierce, Benjamin C. and Head, Andrew",10.1145/3720426,2025,"Interactive theorem provers, or proof assistants, are important tools across many areas of computer science and mathematics, but even experts find them challenging to use effectively. To improve their design, we need a deeper, user-centric understanding of proof assistant usage.
 
 
 
We present the results of an observation study of proof assistant users. We use contextual inquiry methodology, observing 30 participants doing their everyday work in Rocq and Lean. We qualitatively analyze their experiences to surface four observations: that proof writers iterate on their proofs by reacting to and incorporating feedback from the proof assistant; that proof progress often involves challenging conversations with the proof assistant; that proofs are constructed in consultation with a wide array of external resources; and that proof writers are guided by design considerations that go beyond ""getting to QED."" Our documentation of these themes clarifies what proof assistant usage looks like currently and identifies potential opportunities that researchers should consider when working to improve the usability of proof assistants."
Unraveling Code Clone Dynamics in Deep Learning Frameworks,"Assi, Maram and Hassan, Safwat and Zou, Ying",10.1145/3721125,2025,"Deep Learning (DL) frameworks play a critical role in advancing artificial intelligence, and their rapid growth underscores the need for a comprehensive understanding of software quality and maintainability. DL frameworks, like other systems, are prone to code clones. Code clones refer to identical or highly similar source code fragments within the same project or even across different projects. Code cloning can have positive and negative implications for software development, influencing maintenance, readability, and bug propagation. While the existing studies focus on studying clones in DL-based applications, to our knowledge, no work has been done investigating clones, their evolution and their impact on the maintenance of DL frameworks. In this paper, we aim to address the knowledge gap concerning the evolutionary dimension of code clones in DL frameworks and the extent of code reuse across these frameworks. We empirically analyze code clones in nine popular DL frameworks, i.e., TensorFlow, Paddle, PyTorch, Aesara, Ray, MXNet, Keras, Jax and BentoML, to investigate (1) the characteristics of the long-term code cloning evolution over releases in each framework, (2) the short-term, i.e., within-release, code cloning patterns and their influence on the long-term trends, and (3) the file-level code clones within the DL frameworks. Our findings reveal that DL frameworks adopt four distinct cloning trends: “Serpentine”, “Rise and Fall”, “Decreasing”, and “Stable” and that these trends present some common and distinct characteristics. For instance, bug-fixing activities persistently happen in clones irrespective of the clone evolutionary trend but occur more in the “Serpentine” trend. Moreover, the within-release level investigation demonstrates that short-term code cloning practices impact long-term cloning trends. The cross-framework code clone investigation reveals the presence of functional and architectural adaptation file-level cross-framework code clones across the nine studied frameworks. We provide insights that foster robust clone practices and collaborative maintenance in the development of DL frameworks."
A Scoping Review of Informed Consent Practices in Human-Computer Interaction Research,"Schwind, Valentin and Tadesse, Netsanet Zelalem and Silva da Cunha, Estefania and Hamidi, Yeganeh and Sultani, Soltan Sanjar and Sehrt, Jessica",10.1145/3721284,2025,"Obtaining informed consent is a fundamental ethical requirement in research involving human subjects, designed to ensure autonomy, respect, and the protection of participants. However, new technologies and research methodologies in Human-Computer Interaction (HCI) present unique challenges in maintaining ethical standards and require a deeper understanding of how consent practices are implemented. This scoping review provides a detailed examination of the frameworks, methodologies, and practices for obtaining informed consent in HCI. We present recognized universal principles while also discussing the ethical guidelines, and legal frameworks that underpin consent processes. Furthermore, we analyze the practices of disclosure, screening, consenting, and confirmation to assess how researchers ensure voluntary participation. The review addresses key criticisms and challenges identified in the literature, suggesting improvements and exploring alternative approaches. By offering comprehensive insights into the complexities of informed consent, we underscore the ongoing need for ethical awareness and continuous refinement of ethical conduct in HCI."
Deception in HRI and Its Implications: A Systematic Review,"Esposito, Raffaella and Rossi, Alessandra and Rossi, Silvia",10.1145/3721297,2025,"Background. People commonly use deception to gain advantages for themselves and their significant ones, such as with children, for educational purposes, or for protecting someone else feelings. As robots increasingly are being used in various human-centered environments, experts in robotics and social sciences are trying to adapt similar deceptive techniques to social robots, such as in assistive and service applications. However, robots’ ability to engage in deceptive behaviors presents both potential benefits and significant ethical challenges. In this work, we present a systematic review to synthesize current research on the implementation of deceptive robotic behaviors during Human–Robot Interactions (HRI), and its effects on people.Methods. Adopting a comprehensive and flexible methodological approach, we systematically searched Scopus and Web of Science without restricting the publication date. The review focused on studies that explicitly examined the effects of robotic deception on human participants, covering a broad spectrum of methodologies, populations, and outcomes.Results. A total of 16 studies met the inclusion criteria, showing that robotic deception in HRI leads to diverse emotional, cognitive, and behavioral responses. The findings indicate that robotic deception can have diverse impacts, ranging from eroding trust to enhancing engagement and performance under certain conditions.Conclusions. Our systematic review highlights the importance of careful design and management in robotic systems to harness the benefits of deception while mitigating its negative impacts on trust. We advise that future research should explore conditions under which deception may be beneficial and develop strategies to effectively manage its use in HRI."
Better Supporting Human Aspects in Mobile eHealth Apps: Development and Validation of Enhanced Guidelines,"Shamsujjoha, Md and Grundy, John and Lu, Qinghua and Khalajzadeh, Hourieh and Li, Li",10.1145/3721429,2025,"eHealth apps are mobile apps that help in self-management of critical illnesses, provide home-based disease management, and assist with personalized care through education, sensing, and interaction. Users of eHealth apps are naturally very diverse in terms of their human aspects, e.g., their emotional reactions to the apps, varying language proficiency, socioeconomic status, educational level, cognitive style, physical and mental challenges, gender, age, personality, etc. Unfortunately, many eHealth apps do not take these user differences sufficiently into account, making them ineffective or even unusable. This paper presents our enhanced and actionable guidelines developed to better support human aspects in mobile eHealth apps. Some of these guidelines are specific, such as collecting minimal personal data or requirements, while others are more generic, applicable specifically to eHealth apps. We discuss how key human aspects, such as usability, accessibility, reliability, and validity, as well as diverse user issues can be addressed in practice with real-life eHealth app examples. We then collected feedback from expert mobile app developers, software engineers, and other relevant eHealth app stakeholders to assess the usefulness and applicability of the proposed guidelines and to identify areas where further refinement and development are needed."
Attacks and Defenses for Generative Diffusion Models: A Comprehensive Survey,"Truong, Vu Tuan and Dang, Luan Ba and Le, Long Bao",10.1145/3721479,2025,"Diffusion models (DMs) have achieved state-of-the-art performance on various generative tasks such as image synthesis, text-to-image, and text-guided image-to-image generation. However, the more powerful the DMs, the more harmful they can potentially be. Recent studies have shown that DMs are prone to a wide range of attacks, including adversarial attacks, membership inference attacks, backdoor injection, and various multi-modal threats. Since numerous pre-trained DMs are published widely on the Internet, potential threats from these attacks are especially detrimental to society, making DM-related security a topic worthy of investigation. Therefore, in this article, we conduct a comprehensive survey on the security aspect of DMs, focusing on various attack and defense methods for DMs. First, we present crucial knowledge of DMs with five main types of DMs, including denoising diffusion probabilistic models, denoising diffusion implicit models, noise conditioned score networks, stochastic differential equations, and multi-modal conditional DMs. We provide a comprehensive survey of recent works investigating different types of attacks that exploit the vulnerabilities of DMs. Then, we thoroughly review potential countermeasures to mitigate each of the presented threats. Finally, we discuss open challenges of DM-related security and describe potential research directions for this topic."
A Survey of Source Code Representations for Machine Learning-Based Cybersecurity Tasks,"Casey, Beatrice and Santos, Joanna C. S. and Perry, George",10.1145/3721977,2025,"Machine learning techniques for cybersecurity-related software engineering tasks are becoming increasingly popular. The representation of source code is a key portion of the technique that can impact the way the model is able to learn the features of the source code. With an increasing number of these techniques being developed, it is valuable to see the current state of the field to better understand what exists and what is not there yet. This article presents a study of these existing machine learning based approaches and demonstrates what type of representations were used for different cybersecurity tasks and programming languages. Additionally, we study what types of models are used with different representations. We have found that graph-based representations are the most popular category of representation, and tokenizers and Abstract Syntax Trees (ASTs) are the two most popular representations overall (e.g., AST and tokenizers are the representations with the highest count of papers, whereas graph-based representations is the category with the highest count of papers). We also found that the most popular cybersecurity task is vulnerability detection, and the language that is covered by the most techniques is C. Finally, we found that sequence-based models are the most popular category of models, and Support Vector Machines are the most popular model overall."
The Role of Social Norms in Human-Robot Interaction: A Systematic Review,"Lawrence, Steven and Jouaiti, Melanie and Hoey, Jesse and Nehaniv, Chrystopher L. and Dautenhahn, Kerstin",10.1145/3722120,2025,"As robots integrate more into daily life, socially-aware robots with specific social attributes and behaviours are necessary. This review aims to explore how social norms in human-robot interaction (HRI) impact robot design and human perception. We searched for relevant articles in the following databases, ACM Digital Library, IEEE Digital Library, Scopus, Springer Link, and PsycINFO. After applying inclusion and exclusion criteria, a final set of 69 articles were included in the review. These articles were categorized based on whether they examined norm conformity or norm violations, and were further sorted into 12 categorical norm labels to assist in analysis and comparison. By examining existing literature, this review uncovers how social norms impact aspects of human-robot interactions like trust, acceptance, and comfort while highlighting the importance of aligning robot design with user expectations. It reveals design challenges such as accounting for cultural variations, context-specific norms, and evolving norms over time. Addressing these challenges has the potential to improve user experiences, promote broader acceptance of robots, and foster successful integration of robots into various domains. The findings contribute to the ongoing discussion on the role of social norms in HRI, offering valuable insights and a foundation for future research."
Nurturing Code Quality: Leveraging Static Analysis and Large Language Models for Software Quality in Education,"AlOmar, Eman Abdullah",10.1145/3722229,2025,"Large Language Models (LLMs), such as ChatGPT, have become widely popular for various software engineering tasks, including programming, testing, code review, and program comprehension. However, their impact on improving software quality in educational settings remains uncertain. This article explores our experience teaching the use of Programming Mistake Detector (PMD) to foster a culture of bug fixing and leverage LLM to improve software quality in the classroom. This article discusses the results of an experiment involving 155 submissions that carried out a code review activity of 1,658 rules. Our quantitative and qualitative analyses reveal that a set of PMD quality issues influences the acceptance or rejection of the issues, and design-related categories that take longer to resolve. Although students acknowledge the potential of using ChatGPT during code review, some skepticism persists. Further, constructing prompts for ChatGPT that possess clarity, complexity, and context nurtures vital learning outcomes, such as enhanced critical thinking, and among the 1,658 issues analyzed, 93% of students indicated that ChatGPT did not identify any additional issues beyond those detected by PMD. Conversations between students and ChatGPT encompass five categories, including ChatGPT’s use of affirmation phrases like “certainly” regarding bug fixing decisions, and apology phrases such as “apologize” when resolving challenges. Through this experiment, we demonstrate that code review can become an integral part of the educational computing curriculum. We envision our findings to enable educators to support students with effective code review strategies, increasing awareness of LLMs, and promoting software quality in education."
"Secure Robotics: Navigating Challenges at the Nexus of Safety, Trust, and Cybersecurity in Cyber-Physical Systems","Haskard, Adam and Herath, Damith",10.1145/3723050,2025,"The growing pervasiveness of robotic and embodied artificial intelligence systems in daily life and within cyber-physical environments highlights a complex web of challenges at the intersection of robotic safety, human-to-robot trust, and cybersecurity. This article explores these challenges by emphasising the crucial role of security in establishing and maintaining trust between humans and robots, which is integral to successfully adopting and operating these systems in human environments. Safety considerations include mitigating the risks of physical harm and environmental damage due to robotic malfunctions or cyberattacks, particularly in autonomous robots requiring high built-in safety measures. From a cybersecurity perspective, these systems face unique challenges due to their complex, interconnected software and hardware components that necessitate robust protection against data breaches to ensure secure data communication. Additionally, the dynamic interaction of these systems with the physical environment adds a layer of complexity, which makes the safety, security, and reliability of these interactions a vital component of the overall security strategy. This article reviews these areas within the cyber-physical systems paradigm by focusing on engineering fail-safe mechanisms, the importance of trust and ethical responsibility in human-robot interactions, and the need for resilient cybersecurity measures. At this nexus, a table of crossover challenges illustrates the intricacy of integrating safety, trust, and security in robotic systems. This article introduces “secure robotics” as a new paradigm to address these collective challenges with a novel model to provide a structured methodology for evaluating and enhancing robotic system performance that symbolises the convergence of theoretical constructs with empirical analysis. By defining secure robotics, this article establishes a framework for advancing robotics in the cyber-physical era in alignment with current technological trends while anticipating future developments. This framework positions secure robotics as a key contributor to the evolution of cyber-physical systems."
UAV Operations Safety Assessment: A Systematic Literature Review,"Asghari, Omid and Ivaki, Naghmeh and Madeira, Henrique",10.1145/3723871,2025,"The significant increase in urban Unmanned Aerial Vehicles (UAVs), due to their benefits and commercial potential, will increase drone density and collision risks. To manage this, Unmanned Aircraft Systems Traffic Management (UTM), European implementation of UTM (U-space), and Air Traffic Management (ATM) are being developed for safe integration with other air traffic. Nonetheless, thorough safety assessments remain essential for ensuring UAV operation safety. In this study, we conducted a two-phase systematic literature review. First, we analyzed existing reviews on UAV operation safety assessments. Second, we examined primary studies with the goal of identifying (i) safety assessment approaches, (ii) employed methods/techniques, (iii) defined and utilized safety metrics, (iv) common tools/simulators, and (v) stages of safety assessment addressed by each technique in the reviewed studies. As a result, we categorized safety assessment approaches into five groups: (1) model-based, (2) analytical-based, (3) data-driven, (4) experimental-based, and (5) hybrid approaches. We found that Monte Carlo simulation and Specific Operations Risk Assessment (SORA) are the most commonly used methods for safety assessment. We identified 42 metrics and classified them into four groups: (1) collision metrics, (2) performance metrics, (3) communication metrics, and (4) reliability metrics. Additionally, we identified 10 tools/simulators used for safety assessment. Finally, we observed that stage 5 (safety risk evaluation) of the safety assessment process is the most frequently covered in the studies reviewed."
The Digital Attention Heuristics: Supporting the User’s Attention by Design,"Monge Roffarello, Alberto and De Russis, Luigi and Lukoff, Kai",10.1145/3725215,2025,"The HCI research community has traditionally considered digital wellbeing an end-user responsibility, designing tools for digital self-control that support them to self-regulate their usage of apps and websites. Yet, these attempts are often ineffective in the long term, as many tech companies still adopt “attention-capture” designs that compromise users’ sense of agency and self-control. Taking a complementary perspective, this paper presents a set of eight heuristics to create user interfaces that preserve and respect user attention by design. The heuristics stem from a systematic literature review and are grounded in the three fundamental psychological needs defined by the self-determination theory, i.e., autonomy, competence, and relatedness. In addition to being informed by theory and research, each heuristic is accompanied by practical strategies and real-world examples, offering designers actionable guidelines to value people’s attention in user interfaces."
Making and Accessibility: A Systematic Literature Review on the Multilayered Dimensions of Accessible Making,"Sarwar, Saquib and Wilson, David",10.1145/3726530,2025,"In recent years, the making phenomenon has shown great potential in personal design and fabrication capabilities using modern fabrication tools (e.g., 3D printers, laser cutters, CNC machines), personal electronics (e.g., Arduino, Raspberry Pi, sensors), and related crafting techniques. With this trend of making, people have also engaged in developing a wide variety of assistive devices (e.g., prostheses, orthotics), adaptations (e.g., cane-hanger/cup-holder for wheelchairs), and support systems (e.g., tactile braille maps). While there are some successful projects, substantive gaps remain between current traction and overall potential in the field of making and accessibility. To better understand the relationship between people with disabilities and making and identify the types of ecosystems involved in these overlapping areas of research, we conducted a systematic literature review on design-related ACM conferences between January 2010 and December 2023. Our analysis highlights the concentration of research across diverse communities, adaptations for accessible making, and the utilization of maker tools for the development of assistive devices. We also highlight trending design, development, and evaluation methodologies adopted to support collaboration between stakeholders with mixed abilities. Based on the findings of this systematic literature review, we critically reflect on the gaps and provide recommendations for future researchers and practitioners in this growing field."
Understanding Research Themes and Interactions at Scale within Blind and Low-vision Research in ACM and IEEE,"Jeanneret Medina, Maximiliano and Thoo, Yong-Joon and Baudet, C\'{e}dric and Froehlich, Jon E. and Ruffieux, Nicolas and Lalanne, Denis",10.1145/3726531,2025,"This article extends our 2023 ASSETS paper, “A Large-Scale Mixed-Methods Analysis of Blind and Low-vision Research in ACM and IEEE,” which provided a field-, technology-, and method-agnostic examination of blind and low-vision (BLV) research. Our mixed-methods approach combined quantitative bibliometric analyses with a qualitative analysis of the field, resulting in four high-level research areas. Building on this analysis, we further explore these areas by identifying and characterizing research themes and examining how the notion of interaction has been used in BLV research through an analysis of co-located terms. Our results highlight the rich diversity, overlap, and complementarity among these themes while highlighting potential areas for interdisciplinary collaboration. Moreover, our investigation into the terms co-located with interaction reveals a predominant focus on the modalities, technologies, and actions involved in interaction, rather than on the qualities of interaction. Our paper extends our previous findings by providing: (1) a finer-grained delineation within and between research areas; (2) a better understanding of the notion of interaction within BLV research; (3) an analysis of the research methods used when developing interactive computing systems for BLV users; and (4) a comparative analysis of prior systematic literature reviews of BLV research and possibilities for future survey contributions in our field."
Survey on Leveraging Uncertainty Estimation Towards Trustworthy Deep Neural Networks: The Case of Reject Option and Post-training Processing,"Hasan, Md Mehedi and Abdar, Moloud and Khosravi, Abbas and Aickelin, Uwe and Lio, Pietro and Hossain, Ibrahim and Rahman, Ashikur and Nahavandi, Saeid",10.1145/3727633,2025,"Although neural networks (especially deep neural networks) have achieved better-than-human performance in many fields, their real-world deployment is still questionable due to the lack of awareness about the limitations in their knowledge. To incorporate such awareness in the machine learning model, prediction with reject option (also known as selective classification or classification with abstention) has been proposed in the literature. In this article, we present a systematic review of the prediction with the reject option in the context of various neural networks. To the best of our knowledge, this is the first study focusing on this aspect of neural networks. Moreover, we discuss different novel loss functions related to the reject option and post-training processing (if any) of network output for generating suitable measurements for knowledge awareness of the model. Finally, we address the application of the rejection option in reducing the prediction time for real-time problems and present a comprehensive summary of the techniques related to the reject option in the context of a wide variety of neural networks. Our code is available on GitHub: ."
"A Systematic Literature Review of Robust Federated Learning: Issues, Solutions, and Future Research Directions","Uddin, Md Palash and Xiang, Yong and Hasan, Mahmudul and Bai, Jun and Zhao, Yao and Gao, Longxiang",10.1145/3727643,2025,"Federated Learning (FL) has emerged as a promising paradigm for training machine learning models across distributed devices while preserving their data privacy. However, the robustness of FL models against adversarial data and model attacks, noisy updates, and label-flipped data issues remain a critical concern. In this article, we present a systematic literature review using the PRISMA framework to comprehensively analyze existing research on robust FL. Through a rigorous selection process using six key databases (ACM Digital Library, IEEE Xplore, ScienceDirect, Springer, Web of Science, and Scopus), we identify and categorize 244 studies into eight themes of ensuring robustness in FL: objective regularization, optimizer modification, differential privacy employment, additional dataset requirement and decentralization orchestration, manifold, client selection, new aggregation algorithms, and aggregation hyperparameter tuning. We synthesize the findings from these themes, highlighting the various approaches and their potential gaps proposed to enhance the robustness of FL models. Furthermore, we discuss future research directions, focusing on the potential of hybrid approaches, ensemble techniques, and adaptive mechanisms for addressing the challenges associated with robust FL. This review not only provides a comprehensive overview of the state-of-the-art in robust FL but also serves as a roadmap for researchers and practitioners seeking to advance the field and develop more robust and resilient FL systems."
Emerging technologies in smart libraries for visually impaired people: challenges and design considerations,"Kotis, Konstantinos and Angoura, Eleni and Lyngri, Eleni Ioanna",10.1145/3727965,2025,"Emerging technologies are transforming cultural spaces in a variety of ways, presenting opportunities and challenges. Autonomous robots, eXtended Reality, Artificial Intelligence, Digital Twins, and Internet of Things are only a few examples of such technologies, with accessibility and inclusivity of people to these technologies to be considered key challenges. In general, the use of emerging technologies in cultural spaces presents exciting opportunities for enhancing visitors’ experience and engaging new participants. However, it is important to also consider the inclusion ability of people with special needs and to ensure that these emerging technologies are used in an accessible-to-all and inclusive way. The aim of this paper is to review the state-of-the-art and current trends in approaches that use emerging technologies in the domain of smart libraries designed to include visually impaired people in a common innovative way for the whole community of visitors, discuss open issues and challenges identified in such a cultural environment/case, and propose a novel approach based on specific design considerations of the specific domain."
How Creative Practitioners Use Tools to Capture Ideas: A Cross-Domain Study,"Rosselli Del Turco, Emilia and Inie, Nanna and Hollan, James D. and Dalsgaard, Peter",10.1145/3727979,2025,"Creative practitioners rely on tools to capture and manage ideas as a foundational aspect of their work. However, we have little knowledge about how idea management practices vary in different creative domains. Combining insights from qualitative surveys ( (N)   (sim) 200) and follow-up in-depth interviews ( (n)   (sim) 60) with creative professionals from four domains (interaction design, research, music, and graphics) of creative work, we report on (1) how ideas are externalized in practitioners’ archives, (2) what they consider important when choosing tools to capture ideas, and (3) how these tool collections resemble and differ from each other. Our cross-domain study demonstrates that participants’ tool use reflects idea capture characteristic needs as well as domain-specific views about the creative process. We conclude with a discussion about capturing as an externalizing activity, practitioners’ use of the term ideas, and four suggestions for directions in the design of creativity support tools."
Developing a Holistic AI Literacy Framework for Children,"Jia, Kaiyue and Leung, Teresa H. M. and Cheung, Ngai Yan Irene and Li, Yixun and Yu, Junnan",10.1145/3727986,2025,"The increasing prevalence of artificial intelligence (AI) in everyday life has intensified the emphasis on teaching AI literacy to children. However, there is no consensus on the specific knowledge and skills that constitute children’s AI literacy, resulting in varied AI learning materials for young people. We systematically searched for educational practices for children’s AI learning in both formal and informal settings and examined the AI learning content taught to children. Our findings led to the development of a holistic AI literacy framework for children, which contains three high-level dimensions and eight content areas of AI literacy: AI awareness (AI definition, AI application, and AI history), AI mechanics (AI input, learning procedure, and AI output), and AI impacts (AI implication and responsible practice). Theoretically, we contribute a research-based, comprehensive, and current framework for children’s AI literacy, advancing its conceptualization in early life stages. Practically, our framework can guide researchers and practitioners in promoting AI education for the next generation."
Unified Empirical Evaluation and Comparison of Session-based Recommendation Algorithms,"Zhang, Qingbo and Zhou, Xiangmin and Zhang, Xiuzhen and Yang, Xiaochun and Wang, Bin and Yi, Xun",10.1145/3728358,2025,"Recently, session-based recommendation systems (SBRSs) have become a highly explored area, and numerous methods have been proposed. The abundance of related work poses a challenge for newcomers in comprehending the current research landscape and burdens researchers during method validation. Offering a thorough research overview helps newcomers understand the current research. Additionally, comparing representative methods in a consistent environment allows researchers to streamline their workload by focusing on the top-performing methods. Existing theory-oriented review articles introduce the main techniques employed in SBRSs but lack a detailed exploration of their specific applications. The most recent neural method evaluated in existing experiment-driven review was published in 2019, and the latest state-of-the-art methods haven’t been included. To address these gaps, this paper offers a more thorough overview of SBRSs. Specifically, we first categorize and overview existing methods. Then, we introduce the main techniques and illustrate their applications. The performance of representative methods is validated under identical experimental conditions to ensure reliable comparative results. Our findings indicate that dataset characteristics significantly impact model performance, and attention mechanisms-based and gated neural networks (GNNs)-based models generally outperform others. Finally, we propose potential directions for future research in SBRSs."
Diffusion-Based Visual Art Creation: A Survey and New Perspectives,"Wang, Bingyuan and Chen, Qifeng and Wang, Zeyu",10.1145/3728459,2025,"The integration of generative AI in visual art has revolutionized not only how visual content is created but also how AI interacts with and reflects the underlying domain knowledge. This survey explores the emerging realm of diffusion-based visual art creation, examining its development from both artistic and technical perspectives. We structure the survey into three phases: data feature and framework identification, detailed analyses using a structured coding process, and open-ended prospective outlooks. Our findings reveal how artistic requirements are transformed into technical challenges and highlight the design and application of diffusion-based methods within visual art creation. We also provide insights into future directions from technical and synergistic perspectives, suggesting that the confluence of generative AI and art has shifted the creative paradigm and opened up new possibilities. By summarizing the development and trends of this emerging interdisciplinary area, we aim to shed light on the mechanisms through which AI systems emulate and, possibly, enhance human capacities in artistic perception and creativity."
Carbon-Efficient Software Design and Development: A Systematic Literature Review,"Danushi, Ornela and Forti, Stefano and Soldani, Jacopo",10.1145/3728638,2025,"The ICT sector, responsible for 2% of global carbon emissions, is under scrutiny calling for methodologies and tools to design and develop software in an environmentally sustainable-by-design manner. However, the software engineering solutions for designing and developing carbon-efficient software are currently scattered over multiple different pieces of literature, which makes it difficult to consult the body of knowledge on the topic. In this article, we precisely conduct a systematic literature review on state-of-the-art proposals for designing and developing carbon-efficient software. We identify and analyse 65 primary studies by classifying them through a taxonomy aimed at answering the 5W1H questions of carbon-efficient software design and development. We first provide a reasoned overview and discussion of the existing guidelines, reference models, measurement solutions, and techniques for measuring, reducing, or minimising the carbon footprint of software. Ultimately, we identify open challenges and research gaps, offering insights for future work in this field."
A Survey on Services Placement Algorithms in Integrated Cloud-Fog / Edge Computing,"Taleb, Imane and Guillaume, Jean-Loup and Duthil, Benjamin",10.1145/3729214,2025,"The evolution of computing paradigms, such as Fog and Edge, has led to the emergence of new applications that require a placement of services close to the end users. Optimal placement of such applications over network nodes is therefore an important issue in integrated Cloud-Fog / Edge computing environments. The service placement problem is challenging due to the complexity of such distributed systems, limited computing resources, placement constraints, global efficiency of the application and quality requirements for end users. This survey attempts to provide a comprehensive and structured presentation of services placement problem research in integrated Cloud-Fog / Edge. We propose a new classification of services placement problem approaches by adopting an algorithmic viewpoint and a breakdown into three categories: graph-based, heuristics and machine-learning approaches. We also provide an analysis and some discussions of the methods, and identify some future challenges and issues in the case of microservices. We hope that this survey will provide a better understanding of the solutions and directions taken by research on this important topic."
Smart Road Traffic Monitoring: Unveiling the Synergy of IoT and AI for Enhanced Urban Mobility,"Saini, Komal and Sharma, Sandeep",10.1145/3729217,2025,"Emerging technologies such as Artificial Intelligence (AI) and the Internet of Things (IoT) have transformed intelligent transportation systems, providing novel solutions to the increasing complexity of managing traffic on roads as cities grow and traffic density rises, particularly in developing countries. Smart road traffic management systems seek to alleviate traffic-related issues, benefiting citizens and society. This study reports on a Preferred Reporting Item for Systematic Reviews and Meta-Analyses (PRISMA)-based systematic literature review (SLR) of 75 papers published between 2014 and 2023. Like prior reviews, this SLR focuses on recent advances in the Internet of Things and Artificial Intelligence for traffic management, covering crucial practical issues such as scalability, data privacy, and resource constraints in growing regions. The study covers significant topics thoroughly, including AI approaches such as machine learning and deep learning, the incorporation of the Internet of Things, and artificial intelligence in traffic management, including the evaluation methods utilized in the examined studies. By synthesizing insights from various studies, recognizing research gaps, and proposing recommendations for future research, this work provides a comprehensive understanding of how recent advances in smart road traffic monitoring will lead to more effective models that improve urban mobility and benefit the community."
Two Decades of Automated AI Planning Methods in Construction and Fabrication: a Systematic Review,"Sherkat, Shermin and Wortmann, Thomas and Wortmann, Andreas",10.1145/3729529,2025,"Task planning and scheduling are crucial for construction or fabrication (CF) processes. Automating them is necessary for more efficient plans in terms of time and resources. However, most construction planning processes are still performed manually despite the existence of various AI methods. Symbolic AI automated task planning (ATP) techniques offer a variety of features to tackle task planning problems, but their application to CF has not been researched yet. This study identifies the current state of research and gaps in the literature regarding these AI techniques while providing directions for future research. We conduct a systematic review that evaluates existing literature on ATP in terms of environmental characteristics, modeling languages, ATP techniques, and results. We searched the ACM, IEEE, Scopus, WOS, and SpringerLink databases for papers published in the last 20 years (2002-2022) that discuss symbolic AI methods used in task planning within the CF fields. Our findings indicate that research on automated planning is currently limited regarding the characteristics of CF environments. Only a few papers have utilized symbolic languages, AI planners, and ATP techniques. No paper has evaluated their planning system in an on-site CF process. As a result, many symbolic languages, planners, and ATP techniques remain unexplored."
A Systematic Review of XR-Enabled Remote Human-Robot Interaction Systems,"Wang, Xian and Shen, Luyao and Lee, Lik-Hang",10.1145/3730574,2025,"The rising interest in creating versatile robots to handle multiple tasks in various environments, with humans interacting through immersive interfaces. This survey provides a comprehensive review of extended reality (XR) applications in remote human-robot interaction (HRI). We developed a systematic search strategy based on the PRISMA methodology, focusing on peer-reviewed publications that demonstrate practical implementations of XR in remote robot control, real robot system deployment, and HRI applications, we analyzed research published between January 2013 and December 2023. From the initial 2,561 articles, 100 met our inclusion criteria were included. We categorized and summarized the domain in detail, delving into the methods used in these articles to achieve intuitive and effective remote HRI, highlighting user experience enhancement and interaction designs. This survey identifies research opportunities, particularly emphasizes that future researchers should explore the potential of XR, such as exploring multimodal enhancement techniques that seamlessly integrate visual, haptic, and auditory feedback for more intuitive teleoperation. Our analysis reveals that while XR shows promising potential in remote HRI, there are significant gaps, such as user-centered design. This survey provides a framework for understanding the current state of XR-based remote HRI, establishing a foundation for future research."
Driving Healthcare Monitoring with IoT and Wearable Devices: A Systematic Review,"Baiense, Jo\~{a}o and Zdravevski, Eftim and Coelho, Paulo and Pires, Ivan Miguel and Velez, Fernando",10.1145/3731595,2025,"Wearable technologies have become a significant part of the healthcare industry, collecting personal health data and extracting valuable information for real-time assistance. This review paper analyzes thirty-five scientific publications on driving healthcare monitoring with IoT and wearable device applications. These papers were considered in a quantitative and qualitative analysis using the Natural Language Processing framework and the PRISMA methodology to filter the search results. The selected papers were published between January 2010 and May 2024 in one of the following scientific databases: IEEE Xplore, Springer, ScienceDirect (i.e., Elsevier), Association for Computing Machinery (ACM), Multidisciplinary Digital Publishing Institute (MDPI), or PubMed Central. The analysis considers population, methods, hardware, features, and communications. The research highlights that data collected from one or numerous sensors is processed and accessible in a database server for various uses, such as informing professional careers or assisting users. The review suggests that robust and efficient driving healthcare monitoring with IoT and wearable devices applications can be designed considering the valuable principles presented in this review."
State-of-the-Art and Challenges of Engineering ML- Enabled Software Systems in the Deep Learning Era,"Assres, Gebremariam and Bhandari, Guru and Shalaginov, Andrii and Gronli, Tor-Morten and Ghinea, Gheorghita",10.1145/3731597,2025,"Emerging from the software crisis of the 1960s, conventional software systems have vastly improved through Software Engineering (SE) practices. Simultaneously, Artificial Intelligence (AI) endeavors to augment or replace human decision-making. In the contemporary landscape, Machine Learning (ML), a subset of AI, leverages extensive data from diverse sources, fostering the development of ML-enabled (intelligent) software systems. While ML is increasingly utilized in conventional software development, the integration of SE practices in developing ML-enabled systems, especially across typical Software Development Life Cycle (SDLC) phases and methodologies in the post-2010 Deep Learning (DL) era, remains underexplored. Our survey of existing literature unveils insights into current practices, emphasizing the interdisciplinary collaboration challenges of developing ML-enabled software, including data quality, ethics, explainability, continuous monitoring and adaptation, and security. The study underscores the imperative for ongoing research and development with focus on data-driven hypotheses, non-functional requirements, established design principles, ML-first integration, automation, specialized testing, and use of agile methods."
Bystander Privacy in Smart Homes: A Systematic Review of Concerns and Solutions,"Saqib, Eimaan and He, Shijing and Choy, Junghyun and Abu-Salma, Ruba and Such, Jose and Bernd, Julia and Javed, Mobin",10.1145/3731755,2025,"Smart home devices, such as security cameras and voice assistants, have seen widespread adoption due to the utility and convenience they offer to users. The deployment of these devices in homes, however, raises privacy concerns for bystanders—people who may not necessarily have a say in the deployment and configuration of these devices, and yet are exposed to or affected by their data collection. Examples of bystanders include guests, short-term tenants, and domestic workers. Prior work has studied the privacy concerns of different bystander groups and proposed design solutions for addressing these concerns. In this paper, we present a systematic review of previous studies, describing how smart home bystanders are defined and classified, and illuminating the range of concerns and solutions proposed in the existing academic literature. We also discuss limitations in prior work, barriers to the uptake of research-based solutions by industry, and identify avenues for future research."
A Comprehensive Survey on Deep Learning-based Predictive Maintenance,"Khan, Uzair and Cheng, Dong and Setti, Francesco and Fummi, Franco and Cristani, Marco and Capogrosso, Luigi",10.1145/3732287,2025,"With the advent of Industrial 4.0 and the push towards Industry 5.0, the data generated by the industries have become surprisingly large. This abundance of data significantly boosts machine and deep learning models for Predictive Maintenance (PdM). The PdM plays a vital role in extending the lifespan of industrial equipment and machines while also helping to reduce the risk of unscheduled downtime. Given its multidisciplinary nature, the field of PdM has been approached from many different angles: this comprehensive survey aims to provide an up-to-date overview focused on all the learning-based industrial PdM strategies, discussing weaknesses and strengths. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing a systematic and complete review of the literature. In particular, firstly, we explore the main learning models used for PdM, mainly Convolutional Neural Networks (ConvNets), Autoencoders (AEs), Generative Adversarial Networks (GANs), and Transformers, also giving an overview of the newest models such as diffusion models and foundation models. Then, we discuss the main learning paradigms applied to PdM, i.e., supervised, unsupervised, ensemble, transfer, federated, and reinforcement learning. Furthermore, this work discusses the pipeline of the data-driven PdM and its benefits, practical applications, datasets, and benchmarks. In addition, the evaluation metrics for each PdM stage and the state-of-the-art hardware devices used are discussed. Finally, the challenges and future work are presented."
IPv6 Routing Protocol for Low-Power and Lossy Networks Security Vulnerabilities and Mitigation Techniques: A Survey,"Zilberman, Aviram and Dvir, Amit and Stulman, Ariel",10.1145/3732776,2025,"The proliferation of the Internet of Things (IoT) has reshaped the way we interact with technology, propelling the Routing Protocol for Low-Power and Lossy Networks (RPL) into a critical role as a communication framework. Amid this transformative landscape, security vulnerabilities within RPL-based IoT networks emerge as a substantial concern. This survey delves into these vulnerabilities, offering insights into their intricacies, potential consequences, and robust mitigation strategies. Commencing with a foundational understanding of IoT networks and their real-world applications, the survey sets the stage for comprehending the significance of Routing Protocol for Low-Power and Lossy Networks (RPL). It unravels the unique characteristics of RPL networks, their Destination-Oriented Directed Acyclic Graph (DODAG) topologies, and their pivotal role in enabling seamless device communication. The survey then delves into the heart of RPL security vulnerabilities. It navigates through diverse attack vectors, such as rank attacks and version number attacks. Each vulnerability is scrutinized, unraveling its technical mechanisms and implications for network stability. Transitioning from vulnerabilities to resilience, the survey offers a panoramic view of mitigation strategies. It dissects the nuances of intrusion detection systems (IDS), exploring trust models, location-based approaches, and hybrid systems. Signature-based, anomaly-based, and specification-based detection mechanisms are evaluated for their potential to mitigate threats within RPL networks. As standards shape the IoT landscape, the survey underscores the pivotal role of RPL within this framework. It emphasizes the necessity of secure standards in mitigating vulnerabilities across interconnected IoT devices."
Collective Intelligence in Humanitarian Voluntary Geographic Information: The Case of the HOT Tasking Manager,"Herrera-Murillo, Dagoberto Jos\'{e} and Ochoa-Ortiz, H\'{e}ctor and Ahmed, Umair and L\'{o}pez-Pellicer, Francisco Javier and Re, Barbara and Polini, Andrea and Nogueras-Iso, Javier",10.1145/3733600,2025,"Voluntary Geographic Information initiatives are transforming the disaster response landscape. Our research provides insights into how the concept of collective intelligence is accomplished in humanitarian mapping initiatives. The main source originates from the data obtained in 746 mapping projects organised by the Humanitarian OpenStreetMap Team between December 2021 and November 2023, where 38,893 contributors completed 312,289 mapping tasks. These data include detailed attributes of the contributors and the states the tasks go through. The methodology adopts a quantitative approach, including descriptive and inferential statistics, and standard process mining techniques. Our results indicate that, in general terms, in humanitarian mapping, a group of contributors from outside the area of interest perform straightforward mapping tasks with limited collaboration among them. The “wisdom” of advanced contributors is the cornerstone that sustains the system. The discussion section elaborates on (1) how these findings suggest that humanitarian mapping projects effectively meet their short-term mapping objectives but fall short if more sustainable mapping objectives are sought and (2) possible strategies for better harnessing the collective intelligence of these efforts."
When Deep Learning Meets Information Retrieval-based Bug Localization: A Survey,"Niu, Feifei and Li, Chuanyi and Liu, Kui and Xia, Xin and Lo, David",10.1145/3734217,2025,"Bug localization is a crucial aspect of software maintenance, running through the entire software lifecycle. Information retrieval-based bug localization (IRBL) identifies buggy code based on bug reports, expediting the bug resolution process for developers. Recent years have witnessed significant achievements in IRBL, propelled by the widespread adoption of deep learning (DL). To provide a comprehensive overview of the current state of the art and delve into key issues, we conduct a survey encompassing 61 IRBL studies leveraging DL. We summarize best practices in each phase of the IRBL workflow, undertake a meta-analysis of prior studies, and suggest future research directions. This exploration aims to guide further advancements in the field, fostering a deeper understanding and refining practices for effective bug localization. Our study suggests that the integration of DL in IRBL enhances the model’s capacity to extract semantic and syntactic information from both bug reports and source code, addressing issues such as lexical gaps, neglect of code structure information, and cold-start problems. Future research avenues for IRBL encompass exploring diversity in programming languages, adopting fine-grained granularity, and focusing on real-world applications. Most importantly, although some studies have started using large language models for IRBL, there is still a need for more in-depth exploration and thorough investigation in this area."
Fingerprinting Voice Commands of VPN-protected Smart Speakers,"Guo, Xiaoguang and Yu, Keyang and Li, Qi and Chen, Dong",10.1145/3734870,2025,"Extensive recent research has shown that it is surprisingly easy to infer Amazon Alexa voice commands over their network traffic data. To prevent these traffic analytics (TA)-based inference attacks, smart home owners are considering deploying virtual private networks (VPNs) to safeguard their smart speakers. In this work, we design a new machine learning-powered attack framework—VoiceAttack that could still accurately fingerprint voice commands on VPN-encrypted voice speaker network traffic. We evaluate VoiceAttack under 5 different real-world settings using Amazon Alexa and Google Home. Our results show that VoiceAttack could correctly infer voice command sentences with a Matthews Correlation Coefficient (MCC) of 0.68 in a closed-world setting and infer voice command categories with an MCC of 0.84 in an open-world setting by eavesdropping VPN-encrypted network traffic data. This presents a significant risk to user privacy and security, as it suggests that external on-path attackers could still potentially intercept and decipher users’ voice commands despite the VPN encryption. We then further examine the sensitivity of voice speaker commands to VoiceAttack. We find that 134 voice speaker commands are highly vulnerable to VoiceAttack. We also present a defense approach—VoiceDefense, which could inject inject appropriate traffic “noise” into voice speaker traffic. And our evaluation results show that VoiceDefense could effectively mitigate VoiceAttack on Amazon Echo and Google Home."
Bias in Federated Learning: A Comprehensive Survey,"Benarba, Nawel and Bouchenak, Sara",10.1145/3735125,2025,"Federated Learning (FL) enables collaborative model training over multiple clients’ data, without sharing these data for better privacy. Addressing bias in FL remains a challenge. In this paper, we first present a taxonomy of FL bias, presenting the causes and the different types of FL bias, namely demographic bias, performance-related bias, and contribution-related bias. We then categorize FL bias mitigation, in terms of used methods and provided guarantees, before providing a comprehensive and comparative analysis of existing works. Finally, we highlight key challenges and open research directions, including the impact of FL bias mitigation on model utility, privacy, and robustness."
Semantic-driven Cross-space Graph Interaction Network for Fine-grained 3D Point Cloud Understanding,"Ren, Peng and Bai, Yunfeng and Li, Xiaoheng and Jia, Jinyuan",10.1145/3735560,2025,"Since irregular 3D point clouds inherently lack connected relations, most approaches focus less on low-level spherical geometric features and high-level distant semantic feature dependencies and interactions, leading to inadequate feature representations for fine-grained point cloud understanding. To tackle the puzzle, we propose a novel and effective semantic-driven cross-space graph interaction network (CrossGIN) to explore and leverage local spatial features and range-aware semantic features across potential dual semantic spaces. Specifically, a local spatial aggregation is designed to capture position structures of spherical geometries by a spatial position module and enhance low-level spatial features using the semantic-aware module. Moreover, a graph interaction filter is proposed to dynamically aggregate the metric long-range semantic clues and better facilitate the adaptive feature interactions between 3D spatial and deep feature space. Finally, comprehensive experiments are conducted for 3D shape classification and object part segmentation tasks on several benchmark datasets such as ScanobjectNN, ModelNet40, and ShapeNetPart. The quantitative and qualitative results demonstrate that our method achieves competitive performance in comparison to recent approaches and verify the effectiveness of various modules."
Judge: Effective State Abstraction for Guiding Automated Web GUI Testing,"Liu, Chenxu and Wang, Junheng and Yang, Wei and Zhang, Ying and Xie, Tao",10.1145/3736162,2025,"Automated web GUI testing approaches aim to maximize the code coverage of a web app within a specific time budget. However, due to the highly dynamic characteristics of web apps, testing approaches often get stuck in loops or repeatedly explore the same app areas. To address this issue, existing approaches conduct state abstraction, grouping similar pages into the same state in an effort to approximate the ideal state (i.e., a state that encompasses all-and-only those pages exhibiting the same behavior from a testing perspective) to reduce repetitive explorations. Typically, these approaches rely on the Document Object Model (DOM) or visual similarity, using predefined thresholds or learning-based classifiers to determine which pages should belong to the same state. However, pages within the same ideal state still exhibit discrepancies, caused by factors such as dynamically loaded data and dynamically expanded UI elements. The varying page complexities and design styles among apps bring even more challenges. These phenomena present substantial obstacles to existing approaches in determining desirable classification thresholds or training desirable classifiers, preventing them from conducting satisfactory state abstraction to guide the testing process.To address the preceding challenges, in this article, we propose Judge, a novel approach based on structure merging and contrastive learning for state abstraction. Judge includes a “merge-and-classify” strategy. In the “merge” phase, Judge iterates through the DOM tree of each given page and merges web element siblings that share the same subtree structure into a single one to abstract and simplify the page, while discarding text contents and HTML attributes of web elements in the process. In this way, Judge mitigates the negative effects introduced by dynamically loaded data and dynamically expanded UI elements, substantially reducing discrepancies between pages in the same ideal state. In the “classify” phase, Judge uses a dedicated contrastive learning model to embed simplified page DOMs into vectors and further conducts classification with a Support Vector Machine (SVM), enabling classification in high-dimensional vector space and improving generalizability across diverse web apps.We evaluate Judge against 13 widely used baseline approaches. The results highlight that Judge outperforms these baseline approaches in classifying page pairs, with an average margin ranging from 8.95% to 28.90% in the F1 score across three manually labeled datasets. Additionally, when compared to the five most effective baseline approaches, Judge demonstrates superiority in guiding the exploration of automated web GUI testing in six widely studied apps, with code coverage improved by an average of 2.62% to 14.12%. The code and data of Judge are publicly accessible."
Exploring Fine-Grained Bug Report Categorization with Large Language Models and Prompt Engineering: An Empirical Study,"Koyuncu, Anil",10.1145/3736408,2025,"Accurate classification of issues is essential for effective project management and timely responses, as the volume of issue reports continues to grow. Manual classification is labor-intensive and error-prone, necessitating automated solutions. While large language models (LLMs) show promise in automated issue labeling, most research focuses on broad categorization (e.g., bugs, feature requests), with limited attention to fine-grained categorization. Understanding specific bug types is crucial, as different bugs require tailored resolution strategies.This study addresses this gap by evaluating LLMs and prompt engineering strategies for fine-grained bug report categorization. We analyze 221,184 fine-grained bug report category labels generated by selected LLMs using various prompt engineering strategies for 1,024 bug reports. We examine how LLMs and prompt engineering influence output characteristics, control over outputs, and categorization performance. Our findings highlight that LLMs and prompt engineering significantly impact output consistency and classification capability, with some yielding consistent results and others introducing variability. Based on these findings, we analyze the agreements and disagreements between LLM-generated labels and human annotations to assess category correctness. Our results suggest that examining label consistency and discrepancies can serve as a complementary method for validating bug report categories, identifying unclear reports, and detecting misclassifications in human annotations."
"Using Hadoop MapReduce for Parallel Genetic Algorithms: A Comparison of the Global, Grid and Island Models","Ferrucci, Filomena and Salza, Pasquale and Sarro, Federica",10.1162/evco_a_00213,2018,"The need to improve the scalability of Genetic Algorithms (GAs) has motivated the research on Parallel Genetic Algorithms (PGAs), and different technologies and approaches have been used. Hadoop MapReduce represents one of the most mature technologies to develop parallel algorithms. Based on the fact that parallel algorithms introduce communication overhead, the aim of the present work is to understand if, and possibly when, the parallel GAs solutions using Hadoop MapReduce show better performance than sequential versions in terms of execution time. Moreover, we are interested in understanding which PGA model can be most effective among the global, grid, and island models. We empirically assessed the performance of these three parallel models with respect to a sequential GA on a software engineering problem, evaluating the execution time and the achieved speedup. We also analysed the behaviour of the parallel models in relation to the overhead produced by the use of Hadoop MapReduce and the GAs' computational effort, which gives a more machine-independent measure of these algorithms. We exploited three problem instances to differentiate the computation load and three cluster configurations based on 2, 4, and 8 parallel nodes. Moreover, we estimated the costs of the execution of the experimentation on a potential cloud infrastructure, based on the pricing of the major commercial cloud providers. The empirical study revealed that the use of PGA based on the island model outperforms the other parallel models and the sequential GA for all the considered instances and clusters. Using 2, 4, and 8 nodes, the island model achieves an average speedup over the three datasets of 1.8, 3.4, and 7.0 times, respectively. Hadoop MapReduce has a set of different constraints that need to be considered during the design and the implementation of parallel algorithms. The overhead of data store (i.e., HDFS) accesses, communication, and latency requires solutions that reduce data store operations. For this reason, the island model is more suitable for PGAs than the global and grid model, also in terms of costs when executed on a commercial cloud provider."
Self-organization in online collaborative work settings,"Lykourentzou, Ioanna and Vinella, Federica Lucia and Ahmed, Faez and Papastathis, Costas and Papangelis, Konstantinos and Khan, Vassilis-Javed and Masthoff, Judith",10.1177/26339137221078005,2022,"As the volume and complexity of distributed online work increases, collaboration among people who have never worked together in the past is becoming increasingly necessary. Recent research has proposed algorithms to maximize the performance of online collaborations by grouping workers in a top-down fashion and according to a set of predefined decision criteria. This approach often means that workers have little say in the collaboration formation process. Depriving users of control over whom they will work with can stifle creativity and initiative-taking, increase psychological discomfort, and, overall, result in less-than-optimal collaboration results—especially when the task concerned is open-ended, creative, and complex. In this work, we propose an alternative model, called Self-Organizing Pairs (SOPs), which relies on the crowd of online workers themselves to organize into effective work dyads. Supported but not guided by an algorithm, SOPs are a new human-centered computational structure, which enables participants to control, correct, and guide the output of their collaboration as a collective. Experimental results, comparing SOPs to two benchmarks that do not allow user agency, and on an iterative task of fictional story writing, reveal that participants in the SOPs condition produce creative outcomes of higher quality, and report higher satisfaction with their collaboration. Finally, we find that similarly to machine learning-based self-organization, human SOPs exhibit emergent collective properties, including the presence of an objective function and the tendency to form more distinct clusters of compatible collaborators."
Generating application-specific data layouts for in-memory databases,"Yan, Cong and Cheung, Alvin",10.14778/3342263.3342630,2019,"Database applications are often developed with object-oriented languages while using relational databases as the backend. To accelerate these applications, developers would manually design customized data structures to store data in memory, and ways to utilize such data structures to answer queries. Doing so is brittle and requires a lot of effort. Alternatively, developers might automate the process by using relational physical design tools to create materialized views and indexes instead. However, the characteristics of object-oriented database applications are often distinct enough from traditional database applications such that classical relational query optimization techniques often cannot speed up queries that arise from such applications, as our experiments show.To address this, we build Chestnut, a data layout generator for in-memory object-oriented database applications. Given a memory budget, Chestnut generates customized in-memory data layouts and query plans to answer queries written using a subset of the Rails API, a common framework for building object-oriented database applications. Chestnut differs from traditional query optimizers and physical designers in two ways. First, Chestnut automatically generates data layouts that are customized for the application after analyzing their queries, hence Chestnut-generated data layouts are designed to be efficient to answer queries from such applications. second, Chestnut uses a novel enumeration and verification-based algorithm to generate query plans that use such data layouts, rather than rule-based approaches as in traditional query optimizers. We evaluated Chestnut on four open-source Rails database applications. The result shows that it can reduce average query processing time by over 3.6X (and up to 42X), as compared to other in-memory relational database engines."
Witan: unsupervised labelling function generation for assisted data programming,"Denham, Benjamin and Lai, Edmund M-K. and Sinha, Roopak and Naeem, M. Asif",10.14778/3551793.3551797,2022,"Effective supervised training of modern machine learning models often requires large labelled training datasets, which could be prohibitively costly to acquire for many practical applications. Research addressing this problem has sought ways to leverage weak supervision sources, such as the user-defined heuristic labelling functions used in the data programming paradigm, which are cheaper and easier to acquire. Automatic generation of these functions can make data programming even more efficient and effective. However, existing approaches rely on initial supervision in the form of small labelled datasets or interactive user feedback. In this paper, we propose Witan, an algorithm for generating labelling functions without any initial supervision. This flexibility affords many interaction modes, including unsupervised dataset exploration before the user even defines a set of classes. Experiments in binary and multi-class classification demonstrate the efficiency and classification accuracy of Witan compared to alternative labelling approaches."
spade: Synthesizing Data Quality Assertions for Large Language Model Pipelines,"Shankar, Shreya and Li, Haotian and Asawa, Parth and Hulsebos, Madelon and Lin, Yiming and Zamfirescu-Pereira, J. D. and Chase, Harrison and Fu-Hinthorn, Will and Parameswaran, Aditya G. and Wu, Eugene",10.14778/3685800.3685835,2024,"Large language models (LLMs) are being increasingly deployed as part of pipelines that repeatedly process or generate data of some sort. However, a common barrier to deployment are the frequent and often unpredictable errors that plague LLMs. Acknowledging the inevitability of these errors, we propose data quality assertions to identify when LLMs may be making mistakes. We present spade, a method for automatically synthesizing data quality assertions that identify bad LLM outputs. We make the observation that developers often identify data quality issues during prototyping prior to deployment, and attempt to address them by adding instructions to the LLM prompt over time. spade therefore analyzes histories of prompt versions over time to create candidate assertion functions and then selects a minimal set that fulfills both coverage and accuracy requirements. In testing across nine different real-world LLM pipelines, spade efficiently reduces the number of assertions by 14% and decreases false failures by 21% when compared to simpler baselines. spade has been deployed as an offering within LangSmith, LangChain's LLM pipeline hub, and has been used to generate data quality assertions for over 2000 pipelines across a spectrum of industries."
Top-10 suggestions from a decade of managing undergraduate software teams,"Feng, Weiqi and LeBlanc, Mark D.",,2019,"Sustaining a multi-year research project with undergraduates is a labor of love that leverages the very best of computer science teaching and research. We present a decade of software development during which we led an interdisciplinary research group focused on the implementation and use of a web-based app for scholars and students who wish to explore their digitized texts. In our experience, scholars, e.g., those from the Humanities, who might like to perform computational analysis in their areas of expertise and/or wish to teach their students how to do so become discouraged too early in the game. Our research model combines interdisciplinary teaching and recruitment during the school year with simultaneous scholarly activity and software development sprints each summer in a blend of graduate school and start-up-like student experiences. Led by faculty in Computer Science and English, 63 undergraduate researchers have participated. Forty-nine of these students (40% of those women) have contributed to the software, many assuming leadership roles over six software releases. Both students and faculty offer lessons learned and our ""top 10"" suggestions for sustaining a large software effort across multiple student cohorts."
Experiential learning framework for smaller computer science programs,"Kissel, Zachary and Stuetzle, Christopher",,2020,Experiential learning (EL) permeates the Computer Science discipline. This work seeks to codify EL practices for computer science pedagogy into five key pillars. These pillars have been successfully applied at a small to mid-sized college within the heavily competitive Boston area. This paper further describes how a computer science department may effectively implement the pillars in their own curriculum.
Assessment of computer science courses in the context of a global knowledge economy,"Popova, Viktoria and Poger, Sofya",,2020,"The role of assessment in academic practices is usually associated with evaluating students' knowledge, skills, and competencies exclusively within a given subject area. Thus, a course in Computer Programming is not likely to include assessment of students' persuasive communication skills. Traditional teaching and assessment have taken a very targeted (and siloed) approach to introducing and practicing both applied and theoretical fields of knowledge. The shortcomings of this practice may not have explicitly affected student readiness to enter the workforce in the last millennium. However, it has become increasingly evident in the last 20 years that the exponentially expanding global knowledge economy requires that knowledge workers, such as professionals in computer science fields, are adept not only in their field of study: graduates should be able to demonstrate proficiency in ""Human Skills,"" as well as be able to operate as ""Business Enablers."" The New Foundational Skills of the Digital Economy report by the Burning Glass [1], reveals the need expressed by employers across various sectors for the new workforce to be equipped with a broader set of skills than that usually required by a single discipline."
Google tech exchange: an industry-academic partnership that prepares black and latinx undergraduates for high-tech careers,"Alvarez, April and Burge, Legand and Emanuel, Shameeka and Gates, Ann and Goldman, Sally and Griffin, Jean and Keeling, Harry and Madda, Mary Jo and Okafor, Bianca and Onowho, Alycia and Washington, Gloria",,2020,"This paper describes Google Tech Exchange, an industry-academic partnership that involves several Historically Black Colleges and Hispanic Serving Institutions. Tech Exchange's mission is to unlock opportunities in the tech industry for Black and Latinx undergraduates. It is an immersive computer science experience for students and faculty. Participants spend a semester or two at Google in Silicon Valley taking or co-teaching computer science courses, including cutting-edge ones not offered at many universities. The 2018-2019 graduates especially valued the community-building, and a high percentage secured technical internships or jobs."
A scalable RPG project for object-oriented software development,"Givens, Robin M.",,2020,"This paper describes the development and outcome of a multi-part, object-oriented project based on role-playing games for a second-year software development course. The objective of the project was to provide an experience with a progressive, large-scale project focused on object-oriented design, implementation, and testing. The first phases of the project emphasized object-oriented programming, and in the final phase students controlled all of the design, implementation, and testing process. The role-playing game genre was chosen for its natural correlation to object-oriented design and its popularity among students. We present an example course schedule, the details of the project, the effects of the COVID-19 pandemic, positive student outcomes, and ideas for scaling the project."
DESlib: a dynamic ensemble selection library in python,"Cruz, Rafael M. O. and Hafemann, Luiz G. and Sabourin, Robert and Cavalcanti, George D. C.",,2020,"DESlib is an open-source python library providing the implementation of several dynamic selection techniques. The library is divided into three modules: (i) dcs, containing the implementation of dynamic classifier selection methods (DCS); (ii) des, containing the implementation of dynamic ensemble selection methods (DES); (iii) static, with the implementation of static ensemble techniques. The library is fully documented (documentation available online on Read the Docs), has a high test coverage (codecov.io) and is part of the scikit-learn-contrib supported projects. Documentation, code and examples can be found on its GitHub page: https://github.com/scikit-learn-contrib/DESlib."
Two-stage approach to multivariate linear regression with sparsely mismatched data,"Slawski, Martin and Ben-David, Emanuel and Li, Ping",,2020,"A tacit assumption in linear regression is that (response, predictor)-pairs correspond to identical observational units. A series of recent works have studied scenarios in which this assumption is violated under terms such as ""Unlabeled Sensing and ""Regression with Unknown Permutation"". In this paper, we study the setup of multiple response variables and a notion of mismatches that generalizes permutations in order to allow for missing matches as well as for one-to-many matches. A two-stage method is proposed under the assumption that most pairs are correctly matched. In the first stage, the regression parameter is estimated by handling mismatches as contaminations, and subsequently the generalized permutation is estimated by a basic variant of matching. The approach is both computationally convenient and equipped with favorable statistical guarantees. Specifically, it is shown that the conditions for permutation recovery become considerably less stringent as the number of responses m per observation increase. Particularly, for m = Ω(log n), the required signal-to-noise ratio no longer depends on the sample size n. Numerical results on synthetic and real data are presented to support the main findings of our analysis."
Beyond english-centric multilingual machine translation,"Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and El-Kishky, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and Goyal, Naman and Birch, Tom and Liptchinsky, Vitaliy and Edunov, Sergey and Grave, Edouard and Auli, Michael and Joulin, Armand",,2021,"Existing work in translation demonstrated the potential of massively multilingual machine translation by training a single model able to translate between any pair of languages. However, much of this work is English-Centric, training only on data which was translated from or to English. While this is supported by large sources of training data, it does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages. We build and open-source a training data set that covers thousands of language directions with parallel data, created through large-scale mining. Then, we explore how to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively to the best single systems from the Workshop on Machine Translation (WMT). We open-source our scripts so that others may reproduce the data, evaluation, and final M2M- 100 model"
A greedy algorithm for quantizing neural networks,"Lybrand, Eric and Saab, Rayan",,2021,"We propose a new computationally efficient method for quantizing the weights of pretrained neural networks that is general enough to handle both multi-layer perceptrons and convolutional neural networks. Our method deterministically quantizes layers in an iterative fashion with no complicated re-training required. Specifically, we quantize each neuron, or hidden unit, using a greedy path-following algorithm. This simple algorithm is equivalent to running a dynamical system, which we prove is stable for quantizing a single-layer neural network (or, alternatively, for quantizing the first layer of a multi-layer network) when the training data are Gaussian. We show that under these assumptions, the quantization error decays with the width of the layer, i.e., its level of over-parametrization. We provide numerical experiments, on multi-layer networks, to illustrate the performance of our methods on MNIST and CIFAR10 data, as well as for quantizing the VGG16 network using ImageNet data."
Assessing Student Outcomes Related to Design for ETAC-ABET Accreditation,"Ma, Lili and Mendoza, Benito",,2022,"This paper describes our assessment model that evaluates both Program Criteria Indicators and Student Outcome (2) which is related to engineering design, in a sophomore-level laboratory course for ETAC-ABET Accreditation. The Program Criteria Indicators are evaluated using two laboratory assignments. The Student Outcome (2) on students' ability to design systems, components, or processes for well-defined engineering technology problems is evaluated using the final design project. Due to the nature of this course which involves 3D modeling, analysis, and design, both the Program Criteria and Student Outcome (2) fit consistently with the course content and objectives. By making the assessment instruments faculty-driven and incorporating assessments as regular course activities, continuous improvement of students' learning experiences is achieved. The assessment cycles span from traditional inperson to half in-person and half online due to COVID-19 disruption and then to online teaching. The presented assessment results contributed to the recent re-accreditation of our programs."
Approximation and optimization theory for linear continuous-time recurrent neural networks,"Li, Zhong and Han, Jiequn and E, Weinan and Li, Qianxiao",,2022,"We perform a systematic study of the approximation properties and optimization dynamics of recurrent neural networks (RNNs) when applied to learn input-output relationships in temporal data. We consider the simple but representative setting of using continuous-time linear RNNs to learn from data generated by linear relationships. On the approximation side, we prove a direct and an inverse approximation theorem of linear functionals using RNNs, which reveal the intricate connections between memory structures in the target and the corresponding approximation efficiency. In particular, we show that temporal relationships can be effectively approximated by RNNs if and only if the former possesses sufficient memory decay. On the optimization front, we perform detailed analysis of the optimization dynamics, including a precise understanding of the difficulty that may arise in learning relationships with long-term memory. The term ""curse of memory"" is coined to describe the uncovered phenomena, akin to the ""curse of dimension"" that plagues high-dimensional function approximation. These results form a relatively complete picture of the interaction of memory and recurrent structures in the linear dynamical setting."
Meta-analysis of heterogeneous data: integrative sparse regression in high-dimensions,"Maity, Subha and Sun, Yuekai and Banerjee, Moulinath",,2022,"We consider the task of meta-analysis in high-dimensional settings in which the data sources are similar but non-identical. To borrow strength across such heterogeneous datasets, we introduce a global parameter that emphasizes interpretability and statistical efficiency in the presence of heterogeneity. We also propose a one-shot estimator of the global parameter that preserves the anonymity of the data sources and converges at a rate that depends on the size of the combined dataset. For high-dimensional linear model settings, we demonstrate the superiority of our identification restrictions in adapting to a previously seen data distribution as well as predicting for a new/unseen data distribution. Finally, we demonstrate the benefits of our approach on a large-scale drug treatment dataset involving several different cancer cell-lines."
Network regression with graph Laplacians,"Zhou, Yidong and M\""{u}ller, Hans-Georg",,2022,"Network data are increasingly available in various research fields, motivating statistical analysis for populations of networks, where a network as a whole is viewed as a data point. The study of how a network changes as a function of covariates is often of paramount interest. However, due to the non-Euclidean nature of networks, basic statistical tools available for scalar and vector data are no longer applicable. This motivates an extension of the notion of regression to the case where responses are network data. Here we propose to adopt conditional Fr\'{e}chet means implemented as M-estimators that depend on weights derived from both global and local least squares regression, extending the Fr\'{e}chet regression framework to networks that are quantified by their graph Laplacians. The challenge is to characterize the space of graph Laplacians to justify the application of Fr\'{e}chet regression. This characterization then leads to asymptotic rates of convergence for the corresponding M-estimators by applying empirical process methods. We demonstrate the usefulness and good practical performance of the proposed framework with simulations and with network data arising from resting-state fMRI in neuroimaging, as well as New York taxi records."
UX in E-Government Services for Citizens: A Systematic Literature Review,"Aldrees, Asma and Gra\v{c}anin, Denis",,2023,"The importance of user experience for the design, development, and deployment of software products has increased significantly, motivating governments to focus on user experience when designing e-government services. We evaluated the user experience of e-government services from citizens' perspectives by focusing on citizens' characteristics and social aspects, which significantly affect their experience with technology. Therefore, we conducted a systematic literature review to investigate user experience in e-government by adopting the Social Progress Index (SPI) as a user metric. We followed accepted, scholarly guidelines to form research questions and identify specific inclusion/exclusion criteria to meet research objectives. Then, we collected 672 articles, published between 2000--2022 from six technology digital libraries, and we selected 75 state-of-the-art studies. These studies were categorized based on SPI class. We synthesized a set of factors that significantly impacts user experience in each class. Then, we identified six major user experience concerns in e-government. We provide an overall analysis of these concerns, including the calculated frequencies that specific concerns appear in published articles, followed by definitions of the theories and models used to evaluate each behavior. We conclude with recommendations for how to enhance user experience in e-government services and mitigate the challenges hindering users from obtaining the best experiences."
Iterative Efforts for Improving Learning Experience in Software Engineering,"Dey, Pradip Peter and Amin, Mohammad and Sinha, Bhaskar Raj",,2023,"In a project-based learning environment, students and teachers jointly made iterative efforts for improving learning experience in software engineering through all major tasks including requirements analysis, design, implementation, and testing. The iterative efforts were implemented in a prototype-based evolutionary process by performing reviews jointly by students and teachers after each major task, and assessing student performance based on their participation in task-related activities. End of course evaluation data, collected in a standard anonymous process, indicated improvements in student learning experience and teaching effectiveness attributable to the iterative efforts. The major advantages of the iterative efforts were engaging students in the review process, and eliminating or reducing plagiarism-based academic dishonesty by emphasizing participation-based grading. One of the major challenges for teachers was making extra efforts for participation-based grading, rather than using automated grading of multiple choice exams and quizzes. In addition, extra efforts were needed to complete three iterations for sizable software engineering projects in a timely manner in order get benefits of iterative efforts in project-based learning environments. There are opportunities for future research in this area for creating a set of revealing software engineering projects of appropriate sizes and explaining their potential benefits in teaching learning environments."
Industry Trends in Software Engineering: Alumni Perspectives,"Anewalt, Karen and Polack, Jennifer",,2023,"It is important for computer science curricula to prepare graduates for their future careers. Alignment efforts between academia and industry benefit both communities. Having data about current industry trends, including tools and critical experiences, allows academia to adjust course assignments and curricula to provide relevant and needed material in today's computer science job market. We present survey responses from industry professionals related to tool, project, communication, and collaboration experiences essential for new employees. The collected data can be used to update and enhance current assignments across curricula. Responding to industry trends and demands can give future computer science professionals valuable experience as they begin their careers."
A Social Good Challenge for Teaching Undergraduate Affective Computing,"Washington, Gloria and Mejias, Marion",,2023,"This paper describes how a social good innovation challenge was used to teach 27 undergraduate and 6 graduate students affective computing techniques. The innovation challenge addressed the UN Sustainable Development Goals (SDGs) and allowed students to create technology solutions to solve problems related to clean water, poverty, and hunger, protecting the planet, human prosperity, and inclusive societies. Course activities were taught using project-based learning and required students to understand their potential users through motivational design thinking techniques. Emotional theory based on emotional artificial intelligence techniques were also taught. Students filled out course evaluations and provided observations on their experience in the course. Student feedback primarily related to motivation to win the design challenge and ability to connect theories taught in class with affective techniques. Challenges related to connecting existing emotional AI software libraries with web-based prototypes. Future iterations of the course will allow more time for integration of existing tools with prototype software."
Approximate post-selective inference for regression with the group LASSO,"Panigrahi, Snigdha and MacDonald, Peter W. and Kessler, Daniel",,2023,"After selection with the Group LASSO (or generalized variants such as the overlapping, sparse, or standardized Group LASSO), inference for the selected parameters is unreliable in the absence of adjustments for selection bias. In the penalized Gaussian regression setup, existing approaches provide adjustments for selection events that can be expressed as linear inequalities in the data variables. Such a representation, however, fails to hold for selection with the Group LASSO and substantially obstructs the scope of subsequent post-selective inference. Key questions of inferential interest--for example, inference for the effects of selected variables on the outcome--remain unanswered. In the present paper, we develop a consistent, post-selective, Bayesian method to address the existing gaps by deriving a likelihood adjustment factor and an approximation thereof that eliminates bias from the selection of groups. Experiments on simulated data and data from the Human Connectome Project demonstrate that our method recovers the effects of parameters within the selected groups while paying only a small price for bias adjustment."
CodaLab competitions: an open source platform to organize scientific challenges,"Pav\~{a}o, Adrien and Guyon, Isabelle and Letournel, Anne-Catherine and Tran, Dinh-Tuan and Bar\'{o}, Xavier and Escalante, Hugo Jair and Escalera, Sergio and Thomas, Tyler and Xu, Zhen",,2023,"CodaLab Competitions is an open source web platform designed to help data scientists and research teams to crowd-source the resolution of machine learning problems through the organization of competitions, also called challenges or contests. CodaLab Competitions provides useful features such as multiple phases, results and code submissions, multi-score leaderboards, and jobs running inside Docker containers. The platform is very flexible and can handle large scale experiments, by allowing organizers to upload large datasets and provide their own CPU or GPU compute workers."
Enhancing Learning in CS Capstone Courses through Advanced Project Matching,"Neda, Barbara Martinez and Weber, Jason Lee and Gago-Masague, Sergio and Wong-Ma, Jennifer",,2024,"Optimal group formation and project matching are critical and challenging tasks for instructors. We developed the Student-Project Matching Tool to optimize these processes and piloted it in a Computer Science Capstone course at the University of California, Irvine. The tool ensures that the team formation process balances individual preferences, project compatibility, and the overall performance potential of each team by considering students' skills and interests and sponsor projects' needs to maximize teams' success. Student perspectives and feedback showed an increase in student satisfaction with their team and the project they were matched to. Similarly, positive sponsor evaluations of the teams demonstrated that sponsors were pleased with the teams they were matched to. This tool provides the basis for effective team formation and project matching in Capstone courses, with a focus on maximizing student learning outcomes, real-world experiences, student-project ownership, and the number of fulfilled skills that each project requires for completion."
Large Language Model-Supported Software Testing with the CS Matrix Taxonomy,"Crandall, Johannah L. and Crandall, Aaron S.",,2024,"New breakthroughs in code synthesis from Generative Pre-Trained Transformers (GPT) and Large Language Model (LLM) algorithms are driving significant changes to software engineering education. Having algorithms able to generate components of a software project means that software developers will need stronger skills in requirements specification to guide code generation as well as stronger skills in code review, testing, and integration to incorporate AI-generated code into projects. Shifts in industry and classroom practices are already occurring with the availability of inline code generation tools like GitHub's Copilot, which makes discussion of pedagogical strategies in this area a timely topic. Of immediate concern in computer science education is the potential for LLM-generated code and code help to undermine the learning of CS students. In order to avoid such undermining in even intentional uses of LLM-enhanced learning supports, it is necessary to clarify the roles such supports need to play in the pedagogical process. The Computer Science Matrix Taxonomy provides a strong framework for organizing software testing learning outcomes as well as delineating the operational space in which LLM-based feedback tools should operate to support those learning outcomes. In this paper, the authors operationalize the CS Matrix Taxonomy for software testing learning outcomes and illustrate the integration of LLM-generated test strategy suggestions as an extension of the peer coding/testing model. The work includes examples of AI-generated code testing suggestions that students would use to help guide their own code synthesis for assignments or projects."
An Evaluation on the Impact of Large Language Models on Computer Science Curricula,"Rhee, Junghwan and Shrestha, Aakankshya and Qian, Gang and Zuo, Fei and Fu, Jicheng and Park, Myungah and Qu, Xianshan and Mylavarapu, Goutam and Sung, Hong",,2024,"Since their introduction, large language model (LLM) services have been widely used in our society, including the computer science education area. While this technology provides various types of intelligent assistance to users, its capabilities and impact on computer science education regarding students' learning need further study. In this paper, we present our manual assessment of LLM services' ability to solve questions in various course assignments and projects in our computer science curriculum. Based on the result of the study, we provide our observations of the extent of LLM services' impact on different computer science disciplines. Suggestions are summarized and offered to computer science instructors on the possible strategies for dealing with LLMs in current and future computer science curriculum designs."
Generative AI and its Impact on the CS Classroom and Programmers,"Lindoo, Ed and Lotfy, Mohamed",,2024,"As the integration of generative artificial intelligence (AI) in educational settings becomes more widespread, students, teachers, and educational institutions face the challenge of utilizing these technologies in a responsible manner. The responsible use of generative AI can help CS and IT students develop critical thinking, enhance their learning experience, facilitate the learning process, can assist in understanding code concepts, programming skills, and/or enhancing the programming knowledge. The aim of this investigation is on how students might utilize, and potentially abuse, generative AI. In this paper we provide examples of how generative AI can be used to generate code modules. We discuss the use of generative AI in programming classes as well as its impact on the future of programming and programmers."
A Learn-By-Doing Software Security Course,"Moreno, Charles and Peterson, Carson and Klingenberg, BJ and DeBruhl, Bruce",,2024,"Over the previous 20 years the need for software engineering and computer security education in undergraduate computing curriculum has become apparent. At Cal Poly, we have adopted stand-alone courses in both of these domains but have identified the intersection of software engineering and cybersecurity as a domain with curricular opportunity. In this paper, we outline a secure software engineering course and share our experience with running this course. Our course is focused on practical hands-on education with three large projects covering producing a secure software product, threat modelling, and malware design. Lastly, we cover the ethical considerations of this course and potential pitfalls of similar secure software engineering courses."
Software Orchestration: A Paradigm for Software Development and Security Assessment Using ChatGPT Requirements,"Elarde, Joseph and Bruster, Barry and Hasan, Mir",,2024,"Software orchestration integrates AI tools like ChatGPT into the software development process, evolving beyond traditional methods. This paper introduces Software Orchestration, blending the concepts of a musical symphony, where a conductor guides an orchestra, with automated processes in computing. Here, AI Neural Networks act as the ""orchestra"" and the Software Engineer as the ""conductor,"" collaboratively crafting, refining, and executing software. This approach combines human expertise with AI capabilities, enhancing software design, development, validation, and documentation. We explore its principles and applications in software development, supported by nine experimental case studies, highlighting its transformative potential in the software industry."
Starting a Civic Engagement Capstone: An Experience Report,"Hills, Mark and Fenwick, James B.",,2024,"In this paper, we reflect on the first semester of a Computer Science capstone course focused on civic engagement projects. This was offered as an alternate section of an existing capstone, with multiple student teams working with a local non-profit. We describe how the student work was structured during the semester, as well as the activities leading up to the semester and occurring immediately after the semester. We also reflect on what worked well, and discuss potential changes in future semesters. We believe this will be helpful to other faculty that are either starting a new capstone course, or modifying an existing capstone course, that will engage community partners with student teams."
Teaching Bioinformatics Students to Lead Reproducible Research,"Darby, Miranda Malouf",,2024,"Reproducibility, the extent to which the results of an experiment will lead to the same conclusion each time the experiment is performed, and robustness, the extent to which an experiment is generalizable to other situations, are essential indicators of the reliability of a study. Unfortunately, many studies that have sought to replicate published work reveal that most results cannot be reproduced. A great deal has been written about ""reproducibility crisis"" over the course of the last decade, yet the problem persists. Ongoing efforts to develop best practices for experimental design, statistical analysis, and the handling of data and other computational resources have not resolved the problem. Best practices are often not followed. It is not sufficient to train bioinformatics students to understand and implement these practices. Many bioinformatics studies involve scientists with a variety of specialties who all contribute data for the bioinformatician to analyze. Therefore, in order to solve the reproducibility crisis, bioinformaticians need communicate well with their colleagues who have different training backgrounds and convince them to follow best practices. This paper describes a course that teaches the best practices for research methodology and data handling, the ability to assess the quality of research results and data produced by others, and the leadership skills to influence others to perform research well."
Memory-efficient sequential pattern mining with hybrid tries,"Hosseininasab, Amin and Van Hoeve, Willem-Jan and Cire, Andre A.",,2024,"This paper develops a memory-efficient approach for Sequential Pattern Mining (SPM), a fundamental topic in knowledge discovery that faces a well-known memory bottleneck for large data sets. Our methodology involves a novel hybrid trie data structure that exploits recurring patterns to compactly store the data set in memory; and a corresponding mining algorithm designed to effectively extract patterns from this compact representation. Numerical results on small to medium-sized real-life test instances show an average improvement of 85% in memory consumption and 49% in computation time compared to the state of the art. For large data sets, our algorithm stands out as the only capable SPM approach within 256GB of system memory, potentially saving 1.7TB in memory consumption."
Penalized overdamped and underdamped Langevin Monte Carlo algorithms for constrained sampling,"G\""{u}rb\""{u}zbalaban, Mert and Hu, Yuanhan and Zhu, Lingjiong",,2024,"We consider the constrained sampling problem where the goal is to sample from a target distribution π(x) ∝ e-f(x) when x is constrained to lie on a convex body C ⊂ ℝd. Motivated by penalty methods from continuous optimization, we propose and study penalized Langevin Dynamics (PLD) and penalized underdamped Langevin Monte Carlo (PULMC) methods for constrained sampling that convert the constrained sampling problem into an unconstrained sampling problem by introducing a penalty function for constraint violations. When f is smooth and gradients of f are available, we show \~{O} (d/ε-10) iteration complexity for PLD to sample the target up to an ε-error where the error is measured in terms of the total variation distance and \~{O}(·) hides some logarithmic factors. For PULMC, we improve this result to \~{O} (√d/ε7) when the Hessian of f is Lipschitz and the boundary of C is sufficiently smooth. To our knowledge, these are the first convergence rate results for underdamped Langevin Monte Carlo methods in the constrained sampling setting that can handle non-convex choices of f and can provide guarantees with the best dimension dependency among existing methods for constrained sampling when the gradients are deterministically available. We then consider the setting where only unbiased stochastic estimates of the gradients of f are available, motivated by applications to largescale Bayesian learning problems. We propose PSGLD and PSGULMC methods that are variants of PLD and PULMC that can handle stochastic gradients and that are scaleable to large datasets without requiring Metropolis-Hasting correction steps. For PSGLD and PSGULMC, when f is strongly convex and smooth, we obtain an iteration complexity of \~{O} (d/ε18) and \~{O} (d√d/ε39) respectively in the 2-Wasserstein distance. For the more general case, when f is smooth and f can be non-convex, we also provide finite-time performance bounds and iteration complexity results. Finally, we illustrate the performance of our algorithms on Bayesian LASSO regression and Bayesian constrained deep learning problems."
OmniSafe: an infrastructure for accelerating safe reinforcement learning research,"Ji, Jiaming and Zhou, Jiayi and Zhang, Borong and Dai, Juntao and Pan, Xuehai and Sun, Ruiyang and Huang, Weidong and Geng, Yiran and Liu, Mickel and Yang, Yaodong",,2024,"AI systems empowered by reinforcement learning (RL) algorithms harbor the immense potential to catalyze societal advancement, yet their deployment is often impeded by significant safety concerns. Particularly in safety-critical applications, researchers have raised concerns about unintended harms or unsafe behaviors of unaligned RL agents. The philosophy of safe reinforcement learning (SafeRL) is to align RL agents with harmless intentions and safe behavioral patterns. In SafeRL, agents learn to develop optimal policies by receiving feedback from the environment, while also fulfilling the requirement of minimizing the risk of unintended harm or unsafe behavior. However, due to the intricate nature of SafeRL algorithm implementation, combining methodologies across various domains presents a formidable challenge. This had led to an absence of a cohesive and efficacious learning framework within the contemporary SafeRL research milieu. In this work, we introduce a foundational framework designed to expedite SafeRL research endeavors. Our comprehensive framework encompasses an array of algorithms spanning different RL domains and places heavy emphasis on safety elements. Our efforts are to make the SafeRL-related research process more streamlined and efficient, therefore facilitating further research in AI safety. Our project is released at: https://github.com/PKU-Alignment/omnisafe."
Adapt DevOps Method to Information Systems Capstone Course Projects,"Zhao, Gary Yu and Tu, Cindy Zhiling",,2025,This paper explores the design of a DevOps-based information systems (IS) capstone course in a Master of Science in Information Systems program. The researchers aim to investigate this teaching case to gather direct student feedback and provide insights for instructors to adapt the DevOps method in IS capstone projects. The study addresses the research questions and identifies key findings through inductive content analysis of 32 student feedback responses from two consecutive semesters. The preliminary results contribute to the field of information systems education and encourage more instructors to engage with DevOps-based IS capstone projects.
The Impact of Course Modality and Size on Learning Outcomes: Applying IaC Principles in IS/Cyber Graduate Course Design,"Szakonyi, Annamaria",,2025,"This study explores the impact of class size and course modality on student learning outcomes, using Infrastructure as Code (IaC) principles to design Information Systems and Cybersecurity graduate courses. To address challenges like larger class sizes, diverse formats, and AI reliance, IaC principles such as scalability, modularity, and repeatability were applied to course design. Tools like LMS blueprints and modular templates ensured consistency across online and in-person formats. Analysis of student feedback and grades showed that smaller classes and individual assignments improved outcomes, while group work fostered collaboration. The findings suggest that IaC-inspired strategies can improve scalability and quality in graduate education."
System and User Strategies to Repair Conversational Breakdowns of Spoken Dialogue Systems: A Scoping Review,"Alghamdi, Essam and Halvey, Martin and Nicol, Emma",,2024,"Spoken Dialogue Systems (SDSs) are critical in facilitating natural and efficient human-machine interaction through speech. SDSs frequently encounter challenges in managing complex dialogues, resulting in communication breakdowns, which include misunderstandings— where the system misunderstands user input— and non-understandings— where the system fails to interpret the input at all. Strategies to repair these breakdowns have been investigated across multiple disciplines; despite this interest, the findings from these studies are inconsistent and hinder comparative analysis due to the use of diverse methodologies and terminologies. To address this gap, this scoping review systematically examines SDS and user repair strategies within a broad spectrum of literature. Based on 36 papers out of 818 found, we provide two comprehensive frameworks: one categorising SDS system-repair strategies into six distinct categories and the other user-repair strategies into five categories. Our analysis reveals a disparity in the literature’s focus on repair strategies, highlighting, in particular, the lack of research on less explored strategies, such as Information and Disclosure repair strategies, providing potential avenues for future research directions in this area."
Research on the application of upper limb assisted exoskeleton: A systematic review,"Hao, Yuyang and Wang, Xupeng and Tang, Xinyao and Liu, Xiaoyi and Liu, Hongyan and Cheng, Xinran",,2025,"In recent years, work-related musculoskeletal disorders (WMSDs) have significantly impacted the quality of the labor force in China. Studies have shown that using upper limb exoskeletons (ULEs) can effectively reduce the muscular burden of shoulder-intensive tasks and extend working life. This reserch focuses on the research progress and outcomes of unpowered ULEs in the industrial sector. A systematic review was conducted using the PRISMA method, analyzing studies from 2019 to 2024. It was found that ULE systems face challenges, particularly in achieving effective human-shoulder coupling. Proposed solutions include increasing shoulder degrees of freedom and replacing rigid structures with flexible materials. Current research limitations include an imbalance in the number of male and female subjects and insufficient quantification of the additional burden imposed by wearing the exoskeleton systems. Future research should prioritize the optimization of mechanical structures and the development of flexible composite materials."
Application and research for digital delivery of bridge engineering design-construction model and data transfer based on IDM,"Zhou, Binghao and Li, Liang and Li, Dengfeng and Jiang, Haifeng and Zhang, Yexing",,2025,"In response to the inconsistent delivery results, information depth, model formats, accuracy, and fragmentation in the implementation process of digital delivery in bridge engineering, a set of standards for the transmission and delivery of design and construction data for bridge projects was developed based on the standard specifications of information data in the application process of BIM technology in bridge engineering, referring to ISO 19650 Information Delivery Manual. Taking a certain bridge project as an example, the application practice of information model deepening design, delivery preparation, deliverables, delivery coordination, delivery, and review was carried out. The results indicate that the transmission and delivery of design and construction data for bridge engineering should focus on the deepening design of concrete structures, steel structures, reinforcement, and temporary works, which can effectively promote BIM collaboration among multiple stakeholders in the bridge engineering BIM system and the continuous accumulation and transmission of data throughout the lifecycle."
Research on the Competency of Talents in Big Data Management and Application Based on Text Mining,"Wu, Na",,2025,"In the digital and intelligent era, new technologies such as big data, artificial intelligence, cloud computing, 5G, and the Internet of Things are constantly developing, and the demand for talents in data management and application is increasing day by day. This paper aims to explore the market's demand for talents in big data management and application, thereby providing directions and ideas for talent cultivation in schools. This paper crawls recruitment information related to big data management and application from&nbsp;51job.com, conducts text mining on the collected 1,710 pieces of data, and uses the LDA topic analysis and TF-IDF model to analyze the knowledge and skills that need to be mastered. The research findings show that the knowledge and skill requirements for talents in big data management and application reflect cross-disciplinary commonalities and skill diversity, mainly including professional skills such as data analysis, data mining, databases, visualization, modeling, and algorithms, as well as core project management capabilities."
Research on the Construction of Financial Intelligence Systems of Colleges and Universities Based on the Integration of Industry and Finance,"Zhuo, Yue and Xiong, Yan and Liu, Xiaojuan and Li, Yiguo",,2025,"Information technology in colleges and universities has emerged rapidly with the development of the information age and has become a development requirement for the modernization of college education. This article focuses on the integration of artificial intelligence technology and the construction of a university financial system, with A college for example, constructed based on the fusion of industry and finance financial intelligence management architecture of colleges and universities; the use of the actual business scenarios on the intelligent financial management module for the Application analysis of financial intelligent management module using real business scenarios; finally puts forward the application suggestions for the development of financial intelligence in colleges and universities."
Research on the application of large language model to financial digital literacy education,"Chen, Yanbing",,2025,"This study investigates the integration of large models into financial digital literacy education in Xi 'an Eurasia University by means of questionnaire. The results show that the large model helps to transcend the limitations of teachers and time and space, and helps to improve students' financial inclusion literacy. This study can be used as a reference for other schools to integrate large models into the classroom and for education equality in poor areas."
Insights into Car Sharing Relocation Policies Using a Simulation-Optimization Approach,"El-Banna, Mahmoud and Albdour, Amani",,2025,"One-way carsharing allows customers to pick up a vehicle from one location and drop it off at another one. While this approach is gaining acceptance over two-way or free-floating carsharing for small populations, it suffers from vehicle imbalances: excess vehicles at some stations and shortages at others. Proper investigations are necessary to minimize these imbalances. This paper compares two relocation policies in a Jordanian pilot case study using discrete event simulation: user-based (adjusting service prices to influence demand) and staff-based (hiring external resources). Results show that the staff-based policy outperformed the user-based policy by 55.4 % in vehicle utilization and by 3.4 % in cycle service level. However, the user-based policy achieved higher overall gains."
The Origin and Opportunities of Developers' Perceived Code Accountability in Open Source AI Software Development,"Bartsch, Sebastian Clemens and Lother, Moritz and Schmidt, Jan-Hendrik and Adam, Martin and Benlian, Alexander",,2025,"Open source (OS) software projects in artificial intelligence (AI), such as TensorFlow and scikit-learn, depend on developers' continuous, voluntary code contributions. However, recent security incidents highlighted substantial risks in such software, requiring examinations of factors motivating developers to continuously contribute high-quality code (i.e., providing secure and reliable code fulfilling its functions). Prior research suggests code accountability (i.e., requirements to explain and justify contributed code) to improve code quality, enforced through external accountability mechanisms such as sanctions and rewards. However, the OS domain often lacks such mechanisms, questioning whether and how code accountability arises in this domain and how it affects code contributions. To address these questions, we conducted 26 semi-structured interviews with developers contributing to OS AI software projects. Our findings reveal that despite the absence of external accountability mechanisms, system-, project-, and individual-related factors evoke developers' perceived code accountability. Notably, we discovered a trade-off as high perceived code accountability is associated with higher code quality but discourages developers from participating in OS AI software projects. Overall, this study contributes to understanding the nuanced roles of perceived code accountability in continuously contributing high-quality code without external accountability mechanisms and highlights the complex trade-offs developers face in OS AI software projects."
Gender in Pixels: Pathways to Non-Binary Representation in Computer Vision,"Beretta, Elena",,2025,"In the field of Computer Vision (CV), the study of bias, including gender bias, has received a significant area of attention in recent years. However, these studies predominantly operate within a binary, cisnormative framework, often neglecting the complexities of non-binary gender identities. To date, there is no comprehensive analysis of how CV is addressing the mitigation of bias for non-binary individuals or how it seeks solutions that transcend a binary view of gender. This systematic scoping review aims to fill this gap by analyzing over 60 papers that delve into gender biases in CV, with a particular emphasis on non-binary perspectives. Our findings indicate that despite the increasing recognition of gender as a multifaceted and complex construct, practical applications of this understanding in CV remain limited and fragmented. The review critically examines the foundational research critiquing the binarism in CV and explores emerging approaches that challenge and move beyond this limited perspective. We highlight innovative solutions, including algorithmic adaptations and the creation of more inclusive and diverse datasets. Furthermore, the study emphasizes the importance of integrating gender theory into CV practices to develop more accurate and representative models. Our recommendations advocate for interdisciplinary collaboration, particularly with Gender Studies, to foster a more nuanced understanding of gender in CV. This study serves as a pivotal step towards redefining gender representation in CV, encouraging researchers and practitioners to embrace and incorporate a broader spectrum of gender identities in their work."
APPRAISE: a Governance Framework for Innovation with Artificial Intelligence Systems,"Dey, Diptish and Bhaumik, Debarati",,2025,"As artificial intelligence (AI) systems increasingly impact society, the EU Artificial Intelligence Act (AIA) is the first legislative attempt to regulate AI systems. This paper proposes a governance framework for organizations innovating with AI systems. Building upon secondary research, the framework aims at driving a balance between four types of pressures that organizations, innovating with AI, experience, and thereby creating responsible value. These pressures encompass AI/technology, normative, value creation, and regulatory aspects. The framework is partially validated through primary research in two phases. In the first phase, a conceptual model is proposed that measures the extent to which organizational tasks result in AIA compliance, using elements from the AIA as mediators and strategic variables such as organization size, extent of outsourcing, and offshoring as moderators. 34 organizations in the Netherlands are surveyed to test the conceptual model. The average actual compliance score of the 34 participants is low, and most participants exaggerate their compliance. Organization size is found to have significant impact on AIA compliance. In phase 2, two case studies are conducted with the purpose of generating in-depth insights to validate the proposed framework. The case studies confirm the interplay of the four pressures on organizations innovating with AI, and furthermore substantiate the governance framework."
"Medical AI, Categories of Value Conflict, and Conflict Bypasses","Victor, Gavin and B\'{e}lisle-Pipon, Jean-Christophe",,2025,"It is becoming clear that, in the process of aligning AI with human values, one glaring ethical problem is that of value conflict. It is not obvious what we should do when two compelling values (such as autonomy and safety) come into conflict with one another in the design or implementation of a medical AI technology. This paper shares findings from a scoping review at the intersection of three concepts---AI, moral value, and health---that have to do with value conflict and arbitration. The paper looks at some important and unique cases of value conflict, and then describes three possible categories of value conflict: personal value conflict, interpersonal or intercommunal value conflict, and definitional value conflict. It then describes three general paths forward in addressing value conflict: additional ethical theory, additional empirical evidence, and bypassing the conflict altogether. Finally, it reflects on the efficacy of these three paths forward as ways of addressing the three categories of value conflict, and motions toward what is needed for better approaching value conflicts in medical AI."
Testing regex generalizability and its implications: a large-scale many-language measurement study,"Davis, James C. and Moyer, Daniel and Kazerouni, Ayaan M. and Lee, Dongyoon",10.1109/ASE.2019.00048,2020,"The regular expression (regex) practices of software engineers affect the maintainability, correctness, and security of their software applications. Empirical research has described characteristics like the distribution of regex feature usage, the structural complexity of regexes, and worst-case regex match behaviors. But researchers have not critically examined the methodology they follow to extract regexes, and findings to date are typically generalized from regexes written in only 1-2 programming languages. This is an incomplete foundation.Generalizing existing research depends on validating two hypotheses: (1) Various regex extraction methodologies yield similar results, and (2) Regex characteristics are similar across programming languages. To test these hypotheses, we defined eight regex metrics to capture the dimensions of regex representation, string language diversity, and worst-case match complexity. We report that the two competing regex extraction methodologies yield comparable corpuses, suggesting that simpler regex extraction techniques will still yield sound corpuses. But in comparing regexes across programming languages, we found significant differences in some characteristics by programming language. Our findings have bearing on future empirical methodology, as the programming language should be considered, and generalizability will not be assured. Our measurements on a corpus of 537,806 regexes can guide data-driven designs of a new generation of regex tools and regex engines.""There are more things in heaven and earth, Horatio, Than are dreamt of in your philosophy.""-Hamlet"
Predicting licenses for changed source code,"Liu, Xiaoyu and Huang, LiGuo and Ge, Jidong and Ng, Vincent",10.1109/ASE.2019.00070,2020,"Open source software licenses regulate the circumstances under which software can be redistributed, reused and modified. Ensuring license compatibility and preventing license restriction conflicts among source code during software changes are the key to protect their commercial use. However, selecting the appropriate licenses for software changes requires lots of experience and manual effort that involve examining, assimilating and comparing various licenses as well as understanding their relationships with software changes. Worse still, there is no state-of-the-art methodology to provide this capability. Motivated by this observation, we propose in this paper Automatic License Prediction (ALP), a novel learning-based method and tool for predicting licenses as software changes. An extensive evaluation of ALP on predicting licenses in 700 open source projects demonstrate its effectiveness: ALP can achieve not only a high overall prediction accuracy (92.5% in micro F1 score) but also high accuracies across all license types."
Empirical evaluation of the impact of class overlap on software defect prediction,"Gong, Lina and Jiang, Shujuan and Wang, Rongcun and Jiang, Li",10.1109/ASE.2019.00071,2020,"Software defect prediction (SDP) utilizes the learning models to detect the defective modules in project, and their performance depends on the quality of training data. The previous researches mainly focus on the quality problems of class imbalance and feature redundancy. However, training data often contains some instances that belong to different class but have similar values on features, and this leads to class overlap to affect the quality of training data. Our goal is to investigate the impact of class overlap on software defect prediction. At the same time, we propose an improved K-Means clustering cleaning approach (IKMCCA) to solve both the class overlap and class imbalance problems. Specifically, we check whether K-Means clustering cleaning approach (KMCCA) or neighborhood cleaning learning (NCL) or IKMCCA is feasible to improve defect detection performance for two cases (i) within-project defect prediction (WPDP) (ii) cross-project defect prediction (CPDP). To have an objective estimate of class overlap, we carry out our investigations on 28 open source projects, and compare the performance of state-of-the-art learning models for the above-mentioned cases by using IKMCCA or KMCCA or NCL VS. without cleaning data. The experimental results make clear that learning models obtain significantly better performance in terms of balance, Recall and AUC for both WPDP and CPDP when the overlapping instances are removed. Moreover, it is better to consider both class overlap and class imbalance."
RefBot: intelligent software refactoring bot,"Alizadeh, Vahid and Ouali, Mohamed Amine and Kessentini, Marouane and Chater, Meriem",10.1109/ASE.2019.00081,2020,"The adoption of refactoring techniques for continuous integration received much less attention from the research community comparing to root-canal refactoring to fix the quality issues in the whole system. Several recent empirical studies show that developers, in practice, are applying refactoring incrementally when they are fixing bugs or adding new features. There is an urgent need for refactoring tools that can support continuous integration and some recent development processes such as DevOps that are based on rapid releases. Furthermore, several studies show that manual refactoring is expensive and existing automated refactoring tools are challenging to configure and integrate into the development pipelines with significant disruption cost.In this paper, we propose, for the first time, an intelligent software refactoring bot, called RefBot. Integrated into the version control system (e.g. GitHub), our bot continuously monitors the software repository, and it is triggered by any ""open"" or ""merge"" action on pull requests. The bot analyzes the files changed during that pull request to identify refactoring opportunities using a set of quality attributes then it will find the best sequence of refactorings to fix the quality issues if any. The bot recommends all these refactorings through an automatically generated pull-request. The developer can review the recommendations and their impacts in a detailed report and select the code changes that he wants to keep or ignore. After this review, the developer can close and approve the merge of the bot's pull request. We quantitatively and qualitatively evaluated the performance and effectiveness of RefBot by a survey conducted with experienced developers who used the bot on both open source and industry projects."
Code-first model-driven engineering: on the agile adoption of MDE tooling,"Boronat, Artur",10.1109/ASE.2019.00086,2020,"Domain models are the most important asset in widely accepted software development approaches, like Domain-Driven Design (DDD), yet those models are still implicitly represented in programs. Model-Driven Engineering (MDE) regards those models as representable entities that are amenable to automated analysis and processing, facilitating quality assurance while increasing productivity in software development processes. Although this connection is not new, very few approaches facilitate adoption of MDE tooling without compromising existing value, their data. Moreover, switching to MDE tooling usually involves re-engineering core parts of an application, hindering backward compatibility and, thereby, continuous integration, while requiring an up-front investment in training in specialized modeling frameworks. In those approaches that overcome the previous problem, there is no clear indication - from a quantitative point of view - of the extent to which adopting state-of-the-art MDE practices and tooling is feasible or advantageous.In this work, we advocate a code-first approach to modeling through an approach for applying MDE techniques and tools to existing object-oriented software applications that fully preserves the semantics of the original application, which need not be modified. Our approach consists both of a semi-automated method for specifying explicit view models out of existing object-oriented applications and of a conservative extension mechanism that enables the use of such view models at run time, where view model queries are resolved on demand and view model updates are propagated incrementally to the original application. This mechanism enables an iterative, flexible application of MDE tooling to software applications, where metamodels and models do not exist explicitly. An evaluation of this extension mechanism, implemented for Java applications and for view models atop the Eclipse Modeling Framework (EMF), has been conducted with an industry-targeted benchmark for decision support systems, analyzing performance and scalability of the synchronization mechanism. Backward propagation of large updates over very large views is instant."
Developer Reputation Estimator (DRE),"Amreen, Sadika and Karnauch, Andrey and Mockus, Audris",10.1109/ASE.2019.00107,2020,"Evidence shows that developer reputation is extremely important when accepting pull requests or resolving reported issues. It is particularly salient in Free/Libre Open Source Software since the developers are distributed around the world, do not work for the same organization and, in most cases, never meet face to face. The existing solutions to expose developer reputation tend to be forge specific (GitHub), focus on activity instead of impact, do not leverage social or technical networks, and do not correct often misspelled developer identities. We aim to remedy this by amalgamating data from all public Git repositories, measuring the impact of developer work, expose developer's collaborators, and correct notoriously problematic developer identity data. We leverage World of Code (WoC), a collection of an almost complete (and continuously updated) set of Git repositories by first allowing developers to select which of the 34 million(M) Git commit author IDs belong to them and then generating their profiles by treating the selected collection of IDs as that single developer. As a side-effect, these selections serve as a training set for a supervised learning algorithm that merges multiple identity strings belonging to a single individual. As we evaluate the tool and the proposed impact measure, we expect to build on these findings to develop reputation badges that could be associated with pull requests and commits so developers could easier trust and prioritize them."
A journey towards providing intelligence and actionable insights to development teams in software delivery,"Sharma, Vibhu Saujanya and Mehra, Rohit and Podder, Sanjay and Burden, Adam P.",10.1109/ASE.2019.00142,2020,"For delivering high-quality artifacts within the budget and on schedule, software delivery teams ideally should have a holistic and in-process view of the current health and future trajectory of the project. However, such insights need to be at the right level of granularity and need to be derived typically from a heterogeneous project environment, in a way that helps development team members with their tasks at hand. Due to client mandates, software delivery project environments employ many disparate tools and teams tend to be distributed, thus making the relevant information retrieval, insight generation, and developer intelligence augmentation process fairly complex. In this paper, we discuss our journey in this area spanning across facets like software project modelling and new development metrics, studying developer priorities, adoption of new metrics, and different approaches of developer intelligence augmentation. Finally, we present our exploration of new immersive technologies for human-centered software engineering."
Why do developers remove lambda expressions in Java?,"Zheng, Mingwei and Yang, Jun and Wen, Ming and Zhu, Hengcheng and Liu, Yepang and Jin, Hai",10.1109/ASE51524.2021.9678600,2022,"Java 8 has introduced lambda expressions, a core feature of functional programming. Since its introduction, there is an increasing trend of lambda adoptions in Java projects. Developers often adopt lambda expressions to simplify code, avoid code duplication or simulate other functional features. However, we observe that lambda expressions can also incur different types of side effects (i.e., performance issues and memory leakages) or even severe bugs, and developers also frequently remove lambda expressions in their implementations. Consequently, the advantages of utilizing lambda expressions can be significantly compromised by the collateral side effects. In this study, we present the first large-scale, quantitative and qualitative empirical study to characterize and understand inappropriate usages of lambda expressions. Particularly, we summarized seven main reasons for the removal of lambdas as well as seven common migration patterns. For instance, we observe that lambdas using customized functional interfaces are more likely to be removed by developers. Moreover, from a complementary perspective, we performed a user study over 30 developers to seek the underlying reasons why they remove lambda expressions in practice. Finally, based on our empirical results, we made suggestions on scenarios to avoid lambda usages for Java developers and also pointed out future directions for researchers."
DeepCVA: automated commit-level vulnerability assessment with deep multi-task learning,"Le, Triet Huynh Minh and Hin, David and Croft, Roland and Babar, M. Ali",10.1109/ASE51524.2021.9678622,2022,"It is increasingly suggested to identify Software Vulnerabilities (SVs) in code commits to give early warnings about potential security risks. However, there is a lack of effort to assess vulnerability-contributing commits right after they are detected to provide timely information about the exploitability, impact and severity of SVs. Such information is important to plan and prioritize the mitigation for the identified SVs. We propose a novel Deep multi-task learning model, DeepCVA, to automate seven Commit-level Vulnerability Assessment tasks simultaneously based on Common Vulnerability Scoring System (CVSS) metrics. We conduct large-scale experiments on 1,229 vulnerability-contributing commits containing 542 different SVs in 246 real-world software projects to evaluate the effectiveness and efficiency of our model. We show that DeepCVA is the best-performing model with 38% to 59.8% higher Matthews Correlation Coefficient than many supervised and unsupervised baseline models. DeepCVA also requires 6.3 times less training and validation time than seven cumulative assessment models, leading to significantly less model maintenance cost as well. Overall, DeepCVA presents the first effective and efficient solution to automatically assess SVs early in software systems."
Is historical data an appropriate benchmark for reviewer recommendation systems? a case study of the gerrit community,"Gauthier, Ian X. and Lamothe, Maxime and Mussbacher, Gunter and McIntosh, Shane",10.1109/ASE51524.2021.9678640,2022,"Reviewer recommendation systems are used to suggest community members to review change requests. Like several other recommendation systems, it is customary to evaluate recommendations using held out historical data. While history-based evaluation makes pragmatic use of available data, historical records may be: (1) overly optimistic, since past assignees may have been suboptimal choices for the task at hand; or (2) overly pessimistic, since ""incorrect"" recommendations may have been equal (or even better) choices.In this paper, we empirically evaluate the extent to which historical data is an appropriate benchmark for reviewer recommendation systems. We replicate the cHRev and WLRRec approaches and apply them to 9,679 reviews from the Gerrit open source community. We then assess the recommendations with members of the Gerrit reviewing community using quantitative methods (personalized questionnaires about their comfort level with tasks) and qualitative methods (semi-structured interviews).We find that history-based evaluation is far more pessimistic than optimistic in the context of Gerrit review recommendations. Indeed, while 86% of those who had been assigned to a review in the past felt comfortable handling the review, 74% of those labelled as incorrect recommendations also felt that they would have been comfortable reviewing the changes. This indicates that, on the one hand, when reviewer recommendation systems recommend the past assignee, they should indeed be considered correct. Yet, on the other hand, recommendations labelled as incorrect because they do not match the past assignee may have been correct as well.Our results suggest that current reviewer recommendation evaluations do not always model the reality of software development. Future studies may benefit from looking beyond repository data to gain a clearer understanding of the practical value of proposed recommendations."
PyExplainer: explaining the predictions of just-in-time defect models,"Pornprasit, Chanathip and Tantithamthavorn, Chakkrit and Jiarpakdee, Jirayus and Fu, Michael and Thongtanunam, Patanamon",10.1109/ASE51524.2021.9678763,2022,"Just-In-Time (JIT) defect prediction (i.e., an AI/ML model to predict defect-introducing commits) is proposed to help developers prioritize their limited Software Quality Assurance (SQA) resources on the most risky commits. However, the explainability of JIT defect models remains largely unexplored (i.e., practitioners still do not know why a commit is predicted as defect-introducing). Recently, LIME has been used to generate explanations for any AI/ML models. However, the random perturbation approach used by LIME to generate synthetic neighbors is still suboptimal, i.e., generating synthetic neighbors that may not be similar to an instance to be explained, producing low accuracy of the local models, leading to inaccurate explanations for just-in-time defect models. In this paper, we propose PyExplainer---i.e., a local rule-based model-agnostic technique for generating explanations (i.e., why a commit is predicted as defective) of JIT defect models. Through a case study of two open-source software projects, we find that our PyExplainer produces (1) synthetic neighbors that are 41%-45% more similar to an instance to be explained; (2) 18%-38% more accurate local models; and (3) explanations that are 69%-98% more unique and 17%-54% more consistent with the actual characteristics of defect-introducing commits in the future than LIME (a state-of-the-art model-agnostic technique). This could help practitioners focus on the most important aspects of the commits to mitigate the risk of being defect-introducing. Thus, the contributions of this paper build an important step towards Explainable AI for Software Engineering, making software analytics more explainable and actionable. Finally, we publish our PyExplainer as a Python package to support practitioners and researchers (https://github.com/awsm-research/PyExplainer)."
Modeling team dynamics for the characterization and prediction of delays in user stories,"Kula, Elvan and van Deursen, Arie and Gousios, Georgios",10.1109/ASE51524.2021.9678939,2022,"In agile software development, proper team structures and effort estimates are crucial to ensure the on-time delivery of software projects. Delivery performance can vary due to the influence of changes in teams, resulting in team dynamics that remain largely unexplored. In this paper, we explore the effects of various aspects of teamwork on delays in software deliveries. We conducted a case study at ING and analyzed historical log data from 765,200 user stories and 571 teams to identify team factors characterizing delayed user stories. Based on these factors, we built models to predict the likelihood and duration of delays in user stories. The evaluation results show that the use of team-related features leads to a significant improvement in the predictions of delay, achieving on average 74%-82% precision, 78%-86% recall and 76%-84% F-measure. Moreover, our results show that team-related features can help improve the prediction of delay likelihood, while delay duration can be explained exclusively using them. Finally, training on recent user stories using a sliding window setting improves the predictive performance; our predictive models perform significantly better for teams that have been stable. Overall, our results indicate that planning in agile development settings can be significantly improved by incorporating team-related information and incremental learning methods into analysis/predictive models."
Adversarial attacks to API recommender systems: time to wake up and smell the coffee?,"Nguyen, Phuong T. and Di Sipio, Claudio and Di Rocco, Juri and Di Penta, Massimiliano and Di Ruscio, Davide",10.1109/ASE51524.2021.9678946,2022,"Recommender systems in software engineering provide developers with a wide range of valuable items to help them complete their tasks. Among others, API recommender systems have gained momentum in recent years as they became more successful at suggesting API calls or code snippets. While these systems have proven to be effective in terms of prediction accuracy, there has been less attention for what concerns such recommenders' resilience against adversarial attempts. In fact, by crafting the recommenders' learning material, e.g., data from large open-source software (OSS) repositories, hostile users may succeed in injecting malicious data, putting at risk the software clients adopting API recommender systems. In this paper, we present an empirical investigation of adversarial machine learning techniques and their possible influence on recommender systems. The evaluation performed on three state-of-the-art API recommender systems reveals a worrying outcome: all of them are not immune to malicious data. The obtained result triggers the need for effective countermeasures to protect recommender systems against hostile attacks disguised in training data."
Bridging the Gap between Academia and Industry in Machine Learning Software Defect Prediction: Thirteen Considerations,"Stradowski, Szymon and Madeyski, Lech",10.1109/ASE56229.2023.00026,2024,"This experience paper describes thirteen considerations for implementing machine learning software defect prediction (ML SDP) in vivo. Specifically, we provide the following report on the ground of the most important observations and lessons learned gathered during a large-scale research effort and introduction of ML SDP to the system-level testing quality assurance process of one of the leading telecommunication vendors in the world --- Nokia. We adhere to a holistic and logical progression based on the principles of the business analysis body of knowledge: from identifying the need and setting requirements, through designing and implementing the solution, to profitability analysis, stakeholder management, and handover. Conversely, for many years, industry adoption has not kept up the pace of academic achievements in the field, despite promising potential to improve quality and decrease the cost of software products for many companies worldwide. Therefore, discussed considerations hopefully help researchers and practitioners bridge the gaps between academia and industry."
Where to Go Now? Finding Alternatives for Declining Packages in the npm Ecosystem,"Mujahid, Suhaib and Costa, Diego Elias and Abdalkareem, Rabe and Shihab, Emad",10.1109/ASE56229.2023.00119,2024,"Software ecosystems (e.g., npm, PyPI) are the backbone of modern software developments. Developers add new packages to ecosystems every day to solve new problems or provide alternative solutions, causing obsolete packages to decline in their importance to the community. Packages in decline are reused less over time and may become less frequently maintained. Thus, developers usually migrate their dependencies to better alternatives. Replacing packages in decline with better alternatives requires time and effort by developers to identify packages that need to be replaced, find the alternatives, asset migration benefits, and finally, perform the migration.This paper proposes an approach that automatically identifies packages that need to be replaced and finds their alternatives supported with real-world examples of open source projects performing the suggested migrations. At its core, our approach relies on the dependency migration patterns performed in the ecosystem to suggest migrations to other developers. We evaluated our approach on the npm ecosystem and found that 96% of the suggested alternatives are accurate. Furthermore, by surveying expert JavaScript developers, 67% of them indicate that they will use our suggested alternative packages in their future projects."
Explainable software bot contributions: case study of automated bug fixes,"Monperrus, Martin",10.1109/BotSE.2019.00010,2019,"In a software project, esp. in open-source, a contribution is a valuable piece of work made to the project: writing code, reporting bugs, translating, improving documentation, creating graphics, etc. We are now at the beginning of an exciting era where software bots will make contributions that are of similar nature than those by humans.Dry contributions, with no explanation, are often ignored or rejected, because the contribution is not understandable per se, because they are not put into a larger context, because they are not grounded on idioms shared by the core community of developers.We have been operating a program repair bot called Repairnator for 2 years and noticed the problem of ""dry patches"": a patch that does not say which bug it fixes, or that does not explain the effects of the patch on the system. We envision program repair systems that produce an ""explainable bug fix"": an integrated package of at least 1) a patch, 2) its explanation in natural or controlled language, and 3) a highlight of the behavioral difference with examples.In this paper, we generalize and suggest that software bot contributions must explainable, that they must be put into the context of the global software development conversation."
Towards an infrastructure for empirical research into software architecture: challenges and directions,"Ho-Quang, Truong and Chaudron, Michel R. V. and Robles, Gregorio and Herwanto, Guntur Budi",10.1109/ECASE.2019.00014,2019,"While the software engineering community at large has embraced empirical studies of source code, empirical studies of software architecture are lacking. In order to increase the possibility and relevance of studies into software architecture, an infrastructure for sharing empirical data on software architecture is essential. This paper contributes by discussing needs and challenges in empirical studies of software architecture. Based on lessons learned we propose CoSARI - a community-wide infrastructure for empirical research into software architecture."
What if i had no smells?,"Falessi, Davide and Russo, Barbara and Mullen, Kathleen",10.1109/ESEM.2017.14,2017,"What would have happened if I did not have any code smell? This is an interesting question that no previous study, to the best of our knowledge, has tried to answer. In this paper, we present a method for implementing a what-if scenario analysis estimating the number of defective files in the absence of smells. Our industrial case study shows that 20% of the total defective files were likely avoidable by avoiding smells. Such estimation needs to be used with the due care though as it is based on a hypothetical history (i.e., zero number of smells and same process and product change characteristics). Specifically, the number of defective files could even increase for some types of smells. In addition, we note that in some circumstances, accepting code with smells might still be a good option for a company."
Which version should be released to app store?,"Nayebi, Maleknaz and Farrahi, Homayoon and Ruhe, Guenther",10.1109/ESEM.2017.46,2017,"Background: Several mobile app releases do not find their way to the end users. Our analysis of 11,514 releases across 917 open source mobile apps revealed that 44.3% of releases created in GitHub never shipped to the app store (market). Aims: We introduce ""marketability"" of open source mobile apps as a new release decision problem. Considering app stores as a complex system with unknown treatments, we evaluate performance of predictive models and analogical reasoning for marketability decisions. Method: We performed a survey with 22 release engineers to identify the importance of marketability release decision. We compared different classifiers to predict release marketability. For guiding the transition of not successfully marketable releases into successful ones, we used analogical reasoning. We evaluated our results both internally (over time) and externally (by developers). Results: Random forest classification performed best with F1 score of 78%. Analyzing 58 releases over time showed that, for 81% of them, analogical reasoning could correctly identify changes in the majority of release attributes. A survey with seven developers showed the usefulness of our method for supporting real world decisions. Conclusions: Marketability decisions of mobile apps can be supported by using predictive analytics and by considering and adopting similar experience from the past."
The significant effects of data sampling approaches on software defect prioritization and classification,"Bennin, Kwabena Ebo and Keung, Jacky and Monden, Akito and Phannachitta, Passakorn and Mensah, Solomon",10.1109/ESEM.2017.50,2017,"Context: Recent studies have shown that performance of defect prediction models can be affected when data sampling approaches are applied to unbalanced training data for building defect prediction models. However, the magnitude (degree and power) of the effect of these sampling methods on the classification and prioritization performances of defect prediction models is still unknown. Goal: To investigate the statistical and practical significance of using resampled data for constructing defect prediction models. Method: We examine the practical effects of six data sampling methods on performances of five defect prediction models. The prediction performances of the models trained on default datasets (no sampling method) are compared with that of the models trained on resampled datasets (application of sampling methods). To decide whether the performance changes are significant or not, robust statistical tests are performed and effect sizes computed. Twenty releases of ten open source projects extracted from the PROMISE repository are considered and evaluated using the AUC, pd, pf and G-mean performance measures. Results: There are statistical significant differences and practical effects on the classification performance (pd, pf and G-mean) between models trained on resampled datasets and those trained on the default datasets. However, sampling methods have no statistical and practical effects on defect prioritization performance (AUC) with small or no effect values obtained from the models trained on the resampled datasets. Conclusions: Existing sampling methods can properly set the threshold between buggy and clean samples, while they cannot improve the prediction of defect-proneness itself. Sampling methods are highly recommended for defect classification purposes when all faulty modules are to be considered for testing."
Gender differences in self and peer assessment in a software engineering capstone course,"Bastarrica, Mar\'{\i}a Cecilia and Simmonds, Jocelyn",10.1109/GE.2019.00014,2019,"Context: Women are generally underrepresented in software development and probably their behavior is biased by the fact that they are usually a minority within teams. The Engineering School at the Universidad de Chile has put in practice a strong women recruitment program. This brought that, for the first time, women reached 20% of the students enrolled in the fifth year software engineering capstone course.Problem: More women are entering the work force but there is still certain prejudice about women performance in STEM in general, and in software development in particular, since it is perceived as a man's activity.Method: In the context of the fifth year capstone course at the CS Department of the Universidad de Chile we conducted a field study in order to analyze the progression of self and peer assessment of men and women students along one semester.Results: We found that, even though peer assessment is similar for women and men, self assessment tends to be lower for women. Also, peer assessment does not vary much along the semester, neither for men nor for women.Conclusions: Women performance in software development teams is highly regarded by teammates. However, women do not seem to be willing to acknowledge their own performance. More research is needed in order to understands the causes of this behavior."
Blueprint model: a new approach to scrum agile methodology,"Godoy, Cristiano P. and Santos, Lanier M. and Cruz, Andre F. and Zerbini, Rafael S. and Silva, Elisangela P. and Pahins, C\'{\i}cero A. L.",10.1109/ICGSE.2019.00014,2019,"Coordinating software development with teams distributed across different sites can be challenging. SIDIA is an R&amp;D institute that is responsible for implementing innovative solutions for the Brazilian and global market through research activities and development. SIDIA has Samsung Company as a strategic partner, and part of its teams work in collaboration with Samsung Mobile division, located in Korea. Both local and remote teams have different skills including software engineering, design, and quality assurance. This report focuses on the experience at designing and deploying a new software development model called Blueprint, a model based on Scrum and Kanban agile methodologies that facilitate (i) global software development, (ii) teams and tasks allocations, and (iii) effective members communication. We also report the lessons learned from adopting the Blueprint model in the development of a project."
Project work division in agile distributed student teams: who develops what?,"Bosni\'{c}, Ivana and \v{C}avrak, Igor",10.1109/ICGSE.2019.00038,2019,"This paper focuses on studying how distributed student teams organize their project work on a software product and how is their work division related to their architectural choices and geographical locations. We present our findings based on the data collected during five years of the long-lasting Distributed Software Development course running among three European universities. Student team decisions are analyzed on 19 distributed projects, carried out using the Scrum framework, and students' insights into this topic are discussed based on the data obtained from students' questionnaires and other course-related data sources. In addition, we investigate how this work division is related to the team dynamics, by analyzing teams' collaboration patterns and collaboration links between different team locations and work roles.The results show that although the students, in general, perceive the strong positive influence of architecture to the work division, younger generations of students tend to move away from dividing the work by location in order to form the sub-teams mixed by location, thus even improving the performance of their projects. A balanced collaboration, regardless of the physical location of sub-teams, might have a more beneficial effect on the quality of the resulting work than a strict work division over a distance."
PaaS - black or white: an investigation into software development model for building retail industry SaaS,"Pham, Vu Viet Hoang and Liu, Xiao and Zheng, Xi and Fu, Min and Deshpande, Sahil Vikas and Xia, Weidong and Zhou, Roger and Abdelrazek, Mohamed",10.1109/ICSE-C.2017.57,2017,"One of the most important goals for Software Engineering is that end users or those people who understand software requirements but without too much programming experience can build their software products or prototypes easily. The recent success of cloud computing has made a big step towards this goal where Platform as a Service (PaaS) can provide general and comprehensive software development services within an integrated online environment for building Software as a Service (SaaS). However, currently, most PaaS are in a ""white-box"" which still requires significant learning efforts for software developers and lets alone inexperienced project managers or end users. Therefore, it is high time that we should comprehensively investigate the challenges for PaaS and provide a suitable development model. In this paper, we firstly identify and analyze the challenges for current White-PaaS through literature review. Afterwards, employing the retail industry as a typical application domain, a novel ""Black-Box"" PaaS framework is proposed which requires much less learning time and supports much more flexible and speedy SaaS design and development."
An empirical study on female participation in software project courses,"Nguyen-Duc, Anh and Jaccheri, Letizia and Abrahamsson, Pekka",10.1109/ICSE-Companion.2019.00094,2019,"Gender issues in software engineering education are gaining research attention due to the desire to promote female participation in the field. The objective of this work is to enhance the understanding of female students' participation in software engineering projects to support gender-aware course optimization. Since 2015, we have investigated the participation of female students in terms of software engineering activities and team dynamics in a software project course that involves a real customer. We found that female students are more active with project management and requirement engineering, while they remain under-represented in highly complex or specific tasks, i.e. architecture work, and user experience design. We found no statistically significant difference in perceived team dynamics between male and female students. Insights on female project activities would facilitate the arrangement of project teams so that learning can be distributed equally across genders."
Learning to boost the efficiency of modern code review,"Heum\""{u}ller, Robert",10.1109/ICSE-Companion52605.2021.00126,2021,"Modern Code Review (MCR) is a standard in all kinds of organizations that develop software. MCR pays for itself through perceived and proven benefits in quality assurance and knowledge transfer. However, the time invest in MCR is generally substantial. The goal of this thesis is to boost the efficiency of MCR by developing AI techniques that can partially replace or assist human reviewers. The envisioned techniques distinguish from existing MCR-related AI models in that we interpret these challenges as graph-learning problems. This should allow us to use state-of-science algorithms from that domain to learn coding and reviewing standards directly from existing projects. The required training data will be mined from online repositories and the experiments will be designed to use standard, quantitative evaluation metrics. This research proposal defines the motivation, research-questions, and solution components for the thesis, and gives an overview of the relevant related work."
Towards Strengthening Software Library Interfaces with Granular and Interactive Type Migrations,"Szalay, Rich\'{a}rd",10.1109/ICSE-Companion58688.2023.00063,2023,"The interface boundaries of software projects are a crucial perimeter from both a design and security point of view. Design decisions of libraries will inadvertently affect client code which can neither legally nor technically change the library's contract. Mistakes allowed by the interface design, such as argument selection defects, can only be caught with existing tools once made. This is made worse in C++, as functions may take parameters which types are implicitly convertible to one another. Instead, I proposed a proactive step to detect and improve when a library interface exhibits properties that may lead to inadvertent misuse. Actionable fixes for the reports are possible through an interactive type refactoring method. Existing refactorings for type migration required the new types and the migration process to be well-defined in advance; otherwise, the code might be changed to a non-compilable state. This paper summarises my thesis proposal in which the problem of weak function interfaces is detected and solved by the Fictive Types method, which first ""colours"" the software project with new type information, deferring the complete rewrite once the required interface of the new type is fully explored."
"Designing Adaptive Developer-Chatbot Interactions: Context Integration, Experimental Studies, and Levels of Automation","Melo, Glaucia",10.1109/ICSE-Companion58688.2023.00064,2023,"The growing demand for software developers and the increasing development complexity have emphasized the need for support in software engineering projects. This is especially relevant in light of advancements in artificial intelligence, such as conversational systems. A significant contributor to the complexity of software development is the multitude of tools and methods used, creating various contexts in which software developers must operate. Moreover, there has been limited investigation into the interaction between context-based chatbots and software developers through experimental user studies. Assisting software developers in their work becomes essential. In particular, understanding the context surrounding software development and integrating this context into chatbots can lead to novel insight into what software developers expect concerning these human-chatbot interactions and their levels of automation. In my research, I study the design of context-based adaptive interactions between software developers and chatbots to foster solutions and knowledge to support software developers at work."
Some Investigations of Machine Learning Models for Software Defects,"Bhutamapuram, Umamaheswara Sharma",10.1109/ICSE-Companion58688.2023.00070,2023,"Software defect prediction (SDP) and software defect severity prediction (SDSP) models alleviate the burden on the testers by providing the automatic assessment of a newly-developed program in a short amount of time. The research on defect prediction or defect severity prediction is primarily focused on proposing classification frameworks or addressing challenges in developing prediction models; however, the primary yet significant gap in the literature is interpreting the predictions in terms of project objectives. Furthermore, the literature indicates that these models have poor predictive performance. In this thesis, we investigate the use of a diversity-based ensemble learning mechanism for the cross-project defect prediction (CPDP) task and self-training semi-supervised learning for the software defect severity prediction, respectively, for obtaining better prediction performances. We also propose a few project-specific performance measures to interpret the predictions in terms of project objectives (such as a reduction in expenditure, time, and failure chances). Through the empirical analysis, we observe that (1) the diversity-based ensemble learning mechanism improves the prediction performance in terms of both the traditional and proposed measures, and (2) the self-training semi-supervised learning model has a positive impact on predicting the severity of a defective module.Once a potential prediction model is developed, any software organisation may utilise its services. How can an organisation showcase their trust in the developed prediction model? To this end, we investigate the feasibility of SDP models in real-world testing environments by providing proofs using the probabilistic bounds. The proofs summarised show that even if the prediction model has a lower failure probability, the probability of obtaining fewer failures in SDP-tested software than in similar but manually tested software is still exponentially small. This result enables the researchers in SDP to avoid proposing prediction models."
The impact of retrieval direction on IR-based traceability link recovery,"Mills, Chris and Haiduc, Sonia",10.1109/ICSE-NIER.2017.14,2017,"The application of Information Retrieval (IR) techniques to software traceability link recovery has been the focus of many studies. These studies have formulated the task of establishing valid trace links between two types of software artifacts as a retrieval problem, where one type of artifacts is selected as the set of queries and the other as the corpus. Previous work selected the sets of queries and corpus artifacts for a study up front, therefore pre-imposing a retrieval direction for finding all trace links. This decision was usually made based on intuition or previous work. We argue that the choice of the query and corpus sets (i.e., retrieval direction) can significantly impact the results of IR-based traceability link recovery and should be made with context in mind, as the best choice may be dependent on the properties of each dataset. More than that, we argue that even within the same system, different traceability links may be best recovered by using different retrieval directions. In this paper we provide the first evidence to support these claims, showing that retrieval direction can have a significant impact on IR performance for traceability link recovery at both the project and individual link level. Moreover, we propose future research directions aimed at predicting the most efficient retrieval direction, as well as approaches leveraging information from both retrieval directions simultaneously."
Accelerating software engineering research adoption with analysis bots,"Beschastnikh, Ivan and Lungu, Mircea F. and Zhuang, Yanyan",10.1109/ICSE-NIER.2017.17,2017,"An important part of software engineering (SE) research is to develop new analysis techniques and to integrate these techniques into software development practice. However, since access to developers is non-trivial and research tool adoption is slow, new analyses are typically evaluated as follows: a prototype tool that embeds the analysis is implemented, a set of projects is identified, their revisions are selected, and the tool is run in a controlled environment, rarely involving the developers of the software. As a result, research artifacts are brittle and it is unclear if an analysis tool would actually be adopted.In this paper, we envision harnessing the rich interfaces provided by popular social coding platforms for automated deployment and evaluation of SE research analysis. We propose that SE analyses can be deployed as analysis bots. We focus on two specific benefits of such an approach: (1) analysis bots can help evaluate analysis techniques in a less controlled, and more realistic context, and (2) analysis bots provide an interface for developers to ""subscribe"" to new research techniques without needing to trust the implementation, the developer of the new tool, or to install the analysis tool locally. We outline basic requirements for an analysis bots platform, and present research challenges that would need to be resolved for bots to flourish."
Teaching agile model-driven engineering for cyber-physical systems,"Ringert, Jan Oliver and Rumpe, Bernhard and Schulze, Christoph and Wortmann, Andreas",10.1109/ICSE-SEET.2017.16,2017,"Agile development methods, model-driven engineering, and cyber-physical systems are important topics in software engineering education. It is not obvious how to teach their combination while respecting individual challenges posed to students and educators. We have devised a software project class for teaching the agile MDE for CPS. The project class was held in three different semesters. In this paper, we report on the setup of our exploratory study and its goals for teaching. We base our evaluation and insights on interviews and questionnaires. Our results show the feasibility of combination of agile MDE for CPS but also the challenges this combination poses to students and educators."
Teaching internet of things (IoT) literacy: a systems engineering approach,"Silvis-Cividjian, Natalia",10.1109/ICSE-SEET.2019.00014,2019,"The Internet of Things (IoT) invades our world with billions of smart, interconnected devices, all programmed to make our lives easier. For educators, teaching such a vast and dynamic field is both a necessity and a challenge. IoT-relevant topics such as programming, hardware, networking and artificial intelligence are already covered in core computing curricula. Does this mean that fresh graduates are well prepared to tackle complex IoT problems? Unfortunately, nothing could be further from the truth. The problem is that IoT devices are complex systems, where software, hardware, and humans interact with each other. From this interaction, unique behavior and hazardous situations can emerge that might easily stay undetected, unless systems are analyzed as a whole. This paper presents two differently flavored courses that teach IoT using a holistic, system-centric approach. The first is a broad introduction to Pervasive Computing, focused on the intelligence of ""Things"". The second is an advanced course that zooms on the process of testing a software-intensive system. The key characteristics of our approach are: (1) teaching only the bare essentials (topics needed for end-to-end engineering of a smart system), (2) a strong, hands-on project component, using microcontroller-based miniature systems, inspired by real-life, and (3) a rich partnership with industry and academic idea incubators. Positive student evaluations gathered during the last five years demonstrate that such an approach brings engagement, self-confidence and realism in IoT classrooms. We believe that this success can be replicated in other courses, by shifting the focus on different IoT-relevant aspects."
Teaching MLOps in Higher Education through Project-Based Learning,"Lanubile, Filippo and Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Quaranta, Luigi",10.1109/ICSE-SEET58685.2023.00015,2023,"Building and maintaining production-grade ML-enabled components is a complex endeavor that goes beyond the current approach of academic education, focused on the optimization of ML model performance in the lab. In this paper, we present a project-based learning approach to teaching MLOps, focused on the demonstration and experience with emerging practices and tools to automatize the construction of ML-enabled components. We examine the design of a course based on this approach, including laboratory sessions that cover the end-to-end ML component life cycle, from model building to production deployment. Moreover, we report on preliminary results from the first edition of the course. During the present year, an updated version of the same course is being delivered in two independent universities; the related learning outcomes will be evaluated to analyze the effectiveness of project-based learning for this specific subject."
Overcoming Challenges in DevOps Education through Teaching Methods,"Ferino, Samuel and Fernandes, Marcelo and Cirilo, Elder and Agnez, Lucas and Batista, Bruno and Kulesza, Uir\'{a} and Aranha, Eduardo and Treude, Christoph",10.1109/ICSE-SEET58685.2023.00022,2023,"DevOps is a set of practices that deals with coordination between development and operation teams and ensures rapid and reliable new software releases that are essential in industry. DevOps education assumes the vital task of preparing new professionals in these practices using appropriate teaching methods. However, there are insufficient studies investigating teaching methods in DevOps. We performed an analysis based on interviews to identify teaching methods and their relationship with DevOps educational challenges. Our findings show that project-based learning and collaborative learning are emerging as the most relevant teaching methods."
Persona-Based Assessment of Software Engineering Student Research Projects: An Experience Report,"Arora, Chetan and Tubino, Laura and Cain, Andrew and Lee, Kevin and Malhotra, Vasudha",10.1109/ICSE-SEET58685.2023.00025,2023,"Students enrolled in software engineering degrees are generally required to undertake a research project in their final year through which they demonstrate the ability to conduct research, communicate outcomes, and build in-depth expertise in an area. Assessment in these projects typically involves evaluating the product of their research via a thesis or a similar artifact. However, this misses a range of other factors that go into producing successful software engineers and researchers. Incorporating aspects such as process, attitudes, project complexity, and supervision support into the assessment can provide a more holistic evaluation of the performance likely to better align with the intended learning outcomes. In this paper, we present on our experience of adopting an innovative assessment approach to enhance learning outcomes and research performance in our software engineering research projects. Our approach adopted a task-oriented approach to portfolio assessment that incorporates student personas, frequent formative feedback, delayed summative grading, and standards-aligned outcomes-based assessment. We report upon our continuous improvement journey in adapting tasks and criteria to address the challenges of assessing student research projects. Our lessons learnt demonstrate the value of personas to guide the development of holistic rubrics, giving meaning to grades and focusing staff and student attention on attitudes and skills rather than a product only."
"""Software is the Easy Part of Software Engineering"" - Lessons and Experiences from A Large-Scale, Multi-Team Capstone Course","Li, Ze Shi and Arony, Nowshin Nawar and Devathasan, Kezia and Damian, Daniela",10.1109/ICSE-SEET58685.2023.00027,2023,"Capstone courses in undergraduate software engineering are a critical final milestone for students. These courses allow students to create a software solution and demonstrate the knowledge they accumulated in their degrees. However, a typical capstone project team is small containing no more than 5 students and function independently from other teams. To better reflect real-world software development and meet industry demands, we introduce in this paper our novel capstone course. Each student was assigned to a large-scale, multi-team (i.e., company) of up to 20 students to collaboratively build software. Students placed in a company gained first-hand experiences with respect to multi-team coordination, integration, communication, agile, and teamwork to build a microservices based project. Furthermore, each company was required to implement plug-and-play so that their services would be compatible with another company, thereby sharing common APIs. Through developing the product in autonomous sub-teams, the students enhanced not only their technical abilities but also their soft skills such as communication and coordination. More importantly, experiencing the challenges that arose from the multi-team project trained students to realize the pitfalls and advantages of organizational culture. Among many lessons learned from this course experience, students learned the critical importance of building team trust. We provide detailed information about our course structure, lessons learned, and propose recommendations for other universities and programs. Our work concerns educators interested in launching similar capstone projects so that students in other institutions can reap the benefits of large-scale, multi-team development."
Improving Grading Outcomes in Software Engineering Projects through Automated Contributions Summaries,"Presler-Marshall, Kai and Heckman, Sarah and Stolee, Kathryn T.",10.1109/ICSE-SEET58685.2023.00030,2023,"Teaming is a key aspect of most professional software engineering positions, and consequently, team-based learning (TBL) features heavily in many undergraduate computer science (CS) and software engineering programs. However, while TBL offers many pedagogical benefits, it is not without challenges. One such challenge is assessment, as the course teaching staff must be able to accurately identify individual students' contributions to both encourage and reward participation. In this paper, we study improvements to grading practises in the context of a CS1.5 introductory software engineering course, where assessing individual students' contributions to weekly lab assignments is done manually by teaching assistants (TAs). We explore the impact of presenting TAs with automated summaries of individual student contributions to their team's GitHub repository. To do so, we propose a novel algorithm, and implement a tool based off of it, AutoVCS. We measure the impact on grading metrics in terms of grading speed, grading consistency, and TA satisfaction. We evaluate our algorithm, as implemented in AutoVCS, in a controlled experimental study on Java-based lab assignments from a recent offering of NC State University's CS1.5 course. We find our automated summaries help TAs grade more consistently and provides students with more actionable feedback. Although TAs grade no faster using automated summaries, they nonetheless strongly prefer grading with the support of them than without. We conclude with recommendations for future work to explore improving consistency in contribution grading for student software engineering teams."
A Metric for Measuring Software Engineering Post-Graduate Outcomes,"Breaux, Travis D. and Moritz, Jennifer",10.1109/ICSE-SEET58685.2023.00032,2023,"Professional software engineering (SE) degree programs provide students with the education and skills needed to enter a new SE career, or take on increasing responsibility within their current career. An important metric for evaluating such programs is the impact that completing the program has on postgraduate, career outcomes. Apart from hiring rates and median salaries, this is challenging to measure, because alumni survey response rates are frequently low, and without alumni feedback, insight into individual career advancement after graduation is difficult to observe. In this paper, we propose a new metric, called Career Velocity, that measures the impact of a degree program on alumni promotion into senior positions. The metric requires tracing alumni directory information, consisting of a person's full name, degree name, and graduation year, to public data that includes employment histories, before computing the number of months prior to promotion into a senior SE position. The metric was developed and evaluated on a mix of six degree programs, including undergraduate and graduate computer science, software engineering and data science programs. The metric was further evaluated by assessing the impact of a graduate's number of months of industry experience prior to graduation. The results suggest that, independent of prior industry experience, specialized education that targets advancement in a specific career class, e.g., software engineering, leads to faster career progression than general education."
DevDocOps: towards automated documentation for DevOps,"Rong, Guoping and Jin, Zefeng and Zhang, He and Zhang, Youwen and Ye, Wenhua and Shao, Dong",10.1109/ICSE-SEIP.2019.00034,2019,"The proliferation of DevOps enables significant acceleration and automation of the delivery and deployment of massive software products. Unfortunately, the development of supporting documents that is vital for large scale software systems in many cases does not keep pace with the rhythm of feature delivery using DevOps in practice, which becomes the bottleneck for many software organizations to deliver full value to the customers as claimed by DevOps. This paper proposes, implements, and evaluates a new approach, DevDocOps, for continuous automated documentation, in particular for DevOps. With DevDocOps, developers are able to create the documents simultaneously with their working versions of software, which largely guarantees the accuracy and integrity of documents as well as significantly increases their delivery speed. Within an established delivery chain, a set of templates are created to collect and transform the required information from its origin to the target documents for delivery. A real system, iDoc, is implemented to map, collect, and synthesizethe information from document templates and automate the documentation process. The iDoc system supports the generation of documents in minutes and the instant feedback loop as well. DevDocOps has been successfully adopted in over 30 large software projects in a top tier global telecommunication enterprise. The lag time between the releases of the product version and its supporting document has been shortened from 1--2 months on average to less than 2 days. DevDocOps extends the scope of DevOps and enhances the value delivery by supporting continuous documentation and bridging the gap between feature delivery and document delivery with automation."
LightSys: lightweight and efficient CI system for improving integration speed of software,"Lim, Geunsik and Ham, MyungJoo and Moon, Jijoong and Song, Wook",10.1109/ICSE-SEIP52600.2021.00009,2021,"The complexity and size increase of software has extended the delay for developers as they wait for code analysis and code merge. With the larger and more complex software, more developers nowadays are developing software with large source code repositories. The tendency for software platforms to immediately update software packages with feature updates and bug-fixes is a significant obstacle. Continuous integration systems may help prevent software flaws during the active development of software packages, even when they are deployed and updated frequently. Herein, we present a portable and modular code review automation system that inspects incoming code changes such as code format and style, performance regression, static analysis, build and deployment tests, and dynamic analysis before merging and changing code. The proposed mechanisms are sufficiently lightweight to be hosted on a regular desktop computer even for numerous developers. The resulting reduced costs allow developers to apply the proposed mechanism to many source code repositories. Experimental results demonstrate that the proposed mechanism drastically reduces overheads and improves usability compared with conventional mechanisms: execution time (6x faster), CPU usage (40% lower), memory consumption (1/180), and no out-of-memory occurrence."
"An empirical study of the landscape of open source projects in Baidu, Alibaba, and Tencent","Han, Junxiao and Deng, Shuiguang and Lo, David and Zhi, Chen and Yin, Jianwei and Xia, Xin",10.1109/ICSE-SEIP52600.2021.00039,2021,"Open source software has drawn more and more attention from researchers, developers and companies nowadays. Meanwhile, many Chinese technology companies are embracing open source and choosing to open source their projects. Nevertheless, most previous studies are concentrated on international companies such as Microsoft or Google, while the practical values of open source projects of Chinese technology companies remain unclear. To address this issue, we conduct a mixed-method study to investigate the landscape of projects open sourced by three large Chinese technology companies, namely Baidu, Alibaba, and Tencent (BAT). We study the categories and characteristics of open source projects, the developer's perceptions towards open sourcing effort for these companies, and the internationalization effort of their open source projects. We collected 1,000 open source projects that were open sourced by BAT in GitHub and performed an online survey that received 101 responses from developers of these projects. Some key findings include: 1) BAT prefer to open source frontend development projects, 2) 88% of the respondents are positive towards open sourcing software projects in their respective companies, 3) 64% of the respondents reveal that the most common motivations for BAT to open source their projects are the desire to gain fame, expand their influence and gain recruitment advantage, 4) respondents believe that the most common internationalization effort is ""providing an English version of readme files"", 5) projects with more internationalization effort (i.e., include an English readme file) are more popular. Our findings provide directions for software engineering researchers and provide practical suggestions to software developers and Chinese technology companies."
Investigating the potential impact of values on requirements and software engineering,"Sutcliffe, Alistair and Sawyer, Pete and Liu, Wei and Bencomo, Nelly",10.1109/ICSE-SEIS52602.2021.00013,2021,"This paper describes an investigation into value-based software engineering and proposes a comprehensive value taxonomy with interpretation of design feature implications. The value taxonomy is used to assess the design of Covid-19 symptom tracker applications, contrasting the UK's NHS phase 1 and 2 designs which adopted centralized, then decentralized, architectures. The value/feature analysis is also applied to the King's/Zoe Covid app which does not detect proximity, instead relying on user self-reporting. Value analysis illuminated design choices but was insufficient to account for download acceptance of the apps. We argue that motivational cost-benefit analysis needs to complement a values-based approach."
QFL: data-driven feedback loop to manage quality in Agile development,"L\'{o}pez, Lidia and Bagnato, Alessandra and Ahberv\'{e}, Antonin and Franch, Xavier",10.1109/ICSE-SEIS52602.2021.00015,2021,"Background: Quality requirements (QRs) describe desired system qualities, playing an important role in the success of software projects. In the context of agile software development (ASD), where the main objective is the fast delivery of functionalities, QRs are often ill-defined and not well addressed during the development process. Software analytics tools help to control quality though the measurement of quality-related software aspects to support decision-makers in the process of QR management. Aim: The goal of this research is to explore the benefits of integrating a concrete software analytics tool, Q-Rapids Tool, to assess software quality and support QR management processes. Method: In the context of a technology transfer project, the Softeam company has integrated Q-Rapids Tool in their development process. We conducted a series of workshops involving Softeam members working in the Modelio product development. Results: We present the Quality Feedback Loop (QFL) process to be integrated in software development processes to control the complete QR life-cycle, from elicitation to validation. As a result of the implementation of QFL in Softeam, Modelio's team members highlight the benefits of integrating a data analytics tool with their project planning tool and the fact that project managers can control the whole process making the final decisions. Conclusions: Practitioners can benefit from the integration of software analytics tools as part of their software development toolchain to control software quality. The implementation of QFL promotes quality in the organization and the integration of software analytics and project planning tools also improves the communication between teams."
Understanding emotions of developer community towards software documentation,"Venigalla, Akhila Sri Manasa and Chimalakonda, Sridhar",10.1109/ICSE-SEIS52602.2021.00018,2021,"The availability of open-source projects facilitates developers to contribute and collaborate on a wide range of projects. As a result, the developer community contributing to such open-source projects is also increasing. Many of the projects involve frequent updates and extensive reuses. A well-updated documentation helps in a better understanding of the software project and also facilitates efficient contribution and reuse. Though software documentation plays an important role in the development and maintenance of software, it also suffers from various issues that include insufficiency, inconsistency, ill-maintainability, and so on. Exploring the perception of developers towards documentation could help in understanding the reasons behind prevalent issues in software documentation. It could further aid in deciding on training that could be given to the developer community towards building more sustainable projects for society. Analyzing sentiments of contributors to a project could provide insights on understanding developer perceptions. Hence, as the first step towards this direction, we analyze sentiments of commit messages specific to the documentation of a software project. To this end, we considered the commit history of 998 GitHub projects from the GHTorrent dataset and identified 10,996 commits that correspond to the documentation of repositories. Further, we apply sentiment analysis techniques to obtain insights on the type of sentiment being expressed in commit messages of the selected commits. We observe that around 45% of the identified commit messages express trust emotion."
What causes my test alarm? automatic cause analysis for test alarms in system and integration testing,"Jiang, He and Li, Xiaochen and Yang, Zijiang and Xuan, Jifeng",10.1109/ICSE.2017.71,2017,"Driven by new software development processes and testing in clouds, system and integration testing nowadays tends to produce enormous number of alarms. Such test alarms lay an almost unbearable burden on software testing engineers who have to manually analyze the causes of these alarms. The causes are critical because they decide which stakeholders are responsible to fix the bugs detected during the testing. In this paper, we present a novel approach that aims to relieve the burden by automating the procedure. Our approach, called Cause Analysis Model, exploits information retrieval techniques to efficiently infer test alarm causes based on test logs. We have developed a prototype and evaluated our tool on two industrial datasets with more than 14,000 test alarms. Experiments on the two datasets show that our tool achieves an accuracy of 58.3% and 65.8%, respectively, which outperforms the baseline algorithms by up to 13.3%. Our algorithm is also extremely efficient, spending about 0.1s per cause analysis. Due to the attractive experimental results, our industrial partner, a leading information and communication technology company in the world, has deployed the tool and it achieves an average accuracy of 72% after two months of running, nearly three times more accurate than a previous strategy based on regular expressions."
Semantically enhanced software traceability using deep learning techniques,"Guo, Jin and Cheng, Jinghui and Cleland-Huang, Jane",10.1109/ICSE.2017.9,2017,"In most safety-critical domains the need for trace-ability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts; however, creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links; however, current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper, we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed state-of-the-art tracing methods including the Vector Space Model and Latent Semantic Indexing."
Analysis and detection of information types of open source software issue discussions,"Arya, Deeksha and Wang, Wenting and Guo, Jin L. C. and Cheng, Jinghui",10.1109/ICSE.2019.00058,2019,"Most modern Issue Tracking Systems (ITSs) for open source software (OSS) projects allow users to add comments to issues. Over time, these comments accumulate into discussion threads embedded with rich information about the software project, which can potentially satisfy the diverse needs of OSS stakeholders. However, discovering and retrieving relevant information from the discussion threads is a challenging task, especially when the discussions are lengthy and the number of issues in ITSs are vast. In this paper, we address this challenge by identifying the information types presented in OSS issue discussions. Through qualitative content analysis of 15 complex issue threads across three projects hosted on GitHub, we uncovered 16 information types and created a labeled corpus containing 4656 sentences. Our investigation of supervised, automated classification techniques indicated that, when prior knowledge about the issue is available, Random Forest can effectively detect most sentence types using conversational features such as the sentence length and its position. When classifying sentences from new issues, Logistic Regression can yield satisfactory performance using textual features for certain information types, while falling short on others. Our work represents a nontrivial first step towards tools and techniques for identifying and obtaining the rich information recorded in the ITSs to support various software engineering activities and to satisfy the diverse needs of OSS stakeholders."
Mining software defects: should we consider affected releases?,"Yatish, Suraj and Jiarpakdee, Jirayus and Thongtanunam, Patanamon and Tantithamthavorn, Chakkrit",10.1109/ICSE.2019.00075,2019,"With the rise of the Mining Software Repositories (MSR) field, defect datasets extracted from software repositories play a foundational role in many empirical studies related to software quality. At the core of defect data preparation is the identification of post-release defects. Prior studies leverage many heuristics (e.g., keywords and issue IDs) to identify post-release defects. However, such the heuristic approach is based on several assumptions, which pose common threats to the validity of many studies. In this paper, we set out to investigate the nature of the difference of defect datasets generated by the heuristic approach and the realistic approach that leverages the earliest affected release that is realistically estimated by a software development team for a given defect. In addition, we investigate the impact of defect identification approaches on the predictive accuracy and the ranking of defective modules that are produced by defect models. Through a case study of defect datasets of 32 releases, we find that that the heuristic approach has a large impact on both defect count datasets and binary defect datasets. Surprisingly, we find that the heuristic approach has a minimal impact on defect count models, suggesting that future work should not be too concerned about defect count models that are constructed using heuristic defect datasets. On the other hand, using defect datasets generated by the realistic approach lead to an improvement in the predictive accuracy of defect classification models."
Why does code review work for open source software communities?,"Alami, Adam and Cohn, Marisa Leavitt and Wasowski, Andrzej",10.1109/ICSE.2019.00111,2019,"Open source software communities have demonstrated that they can produce high quality results. The overall success of peer code review, commonly used in open source projects, has likely contributed strongly to this success. Code review is an emotionally loaded practice, with public exposure of reputation and ample opportunities for conflict. We set off to ask why code review works for open source communities, despite this inherent challenge. We interviewed 21 open source contributors from four communities and participated in meetings of ROS community devoted to implementation of the code review process.It appears that the hacker ethic is a key reason behind the success of code review in FOSS communities. It is built around the ethic of passion and the ethic of caring. Furthermore, we observed that tasks of code review are performed with strong intrinsic motivation, supported by many non-material extrinsic motivation mechanisms, such as desire to learn, to grow reputation, or to improve one's positioning on the job market.In the paper, we describe the study design, analyze the collected data and formulate 20 proposals for how what we know about hacker ethics and human and social aspects of code review, could be exploited to improve the effectiveness of the practice in software projects."
Playing Planning Poker in Crowds: Human Computation of Software Effort Estimates,"Alhamed, Mohammed and Storer, Tim",10.1109/ICSE43902.2021.00014,2021,"Reliable cost effective effort estimation remains a considerable challenge for software projects. Recent work has demonstrated that the popular Planning Poker practice can produce reliable estimates when undertaken within a software team of knowledgeable domain experts. However, the process depends on the availability of experts and can be time-consuming to perform, making it impractical for large scale or open source projects that may curate many thousands of outstanding tasks. This paper reports on a full study to investigate the feasibility of using crowd workers supplied with limited information about a task to provide comparably accurate estimates using Planning Poker. We describe the design of a Crowd Planning Poker (CPP) process implemented on Amazon Mechanical Turk and the results of a substantial set of trials, involving more than 5000 crowd workers and 39 diverse software tasks. Our results show that a carefully organised and selected crowd of workers can produce effort estimates that are of similar accuracy to those of a single expert."
Early Life Cycle Software Defect Prediction: Why? How?,"Shrikanth, N. C. and Majumder, Suvodeep and Menzies, Tim",10.1109/ICSE43902.2021.00050,2021,"Many researchers assume that, for software analytics, ""more data is better."" We write to show that, at least for learning defect predictors, this may not be true.To demonstrate this, we analyzed hundreds of popular GitHub projects. These projects ran for 84 months and contained 3,728 commits (median values). Across these projects, most of the defects occur very early in their life cycle. Hence, defect predictors learned from the first 150 commits and four months perform just as well as anything else. This means that, at least for the projects studied here, after the first few months, we need not continually update our defect prediction models.We hope these results inspire other researchers to adopt a ""simplicity-first"" approach to their work. Some domains require a complex and data-hungry analysis. But before assuming complexity, it is prudent to check the raw data looking for ""short cuts"" that can simplify the analysis."
"Same File, Different Changes: The Potential of Meta-Maintenance on GitHub","Hata, Hideaki and Kula, Raula Gaikovina and Ishio, Takashi and Treude, Christoph",10.1109/ICSE43902.2021.00076,2021,"Online collaboration platforms such as GitHub have provided software developers with the ability to easily reuse and share code between repositories. With clone-and-own and forking becoming prevalent, maintaining these shared files is important, especially for keeping the most up-to-date version of reused code. Different to related work, we propose the concept of meta-maintenance---i.e., tracking how the same files evolve in different repositories with the aim to provide useful maintenance opportunities to those files. We conduct an exploratory study by analyzing repositories from seven different programming languages to explore the potential of meta-maintenance. Our results indicate that a majority of active repositories on GitHub contains at least one file which is also present in another repository, and that a significant minority of these files are maintained differently in the different repositories which contain them. We manually analyzed a representative sample of shared files and their variants to understand which changes might be useful for meta-maintenance. Our findings support the potential of meta-maintenance and open up avenues for future work to capitalize on this potential."
Representation of Developer Expertise in Open Source Software,"Dey, Tapajit and Karnauch, Andrey and Mockus, Audris",10.1109/ICSE43902.2021.00094,2021,"Background: Accurate representation of developer expertise has always been an important research problem. While a number of studies proposed novel methods of representing expertise within individual projects, these methods are difficult to apply at an ecosystem level. However, with the focus of software development shifting from monolithic to modular, a method of representing developers' expertise in the context of the entire OSS development becomes necessary when, for example, a project tries to find new maintainers and look for developers with relevant skills. Aim: We aim to address this knowledge gap by proposing and constructing the Skill Space where each API, developer, and project is represented and postulate how the topology of this space should reflect what developers know (and projects need). Method: we use the World of Code infrastructure to extract the complete set of APIs in the files changed by open source developers and, based on that data, employ Doc2Vec embeddings for vector representations of APIs, developers, and projects. We then evaluate if these embeddings reflect the postulated topology of the Skill Space by predicting what new APIs/projects developers use/join, and whether or not their pull requests get accepted. We also check how the developers' representations in the Skill Space align with their self-reported API expertise. Result: Our results suggest that the proposed embeddings in the Skill Space appear to satisfy the postulated topology and we hope that such representations may aid in the construction of signals that increase trust (and efficiency) of open source ecosystems at large and may aid investigations of other phenomena related to developer proficiency and learning."
Leaving My Fingerprints: Motivations and Challenges of Contributing to OSS for Social Good,"Huang, Yu and Ford, Denae and Zimmermann, Thomas",10.1109/ICSE43902.2021.00096,2021,"When inspiring software developers to contribute to open source software, the act is often referenced as an opportunity to build tools to support the developer community. However, that is not the only charge that propels contributions--- growing interest in open source has also been attributed to software developers deciding to use their technical skills to benefit a common societal good. To understand how developers identify these projects, their motivations for contributing, and challenges they face, we conducted 21 semi-structured interviews with OSS for Social Good (OSS4SG) contributors. From our interview analysis, we identified themes of contribution styles that we wanted to understand at scale by deploying a survey to over 5765 OSS and Open Source Software for Social Good contributors. From our quantitative analysis of 517 responses, we find that the majority of contributors demonstrate a distinction between OSS4SG and OSS. Likewise, contributors described definitions based on what societal issue the project was to mitigate and who the outcomes of the project were going to benefit. In addition, we find that OSS4SG contributors focus less on benefiting themselves by padding their resume with new technology skills and are more interested in leaving their mark on society at statistically significant levels. We also find that OSS4SG contributors evaluate the owners of the project significantly more than OSS contributors. These findings inform implications to help contributors identify high societal impact projects, help project maintainers reduce barriers to entry, and help organizations understand why contributors are drawn to these projects to sustain active participation."
Automatic Unit Test Generation for Machine Learning Libraries: How Far Are We?,"Wang, Song and Shrestha, Nishtha and Subburaman, Abarna Kucheri and Wang, Junjie and Wei, Moshi and Nagappan, Nachiappan",10.1109/ICSE43902.2021.00138,2021,"Automatic unit test generation that explores the input space and produces effective test cases for given programs have been studied for decades. Many unit test generation tools that can help generate unit test cases with high structural coverage over a program have been examined. However, the fact that existing test generation tools are mainly evaluated on general software programs calls into question about its practical effectiveness and usefulness for machine learning libraries, which are statistically-orientated and have fundamentally different nature and construction from general software projects.In this paper, we set out to investigate the effectiveness of existing unit test generation techniques on machine learning libraries. To investigate this issue, we conducted an empirical study on five widely-used machine learning libraries with two popular unit test case generation tools, i.e., EVOSUITE and Randoop. We find that (1) most of the machine learning libraries do not maintain a high-quality unit test suite regarding commonly applied quality metrics such as code coverage (on average is 34.1%) and mutation score (on average is 21.3%), (2) unit test case generation tools, i.e., EVOSUITE and Randoop, lead to clear improvements in code coverage and mutation score, however, the improvement is limited, and (3) there exist common patterns in the uncovered code across the five machine learning libraries that can be used to improve unit test case generation tasks."
A Comprehensive Study of Real-World Bugs in Machine Learning Model Optimization,"Guan, Hao and Xiao, Ying and Li, Jiaying and Liu, Yepang and Bai, Guangdong",10.1109/ICSE48619.2023.00024,2023,"Due to the great advance in machine learning (ML) techniques, numerous ML models are expanding their application domains in recent years. To adapt for resource-constrained platforms such as mobile and Internet of Things (IoT) devices, pre-trained models are often processed to enhance their efficiency and compactness, using optimization techniques such as pruning and quantization. Similar to the optimization process in other complex systems, e.g., program compilers and databases, optimizations for ML models can contain bugs, leading to severe consequences such as system crashes and financial loss. While bugs in training, compiling and deployment stages have been extensively studied, there is still a lack of systematic understanding and characterization of model optimization bugs (MOBs).In this work, we conduct the first empirical study to identify and characterize MOBs. We collect a comprehensive dataset containing 371 MOBs from TensorFlow and PyTorch, the most extensively used open-source ML frameworks, covering the entire development time span of their optimizers (May 2019 to August 2022). We then investigate the collected bugs from various perspectives, including their symptoms, root causes, life cycles, detection and fixes. Our work unveils the status quo of MOBs in the wild, and reveals their features on which future detection techniques can be based. Our findings also serve as a warning to the developers and the users of ML frameworks, and an appeal to our research community to enact dedicated countermeasures."
A Qualitative Study on the Implementation Design Decisions of Developers,"Liang, Jenny T. and Arab, Maryam and Ko, Minhyuk and Ko, Amy J. and LaToza, Thomas D.",10.1109/ICSE48619.2023.00047,2023,"Decision-making is a key software engineering skill. Developers constantly make choices throughout the software development process, from requirements to implementation. While prior work has studied developer decision-making, the choices made while choosing what solution to write in code remain understudied. In this mixed-methods study, we examine the phenomenon where developers select one specific way to implement a behavior in code, given many potential alternatives. We call these decisions implementation design decisions. Our mixed-methods study includes 46 survey responses and 14 semi-structured interviews with professional developers about their decision types, considerations, processes, and expertise for implementation design decisions. We find that implementation design decisions, rather than being a natural outcome from higher levels of design, require constant monitoring of higher level design choices, such as requirements and architecture. We also show that developers have a consistent general structure to their implementation decision-making process, but no single process is exactly the same. We discuss the implications of our findings on research, education, and practice, including insights on teaching developers how to make implementation design decisions."
PTPDroid: Detecting Violated User Privacy Disclosures to Third-Parties of Android Apps,"Tan, Zeya and Song, Wei",10.1109/ICSE48619.2023.00050,2023,"Android apps frequently access personal information to provide customized services. Since such information is sensitive in general, regulators require Android app vendors to publish privacy policies that describe what information is collected and why it is collected. Existing work mainly focuses on the types of the collected data but seldom considers the entities that collect user privacy, which could falsely classify problematic declarations about user privacy collected by third-parties into clear disclosures. To address this problem, we propose PTPDroid, a flow-to-policy consistency checking approach and an automated tool, to comprehensively uncover from the privacy policy the violated disclosures to third-parties. Our experiments on real-world apps demonstrate the effectiveness and superiority of PTPDroid, and our empirical study on 1,000 popular real-world apps reveals that violated user privacy disclosures to third-parties are prevalent in practice."
AI-Based Question Answering Assistance for Analyzing Natural-Language Requirements,"Ezzini, Saad and Abualhaija, Sallam and Arora, Chetan and Sabetzadeh, Mehrdad",10.1109/ICSE48619.2023.00113,2023,"By virtue of being prevalently written in natural language (NL), requirements are prone to various defects, e.g., inconsistency and incompleteness. As such, requirements are frequently subject to quality assurance processes. These processes, when carried out entirely manually, are tedious and may further overlook important quality issues due to time and budget pressures. In this paper, we propose QAssist - a question-answering (QA) approach that provides automated assistance to stakeholders, including requirements engineers, during the analysis of NL requirements. Posing a question and getting an instant answer is beneficial in various quality-assurance scenarios, e.g., incompleteness detection. Answering requirements-related questions automatically is challenging since the scope of the search for answers can go beyond the given requirements specification. To that end, QAssist provides support for mining external domain-knowledge resources. Our work is one of the first initiatives to bring together QA and external domain knowledge for addressing requirements engineering challenges. We evaluate QAssist on a dataset covering three application domains and containing a total of 387 question-answer pairs. We experiment with state-of-the-art QA methods, based primarily on recent large-scale language models. In our empirical study, QAssist localizes the answer to a question to three passages within the requirements specification and within the external domain-knowledge resource with an average recall of 90.1% and 96.5%, respectively. QAssist extracts the actual answer to the posed question with an average accuracy of 84.2%."
On-Demand Security Requirements Synthesis with Relational Generative Adversarial Networks,"Koscinski, Viktoria and Hashemi, Sara and Mirakhorli, Mehdi",10.1109/ICSE48619.2023.00139,2023,"Security requirements engineering is a manual and error-prone activity that is often neglected due to the knowledge gap between cybersecurity professionals and software requirements engineers. In this paper, we aim to automate the process of recommending and synthesizing security requirements specifications and therefore supporting requirements engineers in soliciting and specifying security requirements. We investigate the use of Relational Generative Adversarial Networks (GANs) in automatically synthesizing security requirements specifications. We evaluate our approach using a real case study of the Court Case Management System (CCMS) developed for the Indiana Supreme Court's Division of State Court Administration. We present an approach based on RelGAN to generate security requirements specifications for the CCMS. We show that RelGAN is practical for synthesizing security requirements specifications as indicated by subject matter experts. Based on this study, we demonstrate promising results for the use of GANs in the software requirements synthesis domain. We also provide a baseline for synthesizing requirements, highlight limitations and weaknesses of RelGAN and define opportunities for further investigations."
Sustainability is Stratified: Toward a Better Theory of Sustainable Software Engineering,"McGuire, Sean and Schultz, Erin and Ayoola, Bimpe and Ralph, Paul",10.1109/ICSE48619.2023.00169,2023,"Background: Sustainable software engineering (SSE) means creating software in a way that meets present needs without undermining our collective capacity to meet our future needs. It is typically conceptualized as several intersecting dimensions or ""pillars""---environmental, social, economic, technical and individual. However; these pillars are theoretically underdeveloped and require refinement. Objectives: The objective of this paper is to generate a better theory of SSE. Method: First, a scoping review was conducted to understand the state of research on SSE and identify existing models thereof. Next, a meta-synthesis of qualitative research on SSE was conducted to critique and improve the existing models identified. Results: 961 potentially relevant articles were extracted from five article databases. These articles were de-duplicated and then screened independently by two screeners, leaving 243 articles to examine. Of these, 109 were non-empirical, the most common empirical method was systematic review, and no randomized controlled experiments were found. Most papers focus on ecological sustainability (158) and the sustainability of software products (148) rather than processes. A meta-synthesis of 36 qualitative studies produced several key propositions, most notably, that sustainability is stratified (has different meanings at different levels of abstraction) and multisystemic (emerges from interactions among multiple social, technical, and sociotechnical systems). Conclusion: The academic literature on SSE is surprisingly non-empirical. More empirical evaluations of specific sustainability interventions are needed. The sustainability of software development products and processes should be conceptualized as multisystemic and stratified, and assessed accordingly."
An Empirical Study of Deep Learning Models for Vulnerability Detection,"Steenhoek, Benjamin and Rahman, Md Mahbubur and Jiles, Richard and Le, Wei",10.1109/ICSE48619.2023.00188,2023,"Deep learning (DL) models of code have recently reported great progress for vulnerability detection. In some cases, DL-based models have outperformed static analysis tools. Although many great models have been proposed, we do not yet have a good understanding of these models. This limits the further advancement of model robustness, debugging, and deployment for the vulnerability detection. In this paper, we surveyed and reproduced 9 state-of-the-art (SOTA) deep learning models on 2 widely used vulnerability detection datasets: Devign and MSR. We investigated 6 research questions in three areas, namely model capabilities, training data, and model interpretation. We experimentally demonstrated the variability between different runs of a model and the low agreement among different models' outputs. We investigated models trained for specific types of vulnerabilities compared to a model that is trained on all the vulnerabilities at once. We explored the types of programs DL may consider ""hard"" to handle. We investigated the relations of training data sizes and training data composition with model performance. Finally, we studied model interpretations and analyzed important features that the models used to make predictions. We believe that our findings can help better understand model results, provide guidance on preparing training data, and improve the robustness of the models. All of our datasets, code, and results are available at https://doi.org/10.6084/m9.figshare.20791240."
An Empirical Study of Pre-Trained Model Reuse in the Hugging Face Deep Learning Model Registry,"Jiang, Wenxin and Synovic, Nicholas and Hyatt, Matt and Schorlemmer, Taylor R. and Sethi, Rohan and Lu, Yung-Hsiang and Thiruvathukal, George K. and Davis, James C.",10.1109/ICSE48619.2023.00206,2023,"Deep Neural Networks (DNNs) are being adopted as components in software systems. Creating and specializing DNNs from scratch has grown increasingly difficult as state-of-the-art architectures grow more complex. Following the path of traditional software engineering, machine learning engineers have begun to reuse large-scale pre-trained models (PTMs) and fine-tune these models for downstream tasks. Prior works have studied reuse practices for traditional software packages to guide software engineers towards better package maintenance and dependency management. We lack a similar foundation of knowledge to guide behaviors in pre-trained model ecosystems.In this work, we present the first empirical investigation of PTM reuse. We interviewed 12 practitioners from the most popular PTM ecosystem, Hugging Face, to learn the practices and challenges of PTM reuse. From this data, we model the decision-making process for PTM reuse. Based on the identified practices, we describe useful attributes for model reuse, including provenance, reproducibility, and portability. Three challenges for PTM reuse are missing attributes, discrepancies between claimed and actual performance, and model risks. We substantiate these identified challenges with systematic measurements in the Hugging Face ecosystem. Our work informs future directions on optimizing deep learning ecosystems by automated measuring useful attributes and potential attacks, and envision future research on infrastructure and standardization for model registries."
Argument structure mining in scientific articles: a comparative analysis,"Song, Ningyuan and Cheng, Hanghang and Zhou, Huimin and Wang, Xiaoguang",10.1109/JCDL.2019.00060,2020,"Scientific articles serve as examples to demonstrate the standard functional units of argumentation, extracting its argument structure would benefit the knowledge discovery, representation and retrieval. This study proposes a coding schema based on the argumentation ontologies, and uses the schema to annotate 40 articles in two research field, then combines with sequential pattern mining to find argument structure model. The results expose a similar linear construction with differences in the specific argument structures of each knowledge domain. Therefore, our models have strong applicability in these two fields."
Metamorphic relation patterns for query-based systems,"Segura, Sergio and Dur\'{a}n, Amador and Troya, Javier and Ruiz-Cort\'{e}s, Antonio",10.1109/MET.2019.00012,2019,"Searching and displaying data based on user queries is a key feature of most software applications such as information systems, web portals, web APIs, and data analytic platforms. The large volume of data managed by these types of systems, henceforth called query-based systems (QBS), makes them extremely hard to test due to the difficulty to assess whether the output of a query is correct, the so-called oracle problem. Metamorphic testing has proved to be a very effective approach to alleviate the oracle problem in QBS, enabling the detection of bugs in data repositories, large e-commerce sites, and some of the most used software applications worldwide such as Google Search and YouTube. We have observed, however, that the metamorphic relations used to test different types of QBS are very similar, regardless of their domain, since all of them exploit standard query features such as filtering and ordering. Inspired by this finding, in this paper we present a catalogue of metamorphic relation patterns to assist testers in the identification and inference of metamorphic relations in QBS. For the definition of the patterns we resorted to the root of most query languages: relational algebra. We show how the proposed patterns help in the identification of metamorphic relations in the e-commerce platform PrestaShop, the email service Gmail, and the mobile application of video streaming HBO."
The impact of using regression models to build defect classifiers,"Rajbahadur, Gopi Krishnan and Wang, Shaowei and Kamei, Yasutaka and Hassan, Ahmed E.",10.1109/MSR.2017.4,2017,"It is common practice to discretize continuous defect counts into defective and non-defective classes and use them as a target variable when building defect classifiers (discretized classifiers). However, this discretization of continuous defect counts leads to information loss that might affect the performance and interpretation of defect classifiers. Another possible approach to build defect classifiers is through the use of regression models then discretizing the predicted defect counts into defective and non-defective classes (regression-based classifiers).In this paper, we compare the performance and interpretation of defect classifiers that are built using both approaches (i.e., discretized classifiers and regression-based classifiers) across six commonly used machine learning classifiers (i.e., linear/logistic regression, random forest, KNN, SVM, CART, and neural networks) and 17 datasets. We find that: i) Random forest based classifiers outperform other classifiers (best AUC) for both classifier building approaches; ii) In contrast to common practice, building a defect classifier using discretized defect counts (i.e., discretized classifiers) does not always lead to better performance.Hence we suggest that future defect classification studies should consider building regression-based classifiers (in particular when the defective ratio of the modeled dataset is low). Moreover, we suggest that both approaches for building defect classifiers should be explored, so the best-performing classifier can be used when determining the most influential features."
Predicting likelihood of requirement implementation within the planned iteration: an empirical study at IBM,"Dehghan, Ali and Neal, Adam and Blincoe, Kelly and Linaker, Johan and Damian, Daniela",10.1109/MSR.2017.53,2017,"There has been a significant interest in the estimation of time and effort in fixing defects among both software practitioners and researchers over the past two decades. However, most of the focus has been on prediction of time and effort in resolving bugs, without much regard to predicting time needed to complete high-level requirements, a critical step in release planning. In this paper, we describe a mixed-method empirical study on three large IBM projects in which we developed and evaluated a process of training a predictive model constituting a set of 29 features in nine categories in order to predict if a requirement will be completed within its planned iteration. We conducted feature engineering through iterative interviews with IBM practitioners as well as analysis of large development repositories of these three projects. Using machine learning techniques, we were able to make predictions on completion time of requirements at four different stages of their lifetime. Using our industrial partner's interest in high precision over recall, we then adopted a cost sensitive learning method and maximized precision of predictions (ranging from 0.8 to 0.97) while maintaining an acceptable recall. We also ranked the features based on their relative importance to the optimized predictive model. We show that although satisfying predictions can be made at early stages, performance of predictions improves over time by taking advantage of requirements' progress data. Furthermore, feature importance ranking results show that although importance of features are highly dependent on project and prediction stage, there are certain features (e.g. requirement creator, time remained to the end of iteration, time since last requirement summary change and number of times requirement has been replanned for a new iteration) that emerge as important across most projects and stages, implying future worthwhile research directions for both researchers and practitioners."
Classifying code comments in Java open-source software systems,"Pascarella, Luca and Bacchelli, Alberto",10.1109/MSR.2017.63,2017,"Code comments are a key software component containing information about the underlying implementation. Several studies have shown that code comments enhance the readability of the code. Nevertheless, not all the comments have the same goal and target audience. In this paper, we investigate how six diverse Java OSS projects use code comments, with the aim of understanding their purpose. Through our analysis, we produce a taxonomy of source code comments; subsequently, we investigate how often each category occur by manually classifying more than 2,000 code comments from the aforementioned projects. In addition, we conduct an initial evaluation on how to automatically classify code comments at line level into our taxonomy using machine learning; initial results are promising and suggest that an accurate classification is within reach."
DeepJIT: an end-to-end deep learning framework for just-in-time defect prediction,"Hoang, Thong and Dam, Hoa Khanh and Kamei, Yasutaka and Lo, David and Ubayashi, Naoyasu",10.1109/MSR.2019.00016,2019,"Software quality assurance efforts often focus on identifying defective code. To find likely defective code early, change-level defect prediction - aka. Just-In-Time (JIT) defect prediction - has been proposed. JIT defect prediction models identify likely defective changes and they are trained using machine learning techniques with the assumption that historical changes are similar to future ones. Most existing JIT defect prediction approaches make use of manually engineered features. Unlike those approaches, in this paper, we propose an end-to-end deep learning framework, named DeepJIT, that automatically extracts features from commit messages and code changes and use them to identify defects. Experiments on two popular software projects (i.e., QT and OPENSTACK) on three evaluation settings (i.e., cross-validation, short-period, and long-period) show that the best variant of DeepJIT (DeepJIT-Combined), compared with the best performing state-of-the-art approach, achieves improvements of 10.36--11.02% for the project QT and 9.51--13.69% for the project OPENSTACK in terms of the Area Under the Curve (AUC)."
Lessons learned from using a deep tree-based model for software defect prediction in practice,"Dam, Hoa Khanh and Pham, Trang and Ng, Shien Wee and Tran, Truyen and Grundy, John and Ghose, Aditya and Kim, Taeksu and Kim, Chul-Joo",10.1109/MSR.2019.00017,2019,"Defects are common in software systems and cause many problems for software users. Different methods have been developed to make early prediction about the most likely defective modules in large codebases. Most focus on designing features (e.g. complexity metrics) that correlate with potentially defective code. Those approaches however do not sufficiently capture the syntax and multiple levels of semantics of source code, a potentially important capability for building accurate prediction models. In this paper, we report on our experience of deploying a new deep learning tree-based defect prediction model in practice. This model is built upon the tree-structured Long Short Term Memory network which directly matches with the Abstract Syntax Tree representation of source code. We discuss a number of lessons learned from developing the model and evaluating it on two datasets, one from open source projects contributed by our industry partner Samsung and the other from the public PROMISE repository."
Predicting good configurations for GitHub and stack overflow topic models,"Treude, Christoph and Wagner, Markus",10.1109/MSR.2019.00022,2019,"Software repositories contain large amounts of textual data, ranging from source code comments and issue descriptions to questions, answers, and comments on Stack Overflow. To make sense of this textual data, topic modelling is frequently used as a text-mining tool for the discovery of hidden semantic structures in text bodies. Latent Dirichlet allocation (LDA) is a commonly used topic model that aims to explain the structure of a corpus by grouping texts. LDA requires multiple parameters to work well, and there are only rough and sometimes conflicting guidelines available on how these parameters should be set. In this paper, we contribute (i) a broad study of parameters to arrive at good local optima for GitHub and Stack Overflow text corpora, (ii) an a-posteriori characterisation of text corpora related to eight programming languages, and (iii) an analysis of corpus feature importance via per-corpus LDA configuration. We find that (1) popular rules of thumb for topic modelling parameter configuration are not applicable to the corpora used in our experiments, (2) corpora sampled from GitHub and Stack Overflow have different characteristics and require different configurations to achieve good model fit, and (3) we can predict good configurations for unseen corpora reliably. These findings support researchers and practitioners in efficiently determining suitable configurations for topic modelling when analysing textual data contained in software repositories."
World of code: an infrastructure for mining the universe of open source VCS data,"Ma, Yuxing and Bogart, Chris and Amreen, Sadika and Zaretzki, Russell and Mockus, Audris",10.1109/MSR.2019.00031,2019,"Open source software (OSS) is essential for modern society and, while substantial research has been done on individual (typically central) projects, only a limited understanding of the periphery of the entire OSS ecosystem exists. For example, how are tens of millions of projects in the periphery interconnected through technical dependencies, code sharing, or knowledge flows? To answer such questions we a) create a very large and frequently updated collection of version control data for FLOSS projects named World of Code (WoC) and b) provide basic tools for conducting research that depends on measuring interdependencies among all FLOSS projects. Our current WoC implementation is capable of being updated on a monthly basis and contains over 12B git objects. To evaluate its research potential and to create vignettes for its usage, we employ WoC in conducting several research tasks. In particular, we find that it is capable of supporting trend evaluation, ecosystem measurement, and the determination of package usage. We expect WoC to spur investigation into global properties of OSS development leading to increased resiliency of the entire OSS ecosystem. Our infrastructure facilitates the discovery of key technical dependencies, code flow, and social networks that provide the basis to determine the structure and evolution of the relationships that drive FLOSS activities and innovation."
Identifying experts in software libraries and frameworks among GitHub users,"Montandon, Jo\~{a}o Eduardo and Silva, Luciana Lourdes and Valente, Marco Tulio",10.1109/MSR.2019.00054,2019,"Software development increasingly depends on libraries and frameworks to increase productivity and reduce time-to-market. Despite this fact, we still lack techniques to assess developers expertise in widely popular libraries and frameworks. In this paper, we evaluate the performance of unsupervised (based on clustering) and supervised machine learning classifiers (Random Forest and SVM) to identify experts in three popular JavaScript libraries: facebook/react, mongodb/node-mongodb, and socketio/socket.io. First, we collect 13 features about developers activity on GitHub projects, including commits on source code files that depend on these libraries. We also build a ground truth including the expertise of 575 developers on the studied libraries, as self-reported by them in a survey. Based on our findings, we document the challenges of using machine learning classifiers to predict expertise in software libraries, using features extracted from GitHub. Then, we propose a method to identify library experts based on clustering feature data from GitHub; by triangulating the results of this method with information available on Linkedin profiles, we show that it is able to recommend dozens of GitHub users with evidences of being experts in the studied JavaScript libraries. We also provide a public dataset with the expertise of 575 developers on the studied libraries."
Splitting APIs: an exploratory study of software unbundling,"Matos, Anderson S. and Filho, Jo\~{a}o B. Ferreira and Rocha, Lincoln S.",10.1109/MSR.2019.00062,2019,"Software unbundling consists of dividing an existing software artifact into smaller ones. Unbundling can be useful for removing clutter from the original application or separating different features that may not share the same purpose, or simply for isolating an emergent functionality that merits to be an application on its own. This phenomenon is frequent with mobile apps and it is also propagating to APIs. This paper proposes a first empirical study on unbundling to understand its effects on popular APIs. We explore the possibilities of splitting libraries into 2 or more bundles based on the use that their client projects make of them. We mine over than 71,000 client projects of 10 open source APIs and automatically generate 2,090 sub-APIs to then study their properties. We find that it is possible to have sets of different ways of using a given API and to unbundle it accordingly; the bundles can vary their representativeness and uniqueness, which is analyzed thoroughly in this study."
git2net: mining time-stamped co-editing networks from large git repositories,"Gote, Christoph and Scholtes, Ingo and Schweitzer, Frank",10.1109/MSR.2019.00070,2019,"Data from software repositories have become an important foundation for the empirical study of software engineering processes. A recurring theme in the repository mining literature is the inference of developer networks capturing e.g. collaboration, coordination, or communication from the commit history of projects. Most of the studied networks are based on the co-authorship of software artefacts defined at the level of files, modules, or packages. While this approach has led to insights into the social aspects of software development, it neglects detailed information on code changes and code ownership, e.g. which exact lines of code have been authored by which developers, that is contained in the commit log of software projects.Addressing this issue, we introduce git2net, a scalable python software that facilitates the extraction of fine-grained co-editing networks in large git repositories. It uses text mining techniques to analyse the detailed history of textual modifications within files. This information allows us to construct directed, weighted, and time-stamped networks, where a link signifies that one developer has edited a block of source code originally written by another developer. Our tool is applied in case studies of an Open Source and a commercial software project. We argue that it opens up a massive new source of high-resolution data on human collaboration patterns."
Standing on shoulders or feet? the usage of the MSR data papers,"Kotti, Zoe and Spinellis, Diomidis",10.1109/MSR.2019.00085,2019,"Introduction: The establishment of the Mining Software Repositories (MSR) Data Showcase conference track has encouraged researchers to provide more data sets as a basis for further empirical studies.Objectives: Examine the usage of the data papers published in the MSR proceedings in terms of use frequency, users, and use purpose.Methods: Data track papers were collected from the MSR Data Showcase and through the manual inspection of older MSR proceedings. The use of data papers was established through citation searching followed by reading the studies that have cited them. Data papers were then clustered based on their content, whereas their citations were classified according to the knowledge areas of the Guide to the Software Engineering Body of Knowledge.Results: We found that 65% of the data papers have been used in other studies, with a long-tail distribution in the number of citations. MSR data papers are cited less than other MSR papers. A considerable number of the citations stem from the teams that authored the data papers. Publications providing repository data and metadata are the most frequent data papers and the most often cited ones. Mobile application data papers are the least common ones, but the second most frequently cited.Conclusion: Data papers have provided the foundation for a significant number of studies, but there is room for improvement in their utilization. This can be done by setting a higher bar for their publication, by encouraging their use, and by providing incentives for the enrichment of existing data collections."
Application-Driven Exascale: The JUPITER Benchmark Suite,"Herten, Andreas and Achilles, Sebastian and Alvarez, Damian and Badwaik, Jayesh and Behle, Eric and Bode, Mathis and Breuer, Thomas and Caviedes-Voulli\`{e}me, Daniel and Cherti, Mehdi and Dabah, Adel and El Sayed, Salem and Frings, Wolfgang and Gonzalez-Nicolas, Ana and Gregory, Eric B. and Mood, Kaveh Haghighi and Hater, Thorsten and Jitsev, Jenia and John, Chelsea Maria and Meinke, Jan H. and Meyer, Catrin I. and Mezentsev, Pavel and Mirus, Jan-Oliver and Nassyr, Stepan and Penke, Carolin and R\""{o}mmer, Manoel and Sinha, Ujjwal and von St. Vieth, Benedikt and Stein, Olaf and Suarez, Estela and Willsch, Dennis and Zhukov, Ilya",10.1109/SC41406.2024.00038,2024,"Benchmarks are essential in the design of modern HPC installations, as they define key aspects of system components. Beyond synthetic workloads, it is crucial to include real applications that represent user requirements into benchmark suites, to guarantee high usability and widespread adoption of a new system. Given the significant investments in leadership-class supercomputers of the exascale era, this is even more important and necessitates alignment with a vision of Open Science and reproducibility. In this work, we present the JUPITER Benchmark Suite, which incorporates 16 applications from various domains. It was designed for and used in the procurement of JUPITER, the first European exascale supercomputer. We identify requirements and challenges and outline the project and software infrastructure setup. We provide descriptions and scalability studies of selected applications and a set of key takeaways. The JUPITER Benchmark Suite is released as open source software with this work at github.com/FZJ-JSC/jubench."
Error-controlled Progressive Retrieval of Scientific Data under Derivable Quantities of Interest,"Wu, Xuan and Gong, Qian and Chen, Jieyang and Liu, Qing and Podhorszki, Norbert and Liang, Xin and Klasky, Scott",10.1109/SC41406.2024.00092,2024,"The unprecedented amount of scientific data has introduced heavy pressure on the current data storage and transmission systems. Progressive compression has been proposed to mitigate this problem, which offers data access with on-demand precision. However, existing approaches only consider precision control on primary data, leaving uncertainties on the quantities of interest (QoIs) derived from it. In this work, we present a progressive data retrieval framework with guaranteed error control on derivable QoIs. Our contributions are three-fold. (1) We carefully derive the theories to strictly control QoI errors during progressive retrieval. Our theory is generic and can be applied to any QoIs that can be composited by the basis of derivable QoIs proved in the paper. (2) We design and develop a generic progressive retrieval framework based on the proposed theories, and optimize it by exploring feasible progressive representations. (3) We evaluate our framework using five real-world datasets with a diverse set of QoIs. Experiments demonstrate that our framework can faithfully respect any user-specified QoI error bounds in the evaluated applications. This leads to over 2.02\texttimes{} performance gain in data transfer tasks compared to transferring the primary data while guaranteeing a QoI error that is less than 1E-5."
HPC with Enhanced User Separation,"Prout, Andrew and Reuther, Albert and Houle, Michael and Jones, Michael and Michaleas, Peter and Anderson, LaToya and Arcand, William and Bergeron, Bill and Bestor, David and Bonn, Alex and Burrill, Daniel and Byun, Chansup and Gadepally, Vijay and Hubbell, Matthew and Jananthan, Hayden and Luszczek, Piotr and Milechin, Lauren and Morales, Guillermo and Mullen, Julie and Rosa, Antonio and Yee, Charles and Kepner, Jeremy",10.1109/SCW63240.2024.00221,2025,"HPC systems used for research run a wide variety of software and workflows. This software is often written or modified by users to meet the needs of their research projects, and rarely is built with security in mind. In this paper we explore several of the key techniques that MIT Lincoln Laboratory Supercomputing Center has deployed on its systems to manage the security implications of these workflows by providing enforced separation for processes, filesystem access, network traffic, and accelerators to make every user feel like they are running on a personal HPC."
Won't take no for an answer: resource-driven requirements adaptation,"Bennaceur, Amel and Zisman, Andrea and McCormick, Ciaran and Barthaud, Danny and Nuseibeh, Bashar",10.1109/SEAMS.2019.00019,2019,"Adaptive composition dynamically and opportunistically uses and combines resources to best satisfy user requirements. However, when available resources cannot satisfy those requirements, no guidance or alternative options are offered by existing composition solutions. In this paper we address this issue by presenting an approach that tries to find substitutions for unavailable resources while satisfying the initial requirements. If no satisfactory substitutions are found, the requirements are adapted based on the resources available. Given that such requirements adaptation might be unbounded, we limit the search space guided by the available resources. Our approach ensures that alternative compositions given to users are achievable using available resources. We demonstrate the validity of our approach by implementing a prototype tool and applying it to support individuals in meal planning to reduce food waste."
The relevance of application domains in empirical findings,"Capiluppi, Andrea and Ajienka, Nemitari",10.1109/SoHeal.2019.00010,2019,"The term 'software ecosystem' refers to a collection of software systems that are related in some way. Researchers have been using different levels of aggregation to define an ecosystem: grouping them by a common named project (e.g., the Apache ecosystem); or considering all the projects contained in online repositories (e.g., the GoogleCode ecosystem).In this paper we propose a definition of ecosystem based on application domains: software systems are in the same ecosystem if they share the same application domain, as described by a similar technological scope, context or objective. As an example, all projects implementing networking capabilities to trade Bitcoin and other virtual currencies can be considered as part of the same ""cryptocurrency"" ecosystem.Utilising a sample of 100 Java software systems, we derive their application domains using the Latent Dirichlet Allocation (LDA) approach. We then evaluate a suite of object-oriented metrics per ecosystem, and test a null hypothesis: 'the OO metrics of all ecosystems come from the same population'.Our results show that the null hypothesis is rejected for most of the metrics chosen: the ecosystems that we extracted, based on application domains, show different structural properties.From the point of view of the interested stakeholders, this could mean that the health of a software system depends on domain-dependent factors, that could be common to the projects in the same domain-based ecosystem."
Invariants of quantum programs: characterisations and generation,"Ying, Mingsheng and Ying, Shenggang and Wu, Xiaodi",10.1145/3009837.3009840,2017,"Program invariant is a fundamental notion widely used in program verification and analysis. The aim of this paper is twofold: (i) find an appropriate definition of invariants for quantum programs; and (ii) develop an effective technique of invariant generation for verification and analysis of quantum programs.  Interestingly, the notion of invariant can be defined for quantum programs in two different ways -- additive invariants and multiplicative invariants -- corresponding to two interpretations of implication in a continuous valued logic: the Lukasiewicz implication and the Godel implication. It is shown that both of them can be used to establish partial correctness of quantum programs.  The problem of generating additive invariants of quantum programs is addressed by reducing it to an SDP (Semidefinite Programming) problem. This approach is applied with an SDP solver to generate invariants of two important quantum algorithms -- quantum walk and quantum Metropolis sampling. Our examples show that the generated invariants can be used to verify correctness of these algorithms and are helpful in optimising quantum Metropolis sampling.  To our knowledge, this paper is the first attempt to define the notion of invariant and to develop a method of invariant generation for quantum programs."
Crowd-annotation and LoD-based semantic indexing of content in multi-disciplinary web repositories to improve search results,"Khan, Arshad and Tiropanis, Thanassis and Martin, David",10.1145/3014812.3014867,2017,"Searching for relevant information in multi-disciplinary web repositories is becoming a topic of increasing interest among the computer science research community. To date, methods and techniques to extract useful and relevant information from online repositories of research data have largely been based on static full text indexing which entails a 'produce once and use forever' kind of strategy. That strategy is fast becoming insufficient due to increasing data volume, concept obsolescence, and complexity and heterogeneity of content types in web repositories. We propose that by automatic semantic annotation of content in web repositories (using Linked Open Data or LoD sources) without using domain-specific ontologies, we can sustain the performance of searching by retrieving highly relevant search results. Secondly, we claim that by expert crowd-annotation of content on top of automatic semantic annotation, we can enrich the semantic index over time to augment the contextual value of content in web repositories so that they remain findable despite changes in language, terminology and scientific concepts. We deployed a custom-built annotation, indexing and searching environment in a web repository website that has been used by expert annotators to annotate webpages using free text and vocabulary terms. We present our findings based on the annotation and tagging data on top of LoD-based annotations and the overall modus operandi. We also analyze and demonstrate that by adding expert annotations to the existing semantic index, we can improve the relationship between query and documents using Cosine Similarity Measures (CSM)."
A Curriculum Model Featuring Oral Communication Instruction and Practice,"Anewalt, Karen and Polack, Jennifer",10.1145/3017680.3017775,2017,"Good oral communication skills are essential for success in the workplace. Burge [3] recently highlighted the need to create a curriculum-wide program emphasizing communication skills. We have implemented a curriculum that provides communication skills instruction and practice at each level of the computer science major. Here we present a description of the coursework emphasizing formal presentation and small group communication skills. We also share findings from a survey of alumni showing their perception of communication preparation. Even in our program that provides significant opportunities for communication skill development, the majority of alumni felt that additional instruction should be integrated into the major curriculum."
Software effort estimation using classical analogy ensembles based on random subspace,"Hosni, Mohamed and Idri, Ali",10.1145/3019612.3019784,2017,"Software effort estimation is one of the important and complex tasks in software project management. It influences almost all the process of software development such as: bidding, planning, and budgeting. Hence, estimating the software project effort in early stages of the software life cycle is considered the key of success of any project. To this goal, many techniques have been proposed to predict the effort required to develop a software system. Unfortunately, there is no consensus about the single best technique. Recently, Ensemble Effort Estimation has been investigated to estimate software effort and consists on generating the software effort by combining more than one solo estimation technique by means of a combination rule. In this paper, we have developed different homogeneous ensembles based on combination of Random Subspace method and Classical Analogy technique using two linear rules over seven datasets. The results confirm that the Random Space ensembles outperform the solo Classical Analogy regardless of the dataset used and that the median rule generates better estimation than the average one."
Empirical evaluation of fuzzy analogy for software development effort estimation,"Abnane, Ibtissam and Idri, Ali and Abran, Alain",10.1145/3019612.3019905,2017,"Software Development Effort Estimation (SDEE) plays a primary role in software project management. Among several techniques suggested for estimating software development effort, analogy-based software effort estimation approaches stand out as promising techniques. In this paper, the performance of Fuzzy Analogy is compared with that of six other SDEE techniques (Linear Regression, Support Vector Regression, Multi-Layer Perceptron, M5P and Classical Analogy). The first step of the evaluation aimed to ensure that the SDEE techniques outperformed random guessing by using the Standardized Accuracy (SA). Then, we used a set of reliable performance measures and Borda count to rank them and identify which techniques are the most accurate. The results suggest that Fuzzy Analogy statistically outperformed the other SDEE techniques regardless of the dataset used."
A Network-Fusion Guided Dashboard Interface for Task-Centric Document Curation,"Jones, Paul and Sharma, Shivani and Moon, Changsung and Samatova, Nagiza F.",10.1145/3025171.3025177,2017,"Knowledge workers are being exposed to more information than ever before, as well as having to work in multi-tasking and collaborative environments. There is an increasing need for interfaces and algorithms to help automatically keep track of documents that are associated with both individual and team tasks. Previous approaches to the problem of automatically applying task labels to documents have been limited to small feature spaces or have not taken into account multi-user environments. Many different clues to potential task associations are available through user, task and document similarity metrics, as well as through temporal patterns in individual and team workflows. We present a network-fusion algorithm for automatic task-centric document curation, and show how this can guide a recent-work dashboard interface, which organizes user's documents and gathers feedback from them. Our approach efficiently computes representations of users, tasks and documents in a common vector space, and can easily take into account many different types of associations through the creation of edges in a multi-layer graph. We have demonstrated the effectiveness of this approach using labelled document corpora from three empirical studies with students and intelligence analysts. We have also shown how to leverage relationships between different entity types to increase classification accuracy by up to 20% over a simpler baseline, and with as little as 10% labelled data."
Attributes that Predict which Features to Fix: Lessons for App Store Mining,"Licorish, Sherlock A. and Savarimuthu, Bastin Tony Roy and Keertipati, Swetha",10.1145/3084226.3084246,2017,"Requirements engineering is assessed as the most important phase of the software development process. This process is especially challenging for app developers, who tend to gather crowd-based feedback after releasing their apps. This feedback is often voluminous, posing prioritization challenges for developers identifying features to fix or add. While previous work has identified frequently mentioned features, and some effort has been dedicated towards providing various prioritization and classification techniques, these do not quite address the prioritization challenge faced by app developers given voluminous app reviews. In fact, there is also need to assess the scale of app reviews' usefulness. We use content analysis and regression to contribute towards this cause by exploring the usefulness of app reviews, and the attributes that predict which app features to fix, respectively. Our outcomes show that reviews tended to either provide information of little value (i.e., no actionable information) or highlighted problems that may directly affect the functionality of app features. For two different apps, we also observe that features that were mentioned the most (the feature frequency attribute) in lower ranked reviews provided by users had the strongest predictive power for identifying severely broken features (as perceived by a developer). However, the ordering did not match with the frequency with which reports were made by users. There were also variances in the attributes that predict which features to fix, for the reviews of different apps. Review mining and prioritization challenges remain given variances in app reviews' content and structure. These findings also point to the need to redesign app review interfaces to consider how reviews are captured."
A Two-staged Survey on Release Readiness,"Al Alam, S. M. Didar and Nayebi, Maleknaz and Pfahl, Dietmar and Ruhe, Guenther",10.1145/3084226.3084254,2017,"Deciding about the content and readiness when shipping a new product release can have a strong impact on the success (or failure) of the product. Having formerly analyzed the state-of-the art in this area, the objective for this paper was to better understand the process and rationale of real-world release decisions and to what extent research on release readiness is aligned with industrial needs. We designed two rounds of surveys with focus on the current (Survey-A) and the desired (Survey-B) process of how to make release readiness decisions. We received 49 and 40 valid responses for Survey-A and Survey-B, respectively.In total, we identified 12 main findings related to the process, the rationale and the tool support considered for making release readiness decisions. We found that reasons for failed releases and the factors considered for making release decisions are context specific and vary with release cycle time. Practitioners confirmed that (i) release readiness should be measured and continuously monitored during the whole release cycle, (ii) release readiness decisions are context-specific and should not be based solely on quality considerations, and iii) some of the observed reasons for failed releases such as low functionality, high cost, and immature service are not adequately studied in research where there is dominance on investigating quality and testing only. In terms of requested tool support, dashboards covering multidimensional aspects of the status of release development were articulated as key requirements."
From Data to Big Data: Moroccan Public Sector,"Khtira, R. and Elasri, B. and Rhanoui, M.",10.1145/3090354.3090401,2017,"Digitalization, interconnection, open data, and the use of internet and social media by governments and citizens have consequently leaded to an enormous growth of data in the public sector. As government data available increases, many big data initiatives have been launched by governments in order to derive insights and create new value in many areas. This paper details some concepts related to government data specifically open data and big data in order to draw the relationships between them and show its potential value. The article also brings into light the impact of digital progress, made so far in the Moroccan Administration, on the growth of data in the public sector, and presents first steps taken by the government toward big data era. In particular, we focus on social media, open government data and e-government initiatives. In addition, the paper showcases examples of applying advanced analytics in the public finances specifically in Tax Administration to uncover insights and make better decisions from large datasets. This article also overviews some challenges facing future big data initiatives in the Moroccan public sector and proposes recommendations to address them."
Neural style transfer: a paradigm shift for image-based artistic rendering?,"Semmo, Amir and Isenberg, Tobias and D\""{o}llner, J\""{u}rgen",10.1145/3092919.3092920,2017,"In this meta paper we discuss image-based artistic rendering (IB-AR) based on neural style transfer (NST) and argue, while NST may represent a paradigm shift for IB-AR, that it also has to evolve as an interactive tool that considers the design aspects and mechanisms of artwork production. IB-AR received significant attention in the past decades for visual communication, covering a plethora of techniques to mimic the appeal of artistic media. Example-based rendering represents one the most promising paradigms in IB-AR to (semi-)automatically simulate artistic media with high fidelity, but so far has been limited because it relies on pre-defined image pairs for training or informs only low-level image features for texture transfers. Advancements in deep learning showed to alleviate these limitations by matching content and style statistics via activations of neural network layers, thus making a generalized style transfer practicable. We categorize style transfers within the taxonomy of IB-AR, then propose a semiotic structure to derive a technical research agenda for NSTs with respect to the grand challenges of NPAR. We finally discuss the potentials of NSTs, thereby identifying applications such as casual creativity and art production."
The Data Warehousing (R) Evolution: Where's it headed next?,"Smith, Jeffrey and Rege, Manjeet",10.1145/3093241.3093268,2017,"This paper provides an overview of the history and current state of data warehousing and corporate analytics. It begins with a quick review of the history of the data warehouse and then does a deeper dive into subsets of this space including data integration, the DBMS, business intelligence and analytics, advanced analytics, and information stewardship. It finishes with a quick review of some of the leading trends in data warehousing including Big Data and the Logical Data Warehouse, Hybrid Transaction Analytical Processing and In-Memory Computing."
Visual Perception of Robots based on Sparse Representation and Optimization,"Fengtao, Xiang and Xueqiang, Gu and Yu, Xie and Liang, Pan and Jian, Zhang",10.1145/3094243.3094251,2017,"According to different working environment, robots have to configure different sensors. One of the most important is vision sensor. The visual perception is capturing and processing images or image sequences of around environment by vision sensors and computers. Currently, sparse representation and optimization is hot research point in the visual perception area. The presentation of Compressed Sensing (CS) theory makes the problem solving by sparse representation and optimization very popular. Some issues are introduced orderly, such as, image reconstruction, image fusion, face/object recognition, feature extraction and dimension reduction, etc. Besides, the paper summarizes some important algorithms proposed by scientists and engineers at home and abroad. Finally, the mathematical frameworks for above issues are established. The paper also proposes some new methods about image fusion based on online dictionary learning, object recognition using discriminative and collaborative representation, sparse preserving projections for dimensional reduction."
Discovering Hidden Course Requirements and Student Competences from Grade Data,"Houbraken, Mara and Sun, Chang and Smirnov, Evgueni and Driessens, Kurt",10.1145/3099023.3099034,2017,"This paper presents a data driven approach to autonomous course-competency requirement and student-competency level discovery starting from the grades obtained by a sufficiently large set of students. The approach relies on collaborative filtering techniques, more precisely matrix decomposition, to derive the hidden competency requirements and levels that together should be responsible for observed grades. The discovered hidden features are translated into human understandable competencies by matching the computed values to expert input. The approach also allows for grade prediction for so far unobserved student course combinations, allowing for personalized study planning and student guidance. The technique is demonstrated on data from a ""Data Science and Knowledge Engineering"" Bachelor study, Maastricht University."
Revisiting unsupervised learning for defect prediction,"Fu, Wei and Menzies, Tim",10.1145/3106237.3106257,2017,"Collecting quality data from software projects can be time-consuming and expensive. Hence, some researchers explore ""unsupervised"" approaches to quality prediction that does not require labelled data. An alternate technique is to use ""supervised"" approaches that learn models from project data labelled with, say, ""defective"" or ""not-defective"". Most researchers use these supervised models since, it is argued, they can exploit more knowledge of the projects. At FSE-16, Yang et al. reported startling results where unsupervised defect predictors outperformed supervised predictors for effort-aware just-in-time defect prediction. If confirmed, these results would lead to a dramatic simplification of a seemingly complex task (data mining) that is widely explored in the software engineering literature. This paper repeats and refutes those results as follows. (1) There is much variability in the efficacy of the Yang et al. predictors so even with their approach, some supervised data is required to prune weaker predictors away. (2) Their findings were grouped across N projects. When we repeat their analysis on a project-by-project basis, supervised predictors are seen to work better. Even though this paper rejects the specific conclusions of Yang et al., we still endorse their general goal. In our our experiments, supervised predictors did not perform outstandingly better than unsupervised ones for effort-aware just-in-time defect prediction. Hence, they may indeed be some combination of unsupervised learners to achieve comparable performance to supervised ones. We therefore encourage others to work in this promising area."
Automated identification of security issues from commit messages and bug reports,"Zhou, Yaqin and Sharma, Asankhaya",10.1145/3106237.3117771,2017,"The number of vulnerabilities in open source libraries is increasing rapidly. However, the majority of them do not go through public disclosure. These unidentified vulnerabilities put developers' products at risk of being hacked since they are increasingly relying on open source libraries to assemble and build software quickly. To find unidentified vulnerabilities in open source libraries and secure modern software development, we describe an efficient automatic vulnerability identification system geared towards tracking large-scale projects in real time using natural language processing and machine learning techniques. Built upon the latent information underlying commit messages and bug reports in open source projects using GitHub, JIRA, and Bugzilla, our K-fold stacking classifier achieves promising results on vulnerability identification. Compared to the state of the art SVM-based classifier in prior work on vulnerability identification in commit messages, we improve precision by 54.55% while maintaining the same recall rate. For bug reports, we achieve a much higher precision of 0.70 and recall rate of 0.71 compared to existing work. Moreover, observations from running the trained model at SourceClear in production for over 3 months has shown 0.83 precision, 0.74 recall rate, and detected 349 hidden vulnerabilities, proving the effectiveness and generality of the proposed approach."
Test optimisation for Highly-Configurable Cyber-Physical Systems,"Markiegi, Urtzi",10.1145/3109729.3109745,2017,"Cyber-Physical Systems (CPS) have become one of the core-enabling technologies for multiple domains, such as manufacturing, healthcare, energy and transportation. Furthermore, these domains are demanding CPS to be highly-configurable in order to respond to multiple and changing market requirements. Testing these Highly-Configurable Cyber-Physical Systems (HCCPS) is challenging. First, when working with CPSs, considerable time is required in order to tackle physical processes during testing. And secondly, in highly-configurable systems, a large number of system variants need to be tested. Consequently, reducing HCCPS testing time is essential.In this context, a research work is presented to reduce the overall testing time of HCCPS, focusing on a merged strategy of product and test cases optimisation. In particular, two approaches are proposed in order to achieve the testing time reduction. The first approach aims to reduce the HCCPS testing time by an iterative allocation of products and test cases. The second approach aims to reduce the HCCPS testing time by a feedback driven dynamic and iterative allocation of products and test cases.A preliminary experiment has been undertaken to test the iterative allocation approach. In this experiment, products to be tested are selected and prioritised. Next, multiple testing iterations are perform until the time-budget is consumed. In each iteration a small number of test cases are allocated for each of the products to be tested. The experiment was evaluated with an academic HCCPS and preliminary results suggest that the proposed approach reduces the fault detection time when compared with traditional approaches."
"An email attachment is worth a thousand words, or is it?","Tsipenyuk, Gregory and Crowcroft, Jon",10.1145/3109761.3109765,2017,"There is an extensive body of research on Social Network Analysis (SNA) based on the email arhive. The network used in the analysis is generally extracted either by capturing the email communication in From, To, Cc and Bcc email header fields or by the entities contained in the email message. In the latter case, the entities could be, for instance, the bag of words, url's, names, phones, etc. It could also include the textual content of attachments, for instance Microsoft Word documents, excel spreadsheets, or Adobe pdfs. The nodes in this network represent users and entities. The edges represent communication between users and relations to the entities. We suggest taking a different approach to the network extraction and use attachments shared between users as the edges. The motivation for this is two-fold. First, attachments represent the ""intimacy"" manifestation of the relation's strength. Second, the statistical analysis of private email archives that we collected and Enron email corpus shows that the attachments contribute in average around 80-90% to the archive's disk-space usage, which means that most of the data is presently ignored in the SNA of email archives. Consequently, we hypothesize that this approach might provide more insight into the social structure of the email archive. We extract the communication and shared attachments networks from Enron email corpus. We further analyze degree, betweenness, closeness, and eigenvector centrality measures in both networks and review the differences and what can be learned from them. We use nearest neighbor algorithm to generate similarity groups for five Enron employees. The groups are consistent with Enron's organizational chart, which validates our approach."
Evaluation of password encrypted key exchange authentication techniques: design approach perspective: evaluation of PAKE protocol,"Vollala, Satyanarayana and S, Indrajeet and Begum, B. Shameedha and Ramasubramanian, N.",10.1145/3109761.3109777,2017,"Password Authenticated Key Exchange (PAKE) is an authentication mechanism used to establish a secure connection between communicating parties with a shared password. Since users are accustomed to employing weak passwords to protect their systems, the vulnerability considerably increases. In order to obtain a robust and practical solution against numerous attacks like undetectable online and offline password guessing attacks, trivial attacks, replay attacks and man-in-the-middle attacks, several variants have been proposed over the years. SPOKE (Simple Password Only Key Exchange) Protocol, J-PAKE (Password Authenticated Key Exchange by Juggling) Protocol and Chaos-Based 3PAKE Protocol are some of the novel proposals that trace their roots to the first PAKE protocol introduced by Bellovin and Merrit. In this paper, significant design approaches and modifications implemented in developing PAKE protocols are discussed. Suitable PAKE schemes are identified in regard to applications and communication environment by analyzing the performance and security aspects of these protocols."
Mining mobile app markets for prioritization of security assessment effort,"Sadeghi, Alireza and Esfahani, Naeem and Malek, Sam",10.1145/3121264.3121265,2017,"Like any other software engineering activity, assessing the security of a software system entails prioritizing the resources and minimizing the risks. Techniques ranging from the manual inspection to automated static and dynamic analyses are commonly employed to identify security vulnerabilities prior to the release of the software. However, none of these techniques is perfect, as static analysis is prone to producing lots of false positives and negatives, while dynamic analysis and manual inspection are unwieldy, both in terms of required time and cost. This research aims to improve these techniques by mining relevant information from vulnerabilities found in the app markets. The approach relies on the fact that many modern software systems, in particular mobile software, are developed using rich application development frameworks (ADF), allowing us to raise the level of abstraction for detecting vulnerabilities and thereby making it possible to classify the types of vulnerabilities that are encountered in a given category of application. By coupling this type of information with severity of the vulnerabilities, we are able to improve the efficiency of static and dynamic analyses, and target the manual effort on the riskiest vulnerabilities."
Multi-Modal Knowledge Representation Learning via Webly-Supervised Relationships Mining,"Nian, Fudong and Bao, Bing-Kun and Li, Teng and Xu, Changsheng",10.1145/3123266.3123443,2017,"Knowledge representation learning (KRL) encodes enormous structured information with entities and relations into a continuous low-dimensional semantic space. Most conventional methods solely focus on learning knowledge representation from single modality, yet neglect the complementary information from others. The more and more rich available multi-modal data on Internet also drive us to explore a novel approach for KRL in multi-modal way, and overcome the limitations of previous single-modal based methods. This paper proposes a novel multi-modal knowledge representation learning (MM-KRL) framework which attempts to handle knowledge from both textual and visual modal web data. It consists of two stages, i.e., webly-supervised multi-modal relationship mining, and bi-enhanced cross-modal knowledge representation learning. Compared with existing knowledge representation methods, our framework has several advantages: (1) It can effectively mine multi-modal knowledge with structured textual and visual relationships from web automatically. (2) It is able to learn a common knowledge space which is independent to both task and modality by the proposed Bi-enhanced Cross-modal Deep Neural Network (BC-DNN). (3) It has the ability to represent unseen multi-modal relationships by transferring the learned knowledge with isolated seen entities and relations into unseen relationships. We build a large-scale multi-modal relationship dataset (MMR-D) and the experimental results show that our framework achieves excellent performance in zero-shot multi-modal retrieval and visual relationship recognition."
"Towards Age-friendly E-commerce Through Crowd-Improved Speech Recognition, Multimodal Search, and Personalized Speech Feedback","Meng, Lei and Nguyen, Quy Hy and Tian, Xiaohai and Shen, Zhiqi and Chng, Eng Siong and Guan, Frank Yunqing and Miao, Chunyan and Leung, Cyril",10.1145/3126973.3129306,2017,"This paper presents an age-friendly system for improving the elderly's online shopping experience. Different from most related studies focusing on website design and content organization, we propose to integrate three assistive techniques to facilitate the elderly's browsing of products in E-commerce platforms, including the crowd-improved speech recognition, the multimodal search, and the personalized speech feedback. The first two techniques, namely, the crowd-improved speech recognition and the multimodal search, work together to allow the elderly search for desired products flexibly using either speech, an image, text, or any combination of them whichever are convenient for the elderly. The personalized speech feedback provides a speech summary of search result in a personalized voice. That is, the elderly are allowed to choose or even create their desired voices, and also can customize the voices in terms of pitch, speaking speed, and loudness. As a whole, the proposed system is expected to help and engage the elderly's E-commerce adoption. Testing on real-world E-commerce product datasets demonstrated the usability of the proposed system."
Clustering Dycom: An Online Cross-Company Software Effort Estimation Study,"Minku, Leandro L. and Hou, Siqing",10.1145/3127005.3127007,2017,"Background: Software Effort Estimation (SEE) can be formulated as an online learning problem, where new projects are completed over time and may become available for training. In this scenario, a Cross-Company (CC) SEE approach called Dycom can drastically reduce the number of Within-Company (WC) projects needed for training, saving the high cost of collecting such training projects. However, Dycom relies on splitting CC projects into different subsets in order to create its CC models. Such splitting can have a significant impact on Dycom's predictive performance. Aims: This paper investigates whether clustering methods can be used to help finding good CC splits for Dycom. Method: Dycom is extended to use clustering methods for creating the CC subsets. Three different clustering methods are investigated, namely Hierarchical Clustering, K-Means, and Expectation-Maximisation. Clustering Dycom is compared against the original Dycom with CC subsets of different sizes, based on four SEE databases. A baseline WC model is also included in the analysis. Results: Clustering Dycom with K-Means can potentially help to split the CC projects, managing to achieve similar or better predictive performance than Dycom. However, K-Means still requires the number of CC subsets to be pre-defined, and a poor choice can negatively affect predictive performance. EM enables Dycom to automatically set the number of CC subsets while still maintaining or improving predictive performance with respect to the baseline WC model. Clustering Dycom with Hierarchical Clustering did not offer significant advantage in terms of predictive performance. Conclusion: Clustering methods can be an effective way to automatically generate Dycom's CC subsets."
Multi-objective search-based approach to estimate issue resolution time,"Al-Zubaidi, Wisam Haitham Abbood and Dam, Hoa Khanh and Ghose, Aditya and Li, Xiaodong",10.1145/3127005.3127011,2017,"Background: Resolving issues is central to modern agile software development where a software is developed and evolved incrementally through series of issue resolutions. An issue could represent a requirement for a new functionality, a report of a software bug or a description of a project task.Aims: Knowing how long an issue will be resolved is thus important to different stakeholders including end-users, bug reporters, bug triagers, developers and managers. This paper aims to propose a multi-objective search-based approach to estimate the time required for resolving an issue.Methods: Using genetic programming (a meta-heuristic optimization method), we iteratively generate candidate estimate models and search for the optimal model in estimating issue resolution time. The search is guided simultaneously by two objectives: maximizing the accuracy of the estimation model while minimizing its complexity.Results: Our evaluation on 8,260 issues from five large open source projects demonstrate that our approach significantly (p &lt; 0.001) outperforms both the baselines and state-of-the-art techniques.Conclusions: Evolutionary search-based approaches offer an effective alternative to build estimation models for issue resolution time. Using multiple objectives, one for measuring the accuracy and the other for the complexity, helps produce accurate and simple estimation models."
A Large-Scale Study of Modern Code Review and Security in Open Source Projects,"Thompson, Christopher and Wagner, David",10.1145/3127005.3127014,2017,"Background: Evidence for the relationship between code review process and software security (and software quality) has the potential to help improve code review automation and tools, as well as provide a better understanding of the economics for improving software security and quality. Prior work in this area has primarily been limited to case studies of a small handful of software projects. Aims: We investigate the effect of modern code review on software security. We extend and generalize prior work that has looked at code review and software quality. Method: We gather a very large dataset from GitHub (3,126 projects in 143 languages, with 489,038 issues and 382,771 pull requests), and use a combination of quantification techniques and multiple regression modeling to study the relationship between code review coverage and participation and software quality and security. Results: We find that code review coverage has a significant effect on software security. We confirm prior results that found a relationship between code review coverage and software defects. Most notably, we find evidence of a negative relationship between code review of pull requests and the number of security bugs reported in a project. Conclusions: Our results suggest that implementing code review policies within the pull request model of development may have a positive effect on the quality and security of software."
An Extensive Analysis of Efficient Bug Prediction Configurations,"Osman, Haidar and Ghafari, Mohammad and Nierstrasz, Oscar and Lungu, Mircea",10.1145/3127005.3127017,2017,"Background: Bug prediction helps developers steer maintenance activities towards the buggy parts of a software. There are many design aspects to a bug predictor, each of which has several options, i.e., software metrics, machine learning model, and response variable.Aims: These design decisions should be judiciously made because an improper choice in any of them might lead to wrong, misleading, or even useless results. We argue that bug prediction configurations are intertwined and thus need to be evaluated in their entirety, in contrast to the common practice in the field where each aspect is investigated in isolation.Method: We use a cost-aware evaluation scheme to evaluate 60 different bug prediction configuration combinations on five open source Java projects.Results: We find out that the best choices for building a cost-effective bug predictor are change metrics mixed with source code metrics as independent variables, Random Forest as the machine learning model, and the number of bugs as the response variable. Combining these configuration options results in the most efficient bug predictor across all subject systems.Conclusions: We demonstrate a strong evidence for the interplay among bug prediction configurations and provide concrete guidelines for researchers and practitioners on how to build and evaluate efficient bug predictors."
Impacts of business intelligence on population health: a systematic literature review,"Cohen, L.",10.1145/3129416.3129441,2017,"""Business Intelligence"" is an area of Information Technology (IT) that involves the collection, analysis and presentation of large amounts of data. BI has been successfully applied to promote good decision making in a variety of environments, and has high potential to make a significant impact in the domain of population health. The promotion of population health is a key concern of government authorities and various health institutions and officials making decisions about interventions that may impact on population health would benefit from the use of information on population health. BI could clearly be a facilitator in this regard, but evidence of its current application and impact in this field is not easily accessible to policy makers. This systematic literature review explored the literature and provided a synthesis of information available on the current use of BI in this area, and evidence of the impact of its use on population health. An array of applications of BI for population health were found, including data warehouses, analytics, reports, data warehouse browsers, OLAP, GIS, Dashboards and Alerts. Evidence of the impact of these applications on population health was mainly anecdotal, with only one empirical study found. Issues and challenges encountered in the development and use of BI are Privacy and Security, Data Quality and Development and Maintenance of BI infrastructure"
Highly Efficient Mining of Overlapping Clusters in Signed Weighted Networks,"Hoang, Tuan-Anh and Lim, Ee-Peng",10.1145/3132847.3133004,2017,"In many practical contexts, networks are weighted as their links are assigned numerical weights representing relationship strengths or intensities of inter-node interaction. Moreover, the links' weight can be positive or negative, depending on the relationship or interaction between the connected nodes. The existing methods for network clustering however are not ideal for handling very large signed weighted networks. In this paper, we present a novel method called LPOCSIN (short for ""Linear Programming based Overlapping Clustering on Signed Weighted Networks"") for efficient mining of overlapping clusters in signed weighted networks. Different from existing methods that rely on computationally expensive cluster cohesiveness measures, LPOCSIN utilizes a simple yet effective one. Using this measure, we transform the cluster assignment problem into a series of alternating linear programs, and further propose a highly efficient procedure for solving those alternating problems. We evaluate LPOCSIN and other state-of-the-art methods by extensive experiments covering a wide range of synthetic and real networks. The experiments show that LPOCSIN significantly outperforms the other methods in recovering ground-truth clusters while being an order of magnitude faster than the most efficient state-of-the-art method."
Continuous Requirements Engineering,"Kirikova, Marite",10.1145/3134302.3134304,2017,"The paper presents fractal functional architecture of the FREEDOM framework of continuous requirements engineering and illustrates its applicability using three methods of continuous requirements engineering. The fractal architecture of the continuous requirements engineering framework assists in representing scalability in requirements engineering. It also supports flexibility of requirements engineering by providing the ability to adjust to different systems development contexts, different software engineering approaches, and different requirements handling methods."
Digitising a medical clerking system with multimodal interaction support,"South, Harrison and Taylor, Martin and Dogan, Huseyin and Jiang, Nan",10.1145/3136755.3136758,2017,"The Royal Bournemouth and Christchurch Hospitals (RBCH) use a series of paper forms to record their interactions with patients; while these have been highly successful, the world is moving digitally and the National Health Service (NHS) has planned to be completely paperless by 2020. Using a project management methodology called Scrum that is supported by a usability evaluation technique called System Usability Scale (SUS) and a workload measurement technique called NASA TLX, a prototype web application system was built and evaluated for the client. The prototype used a varied set of input mediums including voice, text and stylus to ensure that users were more likely to adopt the system. This web based system was successfully developed and evaluated at RBCH. This evaluation showed that the application was usable and accessible but raised many different questions about the nature of software in hospitals. While the project looked at how different input mediums can be used in a hospital, it found that just because it is possible to input data is some familiar format (e.g. voice), it is not always in the best interest of the end-users and the patients."
Mutated Policies: Towards Proactive Attribute-based Defenses for Access Control,"Rubio-Medrano, Carlos E. and Lamp, Josephine and Doup\'{e}, Adam and Zhao, Ziming and Ahn, Gail-Joon",10.1145/3140549.3140553,2017,"Recently, both academia and industry have recognized the need for leveraging real-time information for the purposes of specifying, enforcing and maintaining rich and flexible authorization policies. In such a context, security-related properties, a.k.a., attributes, have been recognized as a convenient abstraction for providing a well-defined representation of such information, allowing for them to be created and exchanged by different independently-run organizational domains for authorization purposes. However, attackers may attempt to compromise the way attributes are generated and communicated by recurring to hacking techniques, e.g., forgery, in an effort to bypass authorization policies and their corresponding enforcement mechanisms and gain unintended access to sensitive resources as a result.In this paper, we propose a novel technique that allows for enterprises to pro-actively collect attributes from the different entities involved in the access request process, e.g., users, subjects, protected resources, and running environments. After the collection, we aim to carefully select the attributes that uniquely identify the aforementioned entities, and randomly mutate the original access policies over time by adding additional policy rules constructed from the newly-identified attributes. This way, even when attackers are able to compromise the original attributes, our mutated policies may offer an additional layer of protection to deter ongoing and future attacks. We present the rationale and experimental results supporting our proposal, which provide evidence of its suitability for being deployed in practice."
Investigating heterogeneous ensembles with filter feature selection for software effort estimation,"Hosni, Mohamed and Idri, Ali and Abran, Alain",10.1145/3143434.3143456,2017,"Ensemble Effort Estimation (EEE) consists on predicting the software development effort by combining more than one single estimation technique. EEE has recently been investigated in software development effort estimation (SDEE) in order to improve the estimation accuracy. The overall results suggested that the EEE yield better prediction accuracy than single techniques. On the other hand, feature selection (FS) methods have been used in the area of SDEE for the purpose of reducing the dimensionality of a dataset size by eliminating the irrelevant and redundant features. Thus, the SDEE techniques are trained on a dataset with relevant features which can lead to improving the accuracy of their estimations. This paper aims at investigating the impact of two Filter feature selection methods: Correlation based Feature Selection (CFS) and RReliefF on the estimation accuracy of Heterogeneous (HT) ensembles. Four machine learning techniques (K-Nearest Neighbor, Support Vector Regression, Multilayer Perceptron and Decision Trees) were used as base techniques for the HT ensembles of this study. We evaluate the accuracy of these HT ensembles when their base techniques were trained on datasets preprocessed by the two feature selection methods. The HT ensembles use three combination rules: average, median, and inverse ranked weighted mean. The evaluation was carried out by means of eight unbiased accuracy measures through the leave-one-out-cross validation (LOOCV) technique over six datasets. The overall results suggest that all the attributes of most datasets used are relevant for building an accurate predictive technique since the ensembles constructed without features selection outperformed in general the ones using features selection. As for the combination rule, the median generally produces better results than the other two used in this empirical study."
Human Capital in the Information Society and the Wage Difference Factors,"Aletdinova, Anna and Bakaev, Maxim",10.1145/3143699.3143744,2017,"In the age of e-civilization, human capital that is capable of knowledge-based innovations gets special attention. We in our paper focus on researching new forms of social and labor relations and the effect of education, work experience and certain other factors on the wages. We supplement traditional labor statistics with online data collected and structured with a dedicated software system and with personal character features assessed with psychological diagnostics methods. We highlight the two groups of workers in modern economy -- freelancers and full-time employees -- and outline benefits and disadvantages of remote work, based on quantitative evaluations obtained from the modified Mincer model. Particularly, we attained the values for the effect of education level, work experience, residence location, and personal features on wages in the Siberian Federal Okrug."
Usability evaluation focused on user experience of repositories related to energy sustainability: A Literature Mapping,"Gonz\'{a}lez-P\'{e}rez, Laura Icela and Ram\'{\i}rez-Montoya, Mar\'{\i}a Soledad and Garc\'{\i}a-Pe\~{n}alvo, Francisco J. and Cruz, Juan Eliezer Quintas",10.1145/3144826.3145385,2017,"This paper presents a systematic literature mapping about two types of studies about interaction using repositories energy sustainability, (1) studies related to users' experience, and (2) studies about usability evaluation, to add 78 studies in this field of knowledge, in order to answer the following research questions: How many studies have been done to evaluate the usability of a Repository? How many studies have been done to use the approach the user experience on repositories? and What are the dimensions and tools used for evaluate usability? Ultimately, the collected information will be used to document the state of a PhD thesis that aims to create a prototype for the usability evaluation of OAR that will lend visibility to the results of a project entitled ""A Binational Laboratory for the Intelligent Management of Energy Sustainability and Technological Formation."""
Learning Analytics as a Core Component for Higher Education Disruption: Governance Stakeholder,"Moreira, Fernando and Gon\c{c}alves, Ramiro and Martins, Jos\'{e} and Branco, Frederico and Au-Yong-Oliveira, Manuel",10.1145/3144826.3145387,2017,"Higher education institutions are at this stage, on the one hand, faced with challenges never seen before and, on the other hand, their action is moving very rapidly into digital learning spaces. These challenges are increasingly complex because of the global competition for resources, students and teachers. In addition, the amount of data produced inside and outside higher education institutions has grown exponentially, so more and more institutions are exploring the potential of Big Data to meet these challenges. In this context, higher education institutions and key stakeholders (students, teachers, and governance) can derive multiple benefits from learning analytics using different data analysis strategies to produce summative, real-time and predictive information and recommendations. However, it may be questioned whether institutions, academic administrative staff as well as including those with responsibility for governance, are prepared for learning analytics? As a response to the question raised in this paper is presented an extension of a disruptive conceptual approach to higher education, using information gathered by IoT and based on Big Data &amp; Cloud Computing and Learning Analytics analysis tools, with the main focus on the stakeholder governance."
Revisiting and Rethinking on the Technical Aspects of Software Requirements,"Ali, Kamran and Xia, Xiaoling",10.1145/3148453.3306274,2018,"Software Requirement specification document is the building block in software development. SRS plays a significantly important role for developing any type of Software. This is the first deliverable to the client. This paper presents the key features and technical aspects of SRS document which help in resolving myriad problems which occur during designing and developing of Software Application. In this study, we will cover major areas of SRS document which will be of crucial help to whole project team for programming any sort of application like web application, mobile application and desktop application."
"Scrumage: A Method for Incorporating Multiple, Simultaneous Pedagogical Styles in the Classroom","Duvall, Shannon and Hutchings, Dugald Ralph and Duvall, Robert C.",10.1145/3159450.3159596,2018,"Pedagogical approaches abound in computer science. Common approaches include flipped classrooms, active learning, gamification, and the traditional lecture-based approach. There are also a wide variety of computer science learning materials including videos, interactive tutorials, and textbooks (whether presented online or on paper). Instructor choices of approach and materials present a series of trade-offs and may favor some groups of students over others. We propose a method, Scrumage, (SCRUM for AGile Education) in an attempt to overcome the necessity of making trade-offs. We allow each student in a course to select among several pedagogical approaches and sets of materials to fit each individual student's learning needs and desires. Scrumage adapts concepts from the Scrum project management technique. In Scrum, project teams are developing a product for a client. In Scrumage, student teams are developing knowledge with support from the instructor. We define and motivate Scrumage and discuss the implementation and outcomes of the technique in a class at our undergraduate institution."
Integrating Project Based Learning and Project Management for Software Engineering Teaching: An Experience Report,"Fioravanti, Maria Lydia and Sena, Bruno and Paschoal, Leo Natan and Silva, La\'{\i}za R. and Allian, Ana P. and Nakagawa, Elisa Y. and Souza, Simone R.S. and Isotani, Seiji and Barbosa, Ellen F.",10.1145/3159450.3159599,2018,"Software Engineering (SE) is an important topic to be taught in Computer Science courses. However, teaching of theoretical concepts with no link to their practical applications or no examples in the student's context may discourage learning, justifying why teaching and learning are great challenges of education in universities. In attempt to bridge such gap, several approaches have been proposed and applied to improve teaching and learning SE such as project based learning (PBL), a well-known approach already applied to teach SE. Nevertheless, there's a lack of understanding about how to better apply PBL and how to take advantage of this approach, for future use. There is also a lack of experience report describing how to use, its challenges and difficulties, what could be hampering to widely adopt it. We present our experience applying a PBL approach combined with project management to create an environment considering aspects such as dealing with managers and real stakeholders. The goal is to bring students closer to the reality of developing a software project in the business context. Our experience indicates positive results on the adoption of a PBL approach. In general, students were enthusiastic and positive about the use of this approach, the presence of a manager and the importance of using real-world problems with real stakeholders."
Using enterprise architecture model analysis and description logics for maturity assessment,"Proen\c{c}a, Diogo and Borbinha, Jos\'{e}",10.1145/3167132.3167140,2018,"A Maturity Model represents a path towards an increasingly organized and systematic way of doing business. It is therefore a widely used technique valuable to assess certain aspects of organizations, as for example business processes. A maturity assessment can enable stakeholders to clearly identify strengths and improvement points, and prioritize actions in order to reach higher maturity levels. Doing maturity assessments can range from simple self-assessment questionnaires to full-blown assessment methods, such as those recommended by the ISO/IEC 15504 or the SEI CMMI. A main caveat of these assessments is the resources they encompass. In addition, many times the lack of automation renders benchmarks not possible. Assuming that the wide spread of Enterprise Architecture practices is making the modeling of business domains a fact, and considering the recent state of the art on the representation of those models as ontologies, this paper proposes how existing semantic technology can be used to automate maturity models assessment methods."
Safetrace: a safety-driven requirement traceability framework on device interaction hazards for MD PnP,"Ou, Andrew Yi-Zong and Rahmaniheris, Maryam and Jiang, Yu and Sha, Lui and Fu, Zhicheng and Ren, Shangping",10.1145/3167132.3167270,2018,"Requirements management and safety analysis have been the key foundations of the successful development of life-critical systems, and the traceability of safety-related artifacts across such systems is becoming ever more important. Unless safety analysts can trace when and how requirements and design change, their analysis will become inconsistent, and eventually fail as proof that a given system can mitigate certain faults during certification processes. However, most prior research on traceability has focused on requirements, design and source code changes, rather than the integration of safety analysis by considering device interactions such as the Medical Device plug-and-play (MD PnP) into traceability and change-impact analysis. To help fill this gap, this paper proposes a safety-driven requirement traceability framework, SafeTrace, that traces the relations between safety requirements, design, and safety analysis, and the impact of requirement and design changes on safety analysis for life-critical systems with a focus on medical device interaction hazards."
A systematic map of data analytics in breast cancer,"Idri, Ali and Chlioui, Imane and Ouassif, Bouchra El",10.1145/3167918.3167930,2018,"Data mining or Data Analytics is a set of techniques that allows to analyzing data from different perspectives and summarizing it into useful information. It is the process of finding correlations or patterns in large historical datasets. It can be applied in almost any field ranging from business to education, then to medicine. Data mining has been increasingly used in medicine, especially in oncology. Breast cancer (BC) becomes the most common cancer among females worldwide and the leading cause of death in developed countries. Many studies have attempted to apply Data mining techniques to detect survivability of cancers in human beings. This paper aims to perform a systematic mapping study to analyze and synthesize studies on the application of Data mining techniques in breast cancer. 403 articles published between 2000 and 2016 were therefore selected and analyzed according to five criteria: year and channel of publication, research type, medical task, empirical type and DM techniques. Results show that conferences and journals are the most publication venues, researchers were more interested in applying DM techniques for diagnosis of BC, historical based evaluation was the empirical type of studies most used in the evaluation of DM techniques in BC, and classification was the most investigated task of DM in BC."
Mooqita: Empowering Hidden Talents with a Novel Work-Learn Model,"Krause, Markus and Schi\""{o}berg, Doris and Smeddinck, Jan David",10.1145/3170427.3174351,2018,"We present a case study of Mooqita, a platform to support learners in online courses by enabling them to earn money, gather real job task experiences, and build a meaningful portfolio. This includes placing optional additional assignments in online courses. Learners solve these individual assignments, provide peer reviews for other learners, and give feedback on each review they receive. Based on these data points teams are selected to work on a final paid assignment. Companies offer these assignments and in return receive interview recommendations from the pool of learners together with solutions for their challenges. We report the results of a pilot deployment in an online programming course offered by UC BerkeleyX. Six learners out of 158 participants were selected for the paid group assignment paying $600 per person. Four of these six were invited for interviews at the participating companies Crowdbotics (2) and Telefonica Innovation Alpha (2)."
CNN-based Commercial Detection in TV Broadcasting,"Li, Mengyue and Guo, Yuchun and Chen, Yishuai",10.1145/3171592.3171619,2017,"TV is an important advertising media. Information of a piece of TV commercial, such as broadcasting time, the duration, the casting and etc., may reflect the business value of the host company of this commercial. An automatic commercial detection system is needed for third-party by business analysis. Previous works about TV commercial detection just apply to one kind of video, like news, sports. And those methods detect commercials at frame level, which need expensive computing cost. In this paper, we design and implement an automatic commercial detection system for TV broadcasting. This system works at shot level and detects commercials in streaming videos, including TV broadcasting and online videos. It consists of two modules, the shot boundary detection module and the shot classification module. We crawl actual broadcasting videos and split them into shots, classify these shots into two classes, commercial and non-commercial. Then, we extract shot features with deep convolutional neural network, and train a support vector machine classifier to complete shot classification. Combining the state-of-the-art convolutional neural network with traditional machine learning techniques, our system can handle various program types and indiscernible commercials, and get precision 93% and recall 95% in realistic TV programs. Except the TV broadcasting, our system works in other video media like online videos."
COSMIC Function Points Evaluation for Software Maintenance,"Hira, Anandi and Boehm, Barry",10.1145/3172871.3172874,2018,"The Common Software Measurement International Consortium (COSMIC) group reviewed the existing functional size methods, such as the International Function Points User Group (IFPUG)'s Function Points (FPs), to develop a functional size metric based on ""the basic principles"" that applies to a wide range of application domains. Though several empirical studies on the COSMIC method verify that COSMIC Function Points (CFPs) successfully accomplished the goal of being applicable to a wide range of application domains and that its size correlate well with effort over a very wide range of sizes, one study of telecom switching software noticed that the correlation between CFPs and cost is very low for small projects (5 CFPs or less). The COSMIC method does not explicitly size data manipulations (such as, mathematical algorithms), which causes it to be less effective for mathematically-intensive software. IFPUG's FPs method has the same drawback of not explicitly measuring mathematical operations, but IFPUG developed the Software Non-Functional Assessment Process (SNAP) to complement a project's functional size. This empirical analysis will determine whether CFPs can be an effective size metric for small, maintenance tasks (between 2 and 12 CFPs) using a dataset consisting of Unified Code Count (UCC)1's maintenance tasks. Additionally, this analysis will consider whether using IFPUG's SNAP with COSMIC's FPs can lead to better effort estimates, as the former provides a method to measure data manipulation. The authors found that tasks adding new features require a different effort estimate model from those that modify existing features."
Empirically Analyzing and Evaluating Security Features in Software Requirements,"Hayrapetian, Allenoush and Raje, Rajeev",10.1145/3172871.3172879,2018,"Software requirements, for complex projects, often contain specifications of non-functional attributes (e.g., security-related features). The process of analyzing such requirements for compliance is laborious and error prone. Due to the inherent free-flowing nature of software requirements, it is appealing to apply Natural Language Processing (NLP) and Machine Learning (ML)-based techniques for analyzing these documents. In this paper, we propose a semi-automatic methodology that assesses the security requirements of software systems with respect to completeness and ambiguity, creating a bridge between the requirements documents and being in compliance with standards. Security standards, such as ISO and OWASP, are compared against software project documents for textual entailment relationships. These entailment results along with the document annotations are used to train a Neural Network model to predict whether a given statement in the document is found within the security standard or not. Hence, this approach aims to identify the appropriate structures that underlie software requirements documents. Once such structures are formalized and empirically validated, they will provide guidelines to software organizations for generating comprehensive and unambiguous requirements specification documents as related to security-oriented features."
Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making,"Veale, Michael and Van Kleek, Max and Binns, Reuben",10.1145/3173574.3174014,2018,"Calls for heightened consideration of fairness and accountability in algorithmically-informed public decisions-like taxation, justice, and child protection-are now commonplace. How might designers support such human values? We interviewed 27 public sector machine learning practitioners across 5 OECD countries regarding challenges understanding and imbuing public values into their work. The results suggest a disconnect between organisational and institutional realities, constraints and needs, and those addressed by current research into usable, transparent and 'discrimination-aware' machine learning-absences likely to undermine practical initiatives unless addressed. We see design opportunities in this disconnect, such as in supporting the tracking of concept drift in secondary data sources, and in building usable transparency tools to identify risks and incorporate domain knowledge, aimed both at managers and at the 'street-level bureaucrats' on the frontlines of public service. We conclude by outlining ethical challenges and future directions for collaboration in these high-stakes applications."
Understanding International Benchmarks on Student Engagement: Awareness and Research Alignment from a Computer Science Perspective,"Morgan, Michael and Sinclair, Jane and Butler, Matthew and Thota, Neena and Fraser, Janet and Cross, Gerry and Jackova, Jana",10.1145/3174781.3174782,2018,"There is an increasing trend to use national survey instruments to measure student engagement. Unfortunately, Computer Science (CS) rates poorly on a number of measures in these surveys, even when compared to related STEM disciplines. Initial research suggests reasons for this poor performance may include a lack of awareness by CS academics of these instruments and the student engagement measures on which they are based, and a misalignment between these measures and the research focus (and teaching practice) of CS educators. This working group carried out an investigation of major engagement instruments to examine the measures they embody and track the achievement of CS with respect to the major international benchmarks. A comprehensive research mapping exercise was then conducted to examine the focus of current CS education research and its alignment to student engagement measures on which the instruments are based. The process enabled identification of examples of best practice in student engagement research in CS education. In order to better understand CS academics' perspectives on engagement a series of interviews were also conducted with CS staff. Our findings indicate that CS engagement results are, if anything, declining further. Analysis of CS education research literature shows that many authors refer to ``engagement'' (and their aim to increase it) but few attach a clear meaning to the term or offer evidence to support a link to improved engagement. Further, many initiatives reported would be unlikely to tick the boxes of the narrow, behaviourally-focussed measures covered by the major instruments. Staff interviews revealed a wide variety of beliefs about what student engagement means and what should be done to promote it in CS, including the view that many activities measured in the instruments are counter-productive for CS. This work aims to promote a greater awareness of the international benchmarks and the aspects of student engagement they measure. The results reported here can be used by CS educators to inform decisions on strategies to improve engagement and how these might relate to existing survey measures."
An adult learner's knowledge model based on ontologies and rule reasoning,"Abyaa, Abir and Idrissi, Mohammed Khalidi and Bennani, Samir",10.1145/3175628.3175656,2017,"One of the biggest challenges of Adaptive e-learning systems is learner modelling. The learner model should represent the learner's characteristics as faithfully as possible in order to provide adaptive learning. Among these characteristics, the learner's knowledge is considered to be the core characteristic of the learner model, as adaptive e-learning systems are centered on the learner's knowledge, since acquiring « knowledge » about a specific domain or concept is considered the main goal of learning and instruction. In this paper, we propose a novel adult learner's knowledge model using ontologies and rule reasoning, by trying to define the different components that construct the learner's knowledge in an exhaustive yet a simple way. The proposed model takes into account different elements of the learner's knowledge, such as the different knowledge types and categories, the learner's prior knowledge accumulated through his/her experiences, his/her misconceptions, errors and the previously learned but forgotten knowledge."
A Data-based Method for Industrial Big Data Project Prioritization,"Kuschicke, Felix and Thiele, Thomas and Meisen, Tobias and Jeschke, Sabina",10.1145/3175684.3175687,2017,"The application of Big Data Techniques (BDT) in discrete manufacturing appears to be very promising, considering lighthouse projects in this area. In general, the goal is to collect all data from manufacturing systems comprehensively, in order to enable new findings and decision support by means of appropriate Industrial Big Data (IBD) analysis procedures. However, due to limited human and economic resources, potential IBD projects need to get prioritized -- in the best case according to their cost-benefit ratio. Available methods for this purpose are insufficient, due to their limited ability to be operationalized, error-proneness, and lack of scientific evidence. In this paper, we discuss how cost-benefit-analysis frameworks can be applied to the preliminary selection of production use cases for the implementation of BDT in larger production systems. It supports the use case selection process from information about production needs, available BDT, and given condition(s) per use case. This concept paper attempts to consolidate the hitherto fragmented discourse on how to prioritize IBD projects, evaluates the challenges of prioritization in this field, and presents a prioritization concept to overcome these challenges."
Data Mining of Project Management Data: An Analysis of Applied Research Studies,"Ertek, Gurdal and Tunc, Murat Mustafa and Zhang, Allan Nengsheng and Tanrikulu, Omer and Asian, Sobhan",10.1145/3176653.3176714,2017,"Data collected and generated through and posterior to projects, such as data residing in project management software and post-project review documents, can be a major source of actionable insights and competitive advantage. This paper presents a rigorous methodological analysis of the applied research published in academic literature, on the application of data mining (DM) for project management (PM). The objective of the paper is to provide a comprehensive analysis and discussion of where and how data mining is applied for project management data and to provide practical insights for future research in the field."
Methods of Pornography Detection: Review,"Karamizadeh, Sasan and Arabsorkhi, Abouzar",10.1145/3177457.3177484,2018,"In recent years, prone images and other such indecent matter are available on the social media and the Internet for children. Filtering of image porn has become one of the big changes for searches; they are tied to finding methods to filter porn images. Social media network is interested in filter porn images from normal ones. Analysis method uses the bright image to automatically detect and filter images in the media. In this paper, we have reviewed methods such as color based, shape based, local and global feature approach, deep learning and bag-of-words for filtering porn images which include comparing with the advantages and disadvantages."
Search Process as Transitions Between Neural States,"Moshfeghi, Yashar and Pollick, Frank E.",10.1145/3178876.3186080,2018,"Search is one of the most performed activities on the World Wide Web. Various conceptual models postulate that the search process can be broken down into distinct emotional and cognitive states of searchers while they engage in a search process. These models significantly contribute to our understanding of the search process. However, they are typically based on self-report measures, such as surveys, questionnaire, etc. and therefore, only indirectly monitor the brain activity that supports such a process. With this work, we take one step further and directly measure the brain activity involved in a search process. To do so, we break down a search process into five time periods: a realisation of Information Need, Query Formulation, Query Submission, Relevance Judgment and Satisfaction Judgment. We then investigate the brain activity between these time periods. Using functional Magnetic Resonance Imaging (fMRI), we monitored the brain activity of twenty-four participants during a search process that involved answering questions carefully selected from the TREC-8 and TREC 2001 Q/A Tracks. This novel analysis that focuses on transitions rather than states reveals the contrasting brain activity between time periods - which enables the identification of the distinct parts of the search process as the user moves through them. This work, therefore, provides an important first step in representing the search process based on the transitions between neural states. Discovering more precisely how brain activity relates to different parts of the search process will enable the development of brain-computer interactions that better support search and search interactions, which we believe our study and conclusions advance."
How modern news aggregators help development communities shape and share knowledge,"Aniche, Maur\'{\i}cio and Treude, Christoph and Steinmacher, Igor and Wiese, Igor and Pinto, Gustavo and Storey, Margaret-Anne and Gerosa, Marco Aur\'{e}lio",10.1145/3180155.3180180,2018,"Many developers rely on modern news aggregator sites such as Reddit and Hacker News to stay up to date with the latest technological developments and trends. In order to understand what motivates developers to contribute, what kind of content is shared, and how knowledge is shaped by the community, we interviewed and surveyed developers that participate on the Reddit programming subreddit and we analyzed a sample of posts on both Reddit and Hacker News. We learned what kind of content is shared in these websites and developer motivations for posting, sharing, discussing, evaluating, and aggregating knowledge on these aggregators, while revealing challenges developers face in terms of how content and participant behavior is moderated. Our insights aim to improve the practices developers follow when using news aggregators, as well as guide tool makers on how to improve their tools. Our findings are also relevant to researchers that study developer communities of practice."
Sentiment analysis for software engineering: how far can we go?,"Lin, Bin and Zampetti, Fiorella and Bavota, Gabriele and Di Penta, Massimiliano and Lanza, Michele and Oliveto, Rocco",10.1145/3180155.3180195,2018,"Sentiment analysis has been applied to various software engineering (SE) tasks, such as evaluating app reviews or analyzing developers' emotions in commit messages. Studies indicate that sentiment analysis tools provide unreliable results when used out-of-the-box, since they are not designed to process SE datasets. The silver bullet for a successful application of sentiment analysis tools to SE datasets might be their customization to the specific usage context.We describe our experience in building a software library recommender exploiting developers' opinions mined from Stack Overflow. To reach our goal, we retrained---on a set of 40k manually labeled sentences/words extracted from Stack Overflow---a state-of-the-art sentiment analysis tool exploiting deep learning. Despite such an effort- and time-consuming training process, the results were negative. We changed our focus and performed a thorough investigation of the accuracy of commonly used tools to identify the sentiment of SE related texts. Meanwhile, we also studied the impact of different datasets on tool performance. Our results should warn the research community about the strong limitations of current sentiment analysis tools."
To distribute or not to distribute? why licensing bugs matter,"Vendome, Christopher and German, Daniel M. and Di Penta, Massimiliano and Bavota, Gabriele and Linares-V\'{a}squez, Mario and Poshyvanyk, Denys",10.1145/3180155.3180221,2018,"Software licenses dictate how source code or binaries can be modified, reused, and redistributed. In the case of open source projects, software licenses generally fit into two main categories, permissive and restrictive, depending on the degree to which they allow redistribution or modification under licenses different from the original one(s). Developers and organizations can also modify existing licenses, creating custom licenses with specific permissive/restrictive terms. Having such a variety of software licenses can create confusion among software developers, and can easily result in the introduction of licensing bugs, not necessarily limited to well-known license incompatibilities. In this work, we report a study aimed at characterizing licensing bugs by (i) building a catalog categorizing the types of licensing bugs developers and other stakeholders face, and (ii) understanding the implications licensing bugs have on the software projects they affect. The presented study is the result of the manual analysis of 1,200 discussions related to licensing bugs carried out in issue trackers and in five legal mailing lists of open source communities. Our findings uncover new types of licensing bugs not addressed in prior literature, and a detailed assessment of their implications."
Competence-confidence gap: a threat to female developers' contribution on github,"Wang, Zhendong and Wang, Yi and Redmiles, David",10.1145/3183428.3183437,2018,"On GitHub, contributing to a new project is crucial for a developer to gain personal growth and maximize impact in the community. It is known that female developers are often hesitant to explore the opportunities to contribute to new projects even when they possess the competence to make valuable contributions. Drawing from the literature of the competence-confidence gap, we develop a fresh explanation for this phenomenon. We validate the theoretical explanation through an empirical study using GitHub's historical data. In this study, we identify all female developers ranking in top 5,000 GitHub users. Using the Granger Causality Test, we find that, for the majority of identified female developers, initiating a pull request to a new repository is ""Granger"" caused by the quick increase of followers in the preceding couple of weeks. For most male developers, our observations show that their new pull requests have no relationship with the dynamics of follower numbers. The results indicate that the competence-confidence gap is a threat to female developers' contribution on GitHub. The research suggests that helping female developers to overcome the competence-confidence gap is critical for encouraging female's contribution open source development, as well as growing their reputations and impacts in the community."
Crowdsourced software development and maintenance,"Lin, Bin",10.1145/3183440.3183447,2018,"As modern software systems are becoming increasingly complex, developers often need to rely on online sources to address problems encountered during software development and maintenance. These resources provide developers with access to peers' expertise, covering knowledge of different software lifecycle phases, including design, implementation, and maintenance. However, exploiting such knowledge and converting it into actionable items is far from trivial, due to the vastness of the information available online as well as to its unstructured nature. In this research, we aim at (partially) crowdsourcing the software design, implementation and maintenance process by exploiting the knowledge embedded in various sources available on the Web (e.g., Stack Overflow discussions, presentations on SlideShare, open source code, etc.). For example, we want to support software design decisions (e.g., whether to use a specific library for the implementation of a feature) by performing opinion mining on the vast amount of information available on the Web, and we want to recommend refactoring operations by learning from the code written in open source systems. The final goal is to improve developers' productivity and code quality."
An experience report on defect modelling in practice: pitfalls and challenges,"Tantithamthavorn, Chakkrit and Hassan, Ahmed E.",10.1145/3183519.3183547,2018,"Over the past decade with the rise of the Mining Software Repositories (MSR)field, the modelling of defects for large and long-lived systems has become one of the most common applications of MSR. The findings and approaches of such studies have attracted the attention of many of our industrial collaborators (and other practitioners worldwide). At the core of many of these studies is the development and use of analytical models for defects. In this paper, we discuss common pitfalls and challenges that we observed as practitioners attempt to develop such models or reason about the findings of such studies. The key goal of this paper is to document such pitfalls and challenges so practitioners can avoid them in future efforts. We also hope that other academics will be mindful of such pitfalls and challenges in their own work and industrial engagements."
One Size Does Not Fit All: In-Test Workload Adaptation for Performance Testing of Enterprise Applications,"Ayala-Rivera, Vanessa and Kaczmarski, Maciej and Murphy, John and Darisa, Amarendra and Portillo-Dominguez, A. Omar",10.1145/3184407.3184418,2018,"The identification of workload-dependent performance issues, as well as their root causes, is a time-consuming and complex process which typically requires several iterations of tests (as this type of issues can depend on the input workloads), and heavily relies on human expert knowledge. To improve this process, this paper presents an automated approach to dynamically adapt the workload (used by a performance testing tool) during the test runs. As a result, the performance issues of the tested application can be revealed more quickly; hence, identifying them with less effort and expertise. Our experimental evaluation has assessed the accuracy of the proposed approach and the time savings that it brings to testers. The results have demonstrated the benefits of the approach by achieving a significant decrease in the time invested in performance testing (without compromising the accuracy of the test results), while introducing a low overhead in the testing environment."
A Factoid Question Answering System for Vietnamese,"Le-Hong, Phuong and Bui, Duc-Thien",10.1145/3184558.3191535,2018,"In this paper, we describe the development of an end-to-end factoid question answering system for the Vietnamese language. This system combines both statistical models and ontology-based methods in a chain of processing modules to provide high-quality mappings from natural language text to entities. We present the challenges in the development of such an intelligent user interface for an isolating language like Vietnamese and show that techniques developed for inflectional languages cannot be applied as is. Our question answering system can answer a wide range of general knowledge questions with promising accuracy on a test set."
Recoin: Relative Completeness in Wikidata,"Balaraman, Vevake and Razniewski, Simon and Nutt, Werner",10.1145/3184558.3191641,2018,"The collaborative knowledge base Wikidata is the central storage of Wikimedia projects, containing over 45 million data items. It acts as the hub for interlinking Wikipedia pages about a specific item in different languages, automates features such as infoboxes in Wikipedia, and is increasingly used for other applications such as data enrichment and question answering. Tracking the quality of Wikidata is an important issue for this project. In this paper we focus particularly on the completeness aspect. Several automated techniques have been adopted by Wikis to track and manage completeness, yet these techniques are generally subjective and do not provide a clear quality estimate at the level of entities. In this paper, we present an approach towards measuring Relative Completeness in Wikidata by comparison with data present for similar entities. This relative completeness approach is easily scalable with the introduction of new classes in the knowledge base, and has been implemented for all available entities in Wikidata. The results provide an intuition on the completeness of an entity comparing it with other similar entities. Here, we present our implementation approach along with a discussion on strategies and open challenges."
Survey of Software Development Effort Estimation Techniques,"Saeed, Ayesha and Butt, Wasi Haider and Kazmi, Farwa and Arif, Madeha",10.1145/3185089.3185140,2018,"Software development effort estimation is one of the most crucial activities in software engineering. Effort estimation permits managers and software engineers to anticipate, forecast, and precisely quote the schedule, budget and manpower requirements. By accurately estimating the effort; software projects can be saved from under run or over run. In this paper we have summarized and then analyzed the past work of software effort estimation in a systematic way. 10 researches were surveyed and explained briefly that how they are contributing towards solving the effort estimation problem in terms of time, cost or test. It also emphasizes that various effort estimation models have different pros and cons and can be used in different context on basis of different types of historical data. The survey discovered the most popular models used for effort prediction are supervised learning algorithms. The trends identified through this survey can help in exploring the potential research areas."
Metric-based evaluation of multiagent systems models,"Damasceno, Lidiane and Werneck, Vera Maria B. and Schots, Marcelo",10.1145/3193954.3193960,2018,"The use of Multiagent Systems (MAS) has been increasing over the years due to their capacity of dealing with problems in a variety of domains. The modeling of such systems is not trivial: besides the knowledge and skills on agent-oriented software engineering and a basic understanding of the target domain to be modeled, it demands familiarity with agent-oriented modeling methodologies. This is not always the case, though, especially for newcomers in the field. This work proposes a set of guidelines in the form of a questionnaire for the evaluation of MAS models, aiming at supporting the verification of their quality. The questionnaire is built upon the results of a systematic mapping conducted to identify how MAS models have been evaluated and what metrics have been used. The proposed guidelines were evaluated through (i) a peer review by experts and (ii) its actual application by graduate students applying modeling methodologies in the context of Guardian Angel (GA), a patient-centered health system that automatically supports patients suffering from chronic diseases. Participants provided an overall positive feedback and proposed some improvements on the questionnaire, most of which were promptly incorporated."
Technical debt as an external software attribute,"Lavazza, Luigi and Morasca, Sandro and Tosi, Davide",10.1145/3194164.3194168,2018,"Background: Technical debt is currently receiving increasing attention from practitioners and researchers. Several metaphors, concepts, and indications concerning technical debt have been introduced, but no agreement exists about a solid definition of technical debt.Objective: We aim at providing a solid basis to the definition of technical debt and the way it should be quantified.Method: We view technical debt as a software quality attribute and therefore we use Measurement Theory, the general reference framework for the quantification of attributes, to define technical debt and its characteristics in a rigorous way.Results: We show that technical debt should be defined as an external software quality attribute. Therefore, it should be quantified via statistical and machine-learning models whose independent variables are internal software quality attributes. Different models may exist, depending on the specific needs and goals of the software product and development environment. Also, technical debt is a multifaceted concept, so different kinds of technical debt exist, related to different quality attributes, such as performance, usability, and maintainability. These different kinds of technical debt should be evaluated individually, so one can better focus on the specific quality issues that need to be addressed.Conclusions: We show that, to provide it with a rigorous basis, technical debt should be considered and measured as an external software attribute. Researchers and practitioners should build models for technical debt and use them to (1) assess the extent of the technical debt and (2) investigate and assess different ways of modifying software to repay technical debt."
Image classification by combining local and mid-level features,"Zhang, Hui and Lu, Yao",10.1145/3194206.3194230,2018,"It is meaningful to study high performance image classification algorithms for massive image management and effective organization. Image feature representations directly affect the performance of classification algorithm, however, in the current popular sparse coding framework for image classification, the relationship between local features is neglected, and there is still considerable room left for further improvement. In this paper, mid-level features are extracted efficiently in the superpixel region, which embeds the structural information of local features. Then we combine the local features and mid-level features to achieve better discriminative ability under the sparse coding framework. Compared to using only local features, the new methods improve the classification performance about 3% on several popular benchmarks."
Designing a next-generation continuous software delivery system: concepts and architecture,"Steffens, Andreas and Lichter, Horst and D\""{o}ring, Jan Simon",10.1145/3194760.3194768,2018,"Continuous Integration and Continuous Delivery are established practices in modern agile software development. The DevOps movement adapted theses practices and places the deployment pipeline at its heart as one of the main requirements to automate the software development process and to deliver and operate software in a more robust way with higher quality.Over the time a lot of systems and tools has been developed to implement the deployment pipeline and to support continuous delivery. But software development is complex, its process even more and due to the individual organization of software vendors no real all-in-one solution for CD exists. Literature identified a lot of challenges when adopting CD and DevOps in an organization.This paper presents a conceptual model and fundamental design decisions for a new generation of software delivery systems tackling some of these issues. Our approach focuses on two specific challenges for adopting CD. The first is the lack of flexibility and maintainability of software delivery systems. The second is the insufficient user support to model and manage delivery processes and pipelines. We introduce an automated mechanism to ease the effort for developers and other stakeholders.Based on these results this paper introduces an architectural proposal for a next-generation continuous software delivery system."
Team composition in software engineering project courses,"Dzvonyar, Dora and Alperowitz, Lukas and Henze, Dominic and Bruegge, Bernd",10.1145/3194779.3194782,2018,"Composing well-balanced, effective development teams for software engineering project courses is important for facilitating learning, fostering student motivation as well as obtaining a successful project outcome. However, team composition is a challenging task for instructors because they have to consider a variety of possibly conflicting criteria such as practical constraints, skill distribution, or project motivation.In this paper, we describe our process for composing development teams based on a pre-defined set of criteria that we have established from our experience conducting project courses since 2008 and constantly refined since. We reflect on these criteria by analyzing the team synergy and project satisfaction of participating students as well as their perspective on challenges in their teams in one concrete instance of a multi-project capstone course. Our findings show that lack of motivation, problems with interpersonal relationships and communication issues affect the less satisfied teams more than the others."
Entity-level sentiment analysis of issue comments,"Ding, Jin and Sun, Hailong and Wang, Xu and Liu, Xudong",10.1145/3194932.3194935,2018,"Emotions and sentiment of software developers can largely influence the software productivity and quality. However, existing work on emotion mining and sentiment analysis is still in the early stage in software engineering in terms of accuracy, the size of datasets used and the specificity of the analysis. In this work, we are concerned with conducting entity-level sentiment analysis. We first build a manually labeled dataset containing 3,000 issue comments selected from 231,732 issue comments collected from 10 open source projects in GitHub. Then we design and develop SentiSW, an entity-level sentiment analysis tool consisting of sentiment classification and entity recognition, which can classify issue comments into &lt;sentiment, entity&gt; tuples. We evaluate the sentiment classification using ten-fold cross validation, and it achieves 68.71% mean precision, 63.98% mean recall and 77.19% accuracy, which is significantly higher than existing tools. We evaluate the entity recognition by manually annotation and it achieves a 75.15% accuracy."
Subjective Association Rule Mining: From Point-based Ranking Sequence to Interval-based Temporal Sequence,"Yang, Pu-Tai and Yang, Kai-Hao and Chen, Ching-Chi and Horng, Shwu-Min",10.1145/3195106.3195174,2018,"Human behavior is complex and ambiguous. This study investigates how human beings consider two significant elements of discrete items: one side is their preference ranking relationships, while the other is their preferred temporal relationships. The development of a means of deriving valuable information from collected user opinions of these two relationships is therefore a core problem in data mining. This study proposes a novel model which simultaneously considers a heterogeneous database of point-based preference ranking relationships and interval-based temporal relationships to discover their association rules, while the interval-based relationships refer to occurrences with beginning and end points. An early-stage survey experiment, conducted to collect authentic users' subjective opinions for discovering the management implications, is also presented in this study."
Designing a capability-centric web tool to support agile team composition and task allocation: a work in progress,"Vishnubhotla, Sai Datta and Mendes, Emilia and Lundberg, Lars",10.1145/3195836.3195855,2018,"A significant number of studies reported models for competence profiling, measuring capabilities of professionals and recommendation systems for roles within agile software development (ASD). These models coordinated in human resource management within ASD. However, in the light of swift, incremental and iterative nature of ASD practices, designing solutions that easily integrate capability measurements with ongoing project management routines, is an important area for investigation. With the support of interviews, grounded theory procedure and workshops, we identified the aspects valued by our industrial collaborator while allocating professionals to tasks. This information was further utilized towards devising a framework for capability-centric Web tool. This tool provides a one-stop solution for project managers to create projects, keep track of capabilities and execute allocation routines."
How do design decisions affect the distribution of software metrics?,"D\'{o}sea, Marcos and Sant'Anna, Cl\'{a}udio and da Silva, Bruno C.",10.1145/3196321.3196337,2018,"Background. Source code analysis techniques usually rely on metric-based assessment. However, most of these techniques have low accuracy. One possible reason is because metric thresholds are extracted from classes driven by distinct design decisions. Previous studies have already shown that classes implemented according to some coarse-grained design decisions, such as programming languages, have different distribution of metric values. Therefore, these design decisions should be taken into account when using benchmarks for metric-based source code analysis. Goal. Our goal is to investigate whether other fine-grained design decisions also influence over distribution of software metrics. Method. We conduct an empirical study to evaluate the distributions of four metrics applied over fifteen real-world systems based on three different domains. Initially, we evaluated the influence of the class design role on the distributions of measures. For this purpose, we have defined an automatic approach to identify the design role played by each class. Then, we looked for other fine-grained design decisions that could have influenced the measures. Results. Our findings show that distribution of metrics are sensitive to the following design decisions: (i) design role of the class (ii) used libraries, (iii) coding style, (iv) exception handling, and (v) logging and debugging code mechanisms. Conclusion. The distribution of software metrics are sensitive to fine-grained design decisions and we should consider taking them into account when building benchmarks for metric-based source code analysis."
Towards just-in-time refactoring recommenders,"Pantiuchina, Jevgenija and Bavota, Gabriele and Tufano, Michele and Poshyvanyk, Denys",10.1145/3196321.3196365,2018,"Empirical studies have provided ample evidence that low code quality is generally associated with lower maintainability. For this reason, tools have been developed to automatically detect design flaws (e.g., code smells). However, these tools are not able to prevent the introduction of design flaws. This means that the code has to experience a quality decay (with a consequent increase of maintenance/evolution costs) before state-of-the-art tools can be applied to identify and refactor the design flaws.Our goal is to develop a new generation of refactoring recommenders aimed at preventing, via refactoring operations, the introduction of design flaws rather than fixing them once they already affect the system. We refer to such a novel perspective on software refactoring as just-in-time refactoring. In this paper, we make a first step towards this direction, presenting an approach able to predict which classes will be affected in the future by code smells."
Anatomy of functionality deletion: an exploratory study on mobile apps,"Nayebi, Maleknaz and Kuznetsov, Konstantin and Chen, Paul and Zeller, Andreas and Ruhe, Guenther",10.1145/3196398.3196410,2018,"One of Lehman's laws of software evolution is that the functionality of programs has to increase over time to maintain user satisfaction. In the domain of mobile apps, though, too much functionality can easily impact usability, resource consumption, and maintenance effort. Hence, does the law of continuous growth apply there? This paper shows that in mobile apps, deletion of functionality is actually common, challenging Lehman's law. We analyzed user driven requests for deletions which were found in 213,866 commits from 1,519 open source Android mobile apps from a total of 14,238 releases. We applied hybrid (open and closed) card sorting and created taxonomies for nature and causes of deletions. We found that functionality deletions are mostly motivated by unneeded functionality, poor user experience, and compatibility issues. We also performed a survey with 106 mobile app developers. We found that 78.3% of developers consider deletion of functionality to be equally or more important than the addition of new functionality. Developers confirmed that they plan for deletions. This implies the need to re-think the process of planning for the next release, overcoming the simplistic assumptions to exclusively look at adding functionality to maximize the value of upcoming releases. Our work is the first to study the phenomenon of functionality deletion and opens the door to a wider perspective on software evolution."
Analyzing requirements and traceability information to improve bug localization,"Rath, Michael and Lo, David and M\""{a}der, Patrick",10.1145/3196398.3196415,2018,"Locating bugs in industry-size software systems is time consuming and challenging. An automated approach for assisting the process of tracing from bug descriptions to relevant source code benefits developers. A large body of previous work aims to address this problem and demonstrates considerable achievements. Most existing approaches focus on the key challenge of improving techniques based on textual similarity to identify relevant files. However, there exists a lexical gap between the natural language used to formulate bug reports and the formal source code and its comments. To bridge this gap, state-of-the-art approaches contain a component for analyzing bug history information to increase retrieval performance. In this paper, we propose a novel approach TraceScore that also utilizes projects' requirements information and explicit dependency trace links to further close the gap in order to relate a new bug report to defective source code files. Our evaluation on more than 13,000 bug reports shows, that TraceScore significantly outperforms two state-of-the-art methods. Further, by integrating TraceScore into an existing bug localization algorithm, we found that TraceScore significantly improves retrieval performance by 49% in terms of mean average precision (MAP)."
CLEVER: combining code metrics with clone detection for just-in-time fault prevention and resolution in large industrial projects,"Nayrolles, Mathieu and Hamou-Lhadj, Abdelwahab",10.1145/3196398.3196438,2018,"Automatic prevention and resolution of faults is an important research topic in the field of software maintenance and evolution. Existing approaches leverage code and process metrics to build metric-based models that can effectively prevent defect insertion in a software project. Metrics, however, may vary from one project to another, hindering the reuse of these models. Moreover, they tend to generate high false positive rates by classifying healthy commits as risky. Finally, they do not provide sufficient insights to developers on how to fix the detected risky commits. In this paper, we propose an approach, called CLEVER (Combining Levels of Bug Prevention and Resolution techniques), which relies on a two-phase process for intercepting risky commits before they reach the central repository. When applied to 12 Ubisoft systems, the results show that CLEVER can detect risky commits with 79% precision and 65% recall, which outperforms the performance of Commit-guru, a recent approach that was proposed in the literature. In addition, CLEVER is able to recommend qualitative fixes to developers on how to fix risky commits in 66.7% of the cases."
Data-driven search-based software engineering,"Nair, Vivek and Agrawal, Amritanshu and Chen, Jianfeng and Fu, Wei and Mathew, George and Menzies, Tim and Minku, Leandro and Wagner, Markus and Yu, Zhe",10.1145/3196398.3196442,2018,"This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulate Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a) which require learning from a large data source or (b) when optimizers need to know the lay of the land to find better solutions, faster.This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource.This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas."
Twenty Years of Creativity Research in Human-Computer Interaction: Current State and Future Directions,"Frich, Jonas and Mose Biskjaer, Michael and Dalsgaard, Peter",10.1145/3196709.3196732,2018,"Creativity has been a growing topic in the ACM community since the 1990s; however, no clear overview of this trend has been offered. We present a thorough survey of 998 creativity-related publications in the ACM Digital Library collected using keyword search to determine prevailing approaches, topics, and characteristics of creativity- oriented Human-Computer Interaction (HCI) research. A selected sample based on yearly citations yielded 221 publications, which were analyzed using constant comparison analysis. We found that HCI is almost exclusively responsible for creativity-oriented publications; they focus on collaborative creativity rather than individual creativity; there is a general lack of definition of the term 'creativity'; empirically based contributions are prevalent; and many publications focus on new tools, often developed by researchers. On this basis, we present three implications for future creativity-oriented HCI research: develop and employ clearer definitions of creativity; go beyond in-vitro studies of novel tools; and move toward interdisciplinary research collaborations."
Making IT work: integrating gender research in computing through a process model,"Draude, Claude and Maa\ss{}, Susanne",10.1145/3196839.3196846,2018,"To become effective in the field of computing, gender research must explain its concepts to non-gender studies scholars and to convince them of their relevance. Our title ""Making IT Work"" claims that both can be done: interlinking gender research and computing, and by that improving IT systems.We present the ""Gender-Extended Research and Development"" (GERD) model, a process model which combines gender and diversity aspects with all phases of computing research and development. The model with seven phases is a generalization of common process models in computing. Gender/diversity expertise has been condensed into a number of aspects that characterize the social context of computing research. They can be related to and reflected during every phase. This reflection is somewhat operationalized by providing a list of questions for each combination of phase and aspect. Case studies as part of the model illustrate either in what way such reflections may help in research and development or what may happen if no such aspects have been included."
An authentic student research experience: fostering research skills and boosting the employability profile of students,"Hatziapostolou, Thanos and Dranidis, Dimitris and Sotiriadou, Anna and Kefalas, Petros and Nikolakopoulos, Ioannis",10.1145/3197091.3197131,2018,"Involving students in research activities has been credited with many benefits, such as increased critical thinking and analysis skills, development of self-confidence and intellectual independence, and consolidating the ability to ``think like a scientist.'' Student research experiences at undergraduate level have been extensively investigated and disseminated. At postgraduate level, however, research is very scant and experience reports on practices used are scarce. This paper discusses an approach to provide an authentic research experience to master's level students in Advanced Software Engineering. We present the learning, teaching, and assessment methods, as well as, important parameters for success, such as techniques for creating a motivational and intellectual research climate and methods for maximizing the quality of results. We also demonstrate how through a research experience, students can not only advance their research skills, but also strengthen their employability profile. The paper includes an evaluation from six cohorts with the results indicating that students consider the experience very valuable. In 2014, our approach has been accredited by the British Computer Society (BCS) as an example of best practice for innovative approaches to assessment and how they enhance the student experience."
Promoting diversity in computing,"Kulkarni, Anagha and Yoon, Ilmi and Pennings, Pleuni S. and Okada, Kazunori and Domingo, Carmen",10.1145/3197091.3197145,2018,"In this paper we present a pilot program at San Francisco State University, Promoting INclusivity in Computing (PINC), that is designed to achieve two goals simultaneously: (i) improving diversity in computing, and (ii) increasing computing literacy in data-intensive fields. To achieve these goals, the PINC program enrolls undergraduate students from non Computer Science (non-CS) fields, such as, Biology, that have become increasingly data-driven, and that traditionally attract diverse student population. PINC incorporates several well-established pedagogical practices, such as, cohort-based program structure, near-peer mentoring, and project-driven learning, to attract, retain, and successfully graduate a highly diverse and interdisciplinary student body. On successful completion of the program, students are awarded a minor in Computing Applications. Since its inception 18 months ago, 60 students have participated in this program. Of these 73% are women, and 51% are underrepresented minorities (URM). 74% of the participating students had nominal or no exposure to computer programming before PINC. Findings from student surveys show that majority of the PINC students now feel less intimidated about computer programming, and vividly see its utility and necessity. For several students, participation in the PINC program has already opened up career pathways (industry and academic summer internships) that were not available to them before."
Self-reported activities of Android developers,"Pascarella, Luca and Geiger, Franz-Xaver and Palomba, Fabio and Di Nucci, Dario and Malavolta, Ivano and Bacchelli, Alberto",10.1145/3197231.3197251,2018,"To gain a deeper empirical understanding of how developers work on Android apps, we investigate self-reported activities of Android developers and to what extent these activities can be classified with machine learning techniques. To this aim, we firstly create a taxonomy of self-reported activities coming from the manual analysis of 5,000 commit messages from 8,280 Android apps. Then, we study the frequency of each category of self-reported activities identified in the taxonomy, and investigate the feasibility of an automated classification approach. Our findings can inform be used by both practitioners and researchers to take informed decisions or support other software engineering activities."
A Nature Inspired metaheuristic for Optimal Leveling of Resources in Project Management,"Tzanetos, Alexandros and Kyriklidis, Christos and Papamichail, Anastasia and Dimoulakis, Apostolis and Dounias, Georgios",10.1145/3200947.3201014,2018,"Resource Leveling is a constrained problem with a large solution space, especially when the projects have numerous tasks. This makes the problem hard to tackle. In this study, a novel algorithm named sonar inspired optimization (SIO) belonging in Nature Inspired Intelligence is applied in both benchmark and artificial resource-leveling problems to investigate its performance. Results are compared to those derived from a Hybrid Genetic Algorithm (HGA) previously developed. Experimental findings show that the proposed algorithm is very promising as in most cases the obtained solutions prove superior or equally good to those of HGA and other competitive approaches. An additional approach of the proposed metaheuristic is that it generates only feasible solutions. The algorithm has been implemented in different programming languages so that in future applications will be user friendly for project managers."
Adopting virtual reality as a medium for software development process education,"Gulec, Ulas and Yilmaz, Murat and Isler, Veysi and O'Connor, Rory V. and Clarke, Paul",10.1145/3202710.3203161,2018,"Software development is a complex process of collaborative endeavour which requires hands-on experience starting from requirement analysis through to software testing and ultimately demands continuous maintenance so as to mitigate risks and uncertainty. Therefore, training experienced software practitioners is a challenging task. To address this gap, we propose an interactive virtual reality training environment for software practitioners to gain virtual experience based on the tasks of software development. The goal is to transport participants to a virtual software development organization where they experience simulated development process problems and conflicting situations, where they will interact virtually with distinctive personalities, roles and characters borrowed from real software development organizations. This PhD in progress paper investigates the literature and proposes a novel approach where participants can acquire important new process knowledge. Our preliminary observations suggest that a complementary VR-based training tool is likely to improve the experience of novice software developers and ultimately it has a great potential for training activities in software development organizations."
A genetic programming based iterated local search for software project scheduling,"Sabar, Nasser R. and Turky, Ayad and Song, Andy",10.1145/3205455.3205557,2018,"Project Scheduling Problem (PSP) plays a crucial role in large-scale software development, directly affecting the productivity of the team and on-time delivery of software projects. PSP concerns with the decision of who does what and when during the software project lifetime. PSP is a combinatorial optimisation problem and inherently NP-hard, indicating that approximation algorithms are highly advisable for real-world instances which are often large in size. In this work, we propose an iterated local search (ILS) algorithm for PSP. ILS is a simple, yet effective for combinatorial optimisation problems. However, its performance highly depends on its perturbation operator which is to guide the search to new starting points. Hereby, we propose a Genetic Programming (GP) approach to evolve perturbation operators based on a range of low-level operators and rules. The evolution process will go along with the iterated search process and supply better operators continuously. The GP based ILS algorithm is tested using a set of well known PSP benchmark instances and compared with state-of-the-art algorithms. The experimental results demonstrated the effectiveness of GP generated perturbation operators as they can outperform existing leading methods."
Acquisition of practical skills in the protected learning space of a scientific community,"Gr\""{o}schel, Michael and Roth-Dietrich, Gabriele",10.1145/3209087.3209090,2018,"Digitalization is constantly forcing companies to refine their products, services and business models. To shape this change, companies expect not only knowledge of the technologies from the graduates of the respective study programs, but also comprehensive methodological and social competences. For this purpose, we describe the concept of a project semester in an Enterprise Computing study programme, which imparts the required skills. The task set by a partner in the industry allows the achievement of different goals and integrates the various dimensions. On this basis, we describe the best practices in the areas of project management, knowledge building, administration and dealing with customers and other stakeholders. The description of actually carried out projects shows the application of our concept and allows the reader to transfer the best practices to his own needs. Finally, we point out the advantages for the project participants and outline expansion potential."
Working across boundaries in smart city research,"Korsgaard, Henrik and Thiel, Sarah-Kristin and Thomas, Vanessa and Erti\""{o}, Titiana",10.1145/3209281.3209315,2018,"In this paper, four early career researchers discuss and reflect upon the unique research space offered by Smart City projects. We do so in an attempt to publicly reconcile some of the tensions and difficulties that we encountered while collaborating across organisational boundaries during three ""Smart City"" projects, which we briefly outline in the paper. We focus our discussion on four types of tensions that we encountered: motivations; accountability; participation, and; qualifying success. We believe that the tensions we encountered in our projects, and that we discuss in this paper, might be experienced similarly by other early career researchers. By sharing our tensions, raising our questions, and proposing some preliminary answers to those questions based on our experiences and reflections, we hope to provoke a discussion amongst our dg.o peers that will lead to improved future collaborations, a supportive community environment and, ultimately, smoother Smart City research projects."
Care and the Practice of Data Science for Social Good,"Zegura, Ellen and DiSalvo, Carl and Meng, Amanda",10.1145/3209811.3209877,2018,"Data science is an interdisciplinary field that extracts insights from data through a multi-stage process of data collection, analysis and use. When data science is applied for social good, a variety of stakeholders are introduced to the process with an intention to inform policies or programs to improve well-being. Our goal in this paper is to propose an orientation to care in the practice of data science for social good. When applied to data science, a logic of care can improve the data science process and reveal outcomes of ""good"" throughout. Consideration of care in practice has its origins in Science and Technology Studies (STS) and has recently been applied by Human Computer Interaction (HCI) researchers to understand technology repair and use in under-served environments as well as care in remote health monitoring. We bring care to the practice of data science through a detailed examination of our engaged research with a community group that uses data as a strategy to advocate for permanently affordable housing. We identify opportunities and experiences of care throughout the stages of the data science process. We bring greater detail to the notion of human-centered systems for data science and begin to describe what these look like."
Demographic Variables in Knowledge Sharing Behavior among IT Engineers in Taiwan,"Tien, Chiang Hsing",10.1145/3209914.3209924,2018,"The purpose of this study was to investigate demographic variables in knowledge sharing. The study conducted a quantitative study, and the participants were IT engineers. Random sampling was used. The questionnaires were sent to 25 IT companies by mails. The cover of the questionnaire explained the purposes of the study. A self-addressed stamped envelope was enclosed. 250 copies of questionnaires were sent and 84 copies were returned. The response rate was 33.6%. The data was analyzed by the independent t-test and One-way ANOVA in IBM SPSS 22. The findings of the study show (1) knowledge sharing behavior differs in terms of education level, (2) knowledge sharing behavior has significant influence in terms of profession, and (3) knowledge sharing behavior does not have significant influence in gender, age group, position grade, job tenure, workplace, marital status, and number of children."
Modeling Dynamic Pairwise Attention for Crime Classification over Legal Articles,"Wang, Pengfei and Yang, Ze and Niu, Shuzi and Zhang, Yongfeng and Zhang, Lei and Niu, ShaoZhang",10.1145/3209978.3210057,2018,"In juridical field, judges usually need to consult several relevant cases to determine the specific articles that the evidence violated, which is a task that is time consuming and needs extensive professional knowledge. In this paper, we focus on how to save the manual efforts and make the conviction process more efficient. Specifically, we treat the evidences as documents, and articles as labels, thus the conviction process can be cast as a multi-label classification problem. However, the challenge in this specific scenario lies in two aspects. One is that the number of articles that evidences violated is dynamic, which we denote as the label dynamic problem. The other is that most articles are violated by only a few of the evidences, which we denote as the label imbalance problem. Previous methods usually learn the multi-label classification model and the label thresholds independently, and may ignore the label imbalance problem. To tackle with both challenges, we propose a unified D ynamic P airwise A ttention M odel (DPAM for short) in this paper. Specifically, DPAM adopts the multi-task learning paradigm to learn the multi-label classifier and the threshold predictor jointly, and thus DPAM can improve the generalization performance by leveraging the information learned in both of the two tasks. In addition, a pairwise attention model based on article definitions is incorporated into the classification model to help alleviate the label imbalance problem. Experimental results on two real-world datasets show that our proposed approach significantly outperforms state-of-the-art multi-label classification methods."
Evolving Project based Learning to Suit Diverse Student Cohorts,"Thevathayan, Charles",10.1145/3210459.3210472,2018,"Software engineering courses are increasingly moving towards a project-based learning (PBL) approach. PBL allows abstract software engineering principles and practices to be learnt experientially making the course more appealing to diverse student cohorts. Employers favor PBL as students get exposed to current industry practices, processes and tools, thus narrowing the industry-academia gap. PBL however, poses a number of challenges. Academic staff need to find efficient ways of managing many additional tasks related to PBL such as getting licenses, developing technical materials, managing projects and handling team issues. Students involved in PBL need to be exposed to teamwork and project management skills concurrently.In this paper we present our experience evolving a hybrid-teaching model by using the action research cycle plan-act-observe-reflect over 3 semesters. The main novelty of our approach is the use of projects with varying levels of scaffolding, which made it possible for students with limited programming background to have an enjoyable and beneficial project experience. To ensure fairness, projects with high levels of scaffolding required implementation of more complex use cases to get higher grades. Student surveys reveal such an approach is effective for diverse undergraduate students."
Bench4BL: reproducibility study on the performance of IR-based bug localization,"Lee, Jaekwon and Kim, Dongsun and Bissyand\'{e}, Tegawend\'{e} F. and Jung, Woosung and Le Traon, Yves",10.1145/3213846.3213856,2018,"In recent years, the use of Information Retrieval (IR) techniques to automate the localization of buggy files, given a bug report, has shown promising results. The abundance of approaches in the literature, however, contrasts with the reality of IR-based bug localization (IRBL) adoption by developers (or even by the research community to complement other research approaches). Presumably, this situation is due to the lack of comprehensive evaluations for state-of-the-art approaches which offer insights into the actual performance of the techniques.  We report on a comprehensive reproduction study of six state-of-the-art IRBL techniques. This study applies not only subjects used in existing studies (old subjects) but also 46 new subjects (61,431 Java files and 9,459 bug reports) to the IRBL techniques. In addition, the study compares two different version matching (between bug reports and source code files) strategies to highlight some observations related to performance deterioration. We also vary test file inclusion to investigate the effectiveness of IRBL techniques on test files, or its noise impact on performance. Finally, we assess potential performance gain if duplicate bug reports are leveraged."
Research Facilitation on a Budget,"Christopher, Jason and Culich, Aaron and Dombrowski, Quinn and McCaffrey, Debra and Neeser, Amy and Wiedlea, Andrew",10.1145/3219104.3219137,2018,"Achieving broad uptake of research computing services is a tremendous challenge when funding for staff positions is constrained. Outreach and ongoing engagement with researchers is both essential and time-consuming, leading to a tension between supporting day-to-day operations and building the kinds of partnerships that ensure ongoing support for the program.Since 2015, Berkeley Research Computing (BRC) has been hiring primarily graduate students into a part-time ""domain consultant"" role (influenced by ACI-REF job descriptions and Campus Champion activities) that addresses the program's staffing needs in an affordable way, while providing those graduate students with the technical training and professional work experience required for professional research facilitator positions. The domain consultant program has evolved from an hourly student position into a codified set of practices informed by IT service management, addressing needs including: in-person consulting, tier 2 triage of HPC troubleshooting tickets, support for cloud computing and compute in virtualized analytics environments, and user training. In addition, consulting engagements are reviewed and discussed regularly, both in team meetings and in one-on-one meetings with the service manager, to provide opportunities for consultants to hone their skills.This paper will also highlight the value of programs such as BRC's for addressing gaps in graduate education practices that can hinder PhD recipients' success when applying for research facilitator positions. It will also illustrate the value of thinking broadly about partnerships when developing a consulting program, by describing the program's recent expansion into research data management, and at the Lawrence Berkeley Lab."
Can Who-Edits-What Predict Edit Survival?,"Yardim, Ali Batuhan and Kristof, Victor and Maystre, Lucas and Grossglauser, Matthias",10.1145/3219819.3219979,2018,"As the number of contributors to online peer-production systems grows, it becomes increasingly important to predict whether the edits that users make will eventually be beneficial to the project. Existing solutions either rely on a user reputation system or consist of a highly specialized predictor that is tailored to a specific peer-production system. In this work, we explore a different point in the solution space that goes beyond user reputation but does not involve any content-based feature of the edits. We view each edit as a game between the editor and the component of the project. We posit that the probability that an edit is accepted is a function of the editor's skill, of the difficulty of editing the component and of a user-component interaction term. Our model is broadly applicable, as it only requires observing data about who makes an edit, what the edit affects and whether the edit survives or not. We apply our model on Wikipedia and the Linux kernel, two examples of large-scale peer-production systems, and we seek to understand whether it can effectively predict edit survival: in both cases, we provide a positive answer. Our approach significantly outperforms those based solely on user reputation and bridges the gap with specialized predictors that use content-based features. It is simple to implement, computationally inexpensive, and in addition it enables us to discover interesting structure in the data."
Variable Selection and Task Grouping for Multi-Task Learning,"Jeong, Jun-Yong and Jun, Chi-Hyuck",10.1145/3219819.3219992,2018,"We consider multi-task learning, which simultaneously learns related prediction tasks, to improve generalization performance. We factorize a coefficient matrix as the product of two matrices based on a low-rank assumption. These matrices have sparsities to simultaneously perform variable selection and learn and overlapping group structure among the tasks. The resulting bi-convex objective function is minimized by alternating optimization, where sub-problems are solved using alternating direction method of multipliers and accelerated proximal gradient descent. Moreover, we provide the performance bound of the proposed method. The effectiveness of the proposed method is validated for both synthetic and real-world datasets."
"Understanding Indoor Scene: Spatial Layout Estimation, Scene Classification, and Object Detection","Ismail, A. S. and Seifelnasr, M. M. and Guo, Hongxing",10.1145/3220162.3220182,2018,"In this paper, we seek to understand scene from different viewpoints such as estimating the spatial layout of indoor scenes, detecting objects in the scene and making scene classification. In the previous work, every step has been done in a separate process so in this work we combine the three steps in one algorithm. To understand the indoor scene this requires interpreting the estimation of multiple scene elements and studying the relation between them. We propose a method that takes a single indoor image and then the relation between scene elements can be automatically learned and applied to totally understand the indoor scenes. Our proposed method consists of two steps: the first step is estimating the spatial layout of indoor scenes and this is done by extracting line segments and estimating the three orthogonal vanishing points. The orientation of the surfaces in the scene is defined by the vanishing points and so constructs the layout of the walls, ceiling, and floor. The second step is studying the relation between scene elements and this means that studying the relation between scene classification, objects in the scene, and layout estimation. We conduct extensive experiments to evaluate our proposed method performance on a single indoor image. We achieve superior accuracy results on two public datasets and provide a robust estimation of scene type, estimation of spatial layout and 3D objects."
A large-scale analysis of YouTube videos depicting everyday thermal camera use,"Mauriello, Matthew Louis and McNally, Brenna and Buntain, Cody and Bagalkotkar, Sapna and Kushnir, Samuel and Froehlich, Jon E.",10.1145/3229434.3229443,2018,"The emergence of low-cost thermographic cameras for mobile devices provides users with new practical and creative prospects. While recent work has investigated how novices use thermal cameras for energy auditing tasks in structured activities, open questions remain about ""in the wild"" use and the challenges or opportunities therein. To study these issues, we analyzed 1,000 YouTube videos depicting everyday uses of thermal cameras by non-professional, novice users. We coded the videos by content area, identified whether common misconceptions regarding thermography were present, and analyzed questions within the comment threads. To complement this analysis, we conducted an online survey of the YouTube content creators to better understand user behaviors and motivations. Our findings characterize common thermographic use cases, extend discussions surrounding the challenges novices encounter, and have implications for the design of future thermographic systems and tools."
"""I told you this last time, right?"": Re-visiting narratives of STEM education","Dziallas, Sebastian and Fincher, Sally",10.1145/3230977.3230989,2018,"The stories we tell ourselves and others - both as individuals and as a community - reflect how we make sense of our lives. Our work using narrative methods has explored how university graduates make sense of their learning experiences and how these fit within their wider learning trajectories. In this paper, we discuss work we conducted with a group of a dozen students who, when first interviewed, were in the second half of their undergraduate education at Olin College of Engineering. All twelve participants were re-interviewed four years later, after they had graduated, using the same narrative protocol that asked them to describe their learning 'life' as if it was a book, and to identify and describe individual chapters of their experience. The pairs of interviews were analysed with respect to their form and their content. In regard to form, a classification of these repeated stories is derived. Thematic analysis of the content examines a) how students come to study and practice computing and b) the continuing, and changing influence of a university education over time, as students construct an individual sense of coherence."
Software product line extraction from variability-rich systems: the robocode case study,"Martinez, Jabier and T\""{e}rnava, Xhevahire and Ziadi, Tewfik",10.1145/3233027.3233038,2018,"The engineering of a Software Product Line (SPL), either by creating it from scratch or through the re-engineering of existing variants, it uses to be a project that spans several years with a high investment. It is often hard to analyse and quantify this investment, especially in the context of extractive SPL adoption when the related software variants are independently created by different developers following different system architectures and implementation conventions. This paper reports an experience on the creation of an SPL by re-engineering system variants implemented around an educational game called Robocode. The objective of this game is to program a bot (a battle tank) that battles against the bots of other developers. The world-wide Robocode community creates and maintains a large base of knowledge and implementations that are mainly organized in terms of features, although not presented as an SPL. Therefore, a group of master students analysed this variability-rich domain and extracted a Robocode SPL. We present the results of such extraction augmented with an analysis and a quantification regarding the spent time and effort. We believe that the results and the a-posteriori analysis can provide insights on global challenges on SPL adoption. We also provide all the elements to SPL educators to reproduce the teaching activity, and we make available this SPL to be used for any research purpose."
Data Acquisition Model for Analyzing Schedule Delays Using KDD: Knowledge Discovery and Datamining,"Shikhli, Mohammed Siraj Eddin and Hammad, Ahmed Mohammed",10.1145/3233347.3233366,2018,"Project schedule delay is a major problem in many projects. It has major consequences and effects on project cost, customer satisfaction level, market share and company reputation. Schedule delay factors can be expressed as direct or indirect causes of schedule delay. Developing an acquisition frame to study schedule delay is a need for any analytic process and there was no such generic model developed to analyze many projects at once regardless of their uniqueness. (Data Acquisition Model) DAM is the way which includes schedule delay factors selected from academic and practical field experience. DAM grouped schedule delay factors according to project life cycle level. EPC lifecycle is known as (Engineering, Procurement and Construction) phases of project life cycle. Some factors were not related to EPC, so adding a criteria using (general) was informative to group non-EPC factors. Shortlisting schedule delay factors in each group was achieved through another criteria (selection criteria). As a conclusion based on the literature review and our practical experience the model might be novel to address schedule delay problem in projects. Data mining techniques have been widely utilized in many industries to extract hidden descriptive or implicit knowledge. A framework was developed based on two main models (hybrid KDD Model) and (Fayyad Model) to discover knowledge in data. DAM might be starting point for (DM) techniques; to generate knowledge about schedule delay factors that will help many decision makers to take corrective action items and control schedule delays in projects."
Prediction of Clinical Outcomes of Spinal Muscular Atrophy Using Motion Tracking Data and Elastic Net Regression,"Chen, David and Rust, Steve and Lin, En-Ju D. and Lin, Simon and Nelson, Leslie and Alfano, Lindsay and Lowes, Linda P.",10.1145/3233547.3233572,2018,"Spinal muscular atrophy (SMA) is a common muscle disease that can lead to high rate of infant mortality. It is important to be able to quickly and accurately diagnose SMAs as well as track disease progression throughout the treatment process. This study introduced a framework for deriving movement features from motion tracking data, and applied a regularized regression method to predict the gold standard clinical measures for SMA, the CHOP INTEND Extremities Scores (CIES). Our results showed the CIES could be predicted with good accuracy using derived motion features and Elastic Net regression. An RMSE of 8.5 points on CIES was achieved in both cross-validation and prediction on the held-out set. A high ROC-AUC of 0.91 was achieved for discriminating SMA infants from Controls on both session and subject levels. It was concluded that motion tracking devices could potentially be used as a low-cost yet effective method to assess and monitor infants with SMA."
Neuroinformatics and Analysis of Connectomic Alterations Due to Cerebral Microhemorrhages in Geriatric Mild Neurotrauma: Microhemorrhages in Geriatric Neurotrauma,"Maher, Alexander S. and Rostowsky, Kenneth A. and Chouwdhury, Nahian F. and Irimia, Andrei",10.1145/3233547.3233598,2018,"Connectomics alterations associated with subtle forms of cerebrovascular neuropathology-such as cerebral microbleeds (CMBs)-can result in substantial neurological and/or cognitive deficits in victims of traumatic brain injury (TBI). Quantifying CMB-related connectome changes in mild TBI (mTBI) patients requires ingenious neuroinformatics to integrate structural magnetic resonance imaging (sMRI) with diffusion-weighted imaging (DWI) for patient-tailored profiling while preserving the data scientist's ability to implement population studies. Such solutions, however, can assist the refinement of rehabilitation protocols and streamline large-scale analysis while accommodating the heterogeneity of mTBI. This study describes a pipeline for the multimodal integration of sMRI/DWI/DTI to quantify white matter (WM) neural network circuitry alterations associated with mTBI-related CMBs. The approach incorporates WM streamline matching, topology-compliant streamline prototyping and along-tract analysis within a unified framework. When applied to the analysis of neuroimaging data acquired from both mTBI and healthy control volunteers, the approach facilitates the identification of patient-specific CMB-related connectomic changes while incorporating the ability to perform group analyses. This pipeline for the identification and profiling of connectopathies can assist the adaptation of clinical rehabilitation protocols to patients' individual needs."
Towards quantifying the development value of code contributions,"Ren, Jinglei and Yin, Hezheng and Hu, Qingda and Fox, Armando and Koszek, Wojciech",10.1145/3236024.3264842,2018,Quantifying the value of developers’ code contributions to a software project requires more than simply counting lines of code or commits. We define the development value of code as a combination of its structural value (the effect of code reuse) and its non-structural value (the impact on development). We propose techniques to automatically calculate both components of development value and combine them using Learning to Rank. Our preliminary empirical study shows that our analysis yields richer results than those obtained by human assessment or simple counting methods and demonstrates the potential of our approach.
Mining file histories: should we consider branches?,"Kovalenko, Vladimir and Palomba, Fabio and Bacchelli, Alberto",10.1145/3238147.3238169,2018,"Modern distributed version control systems, such as Git, offer support for branching — the possibility to develop parts of software outside the master trunk. Consideration of the repository structure in Mining Software Repository (MSR) studies requires a thorough approach to mining, but there is no well-documented, widespread methodology regarding the handling of merge commits and branches. Moreover, there is still a lack of knowledge of the extent to which considering branches during MSR studies impacts the results of the studies. In this study, we set out to evaluate the importance of proper handling of branches when calculating file modification histories. We analyze over 1,400 Git repositories of four open source ecosystems and compute modification histories for over two million files, using two different algorithms. One algorithm only follows the first parent of each commit when traversing the repository, the other returns the full modification history of a file across all branches. We show that the two algorithms consistently deliver different results, but the scale of the difference varies across projects and ecosystems. Further, we evaluate the importance of accurate mining of file histories by comparing the performance of common techniques that rely on file modification history — reviewer recommendation, change recommendation, and defect prediction — for two algorithms of file history retrieval. We find that considering full file histories leads to an increase in the techniques’ performance that is rather modest."
Noise and heterogeneity in historical build data: an empirical study of Travis CI,"Gallaba, Keheliya and Macho, Christian and Pinzger, Martin and McIntosh, Shane",10.1145/3238147.3238171,2018,"Automated builds, which may pass or fail, provide feedback to a development team about changes to the codebase. A passing build indicates that the change compiles cleanly and tests (continue to) pass. A failing (a.k.a., broken) build indicates that there are issues that require attention. Without a closer analysis of the nature of build outcome data, practitioners and researchers are likely to make two critical assumptions: (1) build results are not noisy; however, passing builds may contain failing or skipped jobs that are actively or passively ignored; and (2) builds are equal; however, builds vary in terms of the number of jobs and configurations.  To investigate the degree to which these assumptions about build breakage hold, we perform an empirical study of 3.7 million build jobs spanning 1,276 open source projects. We find that: (1) 12% of passing builds have an actively ignored failure; (2) 9% of builds have a misleading or incorrect outcome on average; and (3) at least 44% of the broken builds contain passing jobs, i.e., the breakage is local to a subset of build variants. Like other software archives, build data is noisy and complex. Analysis of build data requires nuance."
Scalable incremental building with dynamic task dependencies,"Konat, Gabri\""{e}l and Erdweg, Sebastian and Visser, Eelco",10.1145/3238147.3238196,2018,"Incremental build systems are essential for fast, reproducible software builds. Incremental build systems enable short feedback cycles when they capture dependencies precisely and selectively execute build tasks efficiently. A much overlooked feature of build systems is the expressiveness of the scripting language, which directly influences the maintainability of build scripts. In this paper, we present a new incremental build algorithm that allows build engineers to use a full-fledged programming language with explicit task invocation, value and file inspection facilities, and conditional and iterative language constructs. In contrast to prior work on incrementality for such programmable builds, our algorithm scales with the number of tasks affected by a change and is independent of the size of the software project being built. Specifically, our algorithm accepts a set of changed files, transitively detects and re-executes affected build tasks, but also accounts for new task dependencies discovered during building. We have evaluated the performance of our algorithm in a real-world case study and confirm its scalability."
Reducing interactive refactoring effort via clustering-based multi-objective search,"Alizadeh, Vahid and Kessentini, Marouane",10.1145/3238147.3238217,2018,"Refactoring is nowadays widely adopted in the industry because bad design decisions can be very costly and extremely risky. On the one hand, automated refactoring does not always lead to the desired design. On the other hand, manual refactoring is error-prone, time-consuming and not practical for radical changes. Thus, recent research trends in the field focused on integrating developers feedback into automated refactoring recommendations because developers understand the problem domain intuitively and may have a clear target design in mind. However, this interactive process can be repetitive, expensive, and tedious since developers must evaluate recommended refactorings, and adapt them to the targeted design especially in large systems where the number of possible strategies can grow exponentially.  In this paper, we propose an interactive approach combining the use of multi-objective and unsupervised learning to reduce the developer's interaction effort when refactoring systems. We generate, first, using multi-objective search different possible refactoring strategies by finding a trade-off between several conflicting quality attributes. Then, an unsupervised learning algorithm clusters the different trade-off solutions, called the Pareto front, to guide the developers in selecting their region of interests and reduce the number of refactoring options to explore. The feedback from the developer, both at the cluster and solution levels, are used to automatically generate constraints to reduce the search space in the next iterations and focus on the region of developer preferences. We selected 14 active developers to manually evaluate the effectiveness our tool on 5 open source projects and one industrial system. The results show that the participants found their desired refactorings faster and more accurate than the current state of the art."
Experiences applying automated architecture analysis tool suites,"Mo, Ran and Snipes, Will and Cai, Yuanfang and Ramaswamy, Srini and Kazman, Rick and Naedele, Martin",10.1145/3238147.3240467,2018,"In this paper, we report our experiences of applying three complementary automated software architecture analysis techniques, supported by a tool suite, called DV8, to 8 industrial projects within a large company. DV8 includes two state-of-the-art architecture-level maintainability metrics—Decoupling Level and Propagation Cost, an architecture flaw detection tool, and an architecture root detection tool. We collected development process data from the project teams as input to these tools, reported the results back to the practitioners, and followed up with telephone conferences and interviews. Our experiences revealed that the metrics scores, quantitative debt analysis, and architecture flaw visualization can effectively bridge the gap between management and development, help them decide if, when, and where to refactor. In particular, the metrics scores, compared against industrial benchmarks, faithfully reflected the practitioners’ intuitions about the maintainability of their projects, and enabled them to better understand the maintainability relative to other projects internal to their company, and to other industrial products. The automatically detected architecture flaws and roots enabled the practitioners to precisely pinpoint, visualize, and quantify the “hotspots"" within the systems that are responsible for high maintenance costs. Except for the two smallest projects for which both architecture metrics indicated high maintainability, all other projects are planning or have already begun refactorings to address the problems detected by our analyses. We are working on further automating the tool chain, and transforming the analysis suite into deployable services accessible by all projects within the company."
Are 20% of files responsible for 80% of defects?,"Walkinshaw, Neil and Minku, Leandro",10.1145/3239235.3239244,2018,"Background: Over the past two decades a mixture of anecdote from the industry and empirical studies from academia have suggested that the 80:20 rule (otherwise known as the Pareto Principle) applies to the relationship between source code files and the number of defects in the system: a small minority of files (roughly 20%) are responsible for a majority of defects (roughly 80%).Aims: This paper aims to establish how widespread the phenomenon is by analysing 100 systems (previous studies have focussed on between one and three systems), with the goal of whether and under what circumstances this relationship does hold, and whether the key files can be readily identified from basic metrics.Method: We devised a search criterion to identify defect fixes from commit messages and used this to analyse 100 active Github repositories, spanning a variety of languages and domains. We then studied the relationship between files, basic metrics (churn and LOC), and defect fixes.Results: We found that the Pareto principle does hold, but only if defects that incur fixes to multiple files count as multiple defects. When we investigated multi-file fixes, we found that key files (belonging to the top 20%) are commonly fixed alongside other much less frequently-fixed files. We found LOC to be poorly correlated with defect proneness, Code Churn was a more reliable indicator, but only for extremely high values of Churn.Conclusions: It is difficult to reliably identify the ""most fixed"" 20% of files from basic metrics. However, even if they could be reliably predicted, focussing on them would probably be misguided. Although fixes will naturally involve files that are often involved in other fixes too, they also tend to include other less frequently-fixed files."
"Is there a ""golden"" feature set for static warning identification? an experimental evaluation","Wang, Junjie and Wang, Song and Wang, Qing",10.1145/3239235.3239523,2018,"Background: The most important challenge regarding the use of static analysis tools (e.g., FindBugs) is that there are a large number of warnings that are not acted on by developers. Many features have been proposed to build classification models for the automatic identification of actionable warnings. Through analyzing these features and related studies, we observe several limitations that make the users lack practical guides to apply these features.Aims: This work aims at conducting a systematic experimental evaluation of all the public available features, and exploring whether there is a golden feature set for actionable warning identification.Method: We first conduct a systematic literature review to collect all public available features for warning identification. We employ 12 projects with totally 60 revisions as our subject projects. We then implement a tool to extract the values of all features for each project revision to prepare the experimental data.Results: Experimental evaluation on 116 collected features demonstrates that there is a common set of features (23 features) which take effect in warning identification for most project revisions. These features can achieve satisfied performance with far less time cost for warning identification.Conclusions: These commonly-selected features can be treated as the golden feature set for identifying actionable warnings. This finding can serve as a practical guideline for facilitating real-world warning identification."
Needs and challenges for a platform to support large-scale requirements engineering: a multiple-case study,"Fucci, Davide and Palomares, Cristina and Franch, Xavier and Costal, Dolors and Raatikainen, Mikko and Stettinger, Martin and Kurtanovic, Zijad and Kojo, Tero and Koenig, Lars and Falkner, Andreas and Schenner, Gottfried and Brasca, Fabrizio and M\""{a}nnist\""{o}, Tomi and Felfernig, Alexander and Maalej, Walid",10.1145/3239235.3240498,2018,"Background: Requirement engineering is often considered a critical activity in system development projects. The increasing complexity of software as well as number and heterogeneity of stakeholders motivate the development of methods and tools for improving large-scale requirement engineering. Aims: The empirical study presented in this paper aim to identify and understand the characteristics and challenges of a platform, as desired by experts, to support requirement engineering for individual stakeholders, based on the current pain-points of their organizations when dealing with a large number requirements. Method: We conducted a multiple case study with three companies in different domains. We collected data through ten semi-structured interviews with experts from these companies. Results: The main pain-point for stakeholders is handling the vast amount of data from different sources. The foreseen platform should leverage such data to manage changes in requirements according to customers' and users' preferences. It should also offer stakeholders an estimation of how long a requirements engineering task will take to complete, along with an easier requirements dependency identification and requirements reuse strategy. Conclusions: The findings provide empirical evidence about how practitioners wish to improve their requirement engineering processes and tools. The insights are a starting point for in-depth investigations into the problems and solutions presented. Practitioners can use the results to improve existing or design new practices and tools."
Software analytics in continuous delivery: a case study on success factors,"Huijgens, Hennie and Spadini, Davide and Stevens, Dick and Visser, Niels and van Deursen, Arie",10.1145/3239235.3240505,2018,"Background: During the period of one year, ING developed an approach for software analytics within an environment of a large number of software engineering teams working in a Continuous Delivery as a Service setting. Goal: Our objective is to examine what factors helped and hindered the implementation of software analytics in such an environment, in order to improve future software analytics activities. Method: We analyzed artifacts delivered by the software analytics project, and performed semi-structured interviews with 15 stakeholders. Results: We identified 16 factors that helped the implementation of software analytics, and 20 factors that hindered the project. Conclusions: Upfront defining and communicating the aims, standardization of data at an early stage, build efficient visualizations, and an empirical approach help companies to improve software analytics projects."
Design of Connected Objects: For a Mediation by Design,"Arruabarrena, B\'{e}a",10.1145/3240117.3240132,2018,"Face \`{a} l'augmentation de la taille, la complexit\'{e}, h\'{e}t\'{e}rog\'{e}n\'{e}it\'{e} et le changement technologique des solutions pour l'internet des objets(IdO), il convient de les d\'{e}crire \`{a} un plus haut niveau d'abstraction (au-del\`{a} du code) avant de les impl\'{e}menter concr\`{e}tement. Toutefois, l'utilisation de ces descriptions de haut niveau dans les processus de conception et d'impl\'{e}mentation des syst\`{e}mes reste tr\`{e}s rudimentaire, ne permettant pas de v\'{e}ritablement guider et faciliter le d\'{e}veloppement logiciel de solutions IdO. Cet article propose une nouvelle approche pour rendre plus simple et plus fluide le d\'{e}veloppement de telles solutions. Notre approche de d\'{e}veloppement de solutions IDO est baptis\'{e}e ROCHDEV. Cette approche repose sur l'utilisation de LIDO, notre langage d\'{e}di\'{e} \`{a} la mod\'{e}lisation d'une solution IdO et sur un couplage fort entre une couche de conception et une couche d'impl\'{e}mentation. Elle consiste tout d'abord \`{a} d\'{e}crire en les mod\'{e}lisant, \`{a} un haut niveau d'abstraction, diff\'{e}rents aspects d'une solution \`{a} l'aide de diff\'{e}rentes vues. Chacune des vues est ensuite \'{e}crite selon une notation textuelle. Enfin, une impl\'{e}mentation en Java de la solution est g\'{e}n\'{e}r\'{e}e."
From Volcano to Toyshop: Adaptive Discriminative Region Discovery for Scene Recognition,"Zhao, Zhengyu and Larson, Martha",10.1145/3240508.3240698,2018,"As deep learning approaches to scene recognition emerge, they have continued to leverage discriminative regions at multiple scales, building on practices established by conventional image classification research. However, approaches remain largely generic, and do not carefully consider the special properties of scenes. In this paper, inspired by the intuitive differences between scenes and objects, we propose Adi-Red, an adaptive approach to discriminative region discovery for scene recognition. Adi-Red uses a CNN classifier, which was pre-trained using only image-level scene labels, to discover discriminative image regions directly. These regions are then used as a source of features to perform scene recognition. The use of the CNN classifier makes it possible to adapt the number of discriminative regions per image using a simple, yet elegant, threshold, at relatively low computational cost. Experimental results on the scene recognition benchmark dataset SUN397 demonstrate the ability of Adi-Red to outperform the state of the art. Additional experimental analysis on the Places dataset reveals the advantages of Adi-Red, and highlight how they are specific to scenes. We attribute the effectiveness of Adi-Red to the ability of adaptive region discovery to avoid introducing noise, while also not missing out on important information."
Automatic processing of Electronic Medical Records using Deep Learning,"Osmani, Venet and Li, Li and Danieletto, Matteo and Glicksberg, Benjamin and Dudley, Joel and Mayora, Oscar",10.1145/3240925.3240961,2018,"Background. Availability of large amount of clinical data is opening up new research avenues in a number of fields. An exciting field in this respect is healthcare, where secondary use of healthcare data is beginning to revolutionize healthcare. Except for availability of Big Data, both medical data from healthcare institutions (such as EMR data) and data generated from health and wellbeing devices (such as personal trackers), a significant contribution to this trend is also being made by recent advances on machine learning, specifically deep learning algorithms.Objectives. The objective of this work was to provide an overview of how automatic processing of Electronic Medical Records (EMR) data using Deep Learning techniques is contributing to understating of evolution of chronic diseases and prediction of risk of developing these diseases and associated complications.Methods. A review of the scientific literature was conducted using scientific databases Google Scholar, PubMed, IEEE, and ACM. Searches were focused on publications containing terms related to both Electronic Medical Records and Deep Learning and their synonyms.Results. The review has shown that a number of studies have reported results that provide unprecedented insights into chronic diseases through the use of deep learning methods to analyze EMR data. However, a major roadblock that may limit how effectively these paradigms can be utilized and adopted into clinical practice is in the interpretability of these models by medical professionals for whom many of them are designed.Conclusions. Despite the identified challenges automatic processing of EMR data with state-of-the-art machine learning approaches, such as deep learning, will push predictive power well beyond the current success rates. Hopefully, we will continue to see findings from these works to continue to transform clinical practices, leading to more cost effective and efficient hospital systems along with better patient outcomes and satisfaction."
Aggregation of security metrics for decision making: a reference architecture,"Ahmed, Yussuf and Naqvi, Syed and Josephs, Mark",10.1145/3241403.3241458,2018,Existing security technologies play a significant role in protecting enterprise systems but they are no longer enough on their own given the number of successful cyberattacks against businesses and the sophistication of the tactics used by attackers to bypass the security defences. Security measurement is different to security monitoring in the sense that it provides a means to quantify the security of the systems while security monitoring helps in identifying abnormal events and does not measure the actual state of an infrastructure's security. The goal of enterprise security metrics is to enable understanding of the overall security using measurements to guide decision making. In this paper we present a reference architecture for aggregating the measurement values from the different components of the system in order to enable stakeholders to see the overall security state of their enterprise systems and to assist with decision making. This will provide a newer dimension to security management by shifting from security monitoring to security measurement.
IoT-enabled Capstone,"Teng, Chia-Chi and Lunt, Barry",10.1145/3241815.3241856,2018,"This paper presents the result of a multi-year effort to incorporate Internet of Things (IoT) into projects of the Information Technology senior capstone class at Brigham Young University. Over the past eight years, the instructors and department faculty sought out research collaborators and industry partners to create and design capstone projects that include IoT technology. Roughly one-third of the 58 projects since year 2010 can be considered IoT-enabled. A detailed analysis of these projects could give insights to simple and meaning ways for students to gain experience and skills in this emerging field."
Experiential Learning in an Online IT Program: A Case Study of Third-Party Capstone Project Sourcing,"Faggiani, Kathy S. and Dafnis, Bill and Kwan, Matthew",10.1145/3241815.3241859,2018,"The value of experiential learning in STEM education is well documented and affirmed by accrediting bodies and computing learning societies. Similarly, industry-academic partnership models that provide real-world experiences have been successfully implemented by many institutions of higher learning. Today, unique challenges exist for blending real-world, industry-based learning experiences in computing curriculum for online learners who may be underserved, lack access to transportation, are geographically dispersed or located in a rural area with limited access to industry sites. This paper presents a pilot study to address a key limitation involved in integrating experiential learning in online programs."
The Pedagogical Potential of Game Jams,"Fowler, Allan and Ni, Xuelei (Sherry) and Preston, Jon",10.1145/3241815.3241862,2018,"In this paper the authors explore the implications and pedagogical potential of game jams. While game jams have been noted for enhancing teamwork, communication, and project management skill-sets, in this paper the authors explore the potential educational benefits of Game Jams. Through an analysis of the academic results of the participants of a game jam site as compared to the control group of non-jam participants, the authors conclude that participation in game jams appear to have a positive impact on students' academic performance in many computing classes. The analysis includes a detailed comparison of over 70,000 student performance measures in general computing courses, focused/higher-level computer science courses, game design and development courses, software engineering courses, and mathematics courses. The results from this study may be used by other academic organizations to provide support for hosting game jam activities."
Rejuvenation of the IT Program at King Saud University: A Change Reflecting Local and Global IT Trends,"Al-Baity, Heyam and Alsaeed, Duaa H. and Bayoumi, Sahar and Al-Twairesh, Nora and Al-Khalifa, Hend",10.1145/3241815.3241866,2018,"The Bachelor's program in Information Technology (B.Sc. IT) at the King Saud University (KSU) conducted a significant upgrade of its curriculum in 2018, aiming to better prepare its graduates for the new trends in the fast-evolving IT industry. The upgraded curriculum includes the introduction of new specialized tracks, namely Data Science, Cyber Security, and Network and Internet of Things (IoT). In this paper, we report our experience of designing the new curriculum over the past couple of years. We first provide a historical overview of the IT degree at KSU. We then describe the program design and its rationale. We then conclude with an assessment of the curriculum outcomes to meet ACM/ABET requirements."
Using code quality features to predict bugs in procedural software systems,"Ara\'{u}jo, Cristiano Werner and Zapalowski, Vanius and Nunes, Ingrid",10.1145/3266237.3266271,2018,"A wide range of metrics have been used as features to build bug (or fault) predictors. However, most of the existing predictors focus mostly on object-oriented (OO) systems, either because they rely on OO metrics or were evaluated mainly with OO systems. Procedural software systems (PSS), less addressed in bug prediction research, often suffer from maintainability problems because they typically consist of low-level applications, using for example preprocessors to cope with variability. Previous work evaluated sets of features (composed of static code metrics) proposed in existing approaches in the PSS context. However, explored metrics are limited to those that are part of traditional metric suites, being often associated with structural code properties. A type of information explored to a smaller extent in this context is the output of code quality tools that statically analyse source code, providing hints of code problems. In this paper, we investigate the use of information collected from quality tools to build bug predictors dedicated to PSS. We specify four features derived from code quality tools or associated with poor programming practices and evaluate the effectiveness of these features. Our evaluation shows that our proposed features improve bug predictors in our investigated context."
Personalized Recommendation System for Advanced Learning Management Systems,"Syed, Thoufeeq Ahmed and Nair, Smitha Sunil Kumaran",10.1145/3268891.3268899,2018,"The information on the web is ever increasing and it is becoming difficult for students to find appropriate information or relevant learning material to satisfy their needs. Machine Learning (ML) and Data Mining (DM) have emerged in a variety of application areas including in Learning Management Systems (LMS). In the learning field, the main focus is on the learning style and learning behavior of the learners. Identifying learning style and learning behavior helps in the development of learning management systems. Effective Personalized Learning Recommendation Systems will not only reduce this burden of information overload by recommending the relevant learning material of their interest to the students, but also provide them with the ""right"" information at the ""right"" time and in the ""right"" way. Educational Data Mining is an emerging interdisciplinary research area of DM that deals with the development of methods to explore data originating in an educational context. In this paper, we present a novel technique for finding the relevant references, i.e., Most Recently Referred (MRR) and All Time Referred (ATR) titles by students in LMS. The MRR references are obtained using a personalized dynamic sliding window, which is able to adapt its size according to the ratio of references/titles mentioned by students' in the previous semester. The ATR references are obtained by selecting references that represent the interest of a larger number of students in a particular reference over the year(s). This novel approach has helped in incrementally updating the association rules mined from the log files of an LMS database. The experiments and the evaluation of the proposed methods show that the MRR and ATR referred titles are in sync in numbers and, hence, we can explicitly recommend the learning material references by using either of the proposed techniques."
Towards a body of knowledge for model-based software engineering,"Ciccozzi, Federico and Famelis, Michalis and Kappel, Gerti and Lambers, Leen and Mosser, Sebastien and Paige, Richard F. and Pierantonio, Alfonso and Rensink, Arend and Salay, Rick and Taentzer, Gabi and Vallecillo, Antonio and Wimmer, Manuel",10.1145/3270112.3270121,2018,"Model-based Software Engineering (MBSE) is now accepted as a Software Engineering (SE) discipline and is being taught as part of more general SE curricula. However, an agreed core of concepts, mechanisms and practices --- which constitutes the Body of Knowledge of a discipline --- has not been captured anywhere, and is only partially covered by the SE Body of Knowledge (SWEBOK). With the goals of characterizing the contents of the MBSE discipline, promoting a consistent view of it worldwide, clarifying its scope with regard to other SE disciplines, and defining a foundation for a curriculum development on MBSE, this paper provides a proposal for an extension of the contents of SWEBOK with the set of fundamental concepts, terms and mechanisms that should constitute the MBSE Body of Knowledge."
CDNA: A Context-Aware Notification System for Driver Interruption,"da Silva, Alberto Vianna Dias and Borges, Lucas and Vieira, Vaninha",10.1145/3274192.3274203,2018,"Inopportune driver notifications are a real problem that may cause distractions and interruptions in traffic, and hence accidents. Notifications on mobile devices are one of the ways by which drivers are interrupted, reaching extremely high amounts in a normal day. Despite that, notifications are valued by users and they are part of the common use of smartphones. Therefore, how to lessen the interruptive potential of notifications without eliminating them completely? To mitigate this problem, the present work proposes a context-aware notification system with the identification of opportune and inopportune moments for drivers notification. The proposed system uses smartphone sensors (gyroscope and GPS) to infer if the driver may be interrupted in a specific moment to receive a notification. Preliminary experiments were performed with people in real driving situations to verify if the system could identify opportune and inopportune moments, and found results indicate that is possible to identify these moments with a general accuracy of 88%."
Measuring and Predicting the Relevance Ratings between FLOSS Projects using Topic Features,"Zheng, Zhiwen and Wang, Liang and Xu, Jingwei and Wu, Tianheng and Wu, Simeng and Tao, Xianping",10.1145/3275219.3275222,2018,"Understanding the relevance between the Free/Libra Open Source Software projects is important for developers to perform code and design reuse, discover and develop new features, keep their projects up-to-date, and etc. However, it is challenging to perform relevance ratings between the FLOSS projects mainly because: 1) beyond simple code similarity, there are complex aspects considered when measuring the relevance; and 2) the prohibitive large amount of FLOSS projects available. To address the problem, in this paper, we propose a method to measure and further predict the relevance ratings between FLOSS projects. Our method uses topic features extracted by the LDA topic model to describe the characteristics of a project. By using the topic features, multiple aspects of FLOSS projects such as the application domain, technology used, and programming language are extracted and further used to measure and predict their relevance ratings. Based on the topic features, our method uses matrix factorization to leverage the partially known relevance ratings between the projects to learn the mapping between different topic features to the relevance ratings. Finally, our method combines the topic modeling and matrix factorization technologies to predict the relevance ratings between software projects without human intervention, which is scalable to a large amount of projects. We evaluate the performance of the proposed method by applying our topic extraction and relevance modeling methods using 300 projects from GitHub. The result of topic extraction experiment shows that, for topic modeling, our LDA-based approach achieves the highest hit rate of 98.3% and the highest average accuracy of 29.8%. And the relevance modeling experiment shows that our relevance modeling approach achieves the minimum average predict error of 0.093, suggesting the effectiveness of applying the proposed method on real-world data sets."
Odyssey-ProcessCase: A Case-Based Software Process Line Approach,"Costa, Diogo M. and Teixeira, Eld\^{a}nae N. and Werner, Claudia M. L.",10.1145/3275245.3275263,2018,"Software processes have been the focus of discussion in the literature, but defining a software process that meets project-specific needs remains a challenge. The Software Process Line (SPrL) technique offers a systematic to identify processes' similarities and variability to support software process reuse. Based on a literature review analysis, a concentration of SPrL approaches that use mapping/rules techniques to support project-specific software process definition was observed. However, the knowledge acquisition process required by this kind of technique is not trivial, due to the unavailability of experts and overhead in domain engineering. This paper presents an incremental learning approach for SPrL, called Odyssey-ProcessCase, focused on the decision-making support to solve SPrL variability during the project-specific software process definition. The approach applies techniques such as Case-Based Reasoning (CBR) and Rule-Based System to offer complementary mechanisms to support the decision-making task aiming at the software process definition from reusable artifacts of SPrL."
Using an Ontology-based Approach for Integrating Applications to support Software Processes,"Renault, Laylla D.C. and Barcellos, Monalessa Perini and de Almeida Falbo, Ricardo",10.1145/3275245.3275269,2018,"Software organizations use several applications to support their software processes. To properly support the software processes, applications should be integrated at different layers (data, service, and process). Moreover, the integration should cover semantic aspects. Therefore, an approach that provides guidelines on how to perform integration at different layers addressing semantic aspects can be helpful. This paper presents an extension of the Ontology-based Approach for Semantic Integration (OBA-SI), focusing on semantic integration at process layer. This extension establishes relationships between integration at data, service and process layers, and uses task ontologies and a process ontology to guide integration at process layer. It was used to provide an integrated solution involving applications supporting the Issue Management and Software Configuration Management processes."
Gamification and Evaluation the Use of the Function Points Analysis Technique in Software Quality Subjects: The Experimental Studies,"Santos, E. D. and Oliveira, S. R. B.",10.1145/3275245.3275290,2018,"With the development of new technologies at all times and the increase of competition among organizations, it is essential that they seek to achieve quality in the development of their applications. An essential tool for this is Function Point Analysis (FPA). This makes it important for students to have contact with this technique as early as possible. Thus, this study aims to use the concepts of gamification to stimulate the support for teaching and engaging students' motivation in the subject of Software Quality taught in the graduate course in computer science at UFPA. For this, classes were defined to teach the FPA technique that used elements of games as motivation for the students. Therefore, this research resulted in an enrichment of the knowledge of these students in the practice of estimation, commonly present and recommended the use in software quality models, such as MR-MPS-SW. This work focuses on contributing to the teaching of the FPA technique for students, aiming at a better preparation for the software development market. It was also verified that the use of gamification elements for the teaching of this estimation technique was efficient, since the participating students were more dedicated to the tasks and were participative in all the different types of classes."
On the usage of pythonic idioms,"Alexandru, Carol V. and Merchante, Jos\'{e} J. and Panichella, Sebastiano and Proksch, Sebastian and Gall, Harald C. and Robles, Gregorio",10.1145/3276954.3276960,2018,"Developers discuss software architecture and concrete source code implementations on a regular basis, be it on question-answering sites, online chats, mailing lists or face to face. In many cases, there is more than one way of solving a programming task. Which way is best may be decided based on case-specific circumstances and constraints, but also based on convention. Having strong conventions, and a common vocabulary to express them, simplifies communication and strengthens common understanding of software development problems and their solutions. While many programming ecosystems have a common vocabulary, Python's relationship to conventions and common language is a particularly pronounced. The ""Zen of Python"", a famous set of high-level coding conventions authored by Tim Peters, states ""There should be one, and preferably only one, obvious way to do it"". This 'one way to do it' is often referred to as the 'Pythonic' way: the ideal solution to a particular problem. Few other programming languages have coined a unique term to label the quality of craftsmanship gone into a software artifact. In this paper, we explore how Python developers understand the term 'Pythonic' by means of structured interviews, build a catalogue of 'pythonic idioms' gathered from literature, and conjecture on the effects of having a language-specific term for quality code, considering the potential it could hold for other programming languages and ecosystems. We find that while the term means different things to novice versus experienced Python developers, it encompasses not only concrete implementation, but a way of thinking — a culture — in general."
Promoting information resource management for e-government through big data approach,"Sarker, Md Nazirul Islam and Hossin, Md Altab and Frimpong, Adasa Nkrumah Kofi and Xiaohua, Yin",10.1145/3277139.3277155,2018,"Big data has a potential to transform traditional government system to data-driven e-government system by utilizing modern analytical techniques. The aim of this article is to explore the applicability of big data for ensuring e-government. An extensive literature review has been administered using various levels of scales and indicators. Literature survey shows that a number of models have been developed to explain e-governance but systematic research on the suitability of big data for e-government is still lacking. This article argues that big data can help the information resource management system of the government for improving transparency and reducing corruption, fastest public service delivery, reducing public hassle, providing easy access to public services, reducing error and reducing poverty through e-services, e-management, e-democracy, and e-commerce. This article further argues that big data has a significant role in cost-effective service delivery to citizens, policy coherence, access to public services, participation and engagement, representation, access to information, open government and corruption control. The finding suggests that big data technologies should be implemented in every public-sector organization by minimizing technological challenges and threats, ensuring the privacy of citizen's information, maximizing utilization of data and promoting information management capacity."
Critical variables for success in the technology adoption process in the framework of digital transformation,"Arbaiza, Cesar Enrique Salas",10.1145/3277139.3277163,2018,"The objective of the research is to identify the most representative variables considered critical for success in adoption processes in technology applicable to projects based on information technology and communications (ICT) in the framework of the digital transformation of organizations. For its identification, a cause-and-effect analysis is applied where the relationships that exist between ICT projects emerge and a categorization of technology projects for digital transformation framed in a classic evolution of projects, it is believed that before undertaking projects of this type it is important to consult this article in order to know in advance the variables that will guarantee successful projects."
Is one hyperparameter optimizer enough?,"Tu, Huy and Nair, Vivek",10.1145/3278142.3278145,2018,"Hyperparameter tuning is the black art of automatically finding a good combination of control parameters for a data miner. While widely applied in empirical Software Engineering, there has not been much discussion on which hyperparameter tuner is best for software analytics.To address this gap in the literature, this paper applied a range of hyperparameter optimizers (grid search, random search, differential evolution, and Bayesian optimization) to a defect prediction problem. Surprisingly, no hyperparameter optimizer was observed to be “best” and, for one of the two evaluation measures studied here (F-measure), hyperparameter optimization, in 50% of cases, was no better than using default configurations. We conclude that hyperparameter optimization is more nuanced than previously believed. While such optimization can certainly lead to large improvements in the performance of classifiers used in software analytics, it remains to be seen which specific optimizers should be applied to a new dataset."
Affective Dynamics and Control in Group Processes,"Hoey, Jesse and Schr\""{o}der, Tobias and Morgan, Jonathan H. and Rogers, Kimberly B. and Nagappan, Meiyappan",10.1145/3279981.3279990,2018,"The computational modeling of groups requires models that connect micro-level with macro-level processes and outcomes. Recent research in computational social science has started from simple models of human behaviour, and attempted to link to social structures. However, these models make simplifying assumptions about human understanding of culture that are of ten not realistic and may be limiting in their generality. In this paper, we present work on Bayesian affect control theory as a more comprehensive, yet highly parsimonious model that integrates artificial intelligence, social psychology, and emotions into a single predictive model of human activities in groups. We illustrate these developments with examples from an ongoing research project aimed at computational analysis of virtual software development teams."
A knowledge discovery in community contributions of big data technologies,"Bieh-Zimmert, Oliver and Felden, Carsten",10.1145/3281375.3281382,2018,"The increasing variety of big data technologies in open source communities is challenging organizations to generate value from those advancements. The technology landscape is missing an overall perspective that clarifies the fragmented understanding of technologies, unpredictable lifecycles, and the unknown adoption for organizations to enable their business with useful technologies. More than one million contributions of features, bugs, and changes were pushed on public available code repositories to develop big data technologies with hidden understanding of the underlying data basis. Using this source could help to identify insights about technological domains as well as their adoption process of contributors to new uprising big data technologies. A knowledge discovery process provided the potential to analyze 269 big data technologies regarding their contribution behavior of over 21,000 contributors. As a result, investigations show an ecosystem of structuring big data technologies based on dynamic contributor networks that have implications on organizations adoption."
Natural language processing (NLP) applied on issue trackers,"Ellmann, Mathias",10.1145/3283812.3283825,2018,"In the domain of software engineering NLP techniques are needed to use and find duplicate or similar development knowledge which are stored in development documentation as development tasks. To understand duplicate and similar development documentations we will discuss different NLP techniques as descriptive statistics, topic analysis and similarity algorithms as N-grams, the Jaccard or LSI algorithm as well as machine learning algorithms as Decision trees or support vector machines (SVM). Those techniques are used to reach a better understanding of the characteristics, the lexical relations (syntactical and semantical) and the classification and prediction of duplicate development tasks. We found that duplicate tasks share conceptual information and are rather created by inexperienced developers. By tuning different features to predict development tasks with a gradient or a Fidelity loss function a system can identify a duplicate tasks with a 100% accuracy."
Function Point Method Based on Hierarchical Convolutional Neural Network,"Jie, Guo",10.1145/3285957.3285979,2018,"Function Point method is vulnerable to personal knowledge and experience in software requirements assessment. Combined with JIRA-the work item tool, CNN method is used to extract the feature value of each item, through pooling layer compression and fully connected layer classification. The function classification and function point count of the item are finally determined, and the total function point count are obtained by weighting the item level. It is shown that this method is not easy to produce excessive deviation and can be used to assist prediction."
Assessing the Critical Sources of Wastes in the Moroccan Construction Industry: An Empirical Study,"Bajjou, Mohamed Saad and Chafi, Anas",10.1145/3286606.3286783,2018,"The performance of construction projects worldwide is significantly impacted by the occurrence of waste; however, few studies have been carried out in this research area especially in developing countries. In this study, 24 types of waste have been identified starting from a review of previous research, and then reinforced by semi-structured interviews with Moroccan construction experts. Thereafter, a structured questionnaire was conducted to survey 330 Moroccan construction practitioners with a view to investigate the critical origins of waste in Morocco. ""Activity start delays"" was found to be the most significant source of waste according to the perceptions of the surveyed respondents. Furthermore, the statistical analysis revealed that the top three ranked waste categories were: 1-unused employee creativity, 2-delay and waiting, and 3-defects/errors/corrections."
What is the Effect of a Software Studio Experience on a Student's Employability?,"Prior, Julia and Laudari, Suman and Leaney, John",10.1145/3286960.3286964,2019,"Our software studio demonstrably increases students' employability, according to the empirical findings of this study, and an evaluation of these findings based on the CareerEDGE Employability Development Profile.We provide a studio environment in which students work in mixed teams on real software projects for clients, under the guidance of industry and academic mentors. This study used open-ended interviews and ethnographic observations in the studio sessions to understand employability success.Skills found important for employability include: collaboration and communication, project management, supporting each other to resolve technical issues, seeking help from industry mentors and academics, social aspects of work (working with clients and mentors), reflection skills and technical skills. These skills were compared with the CareerEDGE Employability Development Profile and found to give good coverage of employability skills.Contributions made by this study to computing education are:• A deep empirical understanding of students' perspectives and what they value about their employability as a result of participating in the software studio• An evaluation of our findings against the CareerEDGE employability framework, in a technical learning environment• Findings from an investigation that is complementary to students' perspectives collected in accordance with the CareerEDGE approach, where the data is collected via a questionnaire with 5-point Likert scale responses; our interviews were open-ended and accompanied by ethnographic observations"
Teaching Accessibility: A Design Exploration of Faculty Professional Development at Scale,"Kawas, Saba and Vonessen, Laura and Ko, Amy J.",10.1145/3287324.3287399,2019,"Most CS students learn little about accessibility in higher education; this is partly because most CS faculty know little about accessibility. Unfortunately, higher education CS faculty lack a model of professional development for learning to teach new topics. Therefore, we investigated the feasibility of a ""micro"" professional development model for teaching accessibility in CS courses that could be used at scale. We conducted 18 semi-structured interviews with U.S. CS faculty, asking them to explore a prototype of a web-based professional development tool that linked accessibility topics to CS topics. We found that many organizational factors limited faculty's autonomy to integrate accessibility in many of their courses. We also found that individual values and knowledge constrained faculty's ability and willingness to both learn and integrate accessibility topics into their courses. However, many faculty expressed desire to teach accessibility in their courses if they had access to even basic accessibility content and materials to use in their courses."
Using Rubrics Integrating Design and Coding to Assess Middle School Students' Open-ended Block-based Programming Projects,"Basu, Satabdi",10.1145/3287324.3287412,2019,"Free-choice, open-ended projects are commonly used to assess student learning in introductory block-based programming (BBP) environments. They are generally assessed in school based on criteria such as the social impact conveyed, whether the projects work without errors, and whether they are creative and engaging. Additionally, researchers have assessed such projects based on the frequency of use of various coding constructs like variables, conditionals, and iterations. This paper presents a novel multi-dimensional rubric for analyzing open-ended BBP projects that integrates assessment of front-end project design and back-end sophistication of use of coding constructs. Further, the novelty of the rubric lies in the fact that instead of relying solely on frequencies, it uses scaled scores based on sophistication of rubric components. Using this rubric, 160 Scratch and App Inventor projects were scored and analyzed. The paper establishes external validity of the rubric and examines what we can learn about student learning from this analysis. Our findings will help K-12 CS educators and curriculum developers recognize what aspects of CS middle school students need most support on, and how to leverage programming environments to provide this support."
Utilizing the Affinity Research Group Model in a Summer Research Experience for Undergraduates Program,"Jelen, Ben and Dunbar, Julia and Monsey, Susan and Richards, Olivia K. and Siek, Katie A.",10.1145/3287324.3287501,2019,"In this paper, we describe how we integrated the Affinity Research Group (ARG) model into a 10-week summer research experience for undergraduates (REU) over the last three years. Each summer, ten REU students participated in: an annual two-day team-building orientation; a four day long ramp-up session to learn about REU projects and the skill sets they would need to master; weekly meetings with graduate and faculty mentors to define objectives, deliverables, and progress; and weekly skill development workshops to learn how to conduct research, prepare for future endeavors, and present oneself in professional settings. A total of seven faculty and twelve graduate students participated in ARG model process meetings to ensure students received comparable experiences and facilitated professional development activities. We provide an overview of our REU ARG model experience and reflect on improvements based on faculty mentor, graduate mentor, and undergraduate researchers' experiences."
An Analytic Hierarchy Process based approach for Information technology governance framework selection,"Bouayad, Hakim and Benabbou, Loubna and Berrado, Abdelaziz",10.1145/3289402.3289515,2018,"Nowadays, the Information Technology (IT) is so dynamic in terms of innovation and the clients' needs are so demanding to gain the competitive advantage that an IT governance framework are a necessity. It permits to the organization to be agile, to implement the digital transformation and then to be ahead of the competitors. As there is no single definition of ITG and there is a multitude of ITG frameworks, the selection of the most suitable framework is not easy. There's a few research done concerning the selection phase of an ITG framework. This paper tries to fill this gap in literature, using the Analytic Hierarchy Process (AHP), a multicriteria decision analysis (MCDA) method that aids a Decision Maker (DM) to decide and choose the best suitable ITG framework to the organization. An application of this method is done for a public pharmaceutical supply chain in a developing country."
Categorizing projects for portfolio selection using clustering techniques,"Elbok, Ghizlane and Berrado, Abdelaziz",10.1145/3289402.3289531,2018,"This work presents a project categorization process that will help practitioner's and decision makers, in a given organization, to group project portfolio components, falling within common strategic orientations. This is a crucial step before they can initiate candidate projects prioritization and selection process based on common criteria.In this regard, we developed an empirically-based categorization approach through clustering techniques where general but most important project attributes are emphasized. It is initiated by considering the organizational strategy and project characteristics, specific to the organization.The suggested approach is then illustrated through the categorization of more than 50 projects in the case of a company operating in the automotive industry."
"Examining the diverse field of ""e-learning"" and its key competencies through job postings","Sun, Yumeng and Hew, Khe Foon and Tang, Ying and Gonda, Donn Emmanuel",10.1145/3290511.3290575,2018,"This study examines the various job description terms as well as key competencies associated with the field of e-learning as revealed in job postings. We systematically analyzed 53 job postings retrieved from 7 websites. The results show that the current top 10 e-learning key competencies are: Content management, Collaboration skills, Website knowledge, Project management, Andragogy knowledge, Interpersonal communication skills, Pedagogy, Digital technology mastery, Oral communication skill and written communication skills. New emerging competencies such as copy rights issues, mastery of programming language and data analytical skills were also identified. These competencies bring opportunities as well as new challenges to e-learning practitioners."
Collaborative Practices with Structured Data: Do Tools Support What Users Need?,"Koesten, Laura and Kacprzak, Emilia and Tennison, Jeni and Simperl, Elena",10.1145/3290605.3300330,2019,"Collaborative work with data is increasingly common and spans a broad range of activities - from creating or analysing data in a team, to sharing it with others, to reusing someone else's data in a new context. In this paper, we explore collaboration practices around structured data and how they are supported by current technology. We present the results of an interview study with twenty data practitioners, from which we derive four high-level user needs for tool support. We compare them against the capabilities of twenty systems that are commonly associated with data activities, including data publishing software, wikis, web-based collaboration tools, and online community platforms. Our findings suggest that data-centric collaborative work would benefit from: structured documentation of data and its lifecycle; advanced affordances for conversations among collaborators; better change control; and custom data access. The findings help us formalise practices around data teamwork, and build a better understanding how people's motivations and barriers when working with structured data."
Can Privacy Be Satisfying? On Improving Viewer Satisfaction for Privacy-Enhanced Photos Using Aesthetic Transforms,"Hasan, Rakibul and Li, Yifang and Hassan, Eman and Caine, Kelly and Crandall, David J. and Hoyle, Roberto and Kapadia, Apu",10.1145/3290605.3300597,2019,"Pervasive photo sharing in online social media platforms can cause unintended privacy violations when elements of an image reveal sensitive information. Prior studies have identified image obfuscation methods (e.g., blurring) to enhance privacy, but many of these methods adversely affect viewers' satisfaction with the photo, which may cause people to avoid using them. In this paper, we study the novel hypothesis that it may be possible to restore viewers' satisfaction by 'boosting' or enhancing the aesthetics of an obscured image, thereby compensating for the negative effects of a privacy transform. Using a between-subjects online experiment, we studied the effects of three artistic transformations on images that had objects obscured using three popular obfuscation methods validated by prior research. Our findings suggest that using artistic transformations can mitigate some negative effects of obfuscation methods, but more exploration is needed to retain viewer satisfaction."
Mapping the Landscape of Creativity Support Tools in HCI,"Frich, Jonas and MacDonald Vermeulen, Lindsay and Remy, Christian and Biskjaer, Michael Mose and Dalsgaard, Peter",10.1145/3290605.3300619,2019,"Creativity Support Tools (CSTs) play a fundamental role in the study of creativity in Human-Computer Interaction (HCI). Even so, there is no consensus definition of the term 'CST' in HCI, and in most studies, CSTs have been construed as one-off exploratory prototypes, typically built by the researchers themselves. This makes it difficult to clearly demarcate CST research, but also to compare findings across studies, which impedes advancement in digital creativity as a growing field of research. Based on a literature review of 143 papers from the ACM Digital Library (1999-2018), we contribute a first overview of the key characteristics of CSTs developed by the HCI community. Moreover, we propose a tentative definition of a CST to help strengthen knowledge sharing across CST studies. We end by discussing our study's implications for future HCI research on CSTs and digital creativity."
Riker: Mining Rich Keyword Representations for Interpretable Product Question Answering,"Zhao, Jie and Guan, Ziyu and Sun, Huan",10.1145/3292500.3330985,2019,"This work studies product question answering (PQA) which aims to answer product-related questions based on customer reviews. Most recent PQA approaches adopt end2end semantic matching methodologies, which map questions and answers to a latent vector space to measure their relevance. Such methods often achieve superior performance but it tends to be difficult to interpret why. On the other hand, simple keyword-based search methods exhibit natural interpretability through matched keywords, but often suffer from the lexical gap problem. In this work, we develop a new PQA framework (named Riker) that enjoys the benefits of both interpretability and effectiveness. Riker mines rich keyword representations of a question with two major components, internal word re-weighting and external word association, which predict the importance of each question word and associate the question with outside relevant keywords respectively, and can be jointly trained under weak supervision with large-scale QA pairs. The keyword representations from Riker can be directly used as input to a keyword-based search module, enabling the whole process to be effective while preserving good interpretability. We conduct extensive experiments using Amazon QA and review datasets from 5 different departments, and our results show that Riker substantially outperforms previous state-of-the-art methods in both synthetic settings and real user evaluations. In addition, we compare keyword representations from Riker and those from attention mechanisms popularly used for deep neural networks through case studies, showing that the former are more effective and interpretable."
The internet of things in undergraduate computer and information science education: exploring curricula and pedagogy,"Burd, Barry and Barker, Lecia and P\'{e}rez, F\'{e}lix Armando Ferm\'{\i}n and Russell, Ingrid and Siever, Bill and Tudor, Liviana and McCarthy, Michael and Pollock, Ian",10.1145/3293881.3295784,2018,"As the Internet of Things (IoT) continues its expansion into homes, businesses, government, and industries, the impact for computer science educators is amplified. In 2017, the ITiCSE IoT working group identified relevant content, tools for teaching, and four IoT course types. The resulting report provided an entry point for educators challenged with setting up a new IoT course, but did not consider the curricular content of a standalone specialization nor effective teaching approaches for this interdisciplinary field. In this report, the 2018 working group builds on its prior work through an updated review of literature and interviews with IoT instructors. The report addresses two research questions. First, what should a curriculum intended to produce IoT specialists include? We propose here a transdisciplinary curriculum that integrates threads from several disciplines on a single campus and we relate it to the ACM/IEEE 2013 Computing Curricula Knowledge Areas. Second, what pedagogical practices should be used to teach IoT? We found very little scholarship describing actual teaching practices, but our interviewees described their approaches and challenges in teaching. We present these as well as descriptions of several relevant teaching approaches in the report."
Learning About Work Tasks to Inform Intelligent Assistant Design,"Trippas, Johanne R. and Spina, Damiano and Scholer, Falk and Awadallah, Ahmed Hassan and Bailey, Peter and Bennett, Paul N. and White, Ryen W. and Liono, Jonathan and Ren, Yongli and Salim, Flora D. and Sanderson, Mark",10.1145/3295750.3298934,2019,"Intelligent assistants can serve many purposes, including entertainment (e.g. playing music), home automation, and task management (e.g. timers, reminders). The role of these assistants is evolving to also support people engaged in work tasks, in workplaces and beyond. To design truly useful intelligent assistants for work, it is important to better understand the work tasks that people are performing. Based on a survey of 401 respondents' daily tasks and activities in a work setting, we present a classification of work-related tasks, and analyze their key characteristics, including the frequency of their self-reported tasks, the environment in which they undertake the tasks, and which, if any, electronic devices are used. We also investigate the cyber, physical, and social aspects of tasks. Finally, we reflect on how intelligent assistants could influence and help people in a work environment to complete their tasks, and synthesize our findings to provide insight on the future of intelligent assistants in support of amplifying personal productivity."
"Slr on Identification &amp; Classification of Non-Functional Requirements Attributes, and Its Representation in Functional Requirements","Nurbojatmiko and Budiardjo, Eko K. and Wibowo, Wahyu C.",10.1145/3297156.3297200,2018,"Software requirements are crucial. Tight relationship between functional and non-functional will determine the quality of requirements, in which ultimately determines the quality of the software being built. Non-Functional Requirements (NFR) often get less attention in the Requirements Engineering (RE) process. In NFR determination, it is necessary to know the attributes and determination criteria that will make easier to determine the relation to FR (Functional Requirements) related. Nowadays many software developers use the Agile Methodology, where the determination of NFR by means of interviews with its stakeholders. Considering the importance of NFR attributes, then the NFR attributes mapping, how to obtain NFR attributes, and how to represent them closely with FR. The research method uses the Prisma Framework. The results of this research determine the NFR attributes domains, identification of NFR attributes, classification of NFR attributes, NFR representation between FR and NFR attributes, are used to obtain application features."
Exploring Violations of Programming Styles: Insights from Open Source Projects,"Yang, Chunyu and Liu, Yan and Yu, Jia",10.1145/3297156.3297227,2018,"Software project is usually a huge cooperative teamwork, programmers in the project usually have to read the code written by others and understand its implementation. A uniform and clean programming style could ensure the readability and maintainability of the project source code, especially when it becomes a legacy project. However, each programmer has his own programming habit and because of the heavy developing tasks, the programming style of the software project is far from satisfactory. Programming style does not resemble software defects which has a serious effect on program executing. Therefore, many programmers ignore the programming style directly instead of improving it. Programming style should be checked before new features are merged into software projects, just like software testing. Developing with the size of software project, some special programming style rules are violated more seriously, which need be highly focused. Furthermore, one of ultimate targets in software quality engineering is to check the programming style automatically with analysis tools because the software projects usually have an enormous quantity of source code. In this paper, static source code analysis is used for detecting the programming style problems. The source file directly or the class files generated by the compiler are scanned then the abstract syntax tree for the source code is generated. With the help of abstract syntax tree, it is possible to detect code snippets that violate the programming style rules by traverse the tree. Our method employs the static code analysis tools to analyze several Java open source projects, and find that the programming style problems which are violated most. According to our method, each problem is also explained from personal habits, JDK version, and other aspects later. Considering all of the analysis results, a special ruleset that is recommended to pay more attention to in the future software developing is proposed. At last, programming style should be highly valued in software development processes in further project management."
Machine learning for improving mobile user satisfaction,"Draa, Ismat Chaib and Bouquillon, Fabien and Niar, Smail and Strugeon, Emmanuelle Grislin-Le",10.1145/3297280.3297398,2019,"Optimizing energy consumption in modern mobile handheld devices plays a crucial role as lowering power consumption impacts system's autonomy and system reliability. Recent mobile platforms have an increasing number of sensors and processing components. Added to the increasing popularity of power-hungry applications, battery life in mobile devices is an important issue. However, we think that the utilization of the large amount of data from the various sensors can be beneficial to detect the changing device context, the user needs or the running application requirements in terms of resources. When these information are used properly, an efficient control of power knobs can be implemented thus reducing the energy consumption. This paper presents URBOC, for User Request Based Optimization Component. This component is an extension of our previous framework [7] ENOrMOUS. This framework was able to identify and classify the user contexts in order to understand user habits, preferences and needs which allow to improve the operating system power scheme. In this paper, we extend the use of ENOrMOUS by allowing the user to send requests in order to extend the battery life up to a specific time. Machine Learning (ML) and data mining algorithms have been used to obtain an efficient trade-off between power consumption reduction opportunities and user requests satisfaction. The proposed solution increases battery life by up to seven hours depending on user requests vs. the out-of-the-box operating system power manager with a negligible overhead."
Debugging Support for Pattern-Matching Languages and Accelerators,"Casias, Matthew and Angstadt, Kevin and Tracy II, Tommy and Skadron, Kevin and Weimer, Westley",10.1145/3297858.3304066,2019,"Programs written for hardware accelerators can often be difficult to debug. Without adequate tool support, program maintenance tasks such as fault localization and debugging can be particularly challenging. In this work, we focus on supporting hardware that is specialized for finite automata processing, a computational paradigm that has accelerated pattern-matching applications across a diverse set of problem domains. While commodity hardware enables high-throughput data analysis, direct interactive debugging (e.g., single-stepping) is not currently supported. We propose a debugging approach for existing commodity hardware that supports step-through debugging and variable inspection of user-written automata processing programs. We focus on programs written in RAPID, a domain-specific language for pattern-matching applications. We develop a prototype of our approach for both Xilinx FPGAs and Micron's Automata Processor that supports simultaneous high-speed processing of data and interactive debugging without requiring modifications to the underlying hardware. Our empirical evaluation demonstrates low clock overheads for our approach across thirteen applications in the ANMLZoo automata processing benchmark suite on FPGAs. Additionally, we evaluate our technique through a human study involving over 60 participants and 20 buggy segments of code. Our generated debugging information increases fault localization accuracy by 22%, or 10 percentage points, in a statistically significant manner (p=0.013)."
Natural Language Processing for Productivity Metrics for Software Development Profiling in Enterprise Applications,"Delaney, Steven and Chan, Christopher Chun Ki and Smith, Doug",10.1145/3299819.3299830,2018,"In this paper, we utilize ontology-based information extraction for semantic analysis and terminology linking from a corpus of software requirement specification documents from 400 enterprise-level software development projects. The purpose for this ontology is to perform semi-supervised learning on enterprise-level specification documents towards an automated method of defining productivity metrics for software development profiling. Profiling an enterprise-level software development project in the context of productivity is necessary in order to objectively measure productivity of a software development project and to identify areas of improvement in software development when compared to similar software development profiles or benchmark of these profiles. We developed a semi-novel methodology of applying NLP OBIE techniques towards determining software development productivity metrics, and evaluated this methodology on multiple practical enterprise-level software projects."
Who should be my teammates: using a conversational agent to understand individuals and help teaming,"Xiao, Ziang and Zhou, Michelle X. and Fu, Wat-Tat",10.1145/3301275.3302264,2019,"We are building an intelligent agent to help teaming efforts. In this paper, we investigate the real-world use of such an agent to understand students deeply and help student team formation in a large university class involving about 200 students and 40 teams. Specifically, the agent interacted with each student in a text-based conversation at the beginning and end of the class. We show how the intelligent agent was able to elicit in-depth information from the students, infer the students' personality traits, and reveal the complex relationships between team personality compositions and team results. We also report on the students' behavior with and impression of the agent. We discuss the benefits and limitations of such an intelligent agent in helping team formation, and the design considerations for creating intelligent agents for aiding in teaming efforts."
Quantle: fair and honest presentation coach in your pocket,"Saukh, Olga and Maag, Balz",10.1145/3302506.3310405,2019,"Great public speakers are made, not born. Practicing a presentation in front of colleagues is common practice and results in a set of subjective judgements what could be improved. In this paper we describe the design and implementation of a mobile app which estimates the quality of speaker's delivery in real time in a fair, repeatable and privacy-preserving way. Quantle estimates the speaker's pace in terms of the number of syllables, words and clauses, computes pitch and duration of pauses. The basic parameters are then used to estimate the talk complexity based on readability scores from the literature to help the speaker adjust his delivery to the target audience. In contrast to speech-to-text-based methods used to implement a digital presentation coach, Quantle does processing locally in real time and works in the flight mode. This design has three implications: (1) Quantle does not interfere with the surrounding hardware, (2) it is power-aware, since 95.2% of the energy used by the app on iPhone 6 is spent to operate the built-in microphone and the screen, and (3) audio data and processing results are not shared with a third party therewith preserving speaker's privacy.We evaluate Quantle on artificial, online and live data. We artificially modify an audio sample by changing the volume, speed, tempo, pitch and noise level to test robustness of Quantle and its performance limits. We then test Quantle on 1017 TED talks held in English and compare computed features to those extracted from the available transcript processed by online text evaluation services. Quantle estimates of syllable and word counts are 85.4% and 82.8% accurate, and pitch is over 90% accurate. We use the outcome of this study to extract typical ranges for each vocal characteristic. We then use Quantle on live data at a social event, and as a tool for speakers to track their delivery when rehearsing a talk. Our results confirm that Quantle is robust to different noise levels, varying distances from the sound source, phone orientation, and achieves comparable performance to speech-to-text methods."
Using Automated Prompts for Student Reflection on Computer Security Concepts,"Chen, Hui and Ciborowska, Agnieszka and Damevski, Kostadin",10.1145/3304221.3319731,2019,"Reflection is known to be an effective means to improve students' learning. In this paper, we aim to foster meaningful reflection via prompts in computer science courses with a significant practical, software development component. To this end we develop an instructional strategy and system that automatically delivers prompts to students based on their commits in a source code repository. The system allows for prompts that instigate reflection in students to be timely with respect to students' work, and delivered automatically, thus easily scaling up the strategy.In this paper, we describe the design of a rule-based prompt delivery system, including a list of security related reflection prompts. We collect preliminary evidence for the reflection strategy in a course targeting mobile development. The evaluation provides evidence that such a system can help realize a reflection-in-action instructional strategy at scale and improve students' learning."
A Scalable Operational Framework for Requirements Validation Using Semantic and Functional Models,"Atoum, Issa",10.1145/3305160.3305166,2019,"A successful operational software depends on adequacy and degrees of freedom in requirements definitions. The software developer in conjunction with the customer validates requirements to ensure the completion of the intended use and the capability of the target application. Notwithstanding, requirements validation is time-consuming, effortless and expensive, and many times involves error-prone manual activities. The difficulty of the problem increases with an increase in the application size, the application domain, and inherit textual requirements constructs. Current approaches to the problem are considered as passive-defect aggregations, domain-specific, or rather fine-grained with formal specifications. We propose a scalable operational framework to learn, predict, and recognize requirements defects using semantic similarity models and the Integration Functional Definition methods. The proposed framework automates the validation process and increases the productivity of software engineers online with customer needs. A proof of concept shows the applicability of our solution to requirements inconsistency defects."
Review of soft magnetic composite permanent magnet motor,"Haixia, Li and Zhiyong, Huang and Geng, Li and Jican, Lin",10.1145/3305275.3305334,2018,"In recent years, researchers in relevant fields in China and abroad have been attaching more and more importance to the research of SMC (soft magnetic composite) permanent magnet motor, which has greatly improved in volume, deadweight and torque density compared with traditional motor, and more and more about the design and research of this kind of permanent magnet motor. This review summarized and summarized the latest research status and research results of SMC materials at home and abroad, introduced the superiority of SMC material on the motor, the design of the permanent magnet motor and the corresponding iron consumption analysis, and finally summarized the prospect of SMC material in the motor."
Ranking warnings from multiple source code static analyzers via ensemble learning,"Ribeiro, Athos and Meirelles, Paulo and Lago, Nelson and Kon, Fabio",10.1145/3306446.3340828,2019,"While there is a wide variety of both open source and proprietary source code static analyzers available in the market, each of them usually performs better in a small set of problems, making it hard to choose one single tool to rely on when examining a program looking for bugs in the source code. Combining the analysis of different tools may reduce the number of false negatives, but yields a corresponding increase in the absolute number of false positives (which is already high for many tools). A possible solution, then, is to filter these results to identify the issues least likely to be false positives. In this study, we post-analyze the reports generated by three tools on synthetic test cases provided by the US National Institute of Standards and Technology. In order to make our technique as general as possible, we limit our data to the reports themselves, excluding other information such as change histories or code metrics. The features extracted from these reports are used to train a set of decision trees using AdaBoost to create a stronger classifier, achieving 0.8 classification accuracy (the combined false positive rate from the used tools was 0.61). Finally, we use this classifier to rank static analyzer alarms based on the probability of a given alarm being an actual bug in the source code."
Large-Scale Talent Flow Forecast with Dynamic Latent Factor Model?,"Zhang, Le and Zhu, Hengshu and Xu, Tong and Zhu, Chen and Qin, Chuan and Xiong, Hui and Chen, Enhong",10.1145/3308558.3313525,2019,"The understanding of talent flow is critical for sharpening company talent strategy to keep competitiveness in the current fast-evolving environment. Existing studies on talent flow analysis generally rely on subjective surveys. However, without large-scale quantitative studies, there are limits to deliver fine-grained predictive business insights for better talent management. To this end, in this paper, we aim to introduce a big data-driven approach for predictive talent flow analysis. Specifically, we first construct a time-aware job transition tensor by mining the large-scale job transition records of digital resumes from online professional networks (OPNs), where each entry refers to a fine-grained talent flow rate of a specific job position between two companies. Then, we design a dynamic latent factor based Evolving Tensor Factorization (ETF) model for predicting the future talent flows. In particular, a novel evolving feature by jointly considering the influence of previous talent flows and global market is introduced for modeling the evolving nature of each company. Furthermore, to improve the predictive performance, we also integrate several representative attributes of companies as side information for regulating the model inference. Finally, we conduct extensive experiments on large-scale real-world data for evaluating the model performances. The experimental results clearly validate the effectiveness of our approach compared with state-of-the-art baselines in terms of talent flow forecast. Meanwhile, the results also reveal some interesting findings on the regularity of talent flows, e.g. Facebook becomes more and more attractive for the engineers from Google in 2016."
Reading Between the Guidelines: How Commercial Voice Assistant Guidelines Hinder Accessibility for Blind Users,"Branham, Stacy M. and Mukkath Roy, Antony Rishin",10.1145/3308561.3353797,2019,"Voice-Activated Personal Assistants (VAPAs)-like Apple Siri and Amazon Alexa - have rapidly become common features on mobile devices and in homes of millions of people around the world. They have proven to be particularly valuable to people with disabilities, chiefly among people with visual impairments. Yet, we still know relatively little about the fundamental metaphors and guidelines for designing voice assistants, and how they might empower and constrain visually impaired users. To address this need, we conducted a qualitative document review of VAPA design guidelines published by top commercial vendors Amazon, Google, Microsoft, Apple and Alibaba. We found that guidelines have many commonalities that surface an underlying assumption that VAPA interfaces should be modeled after human-human conversation. We draw on prior work about needs of people with visual impairments to critique this taken-for-granted human-human conversation metaphor and offer amendments to prevailing design guidelines that can make this now-pervasive platform more fully achieve its potential to become universally usable."
ICEBERG: Imagery Cyber-infrastructure and Extensible Building blocks to Enhance Research in the Geosciences. (A Research Programmer’s Perspective),"Spitzbart, Bradley D. and Lynch, Heather J. and Turilli, Matteo and Jha, Shantenu",10.1145/3311790.3396640,2020,"The ICEBERG (Imagery Cyber-infrastructure and Extensible Building blocks to Enhance Research in the Geosciences) project (NSF&nbsp;#&nbsp;1740595) aims to (1) develop open source image classification tools tailored to high-resolution satellite imagery of the Arctic and Antarctic to be used on HPDC resources, (2) create easy-to-use interfaces to facilitate the development and testing of algorithms for application to specific geoscience requirements, (3) apply these tools through use cases that span the biological, hydrological, and geoscience needs of the polar community, and (4) transfer these tools to the larger non-polar community. Here we report on the project status and lessons learned."
A Survey on Research of Code Comment,"Yang, Bai and Liping, Zhang and Fengrong, Zhao",10.1145/3312662.3312710,2019,"Code comments are one of the effective means for assisting programmers to understand the source code. High-quality code comments play an important role in areas such as software maintenance and software reuse. Good code comments can help programmers understand the role of source code and facilitate comprehension of programs and software maintenance tasks quickly. But in reality, most programmers only pay attention to the code and ignore comments and documents, which greatly reduce the program's readability and maintainability. This paper has compiled the relevant research on code comments so far, mainly including four aspects: automatic generation of code comments, consistency of code comments, classification of code comments, and quality evaluation of code comments. By analyzing relevant methods in this research field, it provides more complete information for future research."
Supporting Software Developers' Focused Work on Window-Based Desktops,"Pilzer, Jan and Rosenast, Raphael and Meyer, Andr\'{e} N. and Huang, Elaine M. and Fritz, Thomas",10.1145/3313831.3376285,2020,"Software developers, like other information workers, continuously switch tasks and applications to complete their work on their computer. Given the high fragmentation and complexity of their work, staying focused on the relevant pieces of information can become quite challenging in today's window-based environments, especially with the ever increasing monitor screen-size. To support developers in staying focused, we conducted a formative study with 18 professionals in which we examined their computer based and eye-gaze interaction with the window environment and devised a relevance model of open windows. Based on the results, we developed a prototype to dim irrelevant windows and reduce distractions, and evaluated it in a user study. Our results indicate that our model was able to predict relevant open windows with high accuracy and participants felt that integrating visual prominence into the desktop environment reduces clutter and distraction, which results in reduced window switching and an increase in focus."
A Participatory Simulation of the Accountable Capitalism Act,"Tomlinson, Bill and Silberman, M. Six and Torrance, Andrew W. and Squire, Kurt and Atwal, Paramdeep S. and Mandalik, Ameya N. and Railkar, Sahil and Black, Rebecca W.",10.1145/3313831.3376326,2020,"Interactive computing systems increasingly allow for experimental evaluations of fundamental issues in law, government, and society. In this paper, we describe a participatory simulation of the Accountable Capitalism Act, a bill proposed in 2018 by US Senator Elizabeth Warren. We present findings from an empirical study conducted using this system, relating to the impact of 1) interactive visualization and 2) the Accountable Capitalism Act legal framework on the behavior of participants acting as corporate directors. From this study, we draw lessons about research possibilities at the juncture of HCI and legal and policy studies. This study contributes an analysis and evaluation of a design probe used to investigate potential impacts of the Accountable Capitalism Act, experimental evidence from a study conducted using the design probe, and guidance for future participatory simulations that seek to inform the design of social institutions."
A Literature Review of Quantitative Persona Creation,"Salminen, Joni and Guan, Kathleen and Jung, Soon-Gyo and Chowdhury, Shammur A. and Jansen, Bernard J.",10.1145/3313831.3376502,2020,"Quantitative persona creation (QPC) has tremendous potential, as HCI researchers and practitioners can leverage user data from online analytics and digital media platforms to better understand their users and customers. However, there is a lack of a systematic overview of the QPC methods and progress made, with no standard methodology or known best practices. To address this gap, we review 49 QPC research articles from 2005 to 2019. Results indicate three stages of QPC research: Emergence, Diversification, and Sophistication. Sharing resources, such as datasets, code, and algorithms, is crucial to achieving the next stage (Maturity). For practitioners, we provide guiding questions for assessing QPC readiness in organizations."
Beyond the Prototype: Understanding the Challenge of Scaling Hardware Device Production,"Khurana, Rushil and Hodges, Steve",10.1145/3313831.3376761,2020,"The hardware research and development communities have invested heavily in tools and materials that facilitate the design and prototyping of electronic devices. Numerous easy-to-access and easy-to-use tools have streamlined the prototyping of interactive and embedded devices for experts and led to a remarkable growth in non-expert builders. However, there has been little exploration of challenges associated with moving beyond a prototype and creating hundreds or thousands of exact replicas - a process that is still challenging for many. We interviewed 25 individuals with experience taking prototype hardware devices into low volume production. We systematically investigated the common issues faced and mitigation strategies adopted. We present our findings in four main categories: (1) gaps in technical knowledge; (2) gaps in non-technical knowledge; (3) minimum viable rigor in manufacturing preparation; and (4) building relationships and a professional network. Our study unearthed several opportunities for new tools and processes to support the transition beyond a working prototype to cost effective low-volume manufacturing. These would complement the aforementioned tools and materials that support design and prototyping."
Pervasive technologies applied to the work environment: Implications for end-users: the foreground for SmartWork concerns and requirements,"Ortet, Sofia and Dantas, Carina and Machado, Natalia and Tageo, Valentina and Quintas, Jo\~{a}o and Haansen, Sonja",10.1145/3316782.3322769,2019,"Pervasive technologies reflect a whole ecosystem of information and communication environments, in which computer sensors and other equipment are unified with various objects, people, information, and computers, as well as with the physical environment to transform experiences. With the demographic change, big challenges arise for an ageing working population that may be addressed by the creation of facilitating environments, merging professional and personal needs into smooth and nonintrusive solutions that enhance older workers to stay longer and more productive in the workforce. This paper intends to present a bibliographic review on the state-of-the-art research devoted to the analysis of the use of pervasive technology, referring to the identified pros and cons of its use in the work environment. It will also highlight the most relevant questions identified in literature as influencing acceptance and adhesion of these technologies in the professional context. Finally, there are some reflections already expressed on the specific use of pervasive technologies by older workers, although it is understood that very little and non-specific data is available in this context. For this purpose, the development of project SmartWork (Horizon 2020) will provide new and relevant data on this subject."
Food Item Recognition and Intake Measurement Techniques,"Shehzad, Adnan and Zafar, Nauman and Hassan, Mir and Shen, Zhidong",10.1145/3318299.3318379,2019,"High-calorie intake can be harmful and result in numerous diseases. Standard intake of a number of calories is fundamental for keeping the right balance of calories in the human body. Currently, some techniques allow users to estimate the calorie count of their food. The latest applications developed to solve under description topic enabled the user to identify calorie part of a food item by taking its photograph. The photograph then passes some pre-processing steps, and after successful segmentation, many physical features are examined such as shape and size etc. Also, dimensions of the food object are determined. The concluding step is then recognition along with calorie estimation. In this paper, different calorie estimation techniques are reviewed. Every method has negative and positive features as well. We also throw light on the deficits of these techniques and some ideas to improve those deficits. The main aim of this review paper is to do a critical analysis of recent studies on accurate calorie estimation and food item recognition. We contribute to building a system that provides tools to monitor calorie intake by estimating calories based on food item recognition and accurate volume calculation."
Is the Stack Distance Between Test Case and Method Correlated With Test Effectiveness?,"Niedermayr, Rainer and Wagner, Stefan",10.1145/3319008.3319021,2019,"Mutation testing is a means to assess the effectiveness of a test suite and its outcome is considered more meaningful than code coverage metrics. However, despite several optimizations, mutation testing requires a significant computational effort and has not been widely adopted in industry. Therefore, we study in this paper whether test effectiveness can be approximated using a more light-weight approach. We hypothesize that a test case is more likely to detect faults in methods that are close to the test case on the call stack than in methods that the test case accesses indirectly through many other methods. Based on this hypothesis, we propose the minimal stack distance between test case and method as a new test measure, which expresses how close any test case comes to a given method, and study its correlation with test effectiveness. We conducted an empirical study with 21 open-source projects, which comprise in total 1.8 million LOC, and show that a correlation exists between stack distance and test effectiveness. The correlation reaches a strength up to 0.58. We further show that a classifier using the minimal stack distance along with additional easily computable measures can predict the mutation testing result of a method with 92.9% precision and 93.4% recall. Hence, such a classifier can be taken into consideration as a light-weight alternative to mutation testing or as a preceding, less costly step to that."
Enabling heterogeneous recommendations in OSS development: what's done and what's next in CROSSMINER,"Nguyen, Phuong T. and Di Rocco, Juri and Di Ruscio, Davide",10.1145/3319008.3319353,2019,"Open source software (OSS) forges contain rich data sources that are useful for supporting development activities. Research has been done to promote techniques and tools for providing open source developers with innovative features aiming at obtaining improvements in terms of development effort, cost savings, and developer productivity, just to mention a few. In the context of the EU H2020 CROSSMINER project we are conceiving a set of recommendations to assist software programmers in different phases of the development process. To this end, we defined a graph-based representation to encode in a homogeneous manner different aspects of OSS ecosystems as well as to incorporate various well-founded recommendation techniques. Following the proposed paradigm, we have implemented recommender systems for providing various artifacts, such as third-party libraries and API usage. The preliminary results we achieved so far are promising: our proposed systems are able to suggest highly relevant items with respect to the current development context. In this paper, we describe what has been achieved so far as well as our planned medium and longer-term objectives. As a proof of concept, we present a use case where we built a context-aware recommender system to recommend API function calls and usage patterns."
Worst case search over a set of forecasting scenarios applied to financial stress-testing,"Finck, Steffen",10.1145/3319619.3326835,2019,"Stress testing is part of today's bank risk management and often required by the governing regulatory authority. Performing such a stress test with stress scenarios derived from a distribution, instead of pre-defined expert scenarios, results in a systematic approach in which new severe scenarios can be discovered. The required scenario distribution is obtained from historical time series via a Vector-Autoregressive time series model.The worst-case search, i.e. finding the scenario yielding the most severe situation for the bank, can be stated as an optimization problem. The problem itself is a constrained optimization problem in a high-dimensional search space. The constraints are the box constraints on the scenario variables and the plausibility of a scenario. The latter is expressed by an elliptic constraint.As the evaluation of the stress scenarios is performed with a simulation tool, the optimization problem can be seen as black-box optimization problem. Evolution Strategy, a well-known optimizer for black-box problems, is applied here. The necessary adaptations to the algorithm are explained and a set of different algorithm design choices are investigated. It is shown that a simple box constraint handling method, i.e. setting variables which violate a box constraint to the respective boundary of the feasible domain, in combination with a repair of implausible scenarios provides good results."
ECJ at 20: toward a general metaheuristics toolkit,"Scott, Eric O. and Luke, Sean",10.1145/3319619.3326865,2019,"ECJ is now 20 years old. Begun as a genetic programming and evolutionary computation library in Java, it has since established itself as historically one of the most popular EC toolkits worldwide. In 2016 we received a National Science Foundation grant to improve ECJ in many ways with an eye toward making it a useful toolkit not just for EC but for the broader metaheuristics community. This paper is a report on our efforts to this end. We discuss new metaheuristics frameworks and representations added to ECJ and the design challenges that they raise for a general-purpose framework, as well as testing facilities and other support tools. We conclude with our future directions for the library."
Knowledge-driven reference-point based multi-objective optimization: first results,"Smedberg, Henrik",10.1145/3319619.3326911,2019,"Multi-objective optimization problems in the real world often involve a decision maker who has certain preferences for the objective functions. When such preferences can be expressed as a reference point, the goal of optimization changes from generating a complete set of Pareto-optimal solutions to generating a small set of non-dominated solutions close to the reference point. Reference-point based optimization algorithms are used for this purpose. The preferences of the decision maker in the objective space can be interpreted as knowledge in the decision space. Extracting this knowledge it-eratively from the solutions generated during optimization, and feeding it back into the optimization algorithm can in principle improve convergence towards the reference point. Since the knowledge is extracted during runtime, this approach is termed as online knowledge-driven optimization. In this paper a recent knowledge discovery technique called flexible pattern mining is used to extract explicit rules that are used to generate new solutions in R-NSGA-II. The performance of the proposed FPM-R-NSGA-II is demonstrated on 3, 5 and 10 objective DTLZ problems. In addition to converging to a set of preferred solutions, FPM-R-NSGA-II also converges to a set of explicit rules which describe the decision maker's preferences in the decision space."
Machines as Teammates in Creative Teams: Digital Facilitation of the Dual Pathways to Creativity,"Przybilla, Leonard and Baar, Luka and Wiesche, Manuel and Krcmar, Helmut",10.1145/3322385.3322402,2019,"Considering recent advances in information systems, we pose the question how well a digital facilitator can support the complex task of creative idea generation in teams--especially compared to a human one. Drawing on the dual pathway to creativity model and extant research in group creativity and information systems, we develop a set of interventions for both human and digital facilitation. We test the hypothesized effects in a 2x2 study design with 24 participants and a human or digital voice assistant as facilitators. We find that objective outcomes of digital facilitation are not significantly different from those of human facilitation. Digital facilitation is, however, significantly worse in subjectively perceived helpfulness. These results add to the scant research on the effects of intelligent systems on team interactions and help inform future research on group effects of intelligent information systems."
Substantive Legal Software Quality: A Gathering Storm?,"Lauritsen, Marc and Steenhuis, Quinten",10.1145/3322640.3326706,2019,"Readily available interactive programs dispense substantive legal guidance, often including bespoke documents. These are found across a wide spectrum of commercial and non-commercial contexts. Consumers are coming to rely on them as alternatives to expensive lawyer services. Yet their quality is uneven and difficult to assess. We are in danger of serious harm being done to unwitting users. How can we avoid an epidemic of artificial misinformation, systematic inaccuracy, and mechanical malpractice? This paper reviews how those dangers play out in real-world application contexts and explores ways in which the AI &amp; Law community might help address them."
BiLO-CPDP: bi-level programming for automated model discovery in cross-project defect prediction,"Li, Ke and Xiang, Zilin and Chen, Tao and Tan, Kay Chen",10.1145/3324884.3416617,2021,"Cross-Project Defect Prediction (CPDP), which borrows data from similar projects by combining a transfer learner with a classifier, have emerged as a promising way to predict software defects when the available data about the target project is insufficient. However, developing such a model is challenge because it is difficult to determine the right combination of transfer learner and classifier along with their optimal hyper-parameter settings. In this paper, we propose a tool, dubbed BiLO-CPDP, which is the first of its kind to formulate the automated CPDP model discovery from the perspective of bi-level programming. In particular, the bi-level programming proceeds the optimization with two nested levels in a hierarchical manner. Specifically, the upper-level optimization routine is designed to search for the right combination of transfer learner and classifier while the nested lower-level optimization routine aims to optimize the corresponding hyper-parameter settings. To evaluate BiLO-CPDP, we conduct experiments on 20 projects to compare it with a total of 21 existing CPDP techniques, along with its single-level optimization variant and Auto-Sklearn, a state-of-the-art automated machine learning tool. Empirical results show that BiLO-CPDP champions better prediction performance than all other 21 existing CPDP techniques on 70% of the projects, while being overwhelmingly superior to Auto-Sklearn and its single-level optimization variant on all cases. Furthermore, the unique bi-level formalization in BiLO-CPDP also permits to allocate more budget to the upper-level, which significantly boosts the performance."
Understanding Factors and Mechanisms for Collaborative Governance Based on Smart Technologies,"Zhu, Zhe and Zhang, Nan",10.1145/3325112.3325237,2019,"New information technologies have brought about marvelous changes to public service and social governance. In increasing numbers of smart city projects, collaboration and public private partnerships (PPPs) are a critical issue that both practitioners and academics have paid close attention. The collaborative leadership perspective is important to understanding PPP practices because a single organization does not have all of the features that leadership competencies require in the new context. Therefore, this study proposes a theoretical model that considers both the static factors and dynamic mechanisms for adapting to urban development driven by technological innovations to explain how core organizations act as leaders in multi-party collaborations in ongoing smart city projects. Through two most-different smart urban service cases in Shanghai and Beijing, we try to provide a clearer picture based on the model that describes how two dynamic mechanisms of leadership, catalyzing innovation and controlling conflicts, influence the fundamental static factors of leadership in the core organization during collaboration. The practical implications for both the public and social sectors also are discussed in this paper."
Exploratory Analysis of Individuals' Mobility Patterns and Experienced Conflicts in Workgroups,"Zakaria, Camellia and Goh, Kenneth and Lee, Youngki and Balan, Rajesh",10.1145/3325426.3329946,2019,"Much research argues the importance of supporting social interactions in teams and communities. The field of mobile sensing alone offers significant advances in recording and understanding human and group behaviours. However, little is known about behavioural changes as a consequence of in-group phenomena. One prominent example is intra-group conflict, which naturally arises between diverse groups of people. We demonstrate the feasibility of our approach to extract mobility patterns of individual's group behaviours sensed from a WiFi indoor localisation system and explore how these patterns relate to their team processes. 62 students enrolled in a project-intensive module, Software Engineering, were tracked over 81 days. Preliminary analysis of mobility patterns and interview data revealed differences in the mobility patterns of individuals based on their experience of conflict."
The Evolving e-Governance Curriculum: A Worldwide mapping of Education Programs,"Sarantis, Demetrios and Ben Dhaou, Soumaya and Alexopoulos, Charalampos and Ronzhyn, Alexander and Pereira, Gabriela Viale and Charalabidis, Yannis",10.1145/3326365.3326415,2019,"From the demand side, the need to build e-Governance capacities and expertise is increasing and requires more and more sophisticated knowledge and competencies to fulfil the stakeholders' needs. The e-Governance profession and skills needs are also becoming more diverse and more specialized. From the supply side, we can also witness a growing interest in the e-Governance learning and programs worldwide at different level. However the programs offered are often not well aligned adapting neither to the government's, nor to the public service needs. The e-Governance curriculum is a key success factor to reduce the gap. It serves as a base of knowledge for a large number of graduates that participate in government digital transformation activities. Within academic and practitioners' communities, there have been constant discussions about the content of the e-Governance curriculum. The objective of this research is to identify and analyse the current situation in e-Governance training worldwide and provide a path forward for future e-Governance program relative curriculum development. For this purpose, the authors applied a systematic secondary data review method to examine the existing e-Governance programs and draw an e-Governance education mapping worldwide. The research establishes the current baseline of e-Governance curricula and describes their fundamental aspects and challenges. Information provided in this article should be valuable to the e-Governance educators and curriculum designers, as well as to the e-Governance practitioners, to better understand the foundational knowledge transmitted to e-Governance graduates."
Artificial Intelligence: Opportunities and Challenges for the Public Sector,"Susar, Deniz and Aquaro, Vincenzo",10.1145/3326365.3326420,2019,"Artificial Intelligence (AI) refers to a set of technologies through which ""intelligent machines are gaining the ability to learn, improve and make calculated decisions in ways that enable them to perform tasks previously thought to rely solely on human experience, creativity, and ingenuity"" [2]. As stated in the 2018 the UN E-Government Survey, major technological developments in software and hardware have enhanced the advancement of AI and its potential impact on society. Increased data collection and usage are magnifying the learning process of computers and improving the quality of AI systems which are heavily data-dependent. On the one hand, AI holds the promise to be a catalyst in accelerating development and allowing developing countries to leapfrog over some traditional obstacles. On the other hand, it brings along challenges such as its impact on the workforce, the ethical implications of some of its applications, and the need for capacity-building which would essentially overhaul the kind of education required for the next generation. This working paper analyzes the above issues from a public sector angle and raises questions which should be considered by public administrators and those working on digital government development in particular. Moreover, the analyses are built on the 2018 United Nations E-Government Survey findings and the working paper is expected to be discussed in detail in the preparatory meetings leading to the next edition of the United Nations E-Government Survey."
Prophet Inequality for Bipartite Matching: Merits of Being Simple and Non Adaptive,"Gravin, Nikolai and Wang, Hongao",10.1145/3328526.3329604,2019,"We consider Bayesian online selection problem of a matching in bipartite graphs, i.e., online weighted matching problem with edge arrivals where online algorithm knows distributions of weights, that corresponds to the intersection of two matroids in Kleinberg and Weinberg [35] model. We consider a simple class of non adaptive vertex-additive policies that assign static prices to all vertices in the graph and accept each edge only if its weight exceeds the sum of the prices of the edge's endpoints. We show existence of a vertex-additive policy with the expected payoff of at least one third of the prophet's payoff and present gradient decent type algorithm that quickly converges to the desired vector of vertex prices. This improves the adaptive online policy of Kleinberg and Weinberg for the intersection of two matroids in two ways: our policy is non adaptive and has better approximation guarantee of 3 instead of previous guarantees 5.82 of Kleinberg and Weinberg and 2•e=5.43 of Feldman et al. [23] against the prophet. We give a complementary lower bound of 2.25 for any online algorithm in the bipartite matching setting."
Interdisciplinary Computing: Applied Computing for Behavioral and Social Sciences,"Carr, Valerie and Jones, Morris and Wei, Belle",10.1145/3328778.3366799,2020,"As the digital economy grows, so does the demand for technology-capable workers who have both computing skills and domain expertise. Growing such a workforce is critical to ensuring the nation's competitiveness, according to a recent National Science Board publication. To address this need, faculty from the Colleges of Engineering and Social Sciences at San J\'{o}se State University worked together to create the Applied Computing for Behavioral and Social Sciences minor degree. The minor targets students in majors such as Psychology and Economics, which have a more diverse student population than that of Computer Science or Engineering. The minor, designed with industry input, includes a four-course sequence that focuses on Python and R and includes topics such as data structures, algorithms, data cleaning and management, and data analysis. Our cohort-based program was built specifically for social science students using social science content, helping to foster a sense of community and belongingness among students. The first full cohort of 26 students graduated in Spring 2019, 48% of whom were female and 23% of whom were underrepresented minorities. Our approach of embedding computing education into the social sciences demonstrates a promising model of broadening participation in computing and meeting the nation's increasing demand for interdisciplinary computing workers in the digital age."
A Project-Based Learning Approach for Enhancing Learning Skills and Motivation in Software Engineering,"P\'{e}rez, Beatriz and Rubio, \'{A}ngel L.",10.1145/3328778.3366891,2020,"Software engineers must be able to manage complex projects, so that skills such as teamwork, leadership or initiative are critical to their successful development. Because of this, it is fundamental that the learning of software engineering as an academic discipline provides solid links between theory and practice. Educational frameworks such as those derived from the European Higher Education Area state that student-centered approaches are a useful tool for achieving these objectives. In this context, we present a project-based learning (PBL) experience report in a software engineering program of a Spanish university. The experience is based on the formation of small heterogeneous teams, which face the initial phases of a software methodology during the development of a project close to a real one. Through a strategy of role rotation and documentation transfer, all students perform different tasks and face different challenges throughout the project. Summative assessment is also adopted, considering not only teacher ratings but also students' peer assessment. The results prove the positive effect of using PBL to improve the training of students in acquiring different skills as future software engineers."
To apply Data Mining for Classification of Crowd sourced Software Requirements,"Taj, Soonh and Arain, Qasim and Memon, Imran and Zubedi, Asma",10.1145/3328833.3328837,2019,"Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements."
A Framework for Strategic Cloud Migration,"Ahmed, Monjur and Singh, Navjot",10.1145/3330482.3330528,2019,"This paper presents a novel framework for organisations to carry out a structured feasibility study on Cloud migration and to decide Cloud Migration Strategy. Following the framework helps an organisation to decide whether Cloud migration is a feasible option for them, and if so, the best strategic approach towards Cloud migration. It is a crucial and sensitive part for any organisation to decide whether they should move to Cloud Computing platform. The decision requires strategic approach with proper feasibility study. Several technological, human, security and financial factors are involved in decision making process to move to the Cloud. The proposed framework helps an organisation to carry out a feasibility study to decide whether to move to the Cloud, and if so, what would be the best approach towards Cloud migration. The proposed framework addresses the factors that an organisation must explore to decide on Cloud migration. Cloud Computing has its own pros and cons. A whimsical decision to move to the Cloud may be disastrous for an organisation. Following the proposed framework will help organisations to carry out a structured and integrated feasibility study deal with the decision on Cloud migration."
On Tradeoffs Between Document Signature Methods for a Legal Due Diligence Corpus,"Roegiest, Adam and Lee, Edward",10.1145/3331184.3331311,2019,"While document signatures are a well established tool in IR, they have primarily been investigated in the context of web documents. Legal due diligence documents, by their nature, have more similar structure and language than we may expect out of standard web collections. Moreover, many due diligence systems strive to facilitate real-time interactions and so time from document ingestion to availability should be minimal. Such constraints further limit the possible solution space when identifying near duplicate documents. We present an examination of the tradeoffs that document signature methods face in the due diligence domain. In particular, we quantify the trade-off between signature length, time to compute, number of hash collisions, and number of nearest neighbours for a 90,000 document due diligence corpus."
HUBzero®: Novel Concepts Applied to Established Computing Infrastructures to Address Communities' Needs,"Gesing, Sandra and Zentner, Michael and Clark, Steve and Stirm, Claire and Haley, Ben",10.1145/3332186.3332238,2019,"The science gateway framework HUBzero® has been enhanced and further developed since its initial vision in 1996 -- always driven by requirements of the diverse communities applying HUBzero® for their research. HUBzero® is part of a computational landscape that has never evolved as fast as in the last decade. Novel frameworks and concepts on the user interface side such as Javascript libraries and Jupyter notebooks support communities in their working environment with easy-to-use user interfaces while novel technologies and concepts in the backend allow for effective and efficient modeling, simulations and processing research tools and data. HUBzero®'s enhancements include extensions for BOINC and XSEDE infrastructures on the backend while offering interactive computations and analytical tools via Jupyter Notebooks, RStudio, and other web applications as publishing environments. The paper goes into detail for novel developments for the three use cases Purdue University Research Repository (PURR), nanoHUB and MyGeoHub. First, PURR has been extended to utilize the enhanced data storage service Data Depot and high-speed networks of the Purdue local campus infrastructure. Second, nanoHUB offers over 500 simulation tools and it has been enhanced with a novel caching system to explore the input parameter space for already computed results via BOINC. The third extension is concerned with builtin features for geospatial data and modeling in MyGeoHub that offers to execute compute-intensive tasks on XSEDE. The diverse extensions can be reused in various hubs developed with HUBzero® requiring such diverse features and accessing different distributed computing infrastructures."
Decoding the Human Brain Activity and Predicting the Visual Stimuli from Magnetoencephalography (MEG) Recordings,"Zarief, Christine Nasser and Hussein, Walid",10.1145/3332340.3332352,2019,"Decoding the human brain activity and predicting the visual stimuli is about retrieving and translating the signals of the human brain by applying the signal processing techniques in order to know the concurrent activities of the human brain. The topic has gotten the attention of many scientific communities in recent years as it has many medical applications. Magnetoencephalography is a non-invasive emerging medical technique and signal processing approach that utilizes the magnetic field of the human brain to decode its internal activities. Consequently, it is difficult to analyze and recognize how the human brain reacts to different visual stimulus by analyzing the signals of MEG without using the progressive techniques of signal processing such as classification. The features that are extracted from the human brain can be classified using different algorithms such as k-nearest neighbor (KNN), support vector machine (SVM), Naive Bayes (NB), and Artificial neural network (ANN) in order to finalize the process of decoding human brain. But, these methods did not produce satisfying results because of the great number of features. In this paper, the problem of classifying MEG signals is presented and Least Square Regression (LSR) classifier is proposed to classify the signals of MEG for two different types of the visual stimuli: face and scrambled face. Our experimental results show more accurate classification performance over other opposing methodologies."
Moroccan Patient-centered Hospital Information System: Global Architecture,"Fakhkhari, Houda and Bounabat, Bouchaib and Bennani, Maria and Bekkali, Rachid",10.1145/3333165.3333175,2019,"In Morocco, most Hospital Information System projects undertaken by the Moroccan health sector are technical-oriented and executed as independent projects, with a focus on the management of administrative and financial activities above patient needs and the importance of business process improvement. This paper aims to fill this gap by proposing a global architecture for a Moroccan 'Patient-centered Health Information System' and a method to deploy it. This proposal is based on national and international eHealth experiences and provides a clear and comprehensive view of what a Moroccan Health Information System should look like. This paper focuses on describing the main requirements that should be implemented to enhance the quality of Moroccan patient care."
Developing a Minimum Viable Product for Big Data and AI Education: Action Research Based on a Two-Year Reform of an Undergraduate Program of Internet and New Media,"Liao, Han-Teng and Wang, Zijia and Wu, Xue",10.1145/3335484.3335509,2019,"The advancement in Big Data and Artificial Intelligence has posed challenges and opportunities for undergraduate education. It raises several questions regarding what is desirable and viable for preparing undergraduate students for future success. This paper presents the rationales and outcomes of a two-year reform of an undergraduate program of Internet and New Media in China, summarizing the ways in which the curriculum design can incorporate Big Data and Artificial Intelligence education for future Internet product managers and HCI professionals. Using the notion of ""minimum viable products"" to frame the action research of education reform, it first describes the ways in which we identify the job market need for Internet product managers and HCI professionals, and explores the learning pathways, departing from the conventional Internet and New Media programs in China. It then documents the results of the minimum required changes to deliver such a viable learning product and the initial evidence from students that validates the designed learning product. The overall findings demonstrate the usefulness and challenges in preparing students for careers in a data-intensive or data-driven world."
Tailored information dashboards: A systematic mapping of the literature,"V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e} and Ther\'{o}n, Roberto",10.1145/3335595.3335628,2019,"Information dashboards are extremely useful tools to exploit knowledge. Dashboards enable users to reach insights and to identify patterns within data at-a-glance. However, dashboards present a series of characteristics and configurations that could not be optimal for every user, thus requiring the modification or variation of its features to fulfill specific user requirements. This variation process is usually referred to as customization, personalization or adaptation, depending on how this variation process is achieved. Given the great number of users and the exponential growth of data sources, tailoring an information dashboard is not a trivial task, as several solutions and configurations could arise. To analyze and understand the current state-of-the-art regarding tailored information dashboards, a systematic mapping has been performed. This mapping focus on answering questions regarding how existing dashboard solutions in the literature manage the customization, personalization and/or adaptation of its elements to produce tailored displays."
Industrial and Academic Software Product Line Research at SPLC: Perceptions of the Community,"Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny",10.1145/3336294.3336310,2019,"We present preliminary insights into the perception of researchers and practitioners of the software product line (SPL) community on previous, current, and future research efforts. We were particularly interested in up-and-coming and outdated topics and whether the views of academics and industry researchers differ. Also, we compared the views of the community with the results of an earlier literature survey published at SPLC 2018. We conducted a questionnaire-based survey with attendees of SPLC 2018. We received 33 responses (about a third of the attendees) from both, very experienced attendees and younger researchers, and from academics as well as industry researchers. We report preliminary findings regarding popular and unpopular SPL topics, topics requiring further work, and industry versus academic researchers' views. Differences between academic and industry researchers become visible only when analyzing comments on open questions. Most importantly, while topics popular among respondents are also popular in the literature, topics respondents think require further work have often already been well researched. We conclude that the SPL community needs to do a better job preserving and communicating existing knowledge and particularly also needs to widen its scope."
Game postmortems vs. developer Reddit AMAs: computational analysis of developer communication,"Lu, Chien and Peltonen, Jaakko and Nummenmaa, Timo",10.1145/3337722.3337727,2019,"Postmortems and Reddit Ask Me Anything (AMA) threads represent communications of game developers through two different channels about their game development experiences, culture, processes, and practices. We carry out a quantitative text mining based comprehensive analysis of online available postmortems and AMA threads from game developers over multiple years. We find and analyze underlying topics from the postmortems and AMAs as well as their variation among the data sources and over time. The analysis is done based on structural topic modeling, a probabilistic modeling technique for text mining. The extracted topics reveal differing and common interests as well as their evolution of prevalence over time in the two text sources. We have found that postmortems put more emphasis on detail-oriented development aspects as well as technically-oriented game design problems whereas AMAs feature a wider variety of discussion topics that are related to a more general game development process, game-play and game-play experience related game design. The prevalences of the topics also evolve differently over time in postmortems versus AMAs."
Research on Software Testing Technical Ability Training based on e-learning,"Shi, Yaqing and Huang, Song",10.1145/3338147.3338173,2019,"Software testing is an important part of Software Quality Assurance. Improving the testing technical ability of software testers is the goal of software testing teaching. Based on e-learning and CDIO education model, this paper proposes a test technical ability training mode integrating courseware resources, exercise environment and training activities, which changes the status of traditional software testing course teaching which emphasizes theory over practice. Teaching practice has proved that the new training method can improve students' interest in learning and effectively promote the training of students' ability of software testing."
Going big: a large-scale study on what big data developers ask,"Bagherzadeh, Mehdi and Khatchadourian, Raffi",10.1145/3338906.3338939,2019,"Software developers are increasingly required to write big data code. However, they find big data software development challenging. To help these developers it is necessary to understand big data topics that they are interested in and the difficulty of finding answers for questions in these topics. In this work, we conduct a large-scale study on Stackoverflow to understand the interest and difficulties of big data developers. To conduct the study, we develop a set of big data tags to extract big data posts from Stackoverflow; use topic modeling to group these posts into big data topics; group similar topics into categories to construct a topic hierarchy; analyze popularity and difficulty of topics and their correlations; and discuss implications of our findings for practice, research and education of big data software development and investigate their coincidence with the findings of previous work."
Bisecting commits and modeling commit risk during testing,"Najafi, Armin and Rigby, Peter C. and Shang, Weiyi",10.1145/3338906.3338944,2019,"Software testing is one of the costliest stages in the software development life cycle. One approach to reducing the test execution cost is to group changes and test them as a batch (i.e. batch testing). However, when tests fail in a batch, commits in the batch need to be re-tested to identify the cause of the failure, i.e. the culprit commit. The re-testing is typically done through bisection (i.e. a binary search through the commits in a batch). Intuitively, the effectiveness of batch testing highly depends on the size of the batch. Larger batches require fewer initial test runs, but have a higher chance of a test failure that can lead to expensive test re-runs to find the culprit. We are unaware of research that investigates and simulates the impact of batch sizes on the cost of testing in industry. In this work, we first conduct empirical studies on the effectiveness of batch testing in three large-scale industrial software systems at Ericsson. Using 9 months of testing data, we simulate batch sizes from 1 to 20 and find the most cost-effective BatchSize for each project. Our results show that batch testing saves 72% of test executions compared to testing each commit individually. In a second simulation, we incorporate flaky tests that pass and fail on the same commit as they are a significant source of additional test executions on large projects. We model the degree of flakiness for each project and find that test flakiness reduces the cost savings to 42%. In a third simulation, we guide bisection to reduce the likelihood of batch-testing failures. We model the riskiness of each commit in a batch using a bug model and a test execution history model. The risky commits are tested individually, while the less risky commits are tested in a single larger batch. Culprit predictions with our approach reduce test executions up to 9% compared to Ericsson’s current bisection approach. The results have been adopted by developers at Ericsson and a tool to guide bisection is in the process of being added to Ericsson’s continuous integration pipeline."
Predicting pull request completion time: a case study on large scale cloud services,"Maddila, Chandra and Bansal, Chetan and Nagappan, Nachiappan",10.1145/3338906.3340457,2019,"Effort estimation models have been long studied in software engineering research. Effort estimation models help organizations and individuals plan and track progress of their software projects and individual tasks to help plan delivery milestones better. Towards this end, there is a large body of work that has been done on effort estimation for projects but little work on an individual checkin (Pull Request) level. In this paper we present a methodology that provides effort estimates on individual developer check-ins which is displayed to developers to help them track their work items. Given the cloud development infrastructure pervasive in companies, it has enabled us to deploy our Pull Request Lifetime prediction system to several thousand developers across multiple software families. We observe from our deployment that the pull request lifetime prediction system conservatively helps save 44.61% of the developer time by accelerating Pull Requests to completion."
Helping developers search and locate task-relevant information in natural language documents,"Marques, Arthur",10.1145/3338906.3341459,2019,"While performing a task, software developers interact with a myriad of natural language documents. Not all information in these documents is relevant to a developer's task forcing them to filter relevant information from large amounts of irrelevant information. If a developer misses some of the necessary information for her task, she will have an incomplete or incorrect basis from which to complete the task. Many approaches mine relevant text fragments from natural language artifacts. However, existing approaches mine information for pre-defined tasks and from a restricted set of artifacts. I hypothesize that it is possible to design a more generalizable approach that can identify, for a particular task, relevant text across different artifact types establishing relationships between them and facilitating how developers search and locate task-relevant information. To investigate this hypothesis, I propose to match a developer's task to text fragments in natural language artifacts according to their semantics. By semantically matching textual pieces to a developer's task we aim to more precisely identify fragments relevant to a task. To help developers in thoroughly navigating through the identified fragments I also propose to synthesize and group them. Ultimately, this research aims to help developers make more informed decisions regarding their software development task. Dr. Gail C. Murphy supervises this work."
Machine-learning supported vulnerability detection in source code,"Sonnekalb, Tim",10.1145/3338906.3341466,2019,"The awareness of writing secure code rises with the increasing number of attacks and their resultant damage. But often, software developers are no security experts and vulnerabilities arise unconsciously during the development process. They use static analysis tools for bug detection, which often come with a high false positive rate. The developers, therefore, need a lot of resources to mind about all alarms, if they want to consistently take care of the security of their software project. We want to investigate, if machine learning techniques could point the user to the position of a security weak point in the source code with a higher accuracy than ordinary methods with static analysis. For this purpose, we focus on current machine learning on code approaches for our initial studies to evolve an efficient way for finding security-related software bugs. We will create a configuration interface to discover certain vulnerabilities, categorized in CWEs. We want to create a benchmark tool to compare existing source code representations and machine learning architectures for vulnerability detection and develop a customizable feature model. At the end of this PhD project, we want to have an easy-to-use vulnerability detection tool based on machine learning on code."
Costing Secure Software Development: A Systematic Mapping Study,"Venson, Elaine and Guo, Xiaomeng and Yan, Zidi and Boehm, Barry",10.1145/3339252.3339263,2019,"Building more secure software is a recent concern for software engineers due to increasing incidences of data breaches and other types of cyber attacks. However, software security, through the introduction of specialized practices in the software development life cycle, leads to an increase in the development cost. Although there are many studies on software cost models, few address the additional costs required to build secure software. We conducted a systematic review in the form of a mapping study to classify and analyze the literature related to the impact of security in software development costs. Our search strategy strove to achieve high completeness by the identification of a quasi-gold-standard set of papers, which we then used to establish a search string and retrieve papers from research databases automatically. The application of inclusion/exclusion criteria resulted in a final set of 54 papers, which were categorized according to the approach to software security cost analysis. Perform Security Review, Apply Threat Modeling, and Perform Security Testing were the three most frequent activities related to cost, and Common Criteria was the most applied standard. We also identified ten approaches to estimating software security costs for development projects; however, their validation remains a challenge, which could be addressed in future studies."
Aberrant Functional Connectivity Dynamics of Superior Temporal Sulcus and Its Associations with GABA Genes Expression in Autism,"Guo, Xiaonan and He, Changchun and Duan, Xujun and Han, Shaoqiang and Xiao, Jinming and Chen, Huafu",10.1145/3340037.3340049,2019,"Autism spectrum disorder (ASD) is associated with functional coordination disturbances among brain regions. Genetic studies implicated that dysfunctional gamma-aminobutyric acid (GABA) system may play an important role in autism etiology. Based on previous reported static functional connectivity abnormalities of the posterior superior temporal sulcus (pSTS) in ASD, the current study aimed to explore the dynamic functional connectivity (dFC) variability of the pSTS in ASD and its associations with GABA receptor genes expression. Resting-state functional magnetic resonance imaging data obtained from the Autism Brain Imaging Data Exchange repository were analyzed in 209 males with ASD and 298 demographically-matched control males. For each subject, dFC maps of the bilateral pSTS were constructed through Flexible Least Squares strategy, and the variance of the dFC time series at each voxel was further calculated to quantify the temporal variability. Finally, gene expression decoding analysis was performed using NeuroVault to associate the dFC variability abnormalities with expression data of the GABA receptor genes. Compared with typical controls, individuals with ASD showed increased dFC variability between the left pSTS and right temporal regions, including the middle temporal gyrus and fusiform gyrus. In addition, decreased dFC variability was found between the right pSTS and bilateral posterior cingulate cortex in ASD. Gene expression decoding result showed that aberrant dFC variability patterns of the bilateral pSTS were related to GABA receptor genes expression. These findings suggest abnormal dFC variability of the pSTS in ASD and implicate the potential links between these connectivity abnormalities and GABA genes expression."
Open knowledge interface: a digital assistant to support students in writing academic assignments,"Resch, Olaf and Yankova, Aglika",10.1145/3340435.3342723,2019,"In the course of their studies, the majority of students at German universities have to write a certain number of academic assignments as an essential part of the academic training in any degree program. This work presents some initial results of our research into the design, implementation and conceptual usage of OKI (Open Knowledge Interface) – a digital assistant intended to support students in writing academic assignments. The core aspects of assistance include project management, context-sensitive help in applying scientific methods and search in open access literature. OKI is a conversational chatbot running inside Telegram-Messenger and allowing an efficient mobile usage, and thus a flexible way of organizing users’ own time and workload. The Open Knowledge Interface project is funded by the German Federal Ministry of Education and Research within the Open Access Guideline, and runs from May 2018 to October 2019. This paper therefore is an attempt to describe some initial conceptual thoughts as well as preliminary results including first user experiences and a brief outlook on future work."
Customer requests matter: early stage software effort estimation using k-grams,"Eren, Kaz\i{}m K\i{}van\c{c} and Ozbey, Can and Eken, Beyza and Tosun, Ay\c{s}e",10.1145/3341105.3373898,2020,"Estimating the software development effort associated with a customer request, immediately after the request has been made, is quite challenging for project managers. Studies on software effort estimation often utilize expert knowledge to build statistical or machine learning models, although it is subjective and human-dependent. Rich text, natural language based descriptions provided by the customers are mostly not incorporated into the models during estimation.The aim of this study is to propose an early stage software effort estimation model that can predict the effort of the related problem immediately after a customer request or bug fix problem is appeared. Our model utilizes the textual descriptions of customer requests collected in the requirement management tool, and other features stored with the customer requests. Our results show that the use of textual data helps to make better predictions for an effort estimation system.Besides the effect of textual data, we asked whether there is a significant difference between the performance of an effort estimation system that can be used for all customers (Unified Model) and Customer Specific Models which is trained using only the related customer's requests. The Pred(25), Standard Accuracy (SA) and Gibbs' A were used during the evaluation. The Unified Model for early stage effort estimation achieves 43% pred(25) and 51% SA values. Customer Specific Models, on the other hand, depending on the customer, obtain 39%-58% Pred(25), and 35%-58% SA values. The results show that there is no significant advantage between the Unified Model and Customer Specific Models, and which model to use should be determined according to the customer."
VANET's Security Concerns and Solutions: A Systematic Literature Review,"Javed, Muhammad Noman and Shafiq, Hammad and Alam, Khubaib Amjad and Jamil, Abid and Sattar, Muhammad Umar",10.1145/3341325.3342028,2019,"Context: Vehicular Ad-hoc network (VANET) is an emerging trend that originates from the field of Mobile Adhoc Network that gained significant attention from the past few years. The communication is realized through the On-Board Unit (OBU) equipped in vehicles. The concern of VANET is to provide a safe journey to the passengers. Although its significant high mobility of vehicles may cause a loss in connection or security attacks. Objective: The aim behind systematic literature review (SLR) is to identify the diverse type of security attacks in VANET and their respective solutions to handle those attack. This study mainly focuses on VANET centric security attacks and categorizes them along with the solutions to handle these attacks. Method: By using kitchenham guideline for systematic review and PRISMA flow for study selection to ensure coverage of all relevant studies we initially came up with 403 articles. Among these studies, 84 studies were qualified to be relevant according to 4 defined research questions. After identification and categorization of VANET centric attacks, proposed solutions a mapping between attacks and solution is provided. Results: Result of this systematic literature review suggest that identifies Access Management is a major concern in VANET with 44% of selected studies followed by Privacy 28% and Sybil attack 5%. Similarly, numerous security concerns such as Message Authentication 3%, Trust Management 4% and Black Hole attack 3% receives considerably lesser attention by the research community. Conclusions: In the context of security-related solutions which technique found to be most widely used to resolve the problem. The contribution of this study is considered important because there is no such study that has provided such an extensive overview of VANET security attacks and their perspective solutions."
Fine-grained Fitting Experience Prediction: A 3D-slicing Attention Approach,"Huang, Shan and Wang, Zhi and Cui, Laizhong and Jiang, Yong and Gao, Rui",10.1145/3343031.3350851,2019,"The comfortableness of fashion items (e.g., footwear) when people actually wear them has become an increasingly important factor in today's fashion experience. However, existing solutions usually only provide general metrics, e.g., a size of a pair of shoes, for people to roughly infer the fitness possibility, failing to tell the details about how much it fits or why it does not fit a person. In this paper, we propose a fine-grained fitting experience prediction framework based on 3D shapes of both fashion items and people's bodies. First, we propose a 3D-slicing sampling method, by extracting a series of parallel slices from an object, to represent the spatial details of the object with a much smaller amount of features. Second, we propose a spatial self-attention based fitness prediction model including a sub-region attention method and a sequence attention method, which can capture users' comfortable preferences for fine-grained regions divided from slices. Our design can capture users' try-on preferences and landmark positions that may or may not fit (e.g., too tight or too loose). Then, we design a multi-position experience module to predict users' fitting experiences, which can help to explore the spatial differences among slices better. Finally, we use subjective experiments over 500 people trying 32 pairs of fashion shoes with detailed places' comfortableness reported in questionnaires to verify our design, which has accuracies of $77.7%$ and $80.9%$ in reporting the comfortableness of tightness and length respectively, and an overall fitness accuracy of $83.6%$."
1.5 Degrees of Separation: Computer Science Education in the Age of the Anthropocene,"Pollock, Ian and Alshaigy, Bedour and Bradley, Andrew and Krogstie, Birgit R. and Kumar, Viraj and Ott, Linda and Peters, Anne-Kathrin and Riedesel, Charles and Wallace, Charles",10.1145/3344429.3372500,2019,"Climate change is the defining challenge now facing our planet. Limiting global warming to 1.5 degrees, as advocated by the Intergovernmental Panel on Climate Change, requires rapid, far-reaching, and unprecedented changes in how governments, industries, and societies function by 2030. Computer Science plays an important role in these efforts, both in providing tools for greater understanding of climate science and in reducing the environmental costs of computing. It is vital for Computer Science students to understand how their chosen field can both exacerbate and mitigate the problem of climate change.We have reviewed the existing literature, interviewed leading experts, and held conversations at the ITiCSE 2019 conference, to identify how universities, departments, and CS educators can most effectively address climate change within Computer Science education. We find that the level of engagement with the issue is still low, and we discuss obstacles at the level of institutional, program and departmental support as well as faculty and student attitudes. We also report on successful efforts to date, and we identify responses, strategies, seed ideas, and resources to assist educators as they prepare their students for a world shaped by climate change."
An Empirical Approach to Understanding Data Science and Engineering Education,"Raj, Rajendra K. and Parrish, Allen and Impagliazzo, John and Romanowski, Carol J. and Aly, Sherif G. and Bennett, Casey C. and Davis, Karen C. and McGettrick, Andrew and Pereira, Teresa Susana Mendes and Sundin, Lovisa",10.1145/3344429.3372503,2019,"As data science is an evolving field, existing definitions reflect this uncertainty with overloaded terms and inconsistency. As a result of the field's fluidity, there is often a mismatch between what data-related programs teach, what employers expect, and the actual tasks data scientists are performing. In addition, the tools available to data scientists are not necessarily the tools being taught; textbooks do not seem to meet curricular needs; and empirical evidence does not seem to support existing program design. Currently, the field appears to be bifurcating into data science (DS) and data engineering (DE), with specific but overlapping roles in the combined data science and engineering (DSE) lifecycle. However, curriculum design has not yet caught up to this evolution. This working group report shows an empirical and data-driven view of the data-related education landscape, and includes several recommendations for both academia and industry that are based on this analysis."
An Evaluation of Parameter Pruning Approaches for Software Estimation,"Tran, Thu and Nguyen, Vu and Truong, Thong and Tran, Chi and Le, Phu",10.1145/3345629.3345633,2019,"Model-based estimation often uses impact factors and historical data to predict the effort of new projects. Estimation accuracy of this approach is highly dependent on how well impact factors are selected. This paper comparatively assesses six methods for prune parameters of effort estimation models, including Stepwise regression, Lasso, constrained regression, GRASP, Tabu search, and PCA. Four data sets were used for evaluation, showing that estimation accuracy varies among the methods but no method consistently outperforms the rest. Stepwise regression prunes estimation model parameters the most while it does not sacrifice much estimation performance. Our study provides further evidence to support the use of Stepwise regression for selecting factors in effort estimation."
Does chronology matter in JIT defect prediction? A Partial Replication Study,"Jahanshahi, Hadi and Jothimani, Dhanya and Ba\c{s}ar, Ay\c{s}e and Cevik, Mucahit",10.1145/3345629.3351449,2019,"BACKGROUND: Just-In-Time (JIT) models, unlike the traditional defect prediction models, detect the fix-inducing changes (or defect inducing changes). These models are designed based on the assumption that past code change properties are similar to future ones. However, as the system evolves, the expertise of developers and/or the complexity of the system also change.AIM: In this work, we aim to investigate the effect of code change properties on JIT models over time. We also study the impact of using recent data as well as all available data on the performance of JIT models. Further, we analyze the effect of weighted sampling on the performance of fix-inducing properties of JIT models. For this purpose, we used datasets from four open-source projects, namely Eclipse JDT, Mozilla, Eclipse Platform, and PostgreSQL.METHOD: We used five families of change code properties such as size, diffusion, history, experience, and purpose. We used Random Forest to train and test the JIT model and Brier Score (BS) and Area Under Curve (AUC) for performance measurement. We applied the Wilcoxon Signed Rank Test on the output to statistically validate whether the performance of JIT models improves using all the available data or the recent data.RESULTS: Our paper suggest that the predictive power of JIT models does not change by time. Furthermore, we observed that the chronology of data in JIT defect prediction models can be discarded by considering all the available data. On the other hand, the importance score of families of code change properties is found to oscillate over time.CONCLUSION: To mitigate the impact of the evolution of code change properties, it is recommended to use weighted sampling approach in which more emphasis is placed upon the changes occurring closer to the current time. Moreover, since properties such as ""Expertise of the Developer"" and ""Size"" evolve with the time, the models obtained from old data may exhibit different characteristics compared to those employing the newer dataset. Hence, practitioners should constantly retrain JIT models to include fresh data."
Robotic Process Automation and Opportunities for Vietnamese Market,"Van Chuong, Le and Hung, Phan Duy and Diep, Vu Thu",10.1145/3348445.3348458,2019,"The Industrial Revolution 4.0 is a trend that has a strong impact on all aspect of socio-economic life in almost all human-related fields as finance, banking, manufacturing, back-office, etc. Robotic Process Automation (RPA) is one of the breakthrough solutions. A key advantage of RPA is that unlike previous IT transformations such as Enterprise Resource Planning (ERP's), RPA does not require a massive upfront investment or a significant change to the current IT systems and processes. In fact, RPA can be implemented relatively quickly when compared to previous digital transformations, as it requires minimal capital or infrastructure. RPA can act as an additional employee that can work between the IT systems and with the back-office processes in various functions. Similarly, to humans, RPA can learn from people and copy their processes, eventually taking over the processes that humans once completed, at a much faster pace. RPA is being more and more developed to work with increasingly complex processes and tasks. In this paper we analyze opportunities of applying RPA to the Vietnam market and fields which RPA can be applied."
Research on Secure Stereoscopic Self-Checking Scheme for Open Source Software,"Zou, Jing and Zeng, Wenjing and Zhao, Yuhang and Liang, Ruigang and Cai, Lijun and Zhao, Yuliang",10.1145/3349341.3349395,2019,"In view of the security problems brought by the use of open source software, we introduce risk management mechanism, risk assessment and risk management mechanism in all aspects of open source software project development life cycle, and then design advanced open source software development security self-checking scheme for open source software. Safety prevention and control measures provide a theoretical basis. According to the existing research process, models and methods, from the perspective of technical management, the self-inspection schemes required for project establishment, R&amp;D and operation phases are designed. Design self-checking solutions for different open source software applications to ensure the security and compliance of projects using open source software applications."
Towards Issue Recommendation for Open Source Communities,"Samer, Ralph and Felfernig, Alexander and Stettinger, Martin",10.1145/3350546.3352514,2019,"In open source software development, a major challenge is the prioritization of new requirements as well as the identification of responsible developers for their implementation. Unlike conventional industrial software development, where requirements engineers have to explicitly define who implements what, in the context of open source development, developers (contributors) usually decide on their own which requirements to implement next. Contributors have to deal with a huge number of requirements where the recognition of the most relevant ones often becomes a crucial task with a high impact on the success of a software project. This fact defines our major motivation for the development of a prioritization tool for the Eclipse community which recommends relevant requirements (issues/bugs) to open source developers. Our tool uses real-world data from Eclipse in order to build a prediction model. We trained and tested our tool with different classifiers such as Naive Bayes (representing our baseline), Decision Tree, and Random Forest. The evaluation results indicate that the Random Forest classifier correctly predicts issues with a precision of 0.88 (F1-score 0.68)."
Software Engineering Practices in the development of applications for Smart Cities: An Experience Report of Teaching in a Contemporary Context,"Viana, Davi and de Oliveira Rosa, Thatiane and Silva, Francisco and Durans, Pablo and Arag\~{a}o, Alexandre and Kon, Fabio and Goldman, Alfredo",10.1145/3350768.3351801,2019,"The heterogeneity of contemporary systems has turned the Software Engineering (SE) area even more challenging, since it is necessary to identify practices that are more adequate according to the technologies and context. Thus, practitioners need to be better prepared for these new systems. The area of Smart Cities (SC) is emerging and presents new challenges in conceptual, technical and academical degrees. This paper presents an experience report of SE practices in the development of SC applications. Five teams used the SC platform and developed their projects, creating software documentation, while applying agile practices. As a result, we identified that some UML diagrams were not adequate to model specific SC's aspects and microservices. Furthermore, we identified that there was a low application of the proposed agile practices, such as pair programming and daily meetings. Despite being the first time this class was taught, we identified gaps that need to be investigated in order to identify which SE practices are more adequate for such context."
Masters of the Process: A Board Game Proposal for Teaching Software Management and Software Development Process,"Sarinho, Victor Travassos",10.1145/3350768.3352459,2019,"Software Engineering (SE) education has a current challenge to provide sufficient hands-on experience for their students. Game-based learning represents a promising alternative to teach computing in higher education, promoting an ""active learning"" and serving as entertaining means for drill and practice. This paper presents ""Masters of the Process"", a board game proposal for teaching software management and software development process competences. For this, the design, construction and evaluation steps of the proposed game are described, as well as the usability results together with a brief comparison among SE board games. As a result, a simple, interactive and multiplayer game was provided, able to teach important SE concepts in a practical, competitive and funny way."
Testing Techniques Selection: A Systematic Mapping Study,"Santos, Italo and Melo, Silvana M. and de Souza, Paulo Sergio Lopes and Souza, Simone R. S.",10.1145/3350768.3352571,2019,"[Context] Software projects must consider the selection of testing techniques and criteria during their life cycles. This practice increases the chances of testing activity to be appropriately performed. In a previous work, an infrastructure to support the selection of testing techniques was proposed for the context of concurrent software. This infrastructure considers information (attributes) of the project to make the selection closer to the testers need. [Objective] This paper extends the previous work by identifying new studies concerning testing techniques selection, project attributes that can be used for this selection and which approaches can be employed to support the combined selection. [Method] A mapping study was conducted and a total of 15 primary studies, published in the last 20 years were selected. Information about approaches to testing techniques selection was analyzed and classified. [Results] The following results were obtained: (i) existing approaches for selection of testing techniques; (ii) proposition of a taxonomy of selection approaches; (iii) characterization of attributes to offer the support for the selection of a testing technique; and (iv) identification of approaches that perform combined selection of testing techniques. [Conclusion] Combining testing techniques is essential to improve the testing activity quality by finding different failure categories and supplementing other techniques limitations. This paper describes an initiative to offer support for the construction of new or combined strategies for the selection of testing techniques capable of being used in practice."
What to account for when accounting for algorithms: a systematic literature review on algorithmic accountability,"Wieringa, Maranke",10.1145/3351095.3372833,2020,"As research on algorithms and their impact proliferates, so do calls for scrutiny/accountability of algorithms. A systematic review of the work that has been done in the field of 'algorithmic accountability' has so far been lacking. This contribution puts forth such a systematic review, following the PRISMA statement. 242 English articles from the period 2008 up to and including 2018 were collected and extracted from Web of Science and SCOPUS, using a recursive query design coupled with computational methods. The 242 articles were prioritized and ordered using affinity mapping, resulting in 93 'core articles' which are presented in this contribution. The recursive search strategy made it possible to look beyond the term 'algorithmic accountability'. That is, the query also included terms closely connected to the theme (e.g. ethics and AI, regulation of algorithms). This approach allows for a perspective not just from critical algorithm studies, but an interdisciplinary overview drawing on material from data studies to law, and from computer science to governance studies. To structure the material, Bovens's widely accepted definition of accountability serves as a focal point. The material is analyzed on the five points Bovens identified as integral to accountability: its arguments on (1) the actor, (2) the forum, (3) the relationship between the two, (3) the content and criteria of the account, and finally (5) the consequences which may result from the account. The review makes three contributions. First, an integration of accountability theory in the algorithmic accountability discussion. Second, a cross-sectoral overview of the that same discussion viewed in light of accountability theory which pays extra attention to accountability risks in algorithmic systems. Lastly, it provides a definition of algorithmic accountability based on accountability theory and algorithmic accountability literature."
Data Science Competency in Organisations: A Systematic Review and Unified Model,"Hattingh, Mari\'{e} and Marshall, Linda and Holmner, Marlene and Naidoo, Rennie",10.1145/3351108.3351110,2019,"The paper presents a systematic literature review of the literature on the competencies that are essential to develop a globally competitive workforce in the field of data science. The systematic review covers a wide range of literature but focuses primarily, but not exclusively, on the computing, information systems, management, and organisation science literature. The paper uses a broad research search strategy covering four separate electronic databases. The search strategy led the researchers to scan 139 titles, abstracts and keywords. Sixty potentially relevant articles were identified, of which 42 met the quality criteria and contributed to the analysis. A critical appraisal checklist assessed the validity of each empirical study. The researchers grouped the findings under six broad competency themes: organisational, technical, analytical, ethical and regulatory, cognitive and social. Thematic analysis was used to develop a unified model of data science competency based on the evidence of the findings. This model will be applied to case studies and survey research in future studies. A unified data science competency model, supported by empirical evidence, is crucial in closing the skills gap, thereby improving the quality and competitiveness of the South Africa's data science workforce. Researchers are encouraged to contribute to the further conceptual development of data science competency."
A Supra-modal Decoding Mechanism: Evidence from Chinese Speakers Learning English,"Wan, Youhui and Don, Lanqing and Da, Weihui",10.1145/3357160.3357671,2019,"Multimodal information interaction is a common way of communication involved in language teaching. The study on the neural mechanism in the process of language information processing will be helpful to make better use of the above way. This research investigated the supra-modal network when Chinese subjects processed English interpretation, and found that the recruited subjects capitalized on the occipital lobe and frontal lobe to achieve semantic processing for both visual and auditory modules. For these Chinese subjects, reading activation tended to be occipital-lobe focused, while in previous models (Models of Wernicke-Geschwind), subjects (native/ nearly native with recruited languages) heavily relied on the temporal lobe. This research indicated that the recruited late bilingual subjects adopted a different network to decode language signals, opposed to models in the previous literature. Moreover, the occipital lobe remained crucial for the recruited Chinese bilinguals to process materials with high levels of difficulty. For learners of similar backgrounds, visualization shall be emphasized through multimodal information interactions during future language instructions, as well as in interpretation teaching."
A Novel Framework for Change Requirement Management (CRM) In Agile Software Development (ASD),"Shehzadi, Zainab and Azam, Farooque and Anwar, Muhammad Waseem and Qasim, Iqra",10.1145/3357419.3357438,2019,"Requirements changes play a huge role in Software Development Life Cycle (SDLC). Therefore, the change requirement management (CRM) is always considered an important activity in SDLC. Changing one requirement have a major impact on other requirements especially when teams are working from geographically different locations then it's become difficult to manage, categorize, organize and track requirement changes therefore CRM is considered as difficult task in SDLC especially in agile methodology. This paper review different research papers and identify the major problems associated with each existing framework such as time complexity, quick cost estimation, verification of implemented and unimplemented requirements and customer notification. On this basis of identified problems this paper proposed a framework that provides a way how to categorize requirements, track implemented/unimplemented requirements, Save/Update requirements in repository and provide notification to stakeholder about current status of requirement. Proposed framework facilitates organizations to manage changing requirements in agile software development (ASD) and reduces overall complexity and traceability issues of RCM."
The Framework of Government Cloud Computing Adoption with TAM in Thailand,"Buavirat, Warune and Kreesuradej, Worapoj and Chaveesuk, Singha",10.1145/3357419.3357458,2019,"Cloud technology/ Cloud Computing had gained significance and popularity in the recent years with the increase in the internet reliability, even increasing access speed and the need for large storage requirements by various users. The cloud technology has been widely used in the various business industries for many years such as data storage, analytics, data management, connecting network, distributed working etc. Cloud computing offer its series of benefits to many business industries, organizations including at government level. The implication of Cloud technology in the government will help to get information in real-time, improve work process with higher efficiency and effectiveness. E-government also known as G-Cloud is the kind of cloud system, which provides user interface to the Government systems for its citizens. The influx of innovation had also brought up many disruptions to many businesses in the industry. Organizations have been involved in the research to overcome such challenges and aims to extend adoption and technology fit with better clarity while overcoming the disruptions that have been faced by its citizens from the government departments and why it's necessary for adopting this technology innovation by their particular organization. This study based on technology acceptance model 3 and task-technology fit model."
"Local-first software: you own your data, in spite of the cloud","Kleppmann, Martin and Wiggins, Adam and van Hardenberg, Peter and McGranaghan, Mark",10.1145/3359591.3359737,2019,"Cloud apps like Google Docs and Trello are popular because they enable real-time collaboration with colleagues, and they make it easy for us to access our work from all of our devices. However, by centralizing data storage on servers, cloud apps also take away ownership and agency from users. If a service shuts down, the software stops functioning, and data created with that software is lost. In this article we propose local-first software, a set of principles for software that enables both collaboration and ownership for users. Local-first ideals include the ability to work offline and collaborate across multiple devices, while also improving the security, privacy, long-term preservation, and user control of data. We survey existing approaches to data storage and sharing, ranging from email attachments to web apps to Firebase-backed mobile apps, and we examine the trade-offs of each. We look at Conflict-free Replicated Data Types (CRDTs): data structures that are multi-user from the ground up while also being fundamentally local and private. CRDTs have the potential to be a foundational technology for realizing local-first software. We share some of our findings from developing local-first software prototypes at the Ink &amp; Switch research lab over the course of several years. These experiments test the viability of CRDTs in practice, and explore the user interface challenges for this new data model. Lastly, we suggest some next steps for moving towards local-first software: for researchers, for app developers, and a startup opportunity for entrepreneurs."
Software process anti-pattern detection in project data,"Picha, Petr and Brada, Premek",10.1145/3361149.3361169,2019,"There is a significant amount of guidance on Project Management (PM) including software development methodologies, best practices and anti-patterns (APs). There is, however, a lack of automated way of applying this knowledge by analyzing readily available data from tools aiding in software PM, such as Application Lifecycle Management (ALM) tools. We propose a method of detecting process and PM anti-patterns in project data which can be used to warn software development teams about a potential threat to the project, or to conduct more general studies on the impact of AP occurrence on project success and product quality. We previously published a concept for the data mining and analysis toolset distinct from other research approaches and related work. Based on this toolset, we devised a formalized basis for our detection method in the form of standardized AP description template and a model for pattern operationalization over project data extracted from ALM tools. The main contribution of this paper is the general method for AP operationalization taking the description template as a starting point, discussed together with its potential limitations. We performed an initial validation of the method on data from student projects, using an AP we encountered in practice called ""Collective Procrastination"" which we also describe in this paper together with its detailed formal operationalization."
JLLAR: A Logging Recommendation Plug-in Tool for Java,"Zhu, Jing and Rong, Guoping and Huang, Guocheng and Gu, Shenghui and Zhang, He and Shao, Dong",10.1145/3361242.3361261,2019,"Logs are the execution results of logging statements in software systems after being triggered by various events, which is able to capture the dynamic behavior of software systems during runtime and provide important information for software analysis, e.g., issue tracking, performance monitoring, etc. Obviously, to meet this purpose, the quality of the logs is critical, which requires appropriately placement of logging statements. Existing research on this topic reveals that where to log? and what to log? are two most concerns when conducting logging practice in software development, which mainly relies on developers' personal skills, expertise and preference, rendering several problems impacting the quality of the logs inevitably. One of the reasons leading to this phenomenon might be that several recognized best practices(strategies as well) are easily neglected by software developers. Especially in those software projects with relatively large number of participants. To address this issue, we designed and implemented a plug-in tool (i.e., JLLAR) based on the Intellij IDEA, which applied machine learning technology to identify and create a set of rules reflecting commonly recognized logging practices. Based on this rule set, JLLAR can be used to scan existing source code to identify issues regarding the placement of logging statements. Moreover, JLLAR also provides automatic code completion and semi code completion (i.e., to provide recommendations) regarding logging practice to support software developers during coding."
Automatic generation of software interfaces for supporting decision-making processes. An application of domain engineering and machine learning,"V\'{a}zquez-Ingelmo, Andrea and Garc\'{\i}a-Pe\~{n}alvo, Francisco J. and Ther\'{o}n, Roberto",10.1145/3362789.3362923,2019,"Information dashboards are sophisticated tools. Although they enable users to reach useful insights and support their decision-making challenges, a good design process is essential to obtain powerful tools. Users need to be part of these design processes, as they will be the consumers of the information displayed. But users are very diverse and can have different goals, beliefs, preferences, etc., and creating a new dashboard for each potential user is not viable. There exist several tools that allow users to configure their displays without requiring programming skills. However, users might not exactly know what they want to visualize or explore, also becoming the configuration process a tedious task. This research project aims to explore the automatic generation of user interfaces for supporting these decision-making processes. To tackle these challenges, a domain engineering, and machine learning approach is taken. The main goal is to automatize the design process of dashboards by learning from the context, including the end-users and the target data to be displayed."
Literature review on the theory of color and its relationship with moods in older people,"Giraldo, F\'{a}ber D. and Casta\~{n}o, Esteban M. and Giraldo, Sebasti\'{a}n and Mej\'{\i}a, Sebasti\'{a}n",10.1145/3364138.3364144,2020,Moods represent one of the elements keys that help determine the behavior of people in their everyday life. The objective of this research is to define a theoretical basis that allows to relate moods with colors using color theory. Through color theory could be assign meaning for each of the colors and highlight the great influence of the same in the moods of the people. The research questions of this project are: 1) is it possible to apply Machine Learning techniques that relate the color theory with moods in older people? 2) What are the most relevant considerations when the Machine Learning is applied to the color theory? This theoretical basis will be used to design a software tool that allow the correlation of colors with moods in older people and make predictions about those moods.
Using Machine Learning Technique for Effort Estimation in Software Development,"Amaral, Weldson and Rivero, Luis and Junior, Geraldo Braz and Viana, Davi",10.1145/3364641.3364670,2019,"Estimates in software projects aim to help practitioners predict more realistic values on software development, impacting the quality of software process activities regarding planning and execution. However, software companies have difficulties when carrying out estimations that represent adequately the real effort needed to execute the software project activities. Although, the literature presents techniques to estimate effort, this activity remains complex. Recently, Machine Learning (ML) techniques are been applied to solve this problem. Through ML techniques it is possible to use databases of finished projects (datasets) to help get more precisely estimations. This research aims to propose a methodology to estimate effort using a ML technique based on decision trees: XGBoost. To evaluate our methodology, we conducted tests with four datasets using two metrics: Mean Magnitude Relative Error and Prediction(25). The preliminary results show consistent results for this methodology for software effort estimation based on the employed metrics, which indicates that our methodology is promising. As further work, new datasets must be analyzed using our methodology, and also an approach using synthetic data to improve the ML training."
Personalized Employee Training Course Recommendation with Career Development Awareness,"Wang, Chao and Zhu, Hengshu and Zhu, Chen and Zhang, Xi and Chen, Enhong and Xiong, Hui",10.1145/3366423.3380236,2020,"As a major component of strategic talent management, learning and development (L&amp;D) aims at improving the individual and organization performances through planning tailored training for employees to increase and improve their skills and knowledge. While many companies have developed the learning management systems (LMSs) for facilitating the online training of employees, a long-standing important issue is how to achieve personalized training recommendations with the consideration of their needs for future career development. To this end, in this paper, we propose an explainable personalized online course recommender system for enhancing employee training and development. A unique perspective of our system is to jointly model both the employees’ current competencies and their career development preferences in an explainable way. Specifically, the recommender system is based on a novel end-to-end hierarchical framework, namely Demand-aware Collaborative Bayesian Variational Network (DCBVN). In DCBVN, we first extract the latent interpretable representations of the employees’ competencies from their skill profiles with autoencoding variational inference based topic modeling. Then, we develop an effective demand recognition mechanism for learning the personal demands of career development for employees. In particular, all the above processes are integrated into a unified Bayesian inference view for obtaining both accurate and explainable recommendations. Finally, extensive experimental results on real-world data clearly demonstrate the effectiveness and the interpretability of DCBVN, as well as its robustness on sparse and cold-start scenarios."
Questions for data scientists in software engineering: a replication,"Huijgens, Hennie and Rastogi, Ayushi and Mulders, Ernst and Gousios, Georgios and Deursen, Arie van",10.1145/3368089.3409717,2020,"In 2014, a Microsoft study investigated the sort of questions that data science applied to software engineering should answer. This resulted in 145 questions that developers considered relevant for data scientists to answer, thus providing a research agenda to the community. Fast forward to five years, no further studies investigated whether the questions from the software engineers at Microsoft hold for other software companies, including software-intensive companies with different primary focus (to which we refer as software-defined enterprises). Furthermore, it is not evident that the problems identified five years ago are still applicable, given the technological advances in software engineering. This paper presents a study at ING, a software-defined enterprise in banking in which over 15,000 IT staff provides in-house software solutions. This paper presents a comprehensive guide of questions for data scientists selected from the previous study at Microsoft along with our current work at ING. We replicated the original Microsoft study at ING, looking for questions that impact both software companies and software-defined enterprises and continue to impact software engineering. We also add new questions that emerged from differences in the context of the two companies and the five years gap in between. Our results show that software engineering questions for data scientists in the software-defined enterprise are largely similar to the software company, albeit with exceptions. We hope that the software engineering research community builds on the new list of questions to create a useful body of knowledge."
Robotics software engineering: a perspective from the service robotics domain,"Garc\'{\i}a, Sergio and Str\""{u}ber, Daniel and Brugali, Davide and Berger, Thorsten and Pelliccione, Patrizio",10.1145/3368089.3409743,2020,"Robots that support humans by performing useful tasks (a.k.a., service robots) are booming worldwide. In contrast to industrial robots, the development of service robots comes with severe software engineering challenges, since they require high levels of robustness and autonomy to operate in highly heterogeneous environments. As a domain with critical safety implications, service robotics faces a need for sound software development practices. In this paper, we present the first large-scale empirical study to assess the state of the art and practice of robotics software engineering. We conducted 18 semi-structured interviews with industrial practitioners working in 15 companies from 9 different countries and a survey with 156 respondents from 26 countries from the robotics domain. Our results provide a comprehensive picture of (i) the practices applied by robotics industrial and academic practitioners, including processes, paradigms, languages, tools, frameworks, and reuse practices, (ii) the distinguishing characteristics of robotics software engineering, and (iii) recurrent challenges usually faced, together with adopted solutions. The paper concludes by discussing observations, derived hypotheses, and proposed actions for researchers and practitioners."
A comprehensive study on challenges in deploying deep learning based software,"Chen, Zhenpeng and Cao, Yanbin and Liu, Yuanqiang and Wang, Haoyu and Xie, Tao and Liu, Xuanzhe",10.1145/3368089.3409759,2020,"Deep learning (DL) becomes increasingly pervasive, being used in a wide range of software applications. These software applications, named as DL based software (in short as DL software), integrate DL models trained using a large data corpus with DL programs written based on DL frameworks such as TensorFlow and Keras. A DL program encodes the network structure of a desirable DL model and the process by which the model is trained using the training data. To help developers of DL software meet the new challenges posed by DL, enormous research efforts in software engineering have been devoted. Existing studies focus on the development of DL software and extensively analyze faults in DL programs. However, the deployment of DL software has not been comprehensively studied. To fill this knowledge gap, this paper presents a comprehensive study on understanding challenges in deploying DL software. We mine and analyze 3,023 relevant posts from Stack Overflow, a popular Q&amp;A website for developers, and show the increasing popularity and high difficulty of DL software deployment among developers. We build a taxonomy of specific challenges encountered by developers in the process of DL software deployment through manual inspection of 769 sampled posts and report a series of actionable implications for researchers, developers, and DL framework vendors."
Learning to extract transaction function from requirements: an industrial case on financial software,"Shi, Lin and Li, Mingyang and Xing, Mingzhe and Wang, Yawen and Wang, Qing and Peng, Xinhua and Liao, Weimin and Pi, Guizhen and Wang, Haiqing",10.1145/3368089.3417053,2020,"In practice, it is very important to determine the size of a proposed software system yet to be built based on its requirements, i.e., early in the development life cycle. The most widely used approach for size estimation is Function Point Analysis (FPA). However, since FPA involves human judgment, the estimation results are some degree of subjective, and the process is labor and cost intensive. In this paper, we propose a novel approach to identify transaction functions from textual requirements automatically by leveraging a set of natural language processing techniques and machine learning models. We evaluate our approach on 1,864 requirements and 104,691 transaction functions taken from 36 financial projects from one banking industry. The results show that the contents of the suggested transaction functions by our approach are high in quality, with low perplexity value of 8.5 and high BLEU score of 34 on average. The types of suggested transaction functions can also be accurately classified, with overall accuracy of 0.99 on average. Our approach can provide reasonable suggestions that assist industrial practitioners to identify transaction functions faster and easier."
"P12 Computing in Italy, England and Alabama, USA","Maiorana, Francesco and Csizmadia, Andrew P. and Richards, Gretchen M.",10.1145/3368308.3415376,2020,"It is recognized that Computing requires many competencies covering a wide range of domains exhibiting an impressive changing rate. This paper examines three primary competencies, which are: 1) Algorithms, Programming, Data, and Computational Thinking (CT); 2) Networks, Internet and Security; and 3) Ethics. Due to the standards and relationship of competences, the authors categorized, algorithms, programming, data and CT together. Thus, we leveraged national guidelines in Italy, mandatory national computing curriculum in England, and state guidelines in Alabama, USA in conjunction with ACM computing frameworks. The primary lessons learned from comparing computing curricula, and implications for implementing computing education curriculum are discussed."
Evaluating Student Participation in Undergraduate Information Technology Programs in the U.S.,"Sabin, Mihaela and Zweben, Stuart and Lunt, Barry and Raj, Rajendra K.",10.1145/3368308.3415396,2020,"Enrollment, retention, and graduation rates of undergraduate students in Information Technology (IT) programs are useful measures of institutional performance. Disaggregated by demographics characteristics, such as gender, race, and ethnicity, analysis of student data across IT programs in the U.S. supports the exploration of the breadth and diversity of student participation in IT. Evaluating undergraduate IT programs is particularly challenging for multiple factors, including: IT programs are not always titled ""Information Technology""; IT programs are not always ABET-accredited; and IT programs may be housed in various academic units, such as business, computing, engineering, technology, or information sciences.This paper builds on prior work used to identify IT programs in the U.S., including the National Center for Education Statistics' Classification of Instructional Programs (CIP) codes, specifically CIP code 11 that designates IT and other computing programs. It also refines CIP code-based program identification and then analyzes 2017-2018 student data from the National Student Clearinghouse Research Center to evaluate IT programs through a student participation lens. The in-depth analysis of student enrollment, retention, and graduation is intended to support IT programs with designing more inclusive learning environments that increase participation of all students, in particular women and racial and ethnic minority students. This paper finally signals the importance of CIP codes that designate IT programs and focuses attention to the role that faculty, IT programs, and SIGITE community at large need to have in CIP code selection to further advance research in IT education."
Bug types fixed by API-migration: a case study,"Meqdadi, Omar and Aljawarneh, Shadi",10.1145/3368691.3368693,2019,"It is broadly known that adaptive maintenance improves software quality by making it accessing to new features and technology that may be included in the new version of an underlying API. This work presents a case study of six large C++ open source systems to investigate the role of API-migrations during bug-fixing activities. The case study involves mining the adaptive commits that were bug fixing revisions and then categorizing the relevant fixed bugs into a set of defined kinds. The study shows that only eight kinds covered 78.95% to 88.89% of the total fixed bugs in the examined systems. Also, the frequency of occurrence of each uncovered kind has been computed for each examined system. Correlation analysis results show that the uncovered kinds have very similar frequencies across all examined systems. Overall, the undertaken study illustrates the true benefits of an adaptive maintenance in the process of bug fixing."
Risk Management for Agile Projects in Offshore Vietnam,"Cuong, Le Gia and Hung, Phan Duy and Bach, Nguyen Luu and Tung, Ta Duc",10.1145/3368926.3369718,2019,"With Agile Development increasingly becoming ""the norm"" in software industry, experienced Project Managers from a hierarchical, 14,000-employee Company often find themselves facing difficulties in adapting their Risk Management practices into Agile projects. While Risk Management in an Agile offshore has been explored in various studies, these works do not offer a direct explanation on why certain Company had placed an emphasis on Risk Management, and how this emphasis has led to difficulties in the Agile era. From this explanation, the paper will propose a model to more seamlessly integrate Risk Management into Agile projects. Empirical results from case studies will also be provided to prove the effectiveness of the model. Lastly, a reference Risk Register (Risks and their Responses) will be provided for Project Managers to use as a starting point in their project."
Knowledge Graph based Automated Generation of Test Cases in Software Engineering,"Nayak, Anmol and Kesri, Vaibhav and Dubey, Rahul Kumar",10.1145/3371158.3371202,2020,"Knowledge Graph (KG) is extremely efficient in storing and retrieving information from data that contains complex relationships between entities. Such a representation is relevant in software engineering projects, which contain large amounts of inter-dependencies between classes, modules, functions etc. In this paper, we propose a methodology to create a KG from software engineering documents that will be used for automated generation of test cases from natural (domain) language requirement statements. We propose a KG creation tool that includes a novel Constituency Parse Tree (CPT) based path finding algorithm for test intent extraction, Conditional Random field (CRF) based Named Entity Recognition (NER) model with automatic feature engineering and a Sentence vector embedding based signal extraction. This paper demonstrates the contributions on an automotive domain software project."
Technology for determining strategic directions for the development of a regional transport and logistics system under digitalization,"Bulatova, Nadezhda and Dugina, Evdokia and Dorzhieva, Elena and Siniavina, Maria",10.1145/3372177.3373353,2020,"The paper is concerned with the development of a regional transport and logistics system under digitalization. The paper proposes a technology for determining strategic directions for regional transport and logistics system (RTLS) development under digitalization which comprises methodological tools for analyzing the spatial structure of freight consumption and freight generation by all the participants in a transportation service system along with an algorithm and a guidance package for arranging modes for the information interaction of its participants under the control of the single information provider, and their transformation, making it possible to ensure product and material exchanges at different levels: regional, interregional and intercountry. The concepts and stipulations of this technology are considered, drawing on the properties of a transport and logistics system as an economic system along with such principles of information provision as the comprehensiveness, divisibility, interrelatedness, orderliness, integrability, complexity, emergent nature and structuredeness of its elements.The objective of the given study is to create a technology for determining strategic directions for RTLS development under digitalization. The study intends to broaden the RTLS research domain, which makes it possible to expand theoretical knowledge of the current trends in its operation in the frame of digitalization. The general scientific methods of inquiry such as observation, analysis, generalization were used to reach the stated objective.Pursuing strategic directions for the development of a transport and logistics system within the framework of the proposed projects will allow for establishing an efficient program/project management system for area development and eliminating regional economic growth limitations related to the lack of harmonization among the participants in transportation processes."
Intelligent Task Recognition: Towards Enabling Productivity Assistance in Daily Life,"Liono, Jonathan and Rahaman, Mohammad Saiedur and Salim, Flora D. and Ren, Yongli and Spina, Damiano and Scholer, Falk and Trippas, Johanne R. and Sanderson, Mark and Bennett, Paul N. and White, Ryen W.",10.1145/3372278.3390703,2020,"We introduce the novel research problem of task recognition in daily life. We recognize tasks such as project management, planning, meal-breaks, communication, documentation, and family care. We capture Cyber, Physical, and Social (CPS) activities of 17 participants over four weeks using device-based sensing, app activity logging, and an experience sampling methodology. Our cohort includes students, casual workers, and professionals, forming the first real-world context-rich task behaviour dataset. We model CPS activities across different task categories, results highlight the importance of considering the CPS feature sets in modelling, especially work-related tasks."
MoraySTF: a novel approach for requirement definition for GSD projects in a mobile ecosystem,"Gon\c{c}alves, Klinsman M. and Vaz, Yasmine G. and Cruz, Eberth F. and Silva, Rafael E. and Souza, Lineker and Azevedo, F\'{a}bio M. and Sardinha, Eduardo D. and Fonseca, Paulo and Pahins, C\'{\i}cero A. L.",10.1145/3372787.3390433,2020,SIDIA is an R&amp;D Institute located in Manaus (AM) focused on research and software development. SIDIA works in partnership with the Global Device Manufacturer developing the Android operating system embedded in mobile devices (smartphones and tablets) for Latin America. This complex scenario involves the development of products in collaboration with partners in different locations to accomplish manufacturing schedules. This poses a challenge in managing software development projects. This experience report describes a tool-based approach aiming to improve the availability of resources (software and hardware under development) as well as the requirements management process. We also report the lessons learned and difficulties of adopting the tool-based approach in the software development process.
On the detection of community smells using genetic programming-based ensemble classifier chain,"Almarimi, Nuri and Ouni, Ali and Chouchen, Moataz and Saidani, Islem and Mkaouer, Mohamed Wiem",10.1145/3372787.3390439,2020,"Community smells are symptoms of organizational and social issues within the software development community that often increase the project costs and impact software quality. Recent studies have identified a variety of community smells and defined them as suboptimal patterns connected to organizational-social structures in the software development community such as the lack of communication, coordination and collaboration. Recognizing the advantages of the early detection of potential community smells in a software project, we introduce a novel approach that learns from various community organizational and social practices to provide an automated support for detecting community smells. In particular, our approach learns from a set of interleaving organizational-social symptoms that characterize the existence of community smell instances in a software project. We build a multi-label learning model to detect 8 common types of community smells. We use the ensemble classifier chain (ECC) model that transforms multi-label problems into several single-label problems which are solved using genetic programming (GP) to find the optimal detection rules for each smell type. To evaluate the performance of our approach, we conducted an empirical study on a benchmark of 103 open source projects and 407 community smell instances. The statistical tests of our results show that our approach can detect the eight considered smell types with an average F-measure of 89% achieving a better performance compared to different state-of-the-art techniques. Furthermore, we found that the most influential factors that best characterize community smells include the social network density and closeness centrality as well as the standard deviation of the number of developers per time zone and per community."
Setting Students up to Succeed in Computing Internships,"Sweetser, Penny and King, Alaine and DeWan, Timothy",10.1145/3373165.3373178,2020,"Work integrated learning and professional practice skills are fundamental to computer science education, in addition to forming a requirement for professional body accreditation of courses. The Australian National University (ANU) offers internship placements as a project-based work integrated learning opportunity for undergraduate and postgraduate students. The ANU Computer Science Internship Program is innovative in its design, as it provides three streams: (1) placement in a host organisation, (2) an academic project-based program, and (3) professional mentoring and support via workshops and peer circles. Students entering internship placements find themselves challenged by leaving the university to engage in work-integrated learning, and even more so if they are international students encountering foreign workplace culture. We support our students in managing and growing through these challenges by providing a supportive network, both academically and professionally. This paper reports on the design, development, ongoing improvements, challenges, and outcomes of our internship program."
"Research on Public Opinion of ""Chuang Youth Entrepreneurship Competition"" Based on Emotion Analysis","Zhao, Chenyang and Tian, Peng",10.1145/3373419.3373423,2020,"[purpose/significance] this paper takes weibo, a communication tool, as the carrier, and analyzes the emotional trend and emotional timing trend of netizens in the ""chuang youth entrepreneurial competition"" through weibo comments, so as to provide reference value for relevant departments in information communication and management and control of the event.[method/process] this paper takes weibo as the data source and ""chuang youth entrepreneurship competition"" as the theme keyword, collects the text information of netizens on chuang youth entrepreneurship competition, USES the emotion analysis method, constructs the public opinion time sequence chart, and analyzes the netizens' emotion situation on chuang youth entrepreneurship competition.[conclusion/result] the research results show that the public's attention and enthusiasm for entrepreneurship is not too high, only limited to specific groups of people to pay attention to this event."
Software code analysis using ensemble learning techniques,"Aggarwal, Simran",10.1145/3373477.3373486,2020,"Ensuing the advent of advancements in software systems, the probability of them containing high severity defects is exponentially on the rise. With each technological addition, the complexity of software is increasing. Reproduction and rectification of a defect requires time and effort. Current state of the art analysis tools cater to the investigation of static aspects of a production level code. However, it is imperative to assess the dynamic development process of a system so as to be able to timely detect erroneous components early on in the development life cycle of a software. A novel automated defect prediction feature enhancement is proposed that analyses the static structure of the current code and state of the software in past releases to extract relevant static and dynamic feature sets. Data generated is modelled for defect trends in the future release of the software by four ensemble classifiers. Results demonstrate the superiority of Voting algorithm for the problem of defect prediction."
Classification Based Software Defect Prediction Model for Finance Software System - An Industry Study,"Zong, Liang",10.1145/3374549.3374553,2020,"Automated software defect prediction is an important and fundamental activity in the domain of software development. Successful software defect prediction can save testing effort thus reduce the time and cost for software development. However, software systems for finance company are inherently large and complex with numerous interfaces with other systems. Thus, identifying and selecting a good model and a set of features is important but challenging problem. In our paper, we first define the problem we want to solve. Then we propose a prediction model based on binary classification and a set of novel features, which is more specific for finance software systems. We collected 15 months real production data and labelled it as our dataset. The experiment shows our model and features can give a better prediction accuracy for finance systems. In addition, we demonstrate how our prediction model helps improve our production quality further. Unlike other research papers, our proposal focuses to solve problem in real finance industry."
Using Collaborative Filtering Algorithm to Estimate the Predictive Power of a Functional Requirement,"Hidalgo, Reynald Jay F. and Fernandez, Proceso L.",10.1145/3377571.3377605,2020,"Collaborative filtering (CF) algorithm uses the preferences expressed by previous users of items being studied and is widely applied to build recommender systems. A collaborative filter predicts items that a user will like based on the vote similar users gave to that item. In this study, we use CF to estimate how much the knowledge of the presence or absence of one software feature can contribute to the correct prediction of the presence or absence of each of the possible remaining features. Completed software project documentations from the Master in Information Technology programs of selected Northern Luzon higher education institutions were first collected. An analysis of these documents revealed 26 unique software features and yielded a binary matrix indicating the presence or absence of a feature in a specific project. Leave-one-out cross-validation was performed to estimate the predictive power of each element of a given holdout vector, using the 26x26 cosine similarity matrix generated from the remaining vectors. The results show that, on average, knowing correctly the presence or absence of only 1 feature can predict with an accuracy of about 58% the presence or absence of the remaining features. This is 8% better than that of a na\""{\i}ve 50-50 random binary guessing algorithm, and somehow indicates the amount of information contributed by one feature value under the CF algorithm."
Understanding the automated parameter optimization on transfer learning for cross-project defect prediction: an empirical study,"Li, Ke and Xiang, Zilin and Chen, Tao and Wang, Shuo and Tan, Kay Chen",10.1145/3377811.3380360,2020,"Data-driven defect prediction has become increasingly important in software engineering process. Since it is not uncommon that data from a software project is insufficient for training a reliable defect prediction model, transfer learning that borrows data/konwledge from other projects to facilitate the model building at the current project, namely cross-project defect prediction (CPDP), is naturally plausible. Most CPDP techniques involve two major steps, i.e., transfer learning and classification, each of which has at least one parameter to be tuned to achieve their optimal performance. This practice fits well with the purpose of automated parameter optimization. However, there is a lack of thorough understanding about what are the impacts of automated parameter optimization on various CPDP techniques. In this paper, we present the first empirical study that looks into such impacts on 62 CPDP techniques, 13 of which are chosen from the existing CPDP literature while the other 49 ones have not been explored before. We build defect prediction models over 20 real-world software projects that are of different scales and characteristics. Our findings demonstrate that: (1) Automated parameter optimization substantially improves the defect prediction performance of 77% CPDP techniques with a manageable computational cost. Thus more efforts on this aspect are required in future CPDP studies. (2) Transfer learning is of ultimate importance in CPDP. Given a tight computational budget, it is more cost-effective to focus on optimizing the parameter configuration of transfer learning algorithms (3) The research on CPDP is far from mature where it is 'not difficult' to find a better alternative by making a combination of existing transfer learning and classification techniques. This finding provides important insights about the future design of CPDP techniques."
Improving the effectiveness of traceability link recovery using hierarchical bayesian networks,"Moran, Kevin and Palacio, David N. and Bernal-C\'{a}rdenas, Carlos and McCrystal, Daniel and Poshyvanyk, Denys and Shenefiel, Chris and Johnson, Jeff",10.1145/3377811.3380418,2020,"Traceability is a fundamental component of the modern software development process that helps to ensure properly functioning, secure programs. Due to the high cost of manually establishing trace links, researchers have developed automated approaches that draw relationships between pairs of textual software artifacts using similarity measures. However, the effectiveness of such techniques are often limited as they only utilize a single measure of artifact similarity and cannot simultaneously model (implicit and explicit) relationships across groups of diverse development artifacts.In this paper, we illustrate how these limitations can be overcome through the use of a tailored probabilistic model. To this end, we design and implement a HierarchiCal PrObabilistic Model for SoftwarE Traceability (Comet) that is able to infer candidate trace links. Comet is capable of modeling relationships between artifacts by combining the complementary observational prowess of multiple measures of textual similarity. Additionally, our model can holistically incorporate information from a diverse set of sources, including developer feedback and transitive (often implicit) relationships among groups of software artifacts, to improve inference accuracy. We conduct a comprehensive empirical evaluation of Comet that illustrates an improvement over a set of optimally configured baselines of ≈14% in the best case and ≈5% across all subjects in terms of average precision. The comparative effectiveness of Comet in practice, where optimal configuration is typically not possible, is likely to be higher. Finally, we illustrate Comet's potential for practical applicability in a survey with developers from Cisco Systems who used a prototype Comet Jenkins plugin."
The effects of required security on software development effort,"Venson, Elaine",10.1145/3377812.3381393,2020,"Problem: developers are increasingly adopting security practices in software projects in response to cyber threats. Despite the additional effort required to perform those practices, current cost models either do not consider security as an input or were not properly validated with empirical data. Hypothesis: increasing degrees of application of security practices and security features, motivated by security risks, lead to growing levels of added software development effort. Such an effort increase can be quantified through a parametric model that takes as input the usage degrees of security practices and requirements and outputs the additional software development effort. Contributions: the accurate prediction of secure software development effort will support the provision of a proper amount of resources to projects. We also expect that the quantification of the security effort will contribute to advance research on the cost-effectiveness of software security."
Building and maintaining a third-party library supply chain for productive and secure SGX enclave development,"Wang, Pei and Ding, Yu and Sun, Mingshen and Wang, Huibo and Li, Tongxin and Zhou, Rundong and Chen, Zhaofeng and Jing, Yiming",10.1145/3377813.3381348,2020,"The big data industry is facing new challenges as concerns about privacy leakage soar. One of the remedies to privacy breach incidents is to encapsulate computations over sensitive data within hardware-assisted Trusted Execution Environments (TEE). Such TEE-powered software is called secure enclaves. Secure enclaves hold various advantages against competing for privacy-preserving computation solutions. However, enclaves are much more challenging to build compared with ordinary software. The reason is that the development of TEE software must follow a restrictive programming model to make effective use of strong memory encryption and segregation enforced by hardware. These constraints transitively apply to all third-party dependencies of the software. If these dependencies do not officially support TEE hardware, TEE developers have to spend additional engineering effort in porting them. High development and maintenance cost is one of the major obstacles against adopting TEE-based privacy protection solutions in production.In this paper, we present our experience and achievements with regard to constructing and continuously maintaining a third-party library supply chain for TEE developers. In particular, we port a large collection of Rust third-party libraries into Intel SGX, one of the most mature trusted computing platforms. Our supply chain accepts upstream patches in a timely manner with SGX-specific security auditing. We have been able to maintain the SGX ports of 159 open-source Rust libraries with reasonable operational costs. Our work can effectively reduce the engineering cost of developing SGX enclaves for privacy-preserving data processing and exchange."
Understanding devops education with grounded theory,"Pang, Candy and Hindle, Abram and Barbosa, Denilson",10.1145/3377814.3381711,2020,"DevOps stands for Development-Operations. It arises from the IT industry as a movement aligning development and operations teams. DevOps is broadly recognized as an IT standard, and there is high demand for DevOps practitioners in industry. Since ACM &amp; IEEE suggest that undergraduate computer science curricula ""must adequately prepare [students] for the workforce"", we studied whether undergraduates acquired adequate DevOps skills to fulfill the demand for DevOps practitioners in industry. We employed Grounded Theory (GT), a social science qualitative research methodology, to study DevOps education from academic and industrial perspectives. In academia, academics were not motivated to learn or adopt DevOps, and we did not find strong evidence of academics teaching DevOps. Academics need incentives to adopt DevOps, in order to stimulate interest in teaching DevOps. In industry, DevOps practitioners lack clearly defined roles and responsibilities, for the DevOps topic is diverse and growing too fast. Therefore, practitioners can only learn DevOps through hands-on working experience. As a result, academic institutions should provide fundamental DevOps education (in culture, procedure, and technology) to prepare students for their future DevOps advancement in industry. Based on our findings, we proposed five groups of future studies to advance DevOps education in academia."
Automatically predicting bug severity early in the development process,"Arokiam, Jude and Bradbury, Jeremy S.",10.1145/3377816.3381738,2020,"Bug severity is an important factor in prioritizing which bugs to fix first. The process of triaging bug reports and assigning a severity requires developer expertise and knowledge of the underlying software. Methods to automate the assignment of bug severity have been developed to reduce the developer cost, however, many of these methods require 70-90% of the project's bug reports as training data and delay their use until later in the development process. Not being able to automatically predict a bug report's severity early in a project can greatly reduce the benefits of automation. We have developed a new bug report severity prediction method that leverages how bug reports are written rather than what the bug reports contain. Our method allows for the prediction of bug severity at the beginning of the project by using an organization's historical data, in the form of bug reports from past projects, to train the prediction classifier. In validating our approach, we conducted over 1000 experiments on a dataset of five NASA robotic mission software projects. Our results demonstrate that our method was not only able to predict the severity of bugs earlier in development, but it was also able to outperform an existing keyword-based classifier for a majority of the NASA projects."
Nocturnal Cough and Snore Detection Using Smartphones in Presence of Multiple Background-Noises,"Vhaduri, Sudip",10.1145/3378393.3402273,2020,"Non-speech human sounds, such as coughs and snores, and their patterns are associated with different respiratory diseases, including asthma, chronic obstructive pulmonary disease (COPD), as well as other health difficulties such as sleep disorders. Thereby, researchers and physicians have been using coughs and snores as symptoms while reporting and assessing respiratory diseases, their stages, and sleep quality. However, so far, the assessments frequently depend on different types of patient-reported surveys, which inherently suffer from various limitations, such as recall biases, human errors. Therefore, automated detection and reporting of coughs and snores can improve the disease assessment and monitoring. In this paper, we present an automated approach to detect coughs and snores from smartphone-microphones using generalized, semi-personalized and personalized modeling schemes. We analyze three separate datasets and different combinations of three types of nocturnal noises (i.e., sounds from air conditioners (AC), dog barks, and sirens) using the Mel-frequency cepstral coefficient (MFCC) features and different classification techniques. We find that a generalized model with the support vector machine (SVM) classifier can achieve an average accuracy of 0.86 ± 0.14, F1 score of 0.86± 0.13, and area under the receiver operating characteristic curve (AUC-ROC) of 0.94 ± 0.08. These performances can further be improved to an average accuracy of 0.96± 0.08, F1 score of 0.96± 0.08, and AUC-ROC of 0.98 ± 0.04 using the personalized random forest (RF) model. The results show the potential for smartphones to automatically report symptoms of respiratory diseases as well as sleep disorders. Furthermore, we find that our models perform consistently well while testing on separate datasets in the presence of multiple background-noises."
An Innovation Activity Framework for Digital Innovation,"Hellwig, Lukas and Pawlowski, Jan and Sch\""{a}fer, Michael",10.1145/3378539.3393857,2020,"The digital transformation poses great challenges for companies on numerous levels. It changes existing processes and the activities associated with them. In this paper, we analyse which activities take place within innovation processes in enterprises, taking into account new approaches such as open, user and digital innovation in the context of digital transformation. In our study, we use a comprehensive literature review to identify all relevant activities along innovation processes. In total, 48 activities within seven phases can be identified on the basis of 16 considered references. We further identify eight specific practices of digital innovation and derive a dependency. Hereby, we contribute to a better understanding of the impact of digital transformation on innovation processes and provide a practice-oriented overview. Furthermore, the findings form a solid basis for further empirical studies."
Challenges of Applying Predictive Analytics in Transport Logistics,"Birkel, Hendrik and Kopyto, Matthias and Lutz, Corinna",10.1145/3378539.3393864,2020,"The field of Predictive Analytics (PA) provides the possibility to utilize large amounts of data to improve forecasting, data-driven decision-making, and competitive advantage. Especially the transport logistics sector, which is characterized by high business-related uncertainties, time-sensitivity, and volatility, highly benefits from accurate resource and production planning. While success factors and framework conditions of applying PA are well-investigated on a theoretical SCM level, findings on internal and external challenges of transport logistics organizations remain scarce. Therefore, based on a multiple case approach, this study offers in-depth insights into six real-world cases of freight forwarders, ocean carriers, and air carriers. The results uncover both internal and external challenges. From the internal perspective, the biggest challenges are related to the technical implementation including the acquisition of globally generated, internal and external data and its harmonization. In addition, stakeholder management and target setting impede the development of PA. Regarding external challenges, relational and external conditions hamper the application. Therefore, especially actions of third-party institutions in terms of standardization and security enhancements are required. This study contributes to the existing literature in various ways as the systematic identification addresses real-world issues of PA in the neglected but crucial area of transport logistics, discussing urgent research needs and highlighting potential solutions. Additionally, the results offer valuable guidance for managers when implementing PA in transport logistics."
Preparing for Projects: IT Student Self-Evaluation of Technical and Professional Skills,"Potter, Leigh Ellen",10.1145/3378539.3393868,2020,"As Information Technology academics we seek to prepare our students for their future endeavours, and these are generally focused on an IT professional career. To commence such a career, our graduates need a range of skills, both technical and professional and the ability to demonstrate these skills. In this paper a study of a graduating class of IT students is examined to explore their own identification and evaluation of their skills. Their self-rated skills are compared to a peer evaluation, assessor evaluation, and an evaluation from the industry partner to determine the accuracy of their self-rated skills evaluation."
Emerging and Changing Tasks in the Development Process for Machine Learning Systems,"Liu, Hanyan and Eksmo, Samuel and Risberg, Johan and Hebig, Regina",10.1145/3379177.3388905,2020,"Integrating machine learning components in software systems is a task more and more companies are confronted with. However, there is not much knowledge today on how the software development process needs to change, when such components are integrated into a software system. We performed an interview study with 16 participants, focusing on emerging and changing task. The results uncover a set of 25 tasks associated to different software development phases, such as requirements engineering or deployment. We are just starting to understand the implications of using machine-learning components on the software development process. This study allows some first insights into how widespread the required process changes are."
The Impact of Software Fault Prediction in Real-World Application: An Automated Approach for Software Engineering,"Ahmed, Md. Razu and Ali, Md. Asraf and Ahmed, Nasim and Zamal, Md. Fahad Bin and Shamrat, F.M. Javed Mehedi",10.1145/3379247.3379278,2020,"Software fault prediction and proneness has long been considered as a critical issue for the tech industry and software professionals. In the traditional techniques, it requires previous experience of faults or a faulty module while detecting the software faults inside an application. An automated software fault recovery models enable the software to significantly predict and recover software faults using machine learning techniques. Such ability of the feature makes the software to run more effectively and reduce the faults, time and cost. In this paper, we proposed a software defect predictive development models using machine learning techniques that can enable the software to continue its projected task. Moreover, we used different prominent evaluation benchmark to evaluate the model's performance such as ten-fold cross-validation techniques, precision, recall, specificity, f 1 measure, and accuracy. This study reports a significant classification performance of 98-100% using SVM on three defect datasets in terms of f1 measure. However, software practitioners and researchers can attain independent understanding from this study while selecting automated task for their intended application."
Traceability Support for Multi-Lingual Software Projects,"Liu, Yalin and Lin, Jinfeng and Cleland-Huang, Jane",10.1145/3379597.3387440,2020,"Software traceability establishes associations between diverse software artifacts such as requirements, design, code, and test cases. Due to the non-trivial costs of manually creating and maintaining links, many researchers have proposed automated approaches based on information retrieval techniques. However, many globally distributed software projects produce software artifacts written in two or more languages. The use of intermingled languages reduces the efficacy of automated tracing solutions. In this paper, we first analyze and discuss patterns of intermingled language use across multiple projects, and then evaluate several different tracing algorithms including the Vector Space Model (VSM), Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA), and various models that combine mono-and cross-lingual word embeddings with the Generative Vector Space Model (GVSM). Based on an analysis of 14 Chinese-English projects, our results show that best performance is achieved using mono-lingual word embeddings integrated into GVSM with machine translation as a preprocessing step."
Detecting Video Game-Specific Bad Smells in Unity Projects,"Borrelli, Antonio and Nardone, Vittoria and Di Lucca, Giuseppe A. and Canfora, Gerardo and Di Penta, Massimiliano",10.1145/3379597.3387454,2020,"The growth of the video game market, the large proportion of games targeting mobile devices or streaming services, and the increasing complexity of video games trigger the availability of video game-specific tools to assess performance and maintainability problems. This paper proposes UnityLinter, a static analysis tool that supports Unity video game developers to detect seven types of bad smells we have identified as relevant in video game development. Such smell types pertain to performance, maintainability and incorrect behavior problems. After having defined the smells by analyzing the existing literature and discussion forums, we have assessed their relevance with a survey involving 68 participants. Then, we have analyzed the occurrence of the studied smells in 100 open-source Unity projects, and also assessed UnityLinter's accuracy. Results of our empirical investigation indicate that developers well-received performance- and behavior-related issues, while some maintainability issues are more controversial. UnityLinter is, in general, accurate enough in detecting smells (86%-100% precision and 50%-100% recall), and our study shows that the studied smell types occur in 39%-97% of the analyzed projects."
Beyond the Code: Mining Self-Admitted Technical Debt in Issue Tracker Systems,"Xavier, Laerte and Ferreira, Fabio and Brito, Rodrigo and Valente, Marco Tulio",10.1145/3379597.3387459,2020,"Self-admitted technical debt (SATD) is a particular case of Technical Debt (TD) where developers explicitly acknowledge their sub-optimal implementation decisions. Previous studies mine SATD by searching for specific TD-related terms in source code comments. By contrast, in this paper we argue that developers can admit technical debt by other means, e.g., by creating issues in tracking systems and labelling them as referring to TD. We refer to this type of SATD as issue-based SATD or just SATD-I. We study a sample of 286 SATD-I instances collected from five open source projects, including Microsoft Visual Studio and GitLab Community Edition. We show that only 29% of the studied SATD-I instances can be tracked to source code comments. We also show that SATD-I issues take more time to be closed, compared to other issues, although they are not more complex in terms of code churn. Besides, in 45% of the studied issues TD was introduced to ship earlier, and in almost 60% it refers to DESIGN flaws. Finally, we report that most developers pay SATD-I to reduce its costs or interests (66%). Our findings suggest that there is space for designing novel tools to support technical debt management, particularly tools that encourage developers to create and label issues containing TD concerns."
The Impact of a Major Security Event on an Open Source Project: The Case of OpenSSL,"Walden, James",10.1145/3379597.3387465,2020,"Context: The Heartbleed vulnerability brought OpenSSL to international attention in 2014. The almost moribund project was a key security component in public web servers and over a billion mobile devices. This vulnerability led to new investments in OpenSSL.Objective: The goal of this study is to determine how the Heart-bleed vulnerability changed the software evolution of OpenSSL. We study changes in vulnerabilities, code quality, project activity, and software engineering practices.Method: We use a mixed methods approach, collecting multiple types of quantitative data and qualitative data from web sites and an interview with a developer who worked on post-Heartbleed changes. We use regression discontinuity analysis to determine changes in levels and slopes of code and project activity metrics resulting from Heartbleed.Results: The OpenSSL project made tremendous improvements to code quality and security after Heartbleed. By the end of 2016, the number of commits per month had tripled, 91 vulnerabilities were found and fixed, code complexity decreased significantly, and OpenSSL obtained a CII best practices badge, certifying its use of good open source development practices.Conclusions: The OpenSSL project provides a model of how an open source project can adapt and improve after a security event. The evolution of OpenSSL shows that the number of known vulnerabilities is not a useful indicator of project security. A small number of vulnerabilities may simply indicate that a project does not expend much effort to finding vulnerabilities. This study suggests that project activity and CII badge best practices may be better indicators of code quality and security than vulnerability counts."
Need for Tweet: How Open Source Developers Talk About Their GitHub Work on Twitter,"Fang, Hongbo and Klug, Daniel and Lamba, Hemank and Herbsleb, James and Vasilescu, Bogdan",10.1145/3379597.3387466,2020,"Social media, especially Twitter, has always been a part of the professional lives of software developers, with prior work reporting on a diversity of usage scenarios, including sharing information, staying current, and promoting one's work. However, previous studies of Twitter use by software developers typically lack information about activities of the study subjects (and their outcomes) on other platforms. To enable such future research, in this paper we propose a computational approach to cross-link users across Twitter and GitHub, revealing (at least) 70,427 users active on both. As a preliminary analysis of this dataset, we report on a case study of 786 tweets by open-source developers about GitHub work, combining automatic characterization of tweet authors in terms of their relationship to the GitHub items linked in their tweets with qualitative analysis of the tweet contents. We find that different developer roles tend to have different tweeting behaviors, with repository owners being perhaps the most distinctive group compared to other project contributors and followers. We also note a sizeable group of people who follow others on GitHub and tweet about these people's work, but do not otherwise contribute to those open-source projects. Our results and public dataset open up multiple future research directions."
The State of the ML-universe: 10 Years of Artificial Intelligence &amp; Machine Learning Software Development on GitHub,"Gonzalez, Danielle and Zimmermann, Thomas and Nagappan, Nachiappan",10.1145/3379597.3387473,2020,"In the last few years, artificial intelligence (AI) and machine learning (ML) have become ubiquitous terms. These powerful techniques have escaped obscurity in academic communities with the recent onslaught of AI &amp; ML tools, frameworks, and libraries that make these techniques accessible to a wider audience of developers. As a result, applying AI &amp; ML to solve existing and emergent problems is an increasingly popular practice. However, little is known about this domain from the software engineering perspective. Many AI &amp; ML tools and applications are open source, hosted on platforms such as GitHub that provide rich tools for large-scale distributed software development. Despite widespread use and popularity, these repositories have never been examined as a community to identify unique properties, development patterns, and trends.In this paper, we conducted a large-scale empirical study of AI &amp; ML Tool (700) and Application (4,524) repositories hosted on GitHub to develop such a characterization. While not the only platform hosting AI &amp; ML development, GitHub facilitates collecting a rich data set for each repository with high traceability between issues, commits, pull requests and users. To compare the AI &amp; ML community to the wider population of repositories, we also analyzed a set of 4,101 unrelated repositories. We enhance this characterization with an elaborate study of developer workflow that measures collaboration and autonomy within a repository. We've captured key insights of this community's 10 year history such as it's primary language (Python) and most popular repositories (Tensorflow, Tesseract). Our findings show the AI &amp; ML community has unique characteristics that should be accounted for in future research."
Behind the Intents: An In-depth Empirical Study on Software Refactoring in Modern Code Review,"Paix\~{a}o, Matheus and Uch\^{o}a, Anderson and Bibiano, Ana Carla and Oliveira, Daniel and Garcia, Alessandro and Krinke, Jens and Arvonio, Emilio",10.1145/3379597.3387475,2020,"Code refactorings are of pivotal importance in modern code review. Developers may preserve, revisit, add or undo refactorings through changes' revisions. Their goal is to certify that the driving intent of a code change is properly achieved. Developers' intents behind refactorings may vary from pure structural improvement to facilitating feature additions and bug fixes. However, there is little understanding of the refactoring practices performed by developers during the code review process. It is also unclear whether the developers' intents influence the selection, composition, and evolution of refactorings during the review of a code change. Through mining 1,780 reviewed code changes from 6 systems pertaining to two large open-source communities, we report the first in-depth empirical study on software refactoring during code review. We inspected and classified the developers' intents behind each code change into 7 distinct categories. By analyzing data generated during the complete reviewing process, we observe: (i) how refactorings are selected, composed and evolved throughout each code change, and (ii) how developers' intents are related to these decisions. For instance, our analysis shows developers regularly apply non-trivial sequences of refactorings that crosscut multiple code elements (i.e., widely scattered in the program) to support a single feature addition. Moreover, we observed that new developers' intents commonly emerge during the code review process, influencing how developers select and compose their refactorings to achieve the new and adapted goals. Finally, we provide an enriched dataset that allows researchers to investigate the context and motivations behind refactoring operations during the code review process."
The Scent of Deep Learning Code: An Empirical Study,"Jebnoun, Hadhemi and Ben Braiek, Houssem and Rahman, Mohammad Masudur and Khomh, Foutse",10.1145/3379597.3387479,2020,"Deep learning practitioners are often interested in improving their model accuracy rather than the interpretability of their models. As a result, deep learning applications are inherently complex in their structures. They also need to continuously evolve in terms of code changes and model updates. Given these confounding factors, there is a great chance of violating the recommended programming practices by the developers in their deep learning applications. In particular, the code quality might be negatively affected due to their drive for the higher model performance. Unfortunately, the code quality of deep learning applications has rarely been studied to date. In this paper, we conduct an empirical study to investigate the distribution of code smells in deep learning applications. To this end, we perform a comparative analysis between deep learning and traditional open-source applications collected from GitHub. We have several major findings. First, long lambda expression, long ternary conditional expression, and complex container comprehension smells are frequently found in deep learning projects. That is, deep learning code involves more complex or longer expressions than the traditional code does. Second, the number of code smells increases across the releases of deep learning applications. Third, we found that there is a co-existence between code smells and software bugs in the studied deep learning code, which confirms our conjecture on the degraded code quality of deep learning applications."
Dataset of Video Game Development Problems,"Politowski, Cristiano and Petrillo, Fabio and Ullmann, Gabriel Cavalheiro and de Andrade Werly, Josias and Gu\'{e}h\'{e}neuc, Yann-Ga\""{e}l",10.1145/3379597.3387486,2020,"Different from traditional software development, there is little information about the software-engineering process and techniques in video-game development. One popular way to share knowledge among the video-game developers' community is the publishing of postmortems, which are documents summarizing what happened during the video-game development project. However, these documents are written without formal structure and often providing disparate information. Through this paper, we provide developers and researchers with grounded dataset describing software-engineering problems in video-game development extracted from postmortems. We created the dataset using an iterative method through which we manually coded more than 200 postmortems spanning 20 years (1998 to 2018) and extracted 1,035 problems related to software engineering while maintaining traceability links to the postmortems. We grouped the problems in 20 different types. This dataset is useful to understand the problems faced by developers during video-game development, providing researchers and practitioners a starting point to study video-game development in the context of software engineering."
A Dataset of Enterprise-Driven Open Source Software,"Spinellis, Diomidis and Kotti, Zoe and Kravvaritis, Konstantinos and Theodorou, Georgios and Louridas, Panos",10.1145/3379597.3387495,2020,"We present a dataset of open source software developed mainly by enterprises rather than volunteers. This can be used to address known generalizability concerns, and, also, to perform research on open source business software development. Based on the premise that an enterprise's employees are likely to contribute to a project developed by their organization using the email account provided by it, we mine domain names associated with enterprises from open data sources as well as through white- and blacklisting, and use them through three heuristics to identify 17 264 enterprise GitHub projects. We provide these as a dataset detailing their provenance and properties. A manual evaluation of a dataset sample shows an identification accuracy of 89%. Through an exploratory data analysis we found that projects are staffed by a plurality of enterprise insiders, who appear to be pulling more than their weight, and that in a small percentage of relatively large projects development happens exclusively through enterprise insiders."
Cheating Death: A Statistical Survival Analysis of Publicly Available Python Projects,"Ali, Rao Hamza and Parlett-Pelleriti, Chelsea and Linstead, Erik",10.1145/3379597.3387511,2020,"We apply survival analysis methods to a dataset of publicly-available software projects in order to examine the attributes that might lead to their inactivity over time. We ran a Kaplan-Meier analysis and fit a Cox Proportional-Hazards model to a subset of Software Heritage Graph Dataset, consisting of 3052 popular Python projects hosted on GitLab/GitHub, Debian, and PyPI, over a period of 165 months. We show that projects with repositories on multiple hosting services, a timeline of publishing major releases, and a good network of developers, remain healthy over time and should be worthy of the effort put in by developers and contributors."
A Survey of Automatic Generation of Code Comments,"Zhao, Fengrong and Zhao, Junqi and Bai, Yang",10.1145/3380625.3380649,2020,"Code comments are a valuable form of documentation attached to code that is the most intuitive and efficient way for programmers to understand software code. Good code comments can help programmers quickly understand the role of source code and facilitate understanding of programs and software maintenance tasks. However, in practice, most programmers only pay attention to the code and ignore the comments and documents, which makes the program's readability and maintainability greatly reduced. Based on the meaning of code comments, this paper discusses the current progress in the field of code comments research, adopts the comparative analysis method, focuses on the classification research of the methods and tools for automatic generation of code comments, expounds its advantages and disadvantages, and reveals the issues that need further study."
Feature-oriented defect prediction,"Str\""{u}der, Stefan and Mukelabai, Mukelabai and Str\""{u}ber, Daniel and Berger, Thorsten",10.1145/3382025.3414960,2020,"Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used."
Long-Term Evaluation of Technical Debt in Open-Source Software,"Molnar, Arthur-Jozsef and Motogna, Simona",10.1145/3382494.3410673,2020,"Background: A consistent body of research and practice have identified that technical debt provides valuable and actionable insight into the design and implementation deficiencies of complex software systems. Existing software tools enable characterizing and measuring the amount of technical debt at selective granularity levels; by providing a computational model, they enable stakeholders to measure and ultimately control this phenomenon. Aims: In this paper we aim to study the evolution and characteristics of technical debt in open-source software. For this, we carry out a longitudinal study that covers the entire development history of several complex applications. The goal is to improve our understanding of how the amount and composition of technical debt changes in evolving software. We also study how new technical debt is introduced in software, as well as identify how developers handle its accumulation over the long term. Method: We carried out our evaluation using three complex, open-source Java applications. All 110 released versions, covering more than 10 years of development history for each application were analyzed using SonarQube. We studied how the amount, composition and history of technical debt changed during development, compared our results across the studied applications and present our most important findings. Results: For each application, we identified key versions during which large amounts of technical debt were added, removed or both. This had significantly more impact when compared to the lines of code or class count increases that generally occurred during development. However, within each version, we found high correlation between file lines of code and technical debt. We observed that the Pareto principle was satisfied for the studied applications, as 20% of issue types generated around 80% of total technical debt. Interestingly, there was a large degree of overlap between the issues that generated most of the debt across the studied applications. Conclusions: Early application versions showed greater fluctuation in the amount of existing technical debt. We found application size to be an unreliable predictor for the quantity of technical debt. Most debt was introduced in applications as part of milestone releases that expanded their feature set; likewise, we identified releases where extensive refactoring significantly reduced the level of debt. We also discovered that technical debt issues persist for a long time in source code, and their removal did not appear to be prioritized according to type or severity."
An Empirical Study of Software Exceptions in the Field using Search Logs,"Hassan, Foyzul and Bansal, Chetan and Nagappan, Nachiappan and Zimmermann, Thomas and Awadallah, Ahmed Hassan",10.1145/3382494.3410692,2020,"Background: Software engineers spend a substantial amount of time using Web search to accomplish software engineering tasks. Such search tasks include finding code snippets, API documentation, seeking help with debugging, etc. While debugging a bug or crash, one of the common practices of software engineers is to search for information about the associated error or exception traces on the internet. Aims: In this paper, we analyze query logs from Bing to carry out a large scale study of software exceptions. To the best of our knowledge, this is the first large scale study to analyze how Web search is used to find information about exceptions. Method: We analyzed about 1 million exception related search queries from a random sample of 5 billion web search queries. To extract exceptions from unstructured query text, we built a novel machine learning model. With the model, we extracted exceptions from raw queries and performed popularity, effort, success, query characteristic and web domain analysis. We also performed programming language-specific analysis to give a better view of the exception search behavior. Results: Using the model with an F1-score of 0.82, our study identifies most frequent, most effort-intensive, or less successful exceptions and popularity of community Q&amp;A sites. Conclusion: These techniques can help improve existing methods, documentation and tools for exception analysis and prediction. Further, similar techniques can be applied for APIs, frameworks, etc."
"How to Complement Learning Analytics with Smartwatches? Fusing Physical Activities, Environmental Context, and Learning Activities","Ciordas-Hertel, George-Petru",10.1145/3382507.3421151,2020,"To obtain a holistic perspective on learning, a multimodal technical infrastructure for Learning Analytics (LA) can be beneficial. Recent studies have investigated various aspects of technical LA infrastructure. However, it has not yet been explored how LA indicators can be complemented with Smartwatch sensor data to detect physical activity and the environmental context. Sensor data, such as the accelerometer, are often used in related work to infer a specific behavior and environmental context, thus triggering interventions on a just-in-time basis. In this dissertation project, we plan to use Smartwatch sensor data to explore further indicators for learning from blended learning sessions conducted in-the-wild, e.g., at home. Such indicators could be used within learning sessions to suggest breaks, or afterward to support learners in reflection processes.We plan to investigate the following three research questions: (RQ1) How can multimodal learning analytics infrastructure be designed to support real-time data acquisition and processing effectively?; (RQ2) how to use smartwatch sensor data to infer environmental context and physical activities to complement learning analytics indicators for blended learning sessions; and (RQ3) how can we align the extracted multimodal indicators with pedagogical interventions.RQ1 was investigated by a structured literature review and by conducting eleven semi-structured interviews with LA infrastructure developers. According to RQ2, we are currently designing and implementing a multimodal learning analytics infrastructure to collect and process sensor and experience data from Smartwatches. Finally, according to RQ3, an exploratory field study will be conducted to extract multimodal learning indicators and examine them with learners and pedagogical experts to develop effective interventions.Researchers, educators, and learners can use and adapt our contributions to gain new insights into learners' time and learning tactics, and physical learning spaces from learning sessions taking place in-the-wild."
Automatic Identification of Decisions from the Hibernate Developer Mailing List,"Li, Xueying and Liang, Peng and Li, Zengyang",10.1145/3383219.3383225,2020,"Decisions run through the whole software development and maintenance processes. Explicitly documenting these decisions helps to organize development knowledge and to reduce its vaporization, thereby controlling the development process and maintenance costs. It can also support the knowledge acquisition process for stakeholders of the project. Meanwhile, developers (e.g., architects) and managers will be able to rely on the decisions made in the past to solve the problems encountered in their current projects. However, identifying decisions from massive textual artifacts, which involves considerable human effort, time, and cost, is usually unaffordable due to limited resources. To address this problem, we conducted an experiment to automatically identify decisions from textual artifacts using machine learning techniques. We created a dataset of 1,300 sentences labelled from the Hibernate developer mailing list, containing 650 decision sentences and non-decision sentences respectively, and trained machine learning models using 160 configurations regarding text preprocessing, feature extraction, and classification algorithms. The results show that (1) the text preprocessing method with Including Stop Words, No Stemming and Lemmatization, and No Filtering Out Sentences performs best when preprocessing posts to identify decisions; (2) the simple Bag-of-Words (BoW) model works best when extracting features to identify decisions; (3) the Support Vector Machine (SVM) algorithm gets the best result when training classifiers to identify decisions; and (4) the SVM algorithm with Including Stop Words (ISW), No Stemming and Lemmatization (NSaL), Filtering Out Sentences by Length (FOSbL), and BoW achieves the best performance (with a precision of 0.640, a recall of 0.932, and an F1-score of 0.759), compared with other configurations when identifying decisions from the mailing list."
Using the Lexicon from Source Code to Determine Application Domain,"Capiluppi, Andrea and Ajienka, Nemitari and Ali, Nour and Arzoky, Mahir and Counsell, Steve and Destefanis, Giuseppe and Miron, Alina and Nagaria, Bhaveet and Neykova, Rumyana and Shepperd, Martin and Swift, Stephen and Tucker, Allan",10.1145/3383219.3383231,2020,"Context: The vast majority of software engineering research is reported independently of the application domain: techniques and tools usage is reported without any domain context. As reported in previous research, this has not always been so: early in the computing era, the research focus was frequently application domain specific (for example, scientific and data processing).Objective: We believe determining the research context is often important. Therefore we propose a code-based approach to identify the application domain of a software system, via its lexicon. We compare its use against the plain textual description attached to the same system.Method: Using a sample of 50 Java projects, we obtained i) the description of each project (e.g., its ReadMe file), ii) the lexicon extracted from its source code, and iii) a list of its main topics extracted with the Latent Dirichlet Allocation (LDA) modelling technique. We assigned a random subset of these data items to different researchers (i.e., 'experts'), and asked them to assign each item to one (or more) application domain. We then evaluated the precision and accuracy of the three techniques.Results: Using the agreement levels between experts, We observed that the 'baseline' dataset (i.e., the ReadMe files) obtained the highest average in terms of agreement between experts, but we also observed that the three techniques had the same mode and median agreement levels. Additionally, in the cases where no agreement was reached for the baseline dataset, the two other techniques provided sufficient additional support.Conclusions: We conclude that the source code is sufficient for determining the application domain, so that classification is possible without special documentation requirements."
Towards continues code recommendation and implementation system: An Initial Framework,"Akbar, Muhammad Azeem and Huang, Zhiqiu and Yu, Zhou and Mehmood, Faisal and Hussain, Yasir and Hamza, Muhammad",10.1145/3383219.3383282,2020,"In the current era, the auto and reliable recommendation system plays a significant role in human life. The code recommender systems are being used in various source code databases to recommend the most suitable source code to the user. While code recommendation, the code analysis concerning 'code quality' and 'code implementation' is important to recommend the most reliable code by considering the objective of the user. The ultimate aim of this research work is to propose a code recommendation and implementation model using the characteristics of DevOps that assist in extracting, analyzing, implementing, and updating the recommender system continuously. The current study presents an initial framework of the proposed code recommender model. The design of the model is based on the data collected through literature review and by conducting an empirical study with experts. We believe that the proposed model will assist the researchers and practitioners to recommend the most secure and suitable source code according to their requirement."
Towards Efficient and Secure Global Software Development using Blockchain,"Akbar, Muhammad Azeem and Al-Sanad, Ahmad and AlSanad, Abeer AbdulAziz and Ghmaei, Abdu and Shafiq, Muhammad and Kamal, Tahir",10.1145/3383219.3383291,2020,"The economic and strategic gain attracts the software industry to adopt the global software development (GSD) phenomenon. Besides the significant advantages, the GSD firms also face certain critical challenges mainly related to security and management perspective. Blockchain is the new revolutionary technology that offers over the globe secure storage via distributed databases. The objective of this study is to investigate how the adoption of blockchain is beneficial for the GSD project. We have conducted interviews with academic researchers and case studies with industry practitioners to identify the impact of blockchain on GSD projects. We have identified ten key critical areas of GSD the effectively addressed by using the blockchain technology in GSD context. A comparative analysis is conducted with the finding of academic researcher and industry practitioners, and the results ((W=0.86, p=0.005) shows that there is a strong agreement between the understanding of academic researchers and industry practitioners. Besides, we have developed a hypothetical model based on the findings of both studies that shows the positive relationship of blockchain implementation in the GSD domain. We believe that the findings of this study will encourage the practitioners and researchers to develop the new plan and strategies for the adoption of blockchain in GSD."
Developing a Digital Scholarship Ecosystem,"Uzwyshyn, Ray",10.1145/3383923.3383926,2020,"This research presents elements necessary to develop a Digital Scholarship Research Ecosystem for a university, college or research institution. Software systems, hardware, human resources and timelines are outlined with brief theoretical overviews and a pragmatic focus on 'open-source' (freely available) software, best-in-class applications and global best practices. Major digital scholarly system components in a larger digital ecosystem are discussed: Online Institutional Collection Repositories (D-SPACE), Online Research Data Repositories (DATAVERSE), Identity Management Systems (ORCID), Electronic Thesis and Dissertation Management Systems (VIREO), Academic Journal Systems (OJS3), Digitization Labs, User Interface Software (OMEKA). System assessment, synergistic possibilities and future directions are reviewed. This research arises from a successful five-year phased implementation of such a digital ecosystem for Texas State University Libraries, a large US university research library system. This scholarly ecosystem is suitable for any university, college, research institution or academic research library interested in setting up or building on such an infrastructure and enabling faculty and graduate students with their scholarly research online."
Enhancing Computing Curriculum with Collaborative Engagement Model to Enrich Undergraduate Research Experience,"Tse, Rita and Lei, Philip and Tang, Su-Kit and Pau, Giovanni",10.1145/3383923.3383958,2020,"Attracting and engaging computer science students to enhance their thinking skills are challenging tasks. In the past 10 years, the Computing Program at the Macao Polytechnic Institute encouraged our undergraduate students in doing research work, striving to enhance their mathematical and algorithmic thinking skills as well as to engage them in their study. We are convinced that these experiences change students' perception of computer science and how they can be part of the innovation engine throughout their career in science and technology. This paper discusses the MPI Collaborative Undergraduate Research Engagement model which consists of three main building blocks: team work and interpersonal skills, domain knowledge building, and technical communications. Besides computer science, we believe that our model can also be applied to other disciplines."
A Framework for Automated Reengineering of BPMN Models by Excluding Inefficient Activities,"Khan, Rimsha and Azam, Farooque and Maqbool, Bilal and Anwar, Muhammad Waseem",10.1145/3384544.3384549,2020,"Business Process Reengineering (BPR), originally floated in the early 1990s, is gaining importance in industry and academia. BPR helps the organization rethink their work rationally by redesigning their current processes and resource consumption. Due to the high rate of software evolution, there is a need to run legacy systems on a new computing platform. BPMN models are subject to erroneous or unnecessary activities that are taking too many resources. Such process models are leading to additional cost and effort. Re-engineering help in improving the legacy system or in this context a set of legacy processes to perform better than before. This work presents a framework for automatic reengineering of a BPMN by identifying activities that are taking too much time and resources but are insignificant to the business process. An extensive literature review has led to the extraction of three important parameters based on which the business process activities can be evaluated as necessary or unnecessary i.e. time, resources and priority of an activity. The proposed model has been validated using a case study on the Claim Management System. This work shall be beneficial for the research community and developers targeting construction of a BPR tool"
A Systematic Literature Review for Software Portability Measurement: Preliminary Results,"Ghandorh, Hamza and Noorwali, Abdulfattah and Nassif, Ali Bou and Capretz, Luiz Fernando and Eagleson, Roy",10.1145/3384544.3384569,2020,"Software developers agree that software portability is a desirable attribute for their software quality. Software portability is mostly acquired by ad-hoc techniques when trying to port existing products. There is a lack of unified measuring approach of software portability in most computing platforms. This paper presents preliminary results of a systematic literature review, conducted to collect evidence on measuring software portability. The evidence was gathered from selected studies and based on a set of meaningful and focused questions. 49 studies of these were selected for data extraction performed against the research questions. We provide an overview of usedproposed measurement metrics of software portability. Our results suggested that there are scattered efforts to understand measurement of software portability, and no census has been achieved."
Body LayARs: A Toolkit for Body-Based Augmented Reality,"Pohl, Henning and Dalsgaard, Tor-Salve and Krasniqi, Vesa and Hornb\ae{}k, Kasper",10.1145/3385956.3418946,2020,"Technological advances are enabling a new class of augmented reality (AR) applications that use bodies as substrates for input and output. In contrast to sensing and augmenting objects, body-based AR applications track people around the user and layer information on them. However, prototyping such applications is complex, time-consuming, and cumbersome, due to a lack of easily accessible tooling and infrastructure. We present Body LayARs, a toolkit for fast development of body-based AR prototypes. Instead of directly programming for a device, Body LayARs provides an extensible graphical programming environment with a device-independent runtime abstraction. We focus on face-based experiences for headset AR, and show how Body LayARs makes a range of body-based AR applications fast and easy to prototype."
Modeling and Benchmarking Computing-in-Memory for Design Space Exploration,"Reis, Dayane and Gao, Di and Angizi, Shaahin and Yin, Xunzhao and Fan, Deliang and Niemier, Michael and Zhuo, Cheng and Hu, X. Sharon",10.1145/3386263.3407580,2020,"The bottleneck between the limited memory bandwidth and high speed processing demands is the main cause of problems associated with high volume of data transfers in data-intensive applications. As a possible remedy to these issues, computing-in-memory (CiM) enables a subset of logic and arithmetic operations to be performed where the data resides, i.e., inside the memory. Various CiM designs have been proposed to date, based on different technologies. Given the variety of options available, picking the right design option for a system/application can be a complex task. When choosing a CiM design, it is important to establish evaluation conditions that are as uniform as possible to make a fair choice between available design options. In this paper, we describe a methodology for an uniform benchmarking of CiM designs. Our approach evaluates devices/circuits, arrays and the overall impact of CiM to a system with a framework based on Eva-CiM. As a case study, we analyze the array-level performance of 7 recent CiM designs implemented with SRAM, DRAM, FeFET-RAM, STT-MRAM, SOT-MRAM, and RRAM. After we identify that the FeFET-RAM-based design shows promising energy and delay savings at the array level, we carry out a system level evaluation showing that FeFET-RAM-based CiM outperforms a CMOS SRAM CiM baseline by an average of 60% across a set of 17 benchmarks (with respect to energy savings). Regarding speedups, both technologies offer virtually the same benefit of about 1.5X when compared to a situation where processing does not happen in memory."
A Literature Review of Automatic Traceability Links Recovery for Software Change Impact Analysis,"Aung, Thazin Win Win and Huo, Huan and Sui, Yulei",10.1145/3387904.3389251,2020,"In large-scale software development projects, change impact analysis (CIA) plays an important role in controlling software design evolution. Identifying and accessing the effects of software changes using traceability links between various software artifacts is a common practice during the software development cycle. Recently, research in automated traceability-link recovery has received broad attention in the software maintenance community to reduce the manual maintenance cost of trace links by developers. In this study, we conducted a systematic literature review related to automatic traceability link recovery approaches with a focus on CIA. We identified 33 relevant studies and investigated the following aspects of CIA: traceability approaches, CIA sets, degrees of evaluation, trace direction and methods for recovering traceability link between artifacts of different types. Our review indicated that few traceability studies focused on designing and testing impact analysis sets, presumably due to the scarcity of datasets. Based on the findings, we urge further industrial case studies. Finally, we suggest developing traceability tools to support fully automatic traceability approaches, such as machine learning and deep learning."
A Model to Detect Readability Improvements in Incremental Changes,"Roy, Devjeet and Fakhoury, Sarah and Lee, John and Arnaoudova, Venera",10.1145/3387904.3389255,2020,"Identifying source code that has poor readability allows developers to focus maintenance efforts on problematic code. Therefore, the effort to develop models that can quantify the readability of a piece of source code has been an area of interest for software engineering researchers for several years. However, recent research questions the usefulness of these readability models in practice. When applying these models to readability improvements that are made in practice, i.e., commits, they are unable to capture these incremental improvements, despite a clear perceived improvement by the developers. This results in a discrepancy between the models we have built to measure readability, and the actual perception of readability in practice.In this work, we propose a model that is able to detect incremental readability improvements made by developers in practice with an average precision of 79.2% and an average recall of 67% on an unseen test set. We then investigate the metrics that our model associates with developer perceived readability improvements as well as non-readability changes. Finally, we compare our model to existing state-of-the-art readability models, which our model outperforms by at least 23% in terms of precision and 42% in terms of recall."
A Human Study of Comprehension and Code Summarization,"Stapleton, Sean and Gambhir, Yashmeet and LeClair, Alexander and Eberhart, Zachary and Weimer, Westley and Leach, Kevin and Huang, Yu",10.1145/3387904.3389258,2020,"Software developers spend a great deal of time reading and understanding code that is poorly-documented, written by other developers, or developed using differing styles. During the past decade, researchers have investigated techniques for automatically documenting code to improve comprehensibility. In particular, recent advances in deep learning have led to sophisticated summary generation techniques that convert functions or methods to simple English strings that succinctly describe that code's behavior. However, automatic summarization techniques are assessed using internal metrics such as BLEU scores, which measure natural language properties in translational models, or ROUGE scores, which measure overlap with human-written text. Unfortunately, these metrics do not necessarily capture how machine-generated code summaries actually affect human comprehension or developer productivity.We conducted a human study involving both university students and professional developers (n = 45). Participants reviewed Java methods and summaries and answered established program comprehension questions. In addition, participants completed coding tasks given summaries as specifications. Critically, the experiment controlled the source of the summaries: for a given method, some participants were shown human-written text and some were shown machine-generated text.We found that participants performed significantly better (p = 0.029) using human-written summaries versus machine-generated summaries. However, we found no evidence to support that participants perceive human- and machine-generated summaries to have different qualities. In addition, participants' performance showed no correlation with the BLEU and ROUGE scores often used to assess the quality of machine-generated summaries. These results suggest a need for revised metrics to assess and guide automatic summarization techniques."
Supporting Program Comprehension through Fast Query response in Large-Scale Systems,"Lin, Jinfeng and Liu, Yalin and Cleland-Huang, Jane",10.1145/3387904.3389260,2020,"Software traceability provides support for various engineering activities including Program Comprehension; however, it can be challenging and arduous to complete in large industrial projects. Researchers have proposed automated traceability techniques to create, maintain and leverage trace links. Computationally intensive techniques, such as repository mining and deep learning, have showed the capability to deliver accurate trace links. The objective of achieving trusted, automated tracing techniques at industrial scale has not yet been successfully accomplished due to practical performance challenges. This paper evaluates high-performance solutions for deploying effective, computationally expensive trace-ability algorithms in large scale industrial projects and leverages generated trace links to answer Program Comprehension Queries. We comparatively evaluate four different platforms for supporting industrial-scale tracing solutions, capable of tackling software projects with millions of artifacts. We demonstrate that tracing solutions built using big data frameworks scale well for large projects and that our Spark implementation outperforms relational database, graph database (GraphDB), and plain Java implementations. These findings contradict earlier results which suggested that GraphDB solutions should be adopted for large-scale tracing problems."
Adaptive Deep Code Search,"Ling, Chunyang and Lin, Zeqi and Zou, Yanzhen and Xie, Bing",10.1145/3387904.3389278,2020,"Searching code in a large-scale codebase using natural language queries is a common practice during software development. Deep learning-based code search methods demonstrate superior performance if models are trained with large amount of text-code pairs. However, few deep code search models can be easily transferred from one codebase to another. It can be very costly to prepare training data for a new codebase and re-train an appropriate deep learning model. In this paper, we propose AdaCS, an adaptive deep code search method that can be trained once and transferred to new codebases. AdaCS decomposes the learning process into embedding domain-specific words and matching general syntactic patterns. Firstly, an unsupervised word embedding technique is used to construct a matching matrix to represent the lexical similarities. Then, a recurrent neural network is used to capture latent syntactic patterns from these matching matrices in a supervised way. As the supervised task learns general syntactic patterns that exist across domains, AdaCS is transferable to new codebases. Experimental results show that: when extended to new software projects never seen in the training data, AdaCS is more robust and significantly outperforms state-of-the-art deep code search methods."
Understanding What Software Engineers Are Working on: The Work-Item Prediction Challenge,"L\""{a}mmel, Ralf and Kerber, Alvin and Praza, Liane",10.1145/3387904.3389294,2020,"Understanding what a software engineer (a developer, an incident responder, a production engineer, etc.) is working on is a challenging problem -- especially when considering the more complex software engineering workflows in software-intensive organizations: i) engineers rely on a multitude (perhaps hundreds) of loosely integrated tools; ii) engineers engage in concurrent and relatively long running workflows; ii) infrastructure (such as logging) is not fully aware of work items; iv) engineering processes (e.g., for incident response) are not explicitly modeled. In this paper, we explain the corresponding 'work-item prediction challenge' on the grounds of representative scenarios, report on related efforts at Facebook, discuss some lessons learned, and review related work to call to arms to leverage, advance, and combine techniques from program comprehension, mining software repositories, process mining, and machine learning."
Towards a Topology for Legacy System Migration,"Strobl, Stefan and Bernhart, Mario and Grechenig, Thomas",10.1145/3387940.3391476,2020,Dealing with legacy systems is a decade old industry challenge. The pressure to efficiently modernise legacy both to meet new business requirements and to mitigate inherent risks is ever growing. Our experience shows a lack of collaboration between researchers and practitioners inhibiting innovation in the field. To facilitate communication between academia and industry and as a byproduct to obtain an up to date picture of the state of affairs we are creating a legacy system migration topology based on generalisations from a multi-case study as well as extensive literature research. We expect the topology to be useful in connecting industry needs and challenges with current and potential future research and to improve bidirectional accessibility.
Conversational Bot for Newcomers Onboarding to Open Source Projects,"Dominic, James and Houser, Jada and Steinmacher, Igor and Ritter, Charles and Rodeghero, Paige",10.1145/3387940.3391534,2020,"This paper targets the problems newcomers face when onboarding to open source projects and the low retention rate of newcomers. Open source software projects are becoming increasingly more popular. Many major companies have started building open source software. Unfortunately, many newcomers only commit once to an open source project before moving on to another project. Even worse, many novices struggle with joining open source communities and end up leaving quickly, sometimes before their first successful contribution. In this paper, we propose a conversational bot that would recommend projects to newcomers and assist in the onboarding to the open source community. The bot would be able to provide helpful resources, such as Stack Overflow related content. It would also be able to recommend human mentors. We believe that this bot would improve newcomers' experience by providing support not only during their first contribution, but by acting as an agent to engage them to the project."
Chat activity is a better predictor than chat sentiment on software developers productivity,"Kuutila, Miikka and M\~{a}ntyl\~{a}, Mika V. and Claes, Ma\""{e}lick",10.1145/3387940.3392224,2020,"Recent works have proposed that software developers' positive emotion has a positive impact on software developers' productivity. In this paper we investigate two data sources: developers chat messages (from Slack and Hipchat) and source code commits of a single co-located Agile team over 200 working days. Our regression analysis shows that the number of chat messages is the best predictor and predicts productivity measured both in the number of commits and lines of code with R2 of 0.33 and 0.27 respectively. We then add sentiment analysis variables until AIC of our model no longer improves and gets R2 values of 0.37 (commits) and 0.30 (lines of code). Thus, analyzing chat sentiment improves productivity prediction over chat activity alone but the difference is not massive. This work supports the idea that emotional state and productivity are linked in software development. We find that three positive sentiment metrics, but surprisingly also one negative sentiment metric is associated with higher productivity."
A machine learning approach for predicting post-stroke aphasia recovery: a pilot study,"Gu, Yiwen and Bahrani, Murtadha and Billot, Anne and Lai, Sha and Braun, Emily J. and Varkanitsa, Maria and Bighetto, Julia and Rapp, Brenda and Parrish, Todd B. and Caplan, David and Thompson, Cynthia K. and Kiran, Swathi and Betke, Margrit",10.1145/3389189.3389204,2020,"The potential recovery of post-stroke aphasia is highly variable and the rehabilitation outcomes are difficult to predict. This interdisciplinary collaboration builds on data collected as part of a large set of behavioral and brain variables in patients with post-stroke aphasia, charting the course of recovery associated with therapy across language domains and examining the basis of neuroplasticity. In this pilot study, we created and tested a predictive framework based on a subset of the data collected and developed machine-learning algorithms that take as input a complex set of brain and behavioral features to classify and predict the participants' responsiveness to therapy. We developed Random Forest models that enabled us to rank the importance of these features. We then compared the contributions of different feature sets and discussed their physiological implications. Our preliminary results suggest the potential of our framework, and, thus, this study takes an important first step towards predicting individualized rehabilitation outcomes."
Serious games to cognitively stimulate older adults: a systematic literature review,"Palumbo, Vanessa and Patern\`{o}, Fabio",10.1145/3389189.3393739,2020,"With the increasing number of older adults in today's population, it is important to promote active ageing in order to offer them better life conditions, and enable them to be autonomous as long as possible despite the problems arising from aging. To guarantee active aging, physical, social and mental conditions must be considered. In this research we mainly focus on the cognitive functions implicated in aging. To cope with the cognitive impairments, many proposals have been put forward that exploit digital interactive technologies in interdisciplinary projects. An area that has high potential is the use of serious games supported by some interactive technology because of their potential attractiveness and engagement. We report on a systematic literature review that has been carried out in order to explore and analyse the various proposals in this field and, on the basis of the results, identify the interactive devices that have been considered, the results achieved in terms of cognitive support, and current trends in this area."
Unpacking Editorial Agreements in Collaborative Video Production,"Okopnyi, Pavel and Juhlin, Oskar and Guribye, Frode",10.1145/3391614.3393652,2020,"Video production is a collaborative process involving creative, artistic and technical elements that require a multitude of specialised skill sets. This open-ended work is often marked by uncertainty and interpretive flexibility in terms of what the product is and should be. At the same time, most current video production tools are designed for single users. There is a growing interest, both in industry and academia, to design features that support key collaborative processes in editing, such as commenting on videos. We add to current research by unpacking specific forms of collaboration, in particular the social mechanisms and strategies employed to reduce interpretive flexibility and uncertainty in achieving agreements between editors and other collaborators. The findings contribute to the emerging design interest by identifying general design paths for how to support collaboration in video editing through scaffolding, iconic referencing, and suggestive editing."
Developing (Almost) Free Distributed System Labs Using Container-based Technique,"Xu, Zichen and Cheng, Jie and Wang, Yuhao and Rao, Hong",10.1145/3393527.3393545,2020,"Developing successful labs is essential to advanced CS courses. As the fast development on novel programming models and system frameworks, our CSE education calls for a collaborative, community-sourced design effort that maximizes the benefit from effective lab practice. However, building one unified lab platform is expensive, while outsourcing lab works to students faces challenges like infrastructure requirements, heterogeneous operating environments, and the desire to incentivize individual work. To address these challenges, we present our recent efforts on developing, deploying, and analyzing up-to-date distributed system labs based on container-based techniques and open source community supports. Our work uses docker containers that provide a light and consistent environment to support fast lab exercise construction and migration. This enables our students host their own labs with a modest laptop or desktop. Further, We provide a machine learning engine that automatically crawls students' homework images and evaluated with the weighted Adaboost algorithm on elected course topics. As such, we allow to build novel and practical distributed computer labs for (almost) free. We deployed and evaluated our work in a continuous three-year course teaching. The results show that our students provide positive feedback on learning and experiencing complex computer system concepts without increasing the complexity of lab setup or assessment."
A W2VV++ Case Study with Automated and Interactive Text-to-Video Retrieval,"Loko\'{c}, Jakub and Sou\'{c}ek, Tom\'{a}\v{s} and Vesel\'{y}, Patrik and Mejzl\'{\i}k, Franti\v{s}ek and Ji, Jiaqi and Xu, Chaoxi and Li, Xirong",10.1145/3394171.3414002,2020,"As reported by respected evaluation campaigns focusing both on automated and interactive video search approaches, deep learning started to dominate the video retrieval area. However, the results are still not satisfactory for many types of search tasks focusing on high recall. To report on this challenging problem, we present two orthogonal task-based performance studies centered around the state-of-the-art W2VV++ query representation learning model for video retrieval. First, an ablation study is presented to investigate which components of the model are effective in two types of benchmark tasks focusing on high recall. Second, interactive search scenarios from the Video Browser Showdown are analyzed for two winning prototype systems implementing a selected variant of the model and providing additional querying and visualization components. The analysis of collected logs demonstrates that even with the state-of-the-art text search video retrieval model, it is still auspicious to integrate users into the search process for task types, where high recall is essential."
Design Space Exploration of Heterogeneous-Accelerator SoCs with Hyperparameter Optimization,"Cong, Thanh and Charot, Fran\c{c}ois",10.1145/3394885.3431415,2021,"Modern SoC systems consist of general-purpose processor cores augmented with large numbers of specialized accelerators. Building such systems requires a design flow allowing the design space to be explored at the system level with an appropriate strategy. In this paper, we describe a methodology allowing to explore the design space of power-performance heterogeneous SoCs by combining an architecture simulator (gem5-Aladdin) and a hyperparameter optimization method (Hyperopt). This methodology allows different types of parallelism with loop unrolling strategies and memory coherency interfaces to be swept. The flow has been applied to a convolutional neural network algorithm. We show that the most energy efficient architecture achieves a 2x to 4x improvement in energy-delay-product compared to an architecture without parallelism. Furthermore, the obtained solution is more efficient than commonly implemented architectures (Systolic, 2D-mapping, and Tiling). We also applied the methodology to find the optimal architecture including its coherency interface for a complex SoC made up of six accelerated-workloads. We show that a hybrid interface appears to be the most efficient; it reaches 22% and 12% improvement in energy-delay-product compared to just only using non-coherent and only LLC-coherent models, respectively."
Innovation and Application of College Students' Education and Management Based on Big Data,"Chen, Chongyang and Xu, Wei",10.1145/3396452.3396464,2020,"With the rapid development and popularization of big data technology in the world and the world's vigorous promotion of informatization, the education and management of college students are facing new opportunities and challenges. This paper takes the innovation and application of big data technology in college students' education and management as the core, analyzes the significance and advantages of big data in the education and management of college students, and deeply studies the innovative application of big data from the aspects of education and management resource sharing, digital education and management and open education and management. In addition, It will take the innovative application of big data in college information announcement system as an example to comprehensively improve the level of college students' education and management by big data mining in the system and other ways."
VR-Participation &amp; Dialogue: Towards Integrated Framework for Virtual Reality-Mediated Consensus and Community Building*,"Porwol, Lukasz and Ojo, Adegboyega",10.1145/3396956.3398259,2020,"Successful e-Participation requires a thriving community of users-citizens who engage and collaborate with governments and decision makers on key democratic and social maters. Effective community building and meaningful social interactions are contingent on strong, organic consensus achieved through engaging dialogue rather than discussions or argumentation.The emerging social Virtual Reality platforms offer new means of immersive communication that brings an opportunity to overcome some of the challenges identified to be hindering state-of-the-art e-Participation from supporting constructive citizen-to-government dialogue. In this paper we investigate the key concepts and explore the principles of dialogue and consensus building in the context of e-Participation. We match those principles with specific VR affordances and propose an Integrative Framework for Virtual-Reality-Mediated Consensus and Community Building. Finally, we discuss the application of the framework to e-Participation."
Learning with Weak Supervision for Email Intent Detection,"Shu, Kai and Mukherjee, Subhabrata and Zheng, Guoqing and Awadallah, Ahmed Hassan and Shokouhi, Milad and Dumais, Susan",10.1145/3397271.3401121,2020,"Email remains one of the most frequently used means of online communication. People spend significant amount of time every day on emails to exchange information, manage tasks and schedule events. Previous work has studied different ways for improving email productivity by prioritizing emails, suggesting automatic replies or identifying intents to recommend appropriate actions. The problem has been mostly posed as a supervised learning problem where models of different complexities were proposed to classify an email message into a predefined taxonomy of intents or classes. The need for labeled data has always been one of the largest bottlenecks in training supervised models. This is especially the case for many real-world tasks, such as email intent classification, where large scale annotated examples are either hard to acquire or unavailable due to privacy or data access constraints. Email users often take actions in response to intents expressed in an email (e.g., setting up a meeting in response to an email with a scheduling request). Such actions can be inferred from user interaction logs. In this paper, we propose to leverage user actions as a source of weak supervision, in addition to a limited set of annotated examples, to detect intents in emails. We develop an end-to-end robust deep neural network model for email intent identification that leverages both clean annotated data and noisy weak supervision along with a self-paced learning mechanism. Extensive experiments on three different intent detection tasks show that our approach can effectively leverage the weakly supervised data to improve intent detection in emails."
XAlgo: a Design Probe of Explaining Algorithms’ Internal States via Question-Answering,"Rebanal, Juan and Combitsis, Jordan and Tang, Yuqi and Chen, Xiang 'Anthony'",10.1145/3397481.3450676,2021,"Algorithms often appear as ’black boxes’ to non-expert users. While prior work focuses on explainable representations and expert-oriented exploration, we propose and study an interactive approach using question answering to explain deterministic algorithms to non-expert users who need to understand the algorithms’ internal states (students learning algorithms, operators monitoring robots, admins troubleshooting network routing). We construct XAlgo—a formal model that first classifies the type of question based on a taxonomy and generates an answer based on a set of rules that extract information from representations of an algorithm’s internal states, the pseudocode. A design probe based on an algorithm learning scenario with 18 participants (9 for a Wizard-of-Oz XAlgo and 9 as a control group) reports findings and design implications based on what kinds of questions people ask, how well XAlgo responds, and what remain as challenges to bridge users’ gulf of algorithm understanding."
Developments in MLflow: A System to Accelerate the Machine Learning Lifecycle,"Chen, Andrew and Chow, Andy and Davidson, Aaron and DCunha, Arjun and Ghodsi, Ali and Hong, Sue Ann and Konwinski, Andy and Mewald, Clemens and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and Singh, Avesh and Xie, Fen and Zaharia, Matei and Zang, Richard and Zheng, Juntai and Zumar, Corey",10.1145/3399579.3399867,2020,"MLflow is a popular open source platform for managing ML development, including experiment tracking, reproducibility, and deployment. In this paper, we discuss user feedback collected since MLflow was launched in 2018, as well as three major features we have introduced in response to this feedback: a Model Registry for collaborative model management and review, tools for simplifying ML code instrumentation, and experiment analytics functions for extracting insights from millions of ML experiments."
Contributions to openroad from abroad: experiences and learnings,"Foga\c{c}a, Mateus and Monteiro, Eder and Danigno, Marcelo and Oliveira, Isadora and Butzen, Paulo F. and Reis, Ricardo",10.1145/3400302.3415737,2020,"The OpenROAD project is an ambitious initiative seeking to develop an automated, open-source RTL-to-GDSII flow. To build its complex toolset, OpenROAD brings together a team of industry experts, veteran scholars, and enthusiastic students from different schools and different countries. This paper first presents our path to becoming OpenROAD contributors, highlighting the nature of the OpenROAD project, the recruitment process, and the necessary logistics. We then summarize the contributions of the Brazilian team to the OpenROAD project; these comprise the development of five tools and more than 10K lines of released code, along with authorship or co-authorship of two publications in the research literature. We also summarize our experiences from working in a large software project: (i) working environment and relationship with people from around the world; (ii) task management and short turnaround times; (iii) continuous integration and testing; etc. Finally, we highlight the challenges of ""refurbishing"" academic research codes for use in the design of production ICs."
A Multidimensional Approach of Evaluating Developers,"Zhang, Changqiang and Chen, Ming",10.1145/3404512.3404520,2020,"In this paper, we propose an approach to assess the ability of developers based on their behavior data from OSS. Specifically, we classify developers' ability into code ability, project management ability, and social ability. Code efficiency is related to the developer's commit record and the pull-request record. The developer's project management ability is achieved by tracking the developer's commit record. We use regular matching to map the commit behavior to the project management behavior and calculate the developer's project management ability according to the proportion of different behaviors. The social ability of developers is related to the data that developers interact with in the open-source community. We dug for developer reviews on commit, issue, and gist fragments. By calculating the proportion of positive emotions in developer reviews and the proportion of developers interacting with others in the reviews, the social ability of developers is obtained. We get behavioral data from 50 random developers. Twitter's data is used to test the effect of different machine learning algorithms on the accuracy of developer comment polarity judgments. It is found that the combination of SVM, xgboost and random forest have the highest prediction accuracy. Finally, we select 5 students to use Likert scale to score the results. Our score shows that the results are basically in line with expectations."
A Systematic Review on Software Project Scheduling and Task Assignment Approaches,"Fatima, Taskeen and Azam, Farooque and Anwar, Muhammad Waseem and Rasheed, Yawar",10.1145/3404555.3404588,2020,"Software Project Scheduling and Task Assignment are important integral aspects of software project management and contributes to the overall success of software projects. Key objective of Task scheduling/ assignment is to minimize the cost and time of the project. This article i.e. a systematic literature review, is in-fact the first of its kind, conducted in the context of task scheduling and assignment in software industry. This study specifically elaborates the models used in task assignment and summarizes the techniques/ machine learning algorithms to solve the software project scheduling problem (SPSP). Our Initial search brought out 1100 research articles. However, after applying the inclusion and exclusion criteria, 23 most relevant researches were segregated and thereafter thoroughly reviewed. The review revealed that there are 2 types of basic models of Task Scheduling i.e. static and dynamic, however, static models are most widely used. For Task Scheduling, evolutionary algorithms, whereas, for Task Assignment, Support Vector Machine (SVM) algorithms are most widely used. Due to lack of real-world data in software projects, most of the researches utilized synthetic data sets for Task Assignment. Exploring the Task Assignment tools during the course of review process, 7 tools were identified, however, TAMRI has been graded as most efficient."
"Experiential Learning in Data Science: Developing an Interdisciplinary, Client-Sponsored Capstone Program","Allen, Genevera I.",10.1145/3408877.3432536,2021,"Interest in data science education and degree programs has rapidly expanded over the past several years. An integral part of many degree programs is a capstone experience, where students complete a major research or real-world project at the culmination of their educational program. In engineering and computer science, many have shown that client-sponsored projects lead to better student engagement and improved training. In this paper, we discuss experiences with developing an interdisciplinary, client-sponsored capstone program in data science and machine learning. We show how we set up the capstone program, including how the program is structured, how projects are set up, how the course is managed, how students are assessed, and outline the newly developed capstone curriculum. Finally, we report results from a cohort of students participating in this capstone program and discuss lessons learned as well as best practices when developing data science capstone programs."
"Application of Analytic Hierarchy Process-Fuzzy Comprehensive Evaluation in Public Transport of Ulaanbaatar City, Mongolia","Nasanjargal, Khaliun and Lu, Jing",10.1145/3409073.3409075,2020,"Ulaanbaatar is considered as one of the most congested cities in the world. The public transport system of a city is a great indicator of its relative level of development. This article applies a combined model composed of the Analytic Hierarchy Process method and the Fuzzy Comprehensive Evaluation method to evaluate the current condition of the bus public transport in Ulaanbaatar, Mongolia. The results found that the current condition of bus public transport in Ulaanbaatar is average, with satisfaction factor as the main determining factor of Ulaanbaatar's bus public transport condition."
A Review of Motion Sickness in Automated Vehicles,"Dam, Abhraneil and Jeon, Myounghoon",10.1145/3409118.3475146,2021,"Automated vehicles (AVs) are the next wave of evolution in the transportation industry, but the progress towards increased levels of automation faces several challenges. One of the major problems, that gets overlooked, is motion sickness. As more drivers become passengers engaging in ‘passenger tasks’, it will lead to greater occurrences of motion sickness, preventing AVs from providing their true benefit to society. In an attempt to encourage more researchers to solve the problem of motion sickness in AVs, this study conducted a literature review following the PRISMA framework to identify the latest research trends and methodologies. Based on the findings and limitations in the existing literature, this study suggests a bird's-eye-view research framework consisting of causation, induction, measurement, and mitigation techniques, that researchers and early practitioners can utilize to conduct research in this field. Furthermore, the paper highlights future research directions in mitigation techniques to combat motion sickness in AVs."
Spatial Prediction of Housing Prices in Beijing Using Machine Learning Algorithms,"Yan, Ziyue and Zong, Lu",10.1145/3409501.3409543,2020,"The real estate industry places key influence on almost every aspect of social economy given its great financing capacity and prolonged upstream and downstream industry chain. Therefore, predicting housing prices is regarded as an emerging topic in the recent decades. Hedonic Regression and Machine Learning Algorithms are two main methods in this field. This study aims to explore the important explanatory features and determine an accurate mechanism to implement spatial prediction of housing prices in Beijing by incorporating a list of machine learning techniques, including XGBoost, linear regression, Random Forest Regression, Ridge and Lasso Model, bagging and boosting, based on the housing price and features data in Beijing, China. Our result shows that compared to traditional hedonic method, machine learning methods demonstrate significant improvements on the accuracy of estimation despite that they are more time-costly. Moreover, it is found that XGBoost is the most accurate model in explaining and prediciting the spatial dynamics of housing prices in Beijing."
Automated Question-Answer Medical Model based on Deep Learning Technology,"Abdallah, Abdelrahman and Kasem, Mahmoud and Hamada, Mohamed A. and Sdeek, Shaymaa",10.1145/3410352.3410744,2020,"Artificial intelligence can now provide more solutions for different problems, especially in the medical field. One of those problems is the lack of answers to any given medical/health-related question. The Internet is full of forums that allow people to ask some specific questions and get great answers for them. Nevertheless, browsing these questions to locate a similar case to your own question, also finding a satisfying accurate answer is difficult and timeconsuming task. This research will introduce a solution to these problems by automating the process of generating qualified answers to these questions and creating a kind of digital doctor. Furthermore, this research will train an end-to-end model using the framework of RNN and the encoder decoder to generate sensible and useful answers to a small set of medical/health issues. The proposed model was trained and evaluated using data from various online services, such as WebMD, HealthTap, eHealthForums, and iCliniq."
Artificial intelligence and its impact on the prediction of economic indicators,"Ram\'{\i}rez, Kevin Mero and Hormaza, Jaime Meza and Soto, Sebasti\'{a}n Ventura",10.1145/3410352.3410827,2020,"Economic indicators are key statistics based on economy, some examples of economic indicators are inflation rate, gross domestic product (GDP), unemployment rate, consumer price indices (CPI), interest rate, exports, consumption of energy, among others. Most of the published studies are focused on contextualizing and predict a particular economic indicator without considering the current general situation on how non-linear models have been used in predicting some of the economic indicators. This article, has analyzed in the scientific production the artificial intelligence methods mostly used in the development of prediction models of economic indicators. The study was carried out by means of a systematic literature review (SLR) using the Web of Science (WOS), Scopus and Google Scholar bibliographic databases (BD) as resources. The documents and general information analyzed qualitatively are filtered between the range of years 2015 to 2019 to which an adequate set of quality and selection criteria were applied. The approach of the research questions allowed to describe the outcomes in categories where the studies by predicted economic indicator and applied artificial intelligence method have been successfully included. The outcomes that have been obtained in this article represent a starting point for researchers, academics and professionals who wish to carry out studies related to the prediction of economic indicators using some artificial intelligence (AI) methods. In conclusion, some of the artificial intelligence methods used to predict economic indicators are artificial neural networks (ANN), adaptive systems of diffuse neuro inference (ANFIS), genetic programming (GP), support vector regression (SVR), machines extreme learning and other machine learning (ML) techniques."
Fake News Detection in Social Media: A Systematic Review,"Medeiros, Francisco D.C. and Braga, Reinaldo Bezerra",10.1145/3411564.3411648,2020,"The growth of social networks platforms leverages the consumption of news due to its easy access, spreading behavior, and low cost. However, this revolution in the way that information is released has provided the growth of something that always walked side by side with the real news: we are talking about fake news. After the 2016 U.S. presidential election this term became more popular and dangerous because of its negative effect on society. In this context, recent contributions has appeared addressing several related topics, such as spreading behavior, methods for spreading contention, and fake news detection algorithms. Despite of the growth of this type of research, it is difficult for a researcher to identify the current state-of-the-art literature about fake news detection. To overcome this obstacle, this paper presents a systematic review of the literature that brings an overview of this research area and analyzes the the high-quality studies about fake news detection. Through this systematic literature review, more than 6,000 articles were found according to our search protocol. Then, we put these studies through stages of screening to ensure that they were quality assessed. Were elected 32 high-quality studies according to our PRISMA flow diagram defined in this paper. These studies were then categorized by their contribution type and algorithm. This work shown that Twitter and Weibo1 are the social media platform most applied by selected studies, and deep learning algorithms given the best detection results, specially LSTM. Besides, this SR exposes the lack of research for fake news detection in other language than english. Finally, we expect this study can help researchers identify the greatest contributions as well as research opportunities."
Seeing Beyond Expert Blind Spots: Online Learning Design for Scale and Quality,"Wang, Xu and Rose, Carolyn and Koedinger, Ken",10.1145/3411764.3445045,2021,"Maximizing system scalability and quality are sometimes at odds. This work provides an example showing scalability and quality can be achieved at the same time in instructional design, contrary to what instructors may believe or expect. We situate our study in the education of HCI methods, and provide suggestions to improve active learning within the HCI education community. While designing learning and assessment activities, many instructors face the choice of using open-ended or close-ended activities. Close-ended activities such as multiple-choice questions (MCQs) enable automated feedback to students. However, a survey with 22 HCI professors revealed a belief that MCQs are less valuable than open-ended questions, and thus, using them entails making a quality sacrifice in order to achieve scalability. A study with 178 students produced no evidence to support the teacher belief. This paper indicates more promise than concern in using MCQs for scalable instruction and assessment in at least some HCI domains."
Exploring Generative Models with Middle School Students,"Ali, Safinah and DiPaola, Daniella and Lee, Irene and Hong, Jenna and Breazeal, Cynthia",10.1145/3411764.3445226,2021,"Applications of generative models such as Generative Adversarial Networks (GANs) have made their way to social media platforms that children frequently interact with. While GANs are associated with ethical implications pertaining to children, such as the generation of Deepfakes, there are negligible efforts to educate middle school children about generative AI. In this work, we present a generative models learning trajectory (LT), educational materials, and interactive activities for young learners with a focus on GANs, creation and application of machine-generated media, and its ethical implications. The activities were deployed in four online workshops with 72 students (grades 5-9). We found that these materials enabled children to gain an understanding of what generative models are, their technical components and potential applications, and benefits and harms, while reflecting on their ethical implications. Learning from our findings, we propose an improved learning trajectory for complex socio-technical systems."
A Meta-Analysis of Human Personality and Robot Acceptance in Human-Robot Interaction,"Esterwood, Connor and Essenmacher, Kyle and Yang, Han and Zeng, Fanpan and Robert, Lionel Peter",10.1145/3411764.3445542,2021,"Human personality has been identified as a predictor of robot acceptance in the human–robot interaction (HRI) literature. Despite this, the HRI literature has provided mixed support for this assertion. To better understand the relationship between human personality and robot acceptance, this paper conducts a meta-analysis of 26 studies. Results found a positive relationship between human personality and robot acceptance. However, this relationship varied greatly by the specific personality trait along with the study sample’s age, gender diversity, task, and global region. This meta-analysis also identified gaps in the literature. Namely, additional studies are needed that investigate both the big five personality traits and other personality traits, examine a more diverse age range, and utilize samples from previously unexamined regions of the globe."
Trust in Collaborative Automation in High Stakes Software Engineering Work: A Case Study at NASA,"Widder, David Gray and Dabbish, Laura and Herbsleb, James D. and Holloway, Alexandra and Davidoff, Scott",10.1145/3411764.3445650,2021,"The amount of autonomy in software engineering tools is increasing as developers build increasingly complex systems. We study factors influencing software engineers’ trust in an autonomous tool situated in a high stakes workplace, because research in other contexts shows that too much or too little trust in autonomous tools can have negative consequences. We present the results of a ten week ethnographic case study of engineers collaborating with an autonomous tool to write control software at the National Aeronautics and Space Administration to support high stakes missions. We find that trust in an autonomous software engineering tool in this setting was influenced by four main factors: the tool’s transparency, usability, its social context, and the organization’s associated processes. Our observations lead us to frame trust as a quality the operator places in their collaboration with the automated system, and we outline implications of this framing and other results for researchers studying trust in autonomous systems, designers of software engineering tools, and organizations conducting high stakes work with these tools."
What We Can Learn From Visual Artists About Software Development,"Li, Jingyi and Hashim, Sonia and Jacobs, Jennifer",10.1145/3411764.3445682,2021,"This paper explores software’s role in visual art production by examining how artists use and develop software. We conducted interviews with professional artists who were collaborating with software developers, learning software development, and building and maintaining software. We found artists were motivated to learn software development for intellectual growth and access to technical communities. Artists valued efficient workflows through skilled manual execution and personal software development, but avoided high-level forms of software automation. Artists identified conflicts between their priorities and those of professional developers and computational art communities, which influenced how they used computational aesthetics in their work. These findings contribute to efforts in systems engineering research to integrate end-user programming and creativity support across software and physical media, suggesting opportunities for artists as collaborators. Artists’ experiences writing software can guide technical implementations of domain-specific representations, and their experiences in interdisciplinary production can aid inclusive community building around computational tools."
Planning for Natural Language Failures with the AI Playbook,"Hong, Matthew K. and Fourney, Adam and DeBellis, Derek and Amershi, Saleema",10.1145/3411764.3445735,2021,"Prototyping AI user experiences is challenging due in part to probabilistic AI models making it difficult to anticipate, test, and mitigate AI failures before deployment. In this work, we set out to support practitioners with early AI prototyping, with a focus on natural language (NL)-based technologies. Our interviews with 12 NL practitioners from a large technology company revealed that, in addition to challenges prototyping AI, prototyping was often not happening at all or focused only on idealized scenarios due to a lack of tools and tight timelines. These findings informed our design of the AI Playbook, an interactive and low-cost tool we developed to encourage proactive and systematic consideration of AI errors before deployment. Our evaluation of the AI Playbook demonstrates its potential to 1) encourage product teams to prioritize both ideal and failure scenarios, 2) standardize the articulation of AI failures from a user experience perspective, and 3) act as a boundary object between user experience designers, data scientists, and engineers."
Adapting to online teaching in software engineering courses,"Motogna, Simona and Marcus, Andrian and Molnar, Arthur-Jozsef",10.1145/3412453.3423194,2020,"The COVID-19 worldwide pandemic caused sudden and unexpected changes in how we teach software engineering and other university courses. This paper presents an empirical study that aims to improve our understanding on how the assessment of student learning changed, in response to the transition from in-class to online courses. A questionnaire was distributed to instructors across the globe. The results indicate that the evaluation methodologies for most reported learning objectives have changed. Not surprising, in-class oral presentations and in-class exams are no longer used by the instructors for evaluations. We observed a trend of having fewer exams and more project-related evaluations after the transition. Not all instructors changed the way they evaluated student learning after the transition, however the majority reported their effort in student learning assessment increased after the transition, whether they made changes in methodologies or not."
An analysis of Monte Carlo simulations for forecasting software projects,"Miranda, Pedro and Faria, J. Pascoal and Correia, Filipe F. and Fares, Ahmed and Gra\c{c}a, Ricardo and Moreira, Jo\~{a}o Mendes",10.1145/3412841.3442030,2021,"Forecasts of the effort or delivery date can play an important role in managing software projects, but the estimates provided by development teams are often inaccurate and time-consuming to produce. This is not surprising given the uncertainty that underlies this activity. This work studies the use of Monte Carlo simulations for generating forecasts based on project historical data. We have designed and run experiments comparing these forecasts against what happened in practice and to estimates provided by developers, when available. Comparisons were made based on the mean magnitude of relative error (MMRE). We did also analyze how the forecasting accuracy varies with the amount of work to be forecasted and the amount of historical data used. To minimize the requirements on input data, delivery date forecasts for a set of user stories were computed based on takt time of past stories (time elapsed between the completion of consecutive stories); effort forecasts were computed based on full-time equivalent (FTE) hours allocated to the implementation of past stories. The MMRE of delivery date forecasting was 32% in a set of 10 runs (for different projects) of Monte Carlo simulation based on takt time. The MMRE of effort forecasting was 20% in a set of 5 runs of Monte Carlo simulation based on FTE allocation, much smaller than the MMRE of 134% of developers' estimates. A better forecasting accuracy was obtained when the number of historical data points was 20 or higher. These results suggest that Monte Carlo simulations may be used in practice for delivery date and effort forecasting in agile projects, after a few initial sprints."
Enhancing Decision Making with Deep Reinforcement Learning in a Context of Novel Coronavirus Outbreak: an Example in Emergency Department,"Jiang, Hongwei and Wang, William Yu Chung and Goh, Tiong-Thye and Zhu, Jie",10.1145/3418094.3418137,2020,"Physicians in hospitals are expected to improve treatment outcome and reduce health care costs. Information systems have been widely adopted in hospitals but not been properly integrated to provide information for decision support. The objective of this research is trying to validate the feasibility of enhancing hospital resource planning system in decision support by utilizing data stored in multiple systems in the hospital with a deep reinforcement learning approach to assist medical practitioner making a more accurate and efficient decision. Following the Design Science Research Method, this research is going to build an artefact to utilize data from electronic health record (EHR) and hospital resource planning (HRP) to provide medical decision support in the emergency department setting."
A Semantic Based Annotation Technique for the Internet of Things,"Alaa El Din Talha, Shorouk",10.1145/3418688.3418696,2020,"Due to the recent deployments of Internet of Things (IoT) technologies in many real-life applications, enormous amount of diverse and real-time streams of data are being generated. To facilitate dealing with the heterogeneity of IoT data streams, semantic technologies became the main element to guarantee data interoperability, with its nature to unify concepts, extend knowledge, and share a machine-readable representation of data. In this paper, we propose an adaptable approach for IoT data semantic annotation, to achieve an efficient way to enrich data semantically considering its heterogeneity, volume, and frequency. A use case is implemented using Apache Kafka, Spark to deal with data streams in real-time, and a Semantic ontology model extending the SOSA standard ontology is developed to annotate data into enriched Resource Description Framework (RDF) triples."
A Systematic Assessment of National Artificial Intelligence Policies: Perspectives from the Nordics and Beyond,"van Berkel, Niels and Papachristos, Eleftherios and Giachanou, Anastasia and Hosio, Simo and Skov, Mikael B.",10.1145/3419249.3420106,2020,"Echoing the evolving interest and impact of artificial intelligence on society, governments are increasingly looking for ways to strategically position themselves as both innovators and regulators in this new domain. One of the most explicit and accessible ways in which governments outline these plans is through national strategy and policy documents. We follow a systematic search strategy to identify national AI policy documents across twenty-five countries. Through an analysis of these documents, including topic modelling, clustering, and reverse topic-search, we provide an overview of the topics discussed in national AI policies and contrast the differences between countries. Furthermore, we analyse the frequency of eleven ethical principles across our corpus. Our paper outlines implications of the differences between geographical and cultural clusters in relation to the future development of artificial intelligence applications."
Business Process Modelling Augmented: Model Driven transformation of User Stories to Processes,"Ba\""{\i}na, Karim and El Hamlaoui, Mahmoud and Kabbaj, Hibatallah",10.1145/3419604.3419793,2020,"The Purpose of our paper is to present a lightweight efficient approach for analysing user stories backlog in order to generate a business process model. A review of literature has been conducted to study contributions in the domain of automatic business process extraction from textual requirements. We found that most of interesting approaches analysing user stories use natural language processing techniques for software projects requirements understanding, and none of them target business process modeling automation. The Originality of our contribution is the proposition of a model driven based parsing of user stories backlog and transformations to generate a process model. This work thus contributes with a novel agile iterative methodology augmenting business process design phase with automation assistant transforming user stories textual requirements into a business process model."
A Review of Open Source Software Maintenance Effort Estimation,"Miloudi, Chaymae and Cheikhi, Laila and Idri, Ali",10.1145/3419604.3419809,2020,"Open Source Software (OSS) is gaining interests of software engineering community as well as practitioners from industry with the growth of the internet. Studies in estimating maintenance effort (MEE) of such software product have been published in the literature in order to provide better estimation. The aim of this study is to provide a review of studies related to maintenance effort estimation for open source software (OSSMEE). To this end, a set of 60 primary empirical studies are selected from six electronic databases and a discussion is provided according to eight research questions (RQs) related to: publication year, publication source, datasets (OSS projects), metrics (independent variables), techniques, maintenance effort (dependent variable), validation methods, and accuracy criteria used in the empirical validation. This study has found that popular OSS projects have been used, Linear Regression, Na\""{\i}ve Bayes and k Nearest Neighbors were frequently used, and bug resolution was the most used regarding the estimation of maintenance effort for the future releases. A set of gaps are identified and recommendations for researchers are also provided."
Mitigating Wild Animals Poaching Through State-of-the-art Multimedia Data Mining Techniques: A Review,"Bakana, Sibusiso R. and Zhang, Yongfei and Twala, Bhekisipho",10.1145/3421558.3421584,2020,"Wild animal poaching, particular rhinos, and elephants in Africa, is a serious destruction for biodiversity and eco-tourism. Governments and numerous Non – Government Organizations (NGOs) spent a great amount of human labor and money every year in preventing poaching. Recently, advanced techniques, like intelligent video surveillance and multimedia data mining, have been adopted to help more efficiently mitigate wild animals poaching. In this paper, we provide a detailed review of the state-of-the-art video surveillance and multimedia data mining techniques for mitigating wild animal poaching from four aspects according to processing steps, namely object detection, object classification, object behavior analysis and invader analysis. More specifically, different algorithms in each aspect are further subdivided into sub-categories and compared in terms of pros, cons, efficiency, and complexity. While these techniques have been thoroughly researched separately, such topics have not been superimposed in the paradigm of wild animals poaching.&nbsp;To the best of our knowledge, this is the first such comprehensive review of the recent advances of the intelligent video understanding and multimedia data mining for mitigating wild animals poaching and hopefully it would help the improvement, implementation, and applications of advanced techniques in preventing wild animal poaching and protecting diverse especially endangered species for the one and only one home for us human being."
Sentiment Polarity of Programmers in an Open Source Software Project: An Exploratory Study,"Carig\'{e}, Rui Santos and de Figueiredo Carneiro, Glauco",10.1145/3422392.3422401,2020,"Context: During the implementation of issues filed in open source software projects, programmers engage and interact in discussions on how to implement them. These discussions provide evidence to investigate emotionally loaded practices embraced by programmers. They interact to explain their point of view regarding the project and the issue under analysis. Objective: Analyze programmers sentiment polarity in an open source software project having releases as a reference for the analysis. Methods: We conducted an exploratory study to characterize the sentiment polarity of comments registered in issues associated with releases of the Moby open source software project. Results: The quantitative analysis identified sentiment polarity variations throughout consecutive releases in line with specific functionalities. Based on a qualitative analysis, we identified these functionalities and specific group of programmers that contributed to those results. Conclusions: We identified initial evidence to contribute for the understanding of the causes underlying the influence of the sentiments of the developers in the context of releases of open source software projects."
Achieving Scalability in Project Based Learning through a Low-Code platform,"Fernandes, Jo\~{a}o Paulo and Ara\'{u}jo, Ricardo and Zenha-Rela, M\'{a}rio",10.1145/3422392.3422482,2020,"Defining an adequate project for a Software Engineering course is a challenging endeavour. Such a project must simulate as faithfully as possible a real industrial project, while accounting, e.g., for: i) the natural lack of experience of the students, ii) their constraints to full-time dedication, and iii) reasonable effort required from the instructors. Additionally, while having a real client from industry may contribute to a more realistic experience, the project itself must be challenging enough to motivate the client while still not unduly burden the students.We report on our experience and share our insights from adopting a state-of-the-art low-code software development platform as the core technology for project-based learning -with a real clientin a one-semester software engineering course. We had to handle i) a large class (200+ students), while providing ii) individual assessment iii) for students from very different backgrounds (majoring in three different topics). While we believe i) and ii) are recurrent, iii) poses a significant challenge in the establishment of a fair pedagogic context.We assess the merit of the experience taking as proxy: i) the students' individual and group performance, assessed both by the instructors and the client, and ii) the results of the course's standard institutional pedagogical survey. We have found evidence that the designed project created an even playing field for students from different backgrounds, while being manageable for the instructors and rewarding for the client."
Contributions to improve the combined selection of concurrent software testing techniques,"Santos, Italo and Furlanetti, Andre B. P. and Melo, Silvana M. and de Souza, Paulo Sergio Lopes and Delamaro, M\'{a}rcio E. and Souza, Simone R. S.",10.1145/3425174.3425214,2020,"[Context] There are a variety of testing techniques available that present different and often complementary characteristics (e.g., cost of application, effectiveness in revealing defects, types of defects). Considering these complementary characteristics make the selection process even more complex. Testers must make decisions on which techniques they will use in a specific situation. Combining different testing techniques outperforms the use of any single technique alone. [Objective] This paper proposes an approach to support the combined selection of testing techniques for concurrent software projects. The approach is implemented in the SeleCTT-v2 tool, supporting testers to find complementary testing techniques in the body of knowledge proposed in SeleCTT-v1. [Method] We conducted a case study to evaluate the combined selection approach (SeleCTT-v2) with the previous version of the SeleCTT-v1 and to investigate how testers perform a combined selection task for concurrent software projects. [Results and Conclusions] The results indicated a rise in the effectiveness of the combined selection of concurrent testing techniques suggested, which demonstrates SeleCTT-v2 is less likely to make an incorrect recommendation of combined techniques for a concurrent software project, thus avoiding costs associated with the incorrect application of a testing technique. If the testers could access a tool that supports the selection process, the effectiveness of the results would have reached a higher value, as evidenced by our approach. Performing the combination of testing techniques is essential to ensure that software under test has all their features tested to prevent possible errors."
Complex Online Material Development in CS Courses,"Haaranen, Lassi and Mariani, Giacomo and Sormunen, Peter and Lehtinen, Teemu",10.1145/3428029.3428053,2020,"Computer Science (CS) education has a tradition of using online materials to teach courses, whether as a part of a course or having fully online courses. Commonly, the materials to present the subject matter are not just static objects like text or images but also contain automatically assessed exercises and other interactive content. This makes the course systems inherently tied to teaching – limiting pedagogical approaches, types of exercises, and available functionality. Increasingly, CS courses utilize multiple systems to handle various learning and course management related activities. The use of multiple systems brings about traditional software engineering problems, such as development, integration, maintenance, and testing. We present two small studies (case study and interviews) to highlight the issues in developing and running modern online CS courses. Based on these two studies we argue that online courses should be developed with a stronger software engineering approach considering the development process and tools. In addition, we define the term Complex Online Learning Material (COLM) to foster discussion and further research into improving instructor practices in online education."
A Syntactic and Semantic Assessment of a Global Software Engineering Domain Ontology,"Rocha, Rodrigo and Bion, Danillo and Azevedo, Ryan and Gomes, Arthur and Cordeiro, Diogo and Leandro, Renan and Silva, Israel and Freitas, Fred",10.1145/3428757.3429143,2021,"Globalization has allowed organizations to intensify the search for solutions that minimize challenges, reduce costs and optimize processes. In this way, global software development has emerged as an attempt to use the best resources for its limitations.In distributed environments, the use of Ontologies brings some benefits such as a uniform understanding of information among teams and ease of communication, as well as making for the lack of a reference model that can be applied in a distributed context.This work aims to propose a viable form of validation for DKDonto a domain ontology developed for Global Software Engineering. The validation allowed a broader and more targeted assessment, different from its original validation, which was carried out in a controlled environment, limited to answering questions already known by the knowledge base itself.The main result of this work is a satisfactory evaluation of the ontology, enabling it to be used and shared by companies or institutions, as well as the presentation of a set of methods and ways to evaluate and verify domain ontologies to be used in different domains."
The Many Faces of Patterns in the Internet of Things,"Sithole, Vusi and Marshall, Linda",10.1145/3429523.3429527,2020,"In recent years, a lot of attention has been given to patterns in the Internet of Things (IoT). In essence, these patterns provide well-known ways to solve problems commonly encountered in the IoT paradigm. However, despite this recognition, conceptualising patterns in the IoT has proven to be difficult. This is mainly because IoT patterns come in many shapes, forms and sizes, and can be abstracted from many levels of the system architecture. In fact, many patterns in the IoT are system-independent and can be abstracted from other fundamental aspects that make up the IoT. This seemingly broad variations of patterns has led to some confusion regarding what constitute a pattern in the IoT. This has raised many unanswered questions in the IoT pattern community, such as (i) if patterns are not only about objects, and if they reach far beyond software architecture, then what is an IoT pattern?, and (ii) what are the solution categories in which the underlying IoT patterns can be abstracted? With these questions in mind, we set out to perform a deeper analysis of the meaning of the word pattern in the context of the IoT. We discovered that the IoT patterns give us a standardised vocabulary to talk about structures larger than objects, modules and procedures. That is, patterns are, in fact, a literature that is ingrained in problem solving and design thinking in the holistic IoT domain."
Manage Assurance for Continuous Improvement Strategy in Rural Internet Service Case Study,"Soetomo, Mohammad Amin and Budiarto, Eka and Wibisono, Nugroho and Marzuki",10.1145/3429789.3429833,2020,"ACM ISBN 978-1-4503-8771-2 The ultimate objective the research is to develop a recommendation guideline to manage assurance of the Internet service in rural areas, using a study case of service company with Internet product VSAT Quota Broadband Internet as the basis. To create such a guideline, the key factors for the improvement strategy have to be identified. This identification is based on qualitative research, assessment, gap and binary pass/fail analysis of the current situation especially in system availability, responsiveness to restore Internet service due to system incidents, and service quality using study case in service company VSAT Quota Broadband Internet operation. The analysis is done with respect to COBIT 2019, the major framework in IT management and governance. As a result of the current research, it is found out that good management of service request and incidence resolution is a key parameter to manage assurance. This result of framework analysis is verified by interview result of the key management persons of IT services for Internet provider in rural areas in Indonesia. Areas which still need improvements are also identified."
Survey of treemap layout algorithms,"Scheibel, Willy and Limberger, Daniel and D\""{o}llner, J\""{u}rgen",10.1145/3430036.3430041,2020,"This paper provides an overview of published treemap layout algorithms from 1991 to 2019 that were used for information visualization and computational geometry. First, a terminology is outlined for the precise communication of tree-structured data and layouting processes. Second, an overview and classification of layout algorithms is presented and application areas are discussed. Third, the use-case-specific adaption process is outlined and discussed. This overview targets practitioners and researchers by providing a starting point for own research, visualization design, and applications."
Rapid Entry into Masters in Computing Program for Non-Majors,"Krenz, Gary and Kaczmarek, Thomas and Moyer, John",10.1145/3430665.3456365,2021,"The COSMIC: Change Opportunity - Start Masters in Computing graduate curriculum initiative strives to provide a rapid entry pathway to a professional Master of Science (MS) degree for individuals who do not have an undergraduate degree in computing, but who wish to cross over to a career in the computing field. The goal of our curriculum is to minimize the time students spend preparing for graduate study and maximize experiences relevant for work after graduation. The COSMIC curriculum initiative is similar in concept to other post-baccalaureate conversion programs. However, customization of the COSMIC bridge course and curriculum pathway makes it possible for conversion students to complete the bridge course in summer, then move directly to standard graduate program courses in fall. The highly focused bridge course includes learning to program in two popular high-level languages, abstract data structures, professional practices and various computing concepts that prepare students for the rigors of the MS program. To recruit low-income students from populations underrepresented in the computing field, federal funding and institutional support was obtained. The funding provides low-income students with financial support that enables them to complete the degree in two years. With projected computing talent shortfalls and the advantages of workforce diversity, it is vital that educational institutions create conversion programs that can be completed relatively quickly, with the needs of non-traditional students in mind."
Bringing Green Software to Computer Science Curriculum: Perspectives from Researchers and Educators,"Saraiva, Jo\~{a}o and Zong, Ziliang and Pereira, Rui",10.1145/3430665.3456386,2021,"Only recently has the software engineering community started conducting research on developing energy efficient software, or green software. This is shadowed when compared to the research already produced in the computer hardware community. While research in green software is rapidly increasing, several recent studies with software engineers show that they still miss techniques, knowledge, and tools to develop greener software. Indeed, all such studies suggest that green software should be part of a modern Computer Science Curriculum.In this paper, we present survey results from both researchers' and educators' perspective on green software education. These surveys confirm the lack of courses and educational material for teaching green software in current higher education. Additionally, we highlight three key pedagogical challenges in bringing green software to computer science curriculum and discussed existing solutions to address these key challenges. We firmly believe that 'green thinking"" and the broad adoption of green software in computer science curriculum can greatly benefit our environment, society, and students in an era where software is everywhere and evolves in an unprecedented speed."
A Machine Learning Approach for Suggesting Feedback in Textual Exercises in Large Courses,"Bernius, Jan Philip and Krusche, Stephan and Bruegge, Bernd",10.1145/3430895.3460135,2021,"Open-ended textual exercises facilitate the comprehension of problem-solving skills. Students can learn from their mistakes when teachers provide individual feedback. However, courses with hundreds of students cause a heavy workload for teachers: providing individual feedback is mostly a manual, repetitive, and time-consuming activity.This paper presents CoFee, a machine learning approach designed to suggest computer-aided feedback in open-ended textual exercises. The approach uses topic modeling to split student answers into text segments and language embeddings to transform these segments. It then applies clustering to group the text segments by similarity so that the same feedback can be applied to all segments within the same cluster.We implemented this approach in a reference implementation called Athene and integrated it into Artemis. We used Athene to review 17 textual exercises in two large courses at the Technical University of Munich with 2,300 registered students and 53 teachers. On average, Athene suggested feedback for 26% of the submissions. Accordingly, 85% of these suggestions were accepted by the teachers, 5% were extended with a comment and then accepted, and 10% were changed."
The Evolution and Emerging Trends of Cloud Computing Adoption Research: Visual Analysis of CiteSpace Based on WOS Papers,"Zhang, Ge and Chen, Yun and Li, Gaoyong",10.1145/3432291.3433641,2020,"With the continuous promotion and application of cloud computing, the issue of its adoption has increasingly attracted widespread attention from the theoretical and practical circles at home and abroad. Therefore, it is of great significance to summarize and analyze the research status and hotspots of cloud computing adoption and promote the subsequent research on cloud computing adoption. This paper uses CiteSpace, a visualized literature analysis software, to draw core authors, core journals, key words cluster and other knowledge maps in the field of cloud computing adoption based on 668 documents related to cloud computing adoption from 2011 to 2019 included in the Web of Science database. The research status and trend of cloud computing adoption are visualized and analyzed. The results of the study indicate that the attention paid to cloud computing adoption is gradually increasing. It shows that the research on the security, risk and other technical characteristics of cloud computing, the research on the influencing factors of cloud computing adoption and user behavior, and the research on the cost and performance of cloud computing are the three main research contents in the field of cloud computing adoption. The research hotspots in the field of cloud computing adoption have experienced the evolution process from specific research issues such as cloud computing business and technology, to the application and implementation of cloud computing in enterprises, to the integration of cloud computing and other emerging technologies."
Children as Robot Designers,"Alves-Oliveira, Patr\'{\i}cia and Arriaga, Patr\'{\i}cia and Paiva, Ana and Hoffman, Guy",10.1145/3434073.3444650,2021,"We present the design process of the robot YOLO aimed at stimulating creativity in children. This robot was developed under a human-centered design approach with participatory design practices during two years and involving 142 children as active contributors at all design stages. The main contribution of this work is the development of methods and tools for child-centered robot design. We adapted existing participatory design practices used with adults to fit children's development stages. We followed the Double-Diamond Design Process Model and rested the design process of the robot on the following principles: low floor and wide walls, creativity provocations, open-ended playfulness, and disappointment avoidance through abstraction. The final product is a social robot designed for and with children. Our results show that YOLO increases their creativity during play, demonstrating a successful robot design project. We identified several guidelines that made the design process successful: the use of toys as tools, playgrounds as spaces, the emphasis of playfulness for child expression, and child policies as allies for design studies. The design process described empowers children's in the design of robots."
Educational Robotics initiatives in Namibia and worldwide,"Shipepe, Annastasia and Jormanainen, Ilkka and Sutinen, Erkki",10.1145/3434780.3436675,2021,"Various educational robotics (ER) initiatives have been carried out worldwide to motivate learners at different levels to learn by working with robotics equipment. A data extraction form was used in this study to review ten ER projects and initiatives, published between 2018 and 2020. A survey was also carried out to gather data regarding electronics and programming in primary schools’ curriculum in Namibia. The articles describing the projects were reviewed based on selected criteria. The aim of this study was to do a systematic review on different ER initiatives and see how the approaches used could be employed to the Namibian context. This review indicated positive results on ER initiatives carried out worldwide."
Software Project Management Effectiveness Evaluation Tool,"Barghoth, Mohamed Ellithey and Salah, Akram and Ismail, Manal A",10.1145/3436829.3436844,2021,"Project management is important for the success of a software project. Enhancing software project management effectiveness measurements should lead the project manager to advance practices that contribute to the successful development of software projects. This study aims to enhance project management effectiveness measurements in the direction of increase the likelihood of software project success. An enhanced evaluation model of software project management effectiveness has been proposed in this study. Correspondingly, the model fed into a developed software project management effectiveness evaluation tool. This measurement tool helps software development managers to evaluate, monitor and improve project management effectiveness in software projects. The developed software project management effectiveness evaluation tool has been validated by a feedback survey. Feedback survey participants have been demonstrated the importance of the developed project management effectiveness measurement tool."
Framework to Measure Agile Software Process Effectiveness in Critical Systems Development,"Mounir, Mourad and Salah, Akram and Kamel, Amr and Moussa, Hanan",10.1145/3436829.3436853,2021,"Adopting Agile development life cycle to develop critical systems was investigated in previous research, however the effectiveness of software development process was not systematically measured in the development of critical systems. In this paper, a measurement framework is proposed to evaluate the effectiveness of the Agile software development process and culture when adopted to develop a critical system. The research also discusses how to apply the proposed framework on different case studies."
Adoption of Case Tools &amp; UML: A Local Study,"Ashour, Osama Ibraheem Ashour and Pusatli, Tolga",10.1145/3436829.3436856,2021,"This research investigates the role of CASE tools in the software development process by considering importance of CASE in adopting a number of standards. A survey was conducted in six selected companies in Ankara in which semi-structured interviews are conducted with information system managers and developers as key informants. As results, CASE and UML are reported as being considerably used in Ankara with CASE tools being used for project members' communication and documentations. Furthermore, CASE tools have been used for some companies for documentation as parts of contracts and to follow standards such as ISO and CMMI. CASE tools are used to facilitate project developments, especially when enforced by the standards and methodologies required in the contracts."
High Performance Computing Education: Current Challenges and Future Directions,"Raj, Rajendra K. and Romanowski, Carol J. and Impagliazzo, John and Aly, Sherif G. and Becker, Brett A. and Chen, Juan and Ghafoor, Sheikh and Giacaman, Nasser and Gordon, Steven I. and Izu, Cruz and Rahimi, Shahram and Robson, Michael P. and Thota, Neena",10.1145/3437800.3439203,2020,"High Performance Computing (HPC) is the ability to process data and perform complex calculations at extremely high speeds. Current HPC platforms can achieve calculations on the order of quadrillions of calculations per second, with quintillions on the horizon. The past three decades witnessed a vast increase in the use of HPC across different scientific, engineering, and business communities on problems such as sequencing the genome, predicting climate changes, designing modern aerodynamics, or establishing customer preferences. Although HPC has been well incorporated into science curricula such as bioinformatics, the same cannot be said for most computing programs. Computing educators are only now beginning to recognize the need for HPC Education (HPCEd).  Building on earlier work, this working group explored how HPCEd can make inroads into computing education, focusing on the undergraduate level. This paper presents the background of HPC and HPCEd, identifies several of the needed core HPC competencies for students, identifies the support needed by educators for HPCEd, and explores the symbiosis between HPCEd and computing education in contemporary areas such as artificial intelligence and data science, as well as how HPCEd can be applied to benefit diverse non-computing domains such as atmospheric science, biological sciences and critical infrastructure protection. Finally, the report makes several recommendations to improve and facilitate HPC education in the future."
Model Augmented Reality Curriculum,"Fominykh, Mikhail and Wild, Fridolin and Klamma, Ralf and Billinghurst, Mark and Costiner, Lisandra S. and Karsakov, Andrey and Mangina, Eleni and Molka-Danielsen, Judith and Pollock, Ian and Preda, Marius and Smolic, Aljosa",10.1145/3437800.3439205,2020,"Augmented Reality (AR) is a rapidly growing field in information and communication technologies, drawing increasing numbers of professionals. Higher education institutions, however, are struggling to keep abreast of its development and to train specialists quickly, providing few courses which sufficiently align with the needs of industry. In addition to this, the field is developing so rapidly that existing courses struggle to keep pace. They also often focus too narrowly on specifics to allow for the building of the formative foundations of AR education. This paper aims to address this need by proposing a blueprint curriculum in Computer Science Education for teaching AR in universities at two levels, foundations and advanced. To begin, we survey the state of the art, identifying common needs and problems in existing courses which focus on AR. We then detail a skills framework comprised of 12 groups of skills suitable to meet industry needs, and built upon it two model lesson plans for a foundation and an advanced course. We conclude with a discussion of assessment techniques and curricular design options of embedding such coursework into existing academic programs and a forecast of the future of this academic field."
Designing Computer Science Competency Statements: A Process and Curriculum Model for the 21st Century,"Clear, Alison and Clear, Tony and Vichare, Abhijat and Charles, Thea and Frezza, Stephen and Gutica, Mirela and Lunt, Barry and Maiorana, Francesco and Pears, Arnold and Pitt, Francois and Riedesel, Charles and Szynkiewicz, Justyna",10.1145/3437800.3439208,2020,"The broadly influential document Computing Curricula 2005 (CC2005) is in the process of being updated through a project called Computing Curricula 2020 (CC2020). CC2020 provides a vision for the future of computing education, including a comprehensive report that contrasts curricular guidelines, and contextualizing those guidelines within the broader landscape of computing education. In the process, a framework of competency-based educational principles has been developed which is closely aligned with other skills and qualifications frameworks. This working group report demonstrates one way in which the transition from current learning-outcomes-based practices to the competency-based practices can be approached. Further, the paper discusses the challenges and insights that have emerged as the learning outcomes for various Knowledge Areas in the CS2013 report were re-expressed in terms of competencies."
Usage of Mobile Technologies for Diseases Inference: A Literature Review,"Ram Khanal, Salik and Reis, Arsenio and Paulino, Dennis and Bhandari, Damodar and Paredes, Hugo and Barroso, Joao",10.1145/3439231.3440618,2021,"The fields of artificial intelligence, knowledge inference, data science, etc. have been deeply studied over time and many theoretical approaches have been developed, including its application to health and diseases inference. The creation of prototype and consumer systems has been restrained by the technology limitations on data acquisition and processing, which has been greatly overcome with the new sensors and mobile devices technologies. So, in this work we go through a literature review of the current state of the art on record to the usage of mobile technologies for diseases inference. The review methodology is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework. The criteria were based on journal articles, prior to 2008, and using the defined keywords. A total of 14 selected articles were analyzed. A general conclusion was attained regarding the current state of maturity of the field, leading to fully functional consumer and professional market products."
Predicting Software Defects with Explainable Machine Learning,"Santos, Geanderson and Figueiredo, Eduardo and Veloso, Adriano and Viggiato, Markos and Ziviani, Nivio",10.1145/3439961.3439979,2021,"Most software systems must evolve to cope with stakeholders’ requirements and fix existing defects. Hence, software defect prediction represents an area of interest in both academia and the software industry. As a result, predicting software defects can help the development team to maintain substantial levels of software quality. For this reason, machine learning models have increased in popularity for software defect prediction and have demonstrated effectiveness in many scenarios. In this paper, we evaluate a machine learning approach for selecting features to predict software module defects. We use a tree boosting algorithm that receives as input a training set comprising records of software features encoding characteristics of each module and outputs whether the corresponding module is defective prone. For nine projects within the widely known NASA data program, we build prediction models from a set of easy-to-compute module features. We then sample this sizable model space by randomly selecting software features to compose each model. This significant number of models allows us to structure our work along model understandability and predictive accuracy. We argue that explaining model predictions is meaningful to provide information to developers on features related to each module defective-prone. We show that (i) features that contribute most to finding the best models may vary depending on the project, and (ii) effective models are highly understandable based on a survey with 40 developers."
Deployment of a Machine Learning System for Predicting Lawsuits Against Power Companies: Lessons Learned from an Agile Testing Experience for Improving Software Quality,"Rivero, Luis and Diniz, Jo\~{a}o and Silva, Giovanni and Borralho, Gabriel and Braz Junior, Geraldo and Paiva, Anselmo and Alves, Erika and Oliveira, Milton",10.1145/3439961.3439991,2021,"The advances in Machine Learning (ML) require software organizations to evolve their development processes in order to improve the quality of ML systems. Within the software development process, the testing stage of an ML system is more critical, considering that it is necessary to add data validation, trained model quality evaluation, and model validation to traditional unit, integration tests and system tests. In this paper, we focus on reporting the lessons learned of using model testing and exploratory testing within the context of the agile development process of an ML system that predicts lawsuits proneness in energy supply companies. Through the development of the project, the SCRUM agile methodology was applied and activities related to the development of the ML model and the development of the end-user application were defined. After the testing process of the ML model, we managed to achieve 93.89 accuracy; 95.58 specificity; 88.84 sensitivity; and 87.09 precision. Furthermore, we focused on the quality of use of the application embedding the ML model, by carrying out exploratory testing. As a result, through several iterations, different types of defects were identified and corrected. Our lessons learned support software engineers willing to develop ML systems that consider both the ML model and the end-user application."
Near and Dear: Designing Relatable VR Agents for Training Games,"Adinolf, Sonam and Wyeth, Peta and Brown, Ross and Simpson, Leonie",10.1145/3441000.3441007,2021,"Agents are becoming increasingly ubiquitous in people’s lives. There is potential for agents to aid scalability of digital learning and training. Research has noted the reduction in personal interactions as such applications reach a broader population. There is a gap in literature with regards to virtual agents as relatable learning companions. This paper presents the findings from three ideation workshops with different expert groups centered around designing playful interactions with a digital agent in Virtual Reality (VR), in the context of cybersecurity training. The workshop discussions were subjected to thematic analysis to extract design insights. The design insights fall into three broad categories: thematic (e.g., different metaphors to translate cybersecurity concepts), stylistic (e.g., less realistic art), and mechanical (e.g., VR gestures). This work contributes to HCI by producing a set of design insights to inform design of agent based VR learning environments."
Generating Stylistic Images by Extending Neural Style Transfer Method,"Patel, Eisha and Krishnan, Sridhar",10.1145/3441233.3441238,2021,"Fine arts have long been considered a reserved mastery for the minority of talented individuals in society. The ability to create paintings using unique visual components such as color, stroke, theme, and other creative aspects is currently beyond the reach of computer algorithms. However, there exist algorithms, which have the capability of imitating an artist's painting style and stamping it on to virtually any image to create a one-of-a-kind piece. This paper introduces the concept of using a convolutional neural network (ConvNet or CNN) to individually separate and recombine the style and content of arbitrary images to generate perceptually striking “art” [2]. Given a content and style image as reference, a pre-trained VGG-16 ConvNet can extract feature maps from various layers. Feature maps hold semantic information about both reference images. Loss functions can be developed for content and style by minimizing the mean-square-error between the feature maps used. These loss functions can be additively combined and optimized to render a stylistic image [6]. This technique is called Neural Style Transfer (NST) originally proposed by Leon Gatys in his 2015 research paper, “A Neural Algorithm of Artistic Style”. This research project attempts to replicate and improve upon the work done by Leon Gatys. The purpose of this research is to experiment using a variety of feature maps and optimizing the loss function to identify visually appealing results. A total variation loss factor is introduced to minimize pixilation and sharpen feature formation. Images generated have been assigned a Mean Opinion Score (MOS) by a group of non-bias individuals to affirm the attractiveness of the results."
Outlining Traceability: A Principle for Operationalizing Accountability in Computing Systems,"Kroll, Joshua A.",10.1145/3442188.3445937,2021,"Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes."
Teaching strategies in software engineering towards industry interview preparedness,"Johnson, William Gregory and Sunderraman, Raj and Bourgeois, Anu G.",10.1145/3442481.3442500,2021,"The Software Engineering (SE) curriculum in undergraduate computer science (CS) education is designed to train students in the process of software and systems development. Traditionally, topics such as software development methodologies, industry nomenclatures, and solution analysis are delivered through lectures and group projects. We propose a novel approach in teaching SE that we call MACROVR: &lt;u&gt;MA&lt;/u&gt;chine learning to select project team members; &lt;u&gt;C&lt;/u&gt;loud technologies required for project control, code versioning, and team communications; &lt;u&gt;RO&lt;/u&gt;tational schedules in Agile/Scrum roles; an individual &lt;u&gt;V&lt;/u&gt;ideo of the team project story board; and &lt;u&gt;R&lt;/u&gt;ubrics for all presentations. Our teaching strategy with this approach utilizes the latest technologies currently employed in industry and corresponds to soft skills commonly assessed in interviews.The goal of our study is to measure if using the MACROVR approach contributes to preparedness for a computing job interview. Most often, this course is taken towards the end of a four-year CS degree program while students are job hunting or seeking an internship in the computing industry. We use an anonymous, fifteen question survey instrument sent to volunteers that indicated they are seeking a computing job and have successfully completed the SE course. The sample is comprised of three sections of the SE course using the MACROVR approach (135 students) and four sections that did not use all of the required strategies and technologies, which we call MACROVR-lite (184 students). Our two cohorts, MACROVR and MACROVR-lite, are each given the same survey questions. We analyze their Likert scale data responses using non-parametric methods. Our findings indicate the MACROVR approach better prepares students with the skills and highly valued qualities for success in computing industry interviews."
"Learning lab ""digital technologies"" keeps distance","G\""{u}nzel, Holger and Brehm, Lars and Humpe, Andreas",10.1145/3442481.3442506,2021,"Over the last years the Learning Lab ""Digital Technologies"" has been developed as a growing platform for the education of digital technologies in the context of university teaching. It significantly increases the motivation and involvement of students through active and collaborative learning in combination with a haptic experience. Unfortunately, in the summer semester 2020 the concept of the haptic experience was suspended due to the corona pandemic and the associated online teaching.In this article we will describe and evaluate how the concept can be adapted to the current situation and implemented online. With new workshops and an adapted online working method the Learning Lab ""Digital Technologies"" overcomes the physical distance. The conversion to constructivism in an online environment presented in the article shows the new possibilities, but also the challenges. Compared to physical workshops, the aspects constructive and emotional do not show clear trends, social interactions become less and the aspect of self-regulation decreases. Nevertheless, the aspects of activity and situation are increasing.Using the example of the new virtual workshop ""Data Management Foundation"" (DMF) - design a solution for data management - based on the relational database Oracle Apex and a specific case ""Second Chance"", the procedure, bottlenecks, and positive results of the execution will be explained. This workshop is also accessible to the existing community which welcomes new lecturers and developers."
Scrum Based Framework for Teaching Software Engineering for Game Development,"Abd El Maksoud, Samah Hassan",10.1145/3442705.3442717,2021,This paper proposes a scrum-based game development framework (GDF) to teach students software engineering concepts essential for video game development. It proposes adaptations to the Scrum management framework to iterate through the game design and development process from concept to completion. The proposal converts a theoretical software engineering course into a project-oriented course to better prepare the students for industry demands. A learn by doing approach is followed to allow students to apply current and previously learned knowledge to design and develop a video game in a studio-like environment.
Prospects for Digital Transformation of Public Administration in Russia,"Balashov, Aleksei and Barabanov, Anton and Degtereva, Viktoriia and Ivanov, Maxim",10.1145/3444465.3444506,2021,"The purpose of the study was to analyze the general prospects for the digitalization of public administration in Russia, as well as to identify the factors that impede this process. The study is based on the application of the method of theoretical generalization. Various measures to improve the efficiency of public administration, to optimize the administrative apparatus are carried out on an ongoing basis in order to find a source of a fundamental breakthrough in increasing the efficiency of the administration process. Despite the significant work that has been done in recent years to create various online platforms for interaction between the government and society and the development of a mechanism for providing public and municipal services in an electronic form, in our country, the digitalization of public administration is still at an early stage.The scale of the changes taking place in our country and in the world in general is so great that digitalization is rightly regarded not only as a driver for development, but also as a driver for the emergence of new paradigms, in particular, of public administration. Additional difficulties in analyzing the problems of digital transformation of public administration are due to the presence of various, often contradictory views on this process, as well as administrative practices. It is necessary to evaluate the capabilities of various technology platforms to optimize the administration and communication process. Recent events related to the coronavirus pandemic have shown the importance of the prompt exchange of information for timely administrative decisions. The results of this study indicate the need to invest in human capital in order to optimize the process of digital transformation of public administration. The study allowed us to generalize the specific problems of Russia regarding digital transformation."
Dynamic Functional Connectivity and Graph Convolution Network for Alzheimer's Disease Classification,"An, Xingwei and Zhou, Yutao and Di, Yang and Ming, Dong",10.1145/3444884.3444885,2021,"Alzheimer's disease (AD) is the most prevalent form of dementia. Traditional methods cannot achieve efficient and accurate diagnosis of AD. This paper introduces a novel method based on dynamic functional connectivity (dFC) that can effectively capture changes in the brain. We compare and combine four different types of features including amplitude of low-frequency fluctuation (ALFF), regional homogeneity (ReHo), dFC and the adjacency matrix of different brain structures between subjects. We use graph convolution network (GCN) which consider the similarity of brain structure between patients to solve the classification problem of non-Euclidean domains. The proposed method's accuracy and the area under the receiver operating characteristic curve achieved 91.3% and 98.4%. This result demonstrated that our proposed method can be used for detecting AD."
Service Interruption: Managing Commitment to Community Partners During a Crisis,"J. Barker, Lecia and Voida, Amy and Nagy, Vaughan",10.1145/3446871.3469756,2021,"While the 2020-2021 COVID-19 pandemic is the most widespread and longest lasting educational disruption of the modern era, it joins a host of other natural and human-made crises affecting university education, such as Hurricane Mar\'{\i}a in Puerto Rico (2017), the Islamic State's closure of Al-Furat University in Syria (2014), Hurricane Katrina in New Orleans (2005), and many others. For service learning classes, generally defined as students learning as they provide service to a community partner, these large scale disruptions create special challenges even when it is possible to move classes online. The COVID-19 pandemic seriously affected the active involvement of community partners, including non-profit organizations, local schools and hospitals, and local governments. Many community organizations struggled to meet increased demand for their assistance while simultaneously cutting personnel due to budget shortfalls. In this paper, we report results from 34 survey respondents who offered service learning classes in undergraduate computer and information science during spring 2020. Despite the turmoil, only three faculty respondents lost their community partner entirely. In response to disruption, nearly half of faculty removed some of the assignments’ requirements, while others made the service project optional or removed it completely. Going online negatively affected students’ ability to collaborate with each other and interact with community partners, activities that are considered important for reaching learning outcomes for service learning. Nevertheless, about two-thirds of faculty reported that their students completed their service assignments and described conditions that led to or detracted from their success. Based on the findings, the authors present several implications for development of future computer and information science service learning offerings that are resilient during times of crisis."
Talent Demand Forecasting with Attentive Neural Sequential Model,"Zhang, Qi and Zhu, Hengshu and Sun, Ying and Liu, Hao and Zhuang, Fuzhen and Xiong, Hui",10.1145/3447548.3467131,2021,"To cope with the fast-evolving business trend, it becomes critical for companies to continuously review their talent recruitment strategies by the timely forecast of talent demand in recruitment market. While many efforts have been made on recruitment market analysis, due to the sparsity of fine-grained talent demand time series and the complex temporal correlation of the recruitment market, there is still no effective approach for fine-grained talent demand forecast, which can quantitatively model the dynamics of the recruitment market. To this end, in this paper, we propose a data-driven neural sequential approach, namely Talent Demand Attention Network (TDAN), for forecasting fine-grained talent demand in the recruitment market. Specifically, we first propose to augment the univariate time series of talent demand at multiple grained levels and extract intrinsic attributes of both companies and job positions with matrix factorization techniques. Then, we design a Mixed Input Attention module to capture company trends and industry trends to alleviate the sparsity of fine-grained talent demand. Meanwhile, we design a Relation Temporal Attention module for modeling the complex temporal correlation that changes with the company and position. Finally, extensive experiments on a real-world recruitment dataset clearly validate the effectiveness of our approach for fine-grained talent demand forecast, as well as its interpretability for modeling recruitment trends. In particular, TDAN has been deployed as an important functional component of intelligent recruitment system of cooperative partner."
Exploitation of ontological approaches in Big Data: A State of the Art,"Djebouri, Djamila and Keskes, Nabil",10.1145/3447568.3448553,2021,"The emergence of web technologies is generating a data deluge called Big Data. All this data is in fact a gold mine to be exploited. However, we are confronted with huge volumes of heterogeneous data (various formats) and varied data (various sources) and in continuous expansion. To deal with this, some research works have introduced ontologies: this is the purpose of this paper. We present the Big Data concept on the one hand and the ontology concept on the other. We first recalled the definitions of Big Data, its main dimensions known by the 3 V (volume, velocity, variety), the fields of application as well as the various problems related to it. We reviewed the different solutions proposed as well as the existing tools by using the NoSQL and the Map-Reduce paradigm implemented in Hadoop and Spark.We then looked at the concept of ontology, starting by recalling the definition of ontology, so an ontology is a conceptual model to represent reality and on which it is possible to develop systems that can be shared and reused. Ontologies are used to represent a domain and reason about its entities.Finally, we presented and discussed some research works that combined ontologies and Big Data. We have found that there is a very abundant literature that deals with big data and ontologies separately, but few studies combine the two concepts together. We will therefore focus on the latter in order to enrich the scientific literature in the domain."
The impact of social norms on students’ online learning behavior: Insights from two randomized controlled trials,"G\""{u}nther, Sebastian A.",10.1145/3448139.3448141,2021,"The provision of comparative feedback is a promising approach in digital learning environments to support learners’ self-regulated learning. Yet, empirical evidence suggests that such feedback can sometimes backfire or may only help learners with relatively high self-regulated learning skills, potentially exacerbating educational inequality. In this paper, we try to overcome such drawbacks by re-evaluating a feedback system based on the social norms theory that has previously led to intriguing results: A social comparison component embedded into the learning platform of a blended learning course (elective module, 58 participants) considerably encouraged online learning during the semester. Moreover, there was no heterogeneity in the behavioral response, suggesting that all subgroups responded similarly to the feedback. To further shed light on the generalizability of these results, this paper presents a follow-up study. Specifically, we conducted a second experiment during the COVID-19 pandemic with a different university course (compulsory module, 118 participants) and a non-overlapping sample and find similar results. The feedback shifted students’ online learning from the end towards the middle of the semester. Overall, the findings suggest that our feedback system has a large impact on students’ online learning and that this desirable impact is present in all subgroup analyses."
The Black Box of Virtual Agent Design: A Literature Review of User Involvement at the IVA Conference,"Chilufya, Emma Mainza and Silvervarg, Annika",10.1145/3448696.3448720,2021,"The field of Intelligent Virtual Agents (IVAs) has evolved immensely with respect to design and development of agents over the years. This has brought the following questions: What processes and methods are used to design virtual agents, and in particular, to what extent and how are users involved in the design process of virtual agents? In this paper, we review papers from the conference “Intelligent Virtual Agents” for the last five years (2015 - 2019) to shed light on these questions. The review included 308 short and long papers, with a focus on the interactive aspects of design. The review showed that only 14% of 308 papers explicitly mentioned/referred to user participation during the design of an IVA. User involvement is classified into two categories: one-time and iterative. The few studies that mention design only report on the use of standard Human-Computer Interaction (HCI) design methods to a rather limited degree."
Are you one of us? How Employers Prioritize among IT Graduates,"Lauv\r{a}s, Per and Raaen, Kjetil and Larsson, Anders Olof",10.1145/3450329.3476854,2021,"When we are designing IT education and degree programs, knowledge about what graduates need to succeed is important. One source of such knowledge are employers. This paper presents a survey of Norwegian employers asking what they are looking for in a recently graduated IT candidate. We confirm that the so called soft skills are highly prized. Among these, we identify a less reported concept of fitting into company or team culture as the most important. Another somewhat unexpected result is that employers do not expect to see a portfolio from graduates. Among technologies mentioned, Java is at the top followed closely by C# and .NET. Python and JavaScript are also mentioned often. Our study suggests that soft skills should be a focus throughout IT education programs."
Revue Syst\'{e}matique de la Litt\'{e}rature sur le Soutien \`{a} la S\'{e}curit\'{e} des Op\'{e}rations de Drones: Systematic Literature Review of Safety Support for Drones Operations,"Rakotonarivo, Balita Heriniaina and Drougard, Nicolas and Conversy, St\'{e}phane and Garcia, J\'{e}r\'{e}mie",10.1145/3450522.3451328,2022,"In this paper, we focus on Unmanned Aircraft Systems (UAS) operations safety support. This is a key issue for operators, who must comply with the european regulation. First, we introduce the important elements of a civil UAS, including the European regulation. Then we describe a systematic literature review on this topic. This results in the identification of the main approaches: Detect &amp; Avoid, Human-Computer Interactions and Human Factors (HCI/HF), aircraft integrity, safety assessment, path planning, geofencing, cybersecurity and UAS traffic management. We further analyze the contributions related to HCI by identifying user tasks, interaction design recommendations for improving the safety of UAS operations and research perspectives. Finally, we discuss aspects that are poorly covered in the reviewed articles."
What’s Inside Unreal Engine? - A Curious Gaze!,"Agrahari, Vartika and Chimalakonda, Sridhar",10.1145/3452383.3452404,2021,"Game developers often use game engines to efficiently develop games and reduce effort by reusing predefined components. However, game engines are complex and difficult to comprehend to extend and integrate with other components. Unreal Engine is one of the most commonly used open-source game engine which is quite complex and involves thousands of APIs. In this paper, we aim to comprehend Unreal Engine by analyzing its underlying software architecture. We accomplish this by considering the architectural changes across various releases of the GitHub repository of Unreal Engine by using an in-house tool called AC2 which uses call graphs and collaboration graphs. We run this tool on three initial releases of two components of the Unreal Engine and observe two kinds of architectural patterns, namely Event-driven and Hierarchical MVC (Model-View-Controller). We also observe the changes in components across releases. We believe that this preliminary study on Unreal Engine may unveil its architectural design and modules and may aid in better comprehension of this complex and widely used game engine for researchers and practitioners."
The Training Mode of Iterated Software Engineering Talents under the Background of Emerging Engineering Education,"Zhu, Zijiang and Dai, Weihuang and Hu, Yi and Li, Junshan",10.1145/3456887.3456902,2021,"In order to adapt to the new situation of international competition and the new demand of strategic development, Emerging Engineering Education (3E) is the new direction of China's engineering education reform. Based on the analysis of the characteristics of software engineering major, this paper discusses the current predicament of software engineering talent training, and puts forward the model of iterative software engineering talent training under the background of 3E. This mode is implemented in three stages in teaching arrangement: iterative advancement in teaching objectives, project-centered practice teaching, and curriculum group mode in teaching organization. This mode practices the idea of industry-university cooperation in educating people, supports the natural extension of ""integration of the core and master programs"" in the mode expansion, and has been widely recognized in the practical talent training."
Innovation and Entrepreneurship Education based Intelligent New Engineering Practice Teaching Mode and Teaching Base Construction Research,"Gao, Qian",10.1145/3456887.3457087,2021,"In order to build a manufacturing power and implement Industry 4.0, outstanding engineers with innovative ability are urgently needed to improve national competitive strength. This paper first analyzes the current situation of the cultivation of innovative talents in computer major in colleges and universities in 4 aspects, then proposes the construction mode of intelligent new engineering practice teaching base based on the innovation and entrepreneurship education, and puts forward specific construction plans and measures. It has been proved by practice that the practical teaching mode and teaching base construction proposed by us can greatly improve students' enthusiasm for innovation and entrepreneurship, and provide students with the high success rate of national innovation and entrepreneurship projects."
An Empirical Study of Multi-discussing Pattern in Open-Source Software Development,"Yang, Cheng and Hu, Dongyang and Zhang, Yang and Wang, Tao and Yu, Yue",10.1145/3457913.3457914,2021,"GitHub enables developers to expediently contribute their comments on multiple issues and switch their discussion between issues, i.e., multi-discussing. Discussing multiple issues simultaneously is able to enhance work efficiency. However, multi-discussing also relies on developers’ rationally allocating their focus, which may result in the different influence on the resolution of issues. Therefore, investigating how multi-discussing affects the issue resolution is a meaningful research question that can help developers understand the benefits and limitations of multi-discussing. Using quantitative and qualitative methods, this paper proposes a groundbreaking study of the impact of multi-discussing on issue resolution in GitHub. First, we collect and analyze data from 624 GitHub projects to explore how multi-discussing affects the overall issue resolution of the project. Further, we investigate how multi-discussing affects the resolution of a single issue. We find that multi-discussing is a common behavior in GitHub. Also, multi-discussing is connected to a shorter average issue resolution latency of the project. However, during a single issue resolution, more multi-discussing behaviors tend to bring longer issue resolution latency. We also conduct the qualitative analysis to explore the developers’ experiences and expectations of multi-discussing."
What Makes Open Source Software Projects Impactful: A Data-Driven Approach,"Yang, Huaiwei and Liu, Shuang and Gui, Lin and Zhao, Yongxin and Sun, Jun and Chen, Junjie",10.1145/3457913.3457932,2021,"With the wide adoption and acceptance of open source version control and hosting systems, more and more companies, including Google, Microsoft, Apple and Facebook, are putting their projects on such platforms, e.g., GitHub. It is very important for open source projects to be impactful, i.e., to attract attentions from the open source development community, so as to gain support on development, testing as well as maintenance from the community. However, the question of what factors affect open source project impact, remains largely open. Given the numerous confounding factors and the complex correlations among the factors, it is a challenge to answer the question. In this study, we gather a large dataset from GitHub and provide empirical insights on this question base on a data-driven approach. We randomly collect 146,286 projects from GitHub and then adopt data analysis techniques to automatically analyze the correlations of different features with the software project impact. We also provide suggestions on how to potentially make open source projects impactful base on our analysis results."
The Trends and Challenges of Emerging Technologies in Higher Education,"Yan, Hongli",10.1145/3459043.3459060,2021,"New Media Consortium (NMC) is a non-profit research organization in the United States, regularly publishes Horizon Report (Higher Education Edition) on the emerging technologies that promote educational reform, the problems that restrict the application of technology, and the major challenges in the field of higher education. The analysis of the Horizon Report 2020 shows that while higher education is affected by various macro-trends, the progress of emerging technologies has brought unprecedented opportunities and challenges to the development and reform of higher education. Through the interpretation of the report, this paper analyzes social, technological, economic, higher education and political trends, emerging technologies and practices, and scenarios of future higher education development, to provide references and Enlightenments for the innovation and reform of higher education."
UI Interactions to Invoke Viewing Experience and Their Effects,"Endo, Toshihiro and Yoshioka, Rentaro",10.1145/3459212.3459213,2021,"A user interaction scheme for a user-interface to extract emotional experience of art appreciation as knowledge is proposed and evaluated. The interaction was designed based on theories supported in the field of art appreciation education to invoke behaviors to enhance the experience. This study examines whether the behavior invoked by the interaction affects the quality of the viewing experience. The results of the experiment confirmed that some of the behaviors invoked by the interaction affect the quality of the experience. The results suggest that when designing user-interfaces to produce quality experiences, it is necessary to pay attention to the behavior leading up to the generation of the impressions that constitute the experience."
Poster: A Scoping Review of Alternative Credit Scoring Literature,"Njuguna, Rebecca and Sowon, Karen",10.1145/3460112.3471972,2021,"This paper covers a scoping review to establish the breadth of alternative credit scoring literature. The field is nascent and gaining popularity due to the crucial role alternative data is playing to accelerate financial inclusion. Historically, evaluating creditworthiness required availability of past financial activity such as loan repayment. Such stringent requirements rendered people with little or no financial history ‘credit invisible’. Advancements in Artificial Intelligence and Machine Learning have enabled scoring algorithms to work with non-financial data such as digital footprints from mobile devices and psychometric data to compute credit scores. Although the largest portion of ‘credit invisibles’ are in developing economies, research in the area is predominantly originating from developed economies and most alternative credit scoring models are trained with data from developed economies. There is need for more research from developing contexts and utilization of alternative data from populations with a smaller digital footprint."
Compressed Oblivious Encoding for Homomorphically Encrypted Search,"Choi, Seung Geol and Dachman-Soled, Dana and Gordon, S. Dov and Liu, Linsheng and Yerukhimovich, Arkady",10.1145/3460120.3484792,2021,"Fully homomorphic encryption (FHE) enables a simple, attractive framework for secure search. Compared to other secure search systems, no costly setup procedure is necessary; it is sufficient for the client merely to upload the encrypted database to the server. Confidentiality is provided because the server works only on the encrypted query and records. While the search functionality is enabled by the full homomorphism of the encryption scheme. For this reason, researchers have been paying increasing attention to this problem. Since Akavia et al. (CCS 2018) presented a framework for secure search on FHE encrypted data and gave a working implementation called SPiRiT, several more efficient realizations have been proposed. In this paper, we identify the main bottlenecks of this framework and show how to significantly improve the performance of FHE-base secure search. In particular, To retrieve l matching items, the existing framework needs to repeat the protocol l times sequentially. In our new framework, all matching items are retrieved in parallel in a single protocol execution. The most recent work by Wren et al. (CCS 2020) requires O(n) multiplications to compute the first matching index. Our solution requires no homomorphic multiplication, instead using only additions and scalar multiplications to encode all matching indices. Our implementation and experiments show that to fetch 16 matching records, our system gives an 1800X speed-up over the state of the art in fetching the query results resulting in a 26X speed-up for the full search functionality."
Creating Lean and Agile Supply Chains with Blockchain,"Zhao, Ruijuan and Bal, Jay and Ma, Xiao",10.1145/3460537.3460558,2021,"This paper aims to explore how blockchain technology can help us to achieve supply chains that are agile but also lean, which is also demanded by the modern business environment. A role playing simulation was used to evaluate the benefits provided by different levels of visibility in a typical supply chain. A distributed ledger enabling supply chain visibility was designed. A supply chain performance framework was developed in order to measure the supply chain performance from both lean and agile aspects. The results show a significant correlation in performance with the degree of visibility, but it is not a linear relationship. An effective LeAgile supply chain can be achieved with the enhancement of visibility. In order to implement visibility, an appropriate information sharing frequency (drumbeat) is needed. Blockchain allows users to more securely and transparently track all types of transactions, to improve Supply Chain Visibility. We also find the complex relationship between trust and visibility. This is one of the first papers, taking human factors into consideration, to investigate how blockchain technology can be used to enhance supply chain visibility, which is needed for leagile supply chain performance."
Distributed Ledger Architecture for Collaborative Megaproject Management with Authenticated Participants,"Natarajan, Ananth",10.1145/3460537.3460563,2021,"Distributed Ledger Technology can benefit the severe and chronic issues in the planning and execution of complex projects. However, while the general potential is recognized, there is little rigorous discussion of the pathway to benefits or detailed description of architectures to realize them. Here, we start by discussing the severity and causes of the problem. We then present an architecture designed to ameliorate them. Project networks are temporally constrained DAGs; new value is created at project milestones corresponding to the DAG vertices. Building on these characteristics, we describe an architecture for collaborative planning and execution with smart contracts at milestones and tokens backed by project created assets."
MACHINE LEARNING FRAMEWORK FOR COVID-19 DIAGNOSIS,"Vangipuram, Sravan kiran and Appusamy, Rajesh",10.1145/3460620.3460624,2021,"With the alarming global health crisis and pandemic, the entire medical industry and every human in this world are desperately looking for new technologies and solutions to monitor and contain the spread of this COVID-19 virus through early detection of its presence among infected patients. The early diagnosis of COVID-19 is hence critical for prevention and limiting this pandemic before it engulfs the humanity. With early diagnosis, the patient may be suggested for self-isolation (or) quarantine under medical supervision. Early detection of COVID-19 can save the patient and minimize the risk of falling prey to CoviD-19. Machine learning, a subset field of Artificial Intelligence can provide a viable solution for early diagnosis of disease and facilitate continuous monitoring of infected patients. AI based approaches can provide a view of the degree of disease severity. In general, Artificial intelligence (AI) could be a better technique for quantitative evaluation of the disease to obtain fruitful results. This paper throws light on the emerging need for AI powered solutions to foster early diagnosis of COVID-19 and suggest an ML based health monitoring framework for diagnosis of infected patients."
Tackling Algorithmic Transparency in Communal Energy Accounting through Participatory Design,"Cech, Florian",10.1145/3461564.3461577,2021,"Algorithmic transparency presents a significant challenge to system developers and users of algorithmic systems alike. Framing the problem as a ‘wicked’ one, this study tackles the issue of transparency in the EnerCoach energy accounting tool through presenting a situated ethnography of the algorithmic system and exploring the issues and challenges of model transparency and post-hoc explainability therein. By engaging stakeholders through participatory design methodologies, both a conceptual understanding of the problem and material solutions thereof are developed and evaluated. The findings show the promising potential of participatory design methodologies to elevate users to a ‘critical audience’, and the solutions co-created by the study participants for the challenge of algorithmic transparency. The results also highlight the complexity of the problem: transparency of algorithmic systems must be understood as a multi-facetted and highly contextual, ‘wicked’ problem that requires diverse methodological interventions to reach ‘satisficing’ solutions."
How HCI Bridges Health and Design in Online Health Communities: A Systematic Review,"Gatos, Do\u{g}a and G\""{u}nay, Asl\i{} and K\i{}rlang\i{}\c{c}, G\""{u}ncel and Kuscu, Kemal and Yantac, Asim Evren",10.1145/3461778.3462100,2021,"This paper presents a systematic review of online health communities (OHCs) published between 2009 and 2020 in the ACM Digital Library. Aiming to consolidate the current issues, design knowledge, challenges, and tensions in OHCs, our analysis identified four high-level aspects related to the use and design of OHCs: (1) temporal: OHCs as transition spaces, (2) spatial: bridging experiential knowledge with medical expertise, (3) technological: exchanging and locating peer support, and (4) tension dimensions in OHCs. We further discuss methodological improvements and computing opportunities for OHC research and how to increase OHC members’ agency in such a medically dominated context. These findings have the potential to inform future OHC designs and help researchers and designers position future contributions."
A Machine Learning Based Ensemble Method for Automatic Multiclass Classification of Decisions,"Fu, Liming and Liang, Peng and Li, Xueying and Yang, Chen",10.1145/3463274.3463325,2021,"Stakeholders make various types of decisions with respect to requirements, design, management, and so on during the software development life cycle. Nevertheless, these decisions are typically not well documented and classified due to limited human resources, time, and budget. To this end, automatic approaches provide a promising way. In this paper, we aimed at automatically classifying decisions into five types to help stakeholders better document and understand decisions. First, we collected a dataset from the Hibernate developer mailing list. We then experimented and evaluated 270 configurations regarding feature selection, feature extraction techniques, and machine learning classifiers to seek the best configuration for classifying decisions. Especially, we applied an ensemble learning method and constructed ensemble classifiers to compare the performance between ensemble classifiers and base classifiers. Our experiment results show that (1) feature selection can decently improve the classification results; (2) ensemble classifiers can outperform base classifiers provided that ensemble classifiers are well constructed; (3) BoW + 50% features selected by feature selection with an ensemble classifier that combines Na\""{\i}ve Bayes (NB), Logistic Regression (LR), and Support Vector Machine (SVM) achieves the best classification result (with a weighted precision of 0.750, a weighted recall of 0.739, and a weighted F1-score of 0.727) among all the configurations. Our work can benefit various types of stakeholders in software development through providing an automatic approach for effectively classifying decisions into specific types that are relevant to their interests."
Development and Application of Sentiment Analysis Tools in Software Engineering: A Systematic Literature Review,"Obaidi, Martin and Kl\""{u}nder, Jil",10.1145/3463274.3463328,2021,"Software development is a collaborative task and, hence, involves different persons. Research has shown the relevance of social aspects in the development team for a successful and satisfying project closure. Especially the mood of a team has been proven to be of particular importance. Thus, project managers or project leaders want to be aware of situations in which negative mood is present to allow for interventions. So-called sentiment analysis tools offer a way to determine the mood based on text-based communication. In this paper, we present the results of a systematic literature review of sentiment analysis tools developed for or applied in the context of software engineering. Our results summarize insights from 80 papers with respect to (1) the application domain, (2) the purpose, (3) the used data sets, (4) the approaches for developing sentiment analysis tools and (5) the difficulties researchers face when applying sentiment analysis in the context of software projects. According to our results, sentiment analysis is frequently applied to open-source software projects, and most tools are based on support-vector machines. Despite the frequent use of sentiment analysis in software engineering, there are open issues, e.g., regarding the identification of irony or sarcasm, pointing to future research directions."
Detection and Elimination of Systematic Labeling Bias in Code Reviewer Recommendation Systems,"Tecimer, K. Ayberk and T\""{u}z\""{u}n, Eray and Dibeklioglu, Hamdi and Erdogmus, Hakan",10.1145/3463274.3463336,2021,"Reviewer selection in modern code review is crucial for effective code reviews. Several techniques exist for recommending reviewers appropriate for a given pull request (PR). Most code reviewer recommendation techniques in the literature build and evaluate their models based on datasets collected from real projects using open-source or industrial practices. The techniques invariably presume that these datasets reliably represent the “ground truth.” In the context of a classification problem, ground truth refers to the objectively correct labels of a class used to build models from a dataset or evaluate a model’s performance. In a project dataset used to build a code reviewer recommendation system, the recommended code reviewer picked for a PR is usually assumed to be the best code reviewer for that PR. However, in practice, the recommended code reviewer may not be the best possible code reviewer, or even a qualified one. Recent code reviewer recommendation studies suggest that the datasets used tend to suffer from systematic labeling bias, making the ground truth unreliable. Therefore, models and recommendation systems built on such datasets may perform poorly in real practice. In this study, we introduce a novel approach to automatically detect and eliminate systematic labeling bias in code reviewer recommendation systems. The bias that we remove results from selecting reviewers that do not ensure a permanently successful fix for a bug-related PR. To demonstrate the effectiveness of our approach, we evaluated it on two open-source project datasets —HIVE and QT Creator— and with five code reviewer recommendation techniques —Profile-Based, RSTrace, Naive Bayes, k-NN, and Decision Tree. Our debiasing approach appears promising since it improved the Mean Reciprocal Rank (MRR) of the evaluated techniques up to 26% in the datasets used."
Assessing Developer Expertise from the Statistical Distribution of Programming Syntax Patterns,"Moradi Dakhel, Arghavan and C. Desmarais, Michel and Khomh, Foutse",10.1145/3463274.3463343,2021,"Accurate assessment of developer expertise is crucial for the assignment of an individual to perform a task or, more generally, to be involved in a project that requires an adequate level of knowledge. Potential programmers can come from a large pool. Therefore, automatic means to provide such assessment of expertise from written programs would be highly valuable in such context. Previous works towards this goal have generally used heuristics such as Line 10 Rule or linguistic information in source files such as comments or identifiers to represent the knowledge of developers and evaluate their expertise. In this paper, we focus on syntactic patterns mastery as an evidence of knowledge in programming and propose a theoretical definition of programming knowledge based on the distribution of Syntax Patterns (SPs) in source code, namely Zipf’s law. We first validate the model and its scalability over synthetic data of “Expert” and “Novice” programmers. This provides a ground truth and allows us to explore the space of validity of the model. Then, we assess the performance of the model over real data from programmers. The results show that our proposed approach outperforms the recent state of the art approaches for the task of classifying programming experts."
Recommender Systems for Software Project Managers,"Wei, Liang and Capretz, Luiz Fernando",10.1145/3463274.3463951,2021,"The design of recommendation systems is based on complex information processing and big data interaction. This personalized view has evolved into a hot area in the past decade, where applications might have been proved to help for solving problem in the software development field. Therefore, with the evolvement of Recommendation System in Software Engineering (RSSE), the coordination of software projects with their stakeholders is improving. This experiment examines four open source recommender systems and implemented a customized recommender engine with two industrial-oriented packages: Lenskit and Mahout. Each of the main functions was examined and issues were identified during the experiment."
ICT Governance in Brazilian Smart Cities: An Integrative Approach in the Context of Digital Transformation,"Claudio Diogo Reis, Luiz and Cristina Bernardini, Flavia and Bacellar Leal Ferreira, Simone and Cappelli, Claudia",10.1145/3463677.3463682,2021,"A smart city can be described as an organizational environment that seeks to provide a better quality of life for citizens by applying digital technologies to public services. In general, ICT governance involves managing investments and ICT resources to guide organizations to achieve their strategic goals. Thus, ICT governance in smart cities should evaluate, direct, and monitor Information Technology to achieve public value for the local community by providing digital services. Most research investigated ICT governance in private and public organizations and smart governance in smart city environments, but few focused on ICT governance in smart cities, especially in developing countries. Thus, this research investigated ICT governance in the context of Brazilian smart cities by developing a conceptual and practical integrative approach for the cities. To achieve this objective, a literature review and a survey with Brazilian professionals were conducted to identify relevant ICT governance practices applied to this context. As a result, this work introduces a set of related practices that municipalities can track when designing smart city projects in a digital transformation environment."
Automation as a Driver of Digital Transformation in Local Government: Exploring Stakeholder Views on an Automation Initiative in a Swedish Municipality,"Lindgren, Ida and Toll, Daniel and Melin, Ulf",10.1145/3463677.3463685,2021,"Local government organizations in Sweden are under pressure from policy makers and leading politicians to accelerate digital transformation of administrative tasks, in order to make public service provision more efficient and effective. As part of this digital transformation, local governments are currently investigating and implementing digital technologies that can execute administrative tasks automatically, without involvement of administrative staff. We explore an automation initiative in a Swedish municipality using a qualitative case study. Our analysis is conducted from a stakeholder perspective, investigating (1) how different stakeholders interpret automation as part of the municipality's ongoing digitalization, and (2) their views on expected outcomes of automation of administrative tasks. Our analysis shows that different stakeholder groups hold different definitions of what digitalization and automation means for their organization, and what outcomes can be expected of automation of administrative tasks in their organization. The analysis further shows that national policy documents encourage local governments to use a specific technical solutions for automation (robotic process automation); however, this technology is viewed as somewhat problematic by the stakeholders working with IT in the organization. Our analysis contributes with an illustration of challenges that municipalities face in their endeavor to find ways of developing automation of administrative tasks, and call for further research on this topic."
e-Government for Public Values creation: a systematic literature review,"Maragno, Giulia and Gastaldi, Luca and Corso, Mariano",10.1145/3463677.3463692,2021,"Scholars and practitioners attention to the benefits enhanced by e-Government projects is progressively increasing. These transformations generated and are continuously generating new challenges for managing public organizations which cannot focus only on administrative efficiency but have to embrace a broader set of values. In this scenario, the adoption of digital technologies within governments could potentially have disrupting effects, both positive and negative. However, despite the general hype, few public organizations have actually realised all the potential associated with the adoption of digital technologies as values enablers: this study aims to investigate the topic by examining how it has evolved throughout the years and organizing the state-of-the-art of the scientific literature on public values creation through e-Government projects. Based on the literature review, we provide a conceptual framework for understanding the factors that are related and influence e-Government implementation and its benefits. Findings suggest that before considering the implementation of an e-Government project, and thus the achievement of specific benefits, a set of different variables should be addressed."
Performance of U.S. Scientific Research Cyberinfrastructure: Structural and Relational Factors for Usage,"Chen, Yu-Che and Cheng, Xiaoyue and Knepper, Richard",10.1145/3463677.3463722,2021,"This paper identifies structural and relational factors for the performance of cyberinfrastructure as a class of cyber-physical systems, filling the gap in our knowledge about social and technical factors, as well as their coupling, for performance. This study draws from the literature on virtual organizations, network governance, and the coupling of socio-technical systems to develop a conceptual framework for performance. The empirical investigation focuses on the NSF-funded cyberinfrastructure program in the United States, arguably one of the largest and most comprehensive cyberinfrastructure programs in the country. The researchers created a unique data set with 11,143 projects over a 15-year period by combining three cyberinfrastructure program data sources. We utilized regression and random forest techniques to study the utilization and amount utilized. The results indicate the importance of structural factors—including resource allocation, field of science, and type of universities/colleges—as well as relational factors—including transaction types, resource involved, and time of coupling. The results also underscore the relevance of multi-level and actor-network perspectives in advancing theories and practice of performance for virtual organizations and network governance."
Toward Exploratory Understanding of Software using Test Suites,"Meier, Dominik and Mattis, Toni and Hirschfeld, Robert",10.1145/3464432.3464438,2021,"Changing software without correctly understanding it often leads to confusion, as developers do not understand how the change corresponds to the new observed behaviour of the system. Today, many software systems are equipped with a test suite. Test suites document code and give feedback on changed program behaviour. We explored ways to use test suites for software comprehension and implemented a tool that provides additional visualisation and gives immediate feedback on software changes. Information about changes in the software and their implications to the test suite are collected using mutation testing. The tool uses this information to present relevant test cases for developers, and additionally prioritise test executions for immediate feedback. Our research indicates that entropy metrics can find test cases that are relevant for a specific context in the source code. Additionally, simple test case prioritisation strategies can already lead to a significant decrease in feedback time. Based on our case study we argue that test suites are not only useful for regression testing but can be used to generate meaningful information for software comprehension activities."
Project 412Connect: Bridging Students and Communities,"DiChristofano, Alex and Hamilton, Michael L. and Linardi, Sera and McCloud, Mara F.",10.1145/3465416.3483304,2021,"In this work, we describe some of the challenges Black-owned businesses face in the United States and specifically in the city of Pittsburgh. Taking into account local dynamics and the communicated desires of Black-owned businesses in the Pittsburgh region, we determine that university students represent an under-utilized market for these businesses. We investigate the root causes for this inefficiency and design and implement a platform, 412Connect (https://www.412connect.org/), to increase online support for Pittsburgh Black-owned businesses from students in the Pittsburgh university community. The site operates by coordinating interactions between student users and participating businesses via targeted recommendations. We describe the project from its conception, paying special attention to our motivation and design choices. These choices are aided by two simple models for badge design and recommendation systems that may be of theoretical interest. Along the way, we highlight challenges and lessons from coordinating a grassroots volunteer project working in conjunction with community partners and the opportunities and pitfalls of engaged scholarship."
Tight Revenue Gaps among Multi-Unit Mechanisms,"Jin, Yaonan and Jiang, Shunhua and Lu, Pinyan and Zhang, Hengjie",10.1145/3465456.3467621,2021,"This paper considers Bayesian revenue maximization in the k-unit setting, where a monopolist seller has k copies of an indivisible item and faces n unit-demand buyers (whose value distributions can be non-identical). Four basic mechanisms among others have been widely employed in practice and widely studied in the literature: Myerson Auction, Sequential Posted-Pricing, (k + 1)-th Price Auction with Anonymous Reserve, and Anonymous Pricing. Regarding a pair of mechanisms, we investigate the largest possible ratio between the two revenues (a.k.a. the revenue gap), over all possible value distributions of the buyers. Divide these four mechanisms into two groups: (i) the discriminating mechanism group, Myerson Auction and Sequential Posted-Pricing, and (ii) the anonymous mechanism group, Anonymous Reserve and Anonymous Pricing. Within one group, the involved two mechanisms have an asymptotically tight revenue gap of 1 + Θ(1 / √k). In contrast, any two mechanisms from the different groups have an asymptotically tight revenue gap of Θ(\l{}og k)."
"Retracted on February 24, 2022: Refined and Intelligent Management Mode of Construction Project Based on BIM and IOT Technology","Zhao, Yuxin and Wang, Qian and Wang, Xiaoyu",10.1145/3465631.3465678,2021,"NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference."
Using Blockchain for Optimal and Transparent Resource Allocation: A Proposed Solution for Fund Allocation: Brief overview,"Voicu-Dorobantu, Roxana and Udokwu, Chibuzor and Bocse, Bogdan",10.1145/3466029.3466056,2021,"The paper presents, in brief, a proposed blockchain-based architecture for optimal resource allocation, specifically financial resources, in a bounded-setting, called game. The solution touches upon actors involved and roles and rules of engagement, while setting the stage for a more complex proposal for future study. The paper is a review, set as a starting point for a more developed solution, which would allow for financial resources peer-sharing in a bounded setting, with automated functions."
Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture,"Lu, Liqiang and Jin, Yicheng and Bi, Hangrui and Luo, Zizhang and Li, Peng and Wang, Tao and Liang, Yun",10.1145/3466752.3480125,2021,"In recent years, attention-based models have achieved impressive performance in natural language processing and computer vision applications by effectively capturing contextual knowledge from the entire sequence. However, the attention mechanism inherently contains a large number of redundant connections, imposing a heavy computational burden on model deployment. To this end, sparse attention has emerged as an attractive approach to reduce the computation and memory footprint, which involves the sampled dense-dense matrix multiplication (SDDMM) and sparse-dense matrix multiplication (SpMM) at the same time, thus requiring the hardware to eliminate zero-valued operations effectively. Existing techniques based on irregular sparse patterns or regular but coarse-grained patterns lead to low hardware efficiency or less computation saving. This paper proposes Sanger, a framework that harvests sparsity in the attention mechanism through synergistic hardware and software co-design. The software part prunes the attention matrix into a dynamic structured pattern, and the hardware part features a reconfigurable architecture that exploits such patterns. Specifically, we dynamically sparsify vanilla attention based on a quantized prediction of the attention matrix. Then, the sparse mask is re-arranged into structured blocks that are more amenable to hardware implementation. The hardware design of Sanger features a score-stationary dataflow that keeps sparse scores stationary in the PE to avoid decoding overhead. Using this dataflow and a reconfigurable systolic array design, we can unify the computation of SDDMM and SpMM operations. Typically, the PEs can be configured during runtime to support different data access and partial sum accumulation schemes. Experiments on BERT show that Sanger can prune the model to 0.08 - 0.27 sparsity without accuracy loss, achieving 4.64X, 22.7X, 2.39X, and 1.47X speedup compared to V100 GPU, AMD Ryzen Threadripper 3970X CPU, as well as the state-of-the-art attention accelerators A3 and SpAtten."
Inventory Estimation Model with Fuzzy Analytic Hierarchy Process and Neural Network Approaches in the Wiring industry,"Rachman, Fauzie and Zulkarnain, Zulkarnain",10.1145/3468013.3468401,2022,"Inventory control is very important in company bussiness. Based on data from the Ministry of Industry, the electricity cable industry is expected to experience growth of around 10% -15%. And it is predicted that this increase will continue to grow for the next few years, given that Indonesia is developing in terms of infrastructure and industry. To keep good in track, a good inventory planning is needed so that the goals are achieved to meet customer needs. Several previous studies on the predictions of the quantity of future product stocks, concluded that inventory, both in the form of raw materials, in-process goods, semi-finished products and finished products. The main contribution of this research is to make decision support models by predicting orders from customers so as to minimize the risk of inventory failure. In order for inventory management to be more efficiently assessed according to experts, the opinions of experts. Therefore, a combination of Fuzzy Analytical Hierarchy Process (Fuzzy AHP) and Artificial Neural Network (ANN) is carried out for inventory management."
Sustainability forecasting for Apache incubator projects,"Yin, Likang and Chen, Zhuangzhi and Xuan, Qi and Filkov, Vladimir",10.1145/3468264.3468563,2021,"Although OSS development is very popular, ultimately more than 80% of OSS projects fail. Identifying the factors associated with OSS success can help in devising interventions when a project takes a downturn. OSS success has been studied from a variety of angles, more recently in empirical studies of large numbers of diverse projects, using proxies for sustainability, e.g., internal metrics related to productivity and external ones, related to community popularity. The internal socio-technical structure of projects has also been shown important, especially their dynamics. This points to another angle on evaluating software success, from the perspective of self-sustaining and self-governing communities. To uncover the dynamics of how a project at a nascent development stage gradually evolves into a sustainable one, here we apply a socio-technical network modeling perspective to a dataset of Apache Software Foundation Incubator (ASFI), sustainability-labeled projects. To identify and validate the determinants of sustainability, we undertake a mix of quantitative and qualitative studies of ASFI projects’ socio-technical network trajectories. We develop interpretable models which can forecast a project becoming sustainable with 93+% accuracy, within 8 months of incubation start. Based on the interpretable models we describe a strategy for real-time monitoring and suggesting actions, which can be used by projects to correct their sustainability trajectories."
Reel life vs. real life: how software developers share their daily life through vlogs,"Chattopadhyay, Souti and Zimmermann, Thomas and Ford, Denae",10.1145/3468264.3468599,2021,"Software developers are turning to vlogs (video blogs) to share what a day is like to walk in their shoes. Through these vlogs developers share a rich perspective of their technical work as well their personal lives. However, does the type of activities portrayed in vlogs differ from activities developers in the industry perform? Would developers at a software company prefer to show activities to different extents if they were asked to share about their day through vlogs? To answer these questions, we analyzed 130 vlogs by software developers on YouTube and conducted a survey with 335 software developers at a large software company. We found that although vlogs present traditional development activities such as coding and code peripheral activities (11%), they also prominently feature wellness and lifestyle related activities (47.3%) that have not been reflected in previous software engineering literature. We also found that developers at the software company were inclined to share more non-coding tasks (e.g., personal projects, time spent with family and friends, and health) when asked to create a mock-up vlog to promote diversity. These findings demonstrate a shift in our understanding of how software developers are spending their time and find valuable to share publicly. We discuss how vlogs provide a more complete perspective of software development work and serve as a valuable source of data for empirical research."
Authorship attribution of source code: a language-agnostic approach and applicability in software engineering,"Bogomolov, Egor and Kovalenko, Vladimir and Rebryk, Yurii and Bacchelli, Alberto and Bryksin, Timofey",10.1145/3468264.3468606,2021,"Authorship attribution (i.e., determining who is the author of a piece of source code) is an established research topic. State-of-the-art results for the authorship attribution problem look promising for the software engineering field, where they could be applied to detect plagiarized code and prevent legal issues. With this article, we first introduce a new language-agnostic approach to authorship attribution of source code. Then, we discuss limitations of existing synthetic datasets for authorship attribution, and propose a data collection approach that delivers datasets that better reflect aspects important for potential practical use in software engineering. Finally, we demonstrate that high accuracy of authorship attribution models on existing datasets drastically drops when they are evaluated on more realistic data. We outline next steps for the design and evaluation of authorship attribution models that could bring the research efforts closer to practical use for software engineering."
Improving the effectiveness of peer code review in identifying security defects,"Paul, Rajshakhar",10.1145/3468264.3473107,2021,"Prior studies found peer code review useful in identifying security defects. That is why most of the commercial and open-source software (OSS) projects embraced peer code review and mandated the use of it in the software development life cycle. However, despite conducting mandatory peer code review practices, many security-critical OSS projects such as Chromium, Mozilla, and Qt are reporting a high number of post-release vulnerabilities to the Common Vulnerabilities and Exposures (CVE) database. Practitioners may wonder if there is any missing piece in the puzzle that leads code reviews to miss those security defects. Therefore, the primary objective of this dissertation study is to improve the effectiveness of peer code review in identifying security defects.  To meet this goal, I plan to empirically investigate: (i) why security defects escape code reviews, (ii) what are the challenges developers face to conduct effective security code reviews, (iii) how to build effective security code review strategy, and (iv) how to make effective utilization of security experts during code reviews."
VUI Influencers: How the Media Portrays Voice User Interfaces for Older Adults,"Sin, Jaisie and Munteanu, Cosmin and Ramanand, Numrita and Tan, Yi Rong",10.1145/3469595.3469603,2021,"Voice User Interfaces (VUIs) such as smart speakers hold promise for older adults (OAs) in terms of usability and convenience. However, their adoption and the extent of their benefits to OAs may be influenced by mass media, as this is a primary source of technology education for OAs. Thus, we aim to obtain a better understanding of how VUIs’ value and utility for OAs are portrayed in the media. We conducted a systematic review and thematic analysis of articles published in ten popular digital news outlets that focus on VUIs and older adults. The analysis reveals several design and engineering factors that are portrayed in media as being relevant or encouraging to older adults’ adoption of VUIs. Given the media's influence of the consumer adoption of new technologies, this analysis brings to light several sociotechnical aspects that are dominant threads within the media discourse related to VUIs. Through this, we suggest areas of focus for the research and design of VUIs that account for these influencing factors."
Do current user testing practices meet the needs of the new interactive paradigms?,"Capdevila, Marc Gonzalez and Saltiveri, Toni Granollers and Garrido, Juan Enrique and M\""{u}ller, Oct\'{a}vio Henrique and Ruas, Leonardo Coelho",10.1145/3471391.3471416,2021,"Human-Computer Interaction (HCI) practitioners count with more than 30-40 years of work already done evaluating the usability of interactive systems. For along time, the evaluation of user interfaces by user testing techniques, has been one of the most successful ways to get feedback from the users. HCI experts usually perform their experimental evaluations sessions in usability labs, indoor facilities with equipment like eye-tracking devices or multi-rooms with specialised software. Although expensive and not available for everybody, this model has worked for a long time. On the other hand, during the last decade the interactive context has completely changed. The rise of new user interactions like voice, virtual reality and augmented reality has conceived new interactive paradigms. The well-known Industry 4.0 and the Internet of Things (IoT) are not just something that might happen in the future, but an unstoppable reality that favours these changes and, because of that, we need new HCI knowledge, methods, and techniques to deal with the challenges brought by them. In this paper we show and discuss about this new reality in order to defend the need for a complete change in the way that we evaluate the Quality in Use of current and future interactive scenarios. We will finish our arguments by proposing a line of research to find new methodologies to improve the Quality in Use laboratories for the present/future."
Software Quality Assessment of a Web Application for Biomedical Data Analysis,"Wiese, Lena and Wiese, Ingmar and Lietz, Kristina",10.1145/3472163.3472172,2021,"Data Science as a multidisciplinary discipline has seen a massive transformation in the direction of operationalisation of analysis workflows. Yet it can be observed that such a workflow consists of potentially many diverse components: like modules in different programming languages, database backends, or web frontends. In order to achieve high efficiency and reproducibility of the analysis, a sufficiently high level of software engineering for the different components as well as an overall software architecture that integrates and automates the different components is needed. For the use case of gene expression analysis, from a software quality point of view we analyze a newly developed web application that allows user-friendly access to the underlying workflow."
Agile principles applied in learning contexts,"Niculescu, Virginia and Suciu, Dan and Bufnea, Darius",10.1145/3472673.3473963,2021,"Agile methodologies have been recently proposed to be used in education. In this paper, we propose a rephrasing of the 12 Agile principles for learning context, and we provide concrete application-oriented interpretations for them. Additionally, a practical agile learning methodology is proposed to offer a framework where these principles could be applied. The principles together with the proposed methodology were applied to a concrete use case which is described and the resulting impact is analysed."
Building a bot for automatic expert retrieval on discord,"Norambuena, Ignacio Nu\~{n}ez and Bergel, Alexandre",10.1145/3472674.3473982,2021,"It is common for software practitioners to look for experts on online chat platforms, such as Discord. However, finding them is a complex activity that requires a deep knowledge of the open source community. As a consequence, newcomers and casual participants may not be able to adequately find experts willing to discuss a particular topic.  Our paper describes a bot that provides a ranked list of Discord users that are experts in a particular set of topics. Our bot uses simple heuristics to model expertise, such as a word occurrence table and word embeddings. Our bot shows that at least half of the retrieved users are indeed experts."
Facial Recognition UX*: A Case Study of Industry-Academic Partnerships to Promote User-Centered Ethics in Facial Recognition,"Roundtree, Aimee",10.1145/3472714.3473647,2021,"This case study discusses the case of an industry-academic partnership with the NEC Foundation, a global leader in facial recognition technology, and a UX and usability BIPOC researcher investigating applications of the technology in emergency response and university settings. The research collaboration aims to provide formative user experience data to help the industry understand the experience of potential users to provide developers and marketers with user-centered, evidence-based guidelines for facial recognition deployment. Research questions include the following: How do service providers and consumers in these areas perceive collecting biometric data in higher education and fire services (which is the primary emergency response service for small and mid‐sized communities), as it pertains to their sense of ethics, privacy, data ownership, consent, and justice? The project investigates user attitudes, concerns, and, per value theory, estimations of tradeoffs between these domains. The case study shares preliminary findings from the work and plans for further investigation. Finally, the case study approaches a preliminary framework for industry-academic collaborations on UX and tech ethics. Partnerships with industry can render research in communication design immediately applicable. Even when they expose fault lines between industry and academia, they offer an essential service to the community and methods of advocacy and accountability that benefit users."
Management of Covid-19 Detection Using Artificial Intelligence in 2020 Pandemic,"Ghaderzadeh, Mustafa and Aria, Mehrad",10.1145/3472813.3472820,2021,"Successful early detection of Covid-19 disease plays an important role in improving the effectiveness of treatment and managing the pandemic. Various diagnostic methods for the rapid detection of COVID-19 are presented. The first and most important test to detect Covid19 is the PCR test. Studies have shown that PCR testing is time-consuming, expensive, and has a large number of false negatives. As a trend in the scientific community, artificial intelligence has succeeded in Covid19 detection and diagnosis. This article identifies the key achievements reflected in the performance measurement indicators of the application of artificial intelligence algorithms in Covid-19 detection. Besides, this study discusses the finding and future lessons as a roadmap for Corona Pandemic Age. Mean diagnosis of all AI algorithms in the studies performed through Radiology modality had sensitivity with an average higher than 95% and a specificity of higher than 92%, which have a higher diagnostic rate than of traditional radiological methods. Based on relevant research in the field of diagnosis of Covid-19, the Health care managers and Specialists could be able to manage the Pandemic much better and scientifically. Equipping devices and Radiological Center by AI Algorithms and software can cause increases covid19 identification tests and hence can cover more people exposed to Diagnosing Test. This study presents a comprehensive review of Artificial Intelligence techniques and evolving deep learning techniques for Covid-19 Detection."
A Systematic Review on Extracting Predictors for Forecasting Complications of Diabetes Mellitus,"Madurapperumage, Anuradha and Wang, William Yu Chung and Michael, Mayo",10.1145/3472813.3473211,2021,"The high global prevalence, irreversible health burden, and expenditures of diabetes mellitus lead to a proliferative research area of prognosis and diagnosis of diabetes and its complications through machine learning techniques. Although the risk scoring and prediction models have been proposed for decades, challenges still persist. Feature selection is one challenging problem that is still remains when building accurate models. Since each risk factor's contribution in predicting complications of diabetes vary with the recognition of novel risk factors, it is a requirement to make an up-to-date standard feature subset that can predict the risk of complications of diabetes mellitus. This research in progress paper proposes a systematic review study that aims to extract frequent feature subsets for predicting complications of diabetes. Diabetic retinopathy, neuropathy, nephropathy, and cardiovascular diseases have been considered as the most common complications of diabetes. PRISMA guidelines will be used to conduct the systematic review. Further, the proposed study will be strengthened by selecting credible journal repositories, optimising the search query, utilising inclusion and exclusion criteria, and determining top-ranked journals. The paper presents data analysis design with feature subsets to predict the aforementioned four complications of diabetes, which is directly beneficial for clinicians, researchers, and model developers to assist their clinical decisions, research purposes, and feature selection phase in model designing respectively."
Salient Error Detection based Refinement for Wide-baseline Image Interpolation,"Chang, Yuan and Chen, Yisong and Wang, Guoping",10.1145/3474085.3475696,2021,"Wide-baseline image interpolation is useful in many multimedia applications such as virtual street roaming and 3D TV. It is also a challenging problem because the large translations and rotations of image patches make it hard to estimate the motion fields between wide-baseline image pairs. We propose a refinement strategy based on salient error detection to improve the result of existing approaches of wide-baseline image interpolation, where we combine the advantages of methods based on piecewise-linear transformation and methods based on variational model. We first use a lightweight interpolation method to estimate the initial motion field between the input image pair, and synthesize the intermediate image as the initial result. Then we detect regions with noticeable artifacts in the initial image to find areas whose motion vectors should be refined. Finally, we refine the motion field of the detected regions using a variational model based method, and obtain the refined intermediate image. The refinement strategy of our method can be used as the post refinement step for many other image interpolation algorithms. We show the effectiveness and efficiency of our method through experiments on different datasets."
A Mining Software Repository Extended Cookbook: Lessons learned from a literature review,"Barros, Daniel and Horita, Flavio and Wiese, Igor and Silva, Kanan",10.1145/3474624.3474627,2021,"The main purpose of Mining Software Repositories (MSR) is to discover the latest enhancements and provide an insight into how to make improvements in a software project. In light of it, this paper updates the MSR findings of the original MSR Cookbook, by first conducting a systematic mapping study to elicit and analyze the state-of-the-art, and then proposing an extended version of the Cookbook. This extended Cookbook was built on four high-level themes, which were derived from the analysis of a list of 112 selected studies. Hence, it was used to consolidate the extended Cookbook as a contribution to practice and research in the following areas by: 1) including studies published in all available and relevant publication venues; 2) including and updating recommendations in all four high-level themes, with an increase of 84% in comments in this study when compared with the original MSR Cookbook; 3) summarizing the tools employed for each high-level theme; and 4) providing lessons learned for future studies. Thus, the extended Cookbook examined in this work can support new research projects, as upgraded recommendations and the lessons learned are available with the aid of samples and tools."
Smart prediction for refactorings in the software test code,"Martins, Luana and Bezerra, Carla and Costa, Heitor and Machado, Ivan",10.1145/3474624.3477070,2021,"Test smells are bad practices to either design or implement a test code. Their presence may reduce the test code quality, harming the software testing activities, primarily from a maintenance perspective. Therefore, defining strategies and tools to handle test smells and improve the test code quality is necessary. State-of-the-art strategies encompass automated support mainly based on hard thresholds of rules, static and dynamic metrics to identify the test smells. Such thresholds are subjective to interpretation and may not consider the complexity of the software projects. Moreover, they are limited as they do not automate test refactoring but only count on developers’ expertise and intuition. In this context, a technique that uses historical implicit or tacit data to generate knowledge could assist the identification and refactoring of test smells. This study aims to establish a novel approach based on machine learning techniques to suggest developers refactoring strategies for test smells. As an expected result, we could understand the applicability of the machine learning techniques to handle test smells and a framework proposal that helps developers in decision-making regarding the refactoring of test smells."
Towards a Human Values Dashboard for Software Development: An Exploratory Study,"Nurwidyantoro, Arif and Shahin, Mojtaba and Chaudron, Michel and Hussain, Waqar and Perera, Harsha and Shams, Rifat Ara and Whittle, Jon",10.1145/3475716.3475770,2021,"Background: There is a growing awareness of the importance of human values (e.g., inclusiveness, privacy) in software systems. However, there are no practical tools to support the integration of human values during software development. We argue that a tool that can identify human values from software development artefacts and present them to varying software development roles can (partially) address this gap. We refer to such a tool as human values dashboard. Further to this, our understanding of such a tool is limited. Aims: This study aims to (1) investigate the possibility of using a human values dashboard to help address human values during software development, (2) identify possible benefits of using a human values dashboard, and (3) elicit practitioners' needs from a human values dashboard. Method: We conducted an exploratory study by interviewing 15 software practitioners. A dashboard prototype was developed to support the interview process. We applied thematic analysis to analyse the collected data. Results: Our study finds that a human values dashboard would be useful for the development team (e.g., project manager, developer, tester). Our participants acknowledge that development artefacts, especially requirements documents and issue discussions, are the most suitable source for identifying values for the dashboard. Our study also yields a set of high-level user requirements for a human values dashboard (e.g., it shall allow determining values priority of a project). Conclusions: Our study suggests that a values dashboard is potentially used to raise awareness of values and support values-based decision-making in software development. Future work will focus on addressing the requirements and using issue discussions as potential artefacts for the dashboard."
Contextual Understanding and Improvement of Metamorphic Testing in Scientific Software Development,"Peng, Zedong and Kanewala, Upulee and Niu, Nan",10.1145/3475716.3484188,2021,"Background: Metamorphic testing emerges as a simple and effective approach for testing scientific software; yet, its adoption in actual scientific software projects is less studied.Aims: In order for the practitioners to better adopt metamorphic testing in their projects, we set out to first gain a deep understanding about the current qualify assurance workflow, testing practices, and tools.Method: We propose to integrate various empirical sources, including artifact analysis, stakeholder interviews, and gap analysis from the literature.Results: Applying our approach to the Open Water Analytics Stormwater Management Model project helped to identify four new needs requiring continued and more research: (1) systematic and explicit formulation of metamorphic relations, (2) metamorphic testing examples specific to the scientific software, (3) correlating metamorphic testing with regression testing, and (4) integrating metamorphic testing with build tools like CMake and continuous integration tools like GitHub Actions.Conclusions: Integrating different empirical sources is promising for establishing a contextual understanding of software engineering practices, and for action research, such as workflow refinements and tool interventions, to be carried out in a principled manner."
Semantic Slicing of Architectural Change Commits: Towards Semantic Design Review,"Mondal, Amit Kumar and Roy, Chanchal K. and Schneider, Kevin A. and Roy, Banani and Nath, Sristy Sumana",10.1145/3475716.3484487,2021,"Software architectural changes involve more than one module or component and are complex to analyze compared to local code changes. Development teams aiming to review architectural aspects (design) of a change commit consider many essential scenarios such as access rules and restrictions on usage of program entities across modules. Moreover, design review is essential when proper architectural formulations are paramount for developing and deploying a system. Untangling architectural changes, recovering semantic design, and producing design notes are the crucial tasks of the design review process. To support these tasks, we construct a lightweight tool [4] that can detect and decompose semantic slices of a commit containing architectural instances. A semantic slice consists of a description of relational information of involved modules, their classes, methods and connected modules in a change instance, which is easy to understand to a reviewer. We extract various directory and naming structures (DANS) properties from the source code for developing our tool. Utilizing the DANS properties, our tool first detects architectural change instances based on our defined metric and then decomposes the slices (based on string processing). Our preliminary investigation with ten open-source projects (developed in Java and Kotlin) reveals that the DANS properties produce highly reliable precision and recall (93-100%) for detecting and generating architectural slices. Our proposed tool will serve as the preliminary approach for the semantic design recovery and design summary generation for the project releases."
Heterogeneous ensemble imputation for software development effort estimation,"Abnane, Ibtissam and Idri, Ali and Hosni, Mohamed and Abran, Alain",10.1145/3475960.3475984,2021,"Choosing the appropriate Missing Data (MD) imputation technique for a given Software development effort estimation (SDEE) technique is not a trivial task. In fact, the impact of the MD imputation on the estimation output depends on the dataset and the SDEE technique used and there is no best imputation technique in all contexts. Thus, an attractive solution is to use more than one single imputation technique and combine their results for a final imputation outcome. This concept is called ensemble imputation and can help to significantly improve the estimation accuracy. This paper develops and evaluates a heterogeneous ensemble imputation whose members were the four single imputation techniques: K-Nearest Neighbors (KNN), Expectation Maximization (EM), Support Vector Regression (SVR), and Decision Trees (DT). The impact of the ensemble imputation was evaluated and compared with those of the four single imputation techniques on the accuracy measured in terms of the standardized accuracy criterion of four SDEE techniques: Case Based Reasoning (CBR), Multi-Layers Perceptron (MLP), Support Vector Regression (SVR) and Reduced Error Pruning Tree (REPTree). The Wilcoxon statistical test was also performed in order to assess whether the results are significant. All the empirical evaluations were carried out over the six datasets, namely, ISBSG, China, COCOMO81, Desharnais, Kemerer, and Miyazaki. Results show that the use of heterogeneous ensemble-based imputation instead single imputation significantly improved the accuracy of the four SDEE techniques. Indeed, the ensemble imputation technique was ranked either first or second in all contexts."
A System for Validating Resistive Neural Network Prototypes,"Hoskins, Brian and Ma, Wen and Fream, Mitchell and Yousuf, Osama and Daniels, Mathew and Goodwill, Jonathan and Madhavan, Advait and Tung, Hoang and Branstad, Mark and Liu, Muqing and Madsen, Rasmus and Mclelland, Jabez and Adam, Gina and Lueker-Boden, Martin",10.1145/3477145.3477260,2021,"Building prototypes of heterogeneous hardware systems based on emerging electronic, magnetic, and photonic devices is an important area of research. The novel implementation of these systems for artificial intelligence poses new and unforeseen challenges in mixed signal data acquisition, hyperparameter optimization, and hardware co-processing. Many emerging devices exhibit unpredictable and stochastic behavior as well as poorly repeatable hysteretic effects or performance degradation. Dealing with these device challenges on top of more traditional hardware problems, like quantization errors, timing constraints, and even hardware and software bugs is an enterprise fraught with pitfalls. Equally important to the construction of the physical prototype is the co-development and integration of a design verification framework that can extensibly allow for predictable behavior of not only the entire system but also all of its parts in a modular way, allowing for seamless integration in both simulation and implementation. This work discusses Daffodil-lib, a Python based prototyping framework which, from hardware to software, enables everything from a script-based simulation to a compiled hardware-timed experiment, to everything in between with no syntactical changes for the end user."
Investigating technological risks and mitigation strategies in software projects,"Dantas, Emanuel and Neto, Ademar Sousa and Valadares, Dalton and Perkusich, Mirko and Ramos, Felipe and Almeida, Hyggo and Perkusich, Angelo",10.1145/3477314.3507062,2022,"Risks are present in any software project. In particular, technological risks are complex, volatile, and difficult to predict accurately. Despite this, the literature is scarce about such risks. This paper address this gap by identifying technological risk factors and strategies to mitigate them in software projects. We conducted and analyzed semi-structured interviews with 25 experts from ten organizations that execute software projects. Participants work with emerging technologies in academia-industry collaboration projects. Our analysis, using grounded theory, led to nine technological risk factors, classified into Research risk or Product risk. Further, for each technological risk, we identified the main mitigation strategy adopted by the practitioners. The results of our study can be used as a catalog to assist organizations in managing technological risks on software projects."
Big Data Analysis Driven Decision Making System Ensuring Energy Security of a Country,"Islam, Mahmudul and Hasan, Mahady",10.1145/3477911.3477921,2021,"Energy is one of the key factors for a country's economic and social growth. Bangladesh is constantly seeking sustainable energy sources for securing its increasing energy demand as energy security is a national concern. Currently, Bangladesh is producing 64% of electricity using natural gas while 25% is coming from petroleum and the rest from coal, renewable sources [1]. To reduce pressure from the natural gas government is trying to diversify the energy sources. To achieve sustainable diversified energy sources, the Sustainable and Renewable Energy Development Authority (SREDA) has taken many initiatives. Energy Master Plan has been created which targets to improve Primary Energy consumption per GDP by 20% in the year 2030. Many software applications have been developed by SREDA to monitor the progress. We propose to develop another system for energy data analysis and generate reports accordingly. Policymakers can use those reports to make strategic decisions to secure the country's energy consumption, distribution and manage the demand side. In this research work, we discussed how a big data-based solution can be developed and used to forecast the energy balance which ensures the energy security of Bangladesh. We also discuss different aspects of the proposed software system in detail including challenges and possible solutions to overcome those challenges."
Remote Early Research Experiences for Undergraduate Students in Computing,"Alm, Cecilia O. and Bailey, Reynold and Miller, Hannah",10.1145/3478431.3499283,2022,"We provide an experience report about a remote framework for early undergraduate research experiences, which was thematically focused on sensing humans computationally. The framework included three complementary components. First, students experienced a team-based research cycle online, spanning formulating research questions, conducting literature review, performing fully remote human subject data collection experiments and data processing, analyzing and making inference over acquired data with computational experimentation, and disseminating findings. Second, the virtual program offered a set of professional development activities targeted to developing skills and knowledge for graduate school and research career trajectories. Third, it offered interactional and cohort-networking programming for community-building. We discuss not only the unique challenges of the virtual format and the steps put in place to address them but also the opportunities that being online afforded to innovate undergraduate research training remotely. We evaluate the remote training intervention through the organizing team's post-program reflection and the students' perceptions conveyed in exit interviews and a mid-program focus group. In addition to outlining lessons learned about more or less successful framework elements, we offer recommendations for applying the framework at other institutions as well as how to transfer activities to in-person formats."
CS Curricular Innovations with a Liberal Arts Philosophy,"Teresco, James D. and Tartaro, Andrea and Holland-Minkley, Amanda and Braught, Grant and Barnard, Jakob and Baldwin, Douglas",10.1145/3478431.3499329,2022,"A liberal arts context offers unique opportunities for curricular innovation that can inform the implementation of computing curricula more broadly. The SIGCSE Committee on Computing Education in Liberal Arts Colleges has collected 18 model curricula during affiliated events at SIGCSE symposia over the past two years. Here we conduct a distillation of the curricula and discuss themes across the curricula including: flexible pathways through majors, interdisciplinary initiatives, and preparing students for a range of careers and their first job. Our discussion focuses on how liberal arts colleges are empowered to think creatively about computing curricula and how research intensive universities, community colleges, and K-12 can leverage these approaches for their context."
Early Detection System of Cataract using Haar-like Feature-Based Cascade Classifiers,"Dewabrata, Pandu and Sari, Yuita Arum and Dwi Novita, Hera and Arifin, Samsul",10.1145/3479645.3479672,2021,
A Data Slicing Method to Improve Machine Learning Model Accuracy in Bankruptcy Prediction,"Ye, Ziyuan",10.1145/3480001.3480008,2021,"High-accuracy bankruptcy prediction has been important to investors and corporate finance officers for decades. With bankruptcy data in China and Poland given, this paper is an exploratory study attempting to aid feature engineering in bankruptcy predictions through a new exploratory method we call “Data Slicing.” Our data slicing analysis relies on making predictions on carefully selected and sliced financial datasets and measuring each sliced dataset's prediction accuracy. According to the findings in this research, the most related metric and the best variable to slice on to get a predictable sliced dataset turn out to be “Solvency Ratio” both in Chinese and Polish data. Simultaneously, using two different sliced datasets, the accuracy of machine learning and deep learning methods is improved. Support Vector Machine, Neural Networks and Random Forest methods are suggested to use in bankruptcy detection for higher accuracy. In summary, investors and other risk management officers are highly recommended to pay attention to firm's ability to pay debts, especially in their valuation attempts and forecasts."
The construction and analysis of the performance evaluation model of graduates' development assistance,"Yan, Wenlang",10.1145/3481127.3481171,2021,"Under the normalization of epidemic prevention and control, it has been a public concern to improve the achievements of the Poverty Alleviation Policy. In particular, efforts in educational institutions have become the key path to achieve the targeted poverty alleviation, whose mission is to build a funding system to support graduated student employment. Based on previous research results, this paper introduces a new system of development funding evaluation. In an anonymous A University, the new system has been utilized to integrate the balanced scorecard and the key performance indicator method to carry out validation analyses. Furthermore, it also summarizes the countermeasures and leads to suggestions which has been proved significantly improving the funding system performance by statistical analyses."
Research on the Construction of Project Management Information System Based on Multi-tier Architecture,"Yan, Yongbing and Song, Yuhui and Zhang, Xia and Ai, Lin",10.1145/3482632.3483995,2021,"The introduction of informatization standard for special qualification of general contracting enterprises has set off a wave of informatization in construction industry. Multi-tier distributed technology has many different concepts and requirements from the previous single-tier or two-tier structure in terms of design ideas and system implementation. In the construction of system structure model, the object-oriented method is used, and the concept of application server is introduced, emphasizing that rational design and use of enterprise objects is the main responsibility of building system model. The project management system described in this paper is designed and implemented based on multi-tier architecture, which maximizes user rights customization in terms of functions, supports multi-project collaborative management, supports concurrent environment with 500 users in performance, has good security performance and scalability, and fully meets the innovative management needs of enterprises for project management methods."
On the use of test smells for prediction of flaky tests,"Camara, Bruno and Silva, Marco and Endo, Andre and Vergilio, Silvia",10.1145/3482909.3482916,2021,"Regression testing is an important phase to deliver software with quality. However, flaky tests hamper the evaluation of test results and can increase costs. This is because a flaky test may pass or fail non-deterministically and to identify properly the flakiness of a test requires rerunning the test suite multiple times. To cope with this challenge, approaches have been proposed based on prediction models and machine learning. Existing approaches based on the use of the test case vocabulary may be context-sensitive and prone to overfitting, presenting low performance when executed in a cross-project scenario. To overcome these limitations, we investigate the use of test smells as predictors of flaky tests. We conducted an empirical study to understand if test smells have good performance as a classifier to predict the flakiness in the cross-project context, and analysed the information gain of each test smell. We also compared the test smell-based approach with the vocabulary-based one. As a result, we obtained a classifier that had a reasonable performance (Random Forest, 0.83%) to predict the flakiness in the testing phase. This classifier presented better performance than vocabulary-based model for cross-project prediction. The Assertion Roulette and Sleepy Test test smell types are the ones associated with the best information gain values."
Mining Evidences of Internet of Robotic Things (IoRT) Software from Open Source Projects,"Albonico, Michel and Rohling, Adair and Santos, Juliano and Varela, Paulo",10.1145/3483899.3483900,2021,"The current world scenario is heading to contactless technologies, where robots are in the center. These systems usually benefit from Internet of Things (IoT) sensing, being named Internet of Robotics Things (IoRT) systems. Developing IoRT software naturally involves high levels of complexity, which may be softened with well-established architectural evidence. In this paper, we aim at mining IoRT software architectural evidence from open source IoRT software repositories. For this, we (i) extract a dataset from GitHub repositories containing real open-source IoRT systems, (ii) mine relevant information from those repositories, (iii) and compile a catalog of architectural software characteristics. The catalog from our study can then be used by practitioners architects."
ModelGame: A Quality Model for Gamified Software Modeling Learning,"J\'{u}nior, Ed and Farias, Kleinner",10.1145/3483899.3483910,2021,"Gamification has been adopted in software development tasks in recent years. This adoption seeks, for example, to improve the engagement of developers while creating UML models or writing code. Empirical studies report that UML models suffer from incompleteness and inconsistency problems. This study conjectures that gamification mechanics can improve learner engagement while learning software modeling, mitigating such problems concerning UML models. The current literature lacks studies that explore gamification and UML model quality in the context of software modeling learning. This article, therefore, proposes ModelGame, which is a quality model to support software modeling learning in a gamified way. It serves as a reference framework so that instructors can obtain a parameterized way to evaluate UML models created by learners. The quality of UML models can be improved by applying gamified activities and providing guidelines aware of quality issues. A qualitative questionnaire was answered by 19 instructors who teach software modeling at higher education institutions. The results show that (1) 94.7% recognize that the proposed model can improve the quality of UML models, indicating that they would adopt the ModelGame in their learning practices; and (2) 47.4% do not use any gamification mechanics in their classes. The results are encouraging, showing the potential for applying and improving the teaching and learning of software modeling."
Graduates' Prediction System Using Artificial Intelligence,"Suresh, Nalina and Hashiyana, Valerianus and Nhinda, Gabriel Tuhafeni and Stephanus, Ismael and Kautwima, Paulus",10.1145/3484824.3484873,2022,"Graduation rates are an essential metric for universities to gauge the effectiveness of their programmes. Prediction models can be used to assess students' likelihood of completing their degrees, as well as to analyse the rate of completion of programmes that they offer.Several studies have been done previously using predictive models in various universities around the world. At King Mongkut's University of Technology North Bangkok (KMUTNB), Prachuabsupakij and Wuttikamolchai developed a web application that used the Decision Tree Algorithm to predict student's graduation. Other studies made use of other machine learning algorithms such as support vector machine, neural network and classification and regression tree algorithms. These studies achieved a successful prediction rate of an average of above 70% accuracy.The University of Namibia does not have any predictive models of any kind and neither has any study of this nature ever been done. It was for this reason that this study was conducted using the School of Computing as the case study. The study would then develop a graduates' prediction system for the school of computing.The system is a web-based application that requires a user to log in before accessing its features, such as displaying the predicted outcomes. A sample of 500 student data was used to create the student dataset. The prediction was done using four different prediction models which were then comparatively analysed. Those prediction models are Neural Network, Decision Tree, Support Vector Machine and Random forest.The web-based application consists of an interactive dashboard to allow the user to visualize the prediction results which can be view using different types of charts all to the user's convenience.This study managed to attain its main object of creating a student graduation prediction system for the school of computing and the sub-objective were also met. The predictions of this study were purely based on the students' academic results in the form of credit scores and other factors were not considered. This study recommends that further research should be done with actual student data from the university's student database records that stretches for over a longer period, say about 5 to 10 years."
Object Detection using Deep Learning: A Review,"Kaur, Biponjot and Singh, Sarbjeet",10.1145/3484824.3484889,2022,"Object detection is one of the most critical and challenging tasks in computer vision. It is the process of finding objects belonging to some predefined categories and determining their location in an image or video. This paper reviews deep learning-based object detection models. The paper discusses some benchmark datasets. The performance evaluation of different detectors on different datasets based on mean Average Precision (mAP) is reviewed. Object detection is used in different fields in different forms. Applications of object detection like pedestrian detection, autonomous driving, face detection, etc., are presented. Finally, the future scope is discussed to work on new techniques for object detection."
Does Code Complexity Affect the Quality of Real-Time Projects? Detection of Code Smell on Software Projects using Machine Learning Algorithms,"Patnaik, Archana and Padhy, Neelamadhab",10.1145/3484824.3484911,2022,"Code smell targets to identify bugs that occur due to incorrect analysis of code during software development life cycle. It is the task of analyzing a code design problem. The primary causes of code smell are complexity in structural design, violation of programming paradigm, and lack of unit-level testing by the software programmer. Our research focuses on the identification of code smell using different machine learning classifiers. We have considered 15 software code metrics of the Junit open source project and developed a hybrid model for code smell detection. Our dataset consists of 45 features which is further reduced by 15 using various feature selection techniques. Random sampling is used to handle the imbalance in the dataset. The project's performance is evaluated using 10 machine learning techniques which including regression, ensemble methods, and classification. Based on the statistical analysis, it is analyzed that the Random forest ensemble classifiers give best result with an accuracy of 99.12 % is the most appropriate technique for detecting different types of bad smells like god class, duplicate code, long method, large class, and refused bequest."
Construction Company Competitiveness Research on Informationization Policies,"Zhang, Xinli and He, Wentao and Gao, Tian",10.1145/3485190.3485195,2021,"Construction company competitiveness (CCC) is positively influenced by promoting Informatization in the construction field. This paper explores this influencing mechanism to enhance CCC with the empirical method and event study. To begin with, we selected an index of stock returns and three important policy events in the past several years. Then we set up a market model and got excess returns by subtracting the actual rate of return and the expected rate of return in the window period. At last, we performed a single sample mean test on it. It shows that since 2016, only one policy has shown effectiveness, and based on it, CCC suggests a positive trend in the short term. And in terms of different sub-sectors, the competitiveness improved to a different degree. Three conclusions can be concluded. Firstly, the effective implementation of informatization policies can significantly enhance the competitiveness of companies. Second, continuous response to policies is an essential part of increasing competitiveness. At last, with the same policy carrying out, sub-sector industries have a diverse impact on competitiveness."
Introduction to A Compromise Programming Based Method for Complex Scheduling and Planning Problems,"Tung Ngo, Son and Jaafar, Jafreezal and Abdul Aziz, Izzatdin and Hoang Nguyen, Giang",10.1145/3485190.3485231,2021,"Planning and Planning (SP) plays a vital role in many fields. However, SP problems become more complex when they require to archive multi goals in decision-making processes that are more difficult to solve and push the decision-maker into a dilemma. This paper introduces an adaptive method based on the compromise programming approach to multi-objective optimization (MOP) in scheduling and planning (SP) problems. The proposed method gives an effective integration of mathematical programming with evolutionary algorithms (EA). Through the technique, decision-makers can validate the models as well as evaluate different decision alternatives. The method is in the development progress. However, we have obtained preliminary results by applying the method for solving some SP problems. These results show the feasibility of the proposed method."
Effect of Sensory-based Technologies on Atypical Sensory Responses of Children with Autism Spectrum Disorder: A Systematic Review,"Deng, Lingling and Rattadilok, Prapa and Saputra Hadian, Gabrielle and Liu, Haoyang",10.1145/3485768.3485782,2021,"Atypical sensory responses are one of the most common issues observed in Autism Spectrum Disorder (ASD), affecting the development of a child's capability for social interaction, independent living and learning. In the past two decades, there has been a growing number of studies of technology-based interventions for atypical sensory responses of individuals with ASD. However, their effects and limitations have not been fully examined. This systematic review investigates the effects of sensory-based technologies (SBTs) on atypical sensory responses of children with ASD. Publications that report on the use of a SBT as an intervention tool were retrieved from four academic databases: “PubMed”, “IEEE Xplore”, “ACM Digital Library” and “Web of Science”. The search finally yielded 18 articles. The results indicated an emerging trend of studies investigating the effects of SBTs on atypical sensory responses over the past decade. Challenges and limitations were found in studies, mainly because the literatures adopted different methods and indicators, small sample sizes, and varying experimental designs. Findings were that the use of SBTs could effectively improve auditory and visual recognition, and some other behavioural outcomes such as attention in children with ASD. Future development of SBTs could further integrate more advanced techniques, such as machine learning, in order to widen the scope of SBTs usage to help more ASD children."
Literature teaching methods: A systematic mapping,"N\'{u}\~{n}ez-Pacheco, Rosa and Guill\'{e}n-Ch\'{a}vez, Evelyn-Paola and Barreda-Parra, Aym\'{e} and S\'{a}nchez-G\'{o}mez, Mª Cruz",10.1145/3486011.3486423,2021,"This research is a study from a qualitative perspective. Its purpose is to carry out a systematic mapping of methods and methodological approaches related to the teaching of literature. The search was carried out in Scopus and Web of Science databases in the period 2016-2021. The results of this study show that the most published countries on this topic are Turkey and Malaysia. In addition, the methodological approach used is qualitative predominantly. Besides, there is a strong link between the teaching of literature with the use of technology and the teaching of English as a foreign language. Consequently, the methods used are appropriate to these purposes. Finally, it highlights the aesthetic, civic, and moral values that emerge from the teaching of literature."
Open approach of scaled agile for organizations and communities dedicated to the development of Open-Source projects,"Gonz\'{a}lez-Bl\'{a}zquez, Jos\'{e} Luis and Garc\'{\i}a-Holgado, Alicia and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e}",10.1145/3486011.3486543,2021,"How organizations develop and provide their services and software projects is undergoing significant changes and the work methodology applied has agility as its key element when it comes to successfully facing this process for the transformation of traditional business models. This scenario is not foreign to companies dedicated to the development of open-source projects, whose products have experienced a boom over the last decade, leading more and more organizations to include them in their portfolios to improve their IT management, accelerate their digital transformation and boost their businesses. These changes are due to the unique qualities and characteristics of free software, which provide organizations with a wide range of helpful solutions for data analysis, distributed communication, or application deployment. It is essential for these types of companies, which are often supported by communities of developers, to have a culture and values aligned with agility, to prevent the numerous problems that may be caused by the lack of agility concerning the sustainability and transparency of their projects, thereby seriously damaging their competitiveness. This work presents the current state of a doctoral research to support a new open approach that provides solutions and proposals to these organizations and Open-Source communities, providing them with values, principles, guidelines, and agile skills with which to respond to the difficulties and complex dynamics that they face in the agile scaling process."
Crowdsourcing in Digital Marketing,"Al-Nafisa, Afnan S and Alnafessah, Ahmad",10.1145/3487664.3487753,2022,"considerable improvement of information communication technologies and computing means that have been effectively utilized among users enable them to participate in different business activities. Companies can exploit the crowdsourcing concept to accomplish their goals and objectives of the digital marketing strategic plan by engaging the users in various marketing activities to increase the marketing campaign’s success rate. Thus, the purpose of this paper is to survey relevant research papers related to the crowdsourcing concept and digital marketing process and crowdsourcing utilization and deployment in the company’s marketing activities. From digital marketing, perspective crowdsourcing might be utilized in various activities such as product development, advertising and promotion, market research, communication, generating innovative ideas and, others."
A Systematic Mapping Literature of Immersive Learning from SVR Publications,"Fernandes, Filipe and Castro, Diego and Werner, Claudia",10.1145/3488162.3488163,2022,"Immersive Learning (iL) is known as a recent area of research that uses three-dimensional virtual environments and multi-sensory devices, also known as immersive technologies, to support the improvement of learning outcomes. This work aims to obtain evidence of theoretical and technological aspects of iL from the Symposium on Virtual and Augmented Reality (SVR) publications. A Systematic Literature Mapping protocol was developed and executed in order to select the primary studies to perform the analysis and data extraction. 76 primary studies helped to answer the research questions. A large part of the contributions by the SVR community are virtual environments that support education in the health area. In addition, some gaps and research opportunities were identified: virtual environments that serve audiences with special needs; development frameworks that consider pedagogical aspects and the use of biometric measures to support the validation of improved learning outcomes."
The Autonomous Vehicle Models to Minimize the Impact of Pandemic,"Prasetyadi, Abdurrakhman and Trianggoro, Cahyo and Yudhi Rezaldi, Muhammad and Nugroho, Budi",10.1145/3489088.3489116,2022,"Cleaner during the Covid-19 pandemic are at the forefront of implementing health protocols in public places because they have to clean the environment with disinfectants, therefore they are at a very high risk of being infected. In addition, access to virus testing facilities is quite problematic during a pandemic. Therefore, several studies have developed autonomous vehicle (AV) models to reduce the spread of virus infections. In this paper, we perform an analysis of an effective AV model to minimize the impact of the pandemic. This study conducted a systematic review using the PRISMA approach and clustering by coupling to a dataset of AV development papers. Data sources were obtained from Scopus, Web of Science (WoS), and Lens. Firstly, AGV models for routine hospital services such as deliver medicine and food and measuring patient temperature. Secondly, providing logistics of goods and daily necessities with the AI-driven transportation model for the self-isolation community. Third, the autonomous UVC-disinfection model was used to clean and disinfect treatment healthcare rooms. Fourth, UAV's models were used to deliver swab test kits for self-isolation communities."
Digital Tools to Promote Healthy Eating for Working-Age Individuals: A Scoping Review,"Pan, Sibo and Ren, Xipei and Vos, Steven and Brombacher, Aarnout",10.1145/3490355.3490356,2022,"In this scoping review, we aimed to understand current developments of digital tools for promoting healthy eating behaviors in a work context among working-age individuals and identify research gaps for future design opportunities. The papers published over the last decade (2010-2021) were searched in three databases: the Association for Computing Machinery (ACM) digital library, the interdisciplinary library Scopus, and the PubMed database. Initially, 2098 papers were identified, of which 16 papers were included in the final analysis. These 16 papers were published in 15 various conference proceedings or journals between 2010 and 2021, and mainly focused on tracking eating moment and promoting healthy food intake. Our findings showed that four types of digital tools for healthy eating promotion were commonly used, including mobile applications, wearables, service, and multicomponent (i.e., a combination between mobile apps and wearables). Moreover, we found that current digital tools made small using a range of existing working infrastructures. Future design research could focus on personalized, interactive, and playful digital tools in human-computer interaction field with behavior change techniques and user-centered approaches to promote healthy eating behaviors in daily work routines."
“It’s Freedom to Put Things Where My Mind Wants”: Understanding and Improving the User Experience of Structuring Data in Spreadsheets,"Chalhoub, George and Sarkar, Advait",10.1145/3491102.3501833,2022,"Despite efforts to augment or replace the 2-dimensional spreadsheet grid with formal data structures such as arrays and tables to ease formula authoring and reduce errors, the flexible grid remains overwhelmingly successful. Why? We interviewed a diverse sample of 21 spreadsheet users about their use of structure in spreadsheets. It emerges that data structuring is subject to a complex network of incentives and constraints, including factors extrinsic to spreadsheets such as the user’s expertise, auxiliary tools, and collaborator needs. Moreover, we find that table columns are an important abstraction, and that operations such as conditional formatting, data validation, and formula authoring can be implemented on table columns, rather than cell ranges. To probe this, we designed 4 click-through prototypes for a follow-up study with 20 participants. We found that although column operations improved the value proposition of structured tables, they are unlikely to supplant the advantages of the flexible grid."
"Much Realistic, Such Wow! A Systematic Literature Review of Realism in Digital Games","Rogers, Katja and Karaosmanoglu, Sukran and Altmeyer, Maximilian and Suarez, Ally and Nacke, Lennart E.",10.1145/3491102.3501875,2022,"Researchers reference realism in digital games without sufficient specificity. Without clarity about the dimensions of realism, we cannot assess how and when to aim for a higher degree of realism, when lower realism suffices, or when purposeful unrealism is ideal for a game and can benefit player experience (PX). To address this conceptual gap, we conducted a systematic review using thematic synthesis to distinguish between types of realism currently found in the digital games literature. We contribute qualitative themes that showcase contradictory design goals of realism/unrealism. From these themes, we created a framework (i.e., a hierarchical taxonomy and mapping) of realism dimensions in digital games as a conceptual foundation. Our themes and framework enable a workable specificity for designing or analyzing types of realism, equip future work to explore effects of specific realism types on PX, and offer a starting point for similar efforts in non-game applications."
Weaving Stories: Toward Repertoires for Designing Things,"Oogjes, Doenja and Wakkary, Ron",10.1145/3491102.3501901,2022,"While much work is underway within the context of posthuman design, this research is often described from a dominantly human perspective. It rarely accounts for the creative capacities of nonhumans in design, such as materials, tools, and software. There is a need to further engage with posthuman theories conceptually, materially, and methodologically. We approach this challenge through Ron Wakkary's concept of repertoires: actions the human designer can take to increase participation of nonhumans in design research practice. This paper reports on potential repertoires' development by exploring three approaches from outside of HCI: describing the landscape, noticing, and translations. We use these methods to account for weaving events that the first author was engaged in. Through critical reflection of these accounts, we contribute three repertoires and an example of applying the theoretical framework of Designing Things."
Making Data Tangible: A Cross-disciplinary Design Space for Data Physicalization,"Bae, S. Sandra and Zheng, Clement and West, Mary Etta and Do, Ellen Yi-Luen and Huron, Samuel and Szafir, Danielle Albers",10.1145/3491102.3501939,2022,"Designing a data physicalization requires a myriad of different considerations. Despite the cross-disciplinary nature of these considerations, research currently lacks a synthesis across the different communities data physicalization sits upon, including their approaches, theories, and even terminologies. To bridge these communities synergistically, we present a design space that describes and analyzes physicalizations according to three facets: context (end-user considerations), structure (the physical structure of the artifact), and interactions (interactions with both the artifact and data). We construct this design space through a systematic review of 47 physicalizations and analyze the interrelationships of key factors when designing a physicalization. This design space cross-pollinates knowledge from relevant HCI communities, providing a cohesive overview of what designers should consider when creating a data physicalization while suggesting new design possibilities. We analyze the design decisions present in current physicalizations, discuss emerging trends, and identify underlying open challenges."
Crystalline: Lowering the Cost for Developers to Collect and Organize Information for Decision Making,"Liu, Michael Xieyang and Kittur, Aniket and Myers, Brad A.",10.1145/3491102.3501968,2022,"Developers perform online sensemaking on a daily basis, such as researching and choosing libraries and APIs. Prior research has introduced tools that help developers capture information from various sources and organize it into structures useful for subsequent decision-making. However, it remains a laborious process for developers to manually identify and clip content, maintaining its provenance and synthesizing it with other content. In this work, we introduce a new system called Crystalline that automatically collects and organizes information into tabular structures as the user searches and browses the web. It leverages natural language processing to automatically group similar criteria together to reduce clutter, and uses passive behavioral signals such as mouse movement and dwell time to infer what information to collect and how to visualize and prioritize it. Our user study suggests that developers are able to create comparison tables about 20% faster with a 60% reduction in operational cost without sacrificing the quality of the tables."
What Pronouns for Pepper? A Critical Review of Gender/ing in Research,"Seaborn, Katie and Frank, Alexa",10.1145/3491102.3501996,2022,
How HCI Adopts Service Design: Unpacking current perceptions and scopes of service design in HCI and identifying future opportunities,"Lee, Jung-Joo and Yap, Christine Ee Ling and Roto, Virpi",10.1145/3491102.3502128,2022,"Service design has gained tractions in Human-Computer Interaction (HCI) as an approach to deal with changes of technology design scopes. In the meantime, there are confusions around definitions of service design and its relevance to HCI. Despite the co-existence of interests and confusions, little research has been done for a comprehensive overview of how HCI interprets and adopts service design. This research performed a systematic literature review on extant HCI publications that claim to use service design. The review findings from the 179 publications revealed varying dimensions of service design taken up in HCI, relations between service design scopes and emerging technologies, as well as unclarity to service design in HCI and HCI's current tendency to use service design for the interaction level rather than the system level. We discuss future design and research opportunities for HCI by integrating the system level dimensions of service design."
Shared User Interfaces of Physiological Data: Systematic Review of Social Biofeedback Systems and Contexts in HCI,"Moge, Clara and Wang, Katherine and Cho, Youngjun",10.1145/3491102.3517495,2022,"As an emerging interaction paradigm, physiological computing is increasingly being used to both measure and feed back information about our internal psychophysiological states. While most applications of physiological computing are designed for individual use, recent research has explored how biofeedback can be socially shared between multiple users to augment human-human communication. Reflecting on the empirical progress in this area of study, this paper presents a systematic review of 64 studies to characterize the interaction contexts and effects of social biofeedback systems. Our findings highlight the importance of physio-temporal and social contextual factors surrounding physiological data sharing as well as how it can promote social-emotional competences on three different levels: intrapersonal, interpersonal, and task-focused. We also present the Social Biofeedback Interactions framework to articulate the current physiological-social interaction space. We use this to frame our discussion of the implications and ethical considerations for future research and design of social biofeedback interfaces."
Hitting the Triple Bottom Line: Widening the HCI Approach to Sustainability,"Scuri, Sabrina and Ferreira, Marta and Jardim Nunes, Nuno and Nisi, Valentina and Mulligan, Cathy",10.1145/3491102.3517518,2022,"Sustainable Development (SD) in its dimensions – environment, economy, and society – is a growing area of concern within the HCI community. This paper advances a systematic literature review on sustainability across the Sustainable Human-Computer Interaction (SHCI) body of work. The papers were classified according to the Triple Bottom Line (TBL) framework to understand how the pillars of SD play into the HCI discourse on sustainability. The economic angle was identified as a gap in SHCI literature. To meet the TBL of SD, however, a balance needs to be sought across all ‘lines’. In this paper, we propose that HCI can advance the discussion and the understanding of the economic concepts around sustainability through taking a sociology perspective on the economic angle of the TBL. We sustain this claim by discussing economic concepts and the role that digital can play in redefining the established foundations of our economic system."
Use Cases for Design Personas: A Systematic Review and New Frontiers,"Salminen, Joni and Wenyun Guan, Kathleen and Jung, Soon-Gyo and Jansen, Bernard",10.1145/3491102.3517589,2022,"Personas represent the needs of users in diverse populations and impact design by endearing empathy and improving communication. While personas have been lauded for their benefits, we could locate no prior review of persona use cases in design, prompting the question: how are personas actually used to achieve these benefits? To address this question, we review 95 articles containing persona application across multiple domains, and identify software development, healthcare, and higher education as the top domains that employ personas. We then present a three-stage design hierarchy of persona usage to describe how personas are used in design tasks. Finally, we assess the increasing trend of persona initiatives aimed towards social good rather than solely commercial interests. Our findings establish a roadmap of best practices for how practitioners can innovatively employ personas to increase the value of designs and highlight avenues of using personas for socially impactful purposes."
“Merging Results Is No Easy Task”: An International Survey Study of Collaborative Data Analysis Practices Among UX Practitioners,"Kuang, Emily and Jin, Xiaofu and Fan, Mingming",10.1145/3491102.3517647,2022,"Analysis is a key part of usability testing where UX practitioners seek to identify usability problems and generate redesign suggestions. Although previous research reported how analysis was conducted, the findings were typically focused on individual analysis or based on a small number of professionals in specific geographic regions. We conducted an online international survey of 279 UX practitioners on their practices and challenges while collaborating during data analysis. We found that UX practitioners were often under time pressure to conduct analysis and adopted three modes of collaboration: independently analyze different portions of the data and then collaborate, collaboratively analyze the session with little or no independent analysis, and independently analyze the same set of data and then collaborate. Moreover, most encountered challenges related to lack of resources, disagreements with colleagues regarding usability problems, and difficulty merging analysis from multiple practitioners. We discuss design implications to better support collaborative data analysis."
Participatory Design Goes to School: Co-Teaching as a Form of Co-Design for Educational Technology,"Nicholson, Rebecca and Bartindale, Tom and Kharrufa, Ahmed and Kirk, David and Walker-Gleaves, Caroline",10.1145/3491102.3517667,2022,"Educational technologies offer benefits in the classroom but there are barriers to their successful integration, including teachers’ pedagogical beliefs and their skills and experience. Participatory Design (PD) approaches offer one way in which teachers can be directly involved in the design of classroom technologies, however PD processes alone fail to address the challenges of integrating technology within existing practices. In this paper we propose co-teaching as a novel form of co-design practice. We describe a two year longitudinal Co-Teaching project resulting in the development and use of three digital designs for the classroom. Using the TPACK model to guide our reflections we offer insights into the ways that co-teaching can support the design and integration of educational technologies. We suggest that co-teaching as a form of co-design practice offers a way to move teachers from passive adopters of technology to active participants in the design and integration of educational technologies."
Automating Contextual Privacy Policies: Design and Evaluation of a Production Tool for Digital Consumer Privacy Awareness,"Windl, Maximiliane and Henze, Niels and Schmidt, Albrecht and Feger, Sebastian S.",10.1145/3491102.3517688,2022,"Users avoid engaging with privacy policies because they are lengthy and complex, making it challenging to retrieve relevant information. In response, research proposed contextual privacy policies (CPPs) that embed relevant privacy information directly into their affiliated contexts. To date, CPPs are limited to concept showcases. This work evolves CPPs into a production tool that automatically extracts and displays concise policy information. We first evaluated the technical functionality on the US’s 500 most visited websites with 59 participants. Based on our results, we further revised the tool to deploy it in the wild with 11 participants over ten days. We found that our tool is effective at embedding CPP information on websites. Moreover, we found that the tool’s usage led to more reflective privacy behavior, making CPPs powerful in helping users understand the consequences of their online activities. We contribute design implications around CPP presentation to inform future systems design."
"Exchanging Best Practices for Supporting Computational and Data-Intensive Research, The Xpert Network","Barakhshan, Parinaz and Eigenmann, Rudolf",10.1145/3491418.3530293,2022,"We present best practices for professionals who support computational and data-intensive (CDI) research projects. The practices resulted from the Xpert Network activities, an initiative that brings together major NSF-funded projects for advanced cyberinfrastructure, national projects, and university teams that include individuals or groups of such professionals. Additionally, our recommendations are based on years of experience building multidisciplinary applications and teaching computing to scientists. This paper focuses particularly on practices that differ from those in a general software engineering context. This paper also describes the Xpert Network initiative where participants exchange best practices, tools, successes, challenges, and general information about their activities, leading to increased productivity, efficiency, and coordination in the ever-growing community of scientists that use computational and data-intensive research methods."
Evaluating research computing training and support as part of a broader digital research infrastructure needs assessment,"Rochlin, Nick and Gardner, Jeff and Kinney, Elizabeth",10.1145/3491418.3530295,2022,"Digital Research Infrastructure (DRI) refers to the suite of tools and services that enables the collection, processing, dissemination, and disposition of research data. This includes strategies for planning, organizing, storing, sharing, computing, and ultimately archiving or destroying one's research data.&nbsp; These services must be supported by highly qualified personnel with the appropriate expertise.&nbsp; From May 17 - June 12, 2021, the University of British Columbia (UBC) Advanced Research Computing (UBC ARC) and the UBC Library from both Vancouver and Okanagan Campuses launched the DRI Needs Assessment Survey to investigate UBC researchers’ needs in 25 distinct DRI tools and services.&nbsp; The survey received a total of 241 responses, and following the survey, three focus groups were conducted with survey respondents to gain additional insights.This paper outlines the DRI Needs Assessment Survey and its findings, focusing on those directly related to UBC ARC services and training in high-performance computing (HPC) and cloud computing (“Cloud”), and discusses next steps for implementing a more collaborative, comprehensive research computing training and support model. Key findings suggest that while advanced research computing infrastructure is a key pillar of DRI, researchers utilizing UBC ARC also rely on a number of other DRI tools and services to conduct their research.&nbsp; These services are widely scattered across various departments and groups within and outside the institution and are oftentimes not well communicated, impacting researchers’ ability to find them.&nbsp; Current research training and support has been found to be inadequate, and there are duplicated service efforts occurring in silos, resulting in an inefficient service model and wasted funds."
Scene Recognition and Narration from Video using Deep Learning Techniques,"Gunupudi, Rajesh Kumar and Achanta, Sai Ramya and D P, Dinesh Chandra and Ganga, Hriday Rao and Vipparthy, Niharika and Annaluri, Sreenivasa Rao",10.1145/3492547.3492566,2021,"The world becomes just a little harder if one of our senses are impaired and if that impaired sense is sight, every facet of our life becomes more challenging? Not every blind person can have a guide. This thought prompted us to work on a project called scene recognition and narration in which the application will recognize and understand the scene presented and dynamically give a verbal output. This application will help in the reduction of accidents and help make the lives of blind people just a little easier. The findings appear to provide good evidence that using widely accessible dense building blocks to approximate the predicted ideal sparse structure is a feasible technique for enhancing neural networks for computer vision. There are two more advantages to this architecture: One is that the number of units in each step is greatly increased, yet the computational complexity is kept under control. A significant number of input filters can be shielded from the final step to the next layer when dimensionality reduction is used widely. Second, visual data is processed on several scales and then aggregated so that the following phase may extract characteristics from many scales at the same time. YOLO Algorithm is used for the proposed architecture."
Change and Project Management in Digital Time: the Case of Kazakhstani University,"Mertaevna Berdykulova, Galiya and Kuralbekovich Kamysbayev, Marat and Zharzhanovich Shildibekov, Erlan and Vladimirovich Ananyev, Timur and Khamitkhanovna Abdinova, Makpal and Yessebaiuly, Erkebulan",10.1145/3492547.3492649,2021,"The article is devoted to management of Kazakhstani University in order to find the opportunities for further deepening of digitalization by introducing general, change and project management practice. Study showed inconsistency of bureaucratic style and disparity of existed concepts with practice. Types of changes, main management problems, quality management system and digitalization processes at the university were analyzed. The basic gap in existed organizational structure of university as a barrier in meeting requirements of new reality of society and new system of relationship inside of university was identified. Issue of the current educational automation associated with digitalization, the shortcomings exposed by the pandemic were figured out. Based on investigation of domestic, foreign and original research, proposals to implement the project management methodology and creation of a change management department in IITU were made. These proposals become the conditions for integration of invaluable methodological experience accumulated by teachers and for implementation of university's mission in digital age. The novelty of the research lies in the development of proposals for the transformation of the Kazakh University in the direction of meeting the requirements of the technological revolution based on the use of project management methodology and principles of change management. As a result, the proposed structure of university based on project management methodology and the creation of a change management department were justified."
Human Resource Management in a Pandemic: the Case of Kazakhstani Company,"Mertaevna Berdykulova, Galiya and Kuralbekovich Kamysbayev, Marat and Shamilevna Omarova, Aigul and Shaidullovna Sagandykova, Saule and Toktasynovna Sharapieva, Zamira and Almatuly Bolat, Rauan",10.1145/3492547.3492652,2021,"Human resource management has undergone changes and gained unique experience during the COVID-19 pandemic like the general management of a Kazakhstani company. The coronavirus and related restrictions have made huge changes to all business processes and human resource strategies not only in Kazakhstan but also in international companies. Therefore, in order to understand what changes in the personnel management system have occurred and taking place a foreign and Kazakhstani experience was studied, a comparative analysis was carried out in the field of personnel management in remote work. As a result, recommendations for improving the existing system of remote work of the company ""Future"" were made. It believes that the recommendations become internal conditions of companies for increasing effectiveness of the Human Resource department in remote work. The novelty of the research lies in the examination of the particular cases of human resource management under the terms of remote work in a pandemic based on comparative analysis of foreign experience with Kazakhstani experience. Methodology of study includes literature review of Kazakhstani and other authors in the field, survey of company employees; interviewing company leaders, head of HR department and employees; analysis of internal corporate documents of the HR department. In addition, the general research methods like an observation, analysis and synthesis, comparison, generalization, classification and definition concepts were used."
Drivers and Barriers for the development of Smart Sustainable Cities: A Systematic Literature Review,"Schuch de Azambuja, Luiza",10.1145/3494193.3494250,2022,"The term Smart Sustainable City (SSC) has been gaining popularity due to the growth of initiatives to address urban problems towards sustainable development. SSC can be considered as a combination of sustainable city and smart city, and some variance between the concepts may be expected. As this is a modern term, the literature falls short of studies presenting factors that hinder and/or facilitate the complex phenomenon of SSC development. Therefore, this paper aims to analyse scientific studies to identify aspects that influence the progress of smart sustainable cities. The methodological approach undertaken was a systematic literature review that included 169 papers. The results offer a comprehensive list of 57 drivers and 63 barriers, classified according to five main dimensions of a smart sustainable city, which are the three sustainability pillars (society, environment, and economy), combined to governance, and urban infrastructure. The findings revealed ‘governance’ as the most significant domain for SSC development, and multistakeholder engagement as one of the main challenges. This study shows that SSC is not a research field itself, but an interdisciplinary concept, contributing to academics, government, and policymakers for eradicating potential interferences in the development of smart and sustainable cities."
Exploring the Challenges of ICT Governance in Brazilian Smart Cities,"Reis, Luiz Claudio Diogo and Bernardini, Flavia Cristina and Leal Ferreira, Simone Bacellar and Cappelli, Claudia",10.1145/3494193.3494251,2022,"ICT governance in smart cities refers to efficient ICT public investment to guide cities in the achievement of their strategic goals so that cities would be able to provide a better quality of life for citizens. Thus, ICT governance in smart cities comes to be a relevant domain of study to increase public value for the local community. Nevertheless, few studies focused on ICT governance in smart cities, especially in the Brazilian context, which evidenced a gap research in this domain. Empirical studies developed in Brazilian smart cities revealed lack of an ICT governance approach in the municipalities. Therefore, in this work, by applying a survey conducted with Brazilian professionals involved with smart city projects, governance, and information technology in the public sector, we introduce a set of challenges, lessons learned, and recommendations concerning ICT governance in smart city projects. The findings are of great importance for academics and practitioners involved with cities' public management concerning ICT-enabled governance."
A Literature Review on Digital Ethics from a Humanistic and Sustainable Perspective,"Teran, Luis and Pincay, Jhonny and Wallimann-Helmer, Ivo and Portmann, Edy",10.1145/3494193.3494295,2022,"The rapid technological transition requires the adoptive approach to the digital conduct of public and private institutions. Countries and companies strive to integrate a balanced understanding of digital ethics and sustainability concepts from various standpoints, which results in a dispersed and uncategorized knowledge base. This work presents a literature review on digital ethics published from 2010 to 2020 in three technical libraries and one library maintained by the community of philosophers. The investigation process integrates a thorough review of digital ethics concepts in the leading academic libraries using keywords representing various concept applications. This study's outcome is a quantitative and sectorial categorization of works on digital ethics, followed by a holistic review of concepts, maturity level, and conclusions on each category. This work aims to understand the trends from a technological and philosophical perspective towards designing a sustainable digital ethical framework applied in digital services that fulfill sufficiency thresholds of justice and do not foster overshooting of planetary boundaries. The first version of a holistic framework based on the literature review is presented at the end of this work. It will be extended in future work."
Fuzzy Comprehensive Evaluation of Online Course Based on Learner Profile and User Experience,"Qi, Wenjing and Yuan, Weihua and Zhao, Li",10.1145/3494885.3494920,2021,"Online learning raises more concerns after it helped many students go through a tough academic year under the shadow of pandemic. But the effects of online courses are not so optimistic. To measure quality of online courses, we propose a comprehensive evaluation framework based on learner profile and user experience, where learner profile is extracted from the online data analysis, and user experience is quantified from data collected in questionnaires. We extract 5 important indices for course evaluation, and quantification model is defined on each individual index; then a fuzzy comprehensive evaluation model is defined to integrates all indices to generate a straightforward evaluation result. Our evaluation models are tested with data from a massive used education platform, it shows that our model is practical and reasonable in online course evaluation."
Application of Artificial Intelligence in Teaching Reform of New Economics,"Wu, Shifeng and Li, Xiaohong and Zhu, Shuo and Shen, Weining and Wu, Jingnan and Sun, Yujie",10.1145/3495018.3495121,2022,
An intelligent Cable Detection System in Shallow Water Area,"Mai, Zhihong and Zhao, Quanzhong and Liu, Zheng and Hou, Weisheng and Zeng, Liang",10.1145/3495018.3495156,2022,"This paper studies the route and buried depth detection methods for submarine cables, which are mainly used to detect the route and buried depth of submarine cables in operation, thereby providing basic data for the subsequent operation and maintenance of submarine cables. Based on the electromagnetic induction theory and considering the geomagnetic field distribution characteristics of the submarine cable, this paper studies detection method of single three-component coil, proposes a method able to intelligent identification of routing and detect the buried depth of the submarine cable in shallow water, and develops the corresponding equipment and calculation software which achieves fine detection effect in final engineering application."
Study on Risk Identification Method of Prefabricated PC Structure Building Construction Project,"Tao, Qin and Tao, Changnan and Wang, Yao",10.1145/3495018.3495369,2022,"Risk identification is not only the primary link but also the fundamental premise in the risk research of prefabricated PC construction projects. Whether the risk can be identified in each link of the project will determine the effect of risk evaluation and risk disposal. In view of this, risk identification should be carried out and implemented in all construction links of prefabricated construction projects. At the same time, with the evolution of the project life cycle, the project risk will change dynamically. Therefore, project risk identification is a systematic and circular process."
Intelligent Segmentation Method of Indoor Scene Area Based on Deep Learning,"He, Jing",10.1145/3495018.3495439,2022,"Indoor area image recognition is a key technology of information processing, and it has a wide range of applications in various fields. This article is based on the deep learning of the indoor scene area intelligent segmentation method, and aims to explore a reliable method of indoor scene area image recognition to provide more practical value for the research in this field. This paper adopts a small area image method to detect the indoor scene area, including related applications of scene recognition and classification technology, attribute-based point cloud segmentation algorithm, acquisition and analysis of related data based on global feature layout, and then application traversal The segmentation result map obtains the salient main target image of this type of scene by detecting the saliency of the indoor scene image. The experimental data of this study confirmed that the indoor object detection data is mainly distributed in the area of larger objects, and the number of training samples is three times the number of test samples, so the experimental data is quite reliable."
A Study on Curriculum System and Informatization through Computer Big Data and Network Multimedia Technology,"Jiang, Yujie",10.1145/3495018.3501139,2022,"Designing and developing a platform that can provide students with learning communication can improve the enthusiasm and autonomy of learning and communication. This platform can help teachers manage students' curriculum projects and effective management of teaching resources. Based on the B/S multi-layer system structure, this paper uses MVC mode, NET and other technologies to study and discuss the construction process of a network teaching resource management platform. The paper analyses the system database and finally realizes the following functional modules: In the teacher module, teachers can view and manage courses, answer questions online, and manage the test system online. In the student module, student users can browse and download course-related information, and can perform online homework and tests. At the same time, students can communicate with other users online. The administrator module can manage all users, manage course information, and course behaviour, and restrict and manage online communication."
Application of Amazon Web Services within teaching &amp; learning at Coventry University Group,"Flood, Daniel and Hall, Alan",10.1145/3498343.3498350,2022,"Since September 2020, CU Group offers a Cloud Computing BSc(Hons) degree program developed with support from Amazon Web Services (AWS), leveraging their platform for teaching and learning and building upon their Cloud Competency Framework (CCF). This paper seeks to share how industry cloud technology stacks can be employed within authentic student learning. This has ensured a scenario led assessment for learning approach as an alternative to a simulated learning environment. CU Group is also an AWS Academy and Educate member, facilitating certification and platform access. This incorporates the experience of an industry leader in direct collaboration with HE to embed cloud skills within existing programs or develop specific curricula to target-fill skill gaps within the cloud landscape. Current students will graduate in August 2022, the first degree level graduates globally for HEIs working within the CCF."
Analyzing Neural Correlations Between Numerical Induction and Letter Induction Based on Data-Brain Driven Integration Evidence,"Ma, Lianfang and Chen, Jianhui and Zhong, Ning",10.1145/3498851.3498969,2022,"Numerical induction and letter induction are two kinds of important subtypes of induction. Analyzing their neural correlations is very important for understanding the common mechanism of induction. Previous comparative studies on number cognition and letter comprehension were mainly based on a group of comparative experiment designs and their neuroimaging data. However, because of the many-to-many structure-function relationships, it is difficult to understand neural correlations between number cognition and letter comprehension, especially in complex cognitive functions, such as induction, only based on single-task or few-task neuroimaging data within an experimental lab. This paper proposes a systematic approach to analyze the similarity and disimilarity of neural pattern between numerical and letter induction by using Data-Brain driven integration evidence. Under the four dimensions of Data-Brain, a group of internal and external evidence is collected. A three stages multi-task analytical method is proposed to understand the similarity and disimilarity of neural pattern between numerical and letter induction, by combining meta-analysis and representational similarity. Results show that more activation specific for inductive reasoning is left MFG and IFG. And number inductive reasoning and letter inductive reasoning have high neural pattern similarity in the IFG and MFG, and a significant main effect of inductive reasoning is in the left MFG. Other hand, the method can supplementary proof some results, it has important implications for understand the brain mechanism of information processing."
What do We Know about Computing Education for K-12 in Non-formal Settings? A Systematic Literature Review of Recent Research,"Gardner, Tracy and Leonard, Hayley C. and Waite, Jane and Sentance, Sue",10.1145/3501385.3543960,2022,"Background and context. Non-formal learning for K-12 computing education enables young people to learn about computing outside the formal curriculum. Many studies have reported on non-formal initiatives but it is not always clear what children and young people have gained from their participation. Objectives. This study set out to investigate non-formal learning initiatives by means of a systematic literature review. The two research questions addressed by the study are: (1) What has been the focus of recent computing education research about K-12 initiatives for young people and (2) What is the impact of non-formal K-12 computing initiatives? Method. A systematic literature review of computing education research was conducted, focused on non-formal initiatives for young people. Research was included from any country, but must be published in English between January 2015 and April 2021. Searches using key terms were performed across three databases. 88 studies were synthesised from over 400 initial results. Findings. The vast majority of studies reported on immersive multi-day settings such as summer camps run by universities (n= 67), with fewer (n=21) reporting on regular ongoing after-school or weekend clubs. The most popular affective outcomes measured by studies were self-efficacy (n=25) and interest (n=22). Measurement of cognitive outcomes, such as knowledge (n= 13) and skills (n=17), was less prevalent. 22 different topics were identified from the studies, with most studies being programming-heavy. The majority of papers measured the short-term impact of these interventions, and generally there was an inconsistent or incomplete reporting of learner characteristics across the studies. Implications. The lack of papers investigating regular after-school initiatives suggests that the majority of non-formal learners are not being studied or that summer school findings are being wrongly extrapolated to this setting. More rigorous research is needed for regular after-school and short-term non-formal contexts to ensure that this set of learners’ experiences is understood and potentially improved."
Surfacing Inequities and Their Broader Implications in the CS Education Research Community,"McGill, Monica M. and Davis, Sloan and Reyes, Joey",10.1145/3501385.3543969,2022,"Problem. Diversity, equity, and inclusion (DEI) need to be embedded throughout the computer science education (CSEd) research community in order to achieve empirically-based strategies in CSEd that is responsive to the needs of all of its constituents. However, there are no comprehensive studies that investigate what the barriers and challenges to DEI are among CSEd researchers. Research Question. When considering DEI among the CSEd research community, what barriers and challenges do different CSEd researchers face when conducting research?Method. We conducted a systematic literature review, developed a survey from the literature, and analyzed the quantitative and qualitative data from participants (n=72). Findings. Beyond finding that over half of the participants reported the COVID-19 pandemic as a barrier to engaging in research, participants reported that working more than an average 40-hour work week each year was a challenge. The lack of computing education being recognized as a subdiscipline within CS departments also was a barrier. Participants also reported that a lack of 1) awareness and adoption of practices from other education research fields and 2) general educational research theory were significant challenges for the CSEd research field. With respect to DEI, participants noted that lack of diversity among CSEd research partners/collaborators, among CSEd researchers in the community and among CSEd research community leadership are challenges for the community. Implications. Employing cultural competence is integral to CSEd research as we, as a community, inherently navigate differences in identities among researchers, and between researchers, practitioners, and participants in the currently unrepresentative and inequitable state of our field. As we grow our attitude, awareness, knowledge, and skill in cultural competence, we produce better-equipped allies, and greater resilience and belonging among community members from historically marginalized groups. We urge the community and relevant stakeholders to understand how to remove the barriers and challenges identified in our study."
Inclusivity Bugs in Online Courseware: A Field Study,"Chatterjee, Amreeta and Letaw, Lara and Garcia, Rosalinda and Reddy, Doshna Umma and Choudhuri, Rudrajit and Kumar, Sabyatha Sathish and Morreale, Patricia and Sarma, Anita and Burnett, Margaret",10.1145/3501385.3543973,2022,"Motivation: Although asynchronous online CS courses have enabled more diverse populations to access CS higher education, research shows that online CS-ed is far from inclusive, with women and other underrepresented groups continuing to face inclusion gaps. Worse, diversity/inclusion research in CS-ed has largely overlooked the online courseware—the web pages and course materials that populate the online learning platforms—that constitute asynchronous online CS-ed’s only mechanism of course delivery. Objective: To investigate this aspect of CS-ed’s inclusivity, we conducted a three-phase field study with online CS faculty, with three research questions: (1)&nbsp;whether, how, and where online CS-ed’s courseware has inclusivity bugs; (2)&nbsp;whether an automated tool can detect them; and (3)&nbsp;how online CS faculty would make use of such a tool. Method: In the study’s first phase, we facilitated online CS faculty members’ use of GenderMag (an inclusive design method) on two online CS courses to find their own courseware’s inclusivity bugs. In the second phase, we used a variant of the GenderMag Automated Inclusivity Detector (AID) tool to automatically locate a “vertical slice” of such courseware inclusivity bugs, and evaluated the tool’s accuracy. In the third phase, we investigated how online CS faculty used the tool to find inclusivity bugs in their own courseware. Results: The results revealed 29 inclusivity bugs spanning 6 categories in the online courseware of 9 online CS courses; showed that the tool achieved an accuracy of 75% at finding such bugs; and revealed new insights into how a tool could help online CS faculty uncover assumptions about their own courseware to make it more inclusive. Implications: As the first study to investigate the presence and types of cognitive- and gender-inclusivity bugs in online CS courseware and whether an automated tool can find them, our results reveal new possibilities for how to make online CS education a more inclusive virtual environment for gender-diverse students."
Multivariate Data Fusion Method Based on 3DGIS and its Application in Engineering Management,"Liu, Chengkun and Ma, Rui and Hu, Binbin and Fan, Qingsong",10.1145/3501409.3501656,2022,"In the life-cycle engineering management of a project, a large amount of diversified and heterogeneous data is involved. How to efficiently manage and visualize various data sources is the basis for realizing the refined management of project construction. Based on the concept of data fusion, this article sorts out the multiple data involved in project engineering management, analyses the data fusion method and process of multivariate structured data and geospatial data. By combining 3DGIS in data integration, management and visualization, data fusion and expression in a three-dimensional environment can be realized. Taking the engineering management system of the bank slope of Song Gang Area as an example, it verifies the validity of the multivariate data fusion method based on 3DGIS in this paper."
Construction and Innovative Application of Chongqing Ecological Environment Big Data Platform,"Huang, Xiaoyan and Wang, Luxiao and Hu, Xiaoming and Jiang, Rong and Zhang, Yanjun and Fu, Juanjuan and Zhang, Xiumei",10.1145/3501409.3501703,2022,"This paper introduces the overall structure design of Chongqing Ecological Environment Big Data Platform ""1 + 5 + N"", that is, constructing a full-coverage ecological environment big data platform, deepening and expanding the comprehensive application system of five kinds of ecological environment big data, construction and improvement of multi-ecological environment business big data system. In summing up the current situation and existing problems in the construction of Chongqing ecological environment big data platform, from four aspects of top-level design, resource sharing, business integration and innovative application, this paper puts forward suggestions on the next step of Chongqing eco-environmental big data construction."
Designing for Care Ecosystems: a Literature Review of Technologies for Children with ADHD,"Stefanidi, Evropi and Sch\""{o}ning, Johannes and Feger, Sebastian S. and Marshall, Paul and Rogers, Yvonne and Niess, Jasmin",10.1145/3501712.3529746,2022,"This paper presents a systematic review of HCI literature focusing on children with ADHD, the prevailing mental health diagnosis in children. Its aim is to (i) chart the state-of-the-art in this domain (e.g. methods used), (ii) identify the ways the HCI community has addressed the needs of children with ADHD (e.g. technologies deployed), and (iii) describe the involvement of the various stakeholders playing a role in their everyday experiences (i.e. their care ecosystem). Our findings show limited engagement of the care ecosystem in the design, development and user studies of current technologies, and shortcomings in designing for multiple ecosystem stakeholders, despite their crucial role. We also find that most HCI contributions are systems aiming to address ADHD-related symptoms. Based on our findings, we provide suggestions for further research and design considerations for future systems that empower and promote the well-being of children with ADHD, while considering their care ecosystem."
Foundations for Esports Curricula in Higher Education,"Scott, Michael James and Summerley, Rory and Besombes, Nicolas and Connolly, Cornelia and Gawrysiak, Joey and Halevi, Tzipora and Jenny, Seth E. and Miljanovic, Michael and Stange, Melissa and Taipalus, Toni and Williams, J. Patrick",10.1145/3502870.3506566,2022,"Esports has generated an industry of increasing economic and cultural importance. In recent years, universities and other higher education institutions have responded to its growth by establishing programmes of study which aim to satisfy the needs of innovators operating in the area. However, there is not yet consensus on what an esports curriculum should include. Despite being a technology-driven sector with ethical and professional dimensions that intersect computing, current ACM and IEEE curricula do not mention esports. Furthermore, existing courses tend to provide teaching and training on a wide variety of topics aside from those traditionally in computer science. These include: live events management; psychological research; sports science; marketing; public relations; video (livestream) production; and community management; in addition to coaching and communication. This working group examined the requirements for developing esports studies at universities with a focus on understanding career prospects in esports and on the challenges presented by its interdisciplinary complexity. Thereby, paving the way for a framework to support the design of esports curricula in higher education."
Evidence for Teaching Practices that Broaden Participation for Women in Computing,"Morrison, Briana B. and Quinn, Beth A. and Bradley, Steven and Buffardi, Kevin and Harrington, Brian and Hu, Helen H. and Kallia, Maria and McNeill, Fiona and Ola, Oluwakemi and Parker, Miranda and Rosato, Jennifer and Waite, Jane",10.1145/3502870.3506568,2022,"Computing has, for many years, been one of the least demographically diverse STEM fields, particularly in terms of women's participation [12, 36]. The last decade has seen a proliferation of research exploring new teaching techniques and their effect on the retention of students who have historically been excluded from computing. This research suggests interventions and practices that can affect the inclusiveness of the computer science classroom and potentially improve learning outcomes for all students. But research needs to be translated into practice, and practices need to be taken up in real classrooms. The current paper reports on the results of a focused systematic ""state-of-the-art"" review of recent empirical studies of teaching practices that have some explicit test of the impact on women in computing. Using the NCWIT Engagement Practices Framework as a means of organization, we summarize this research, outline the practices that have the most empirical support, and suggest where additional research is needed."
Professional Competencies in Computing Education: Pedagogies and Assessment,"Raj, Rajendra and Sabin, Mihaela and Impagliazzo, John and Bowers, David and Daniels, Mats and Hermans, Felienne and Kiesler, Natalie and Kumar, Amruth N. and MacKellar, Bonnie and McCauley, Ren\'{e}e and Nabi, Syed Waqar and Oudshoorn, Michael",10.1145/3502870.3506570,2022,"Competency-based learning has been a successful pedagogical approach for centuries, but only recently has it gained traction within computing. Competencies, as defined in Computing Curricula 2020, comprise knowledge, skills, and professional dispositions. Building on recent developments in competency and computing education, this working group examined relevant pedagogical theories, investigates various skill frameworks, reviewed competencies and standard practices in other professional disciplines such as medicine and law. It also investigated the integrative nature of content knowledge, skills, and professional dispositions in defining professional competencies in computing education. In addition, the group explored appropriate pedagogies and competency assessment approaches. It also developed guidelines for evaluating student achievement against relevant professional competency frameworks and explores partnering with employers to offer students genuine professional experience. Finally, possible challenges and opportunities in moving from traditional knowledge-based to competency-based education were also examined. This report makes recommendations to inspire educators of future computing professionals and smooth students' transition from academia to employment."
From Crowdsourced Software Development to Crowdtesting,"Tsai, Wei-Tek and Zhang, Li and Hu, Shufeng",10.1145/3503181.3503185,2022,"Crowdsourcing Software Development (CSD) has existed and developed for many years. Over the years, CSD has made new progress and changes. The original intention of CSD is to reduce the cost of software development. However, the crowdsourcing approach encountered difficulties in software development, so it turned to software testing. Crowdsourcing Software Testing (CST) has had some successful cases. It has many advantages, such as reducing the cost and time of software testing. In this paper, we analyze the problems that crowdsourcing faces in software development and come to the view that crowdsourcing is more suitable for software testing. We analyzed the Quadrilateral Co-petition Model in the CST platforms, and gave the methods for optimization. We also proposed a new software testing program that integrates open source sharing and crowdsourcing methods."
Feature and variability extraction from Agile specifications and their related source code for software product line migration,"Georges, Thomas",10.1145/3503229.3547065,2022,"Migrating a set of similar software products into a Software Product Line is a time-consuming and costly process which, ultimately, provides an important gain in time and customization. Conducting this migration within an agile development process is a complex process which requires discipline and adaptation. We think it can be beneficial to drive the migration by leveraging agile software specifications and the source code versioning platform. Currently, we are working on a method, whose design is explained in this paper, which exploits: (1) Epics and User stories to identify features and variability and (2) the source code associated to code merges related to User stories and Epics to locate them. We plan to extract features and variability inside Epics and User stories using Natural Language Processing (NLP) techniques. Then we plan to investigate how formal concept analysis (FCA) and relational concept analysis (RCA) can assist feature model synthesis and establish mappings between features and source code. These knowledge discovery methods have been chosen for their ability to highlight and hierarchically organize groups of similar artefacts. FCA only considers artefact description to establish groups of similar artefacts. RCA groups similarly described artefacts that, in addition, share similar relationships to other artefact groups. We also plan to evaluate the method within the context of a company (ITK) with which we collaborate, using its code base and the associated project management artifacts. We also will assess how the method can be generalized to public projects in source code versioning platforms."
Anchoring AI/Machine Learning on the African Technological Innovation and Investment Table,"Kabanda, Gabriel",10.1145/3503491.3503495,2022,"Sustainable development begins with education primarily through lifelong open and distance learning (ODL). Africa faces an economic and sustainable development problem characterized by relatively low GDP per capita and limited capacity for industrialization and modernization which has not been adequately resolved by the numerous economic frameworks used in the past. A Pragmatism paradigm intricately related to the Mixed Methods approach was used in this research premised on an endogenous growth model supported by knowledge generation strategies and anchored on AI/ML. The research proved that the resultant E-Business and E-learning supply chain gives a huge Cash Return on Investment in the higher education sector and other sectors of the economy. Technological innovation and investment anchored on the AI/Machine Learning paradigms and which takes advantage of the advances in technology (Cloud Computing, AI/ Machine Learning, Cybersecurity, Big Data Analytics, etc.) will create wealth and establish an innovation-led knowledge economy through breaking silos, synergizing and creating smart partnerships in the National Science, Technology and Innovation System (NSTIS). The phenomenal growth in the use of internet-based technologies has subjected organizations to cyberattacks, and so there is a need to revamp cyberdefense strategies for most organizations and take advantage of Cloud Computing facilities."
"Leveling Up: A Trajectory of OpenROAD, TILOS and Beyond","Kahng, Andrew B.",10.1145/3505170.3511479,2022,"Since June 2018, the OpenROAD project has developed an open-source, RTL-to-GDS EDA system within the DARPA IDEA program. The tool achieves no-human-in-loop generation of design-rule clean layout in 24 hours. This enables system innovation and design space exploration, while also democratizing hardware design by lowering barriers of cost, expertise and risk. Since November 2021, The Institute for Learning-enabled Optimization at Scale (TILOS), an NSF AI institute for advances in optimization partially supported by Intel, has begun its work toward a ""new nexus'' of AI, optimization, and the leading edge of practice for use domains that include IC design. This paper traces a trajectory of ""leveling up'' in the research enablement for IC physical design automation and EDA in general. This trajectory has OpenROAD and TILOS as waypoints, and advances themes of openness, infrastructure, and culture change."
Augmented and Virtual Reality-Driven Interventions for Healthy Behavior Change: A Systematic Review,"Paul Odenigbo, Ifeanyi and AlSlaity, Alaa and Orji, Rita",10.1145/3505284.3529964,2022,"Augmented Reality (AR) and Virtual Reality (VR) have shown potential benefits in managing healthy behavior. This paper presents a systematic review of AR- or VR-driven interventions for promoting healthy behaviors. The review investigates the design, implementations of the intervention, persuasive strategies, intervention platforms, underlying technologies, current trends, and research gaps. Our review of the past 10-years' work in the area reveals that 1) the considered papers focused on seven main healthy behaviors, where “alcohol use” emerged as the most commonly considered behavior; 2) trustworthiness emerged as the most commonly used persuasive strategy; 3) youth are the most targeted audience; 4) VR is more common than AR; and 5) most AR- or VR-driven interventions are perceived to be effective in motivating healthy behavior in people. We also uncover how they use Artificial Intelligence and Object Tracking in this space. Finally, we identify gaps and offer recommendations for advancing research in this area."
The Implementation of Project-based Learning Approach in Technical Courses: An Investigation into Students’ Perceptions,"Thi Van Pham, Anh and Huu Tran, Thien",10.1145/3505711.3505728,2022,"Project-based learning is widely regarded as a viable strategy for enhancing student learning in higher education. Project-based learning effectively bridges the theoretical and practical education gaps by encouraging true knowledge, initiative, and a greater comprehension of the subject. This study was conducted to explore students’ perceptions towards the implementation of project-based learning in technical courses in a college in Vietnam where this method has been employed for several years. Data were collected from 98 students majoring in website design and software engineering courses. The questionnaire was developed in Google Form, and the link was sent to the participants via their email addresses. The results show that most of the students have positive perceptions of project-based learning approach. Students indicated that they gained confidence, enhanced their critical thinking and problem-solving abilities in computing disciplines, and enhanced their ability to communicate, engage, and work with team members, contributing to the overall quality of education."
"Scalability, Sustainability, and Ethicality of Multimodal Learning Analytics","Yan, Lixiang and Zhao, Linxuan and Gasevic, Dragan and Martinez-Maldonado, Roberto",10.1145/3506860.3506862,2022,"Multimodal Learning Analytics (MMLA) innovations are commonly aimed at supporting learners in physical learning spaces through state-of-the-art sensing technologies and analysis techniques. Although a growing body of MMLA research has demonstrated the potential benefits of sensor-based technologies in education, whether their use can be scalable, sustainable, and ethical remains questionable. Such uncertainty can limit future research and the potential adoption of MMLA by educational stakeholders in authentic learning situations. To address this, we systematically reviewed the methodological, operational, and ethical challenges faced by current MMLA works that can affect the scalability and sustainability of future MMLA innovations. A total of 96 peer-reviewed articles published after 2010 were included. The findings were summarised into three recommendations, including i) improving reporting standards by including sufficient details about sensors, analysis techniques, and the full disclosure of evaluation metrics, ii) fostering interdisciplinary collaborations among experts in learning analytics, software, and hardware engineering to develop affordable sensors and upgrade MMLA innovations that used discontinued technologies, and iii) developing ethical guidelines to address the potential risks of bias, privacy, and equality concerns with using MMLA innovations. Through these future research directions, MMLA can remain relevant and eventually have actual impacts on educational practices."
A Comparison of Learning Analytics Frameworks: a Systematic Review,"Khalil, Mohammad and Prinsloo, Paul and Slade, Sharon",10.1145/3506860.3506878,2022,"While learning analytics frameworks precede the official launch of learning analytics in 2011, there has been a proliferation of learning analytics frameworks since. This systematic review of learning analytics frameworks between 2011 and 2021 in three databases resulted in an initial corpus of 268 articles and conference proceeding papers based on the occurrence of “learning analytics” and “framework” in titles, keywords and abstracts. The final corpus of 46 frameworks were analysed using a coding scheme derived from purposefully selected learning analytics frameworks. The results found that learning analytics frameworks share a number of elements and characteristics such as source, development and application focus, a form of representation, data sources and types, focus and context. Less than half of the frameworks consider student data privacy and ethics. Finally, while design and process elements of these frameworks may be transferable and scalable to other contexts, users in different contexts will be best-placed to determine their transferability/scalability."
Design and deployment of a customer journey management system: the CJMA approach,"Nguyen Chan, Nam and Nguyen Vo, Duc Loc and Pham-Nguyen, Cuong and Le Dinh, Thang and Dam, Nguyen Anh Khoa and Pham Thi, Thanh Thoa and Vu Thi, My Hang",10.1145/3508072.3508075,2022,"Customer journey management has been recently in high demand across organizations as a means of better understanding customer behavior, predicting user needs, and enhancing customer experience to achieve their business goals. Therefore, there is an urgent need for an affordable solution assisting enterprises, especially small and medium-sized enterprises (SMEs), in automatically extracting valuable customer journey insights from their existing data sources. For this reason, this paper presents an approach, called CJMA (Customer Journey Master) approach, for designing and deploying a customer journey management system. The proposed approach incorporates several journey analysis capabilities based on three process mining methods: process discovery, trace clustering, and decision mining. The proposed system was developed on top of Python’s Django web framework with four main functions: data centralizing, process modeling for all customer paths, customer journey clustering and customer decision predicting throughout journeys. The performance of the system has been evaluated based on the three criteria: execution time, accuracy and understandability of analytical findings, which produced high outcomes using the Google Merchandise Store dataset."
Fast changeset-based bug localization with BERT,"Ciborowska, Agnieszka and Damevski, Kostadin",10.1145/3510003.3510042,2022,"Automatically localizing software bugs to the changesets that induced them has the potential to improve software developer efficiency and to positively affect software quality. To facilitate this automation, a bug report has to be effectively matched with source code changes, even when a significant lexical gap exists between natural language used to describe the bug and identifier naming practices used by developers. To bridge this gap, we need techniques that are able to capture software engineering-specific and project-specific semantics in order to detect relatedness between the two types of documents that goes beyond exact term matching. Popular transformer-based deep learning architectures, such as BERT, excel at leveraging contextual information, hence appear to be a suitable candidate for the task. However, BERT-like models are computationally expensive, which precludes them from being used in an environment where response time is important.In this paper, we describe how BERT can be made fast enough to be applicable to changeset-based bug localization. We also explore several design decisions in using BERT for this purpose, including how best to encode changesets and how to match bug reports to individual changes for improved accuracy. We compare the accuracy and performance of our model to a non-contextual baseline (i.e., vector space model) and BERT-based architectures previously used in software engineering. Our evaluation results demonstrate advantages in using the proposed BERT model compared to the baselines, especially for bug reports that lack any hints about related code elements."
Towards language-independent brown build detection,"Olewicki, Doriane and Nayrolles, Mathieu and Adams, Bram",10.1145/3510003.3510122,2022,"In principle, continuous integration (CI) practices allow modern software organizations to build and test their products after each code change to detect quality issues as soon as possible. In reality, issues with the build scripts (e.g., missing dependencies) and/or the presence of ""flaky tests"" lead to build failures that essentially are false positives, not indicative of actual quality problems of the source code. For our industrial partner, which is active in the video game industry, such ""brown builds"" not only require multidisciplinary teams to spend more effort interpreting or even re-running the build, leading to substantial redundant build activity, but also slows down the integration pipeline. Hence, this paper aims to prototype and evaluate approaches for early detection of brown build results based on textual similarity to build logs of prior brown builds. The approach is tested on 7 projects (6 closed-source from our industrial collaborators and 1 open-source, Graphviz). We find that our model manages to detect brown builds with a mean F1-score of 53% on the studied projects, which is three times more than the best baseline considered, and at least as good as human experts (but with less effort). Furthermore, we found that cross-project prediction can be used for a project's onboarding phase, that a training set of 30-weeks works best, and that our retraining heuristics keep the F1-score higher than the baseline, while retraining only every 4--5 weeks."
Generating and visualizing trace link explanations,"Liu, Yalin and Lin, Jinfeng and Anuyah, Oghenemaro and Metoyer, Ronald and Cleland-Huang, Jane",10.1145/3510003.3510129,2022,"Recent breakthroughs in deep-learning (DL) approaches have resulted in the dynamic generation of trace links that are far more accurate than was previously possible. However, DL-generated links lack clear explanations, and therefore non-experts in the domain can find it difficult to understand the underlying semantics of the link, making it hard for them to evaluate the link's correctness or suitability for a specific software engineering task. In this paper we present a novel NLP pipeline for generating and visualizing trace link explanations. Our approach identifies domain-specific concepts, retrieves a corpus of concept-related sentences, mines concept definitions and usage examples, and identifies relations between cross-artifact concepts in order to explain the links. It applies a post-processing step to prioritize the most likely acronyms and definitions and to eliminate non-relevant ones. We evaluate our approach using project artifacts from three different domains of interstellar telescopes, positive train control, and electronic healthcare systems, and then report coverage, correctness, and potential utility of the generated definitions. We design and utilize an explanation interface which leverages concept definitions and relations to visualize and explain trace link rationales, and we report results from a user study that was conducted to evaluate the effectiveness of the explanation interface. Results show that the explanations presented in the interface helped non-experts to understand the underlying semantics of a trace link and improved their ability to vet the correctness of the link."
Recommending good first issues in GitHub OSS projects,"Xiao, Wenxin and He, Hao and Xu, Weiwei and Tan, Xin and Dong, Jinhao and Zhou, Minghui",10.1145/3510003.3510196,2022,"Attracting and retaining newcomers is vital for the sustainability of an open-source software project. However, it is difficult for newcomers to locate suitable development tasks, while existing ""Good First Issues"" (GFI) in GitHub are often insufficient and inappropriate. In this paper, we propose RecGFI, an effective practical approach for the recommendation of good first issues to newcomers, which can be used to relieve maintainers' burden and help newcomers onboard. RecGFI models an issue with features from multiple dimensions (content, background, and dynamics) and uses an XGBoost classifier to generate its probability of being a GFI. To evaluate RecGFI, we collect 53,510 resolved issues among 100 GitHub projects and carefully restore their historical states to build ground truth datasets. Our evaluation shows that RecGFI can achieve up to 0.853 AUC in the ground truth dataset and outperforms alternative models. Our interpretable analysis of the trained model further reveals interesting observations about GFI characteristics. Finally, we report latest issues (without GFI-signaling labels but recommended as GFI by our approach) to project maintainers among which 16 are confirmed as real GFIs and five have been resolved by a newcomer."
What makes a good commit message?,"Tian, Yingchen and Zhang, Yuxia and Stol, Klaas-Jan and Jiang, Lin and Liu, Hui",10.1145/3510003.3510205,2022,"A key issue in collaborative software development is communication among developers. One modality of communication is a commit message, in which developers describe the changes they make in a repository. As such, commit messages serve as an ""audit trail"" by which developers can understand how the source code of a project has changed---and why. Hence, the quality of commit messages affects the effectiveness of communication among developers. Commit messages are often of poor quality as developers lack time and motivation to craft a good message. Several automatic approaches have been proposed to generate commit messages. However, these are based on uncurated datasets including considerable proportions of poorly phrased commit messages. In this multi-method study, we first define what constitutes a ""good"" commit message, and then establish what proportion of commit messages lack information using a sample of almost 1,600 messages from five highly active open source projects. We find that an average of circa 44% of messages could be improved, suggesting the use of uncurated datasets may be a major threat when commit message generators are trained with such data. We also observe that prior work has not considered semantics of commit messages, and there is surprisingly little guidance available for writing good commit messages. To that end, we develop a taxonomy based on recurring patterns in commit messages' expressions. Finally, we investigate whether ""good"" commit messages can be automatically identified; such automation could prompt developers to write better commit messages."
Applications of Big Data within Finance: Fraud Detection and Risk Management within the Real Estate Industry,"Eltweri, Ahmed and Faccia, Alessio and KHASSAWNEH, OSAMA",10.1145/3510249.3510262,2022,"Big Data within the world of finance is about large, complex, and diverse (unstructured and structured) data sets that may be employed in providing solutions across the world for business challenges within banking companies and financial services that are long-standing. Big Data helps in enhancing the significance of FinTech in offering numerous financial services for users, facilitating the distribution of new payment, financing, and exchange services within an increasingly large proportion of the population. Technological developments have changed our lives profoundly, particularly over the last two decades. All fields of the economy have been changed, so it is hardly surprising that the world of real estate has been impacted by technological advances. Indeed, great technological strides have been made within the financial world that has allowed both professionals and amateurs to employ technical, innovative solutions that may lead to improved performance both within personally used commercial activity and for the purposes of commerce. Various applications of Big Data have been very beneficial for the world of finance because of new innovations in various technologies. The focus of this research has been upon the undertaking of a systematic analysis related to the technologies that are considered most important and that currently allow great progress to be made in fraud detection and risk management within the real estate industry by analysing the data collected. The particular focus of the research has been to highlight 3 particular interest areas, namely: i) FinTech and Big Data, ii) risk management, iii) fraud detection. A recent case study related to scandals that have arisen in the FinTech industry has provided further help in support of the research hypotheses and the conclusions are drawn."
Reading to write code: an experience report of a reverse engineering and modeling course,"Ryan, Brooke and Soria, Adriana Meza and Dreef, Kaj and van der Hoek, Andr\'{e}",10.1145/3510456.3514164,2022,"A substantial portion of any software engineer's job is reading code. Despite the criticality of this skill in a budding software engineer, reading code---and more specifically, techniques on how to read code when integrating oneself into a large existing software project---is often neglected in the typical software engineering education. As part of a new professional Master of Software Engineering at the University of California, Irvine, we designed and delivered a ""reading to write code"" course from the ground up. Titled Reverse Engineering and Modeling, the course introduces students to techniques they can use to become familiar with a large code base, so they are able to make meaningful contributions. In this paper, we briefly introduce the Master program and its underlying philosophy, articulate the course's learning outcomes, present the design of the course, and provide a detailed reflection on our experiences in terms of what went well, what did not go well, what we do not know yet, and what our next steps are in preparing for the forthcoming incarnation of the course in Spring 2022. In so doing, we hope to provide a baseline together with lessons learned for others who may be interested in instituting a similar course at their institution."
"Code reviewer recommendation in tencent: practice, challenge, and direction","Chen, Qiuyuan and Kong, Dezhen and Bao, Lingfeng and Sun, Chenxing and Xia, Xin and Li, Shanping",10.1145/3510457.3513035,2022,"Code review is essential for assuring system quality in software engineering. Over decades in practice, code review has evolved to be a lightweight tool-based process focusing on code change: the smallest unit of the development cycle, and we refer to it as Modern Code Review (MCR). MCR involves code contributors committing code changes and code reviewers reviewing the assigned code changes. Such a reviewer assigning process is challenged by efficiently finding appropriate reviewers. Recent studies propose automated code reviewer recommendation (CRR) approaches to resolve such challenges. These approaches are often evaluated on open-source projects and obtain promising performance.However, the code reviewer recommendation systems are not widely used on proprietary projects, and most current reviewer selecting practice is still manual or, at best, semi-manual. No previous work systematically evaluated these approaches' effectiveness and compared each other on proprietary projects in practice. In this paper, we performed a quantitative analysis of typical recommendation approaches on proprietary projects in Tencent. The results show an imperfect performance of these approaches on proprietary projects and reveal practical challenges like the ""cold start problem"". To better understand practical challenges, we interviewed practitioners about the expectations of applying reviewer recommendations to a production environment. The interview involves the current systems' limitations, expected application scenario, and information requirements. Finally, we discuss the implications and the direction of practical code reviewer recommendation tools."
"""Project smells"": experiences in analysing the software quality of ML projects with mllint","van Oort, Bart and Cruz, Lu\'{\i}s and Loni, Babak and van Deursen, Arie",10.1145/3510457.3513041,2022,"Machine Learning (ML) projects incur novel challenges in their development and productionisation over traditional software applications, though established principles and best practices in ensuring the project's software quality still apply. While using static analysis to catch code smells has been shown to improve software quality attributes, it is only a small piece of the software quality puzzle, especially in the case of ML projects given their additional challenges and lower degree of Software Engineering (SE) experience in the data scientists that develop them. We introduce the novel concept of project smells which consider deficits in project management as a more holistic perspective on software quality in ML projects. An open-source static analysis tool mllint was also implemented to help detect and mitigate these. Our research evaluates this novel concept of project smells in the industrial context of ING, a global bank and large software- and data-intensive organisation. We also investigate the perceived importance of these project smells for proof-of-concept versus production-ready ML projects, as well as the perceived obstructions and benefits to using static analysis tools such as mllint. Our findings indicate a need for context-aware static analysis tools, that fit the needs of the project at its current stage of development, while requiring minimal configuration effort from the user."
"A cross-company ethnographic study on software teams for DevOps and microservices: organization, benefits, and issues","Zhou, Xin and Huang, Huang and Zhang, He and Huang, Xin and Shao, Dong and Zhong, Chenxin",10.1145/3510457.3513054,2022,"Context: DevOps and microservices are acknowledged to be important new paradigms to tackle contemporary software demands and provide capabilities for rapid and reliable software development. Industrial reports show that they are quickly adopted together in massive software companies. However, because of the technical and organizational requirements, many difficulties against efficient implementation of the both emerge in real software teams. Objectives: This study aims to discover the organization, benefits and issues of software teams using DevOps &amp; microservices from an immersive perspective. Method: An ethnographic study was carried out in three companies with different business, size, products, customers, and degree of globalization. All the three companies claimed their adoption of DevOps and microservices. Seven months (cumulative) of participant observations and nine interviews with practitioners were conducted to collect the data of software teams related to DevOps and microservices. A cross-company empirical investigation using grounded theory was done by analyzing the archive data. Results: The virtual software teams were organized for adopting DevOps and microservices under the stubborn organizational structure. The adoption of DevOps and microservices brings benefits to rapid delivery, ability improvements and burden reduction, whilst the high cost and lack of practical guidance were emerged. Two major issues of adopting DevOps and microservices in software teams (i.e. fragmentary DevOps and abuse of microservices) were found common in the companies. Moreover, our observations and interviews reflect that in software teams, the relationship between DevOps and microservices is not significant, which differs from the relationship described in the previous studies. Four lessons for practitioners and four implications for researchers were discussed based on our findings. Conclusion: Our findings contribute to the understanding of the organization, benefits and issues of adopting DevOps and microservices from an immersive perspective of software teams."
Bus factor in practice,"Jabrayilzade, Elgun and Evtikhiev, Mikhail and T\""{u}z\""{u}n, Eray and Kovalenko, Vladimir",10.1145/3510457.3513082,2022,"Bus factor is a metric that identifies how resilient is the project to the sudden engineer turnover. It states the minimal number of engineers that have to be hit by a bus for a project to be stalled. Even though the metric is often discussed in the community, few studies consider its general relevance. Moreover, the existing tools for bus factor estimation focus solely on the data from version control systems, even though there exists other channels for knowledge generation and distribution. With a survey of 269 engineers, we find that the bus factor is perceived as an important problem in collective development, and determine the highest impact channels of knowledge generation and distribution in software development teams. We also propose a multimodal bus factor estimation algorithm that uses data on code reviews and meetings together with the VCS data. We test the algorithm on 13 projects developed at JetBrains and compared its results to the results of the state-of-the-art tool by Avelino et al. against the ground truth collected in a survey of the engineers working on these projects. Our algorithm is slightly better in terms of both predicting the bus factor as well as key developers compared to the results of Avelino et al. Finally, we use the interviews and the surveys to derive a set of best practices to address the bus factor issue and proposals for the possible bus factor assessment tool."
Software engineers' response to public crisis: lessons learnt from spontaneously building an informative COVID-19 dashboard,"Wang, Han and Wu, Chao and Chen, Chunyang and Turhan, Burak and Chen, Shiping and Whittle, Jon",10.1145/3510458.3513010,2022,"The Coronavirus disease 2019 (COVID-19) outbreak quickly spread around the world, resulting in over 240 million infections and 4 million deaths by Oct 2021. While the virus is spreading from person to person silently, fear has also been spreading around the globe. The COVID-19 information from the Australian Government is convincing but not timely or detailed, and there is much information on social networks with both facts and rumors. As software engineers, we have spontaneously and rapidly constructed a COVID-19 information dashboard aggregating reliable information semi-automatically checked from different sources for providing one-stop information sharing site about the latest status in Australia. Inspired by the John Hopkins University COVID-19 Map, our dashboard contains the case statistics, case distribution, government policy, latest news, with interactive visualization. In this paper, we present a participant's in-person observations in which the authors acted as founders of https://covid-19-au.com/ serving more than 830K users with 14M page views since March 2020. According to our first-hand experience, we summarize 9 lessons for developers, researchers and instructors. These lessons may inspire the development, research and teaching in software engineer aspects for coping with similar public crises in the future.The 2019 Coronavirus Disease (COVID-19) outbreak has spread rapidly around the world. By October 2021, it has caused more than 240 million infections and 4 million deaths. Although the world is acting against the virus, some information on the Internet has not been updated in time, and there are also many rumors on social media. Therefore, software engineers have developed COVID-19 information dashboards such as the Johns Hopkins University COVID-19 Map and the World Health Organization COVID-19 website to provide the public with one-stop reliable COVID-19 related information. The author has also developed a COVID-19 dashboard https://covid-19-au.com/ that provides case statistics, case distribution, government policies, latest news, and interactive visualization during the pandemic in Australia. It has been popular since March 2020 and has provided 14 million page views to more than 830,000 users. In this paper, the authors discussed how they built the COVID-19 dashboard website and how they formed and managed a team of volunteers to help and maintain the project. More importantly, the authors summarized 9 lessons for developers, researchers and instructors based on experience. These courses may inspire them in development, research and teaching to deal with similar public crises in the future, and ultimately bring accurate information to public users more effectively."
Augmenting AI and Human Capabilities in Competency-Based Learning,"Mady, Ashraf and Niese, Bethany",10.1145/3510606.3550210,2023,"The nature of education and training have dramatically changed due in large part to the advancement of technology. These advancements require changes in the skillsets of the learner. Research and history have shown that continuing to hire talents to fill gaps of knowledge and ad-hoc fixes are not typically enough to keep up with rapid innovation and change. This conceptual paper provides an innovative conceptualization of how to capitalize on the value of emerging AI technologies for education and training. We propose a holistic approach to augment AI and human capabilities in a competency-based learning environment. We argue that to successfully create an AI-based augmented training system, the fit between the learning task and the AI technology as well as the human-AI augmentation must be considered. These areas are explained in the specific context of competency-based learning."
Computer Aided Medical Teaching System Based on Virtual Reality Technology,"Yu, Hang and Shen, Chao and Li, Yunna and Ruan, Jianing",10.1145/3510858.3510891,2022,"The development of computer technology has promoted the further improvement of virtual reality technology. However, the traditional medical teaching model only emphasizes theory, lacks an experimental teaching environament, and is difficult to carry out situational teaching, and its effect is greatly reduced. Therefore, this article starts from the virtual reality technology and studies the computer-assisted medical teaching system, with the purpose of improving the level of medical teaching through virtual technology, so that students can easily understand and master medical knowledge. This article mainly uses experimental methods and data analysis methods to study virtual reality technology and related algorithms, and describes the medical teaching system and computer-assisted system. The experimental results show that in the virtual environment setting, the response time of the system designed in this paper is 2.619 seconds. Compared with several other performance surveys, the energy consumption is longer, but it is also in a reasonable range."
Design of Automated Financial Information Processing System under the Background of Artificial Intelligence,"Zhu, Songgui",10.1145/3510858.3510943,2022,"The development of computer technology promotes the widespread application of artificial intelligence. It has huge advantages in information processing. As big data and cloud computing take root in the hearts of the people, enterprises have put forward higher requirements for financial management models, and there are more and more technical requirements for the automated processing of financial information. Therefore, this article aims to design an automated information processing system by studying the characteristics of artificial intelligence technology and financial information, so as to facilitate the automatic financial analysis and bring efficiency to the enterprise. This article mainly applied analysis method, comparison method and experiment method to design, analyze and test the automated financial information processing system under artificial intelligence, and obtained relevant data. The experimental results show that the accuracy of the system designed in this paper is above 96%, which is in line with the financial needs of the enterprise."
Programming Languages and Law: A Research Agenda,"Grimmelmann, James",10.1145/3511265.3550447,2022,"If code is law, then the language of law is a programming language. Lawyers and legal scholars can learn about law by studying programming-language theory, and programming-language tools can be usefully applied to legal problems. This article surveys the history of research into programming languages and law and presents ten promising avenues for future efforts. Its goals are to explain how the combination of programming languages and law is distinctive within the broader field of computer science and law, and to demonstrate with concrete examples the remarkable power of programming-language concepts in this new domain."
"Designing, Developing and Deploying an Enterprise Scale Network Monitoring System","Basu, Arkadip and Singh, Rishi and Yu, Chenyang and Prasad, Amarjeet and Banerjee, Kunal",10.1145/3511430.3511446,2022,"Walmart carries out its retail business across 27 countries both in the form of brick-and-mortar (∼ 11,500 stores and clubs) and e-commerce. To ensure smooth customer experience across the globe, we need to monitor the health of all devices ranging from networking hardware, storage spaces to compute servers spread across geographies all the time. Specifically, we need to monitor which device is facing as issue, when did this happen and what kind of alert does it call for. Swift remediation is carried out in a pro-active manner, i.e., before a device fails, and sometimes in re-active manner, i.e., after a device has failed. Tackling this challenge at an enterprise scale requires various technologies working together in a seamless manner. In this work, we give an insight about how the problem of network monitoring is handled at Walmart and elaborate on the design decisions taken."
Role of WSDL Metrics in the Detection of Web Service Anti-Patterns,"Tummalapalli, Sahithi",10.1145/3511430.3511459,2022,"Many IT businesses now employ service-oriented architecture (SOA) to develop their systems. A service-based system (SBS) can be updated to accommodate new user needs, just like many other complicated structures. Continuously improving service-based systems to fulfill customer requests will lower software development quality, resulting in Anti-patterns in web services. Anti-patterns are frequently picked as a viable solution to a problem. However, they come with more drawbacks than advantages. Anti-patterns sabotage software systems’ long-term viability and perception."
Commit-Checker: A human-centric approach for adopting bug inducing commit detection using machine learning models,"Oishie, Naz Zarreen Zarreen and Roy, Banani",10.1145/3511430.3511463,2022,
Why Embedding Indigenous Cultural Awareness in ICT Curriculum is an Imperative,"Herbert, Nicole and Springer, Matthew and Pratik, Pratik and Lin, Zhixi",10.1145/3511861.3511882,2022,"Aboriginal and Torres Strait Islander disadvantage is widely acknowledged, and Information and Communication Technology (ICT) is increasingly recognised as a significant contributor to Aboriginal and Torres Strait Islander disadvantage. In 2017, all Australian universities committed to ensuring all students engage with Aboriginal and Torres Strait Islander cultural content as integral parts of their curriculum. ICT curricula across Australia have mostly not engaged in the processes of extensive curriculum indigenisation which focus on embedding cultural content throughout a course. This study seeks to uncover the issues that arise for Indigenous peoples from the prevalence of ICT, by exploring relevant academic research papers and government reports. This knowledge will be used to identify why cultural content should be embedded within ICT curricula. The study concludes that it is imperative for graduating ICT professionals to have an awareness of Aboriginal and Torres Strait Islander knowledges and perspectives to reduce the impact of Aboriginal and Torres Strait Islander disadvantage caused by ICT. The findings are a major contribution to the growing area of research on why and how to embed Aboriginal and Torres Strait Islander cultural knowledges and perspectives within an ICT curriculum."
Role and its Impacts of Computer Application in Management and Business,"Baa, Roshan",10.1145/3512676.3512684,2022,"Computer Applications are specially designed programmes that help users to perform particular tasks efficiently. The use of computer applications is crucial for businesses to cope up with the fast-moving global business world. On one hand, it has increased speed and accuracy, on the other hand, it has created problems such as unemployment, skill shortage and increasing operational expenses. Unemployment and skill shortage are serious problems in India where a large volume of people are poor and not literate even today. Such an issue is also increasing the hiring and retention cost for business firms. This research is exploratory research that tries to identify and define the impacts of computer applications on management and business. This initial study will open the door to the way various other research works."
A Penalty Default Approach to Preemptive Harm Disclosure and Mitigation for AI Systems,"Yew, Rui-Jie and Hadfield-Menell, Dylan",10.1145/3514094.3534130,2022,"As AI industry matures, it is important to ensure that the organizations developing these systems have sufficient incentives to identify and mitigate risks and harm. Unfortunately, the profit motive is often misaligned with this goal. Successful work to identify or reduce risk rarely has direct tangible benefits. In this paper, we consider the use of regulatory penalty defaults as a way to counter these perverse incentives. A regulatory penalty default regime consists of two parts: a regulatory penalty default and a mechanism to bargain around the default. The regulatory penalty default induces private actors to research and mitigate potential harms in order to limit liability, making the benefits of risk mitigation tangible. The bargaining mechanism provides incentives for companies to go beyond achieving a prescriptive threshold of compliance in creating a compelling case for escape from the default. With a focus on the policy landscape in the United States, we propose and discuss potential regulatory penalty default regimes for AI systems. For each of our proposals, we also discuss accompanying regulatory pathways for the bargaining process. While regulatory penalty default regimes are not a panacea (we discuss several drawbacks of the proposed methods), they are an important tool to consider in the regulation of AI systems."
How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Review,"Bertrand, Astrid and Belloum, Rafik and Eagan, James R. and Maxwell, Winston",10.1145/3514094.3534164,2022,"The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to complex AI systems. Although it is usually considered an essentially technical field, effort has been made recently to better understand users' human explanation methods and cognitive constraints. Despite these advances, the community lacks a general vision of what and how cognitive biases affect explainability systems. To address this gap, we present a heuristic map which matches human cognitive biases with explainability techniques from the XAI literature, structured around XAI-aided decision-making. We identify four main ways cognitive biases affect or are affected by XAI systems: 1) cognitive biases affect how XAI methods are designed, 2) they can distort how XAI techniques are evaluated in user studies, 3) some cognitive biases can be successfully mitigated by XAI techniques, and, on the contrary, 4) some cognitive biases can be exacerbated by XAI techniques. We construct this heuristic map through the systematic review of 37 papers-drawn from a corpus of 285-that reveal cognitive biases in XAI systems, including the explainability method and the user and task types in which they arise. We use the findings from our review to structure directions for future XAI systems to better align with people's cognitive processes."
State of the Art in AAC: A Systematic Review and Taxonomy,"Curtis, Humphrey and Neate, Timothy and Vazquez Gonzalez, Carlota",10.1145/3517428.3544810,2022,"People with complex communication needs (CCNs) can use high-tech augmentative and alternative communication (AAC) devices and systems to compensate for communication difficulties. While many use AAC effectively, much research has highlighted challenges – for instance, high rates of abandonment and solutions which are not appropriate for their end-users. Presently, we lack a detailed survey of this field to comprehend these shortcomings and understand how the accessibility community might direct its efforts to design more effective AAC. In response to this, we conduct a systematic review and taxonomy of high-tech AAC devices and interventions, reporting results from 562 articles identified in the ACM DL and SCOPUS databases. We provide a taxonomical overview of the current state of AAC devices – e.g. their interaction modalities and characteristics. We describe the communities of focus explored, and the methodological approaches used. We contrast findings in the broader accessibility and HCI literature to delineate future avenues for exploration in light of the current taxonomy, offer a reassessment of the norms and incumbent research methodologies and present a discourse on the communities of focus for AAC and interventions."
Measurement and analysis of implied identity in ad delivery optimization,"Kaplan, Levi and Gerzon, Nicole and Mislove, Alan and Sapiezynski, Piotr",10.1145/3517745.3561450,2022,"Online services such as Facebook and Google serve as a popular way by which users today are exposed to products, services, viewpoints, and opportunities. These services implement advertising platforms that enable precise targeting of platform users, and they optimize the delivery of ads to the subset of the targeted users predicted to be most receptive. Unfortunately, recent work has shown that such delivery can---often without the advertisers' knowledge---show ads to biased sets of users based only on the content of the ad. Such concerns are particularly acute for ads that contain pictures of people (e.g., job ads showing workers), as advertisers often select images to carefully convey their goals and values (e.g., to promote diversity in hiring). However, it remains unknown how ad delivery algorithms react to---and make delivery decisions based on---demographic features of people represented in such ad images. Here, we examine how one major advertising platform (Facebook) delivers ads that include pictures of people of varying ages, genders, and races. We develop techniques to isolate the effect of these demographic variables, using a combination of both stock photos and realistic synthetically-generated images of people. We find dramatic skews in who ultimately sees ads solely based on the demographics of the person in the ad. Ads are often delivered disproportionately to users similar to those pictured: images of Black people are shown more to Black users, and the age of the person pictured correlates positively with the age of the users to whom it is shown. But, this is not universal, and more complex effects emerge: older women see more images of children, while images of younger women are shown disproportionately to men aged 55 and older. These findings bring up novel technical, legal, and policy questions and underscore the need to better understand how platforms deliver ads today."
What is software quality for AI engineers? towards a thinning of the fog,"Golendukhina, Valentina and Lenarduzzi, Valentina and Felderer, Michael",10.1145/3522664.3528599,2022,"It is often overseen that AI-enabled systems are also software systems and therefore rely on software quality assurance (SQA). Thus, the goal of this study is to investigate the software quality assurance strategies adopted during the development, integration, and maintenance of AI/ML components and code. We conducted semi-structured interviews with representatives of ten Austrian SMEs that develop AI-enabled systems. A qualitative analysis of the interview data identified 12 issues in the development of AI/ML components. Furthermore, we identified when quality issues arise in AI/ML components and how they are detected. The results of this study should guide future work on software quality assurance processes and techniques for AI/ML components."
Data is about detail: an empirical investigation for software systems with NLP at core,"Singhal, Anmol and Anish, Preethu Rose and Sonar, Pratik and Ghaisas, Smita S",10.1145/3522664.3528604,2022,"Businesses continue to operate under increasingly complex demands such as ever-evolving regulatory landscape, personalization requirements from software apps, and stricter governance with respect to security and privacy. In response to these challenges, large enterprises have been emphasizing automation across a wide range, starting with business processes all the way to customer experience. As AI continues to be a core component of software systems being developed, data assumes a predominant role. AI-centric software systems of industrial scale need large amounts of training data, that in our experience, has introduced several challenges. In this paper, through an empirical study based on interviews with AI practitioners, we present current challenges that need to be addressed in 'data requirements' of Software Systems with NLP at the Core (SSNLPCore). We further discuss the impact of the challenges and techniques currently employed by practitioners for addressing them. Our findings reveal that a focus on details pertaining to data is required early into the project lifecycle, which include aspects such as how we may select, process, and annotate data. This can ensure that the AI component is effective in meeting business goals of software systems."
Preliminary insights to enable automation of the software development process in software StartUps: an investigation study from the use of artificial intelligence and machine learning,"Borges, Olimar and Lenarduzzi, Valentina and Prikladnicki, Rafael",10.1145/3522664.3528610,2022,"Artificial Intelligence (AI) and Machine Learning (ML) tools and techniques have increasingly effectively supported Software Engineering (SE) tasks, whether for requirements classification, software refactoring, defect prediction, and many others. In the context of software StartUps, where innovative and scalable software products are developed, dealing with the pressure of fast delivery of a working solution becomes a challenging factor. We aim to investigate AI and ML techniques used by SE practitioners and entrepreneurs to support their Software Development Processes (SDP) and thus enable their use by software StartUps. We seek to identify this information through the application of an online Survey instrument, mainly disseminated in Brazil and Finland. This preliminary study provides insights that can support improving the SDP in StartUps."
Identifying the scope of the implications of a Digital Transformation: A formal approach to define the business dimensions involved,"Liborio Zapata, Melissa and Berrah, Lamia and Tabourot, Laurent",10.1145/3524338.3524348,2022,"Companies around the world are finding themselves in a race against the relentless evolution of digital technologies that are accelerating innovation and creating a highly competitive environment. Many works in research and practice are trying to guide companies to a Digital Transformation (DT) that allows them to take full advantage of new technologies. However, generic solutions, that are mainly focused on technological aspects, make it clear that there is a lack of understanding of the whole scope of their implications. In this sense, this work takes a step back to analyze the origin of the shortcomings of current solution approaches. The results point out a lack of theoretical foundation on identifying the business dimensions implicated in a DT that define its scope. In this sense, this study contributes with a more comprehensive view of the DT process by using a formal approach to define the business dimensions involved in such a change based on the principles of the Socio-Technical Systems (STS) theory. As a result, this proposal goes beyond the purely technological views to (1) identify five business dimensions involved in the DT process: technology, processes, structure, competencies and culture and (2) recognize the key role of strategy and performance measurement, not as dimensions but as external elements that drive and control the DT process. A multiple case study of the DT process of three French manufacturers is presented to validate the proposition. General remarks and future research concerning the implementation of these dimensions conclude this study."
Video game project management anti-patterns,"Ullmann, Gabriel C. and Politowski, Cristiano and Gu\'{e}h\'{e}neuc, Yann-Ga\""{e}l and Petrillo, Fabio and Montandon, Jo\~{a}o Eduardo",10.1145/3524494.3527623,2022,"Project Management anti-patterns are well-documented in the software-engineering literature, and studying them allows understanding their impacts on teams and projects. The video game development industry is known for its mismanagement practices, and therefore applying this knowledge would help improve game developers' productivity and well-being. In this paper, we map project management anti-patterns to anti-patterns reported by game developers in the gray literature. We read 440 postmortem problems, identified anti-pattern candidates, and related them with definitions from the software-engineering literature. We discovered that most anti-pattern candidates could be mapped to anti-patterns in the software-engineering literature, except for Feature Creep, Feature Cuts, Working on Multiple Projects, and Absent or Inadequate Tools. We discussed the impact of the unmapped candidates on the development process while also drawing a parallel between video games and traditional software development. Future works include validating the definitions of the candidates via survey with practitioners and also considering development anti-patterns."
What makes a game high-rated? towards factors of video game success,"Ullmann, Gabriel C. and Politowski, Cristiano and Gu\'{e}h\'{e}neuc, Yann-Ga\""{e}l and Petrillo, Fabio",10.1145/3524494.3527628,2022,"As the video game market grows larger, it becomes harder for games to stand out from the crowd. Launching a successful game encompasses different factors, some of which are not well-known. In this paper, we investigate some factors that affect game scores, considering high-rated video games from a dataset of 200 projects. Results show that smaller team sizes are often linked to higher scores. On the other hand, the level of freedom given to developers, as well as genre, graphical perspective, game modes and platforms do not correlate to score. Additionally, teams from successful games also experience more crunch time while fewer problems with schedule and budget allocation. Further analysis shows that team, technical, and game design factors should be the main focus of the game developers."
CodePanorama: a language agnostic tool for visual code inspection,"Etter, Marc and Mehta, Farhad",10.1145/3524610.3527874,2022,"Software projects change hands frequently. Oftentimes, developers are interested in the quality of the code before taking over responsibility on a project. This quality is commonly assessed using various code metrics, reducing the code into a handful of numbers. While useful, these numerical reductions quickly become detached from the real code. CodePanorama uses an alternative approach to summarize code not into numbers, but into images. By generating zoomed-out images of the code-base, the human eye can quickly spot anomalies without the need to rely on numerical metrics and statistics. This paper describes the tool CodePanorama, the images it generates, and the insights that can be gained from these images. We finally invite the software engineering community to start using it."
Demystifying software release note issues on GitHub,"Wu, Jianyu and He, Hao and Xiao, Wenxin and Gao, Kai and Zhou, Minghui",10.1145/3524610.3527919,2022,"Release notes (RNs) summarize main changes between two consecutive software versions and serve as a central source of information when users upgrade software. While producing high quality RNs can be hard and poses a variety of challenges to developers, a comprehensive empirical understanding of these challenges is still lacking. In this paper, we bridge this knowledge gap by manually analyzing 1,731 latest GitHub issues to build a comprehensive taxonomy of RN issues with four dimensions: Content, Presentation, Accessibility, and Production. Among these issues, nearly half (48.47%) of them focus on Production; Content, Accessibility, and Presentation take 25.61%, 17.65%, and 8.27%, respectively. We find that: 1) RN producers are more likely to miss information than to include incorrect information, especially for breaking changes; 2) improper layout may bury important information and confuse users; 3) many users find RNs inaccessible due to link deterioration, lack of notification, and obfuscate RN locations; 4) automating and regulating RN production remains challenging despite the great needs of RN producers. Our taxonomy not only pictures a roadmap to improve RN production in practice but also reveals interesting future research directions for automating RN production."
"Backports: change types, challenges and strategies","Chakroborti, Debasish and Schneider, Kevin A. and Roy, Chanchal K.",10.1145/3524610.3527920,2022,"Source code repositories allow developers to manage multiple versions (or branches) of a software system. Pull-requests are used to modify a branch, and backporting is a regular activity used to port changes from a current development branch to other versions. In open-source software, backports are common and often need to be adapted by hand, which motivates us to explore backports and backporting challenges and strategies. In our exploration of 68,424 backports from 10 GitHub projects, we found that bug, test, document, and feature changes are commonly backported. We identified a number of backporting challenges, including that backports were inconsistently linked to their original pull-request (49%), that backports had incompatible code (13%), that backports failed to be accepted (10%), and that there were backporting delays (16 days to create, 5 days to merge). We identified some general strategies for addressing backporting issues. We also noted that backporting strategies depend on the project type and that further investigation is needed to determine their suitability. Furthermore, we created the first-ever backports dataset that can be used by other researchers and practitioners for investigating backports and backporting."
A first look at duplicate and near-duplicate self-admitted technical debt comments,"Yasmin, Jerin and Sheikhaei, Mohammad Sadegh and Tian, Yuan",10.1145/3524610.3528387,2022,"Self-admitted technical debt (SATD) refers to technical debt that is intentionally introduced by developers and explicitly documented in code comments or other software artifacts (e.g., issue reports) to annotate sub-optimal decisions made by developers in the software development process.In this work, we take the first look at the existence and characteristics of duplicate and near-duplicate SATD comments in five popular Apache OSS projects, i.e., JSPWiki, Helix, Jackrabbit, Archiva, and SystemML. We design a method to automatically identify groups of duplicate and near-duplicate SATD comments and track their evolution in the software system by mining the commit history of a software project. Leveraging the proposed method, we identified 3,520 duplicate and near-duplicate SATD comments from the target projects, which belong to 1,141 groups. We manually analyze the content and context of a sample of 1,505 SATD comments (by sampling 100 groups for each project) and identify if they annotate the same root cause. We also investigate whether duplicate SATD comments exist in code clones, whether they co-exist in the same file, and whether they are introduced and removed simultaneously. Our preliminary study reveals several surprising findings that would shed light on future studies aiming to improve the management of duplicate SATD comments. For instance, only 48.5% duplicate SATD comment groups with the same root cause exist in regular code clones, and only 33.9% of the duplicate SATD comment pairs are introduced in the same commit."
MSCCD: grammar pluggable clone detection based on ANTLR parser generation,"Zhu, Wenqing and Yoshida, Norihiro and Kamiya, Toshihiro and Choi, Eunjong and Takada, Hiroaki",10.1145/3524610.3529161,2022,"For various reasons, programming languages continue to multiply and evolve. It has become necessary to have a multilingual clone detection tool that can easily expand supported programming languages and detect various code clones is needed. However, research on multilingual code clone detection has not received sufficient attention. In this study, we propose MSCCD (Multilingual Syntactic Code Clone Detector), a grammar pluggable code clone detection tool that uses a parser generator to generate a code block extractor for the target language. The extractor then extracts the semantic code blocks from a parse tree. MSCCD can detect Type-3 clones at various granularities. We evaluated MSCCD's language extensibility by applying MSCCD to 20 modern languages. Sixteen languages were perfectly supported, and the remaining four were provided with the same detection capabilities at the expense of execution time. We evaluated MSCCD's recall by using BigCloneEval and conducted a manual experiment to evaluate precision. MSCCD achieved equivalent detection performance equivalent to state-of-the-art tools."
BotHunter: an approach to detect software bots in GitHub,"Abdellatif, Ahmad and Wessel, Mairieli and Steinmacher, Igor and Gerosa, Marco A. and Shihab, Emad",10.1145/3524842.3527959,2022,"Bots have become popular in software projects as they play critical roles, from running tests to fixing bugs/vulnerabilities. However, the large number of software bots adds extra effort to practitioners and researchers to distinguish human accounts from bot accounts to avoid bias in data-driven studies. Researchers developed several approaches to identify bots at specific activity levels (issue/pull request or commit), considering a single repository and disregarding features that showed to be effective in other domains. To address this gap, we propose using a machine learning-based approach to identify the bot accounts regardless of their activity level. We selected and extracted 19 features related to the account's profile information, activities, and comment similarity. Then, we evaluated the performance of five machine learning classifiers using a dataset that has more than 5,000 GitHub accounts. Our results show that the Random Forest classifier performs the best, with an F1-score of 92.4% and AUC of 98.7%. Furthermore, the account profile information (e.g., account login) contains the most relevant features to identify the account type. Finally, we compare the performance of our Random Forest classifier to the state-of-the-art approaches, and our results show that our model outperforms the state-of-the-art techniques in identifying the account type regardless of their activity level."
A versatile dataset of agile open source software projects,"Tawosi, Vali and Al-Subaihin, Afnan and Moussa, Rebecca and Sarro, Federica",10.1145/3524842.3528029,2022,"Agile software development is nowadays a widely adopted practise in both open-source and industrial software projects. Agile teams typically heavily rely on issue management tools to document new issues and keep track of outstanding ones, in addition to storing their technical details, effort estimates, assignment to developers, and more. Previous work utilised the historical information stored in issue management systems for various purposes; however, when researchers make their empirical data public, it is usually relevant solely to the study's objective. In this paper, we present a more holistic and versatile dataset containing a wealth of information on more than half a million issues from 44 open-source Agile software, making it well-suited to several research avenues, and cross-analyses therein, including effort estimation, issue prioritization, issue assignment and many more. We make this data publicly available on GitHub to facilitate ease of use, maintenance, and extensibility."
Empirical standards for repository mining,"Chatterjee, Preetha and Sharma, Tushar and Ralph, Paul",10.1145/3524842.3528032,2022,"The purpose of scholarly peer review is to evaluate the quality of scientific manuscripts. However, study after study demonstrates that peer review neither effectively nor reliably assesses research quality. Empirical standards attempt to address this problem by modelling a scientific community's expectations for each kind of empirical study conducted in that community. This should enhance not only the quality of research but also the reliability and predictability of peer review, as scientists adopt the standards in both their researcher and reviewer roles. However, these improvements depend on the quality and adoption of the standards. This tutorial will therefore present the empirical standard for mining software repositories, both to communicate its contents and to get feedback from the attendees. The tutorial will be organized into three parts: (1) brief overview of the empirical standards project; (2) detailed presentation of the repository mining standard; (3) discussion and suggestions for improvement."
Towards reliable agile iterative planning via predicting documentation changes of work items,"Pasuksmit, Jirat and Thongtanunam, Patanamon and Karunasekera, Shanika",10.1145/3524842.3528445,2022,"In agile iterative development, an agile team needs to analyze documented information for effort estimation and sprint planning. While documentation can be changed, the documentation changes after sprint planning may invalidate the estimated effort and sprint plan. Hence, to help the team be aware of the potential documentation changes, we developed DocWarn to estimate the probability that a work item will have documentation changes. We developed three variations of DocWarn, which are based on the characteristics extracted from the work items (DocWarn-C), the natural language text (DocWarn-T), and both inputs (DocWarn-H).Based on nine open-source projects that work in sprints and actively maintain documentation, DocWarn can predict the documentation changes with an average AUC of 0.75 and an average F1-Score of 0.36, which are significantly higher than the baselines. We also found that the most influential characteristics of a work item for determining the future documentation changes are the past tendency of the developers and the length of description text. Based on the qualitative assessment, we found that 40%-68% of the correctly predicted documentation changes were related to scope modification. With the prediction of DocWarn, the team will be better aware of the potential documentation changes during sprint planning, allowing the team to manage the uncertainty and reduce the risk of unreliable effort estimation and sprint planning."
Beyond duplicates: towards understanding and predicting link types in issue tracking systems,"L\""{u}ders, Clara Marie and Bouraffa, Abir and Maalej, Walid",10.1145/3524842.3528457,2022,"Software projects use Issue Tracking Systems (ITS) like JIRA to track issues and organize the workflows around them. Issues are often inter-connected via different links such as the default JIRA link types Duplicate, Relate, Block, or Subtask. While previous research has mostly focused on analyzing and predicting duplication links, this work aims at understanding the various other link types, their prevalence, and characteristics towards a more reliable link type prediction. For this, we studied 607,208 links connecting 698,790 issues in 15 public JIRA repositories. Besides the default types, the custom types Depend, Incorporate, Split, and Cause were also common. We manually grouped all 75 link types used in the repositories into five general categories: General Relation, Duplication, Composition, Temporal / Causal, and Workflow. Comparing the structures of the corresponding graphs, we observed several trends. For instance, Duplication links tend to represent simpler issue graphs often with two components and Composition links present the highest amount of hierarchical tree structures (97.7%). Surprisingly, General Relation links have a significantly higher transitivity score than Duplication and Temporal / Causal links.Motivated by the differences between the link types and by their popularity, we evaluated the robustness of two state-of-the-art duplicate detection approaches from the literature on the JIRA dataset. We found that current deep-learning approaches confuse between Duplication and other links in almost all repositories. On average, the classification accuracy dropped by 6% for one approach and 12% for the other. Extending the training sets with other link types seems to partly solve this issue. We discuss our findings and their implications for research and practice."
Inspect4py: a knowledge extraction framework for python code repositories,"Filgueira, Rosa and Garijo, Daniel",10.1145/3524842.3528497,2022,"This work presents inspect4py, a static code analysis framework designed to automatically extract the main features, metadata and documentation of Python code repositories. Given an input folder with code, inspect4py uses abstract syntax trees and state of the art tools to find all functions, classes, tests, documentation, call graphs, module dependencies and control flows within all code files in that repository. Using these findings, inspect4py infers different ways of invoking a software component. We have evaluated our framework on 95 annotated repositories, obtaining promising results for software type classification (over 95% F1-score). With inspect4py, we aim to ease the understandability and adoption of software repositories by other researchers and developers.Code: https://github.com/SoftwareUnderstanding/inspect4pyDOI: https://doi.org/10.5281/zenodo.5907936License: Open (BSD3-Clause)"
Technical debt prioritization: a developer's perspective,"Pina, Diogo and Seaman, Carolyn and Goldman, Alfredo",10.1145/3524843.3528096,2022,"Background: The prioritization of technical debt is an essential task in managing software projects because, with current analysis tools, it is possible to find thousands of technical debt items in the software that would take months or even years to be fully paid. Aims: In this study, we aim to understand which criteria software developers use to prioritize code technical debt in real software projects. Methods: We performed a survey to collect data from open-source software projects in order to reach a large and diverse set of experiences. We analyzed the data using Straussian Grounded Theory techniques: open coding, axial coding, and selective coding. Results: We grouped the criteria into 15 categories and divided them into 2 super-categories related to paying off the technical debt and 3 related to not paying it. Conclusions: When participants decided to pay off technical debt, they wanted to do it soon. However, when they decided not to pay it, it is often because the debt occurred intentionally due to a project decision. Also, participants using similar criteria for their decisions tended to choose similar priority levels for those decisions. Finally, we observed that each software project needs to tailor the rules used to identify code technical debt to their project context."
Comprehending the use of intelligent techniques to support technical debt management,"Albuquerque, Danyllo and Guimaraes, Everton and Tonin, Graziela and Perkusich, Mirko and Almeida, Hyggo and Perkusich, Angelo",10.1145/3524843.3528097,2022,"Technical Debt (TD) refers to the consequences of taking shortcuts when developing software. Technical Debt Management (TDM) becomes complex since it relies on a decision process based on multiple and heterogeneous data, which are not straightforward to be synthesized. In this context, there is a promising opportunity to use Intelligent Techniques to support TDM activities since these techniques explore data for knowledge discovery, reasoning, learning, or supporting decision-making. Although these techniques can be used for improving TDM activities, there is no empirical study exploring this research area. This study aims to identify and analyze solutions based on Intelligent Techniques employed to support TDM activities. A Systematic Mapping Study was performed, covering publications between 2010 and 2020. From 2276 extracted studies, we selected 111 unique studies. We found a positive trend in applying Intelligent Techniques to support TDM activities, being Machine Learning, Reasoning Under Uncertainty, and Natural Language Processing the most recurrent ones. Identification, measurement, and monitoring were the more recurrent TDM activities, whereas Design, Code, and Architectural were the most frequently investigated TD types. Although the research area is up-and-coming, it is still in its infancy, and this study provides a baseline for future research."
Automated generation of metamorphic relations for query-based systems,"Segura, Sergio and Alonso, Juan C. and Martin-Lopez, Alberto and Dur\'{a}n, Amador and Troya, Javier and Ruiz-Cort\'{e}s, Antonio",10.1145/3524846.3527338,2023,"Searching and displaying data based on user queries is a pervasive feature of most software applications such as information systems, web portals, and web APIs. The large volume of data managed by these types of systems, henceforth called query-based systems (QBSs), makes them extremely hard to test due to the difficulty to assess whether the output of a query is correct, the so-called test oracle problem. Metamorphic testing has proved to be a very effective approach to alleviate the oracle problem in QBSs, by exploiting the relations among multiple executions of the QBS under test, so-called metamorphic relations (MRs). However, the identification of MRs mostly remains a manual and creative task, limiting the applicability of the approach. In this paper, we propose a method for the automated generation of MRs in QBSs starting from a lightweight specification of the query parameters of the system. Evaluation results show that hundreds of MRs can be automatically identified in real-world systems like IMDb, SkyScanner, or YouTube in just a few seconds."
"Embedded Systems Education in the 2020s: Challenges, Reflections, and Future Directions","Pasricha, Sudeep",10.1145/3526241.3530348,2022,"Embedded computing systems are pervasive in our everyday lives, imparting digital intelligence to a variety of electronic platforms used in our vehicles, smart appliances, wearables, mobile devices, and computers. The need to train the next generation of embedded systems designers and engineers with relevant skills across hardware, software, and their co-design remains pressing today. This paper describes the evolution of embedded systems education over the past two decades and challenges facing the designers and instructors of embedded systems curricula in the 2020s. Reflections from over a decade of teaching the design of embedded computing systems are presented, with insights on strategies that show promise to address these challenges. Lastly, some important future directions in embedded systems education are highlighted."
Development of directions and an information system for identifying the risks of implementing infrastructure projects based on public-private partnerships,"Samokhvalov, Ivan and Babkin, Ivan",10.1145/3527049.3527079,2022,"The paper analyzes the current state of development of high-speed rails (HSR) in Russia, international experience in implementing large infrastructure projects with state participation (projects to develop HSR and/or projects similar in scale), in order to identify risks and consider the need for using digital technologies for risk management. The basic provisions of public-private partnerships (PPP), the concept of risk sharing between the participants, taking into consideration social and economic effects. The main task is to analyze ways of identifying potential risks at an early stage of the project as a basis for building a risk management system consistent with generally accepted international and national practices, which in turn will provide high-quality risk management at all subsequent stages of project implementation, as well as scaling the effects on future PPP projects. The need to study this issue is determined by the high public interest in projects with state participation, especially in view of the consequences of the COVID-19 epidemic, in which the state order plays an important role in economic recovery. Any realized risks of PPP projects cause not only economic damage, but also undermine public confidence in the government, so the search for new or improved tools for risk management is the key task of any participant of PPP projects. Attention is focused on the simultaneous diversity and recurrence of risks arising during the implementation of projects, which indicates the need for centralized consideration of most risks in the proper organization of management processes. As a result of the study, it is proposed to consider the state as a single customer of infrastructure projects, regardless of their affiliation to different independent initiators, and to develop an information system for risk management in accordance with international best practices. This approach will provide PPP infrastructure projects with unified risk management tools and create a natural environment for rapid exchange of information and experience for more effective management of subsequent projects."
Forming a Business Model for Digital Interfirm Interaction in a Cluster,"Leonova, Oksana and Pavlova, Anna and Pronyaeva, Lyudmila and Gorovoy, Alexandr and Kuznetsov, Maxim",10.1145/3527049.3527141,2022,"A cluster is an integrated network-type system consisting of legally independent participants interested in business cooperation. Achieving the goals and effectiveness of the cluster's functioning are directly dependent on creating an optimal environment for interaction between its members, which is ensured by the development of information systems adapted to the management needs of the cluster. The aim of the study is to develop a business model of digital interfirm interaction in a cluster, taking into consideration its complex organizational structure, features of functioning, contributing to improving the effectiveness of cooperation and streamlining management communications. The developed business model of the inter firm interaction in a cluster allows forming a virtual communication environment of its members using modern digital tools, including elements open for the interaction with the external environment, elements of the closed internal environment (implementation of the cluster program and projects, reporting, activity monitoring, financing, etc.). This ensures continuity and consistency of communication at various stages of cluster development and increases management effectiveness. The implementation of the proposed business model is based on the application of a new combined type of digital interaction within the cluster, satisfying the information needs of all elements of the cluster management system through the application of the recommended digital tools for each dedicated block of the interaction with the characteristics of the expected results."
How Researchers Manage Ideas,"Inie, Nanna and Frich, Jonas and Dalsgaard, Peter",10.1145/3527927.3532813,2022,"Research ideas are pivotal in research practice. While research domains, topics, and methods are often outlined by specific research fields, the process of capturing and developing research ideas is less categorical. Conceiving and developing research ideas requires continuous creative thinking, usually supported by various different tools, each more or less carefully selected by a researcher to fulfill a specific purpose. In this paper, we investigate the creative work practices of academic researchers, with a focus on the workflows and tools they employ to manage ideas. Through a qualitative survey (n=51) and in-depth interviews (n=18) with researchers from a wide range of fields, we identify and describe typical processes of managing research ideas, different types of research ideas (a research question or problem, a method, a hypothesis or antithesis, and a theory), properties of good research ideas, as well as potentials for tools and technology to support idea management for researchers."
A framework for class activities to cultivate responsible leadership in software engineering students,"Goyal, Devender and Cortinovis, Renato and Capretz, Luiz Fernando",10.1145/3528579.3529167,2022,"Software and information technologies are becoming increasingly integrated and pervasive in human society, and range from automated decision making to running critical infrastructure like utilities and financial institutions. There is also a growing awareness of the need to develop leaders who will harness these technologies in fair and inclusive ways. Many academic and industry researchers are advocating for the responsible use of information technologies and some academic and research institutions such as IEEE and ACM have published codes of ethics to spread awareness about these issues. In this regard, a number of academic researchers, including the authors of this paper, have expressed the need to teach students computer and information ethics as well as professional and leadership skills. In this paper, we propose an approach that is potentially effective in helping students develop leadership and communication skills as well as learn broader skills of professional responsibility. The proposed approach is modeled after Toastmasters, a very successful association present in over 140 countries with almost 350,000 members across more than 16,000 clubs. We describe our goal and give a general description of a Toastmasters club and how it is conducted. Further, we describe some activities and projects having CS/SE context that can be done by students as part of a relevant class. Finally, we briefly describe the approach that we are undertaking in our first pilot activities and their integration with additional synergetic strategies."
Are Active and Assisted Living applications addressing the main acceptance concerns of their beneficiaries? Preliminary insights from a scoping review,"Colantonio, Sara and Jovanovic, Mladjan and Zdravevski, Eftim and Lameski, Petre and Tellioglu, Hilda and Kampel, Martin and Florez-Revuelta, Francisco",10.1145/3529190.3534753,2022,"Active and Assisted Living (AAL) technologies stand as a promising mean to respond to the big societal challenges related to health and social care. Nevertheless, despite their great potential and the recent boost ensured by the advances in Artificial Intelligence for data processing, the uptake in real-life settings of AAL technologies is still in its infancy. Several concerns seem to hinder the willingness of the targeted beneficiaries to integrate such technologies in their routines and living settings. Some studies and surveys have tried so far to identify and analyze these concerns and the factors that affect the immediate acceptance and long-term usage of AAL technologies, thus identifying accessibility, usability, privacy, safety, security and reliability as the core ones. Nevertheless, no attempts have been done yet to verify the reception of these analyses from a technological and implementation standpoint. This paper fills this gap by reporting the preliminary results of a scoping review of the AAL literature. The review investigates the solutions developed in the last five years that address various groups of beneficiaries and their concerns with respect to technology adoption. The results obtained aim to aid researchers, social and health care professionals, end users and technology providers understand the state of play of technological solutions, evaluation studies and the overall discussions that are appearing in the literature to address and respond to the end-users’ concerns."
How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Study,"Chazette, Larissa and Kl\""{u}nder, Jil and Balci, Merve and Schneider, Kurt",10.1145/3529320.3529321,2022,"Quality aspects such as ethics, fairness, and transparency have been proven to be essential for trustworthy software systems. Explainability has been identified not only as a means to achieve all these three aspects in systems, but also as a way to foster users’ sentiments of trust. Despite this, research has only marginally focused on the activities and practices to develop explainable systems. To close this gap, we recommend six core activities and associated practices for the development of explainable systems based on the results of a literature review and an interview study. First, we identified and summarized activities and corresponding practices in the literature. To complement these findings, we conducted interviews with 19 industry professionals who provided recommendations for the development process of explainable systems and reviewed the activities and practices based on their expertise and knowledge. We compared and combined the findings of the interviews and the literature review to recommend the activities and assess their applicability in industry. Our findings demonstrate that the activities and practices are not only feasible, but can also be integrated in different development processes."
Hacking or Engineering? Towards an Extended Entrepreneurial Software Engineering Model,"Kuhrmann, Marco and Muench, Juergen and Klunder, Jil",10.1145/3529320.3529328,2022,"Startups play a key role in software-based innovation. They make an important contribution to an economy’s ability to compete and innovate, and their importance will continue to grow due to increasing digitalization. However, the success of a startup depends primarily on market needs and the ability to develop a solution that is attractive enough for customers to choose. A sophisticated technical solution is usually not critical, especially in the early stages of a startup. It is not necessary to be an experienced software engineer to start a software startup. However, this can become problematic as the solution matures and software complexity increases. Based on a proposed solution for systematic software development for early-stage startups, in this paper, we present the key findings of a survey study to identify the methodological and technical priorities of software startups. Among other things, we found that requirements engineering and architecture pose challenges for startups. In addition, we found evidence that startups’ software development approaches do not tend to change over time. An early investment in a more scalable development approach could help avoid long-term software problems. To support such an investment, we propose an extended model for Entrepreneurial Software Engineering that provides a foundation for future research."
Pattern with partners: A systematic approach to handle knowledge sharing in GSD projects,"M. Goncalves, Klinsman and Pereira, Marcelo and Monteiro, Gelson and Fontao, Awdren",10.1145/3529320.3529333,2022,"Eldorado is an R&amp;D Institute responsible for OS customization for Android Smartphone Software worldwide. Eldorado is based in Brazil and works closely with other teams worldwide (China, United States, and India). Handling the project management considering this complex GSD scenario brings many challenges, one of them is the knowledge sharing on new development processes. As each region has its own set of procedures and specific carriers, the newcomers need to learn the new working processes. Even having a Wiki space to share information, the members report great difficulty using it, especially when they start to work on a project from a different region. For this reason, we developed a labeling-oriented project management approach based on identifying and defining project patterns. This experience report describes how we reduced the knowledge information required to manage a mobile software development process, the lessons learned applying the developed approach in mobile projects, and the future steps to improve it."
Open Learning Analytics: A Systematic Review of Benchmark Studies using Open University Learning Analytics Dataset (OULAD),"Alhakbani, Haya A. and Alnassar, Fatema M.",10.1145/3529399.3529413,2022,"Virtual learning has gained increased importance because of the recent pandemic situation. A mass shift to virtual means of education delivery has been observed over the past couple of years, forcing the community to develop efficient performance assessment tools. Open University Learning Analytics Dataset (OULAD) is one of the most comprehensive and benchmark datasets in the learning analytics domain. This paper presents the review of benchmark studies performed using OULAD to assess the performance of students in a Virtual Learning Environment (VLE). The presented review aims to highlight the status of technological advancements in this domain and potential future research directions."
How Do Software Companies Deal with Artificial Intelligence Ethics? A Gap Analysis,"Vakkuri, Ville and Kemell, Kai-Kristian and Tolvanen, Joel and Jantunen, Marianna and Halme, Erika and Abrahamsson, Pekka",10.1145/3530019.3530030,2022,"The public and academic discussion on Artificial Intelligence (AI) ethics is accelerating and the general public is becoming more aware AI ethics issues such as data privacy in these systems. To guide ethical development of AI systems, governmental and institutional actors, as well as companies, have drafted various guidelines for ethical AI. Though these guidelines are becoming increasingly common, they have been criticized for a lack of impact on industrial practice. There seems to be a gap between research and practice in the area, though its exact nature remains unknown. In this paper, we present a gap analysis of the current state of the art by comparing practices of 39 companies that work with AI systems to the seven key requirements for trustworthy AI presented in the “The Ethics Guidelines for Trustworthy Artificial Intelligence”. The key finding of this paper is that there is indeed notable gap between AI ethics guidelines and practice. Especially practices considering the novel requirements for software development, requirements of societal and environmental well-being and diversity, nondiscrimination and fairness were not tackled by companies."
An Empirical Study of Blockchain Repositories in GitHub,"Das, Ajoy and Uddin, Gias and Ruhe, Guenther",10.1145/3530019.3530041,2022,"Blockchain is a distributed ledger technique that guarantees the traceability of transactions. Blockchain is adopted in multiple domains like finance (e.g., cryptocurrency), healthcare, security, and supply chain. In the open-source software (OSS) portal GitHub, we observe a growing adoption of Blockchain-based solutions. Given the rapid emergence of Blockchain-based solutions in our daily life and the evolving cryptocurrency market, it is important to know the status quo, how developers generally interact in those repos, and how much freedom they have in applying code changes. We report an empirical study of 3,664 Blockchain software repositories from GitHub. We divide the Blockchain repositories into two categories: Tool (e.g., SDKs) and Applications (e.g., service/solutions developed using SDKs). The Application category is further divided into two sub-categories: Crypto and Non-Crypto applications. In all Blockchain repository categories, the contribution interactions on commits are the most common interaction type. We found that more organizations contributing to the Blockchain repos than individual users. The median numbers of internal and external users in tools are higher than the application repos. We observed a higher degree of collaboration (e.g., for maintenance efforts) among users in Blockchain tools than those in the application repos. Among the artifacts, issues have a greater number of interactions than commits and pull requests. Related to autonomy we found that less than half of total project contributions are autonomous. Our findings offer implications to Blockchain stakeholders, like developers to stay aware of OSS practices around Blockchain software."
AI-Based Software Defect Prediction for Trustworthy Android Apps,"sadaf, saadia and Iqbal, Danish and Buhnova, Barbora",10.1145/3530019.3531330,2022,"The present time in the industry is a time where Android Applications are in a wide range with its widespread of the users also. With the increased use of Android applications, the defects in the Android context have also been increasing. The malware of defective software can be any pernicious program with malignant effects. Many techniques based on static, dynamic, and hybrid approaches have been proposed with the combination of Machine learning (ML) or Artificial Intelligence (AI) techniques. In this regard. Scientifically, it is complicated to examine the malignant effects. A single approach cannot predict defects alone, so multiple approaches must be used simultaneously. However, the proposed techniques do not describe the types of defects they address. The paper aims to propose a framework that classifies the defects. The Artificial Intelligence (AI) techniques are described, and the different defects are mapped to them. The mapping of defects to AI techniques is based on the types of defects found in the Android Context. The accuracy of the techniques and the working criteria has been set as the mapping metrics. This will significantly improve the quality and testing of the product. However, the appropriate technique for a particular type of defect could be easily selected. This will reduce the cost and time efforts put into predicting defects."
Towards Continuous Streamflow Monitoring with Time-Lapse Cameras and Deep Learning,"Gupta, Amrita and Chang, Tony and Walker, Jeffrey and Letcher, Benjamin",10.1145/3530190.3534805,2022,"Effective water resources management depends on monitoring the volume of water flowing through streams and rivers, but collecting continuous discharge measurements using traditional streamflow gages is prohibitively expensive. Time-lapse cameras offer a low-cost option for streamflow monitoring, but training models for predicting streamflow directly from images requires streamflow data to use as labels, which are often unavailable. We address this data gap by proposing the alternative task of Streamflow Rank Estimation (SRE), in which the goal is to predict relative measures of streamflow such as percentile rank rather than absolute flow. In particular, we use a learning-to-rank framework to train SRE models using pairs of stream images ranked in order of discharge by an annotator, obviating the need for discharge training data and thus facilitating monitoring streamflow conditions at streams without gages. We also demonstrate a technique for converting SRE model predictions to stream discharge estimates given an estimated streamflow distribution. Using data and images from six small US streams, we compare the performance of SRE with conventional regression models trained to predict absolute discharge. Our results show that SRE performs nearly as well as regression models on relative flow prediction. Further, we observe that the accuracy of absolute discharge estimates obtained by mapping SRE model predictions through a discharge distribution largely depends on how well the assumed discharge distribution matches the field observed data."
Towards operationalizing the communal production and management of public (open) data: a pedestrian network case study: A pedestrian network case study in operationalizing communal open data,"Bolten, Nicholas and Caspi, Anat",10.1145/3530190.3534821,2022,"Data is an inseparable part of community management. Data openness and transparency has been a driver for change in government accountability and public engagement by providing unprecedented access to information. More prominently, there exists enthusiasm about the possibilities created by new and more extensive sources of data to improve our understanding and management of communities. This work examines a case study in collecting and operationalizing sustainable open data and specifically open government or civic data - information, public or otherwise, which anyone is free to access, analyze and re-use for any purpose - through a platform and community organizing effort in crowdsourcing open pedestrian network data. We outline a number of tensions or challenges in opening data, specifically in a number of realms where public interest stands to benefit from uses of the data, yet no single commercial or governmental entity is either liable or has a clear monetary interest associated with freely opening that data. In these specific cases, collection of these open data becomes a community-based challenge to undertake, which raises a number of additional socio-technical, political, and data provenance considerations. Beyond the technical contributions of our framework (in the open-source tools to support community activities, our case study contributes a number of insights and recommendations regarding community engagement, use of participatory co-design jointly with data collection tools, and planning for sustainable data stewardship in the involved communities."
A Proposal for Enhancing Agile Requirements Engineering with Prototyping and Enriched User Stories,"Keshk, Nader and El-Ramly, Mohammad and Salah, Akram",10.1145/3531056.3542773,2022,"Agile software development is the most developed and widely used family of software development processes. Agile Requirement Engineering (ARE) faces many challenges, such as managing with very little documentation and specifications, ensuring that the development team understands the requirements, and understanding all client needs. In agile methodologies, user stories are the primary means for capturing requirements. But most of the time, user stories are not enough to describe the requirements to the development team to the required level of details. Moreover, sometimes, the client does not have a clear vision of the system features from the beginning. S/he discovers them during or even after delivery, leading to an increase in the amount of rework. Hence, some researchers have suggested solutions to these challenges by using prototypes to support the user stories in explaining the requirements. Others suggested using enriched user stories to get more details about how to implement the requirements. In this paper, we discuss the challenges facing ARE, the role that each of prototyping and enriched user stories can play in improving ARE, and the limitations of each one if used alone. We propose a roadmap to improve ARE by a hybrid process that merges prototyping and enriched user stories along with extra validation steps."
Making the Unaccountable Internet: The Changing Meaning of Accounting in the Early ARPANET,"Cooper, A. Feder and Vidan, Gili",10.1145/3531146.3533137,2022,"Contemporary concerns over the governance of technological systems often run up against narratives about the technical infeasibility of designing mechanisms for accountability. While in recent AI ethics literature these concerns have been deliberated predominantly in relation to machine learning, other instances in the history of computing also presented circumstances in which computer scientists needed to un-muddle what it means to design accountable systems. One such compelling narrative can frequently be found in canonical histories of the Internet that highlight how its original designers’ commitment to the “End-to-End” architectural principle precluded other features from being implemented, resulting in the fast-growing, generative, but ultimately unaccountable network we have today. This paper offers a critique of such technologically essentialist notions of accountability and the characterization of the “unaccountable Internet” as an unintended consequence. It explores the changing meaning of accounting and its relationship to accountability in a selected corpus of requests for comments (RFCs) concerning the early Internet’s design from the 1970s and 80s. We characterize four ways of conceptualizing accounting: as billing, as measurement, as management, and as policy, and demonstrate how an understanding of accountability was constituted through these shifting meanings. We link together the administrative and technical mechanisms of accounting for shared resources in a distributed system and an emerging notion of accountability as a social, political, and technical category, arguing that the former is constitutive of the latter. Recovering this history is not only important for understanding the processes that shaped the Internet, but also serves as a starting point for unpacking the complicated political choices that are involved in designing accountability mechanisms for other technological systems today."
Keep Your Friends Close and Your Counterfactuals Closer: Improved Learning From Closest Rather Than Plausible Counterfactual Explanations in an Abstract Setting,"Kuhl, Ulrike and Artelt, Andr\'{e} and Hammer, Barbara",10.1145/3531146.3534630,2022,"Counterfactual explanations (CFEs) highlight changes to a model’s input that alter its prediction in a particular way. s have gained considerable traction as a psychologically grounded solution for explainable artificial intelligence (XAI). Recent innovations introduce the notion of plausibility for automatically generated s, enhancing their robustness by exclusively creating plausible explanations. However, practical benefits of this constraint on user experience are yet unclear. In this study, we evaluate objective and subjective usability of plausible s in an iterative learning task. We rely on a game-like experimental design, revolving around an abstract scenario. Our results show that novice users benefit less from receiving plausible rather than closest s that induce minimal changes leading to the desired outcome. Responses in a post-game survey reveal no differences for subjective usability between both groups. Following the view of psychological plausibility as comparative similarity, users in the closest condition may experience their s as more psychologically plausible than the computationally plausible counterpart. In sum, our work highlights a little-considered divergence of definitions of computational plausibility and psychological plausibility, critically confirming the need to incorporate human behavior, preferences and mental models already at the design stages of XAI. All source code and data of the current study are available: https://github.com/ukuhl/PlausibleAlienZoo"
A Scoping Review of Ethics Across SIGCHI,"Nunes Vilaza, Giovanna and Doherty, Kevin and McCashin, Darragh and Coyle, David and Bardram, Jakob and Barry, Marguerite",10.1145/3532106.3533511,2022,"Ethical deliberation has proved a consistent feature of Human-Computer Interaction (HCI) since its earliest years, spanning the respectful involvement of research participants to design choices impacting fairness, freedom and welfare. Despite growing discussions, applied knowledge and practical approaches for navigating complex moral dilemmas remain challenging to grasp. Motivated by the need for a structured overview, this paper contributes a scoping review of ethics as discussed across 129 full-length SIGCHI papers containing the search term ‘ethic*’ in their title, abstract or authors’ keywords over the last ten years. Findings show increasing prioritisation of the topic, particularly within Artificial Intelligence. Value-Sensitive and Critical Design appear as the most frequently applied orientations, and participatory approaches are more prevalent than those without end-user input. Engaging with a spectrum from personal to societal concerns, the SIGCHI literature thus echos calls for critical perspectives on user-centred processes and the need to establish more sustainable responsibility structures."
Blockchain Technology Research Hotspots from the Perspective of Smart City,"LIU, Feng and FAN, Hao-yang",10.1145/3532640.3532654,2022,"It is very important to understand the current situation of blockchain technology on a global scale. The large-scale application of blockchain technology is an inevitable trend. This article revolves around published papers related to blockchain technology, using keywords and abstract content to search for Chinese papers and English papers. Relevance analysis and sorting through the retrieved documents with six core layers of blockchain respectively: Application Layer, Contract Layer, Actuator Layer, Consensus Layer, Network Layer and Data Layer. Based on the analysis results, the study found that China's research is more preference and application of landing and industry, while international research is more preferred to the research of the underlying technology of finance and blockchain. This article illustrates the current status of blockchain technology and applications from the smart city, and contribute to scholars and technicians who are interested in or research blockchain."
Towards Brain Metrics for Improving Multi-Agent Adaptive Human-Robot Collaboration: A Preliminary Study,"Howell-Munson, Alicia and Doherty, Emily and Gavriel, Peter and Nicolas, Claire and Norton, Adam and Neamtu, Rodica and Yanco, Holly and Wu, Yi-Ning and Solovey, Erin T.",10.1145/3533406.3533419,2022,"When humans work closely together, they can pick up subtle cues from their team members and adapt their behavior appropriately. Humans working closely with robots may also give off cues, but the robots cannot detect these signals and therefore cannot change behavior. In this paper, we focus on heterogeneous multi-human and robot teams. Such scenarios exist frequently in search and rescue operations as well as space missions, where robots perform tasks that are unsafe or even impossible for humans. At the same time, human team members collaborate to make important decisions that influence and direct the robots’ work. These decisions often have to be made quickly with high levels of uncertainty, with simultaneous physical and mental demands on the human. In this project, we aim to explore the following questions: Can brain data provide insights that could improve team performance? Could we use these signals to detect when someone is experiencing excessive workload? Could we detect an impact on team performance caused by the robot?"
Investigating Information Security in Systems-of-Systems,"Dias, Roberto Monteiro and Zacarias, Rodrigo Oliveira and Varella, Jorge Luis de Lima and dos Santos, Rodrigo Pereira",10.1145/3535511.3535523,2022,"Context: Changes in society have made information systems more complex. This also happens to a category of systems defined as system-of-systems (SoS) and system-of-information systems (SoIS). Problem: Although SoS offers benefits to organizations, the difficulty of IT managers in dealing with information security in these systems can leave them vulnerable to threats and impacts caused by cyber-attacks. Solution: This study presents mechanisms and technologies that should be implemented to ensure that communication between systems is treated from the perspective of information security. IS theory: This research is based on the General Systems Theory that allows to understand SoS as a type of complex system. With the increase in tasks complexity, constituent systems collaborate and offer functionalities that could not be achieved by them in an isolated form. Method: A systematic mapping study (SMS) was carried out to identify how information security technologies are used in the context of SoS. Moreover, a survey research was conducted to analyze information security aspects in order to evaluate the results obtained in the SMS with respect to their applicability in industry. Summary of Results: 18 studies were reviewed in the SMS and 32 experts participated in the survey. Both studies show that stakeholders need to understand vulnerabilities, exposure, and the contribution technology makes to prevent cyberattacks and mitigate SoS risks. Contributions and Impact in the IS area: This work presents an overview of information security in SoS, highlighting related technologies so that stakeholders can reflect on cyber threats in decision-making processes in organizations, exploring the grand research challenge in IS “Smart Systems-of-Information Systems: Foundations and an Assessment Model for Research Development”."
Commensality or Reverie in Eating? Exploring the Solo Dining Experience,"Bocanegra, Mimi and Lemke, Mailin and de Vries, Roelof A.J. and Ludden, Geke D.S.",10.1145/3536221.3556577,2022,"Commensality, the act of eating together, is commonly associated with many benefits. Dining solo, in contrast, is frequently connected to adverse effects on a person. There is a growing interest in human-computer interaction (HCI) and design in how innovations can enhance eating experiences by, for example, facilitating commensality. The steadily growing number of people eating alone and the associated risks beg the question of how HCI and design could contribute to and improve the solo dining experience and whether or not mimicking or facilitating commensality is what solo diners want. This two-phased study reports on the context exploration of the multimodal solitary dining experience. In the first phase, we scoped the literature describing the benefits and drawbacks of solo dining and commensality. For the second phase, a digital food diary was developed and completed by six solo diners to collect further insights and user requirements. Photos and annotations collected as part of the food diary were analyzed using content analysis. The results indicate several advantages of eating alone, including feeling relaxed, perceiving solo dining as a moment of self-pampering, and appreciating the cooking experience. Overall, it seems that solo dining is not merely a lack of commensality, but a unique experience in and of itself, where people seem to strive towards finding reverie in eating."
SoK: Applications and Challenges of using Recommender Systems in Cybersecurity Incident Handling and Response,"Hus\'{a}k, Martin and \v{C}erm\'{a}k, Milan",10.1145/3538969.3538981,2022,"Incident handling, a fundamental activity of a cybersecurity incident response team, is a complex discipline that consumes a significant amount of personnel’s time and costs. There are continuous efforts to facilitate incident handling and response in terms of providing procedural or decision support and processing relevant data. In this paper, we survey the approaches towards (semi-)automated incident handling and response backed by recommender systems that are successful in other domains. We discuss which phases and tiers of incident handling can be automated and to what level while evaluating the maturity of proposed approaches and tools. While we did not find a full-scale recommender system that would guide the user through incident handling and suggest which steps to take, many of them aim at particular problems. The discussed issues are not resolved yet but seem to get the attention of researchers and will likely be investigated in the future."
Job Adverts Analyzer for Cybersecurity Skills Needs Evaluation,"Ricci, Sara and Sikora, Marek and Parker, Simon and Lendak, Imre and Danidou, Yianna and Chatzopoulou, Argyro and Badonnel, Remi and Alksnys, Donatas",10.1145/3538969.3543821,2022,"This article presents a new free web-based application, the Cybersecurity Job Ads Analyzer, which has been created to collect and analyse job adverts using a machine learning algorithm. This algorithm enables the detection of the skills required in advertised cybersecurity work positions. The application is both interactive and dynamic allowing for automated analyses and for the underlying database of job adverts to be easily updated. Through the Cybersecurity Job Ads Analyzer, it is possible to explore the skills required over time, and thereby enable academia and other training providers to better understand and address the needs of the industry. We will describe in detail the user interface and technical background of the application, as well as highlight the preliminary statistical results we have obtained from analysing the current database of job adverts."
Dual Contrastive Transformer for Hierarchical Preference Modeling in Sequential Recommendation,"Huang, Chengkai and Wang, Shoujin and Wang, Xianzhi and Yao, Lina",10.1145/3539618.3591672,2023,"Sequential recommender systems (SRSs) aim to predict the subsequent items which may interest users via comprehensively modeling users' complex preference embedded in the sequence of user-item interactions. However, most of existing SRSs often model users' single low-level preference based on item ID information while ignoring the high-level preference revealed by item attribute information, such as item category. Furthermore, they often utilize limited sequence context information to predict the next item while overlooking richer inter-item semantic relations. To this end, in this paper, we proposed a novel hierarchical preference modeling framework to substantially model the complex low- and high-level preference dynamics for accurate sequential recommendation. Specifically, in the framework, a novel dual-transformer module and a novel dual contrastive learning scheme have been designed to discriminatively learn users' low- and high-level preference and to effectively enhance both low- and high-level preference learning respectively. In addition, a novel semantics-enhanced context embedding module has been devised to generate more informative context embedding for further improving the recommendation performance. Extensive experiments on six real-world datasets have demonstrated both the superiority of our proposed method over the state-of-the-art ones and the rationality of our design."
The Tale of Two MSMARCO - and Their Unfair Comparisons,"Lassance, Carlos and Clinchant, Stephane",10.1145/3539618.3592071,2023,"The MS MARCO-passage dataset has been the main large-scale dataset open to the IR community and it has fostered successfully the development of novel neural retrieval models over the years. But, it turns out that two different corpora of MS MARCO are used in the literature, the official one and a second one where passages were augmented with titles, mostly due to the introduction of the Tevatron code base. However, the addition of titles actually leaks relevance information, while breaking the original guidelines of the MS MARCO-passage dataset. In this work, we investigate the differences between the two corpora and demonstrate empirically that they make a significant difference when evaluating a new method. In other words, we show that if a paper does not properly report which version is used, reproducing fairly its results is basically impossible. Furthermore, given the current status of reviewing, where monitoring state-of-the-art results is of great importance, having two different versions of a dataset is a large problem. This is why this paper aims to report the importance of this issue so that researchers can be made aware of this problem and appropriately report their results."
23 shades of self-admitted technical debt: an empirical study on machine learning software,"OBrien, David and Biswas, Sumon and Imtiaz, Sayem and Abdalkareem, Rabe and Shihab, Emad and Rajan, Hridesh",10.1145/3540250.3549088,2022,"In software development, the term “technical debt” (TD) is used to characterize short-term solutions and workarounds implemented in source code which may incur a long-term cost. Technical debt has a variety of forms and can thus affect multiple qualities of software including but not limited to its legibility, performance, and structure. In this paper, we have conducted a comprehensive study on the technical debts in machine learning (ML) based software. TD can appear differently in ML software by infecting the data that ML models are trained on, thus affecting the functional behavior of ML systems. The growing inclusion of ML components in modern software systems have introduced a new set of TDs. Does ML software have similar TDs to traditional software? If not, what are the new types of ML specific TDs? Which ML pipeline stages do these debts appear? Do these debts differ in ML tools and applications and when they get removed? Currently, we do not know the state of the ML TDs in the wild. To address these questions, we mined 68,820 self-admitted technical debts (SATD) from all the revisions of a curated dataset consisting of 2,641 popular ML repositories from GitHub, along with their introduction and removal. By applying an open-coding scheme and following upon prior works, we provide a comprehensive taxonomy of ML SATDs. Our study analyzes ML SATD type organizations, their frequencies within stages of ML software, the differences between ML SATDs in applications and tools, and quantifies the removal of ML SATDs. The findings discovered suggest implications for ML developers and researchers to create maintainable ML systems."
Nalanda: a socio-technical graph platform for building software analytics tools at enterprise scale,"Maddila, Chandra and Shanbhogue, Suhas and Agrawal, Apoorva and Zimmermann, Thomas and Bansal, Chetan and Forsgren, Nicole and Agrawal, Divyanshu and Herzig, Kim and van Deursen, Arie",10.1145/3540250.3558949,2022,"Software development is information-dense knowledge work that requires collaboration with other developers and awareness of artifacts such as work items, pull requests, and file changes. With the speed of development increasing, information overload and information discovery are challenges for people developing and maintaining these systems. Finding information about similar code changes and experts is difficult for software engineers, especially when they work in large software systems or have just recently joined a project. In this paper, we build a large scale data platform named Nalanda platform to address the challenges of information overload and discovery. Nalanda contains two subsystems: (1) a large scale socio-technical graph system, named Nalanda graph system, and (2) a large scale index system, named Nalanda index system that aims at satisfying the information needs of software developers. To show the versatility of the Nalanda platform, we built two applications: (1) a software analytics application with a news feed named MyNalanda that has Daily Active Users (DAU) of 290 and Monthly Active Users (MAU) of 590, and (2) a recommendation system for related work items and pull requests that accomplished similar tasks (artifact recommendation) and a recommendation system for subject matter experts (expert recommendation), augmented by the Nalanda socio-technical graph. Initial studies of the two applications found that developers and engineering managers are favorable toward continued use of the news feed application for information discovery. The studies also found that developers agreed that a system like Nalanda artifact and expert recommendation application could reduce the time spent and the number of places needed to visit to find information."
Understanding automated code review process and developer experience in industry,"Kim, Hyungjin and Kwon, Yonghwi and Joh, Sangwoo and Kwon, Hyukin and Ryou, Yeonhee and Kim, Taeksu",10.1145/3540250.3558950,2022,"Code Review Automation can reduce human efforts during code review by automatically providing valuable information to reviewers. Nevertheless, it is a challenge to automate the process for large-scale companies, such as Samsung Electronics, due to their complexity: various development environments, frequent review requests, huge size of software, and diverse process among the teams. In this work, we show how we automated the code review process for those intricate environments, and share some lessons learned during two years of operation. Our unified code review automation system, Code Review Bot, is designed to process review requests holistically regardless of such environments, and checks various quality-assurance items such as potential defects in the code, coding style, test coverage, and open source license violations. Some key findings include: 1) about 60% of issues found by Code Review Bot were reviewed and fixed in advance of product releases, 2) more than 70% of developers gave positive feedback about the system, 3) developers rapidly and actively responded to reviews, and 4) the automation did not much affect the amount or the frequency of human code reviews compared to the internal policy to encourage code review activities. Our findings provide practical evidence that automating code review helps assure software quality."
An empirical study of deep transfer learning-based program repair for Kotlin projects,"Kim, Misoo and Kim, Youngkyoung and Jeong, Hohyeon and Heo, Jinseok and Kim, Sungoh and Chung, Hyunhee and Lee, Eunseok",10.1145/3540250.3558967,2022,"Deep learning-based automated program repair (DL-APR) can automatically fix software bugs and has received significant attention in the industry because of its potential to significantly reduce software development and maintenance costs. The Samsung mobile experience (MX) team is currently switching from Java to Kotlin projects. This study reviews the application of DL-APR, which automatically fixes defects that arise during this switching process; however, the shortage of Kotlin defect-fixing datasets in Samsung MX team precludes us from fully utilizing the power of deep learning. Therefore, strategies are needed to effectively reuse the pretrained DL-APR model. This demand can be met using the Kotlin defect-fixing datasets constructed from industrial and open-source repositories, and transfer learning.  
This study aims to validate the performance of the pretrained DL-APR model in fixing defects in the Samsung Kotlin projects, then improve its performance by applying transfer learning. We show that transfer learning with open source and industrial Kotlin defect-fixing datasets can improve the defect-fixing performance of the existing DL-APR by 307%. Furthermore, we confirmed that the performance was improved by 532% compared with the baseline DL-APR model as a result of transferring the knowledge of an industrial (non-defect) bug-fixing dataset. We also discovered that the embedded vectors and overlapping code tokens of the code-change pairs are valuable features for selecting useful knowledge transfer instances by improving the performance of APR models by up to 696%. Our study demonstrates the possibility of applying transfer learning to practitioners who review the application of DL-APR to industrial software."
Research on Employment Promotion System Based on Smart Education,"Fan, Yi and Yang, Dequan and Zhu, Mengyu and Wang, Keyong",10.1145/3543321.3543328,2022,"With the advent of the 5G era, the wisdom of education has long become an essential part of building an intelligent city. Intelligent education has a variety of forms of education realization to provide for social development. At the same time, it has also put forward the requirements for the shortage of high-end talents and reflected the ultimate goal of education in the grim employment situation. With the ultimate realization of human social resources development and education as the goal, through policy support, the establishment of entrepreneurship and employment promotion system, improve the operation mode of entrepreneurship and employment. Through policy support, we will establish a strategy to promote entrepreneurship and engagement and enhance the operation mode of entrepreneurship and employment. To build an intelligent campus management model as the starting point, comprehensively improve the scope of innovative education, shorten the distance between students and job seekers, the job market. Develop new channels to train professional talents and enhance teaching quality; Decentralized integration of schools, employers, education and training institutions, students and job seekers, and other information resources, unified macro allocation by promoting employment, to create a new model of talent training."
Mind the Gap: Towards an Understanding of Government Decision-Making based on Artificial Intelligence,"Valle-Cruz, David and Garc\'{\i}a-Contreras, Rigoberto and Mu\~{n}oz-Ch\'{a}vez, J. Patricia",10.1145/3543434.3543445,2022,"Decision-making has become more critical for organizations in the 21st century. The citizens’ countless needs and the emerging problems (internal and external) faced by governments increase the complexity of government decisions worldwide. The research question guiding this attempt is: How is government decision-making grounded on artificial intelligence (AI)? Based on the PRISMA approach and empirical analysis of some international cases are adopted. The authors analyze different organizational and environmental factors, the objectives, benefits, and risks of AI-supported decision-making. The findings show an increasing interest in the research on government decision-making based on AI. Finally, there is the potential of AI to support decision-making for the benefit of citizens and public value generation, collaboratively between governments, industry, and society. Future work will further analyze AI-based decision-making in government in depth."
Framework for designing interoperable public service architectures with exemplification along small-scale procurement and PEPPOL,"Schmitz, Andreas and Mondorf, Ansgar and Wimmer, Maria A.",10.1145/3543434.3543473,2022,"Digitalization of public services, particularly in cross-border contexts, demands for a high level of interoperability. To effectively and systematically cater for interoperability in public service design, architecture development methodologies represent important means of support. The Design Science Research Methodology (DSRM) or the The Open Group Architecture Framework (TOGAF) Architecture Development Method (ADM) provide great tools for rigorously developing digital public services and service architectures. However, these approaches do not per se sufficiently address interoperability. This paper proposes a methodical framework based on DSRM and TOGAF to develop public service architectures, which tackles all layers of interoperability of the European Interoperability Framework (EIF). The proposed framework is tested on the use-case of small-scale public procurement services and on PEPPOL standard specifications. The framework comprises six phases, covering the four layers of interoperability and the TOGAF viewpoints while enabling an iterative design process, which is inspired by DSRM."
Need for skilled workers in the area of Data Science and Cloud Computing in Styria,"Raab, Raphaele and Granigg, Wolfgang and Melcher, Michael",10.1145/3543712.3543749,2022,"The aim of this paper is to discuss the results of a survey conducted to assess the need for skilled workers in the areas Data Science &amp; Cloud Computing in Styria, Austria. Firstly, the relevant roles and skills in the abovementioned areas had to be selected. Initially, this selection process is described. Consequently, a survey was designed and given to a representative group of companies. The survey includes questions regarding the need for skilled workers with respect to the domains and the selected skills in the areas Data Science &amp; Cloud Computing. Moreover, the respondents were asked about the importance of further education and the necessity of academic education in these areas. Overall, our survey concludes that the requirements for skilled workers in the areas of Data Science and Cloud Computing in Styria will increase significantly in the coming years."
A Survey of Natural Design for Interaction,"Hirsch, Linda and Li, Jingyi and Mayer, Sven and Butz, Andreas",10.1145/3543758.3543773,2022,"The term “Natural Design” has various meanings and applications within and beyond the human-computer interaction community. Yet, there is no consensus on whether it is a relevant design approach or only a descriptive term without profound meaning. We investigated the current understanding and design potential of “Natural Design” for interaction in a systematic literature review. By analyzing and rating 113 papers, we identified 47 relevant papers that applied Natural Design in different contexts. The understanding of the approach changes from nature-related inspirations to context-dependent naturalness based on increasing familiarity or expectations. We present a structured overview of these relevant papers, contribute a systematic Natural Design model for interaction and add 20 implications for applying Natural Design to natural user interfaces, natural interaction, or computation. We identified “Natural Design” as a relevant design approach to create intuitive and embedded interfaces that can profit from related concepts outside human-computer interaction."
Tooling for Developing Data-Driven Applications: Overview and Outlook,"Weber, Thomas and Hu\ss{}mann, Heinrich",10.1145/3543758.3543779,2022,"Machine Learning systems are, by now, an essential part of the software landscape. From the development perspective this means a paradigmatic shift, which should be reflected in the way we write software. For now, the majority of developers relies on traditional tools for data-driven development, though. To determine how research into tools is catching up, we conducted a systematic literature review, searching for tools dedicated to data-driven development. Of the 1511 search results, we analyzed 76 relevant publications in detail. The diverse sample indicated a strong interest in this topic from different domains, with different approaches and methods. While there are a number of common trends, e.g. the use of visualization, in these tools, only a limited, although increasing, number of these tools has so far been evaluated comprehensively. We therefore summarize trends, strengths and weaknesses in the status quo for data-driven development tools and conclude with a number of potential future directions this field."
Wikidata: The Making Of,"Vrande\v{c}i\'{c}, Denny and Pintscher, Lydia and Kr\""{o}tzsch, Markus",10.1145/3543873.3585579,2023,"Wikidata, now a decade old, is the largest public knowledge graph, with data on more than 100 million concepts contributed by over 560,000 editors. It is widely used in applications and research. At its launch in late 2012, however, it was little more than a hopeful new Wikimedia project, with no content, almost no community, and a severely restricted platform. Seven years earlier still, in 2005, it was merely a rough idea of a few PhD students, a conceptual nucleus that had yet to pick up many important influences from others to turn into what is now called Wikidata. In this paper, we try to recount this remarkable journey, and we review what has been accomplished, what has been given up on, and what is yet left to do for the future."
"Towards a Semantic Approach for Linked Dataspace, Model and Data Cards","Donald, Andy and Galanopoulos, Apostolos and Curry, Edward and Mu\~{n}oz, Emir and Ullah, Ihsan and Waskow, M. A. and Dabrowski, Maciej and Kalra, Manan",10.1145/3543873.3587659,2023,"The vast majority of artificial intelligence practitioners overlook the importance of documentation when building and publishing models and datasets. However, due to the recent trend in the explainability and fairness of AI models, several frameworks have been proposed such as Model Cards, and Data Cards, among others, to help in the appropriate re-usage of those models and datasets. In addition, because of the introduction of the dataspace concept for similar datasets in one place, there is potential that similar Model Cards, Data Cards, Service Cards, and Dataspace Cards can be linked to extract helpful information for better decision-making about which model and data can be used for a specific application. This paper reviews the case for considering a Semantic Web approach for exchanging Model/Data Cards as Linked Data or knowledge graphs in a dataspace, making them machine-readable. We discuss the basic concepts and propose a schema for linking Data Cards and Model Cards within a dataspace. In addition, we introduce the concept of a dataspace card which can be a starting point for extracting knowledge about models and datasets in a dataspace. This helps in building trust and reuse of models and data among companies and individuals participating as publishers or consumers of such assets."
Stratifying large software files to improve prediction performance in software defect prediction,"Alshehri, Yasser Ali and Alnazzawi, Noha and Hijazi, Haneen and Alharbi, Rawan",10.1145/3543895.3543924,2023,"Size is one of the significant factors associated with bugs, and it has been used to predict software faults. We believe that stratifying software files based on size can play an essential role in improving prediction performance. This study explored the effect of size by stratifying our sample based on each unit’s size and distributing software units in multiple stratified groups based on an equal distribution approach. We stratified the Eclipse Europa project files, and we reported the performance of each stratified group and compared them. We used two popular classifiers, decision tree J48, and random forest, to implement this experiment. These classifiers presented similar results on the same group of files. The results indicated that predicting faults with large files is better than predicting those in small files. In addition, the results showed higher median values of all performance measures and less variation in each measure."
Project Information Management of BIM Technology in the Context of intelligent Construction Site,"Wang, Hechun",10.1145/3544109.3544144,2022,"The traditional construction mode of construction site with CAD as the core is gradually unable to meet the needs of productivity development. Therefore, according to the needs of the national business innovation strategy and the experience of foreign advanced BIM technology, a new engineering project information management system with BIM technology as the core has been formulated. According to the development situation and application status of existing BIM technology at home and abroad, this article summarizes the above-mentioned application results, and analyzes some inherent shortcomings of the existing model. On this basis, targeted access and application of BIM technology are carried out in accordance with the characteristics of the visual operation platform and intelligent construction site. Through the comparison of simulation examples, it is concluded that the intelligent construction site information management level after BIM access has been significantly improved."
Education Internationalization Strategy Management System Based on Fuzzy Neural Network,"Huang, Rong",10.1145/3544109.3544339,2022,
A Human-Centered Review of Algorithms in Decision-Making in Higher Education,"McConvey, Kelly and Guha, Shion and Kuzminykh, Anastasia",10.1145/3544548.3580658,2023,"The use of algorithms for decision-making in higher education is steadily growing, promising cost-savings to institutions and personalized service for students but also raising ethical challenges around surveillance, fairness, and interpretation of data. To address the lack of systematic understanding of how these algorithms are currently designed, we reviewed an extensive corpus of papers proposing algorithms for decision-making in higher education. We categorized them based on input data, computational method, and target outcome, and then investigated the interrelations of these factors with the application of human-centered lenses: theoretical, participatory, or speculative design. We found that the models are trending towards deep learning, and increased use of student personal data and protected attributes, with the target scope expanding towards automated decisions. However, despite the associated decrease in interpretability and explainability, current development predominantly fails to incorporate human-centered lenses. We discuss the challenges with these trends and advocate for a human-centered approach."
"What is in the Cards: Exploring Uses, Patterns, and Trends in Design Cards","Hsieh, Gary and Halperin, Brett A. and Schmitz, Evan and Chew, Yen Nee and Tseng, Yuan-Chi",10.1145/3544548.3580712,2023,"Card-based design tools–design cards–increasingly present opportunities to support practitioners. However, the breadth and depth of the design card landscape remain underexplored. In this work, we surveyed 103 design practitioners to assess current usages and associated barriers. Additionally, we analyzed and classified 161 decks of design cards from 1952-2020. We held a workshop with four experienced practitioners to generate initial categories, and then coded the remaining decks. We found that the cards contain seven different types of design knowledge: Creative Inspiration; Human Insights; Material &amp; Domain; Methods &amp; Tooling; Problem Definition; Team Building; and Values in Practice. The content of these cards can support designers across design stages; however, most are intended to support the early stages of design (e.g., research and ideation) rather than later design stages (e.g., prototyping and implementation). We share additional patterns uncovered and provide recommendations to support the future development and adoption of these tools."
A Descriptive Analysis of a Formative Decade of Research in Affective Haptic System Design,"Vyas, Preeti and Desai, Unma Mayur and Yamakawa, Karin and Maclean, Karon",10.1145/3544548.3580735,2023,"The global pandemic exposed serious drawbacks in relying on communication modalities in which social touch, however important, is absent. Considerable research has explored haptic technologies for sensing or displaying social touch and influencing affective state, for wellness, social communication, emotion regulation, and affect therapy. However, this Affective Haptic System design (AHSD)&nbsp;work varies widely in purpose and origin discipline, making it difficult to perceive overall progress and identify primary obstacles to practical deployment. We conducted a scoping review and conceptual analysis with a design lens, identifying 110 papers from the last decade in 11 ACM and IEEE venues that regularly attract AHSD&nbsp;work. Our analysis identified 38 dimensions within 8 facets: demographic, theoretical grounding, impact, system specification, usage specification, ethical consideration, technology, and evaluation. We visualize trends, disciplinary mixing, and topical focus over time, and highlight major advances while pinning down crucial gaps that can be addressed in the future."
Responsible &amp; Inclusive Cards: An Online Card Tool to Promote Critical Reflection in Technology Industry Work Practices,"Elsayed-Ali, Salma and Berger, Sara E and Santana, Vagner Figueredo De and Becerra Sandoval, ‪Juana Catalina",10.1145/3544548.3580771,2023,"Societal implications of technology are often considered after public deployment. However, broader impacts ought to be considered during the onset and throughout development to reduce potential for harmful uses, biases, and exclusions. There is a need for tools and frameworks that help technologists become more aware of broader contexts of their work and engage in more responsible and inclusive practices. In this paper, we introduce an online card tool containing questions to scaffold critical reflection about projects’ impacts on society, business, and research. We present the iterative design of the Responsible &amp; Inclusive Cards and findings from five workshops (n=21 participants) with teams distributed across a multinational technology corporation, as well as interviews with people with disabilities to assess gameplay and mental models. We found the tool promoted discussions about challenging topics, reduced power gaps through democratized turn-taking, and enabled participants to identify concrete areas to improve their practice."
How to Communicate Robot Motion Intent: A Scoping Review,"Pascher, Max and Gruenefeld, Uwe and Schneegass, Stefan and Gerken, Jens",10.1145/3544548.3580857,2023,"Robots are becoming increasingly omnipresent in our daily lives, supporting us and carrying out autonomous tasks. In Human-Robot Interaction, human actors benefit from understanding the robot’s motion intent to avoid task failures and foster collaboration. Finding effective ways to communicate this intent to users has recently received increased research interest. However, no common language has been established to systematize robot motion intent. This work presents a scoping review aimed at unifying existing knowledge. Based on our analysis, we present an intent communication model that depicts the relationship between robot and human through different intent dimensions (intent type, intent information, intent location). We discuss these different intent dimensions and their interrelationships with different kinds of robots and human roles. Throughout our analysis, we classify the existing research literature along our intent communication model, allowing us to identify key patterns and possible directions for future research."
12 Ways to Empower: Designing for Children’s Digital Autonomy,"Wang, Ge and Zhao, Jun and Van Kleek, Max and Shadbolt, Nigel",10.1145/3544548.3580935,2023,"In recent years, growing research has been made on supporting children to become more autonomous in the digital environment around them. However, there has been little consensus regarding the conceptualisation of digital autonomy for children in the HCI community and how best they can be supported. Through a systematic review of autonomy-supportive designs within HCI research, this paper makes three contributions: a landscape overview of the existing conceptualisation of Digital Autonomy for children within HCI; a framework of 12 distinct design mechanisms for supporting children’s digital autonomy, clustered into 5 categories by their common mechanisms; and an identification of 5 critical design considerations for future support of children’s digital autonomy. Our findings provide a critical understanding of current support for children’s digital autonomy in HCI. We highlight the importance of considering children’s digital autonomy from multi-perspectives and suggest critical factors and gaps to be considered for future autonomy-supportive designs."
“Who is the right homeless client?”: Values in Algorithmic Homelessness Service Provision and Machine Learning Research,"Showkat, Dilruba and Smith, Angela D. R. and Lingqing, Wang and To, Alexandra",10.1145/3544548.3581010,2023,"Homelessness presents a long-standing problem worldwide. Like other welfare services, homeless services have gained increased traction in Machine Learning (ML) research. Unhoused persons are vulnerable and using their data in the ML pipeline raises serious concerns about the unintended harms and consequences of prioritizing different ML values. To address this, we conducted a critical analysis of 40 research papers identified through a systematic literature review in ML homelessness service provision research. We found that the values of novelty, performance, and identifying limitations were uplifted in these papers, whereas (in)efficiency, (low/high) cost, fast, (violated) privacy, and (homeless condition) reproducibility valuescollapse. Consequently, unhoused persons were lost (i.e., humans were deprioritized) at multi-level ML abstraction of predictors, categories, and algorithms. Our findings illuminate potential pathways forward at the intersection of data science, HCI and STS by situating humans at the center to support this vulnerable community."
What Do We Mean When We Talk about Trust in Social Media? A Systematic Review,"Zhang, Yixuan and Gaggiano, Joseph D and Yongsatianchot, Nutchanon and Suhaimi, Nurul M and Kim, Miso and Sun, Yifan and Griffin, Jacqueline and Parker, Andrea G",10.1145/3544548.3581019,2023,"Do people trust social media? If so, why, in what contexts, and how does that trust impact their lives? Researchers, companies, and journalists alike have increasingly investigated these questions, which are fundamental to understanding social media interactions and their implications for society. However, trust in social media is a complex concept, and there is conflicting evidence about the antecedents and implications of trusting social media content, users, and platforms. More problematic is that we lack basic agreement as to what trust means in the context of social media. Addressing these challenges, we conducted a systematic review to identify themes and challenges in this field. Through our analysis of 70 papers, we contribute a synthesis of how trust in social media is defined, conceptualized, and measured, a summary of trust antecedents in social media, an understanding of how trust in social media impacts behaviors and attitudes, and directions for future work."
When XR and AI Meet - A Scoping Review on Extended Reality and Artificial Intelligence,"Hirzle, Teresa and M\""{u}ller, Florian and Draxler, Fiona and Schmitz, Martin and Knierim, Pascal and Hornb\ae{}k, Kasper",10.1145/3544548.3581072,2023,"Research on Extended Reality (XR) and Artificial Intelligence (AI) is booming, which has led to an emerging body of literature in their intersection. However, the main topics in this intersection are unclear, as are the benefits of combining XR and AI. This paper presents a scoping review that highlights how XR is applied in AI research and vice versa. We screened 2619 publications from 203 international venues published between 2017 and 2021, followed by an in-depth review of 311 papers. Based on our review, we identify five main topics at the intersection of XR and AI, showing how research at the intersection can benefit each other. Furthermore, we present a list of commonly used datasets, software, libraries, and models to help researchers interested in this intersection. Finally, we present 13 research opportunities and recommendations for future work in XR and AI research."
Virtual and Augmented Reality for Environmental Sustainability: A Systematic Review,"Cosio, Laura D and Buruk, O\u{g}uz 'Oz' and Fern\'{a}ndez Galeote, Daniel and Bosman, Isak De Villiers and Hamari, Juho",10.1145/3544548.3581147,2023,"In recent years, extended reality (XR) technology has seen a rise in use in environmental subjects, i.e., climate change or biodiversity loss, as a potential tool to inform and engage the public with current and future environmental issues. However, research on the potential of XR technology for environmental sustainability is still in the early stages, and there is no clear synthesis of the methods studied in this field. To provide a clearer view of existing approaches and research objectives, we systematically reviewed current literature dealing with XR use in environmental topics. Although the results indicate that the volume of literature exploring XR in environmental applications is increasing, empirical evidence of its impact is limited, hindering the possibility of presently drawing significant conclusions on its potential benefits. Based on our analyses, we identified thematic, theoretical, and methodological knowledge gaps and provide a guideline to aid future research in the field."
Measuring and Understanding Trust Calibrations for Automated Systems: A Survey of the State-Of-The-Art and Future Directions,"Wischnewski, Magdalena and Kr\""{a}mer, Nicole and M\""{u}ller, Emmanuel",10.1145/3544548.3581197,2023,"Trust has been recognized as a central variable to explain the resistance to using automated systems (under-trust) and the overreliance on automated systems (over-trust). To achieve appropriate reliance, users’ trust should be calibrated to reflect a system’s capabilities. Studies from various disciplines have examined different interventions to attain such trust calibration. Based on a literature body of 1000+ papers, we identified 96 relevant publications which aimed to calibrate users’ trust in automated systems. To provide an in-depth overview of the state-of-the-art, we reviewed and summarized measurements of the trust calibration, interventions, and results of these efforts. For the numerous promising calibration interventions, we extract common design choices and structure these into four dimensions of trust calibration interventions to guide future studies. Our findings indicate that the measurement of the trust calibration often limits the interpretation of the effects of different interventions. We suggest future directions for this problem."
On Hackathons: A Multidisciplinary Literature Review,"Chau, Connie W. and Gerber, Elizabeth M.",10.1145/3544548.3581234,2023,"The number of hackathon events worldwide has nearly quadrupled in the last five years. Despite exponential growth across diverse industries and increasing interest across academic disciplines, our integrated understanding of the phenomena of hackathons is limited. We conduct the first multidisciplinary literature review of publications from 1999 to 2022 to understand the conceptualization of the phenomena over time. We find that hackathon research can be categorized into 4 core areas (purpose, format, processes, and outcomes). Research was first driven by a purpose (innovation, learning, and collaboration), followed by an examination of how formats adjust to purpose to influence what happens (processes) and what is produced (outcomes), and critical reviews of the hackathon phenomena. We contribute a unifying framework with these four core areas to inform future directions of hackathon research and practice, as well as a discussion of the need for longitudinal and multidisciplinary research of hackathons."
Understanding Collaborative Practices and Tools of Professional UX Practitioners in Software Organizations,"Feng, K. J. Kevin and Li, Tony W and Zhang, Amy X.",10.1145/3544548.3581273,2023,"User experience (UX) has undergone a revolution in collaborative practices, due to tools that enable quick feedback and continuous collaboration with a varied team across a design’s lifecycle. However, it is unclear how this shift in collaboration has been received in professional UX practice, and whether new pain points have arisen. To this end, we conducted a survey (N = 114) with UX practitioners at software organizations based in the U.S. to better understand their collaborative practices and tools used throughout the design process. We found that while an increase in collaborative activity enhanced many aspects of UX work, some long-standing challenges—such as handing off designs to developers—still persist. Moreover, we observed new challenges emerging from activities enabled by collaborative tools such as design system management. Based on our findings, we discuss how UX practices can improve collaboration moving forward and provide concrete design implications for collaborative UX tools."
"On Selective, Mutable and Dialogic XAI: a Review of What Users Say about Different Types of Interactive Explanations","Bertrand, Astrid and Viard, Tiphaine and Belloum, Rafik and Eagan, James R. and Maxwell, Winston",10.1145/3544548.3581314,2023,"Explainability (XAI) has matured in recent years to provide more human-centered explanations of AI-based decision systems. While static explanations remain predominant, interactive XAI has gathered momentum to support the human cognitive process of explaining. However, the evidence regarding the benefits of interactive explanations is unclear. In this paper, we map existing findings by conducting a detailed scoping review of 48 empirical studies in which interactive explanations are evaluated with human users. We also create a classification of interactive techniques specific to XAI and group the resulting categories according to their role in the cognitive process of explanation: ""selective"", ""mutable"" or ""dialogic"". We identify the effects of interactivity on several user-based metrics. We find that interactive explanations improve perceived usefulness and performance of the human+AI team but take longer. We highlight conflicting results regarding cognitive load and overconfidence. Lastly, we describe underexplored areas including measuring curiosity or learning or perturbing outcomes."
Literature Reviews in HCI: A Review of Reviews,"Stefanidi, Evropi and Bentvelzen, Marit and Wo\'{z}niak, Pawe\l{} W. and Kosch, Thomas and Wo\'{z}niak, Miko\l{}aj P. and Mildner, Thomas and Schneegass, Stefan and M\""{u}ller, Heiko and Niess, Jasmin",10.1145/3544548.3581332,2023,"This paper analyses Human-Computer Interaction (HCI) literature reviews to provide a clear conceptual basis for authors, reviewers, and readers. HCI is multidisciplinary and various types of literature reviews exist, from systematic to critical reviews in the style of essays. Yet, there is insufficient consensus of what to expect of literature reviews in HCI. Thus, a shared understanding of literature reviews and clear terminology is needed to plan, evaluate, and use literature reviews, and to further improve review methodology. We analysed 189 literature reviews published at all SIGCHI conferences and ACM Transactions on Computer-Human Interaction (TOCHI) up until August 2022. We report on the main dimensions of variation: (i) contribution types and topics; and (ii) structure and methodologies applied. We identify gaps and trends to inform future meta work in HCI and provide a starting point on how to move towards a more comprehensive terminology system of literature reviews in HCI."
Algorithmic Power or Punishment: Information Worker Perspectives on Passive Sensing Enabled AI Phenotyping of Performance and Wellbeing,"Das Swain, Vedant and Gao, Lan and Wood, William A and Matli, Srikruthi C and Abowd, Gregory D. and De Choudhury, Munmun",10.1145/3544548.3581376,2023,"We are witnessing an emergence in Passive Sensing enabled AI (PSAI) to provide dynamic insights for performance and wellbeing of information workers. Hybrid work paradigms have simultaneously created new opportunities for PSAI, but have also fostered anxieties of misuse and privacy intrusions within a power asymmetry. At this juncture, it is unclear if those who are sensed can find these systems acceptable. We conducted scenario-based interviews of 28 information workers to highlight their perspectives as data subjects in PSAI. We unpack their expectations using the Contextual Integrity framework of privacy and information gathering. Participants described appropriateness of PSAI based on its impact on job consequences, work-life boundaries, and preservation of flexibility. They perceived that PSAI inferences could be shared with selected stakeholders if they could negotiate the algorithmic inferences. Our findings help envision worker-centric approaches to implementing PSAI as an empowering tool in the future of work."
Aspirations and Practice of ML Model Documentation: Moving the Needle with Nudging and Traceability,"Bhat, Avinash and Coursey, Austin and Hu, Grace and Li, Sixian and Nahar, Nadia and Zhou, Shurui and K\""{a}stner, Christian and Guo, Jin L.C.",10.1145/3544548.3581518,2023,"The documentation practice for machine-learned (ML) models often falls short of established practices for traditional software, which impedes model accountability and inadvertently abets inappropriate or misuse of models. Recently, model cards, a proposal for model documentation, have attracted notable attention, but their impact on the actual practice is unclear. In this work, we systematically study the model documentation in the field and investigate how to encourage more responsible and accountable documentation practice. Our analysis of publicly available model cards reveals a substantial gap between the proposal and the practice. We then design a tool named DocML aiming to (1) nudge the data scientists to comply with the model cards proposal during the model development, especially the sections related to ethics, and (2) assess and manage the documentation quality. A lab study reveals the benefit of our tool towards long-term documentation quality and accountability."
Mental Wellbeing at Work: Perspectives of Software Engineers,"Wong, Novia and Jackson, Victoria and Van Der Hoek, Andr\'{e} and Ahmed, Iftekhar and Schueller, Stephen M. and Reddy, Madhu",10.1145/3544548.3581528,2023,"Software engineers exhibit higher burnout and suicide rates compared to many other information workers. Consequently, mental wellbeing is a growing concern to technology organizations. To better understand the challenges of supporting mental wellbeing in the context of the work of software engineering, we conducted 14 interviews with software engineers. We examine the different aspects of their lived experiences with mental wellbeing at work, their strategies for managing mental wellbeing, the challenges they face in using these strategies, and recommendations they have for mental wellbeing technologies. We contribute to the HCI literature by discussing how mental wellbeing should be considered within the context of work across individual, team, and organization levels, and highlight the need for integrating mental wellbeing into the technologies employees use at work."
Heterogeneous Graph Neural Networks for Software Effort Estimation,"Phan, Hung and Jannesari, Ali",10.1145/3544902.3546248,2022,"Background. Software effort can be measured by story point [35]. Story point estimation is important in software projects’ planning. Current approaches for automatically estimating story points focus on applying pre-trained embedding models and deep learning for text regression to solve this problem. These approaches require expensive embedding models and confront challenges that the sequence of text might not be an efficient representation for software issues which can be the combination of text and code. Aims. We propose HeteroSP, a tool for estimating story points from textual input of Agile software project issues. We select GPT2SP [12] and Deep-SE [8] as the baselines for comparison. Method. First, from the analysis of the story point dataset [8], we conclude that software issues are actually a mixture of natural language sentences with quoted code snippets and have problems related to large-size vocabulary. Second, we provide a module to normalize the input text including words and code tokens of the software issues. Third, we design an algorithm to convert an input software issue to a graph with different types of nodes and edges. Fourth, we construct a heterogeneous graph neural networks model with the support of fastText [6] for constructing initial node embedding to learn and predict the story points of new issues. Results. We did the comparison over three scenarios of estimation, including within project, cross-project within the repository, and cross-project cross repository with our baseline approaches. We achieve the average Mean Absolute Error (MAE) as 2.38, 2.61, and 2.63 for three scenarios. We outperform GPT2SP in 2/3 of the scenarios while outperforming Deep-SE in the most challenging scenario with significantly less amount of running time. We also compare our approaches with different homogeneous graph neural network models and the results show that the heterogeneous graph neural networks model outperforms the homogeneous models in story point estimation. For time performance, we achieve about 570 seconds as the time performance in both three processes: node embedding initialization, model construction, and story point estimation. HeterpSP’s artifacts are available at [22]. Conclusion. HeteroSP, a heterogeneous graph neural networks model for story point estimation, achieved good accuracy and running time."
A Preliminary Study of Bots Usage in Open Source Community,"Wu, Xiaojun and Gao, Anze and Zhang, Yang and Wang, Tao and Tang, Yi",10.1145/3545258.3545284,2022,"Bots are seen as a promising approach in software development, which help to deal with the ever-increasing complexity of modern software engineering and development. The number of bots in open source community, such as GitHub, has expanded substantially over the last three years. Due to its increasing popularity, it is essential to characterize the current usage of bots in practices. In this paper, we present an empirical study of bots usage in GitHub community. By analyzing 7,399 projects from GitHub, we find that 4,148 (56%) projects have used bots. Through automatic identification and manual detection, we collect a total of 196 bots. We then analyze and classify them into 4 categories and 14 topics. Finally, we discuss some raised implications for bots in current GitHub community."
"Experience Report: A Distributed, Hybrid Course to Teach Global Software Engineering Course During a Global Pandemic","Titze, Julian and Brockmann, Patricia and Marutschke, Daniel Moritz and Kryssanov, Victor",10.1145/3545862.3545872,2022,"In an age of geographically distributed software development teams, international communication skills are becoming ever more important to IT students. During the Covid-19 pandemic, contact and travel restrictions have made it almost impossible for students to gain international experience by spending a semester in a foreign country. One possible solution is to conduct distributed courses in software engineering as a virtual cooperation between two universities. Experiences gained during a hybrid, distributed course on global software engineering are presented. The challenges encountered when coordinating a course that takes place simultaneously in two countries are reported. Feedback from students and professors are discussed and recommendations for future work are derived from these lessons learned."
Students' Perceptions on Engaging Database Domains and Structures,"Miedema, Daphne and Taipalus, Toni and Aivaloglou, Efthimia",10.1145/3545945.3569727,2023,"Several educational studies have argued for the contextualization of assignments, i.e., for providing a context or a story instead of an abstract or symbolic problem statement. Such contextualization may have beneficial effects such as higher student engagement and lower dropout rates. In the domain of database education, textbooks and educators typically provide an example database for context. These are then used to introduce key concepts related to database design, and to illustrate querying. However, it remains unstudied what kinds of database contexts are engaging for novices. In this paper, we study which aspects of database domain and complexity students find engaging through student reflections on a database creation assignment. We identify six factors regarding engaging domains, and five factors for engaging complexity. The main factor for domain-related engagement was Personal interest, the main factor for complexity engagement was Matching information requirements. Our findings can help database educators and book authors to design engaging exercise databases targeted for novices."
Altered States of Consciousness in Human-Computer Interaction: A Review,"Jung, Sangwon and Buruk, O\u{g}uz 'Oz and Hamari, Juho",10.1145/3546155.3546667,2022,"There has been increasing interest shown in experiences such as lucid dreams, hallucinations, or awe that arise in HCI. Altered States of Consciousness (ASC) is the umbrella term for these experiences, yet it has been subject to fragmented study, and design knowledge to help individuals working on technology-driven ASCs is lacking. This paper investigates HCI studies involving ASC artefacts through a scoping review. The findings relate to (1) ASC induction methods, (2) ASC experiences through artefacts, (3) ASC artefacts, and (4) the technology of ASC artefacts. The returned literature shows that HCI studies have mainly explored psychologically induced ASCs, and XR technologies and embodied interaction are widely used in ASC research. Meanwhile, physical artefact design including active body movements and the integration of games and play approaches featured as prospective directions. These results will contribute to the knowledge of those studying and designing ASC artefacts."
Feature selection based on intuitionistic hesitant fuzzy regularized LASSO regression,"Zhang, Yijin and Huang, Jie and Luo, Miao and Tu, Shengxia",10.1145/3547578.3547607,2022,"Excellent feature selection methods can reduce the data dimensionality and improve the efficiency of machine learning tasks. Logistic regression model is one of the models that are widely used for feature selection. In this paper, we propose a regularized LASSO logistic regression model by intuitionistic hesitant fuzzy correlation coefficients, which introduces fuzzy information into the logistic regression model to further enhance its feature selection capability. We design experiments to verify the effectiveness of the feature selection method proposed in this paper. The experimental results show that the method can perform feature selection effectively, and the selected variables can complete the classification task accurately."
HyperDbg: Reinventing Hardware-Assisted Debugging,"Karvandi, Mohammad Sina and Gholamrezaei, MohammadHosein and Khalaj Monfared, Saleh and Meghdadizanjani, Soroush and Abbassi, Behrooz and Amini, Ali and Mortazavi, Reza and Gorgin, Saeid and Rahmati, Dara and Schwarz, Michael",10.1145/3548606.3560649,2022,"Software analysis, debugging, and reverse engineering have a crucial impact in today's software industry. Efficient and stealthy debuggers are especially relevant for malware analysis. However, existing debugging platforms fail to address a transparent, effective, and high-performance low-level debugger due to their detectable fingerprints, complexity, and implementation restrictions. In this paper,footnote[2]For the extended version of this paper which includes additional appendices, refer to: https://arxiv.org/abs/2207.05676 . we present a new hypervisor-assisted debugger for high-performance and stealthy debugging of user and kernel applications. To accomplish this, HyperDbg relies on state-of-the-art hardware features available in today's CPUs, such as VT-x and Extended Page Table (EPT). In contrast to other widely used existing debuggers, we design HyperDbg using a custom hypervisor, making it independent of OS functionality or API. We propose hardware-based instruction-level emulation and OS-level API hooking via extended page tables to increase the stealthiness. Our results of the dynamic analysis of 10,853 malware samples show that HyperDbg 's stealthiness allows debugging on average 22% and 26% more samples thanWinDbg andx64dbg, respectively. Moreover, in contrast to existing debuggers, HyperDbg is not detected by any of the 13 tested packers and protectors. We improve the performance over other debuggers by deploying a VMX-compatible script engine, eliminating unnecessary context switches. Our experiment on three concrete debugging scenarios shows that compared toWinDbg as the only kernel debugger, HyperDbg performs step-in, conditional breaks, and syscall recording, 2.98x, 1319x, and 2018x faster, respectively. We finally show real-world applications, such as a 0-day analysis, structure reconstruction for reverse engineering, software performance analysis, and code-coverage analysis."
Earthwork Optimal Allocation Model of Road Engineering Based on BIM Technology,"Yu, Lingfeng",10.1145/3548608.3559231,2022,"Earthwork adjustment plays an important role in the construction of the whole project and the impact on the environment. It is important to formulate earthwork allocation scheme on the basis of accurate earthwork calculation. We use professional equipment to collect the electronic initial topographic map, and then use it to calculate the design elevation to generate the electronic design topographic map. The earthwork volume and centre of gravity, the unit transportation cost, and the earthwork allocation scheme with the minimum transportation cost can be finally calculated. This paper presents a model to establish the optimal allocation model of road engineering earthwork based on BIM Technology, and a case is used to verify the model is economical, scientific and reasonable."
On the application of machine learning models to assess and predict software reusability,"Yeow, Matthew Yit Hang and Chong, Chun Yong and Lim, Mei Kuan",10.1145/3549034.3561177,2022,"Software reuse has proven to be an effective strategy for developers to significantly increase software quality, reduce costs and increase the effectiveness of software development. Research in software reuse typically addresses two main hurdles: reduce the time and effort required to identify reusable candidates, and avoid selecting low-quality software components that may lead to higher cost of development (i.e., solving bugs, errors, refactoring). Inherently, human judgment falls short in the aspect of reliability and effectiveness. Hence this paper investigates the applicability of Machine Learning (ML) algorithms in assessing software reuse. We collected more than 32k open-source projects and employed GitHub fork as the ground truth to its reuse. We developed ML classification pipelines based on both internal and external software metrics to perform software reuse prediction. Our best-performing ML classification model achieved an accuracy of 86%, outperforming existing research in prediction performance and data coverage. Subsequently, we leverage our results by identifying key software characteristics that make software highly reusable. Our results show that size-related metrics (i.e., number of setters, methods, attributes) are the most impactful in contributing to the reuse of the software."
A Systematic Survey on COVID 19 Detection and Diagnosis by Utilizing Deep Learning Techniques and Modalities of Radiology,"Agrawal, Shrishtee and Singh, Abhishek and Tiwari, Abhishek and Mishra, Anushri and Tripathi, Abhinandan",10.1145/3549206.3549283,2022,"One of the most difficult aspects of the present COVID19 pandemic is early identification and diagnosis of COVID19, as well as exact segregation of non-COVID19 individuals at low cost and the sickness is in its early stages. Despite their widespread use in diagnostic centres, diagnostic approaches based solely on radiological imaging have flaws given the disease's novelty. As a result, to evaluate radiological pictures, healthcare practitioners and computer scientists frequently use machine learning and deep learning models. Based on a search strategy, from November 2019 to July 2020, researchers scanned the three different databases of Scopus, PubMed, and Web of Science for this study. Machine learning and deep learning are well-established artificial intelligence domains for data mining, analysis, and pattern recognition. Deep learning in which data is passed through many layers and automatically learning the composition of each layer from large dataset and it enables a new way that evaluates the complete image without human guidance to discern which insights are valuable, with applications ranging from object detection to medical image. Deep learning with CNN may have a significant effect on the automatic recognition and extraction of crucial features from X-ray and CT Scan images related to Covid19 analysis. According to the results, models based on deep learning possess amazing abilities to offer a precise and systematic system for detecting and diagnosing COVID19. In the field of COVID19 radiological imaging, deep learning software decreases false positive and false negative errors in the identification and diagnosis of the disease. It is providing a once-in-a-lifetime opportunity to provide patients with quick, inexpensive, and safe diagnostic services while also reducing the epidemic's impact on nursing and medical staff."
A refined model of ill-definedness in project-based learning,"Rump, Arthur and Zaytsev, Vadim",10.1145/3550356.3556505,2022,"Project-based courses are crucial to gain practically relevant knowledge in modelling and programming education. However, they fall into the ""ill-defined"" domain: there are many possible solutions; the quality of a deliverable is subjective and not formally assessable; reaching the goals means designing new artefacts and analysing new information; and the problem cannot always be divided into independent tasks. In this paper, we refine the existing two-dimensional (verifiability and solution space) classification of ill-defined classes of problems, contemplate methods and approaches for assessment of projects, and apply the model to analyse two study units of two different computer science programmes."
Low-Resources Project-Specific Code Summarization,"Xie, Rui and Hu, Tianxiang and Ye, Wei and Zhang, Shikun",10.1145/3551349.3556909,2023,"Code summarization generates brief natural language descriptions of source code pieces, which can assist developers in understanding code and reduce documentation workload. Recent neural models on code summarization are trained and evaluated on large-scale multi-project datasets consisting of independent code-summary pairs. Despite the technical advances, their effectiveness on a specific project is rarely explored. In practical scenarios, however, developers are more concerned with generating high-quality summaries for their working projects. And these projects may not maintain sufficient documentation, hence having few historical code-summary pairs. To this end, we investigate low-resource project-specific code summarization, a novel task more consistent with the developers’ requirements. To better characterize project-specific knowledge with limited training samples, we propose a meta transfer learning method by incorporating a lightweight fine-tuning mechanism into a meta-learning framework. Experimental results on nine real-world projects verify the superiority of our method over alternative ones and reveal how the project-specific knowledge is learned."
ASTOR: An Approach to Identify Security Code Reviews,"Paul, Rajshakhar",10.1145/3551349.3559509,2023,"During code reviews, software developers often raise security concerns if they find any. Ignoring such concerns can bring a severe impact on the performance of a software product. This risk can be reduced if we can automatically identify such code reviews that trigger security concerns so that we can perform additional scrutiny from the security experts. Therefore, the objective of this study is to develop an automated tool to identify code reviews that trigger security concerns. With this goal, I developed an approach named ASTOR, where I combine two separate deep learning-based classifiers– (i) using code review comments and (ii) using the corresponding code context, and make an ensemble using Logistic Regression. Based on stratified ten-fold cross-validation, the best ensemble model achieves the F1-score of 79.8% with an accuracy of 88.4% to automatically identify code reviews that raise security concerns."
End-to-End Rationale Reconstruction,"Dhaouadi, Mouna and Oakes, Bentley James and Famelis, Michalis",10.1145/3551349.3559547,2023,"The logic behind design decisions, called design rationale, is very valuable. In the past, researchers have tried to automatically extract and exploit this information, but prior techniques are only applicable to specific contexts and there is insufficient progress on an end-to-end rationale information extraction pipeline. Here we outline a path towards such a pipeline that leverages several Machine Learning (ML) and Natural Language Processing (NLP) techniques. Our proposed context-independent approach, called Kantara, produces a knowledge graph representation of decisions and of their rationales, which considers their historical evolution and traceability. We also propose validation mechanisms to ensure the correctness of the extracted information and the coherence of the development process. We conducted a preliminary evaluation of our proposed approach on a small example sourced from the Linux Kernel, which shows promising results."
Identification and Mitigation of Toxic Communications Among Open Source Software Developers,"Sarker, Jaydeb",10.1145/3551349.3559570,2023,"Toxic and unhealthy conversations during the developer’s communication may reduce the professional harmony and productivity of Free and Open Source Software (FOSS) projects. For example, toxic code review comments may raise pushback from an author to complete suggested changes. A toxic communication with another person may hamper future communication and collaboration. Research also suggests that toxicity disproportionately impacts newcomers, women, and other participants from marginalized groups. Therefore, toxicity is a barrier to promote diversity, equity, and inclusion. Since the occurrence of toxic communications is not uncommon among FOSS communities and such communications may have serious repercussions, the primary objective of my proposed dissertation is to automatically identify and mitigate toxicity during developers’ textual interactions. On this goal, I aim to: i) build an automated toxicity detector for Software Engineering (SE) domain, ii) identify the notion of toxicity across demographics, and iii) analyze the impacts of toxicity on the outcomes of Open Source Software (OSS) projects."
From Classroom to Online Education – An Educators Insights,"Tedeschi, Mary",10.1145/3551902.3551977,2023,"Face to face activities at universities became difficult in the spring of 2020 with the worldwide outbreak of the SARS virus version COVID19. The initial information was that all classes were to be done online for a short period only, and we believed that we would return to the classroom in two weeks. But within a short period, all face-to-face classes were cancelled and replaced with remote lectures utilizing online Learning Management Systems (LMS) and video conferencing. For many colleges, most classes are still conducted this way in the spring of 2021. The large scale of online education has exposed a number of problems and challenges that although somewhat known have taken on greater significance. This paper describes online teaching models, learning styles, engagement, and interaction models to create a foundation for a set of patterns that capture ways of dealing with these problems – solutions that have been developed and applied for online education even before the pandemic hit but are now being more broadly used. The paper was written over the course of 3 semesters, with examples from teaching at 3 different colleges, depicting the transition from face-to-face teaching coming to an abrupt halt and the rapid transition to pure on-line mode. To support remote activities, the universities contracted some sort of software. St John's University contracted Cisco WebEx, which was available to faculty and students. The City University of New York, (CUNY) system has unique software within each College. At The College of Technology, we used Collaborate Ultra, a tool part of the LMS (Blackboard). Baruch College offered this tool and Zoom was offered with a license from the school. During the second semester, zoom licensing was enhanced and offered throughout the CUNY system. Most of the patterns in this paper depend on an LMS system being in place, as they cover practical topics like course design and how to deal with testing and exams, but also move into areas of student engagement and motivation. Definitions of user experience may be vague and conflicting, as each student and school is unique. The motivation of this paper was the pandemic; however, the findings show engagement as being positive, and yet still exploratory. We engage with technology because it allows us to achieve our purposes. This work contributes to the understanding of how we can apply patterns for online education and shows the start of a whole new pattern language that can benefit educators as well as students as we move forward with a new educational model."
Embodiment in interactive installations: results from a systematic literature review,"Duarte, Emanuel Felipe and Mendoza, Yusseli Lizeth M\'{e}ndez and de Queiroz, Maria J\^{e}sca Nobre and Baranauskas, M. Cec\'{\i}lia C.",10.1145/3554364.3559118,2022,"Ubiquitous and pervasive technologies, alongside the expanding role of the computer in our lives, present new dimensions to the concept of embodiment within the Human-Computer Interaction (HCI) field. Interactive installations, in particular, afford the technological and conceptual exploration of embodiment due to a high degree of creative freedom and more open objectives, making it a promising context to be investigated. In this paper, we present a systematic literature review on approaches to embodiment in the context of interactive installations. We searched four major digital libraries and screened a total of 3245 studies following a protocol described in the paper. The final selection consists of 67 entries. Our analysis shows a multitude of theoretical and practical approaches to embodiment in interactive installations, but also significant compatibility and combined use. We present detailed summaries of and discussion on how embodiment is conceptualized, put into practice, and what technologies are involved, revealing new perspectives and possibilities for investigating embodiment in HCI."
Use Cases for Software Development Analytics: A Case Study,"Rique, Thiago and Dantas, Emanuel and Perkusich, Mirko and Gorg\^{o}nio, Kyller and Almeida, Hyggo and Perkusich, Angelo",10.1145/3555228.3555239,2022,"Context Software engineering activities provide practitioners with large volumes of data that software analytics tools can use for many purposes, including defect prediction and effort estimation. However, the adoption of such tools depends on the information they provide and the real needs of practitioners. While existing research has focused on what developers need, the needs of managers are not well understood. Aims This study provides an in-depth analysis of the information needs of software practitioners from one organization that performs research, development, and innovation projects with industry partners. Understanding these practitioners’ needs enables the development of better analytics solutions to support managerial decision-making. Method We interviewed practitioners in leadership positions and analyzed the collected data using Grounded Theory coding techniques, i.e., open and selective coding. Results We identified 19 software analytics use cases and classified them into four dimensions: quality, people, project management, and knowledge management. We also elicited several indicators to meet the identified use cases and captured key aspects concerning the organization’s analytics scenario. Conclusions Although our results are particularly relevant to organizations similar to the one in which we conducted the study, they aim to serve as input for implementing new analytics solutions by practitioners and researchers in general."
Towards a Recommender System-based Process for Managing Risks in Scrum Projects,"Sousa Neto, Ademar and Ramos, Felipe and Albuquerque, Danyllo and Dantas, Emanuel and Perkusich, Mirko and Almeida, Hyggo and Perkusich, Angelo",10.1145/3555776.3577748,2023,"Agile Software Development (ASD) implicitly manages risks through, for example, its short development cycles (i.e., iterations). The absence of explicit risk management activities in ASD might be problematic since this approach cannot handle all types of risks, might cause risks (e.g., technical debt), and does not promote knowledge reuse throughout an organization. Thus, there is a need to bring discipline to agile risk management. This study focuses on bringing such discipline to organizations that conduct multiple projects to develop software products using ASD, specifically, the Scrum framework, which is the most popular way of adopting ASD. For this purpose, we developed a novel solution that was articulated in partnership with an industry partner. It is a process to complement the Scrum framework to use a recommender system that recommends risks and response plans for a target project, given the risks registered for similar projects in an organization's risk memory (i.e., database). We evaluated the feasibility of the proposed recommender system solution using pre-collected datasets from 17 projects from our industry partner. Since we used the KNN algorithm, we focused on finding the best configuration of k (i.e., the number of neighbors) and the similarity measure. As a result, the configuration with the best results had k = 6 (i.e., six neighbors) and used the Manhattan similarity measure, achieving precision = 45%; recall = 90%; and F1-score = 58%. The results show that the proposed recommender system can assist Scrum Teams in identifying risks and response plans, and it is promising to aid decision-making in Scrum-based projects. Thus, we concluded that our proposed recommender system-based risk management process is promising for helping Scrum Teams address risks more efficiently."
TechSpaces: Identifying and Clustering Popular Programming Technologies,"Miranda, Guilherme and Montandon, Jo\~{a}o Eduardo and Valente, Marco Tulio",10.1145/3559712.3559715,2022,"Background: Software ecosystems are becoming increasingly complex and large. Therefore, discovering and selecting the right libraries and frameworks for use in a project is becoming a challenging task. Existing commercial services that support this task rely on annual surveys with developers to provide a landscape of the most popular technologies in a given ecosystem. Aims: In this paper, we outline a semi-automated technique for this purpose, which we call TechSpaces. Method: Our proposal relies on community detection and well-known NLP algorithms to automatically extract groups of related technologies, using as primary data source tags associated with Stack Overflow questions. Results: We describe the first results of using our technique to identify popular and inter-related technologies in five programming language ecosystems. Evaluation: We compare our technique against two other tools in the literature. Conclusions: The proposed technique shows potential to assist IT professionals in taking technical decisions supported by crowd knowledge. However, further improvements are needed to make it a viable choice. For instance, we envision the usage of other data sources (e.g., GitHub and Wikipedia) can contribute to improve the accuracy and expressiveness of our graph representations."
How do Trivial Refactorings Affect Classification Prediction Models?,"Pinheiro, Darwin and Bezerra, Carla Ilane Moreira and Uchoa, Anderson",10.1145/3559712.3559720,2022,"Refactoring is defined as a transformation that changes the internal structure of the source code without changing the external behavior. Keeping the external behavior means that after applying the refactoring activity, the software must produce the same output as before the activity. The refactoring activity can bring several benefits, such as: removing code with low structural quality, avoiding or reducing technical debt, improving code maintainability, reuse or readability. In this way, the benefits extend to internal and external quality attributes. The literature on software refactoring suggests carrying out studies that invest in improving automated solutions for detecting and correcting refactoring. Furthermore, few studies investigate the influence that a less complex type of refactoring can have on predicting more complex refactorings. This paper investigates how less complex (trivial) refactorings affect the prediction of more complex (non-trivial) refactorings. To do this, we classify refactorings based on their triviality, extract metrics from the code, contextualize the data and train machine learning algorithms to investigate the effect caused. Our results suggest that: (i) machine learning with tree-based models (Random Forest and Decision Tree) performed very well when trained with code metrics to detect refactorings; (ii) separating trivial from non-trivial refactorings into different classes resulted in a more efficient model, indicative of improving the accuracy of automated solutions based on machine learning; and, (iii) using balancing techniques that increase or decrease samples randomly is not the best strategy to improve datasets composed of code metrics."
Seeding industry knowledge in computer science education,"Lal, Manoj Kumar",10.1145/3561833.3564559,2022,Industry knowledge forms the subject matter of the software being built in the IT industry. Academia faces challenges in seeding industry knowledge in its curriculum resulting in industry-academia gap. Knowledge Driven Development (KDD) is a knowledge management framework comprising of two complimenting components; Domain Knowledge Framework (DKF) and Atomic Knowledge Model (AKM). DKF addresses the industry knowledge gap via a domain agnostic common structure. The common structure is an experience-based empirical formulae bringing science to the art of learning industry knowledge. This paper explains the DKF proposition that claims to increase the employability of students significantly.
Temporal Tensions in Digital Story Mapping for Housing Justice: Rethinking Time and Technology in Community-Based Design,"Halperin, Brett A. and McElroy, Erin",10.1145/3563657.3596088,2023,"In this paper, we explore temporal and technological conjunctures of community-based design responsiveness based upon our experiences of making an interactive digital story map amid housing crises. Through reflexive ethnographic work with the Anti-Eviction Mapping Project, we trace three intertwined genres of temporality in our community-based design process: slow design with care; rapid design in response to the urgency of crises; and disruptive design that seeps in from Silicon Valley, proving to “move fast and break things.” We assess temporal tensions and contradictions in an evolving data landscape by examining struggles over our pace of practice, time-based media lifecycles, and tooling trade-offs. Theorizing community-based design dilemmas, we contemplate ways of mindfully designing with time, making the past present via digital storytelling, and reimagining technological relations over time."
Semantic Recognition and Representation of Management Business Processes,"Zhai, Mengyue and Yu, Jianjun",10.1145/3565291.3565325,2022,"Within a company or research institute, management business process specifications are usually recorded in the form of unstructured textual information. So far, it is a time-consuming and costly operation to obtain business processes from the text data of process descriptions. To solve this problem, a system for automatic extraction and management of business processes is designed. Based on various management of business normative text data, the management business process elements and their semantic relationships are identified by using named entity recognition, relation extraction and other technologies. The results show that the system can reduce the workload of manually extracting out management business processes to a certain extent."
Design and Implementation of Survey and Design Enterprise File Sorting System,"Li, Xiaoxiu and Wei, Junyong and Deng, Xiaoxia and Shi, Xiang and Liang, Yanqiao and Wei, Yanling",10.1145/3565387.3565422,2022,"In recent years, the scale of survey and design enterprises has grown and the number of projects has also increased. The files generated during the survey and design process also increased. Files for civil engineering projects must be archived. In most cases, the files for these projects are directly stored in the system database without standardized sorting. When employees use a project file, they need to search in a messy and unhierarchical file directory, so that the search efficiency is low. Therefore, we designed and implemented a file sorting system for survey and design enterprises. In the paper, we introduce the system design and system realization of the file sorting system in detail. The system is based on B/S architecture and uses the Java programming language to integrate the rules of file sorting into the system to improve the quality of file sorting and ensure the authenticity, availability, security and integrity of files. After systematic processing, the files are uniformly coded and arranged, and employees can find a certain civil engineering project file more clearly."
Illustrating Robot Movements,"Johansen, Stine S. and Donovan, Jared W. and Rittenbruch, Markus",10.1145/3568162.3576956,2023,"In efforts to disseminate research on human-robot interaction, many researchers use illustrations in the form of sketches, photographs, and 3D models of robot movements. These illustrations are not only useful for building on the research, but they also capture ways researchers think about robot movement. In this paper, we review papers from the ACM/IEEE International Conference on Human-Robot Interaction in which such illustrations are presented supplementary to the text. We analyse a total of 181 illustrations from 137 papers to understand the diverse ways in which robot movements are illustrated as well as how each style supports and limits information about the movements. We identify 10 basic styles that are used. Based on a visual analysis of these styles, we provide a detailed examination of each. This paper contributes with an overview that can be used to support future dissemination within the HRI research community. We present four aspects to consider for future illustrations and a discussion on how our findings could be utilised in early design processes."
Can Technology Replace the Teachers’ Role in Higher Education Settings? A Systematic Literature Review,"Rifah, Lailatul and Zamahsari, Gamal Kusuma",10.1145/3568231.3568266,2023,"As technologies continue to advance and become more complex, they will have significant ramifications for formal education contexts. It is mentioned that digital technologies will remain in our virtual and physical classrooms for the foreseeable future. Moreover, when it comes to the direct contact between students and their teachers, it is challenging to simulate a teacher’s ability to read and respond to students’ emotions. Therefore, the purpose of this study is to obtain data from a variety of sources to determine whether technology can replace teachers using PRISMA Protocol as the research methodology with 39 previous studies from the SCOPUS journal. According to the findings of this study, it can be concluded that technology cannot replace the rules and positions of teachers, particularly when it comes to psychological and value issues, because ICTs cannot make every potential decision about other people."
Children's Fundamental Rights in Human-Robot Interaction Research: A Systematic Review,"DiPaola, Daniella and Charisi, Vicky and Breazeal, Cynthia and Sabanovic, Selma",10.1145/3568294.3580148,2023,"Citizens and policy institutions increasingly express their concerns regarding the emerging challenges in the context of Artificial Intelligence (AI) and have concrete demands for the protection of human rights. In parallel, studies in the field of AI and Human-Robot Interaction (HRI) indicate the impact of social robots on children's development. We conducted a systematic review based on UNICEF's AI Policy Guidance to map the landscape of research on social robots and children's rights. We used the PRISMA method and identified N=37 papers that address one of the rights, which we then annotated to indicate tendencies and areas of alignment and misalignment with the UNICEF guidance. Our findings reveal that although the field of HRI is looking at specific rights, with a focus on inclusion, some of the rights have been under-researched. Furthermore, we observed a misalignment between HRI and UNICEF regarding the terminology. With this paper, we hope to bring awareness to the field of HRI regarding children's rights and to highlight directions for alignment among research, societal needs, and policy."
An improved NSGA-II based-on project scheduling principles for workforce scheduling optimization in warehouse,"Vu, Thi-Huong-Giang and Nguyen, Thi-Hong-Anh",10.1145/3568562.3568603,2022,"This paper presents an approach to workforce scheduling optimization in a warehouse, considering multiple constraints such as precedent constraints among tasks, skill constraints between tasks and human resources, and working time constraints of human resources. The expected result is a set of optimized schedules for three objectives: order cycle time, total labor cost, and order on-time rate. By viewing a schedule under the lens of project management, we improve the NSGA-II with a customized initial population creation, and enhanced crossover and mutation operators. The algorithm is then tested against real data from an existing enterprise resource planning (ERP) application, including human, machine resources and tasks. The results show to perform well, thus making the algorithm applicable in the warehouse environment."
Sports Visual Data Analysis with Deep Vision,"Huynh, Minh-Hieu and Bui-Xuan, Bao and Tran, Minh-Triet",10.1145/3568562.3568633,2022,"The sports industry is surrounded by opportunities in all facets, including sports marketing, sports media, education institutions, sponsorship, and so on. They are adopting the newest technology such as computer vision to improve coaching task as well as benefit the sport broadcasting industry. In this paper, we examine action spotting in sports, particularly soccer. We found the SoccerNet challenge, which is a soccer video understanding challenge. SoccerNet provides a dataset for action spotting in soccer of 550 videos of famous leagues in recent years along with the event annotations in 17 classes. Action spotting is a concept that SoccerNet defined as finding the anchor time (or spot) that identifies an event. Inspired by previous successful methods under the action spotting task NetVLAD++ and AudioVid, we propose a multimodal architecture that learns temporal features of both visual and audio representation of video data. Our method achieves an mAP of 56.58%, which rises 3.18% higher than NetVLAD++ and 16.68% higher than AudioVid. Furthermore, we integrate that research into a web-based platform in order to contribute to the annotation industry. This platform consists of a client application written in ReactJS and a backend server. Both of them are deployed on the Google Cloud Platform so that users can experience our platform on any device without complex installation."
ml-Codesmell: A code smell prediction dataset for machine learning approaches,"Nguyen Thanh, Binh and Nguyen N. H., Minh and Le Thi My, Hanh and Nguyen Thanh, Binh",10.1145/3568562.3568643,2022,"In recent years, many studies on detecting code smells in source code have published datasets with limited characteristics, such as the ambiguity of code smell definitions leads to different interpretations for each code smell, the number of samples of the datasets is small, and the features of the datasets are heterogeneous. Therefore, comparing performance between detecting code smell models is challenging, and the datasets are often not reusable in other code smell detection studies. In this work, we propose the ml-Codesmell dataset created by analyzing source code and extracting massive source code metrics with many labelled code smells. The proposed dataset has been used to train and predict code smell using machine learning algorithms. Based on the high confidential F1-score in evaluation, the ml-Codesmell dataset demonstrates a strong correlation between features and labels. Regarding these advantages, the ml-Codesmell dataset is expected to be helpful for studies on detecting code smell using machine learning approaches in software development."
Taking Stock of Concept Inventories in Computing Education: A Systematic Literature Review,"Ali, Murtaza and Ghosh, Sourojit and Rao, Prerna and Dhegaskar, Raveena and Jawort, Sophia and Medler, Alix and Shi, Mengqi and Dasgupta, Sayamindu",10.1145/3568813.3600120,2023,"Background and context. Concept inventories (CIs) are a widely used tool in STEM education that can help instructors identify specific misconceptions students hold about key concepts. Over the past several years, much research has been published contributing to CIs in computer science education. Objectives. This research aims to investigate the state of CIs in computer science education through a systematic literature review built around two research questions: 1) What do we know about CIs for computer science today, and 2) Have researchers been able to effectively tackle the 4 challenges specific to computer science CIs outlined by Taylor et al. in their previous 2014 literature review on this topic? Method. We conducted a systematic literature review focusing on CIs in computer science education. We included research from any country, provided it was published in English between 2004 and 2022. We searched with key terms across four databases (Compendex, Inspec, Education Source, and ERIC) and collated a final list of 65 studies for analysis from 175 initial results. Findings. From these 65 papers, we identified 33 total CIs in computer science, 12 of which are currently validated. Of the 65 studies, the majority used established techniques for choosing concepts, identifying misconceptions, and/or developing questions, but a few (n = 4) reported on successful alternative techniques. We found that although no papers explicitly articulated an objective to tackle the aforementioned challenges, they are a recurring theme in the literature and have been addressed in various ways. Implications. At the time of the previous literature review, there were few computer science CIs. Our findings are promising, suggesting that researchers have found effective ways to tackle Taylor et al.’s challenges and that the number of CIs across computer science is gradually increasing. Future work is warranted regarding non-traditional techniques researchers proposed for developing CIs."
Developing Novice Programmers’ Self-Regulation Skills with Code Replays,"Xie, Benjamin and Lim, Jared Ordona and Pham, Paul K.D. and Li, Min and Ko, Amy J.",10.1145/3568813.3600127,2023,"Learning programming benefits from self-regulation, but novices lack support for developing these skills of cognitive control. To support their development, we designed Code Replayer, an online tool that enables novice programmers to practice programming and then replay their coding process to reflect and identify process improvements. To evaluate the impact of replaying code on self-regulation, we conducted a formative qualitative evaluation with 21 novice programmers who used Code Replayer to practice writing code. We found that after watching code replays, participants more frequently interpreted problem prompts and planned their solutions, two crucial self-regulation behaviors that novices often overlook. We interpret our results by focusing on two focal points in the design of code replays as a programming self-regulation intervention: interpreting pauses in replays and ensuring replays of struggle are more informative and less detrimental."
"The Cause, Concept and Value of Project Sharing——Based on the Example of Overseas Infrastructure Industry","Shen, Zhifeng and Li, Meng",10.1145/3568834.3568872,2023,"In recent years, the phenomenon of project sharing has begun to appear in the national macro-level, industrial micro-level and enterprise micro-level, but few scholars have conducted in-depth academic research on project sharing management (PSM). As a new kind of project management mode, project sharing is different from the existing project management mode, which is driven by the new sharing of economic theory, information technology and digital system environment. Based on the research of relevant theories and project sharing practice at home and abroad, this paper analyzes the deep-seated theoretical origin and practical basis of the rise of PSM, explores the main characteristics and connotation of PSM, defines the concept of PSM, reveals the value of PSM, and then discusses the main problems existing in the practice of PSM with case analysis. The research results provide new ideas and methods for the field of project management, which have important theoretical and practical values."
The Application of Artificial Intelligence in Translation Teaching,"Yang, Chaoying",10.1145/3568923.3568933,2023,"The purpose of this paper is to analyze the concept of AI translation and the current situation of its application in college teaching. Taking the interpretation teaching as an example, this paper introduces the specific methods of applying AI technology in translation teaching from three aspects: pre-class preparation, classroom teaching and after-class practice, and tries to provide a way to change the traditional teaching methods and teaching mode, fully mobilize students’ subjective initiative, strongly stimulate students’ curiosity and their thirst for knowledge, and make students’ have more innovative ability and spirit."
HCI and Digital Twins – A Critical Look: A Literature Review,"Vainionp\""{a}\""{a}, Fanny and Kinnula, Marianne and Kinnula, Atte and Kuutti, Kari and Hosio, Simo",10.1145/3569219.3569376,2022,"Digital twins (DTs) are one form of datafication. They are virtual reflections of physical world entities, of objects and phenomena, and are rapidly becoming an asset for innovation. There is a growing body of literature on DTs in various technology-related fields. A critical thread has emerged within this body, warning on the danger to forget that the digital part is always only a partial representation of real life, and that this partiality is always selective and biased for a specific purpose. It may thus serve some group of stakeholders better than others. We contribute with a literature review on the current understanding and use of the DT concept in the field of HCI. Our results consolidate the current understanding of DTs’ potential in HCI and note the omission of the critical perspective within the reviewed literature. The paper opens discussion of what HCI can bring to DT development and use."
A systematic literature review of clone evolution,"Zhong, Yan and Zhang, Xunhui and Tao, Wang and Zhang, Yanzhi",10.1145/3569966.3570091,2022,"Code clones are identical or nearly similar code fragments often introduced into software systems by programmers with software modification and maintenance. During the evolution of the software system, code clones may experience multiple changes, such as the increase in number, disappearance, location change, etc. These changes increase the difficulty of clone management and possibly introduce bugs into the software, leading to the high price of clone management and maintenance. Therefore, it is necessary to study the clone evolution. In this paper, we summarize the research works in code clone evolution in recent decades. Based on the previous review and survey, we found a total of 47 relevant papers and divided them into five categories with the help of the LDA model. We present our analysis of the current research and discussion about the possible future progress in this paper. The final result of the debate is that we believe the future work will divide into two aspects. On the one hand, developing clone management tools based on the current results become a possible direction; on the other hand, development and improvement may appear in existing tools with more theoretical support due to more knowledge of the evolutionary characteristics of clones."
Trends in Sunspots &amp; Agriculture Stock Prices -&nbsp;Finding Correlations and Predicting Trends Using Machine Learning and Deep Neural Networks,"Liu, Kwan Yeung",10.1145/3569966.3570098,2022,"According to previous research, sunspots and weather conditions have both direct and latent economic impacts, such as human financial activities. The goal of this project was to see if machine learning and deep neural network methods could reveal a link between natural phenomena, specifically sunspots, weather, and agricultural stock price trends. I suggested that some of these natural events could be related to the price trends of individual equities. Using machine learning and deep neural network methods, I analysed at both the general Dow Jones index level and the particular agriculture stock level. Outperforming other models, the LSTM (Long-Short-Term Memory) model produced an MSE (Mean Squared Error) error of 9.91 between the sunspot number and various agricultural price patterns, which was far lower than my hypothesis. The outcome shifts from standard algorithm trading to a completely new aspect, with (space) meteorological factors playing critical roles for the first time. The implications of these results extended far beyond commercial advantages. The findings provided unique proof that not only our commercial world is impacted by space weather, the impact of which can also be digitally recorded and anticipated. This preliminary but effective study established a computer link between space weather and human business behavior, sparking one's vivid imagination of the forces at work."
Role of Digital Healthcare in the Well-being of Elderly People: A Systematic Review,"Nautiyal, Saurabh and Shrivastava, Abhishek and Deka, Chinmoy and Chauhan, Praveen",10.1145/3570211.3570214,2023,"The delivery of quality healthcare services to the growing number of older people is a crucial concern for any country. Digital healthcare technologies may play an essential role in providing high-quality health services. In the case of elderly healthcare, such technologies are helping the elderly with accessible quality healthcare. This study identifies such research work where the elderly population is the primary user of the technologies related to the healthcare domain. We extracted the research articles from three databases: ACM Digital, IEEE and Science Direct, to conduct the systematic review. We used a web application to avoid biases in the research articles selection procedure and performed a quality assessment. The study selection process yielded fifteen research articles published between January 2011 to February 2022. This study investigates the various uses of digital interventions in elderly healthcare and provides a comprehensive overview of demographic information, study characteristics, research areas, significant challenges and future directions. Moreover, this study discusses the implications of the results and provides future recommendations. Hence, we believe this systematic review will be helpful for other researchers working in the field of elderly healthcare and similar research areas."
Deep learning-assisted ADHD diagnosis,"Gao, Runqing and Deng, Kesui and Xie, Miaoyun",10.1145/3570773.3570849,2022,"Attention deficit hyperactivity disorder (ADHD) can have a negative impact on children's development, even into adulthood, so the early diagnosis and screening for ADHD can be an important prerequisite for later intervention. However, the traditional diagnostic methods have limitations in terms of objectivity, convenience and efficiency. With the development of artificial intelligence, deep learning, as an emerging computer technology that can deal with massive data and variables, has gradually been applied to early prediction of ADHD in children and aiding diagnosis. From the traditional diagnostic methods to one based on conventional feature analysis, such as the diagnosis of ADHD in children based on EEG data analysis. With the continuous development of computer technology, the analysis and diagnosis of EEG data based on deep learning, and the combination of deep learning model and computer vision technology have been emerged. Due to the incompleteness of the analysis and diagnosis of unimodal data, the deep learning models of multimodal data can have a strong integrity, which has become a hot spot at present. However, deep learning still has limitations in hardware cost and algorithm selection. In the future, further research is needed in deep learning-assisted diagnosis to continuously optimize the algorithm and accelerate the improvement of ADHD intelligent identification and diagnosis ability."
How Agile Organizations Use Metrics: A Systematic Literature Mapping,"Leal, St\'{e}phanie and Hauck, Jean and Bertan, Monique and Vieira, Gustavo",10.1145/3571473.3571479,2023,"Agile Software Development Methods (ASDMs) have addressed the complexity of managing software development processes through small increments, continuous and frequent delivery, openness to change, and work visibility. These practices have been complemented by the use metrics adapted to ASDMs context, generating a large number and types of metrics. Thus, this secondary study presents a Systematic Literature Mapping to understand how agile organizations use metrics. Seven research questions were defined, and 33 primary studies related to the use of agile metrics were selected. The results indicate that most organizations that use agile metrics use Scrum, develop large size general-purpose software products, desktop, or web applications, in small teams. The metrics are selected by primary studies based on literature search, interviews with the team members or expert groups. The majority of the organizations use Product Quality, Resource and Cost, and Process performance metrics, focusing in the agile team, management and customer. The primary studies results indicate that using agile metrics can positively impact the quality of processes, products, and services."
Teaching software processes from different application domains,"Bezerra, Carla and Coutinho, Emanuel",10.1145/3571473.3571488,2023,"In a current application development scenario in different environments, technologies and contexts, such as IoT, Blockchain, Machine Learning and Cloud Computing, there is a need for particular solutions for domain-specific software development processes. The proper definition of software processes requires understanding the involved teams and organization’s particularities and specialized technical knowledge in Software Engineering. Although it is an essential part of Software Engineering, many university curricula do not dedicate as much effort to teach software processes, focusing more on the basic principles of Software Engineering, such as requirements, architecture and programming languages. Another important aspect of software processes is modeling. The modeling of a software process provides a basis for managing, automating and supporting the software process improvement. In this context, teaching software process modeling becomes challenging, mainly due to the great emphasis on theory and few practices. This work presents an experience report teaching the definition and modeling of software processes in different domains. We applied in the discipline of software processes a practice for defining and modeling processes in various application domains, such as: IoT, cloud, mobile, critical systems, self-adaptive systems and games. The processes were modeled in the Eclipse Process Framework (EPF) Composer tool based on references from the literature for each domain. In the end, we evaluated the process modeling practice with the students. We concluded that the modeling tool and the maturity in the domain are essential for the good performance of the process."
Exploring Technical Debt on IoT Software Projects,"Rios, Nicolli and Sp\'{\i}nola, Rodrigo and Travassos, Guilherme",10.1145/3571473.3571495,2023,"It is common for software projects to incur technical debt (TD) during their development process. It should be no different for IoT software projects, mainly characterized by integrating devices and communication technologies. This work reduces a gap in the technical literature by presenting an investigation of TD in the context of IoT software projects. To this end, it describes how TD has been perceived, identified, and represented when developing two IoT software projects regarding monitoring an individual's oxygenation level (OximetroIoT) and biosafety indicators in classrooms and laboratories (SAFE-UFRJ). By applying a TD management strategy, we identify, monitor, and pay TD items incurred during the development process of those projects. It allowed us to identify 153 TD items grouped into 16 different TD types (three new ones compared with current taxonomies) and 75 effects due to their presence. Overall, the investigated IoT software projects experience TD differently from software projects."
The migration from forced remote work to hybrid work and its impacts on software quality: the case of a multinational company,"De Quadros, Everton Luis Luz and Lisboa, Anielle Severo and Souza, Marilaine Quadros Becker and Prikladnicki, Rafael and Chaves, Marcirio Silveira",10.1145/3571473.3571498,2023,"Context: many tech companies have had to adapt their software development processes to the reality of forced remote work due to COVID-19. Later, many of these companies transitioned to hybrid work, interspersing remote work with face-to-face work. Goal: this article aims to understand the challenges and impacts of this transition to software quality and to create hybrid software development teams. Method: an empirical study was carried out in the technology department of a Brazilian multinational company, using a multivocal literature review combined with a field study with semi-structured interviews and discursive textual analysis. Results: this study identified three main dimensions of the impact of the transition from remote to hybrid model: people, processes, and organization. In each of them, specific items were identified and will be detailed in this article. Conclusions: permanent or hybrid remote work models have enormous challenges and require studies in the specific scenarios of each company. It is necessary to understand these challenges to propose solutions that simultaneously facilitate the work of teams and guarantee the quality of the projects."
A Review of International Models of Computer Science Teacher Education,"Yadav, Aman and Connolly, Cornelia and Berges, Marc and Chytas, Christos and Franklin, Crystal and Hij\'{o}n-Neira, Raquel and Macann, Victoria and Margulieux, Lauren and Ottenbreit-Leftwich, Anne and Warner, Jayce R.",10.1145/3571785.3574123,2022,"Throughout the world, Computer Science Education (CSE) has ex- panded exponentially over the past decade, focused on teaching primary and secondary students computing ideas and tools. To teach all these students computer science (CS), models for teacher preparation range from one and done professional learning work- shops to full certificate and licensure programs. This report provides a landscape of how CS teachers are prepared academically in var- ious countries and makes evidence-based recommendations for how teachers should be educated to develop knowledge and skill to teach computer science. It also discusses how to develop these knowledge systems while promoting instruction that is equitable and centers students in the classroom. We brought together a group of international computer science education scholars who have been engaged in teacher preparation. In addition to what knowl- edge teachers need to teach CS, we also focused on how the field is preparing teachers and the role of computer science in the design of technology tools to achieve goals while mitigating potential societal harms."
Motivation and Strategies for Effective Inclusion of Cloud Solution Provider Certifications in Computing Curricula,"Paterson, James H. and Adams, Joshua and Foster, Derek and Baghban Karimi, Ouldooz and Kazmi, Zain and Lennon, Ruth G. and Nazir, Sajid and Stott, Lee and White, Laurie",10.1145/3571785.3574128,2022,"A series of Working Groups has met at previous ITiCSE conferences to explore ways of incorporating cloud computing into courses and curricula, including mapping industry job skills to knowledge areas and those areas to student learning objectives. The importance of industry-standard learning content and certification, produced by cloud vendors and others, was apparent throughout this work.  This Working Group has focused on the role of certification within cloud computing curricula, from the viewpoints of a range of stakeholders: students, graduates, institutions, vendors and other certification providers; and employers, with the aim to provide insights and recommendations for educators who are considering whether to integrate cloud certifications into their courses. We reviewed the landscape of certifications provided by the most widely recognised cloud vendors, based on publicly available information and the knowledge embedded within the Working Group through the inclusion of vendor representatives in the membership. An overview is provided of the scope of available certifications and their mapping to our knowledge areas and learning outcomes, and of the influence that standards have or should have on learning design. We then explored the perspectives of stakeholders, through surveys of students on courses with a cloud computing element and of employers who have employed graduates of those courses, drawing conclusions on the awareness of certifications and specific vendors within each of those stakeholder groups, and on differences between the groups on the perceived importance of certifications for employability. Finally we explored approaches to integrating certification in academic cloud curricula, and challenges involved in doing so, through thematic analysis of in-depth interviews with a range of educators who have experience of doing so successfully. A set of recommendations for educators is presented, based on the findings of the Working Group's activities."
What’s The Talk on VUI Guidelines? A Meta-Analysis of Guidelines for Voice User Interface Design,"Murad, Christine and Candello, Heloisa and Munteanu, Cosmin",10.1145/3571884.3597129,2023,"Over the past decade, voice user interface (VUI) design has been steadily growing, along with a growing VUI presence in consumer markets. However, there is currently a lack of widely-established guidelines for VUI design. While many sets of VUI guidelines have been proposed, they tend to be developed independently of each other, leading to a lack of consensus on appropriate guidelines for VUI design. This can hinder the wider adoption of practical VUI guidelines. To address this gap, we performed a large-scale meta-analysis of 336 VUI design guidelines that have been proposed in academic literature. Using thematic analysis, we present a unified and synthesized set of 14 guidelines, representing the most universally proposed principles of VUI design as captured by the 336 VUI guidelines identified in academic literature. We hope that this synthesized set can address several of the challenges to the adoption of VUI guidelines in design practice."
The Bot on Speaking Terms: The Effects of Conversation Architecture on Perceptions of Conversational Agents,"Wei, Christina Ziying and Kim, Young-Ho and Kuzminykh, Anastasia",10.1145/3571884.3597139,2023,"Conversational agents mimic natural conversation to interact with users. Since the effectiveness of interactions strongly depends on users’ perception of agents, it is crucial to design agents’ behaviors to provide the intended user perceptions. Research on human-agent and human-human communication suggests that speech specifics are associated with perceptions of communicating parties, but there is a lack of systematic understanding of how speech specifics of agents affect users’ perceptions. To address this gap, we present a framework outlining the relationships between elements of agents’ conversation architecture (dialog strategy, content affectiveness, content style and speech format) and aspects of users’ perception (interaction, ability, sociability and humanness). Synthesized based on literature reviewed from the domains of HCI, NLP and linguistics (n=57), this framework demonstrates both the identified relationships and the areas lacking empirical evidence. We discuss the implications of the framework for conversation design and highlight the inconsistencies with terminology and measurements."
Accessibility Research in Digital Audiovisual Media: What Has Been Achieved and What Should Be Done Next?,"Nevsky, Alexandre and Neate, Timothy and Simperl, Elena and Vatavu, Radu-Daniel",10.1145/3573381.3596159,2023,"The consumption of digital audiovisual media is a mainstay of many people’s lives. However, people with accessibility needs often have issues accessing this content. With a view to addressing this inequality, there exists a wide range of interventions that researchers have explored to bridge this accessibility gap. Despite this work, our understanding of the capability of these interventions is poor. In this paper, we address this through a systematic review of the literature, creating a dataset of and analysing N = 181 scientific papers. We have found that certain areas have accrued a disproportionate amount of attention from the research community – for example, blind and visually impaired and d/Deaf and hard of hearing people account for of papers (N = 170). We describe challenges researchers have addressed, end-user communities of focus, and interventions examined. We conclude by evaluating gaps in the literature and areas that could use more focus on in the future."
"Help, My Game Is Toxic! First Insights from a Systematic Literature Review on Intervention Systems for Toxic Behaviors in Online Video Games","Wijkstra, Michel and Rogers, Katja and Mandryk, Regan L. and Veltkamp, Remco C. and Frommel, Julian",10.1145/3573382.3616068,2023,"Toxicity is a common problem in online games. Players regularly experience negative, hateful, or inappropriate behavior during gameplay. Intervention systems can help combat toxicity but are not widely available and or even comprehensively studied regarding their approaches and effectiveness. To assess the current state of toxicity intervention research, we are conducting a systematic literature review about intervention methods for toxic behaviors in online video games. In this work-in-progress, we report the research protocol for this review and the results from a preliminary analysis. We collected 1176 works from 4 digital libraries and performed abstract and full-text screening, resulting in 30 relevant papers containing 36 intervention systems. By analyzing these intervention systems, we found: 1) Most research proposes novel approaches (n = 28) instead of analyzing existing interventions. 2) Most systems intervene only after toxicity occurs (n = 31) with few interventions that act before toxicity. 3) Only few interventions are evaluated with players and in commercial settings (n = 5), highlighting the potential for more research with higher external validity. In our ongoing work, we are conducting an in-depth analysis of the interventions providing insights into their approaches and effectiveness. This work is the first step toward effective toxicity interventions that can mitigate harm to players."
Design of Lightweight Human Resource Management System Based on Scrum,"Yu, Jiujiu and Zhang, Jishan and Yu, Yanxing and Wu, Ning and Sun, Wenling and Mei, Yingying and Zhu, Canglu",10.1145/3573428.3573637,2023,"The research is based on Scrum of agile development, and taking a human resource management system of ABC Company as an example. This study is devoted to the rapid development of this system with the lightweight design framework of Spring Boot, a feasible development process of the first Sprint for Scrum is described and the feedback on application is positively. Finally, further work is put forward in the future. Firstly, an integrated platform for common interface should be developed to connect with other related sub-systems for ABC Company. Secondly, artificial intelligence algorithms are required to be embedded in the system to achieve the function of intelligent push of talent information."
Research on Mobile Platform for Field Operation of Power Grid Infrastructure under the Digital Transformation of Power Grid,"Wang, Zhen and Fan, Yongxue and Ding, Lei and Yang, Yu",10.1145/3575828.3575839,2023,"Under the background of digital transformation of power grid, how to tap the value of digital technology to enable power grid infrastructure management business has become the focus of current power grid construction business digitalization. This paper analyzes the characteristics of digital technology represented by mobile applications, combines the requirements of power grid infrastructure field operations, deeply studies the application direction of digital technology and power grid infrastructure business, builds a mobile platform for infrastructure field operations, conducts relevant analysis from the perspectives of site construction and construction management, and standardizes the project site management processes such as daily management, safe operation, and construction supervision management, The design method of system component layering is adopted to complete the system function development. The system gives full play to the advantages of digital technology, comprehensively improves the power grid infrastructure site operation management ability, and effectively promotes the digital transformation of power grid construction."
Self-Adaptive Privacy in Cloud Computing: An overview under an interdisciplinary spectrum,"Kitsiou, Angeliki and Pantelelis, Michail and Mavroeidi, Aikaterini-Georgia and Sideri, Maria and Simou, Stavros and Vgena, Aikaterini and Tzortzaki, Eleni and Kalloniatis, Christos",10.1145/3575879.3575968,2023,"The rapid development of cloud computing environments has resulted in various advances and personalized services for users, raising thus several privacy issues. Towards this, research focused on privacy safeguard in the cloud, indicating solutions on the area of self-adaptive privacy. A detailed review is produced to bring forward the carried out work and to analyze it in terms of privacy interdisciplinary standards. In this regard, our work presents the existing self-adaptive privacy approaches and identifies the context for which they have been developed. Moreover, a corresponding classification scheme is provided. The findings give also insights on the proposed tools, which were critically analyzed. This review aims at indicating the developments and limitations of the area, providing potentials of future work in less discussed aspects of the self-adaptive privacy in cloud under an interdisciplinary point of view."
Quantum Machine Learning in Drug Discovery: Current State and Challenges,"Avramouli, Maria and Savvas, Ilias and Vasilaki, Anna and Garani, Georgia and Xenakis, Apostolos",10.1145/3575879.3576024,2023,"The drug discovery process is a time-consuming and quite expensive process. The predictive models of machine learning algorithms have been used efficiently for years in various stages of the drug discovery pipeline. The complexity of these algorithms increases as the size of the molecule increases, adding a single atom to a molecule increases the number of possible combinations. Quantum computers with quantum supremacy can play an important role in complex calculations. Combining the two technologies in practice is a complex endeavor that requires diverse, interdisciplinary teams of scientists working together to be able to integrate the two technologies with the goal of reducing cost and time in drug discovery."
The current state of using learning analytics to measure and support K-12 student engagement: A scoping review,"Bond, Melissa and Viberg, Olga and Bergdahl, Nina",10.1145/3576050.3576085,2023,"Student engagement has been identified as a critical construct for understanding and predicting educational success. However, research has shown that it can be hard to align data-driven insights of engagement with observed and self-reported levels of engagement. Given the emergence and increasing application of learning analytics (LA) within K-12 education, further research is needed to understand how engagement is being conceptualized and measured within LA research. This scoping review identifies and synthesizes literature published between 2011-2022, focused on LA and student engagement in K-12 contexts, and indexed in five international databases. 27 articles and conference papers from 13 different countries were included for review. We found that most of the research was undertaken in middle school years within STEM subjects. The results show that there is a wide discrepancy in researchers’ understanding and operationalization of engagement and little evidence to suggest that LA improves learning outcomes and support. However, the potential to do so remains strong. Guidance is provided for future LA engagement research to better align with these goals."
Moral Machines or Tyranny of the Majority? A Systematic Review on Predictive Bias in Education,"Li, Lin and Sha, Lele and Li, Yuheng and Rakovi\'{c}, Mladen and Rong, Jia and Joksimovic, Srecko and Selwyn, Neil and Ga\v{s}evi\'{c}, Dragan and Chen, Guanliang",10.1145/3576050.3576119,2023,"Machine Learning (ML) techniques have been increasingly adopted to support various activities in education, including being applied in important contexts such as college admission and scholarship allocation. In addition to being accurate, the application of these techniques has to be fair, i.e., displaying no discrimination towards any group of stakeholders in education (mainly students and instructors) based on their protective attributes (e.g., gender and age). The past few years have witnessed an explosion of attention given to the predictive bias of ML techniques in education. Though certain endeavors have been made to detect and alleviate predictive bias in learning analytics, it is still hard for newcomers to penetrate. To address this, we systematically reviewed existing studies on predictive bias in education, and a total of 49 peer-reviewed empirical papers published after 2010 were included in this study. In particular, these papers were reviewed and summarized from the following three perspectives: (i) protective attributes, (ii) fairness measures and their applications in various educational tasks, and (iii) strategies for enhancing predictive fairness. These findings were summarized into recommendations to guide future endeavors in this strand of research, e.g., collecting and sharing more quality data containing protective attributes, developing fairness-enhancing approaches which do not require the explicit use of protective attributes, validating the effectiveness of fairness-enhancing on students and instructors in real-world settings."
Building Technological Improvisation Skills through Student-devised Coursework Topics,"Johnson, Colin G.",10.1145/3576882.3617917,2023,"The ability of improvise solutions to problems using a variety of technologies is an important, if often tacit, desired outcome from advanced computer science education. This paper will describe experience from three modules at two universities where students design their own assessment topic, based on a set of requirements aligned with the learning outcomes.The paper discusses why these technological improvisation skills are important, and how students can learn to cope with combining a variety variety of techniques, ideas and technologies that don't immediately fit well together. This helps students to build important high-level and meta-cognitive skills, and at a practical level provides students with projects that they can demonstrate to prospective employers after graduation.We discuss how these assessments were presented to students, and how the resulting student activities were aligned with learning objectives. We describe how complex ideas such as added value were explained to students, and how these assignments encouraged students to work on projects of ambition and substance, and to encourage their curiosity.Finally, we discuss some difficulties with this kind of assessment, and how we have endeavoured to tackle these difficulties. These include how such a diversity of assessments were marked fairly and consistently, how students who are struggling with the core module content can engage effectively, and how these kind of assessments can be devised without an excessive burden on teaching staff."
"CaNRun: Non-Contact, Acoustic-based Cadence Estimation on Treadmills using Smartphones","Xuan, Ziyi and Liu, Ming and Nie, Jingping and Zhao, Minghui and Xia, Stephen and Jiang, Xiaofan",10.1145/3576914.3589561,2023,"Running with a consistent cadence (number of steps per minute) is important for runners to help reduce risk of injury, improve running form, and enhance overall bio-mechanical efficiency. We introduce CaNRun, a non-contact and acoustic-based system that uses sound captured from a mobile device placed on a treadmill to predict and report running cadence. CaNRun obviates the need for runners to utilize wearable devices or carry a mobile device on their body while running on a treadmill. CaNRun leverages a long short-term memory (LSTM) network to extract steps observed from the microphone to robustly estimate cadence. Through an 8-person study, we demonstrate that CaNRun achieves cadence detection accuracy without calibration for individual users, which is comparable to the accuracy of the Apple Watch despite being non-contact."
Detecting When the Mind Wanders Off Task in Real-time: An Overview and Systematic Review,"Kuvar, Vishal and Kam, Julia W. Y. and Hutt, Stephen and Mills, Caitlin",10.1145/3577190.3614126,2023,"Research on the ubiquity and consequences of task-unrelated thought (TUT; often used to operationalize mind wandering) in several domains recently sparked a surge in efforts to create “stealth measurements” of TUT using machine learning. Although these attempts have been successful, they have used widely varied algorithms, modalities, and performance metrics — making them difficult to compare and inform future work on best practices. We aim to synthesize these findings through a systematic review of 42 studies identified following PRISMA guidelines to answer two research questions: 1) are there any modalities that are better indicators of TUT than the rest; and 2) do multimodal models provide better results than unimodal models? We found that models built on gaze typically outperform other modalities and that multimodal models do not present a clear edge over their unimodal counterparts. Our review highlights the typical steps involved in model creation and the choices available in each step to guide future research, while also discussing the limitations of the current “state of the art” — namely the barriers to generalizability."
Detecting Backdoors in Collaboration Graphs of Software Repositories,"Ganz, Tom and Ashraf, Inaam and H\""{a}rterich, Martin and Rieck, Konrad",10.1145/3577923.3583657,2023,"Software backdoors pose a major threat to the security of computer systems. Minor modifications to a program are often sufficient to undermine security mechanisms and enable unauthorized access to a system. The direct approach of detecting backdoors using static or dynamic program analysis is a daunting task that becomes increasingly futile with the attacker's capabilities. As a remedy, we introduce an orthogonal strategy for the detection of software backdoors. Instead of searching for concealed functionality in program code, we propose to analyze how a software has been developed and locate clues for malicious activities in its version history, such as in a Git repository. To this end, we model the version history as a collaboration graph that reflects how, when and where developers have committed changes to the software. We develop a method for anomaly detection using graph neural networks that builds on this representation and is able to detect spatial and temporal anomalies in the development process. % We evaluate our approach using a collection of real-world backdoors added to Github repositories. Compared to previous work, our method identifies a significantly larger number of backdoors with a low false-positive rate. While our approach cannot rule out the presence of software backdoors, it provides an alternative detection strategy that complements existing work focused only on program analysis."
Early Progress on Enhancing Existing Software Engineering Courses to Cultivate Performance Awareness,"Bondi, Andr\'{e} Benjamin and Xiao, Lu",10.1145/3578245.3584352,2023,"Software engineering and computer science courses are frequently focused on particular areas in a way that neglects such cross-cutting quality attributes as performance, reliability, and security. We will describe the progress we have made in developing enhancements to some of our existing software engineering courses to draw attention and even lay the foundations of an awareness of performance considerations in the software development life cycle. In doing so, we wish to make performance considerations integral to the software engineering mindset while avoiding the need to remove current material from our existing courses. This work is part of an NSF-funded project for undergraduate curriculum development."
A Comparative Analysis on the Detection of Web Service Anti-Patterns Using Various Metrics,"Tummalapalli, Sahithi and Kumar, Lov and Neti, Lalita Bhanu Murthy and Krishna, Aneesh",10.1145/3578527.3578534,2023,"Nowadays, the application of machine learning for developing prediction models is one of the most critical research areas. Early prediction of anti-patterns using machine learning can help developers, and testers fix the design issues and utilize the resources effectively. This work analyzes four different sets of metrics, i.e., source code, WSDL, text, and sequence metrics, to develop web service anti-pattern prediction models. These sets of metrics are treated as an input for models trained using thirty-eight classification techniques to build a model. The experimental finding shows that the models trained using sequence metrics produce better results. The experimental finding also confirmed that the models trained on balanced data achieved better performance than the original data. Further, it is also found that the models trained using CNN and LSTM deep learning approach achieve better results compared to other techniques."
Analyzing Students’ Learning Engagements Using PLS-SEM: A Case Study in A Small Private Online Programming Course,"Daramsenge, Bilegjargal and Hsueh, Nien-Lin and Lai, Lien-Chi",10.1145/3578837.3578870,2023,"Nowadays, particularly during the epidemic situation, the increasing popularity of online-offline education. In computer science education, a small private online programming course (SPOPC) is an important curriculum course to enhance the students’ problem-solving, creative thinking and teamwork activities. The main purpose of this study is to investigate the influences of students’ composite engagements (learning performances and personal feelings) in a SPOPC. We applied an online engagement framework for higher education and partial least squares structural equation modeling (PLS-SEM) approach. Our analysis was conducted from a Python programming course at Feng Chia University, Taiwan, held in the fall semester of 2021–2022 through OpenEdu platform. Our results show that (1) Strongest mediator performance was ‘Video watching’ affected by four significant relations; (2) Behavioral engagement ‘BE’ and emotional engagement ‘EE’ are positively affected ‘Video watching’ performance and they were inextricably linked factors; (3) Social engagement ‘SE’ significantly affected the ‘Term project’ and ‘Final score’ through ‘Midtest’; (4) Cognitive engagement ‘CE’ did not significantly affect the ‘Term project’; Most important is that (5) Students’ first feelings about the course (the first part of the semester) are important factors in deciding to continue a related course or drop out of the course in the future. These findings have valuable practical implications for course instructors who design and lead the programming course."
A Fintech Workshop Course Applies Project-Based Learning in the Intelligent Query of the Money Laundering Control Act,"Lo, Wen-Chi",10.1145/3578837.3578876,2023,"This article considers the application of project-based learning in the intelligent query of the money laundering control act through the course implementation method. This course is designed and implemented to provide practical applications of financial technology for college students in the departments and classes of the Financial Technology College. The teaching goal of this workshop course is to develop students' knowledge and skills and apply them to practical cases, such as using LINE BOT as the most popular human-machine interface and underlying technologies such as Django website building tools SQLite3 database, and Python Crawler technology. The workshop involves completing tasks as a class utilizing students' critical thinking, communication skills, teamwork, and entrepreneurial spirit. From the perspective of the economic environment, people focus on the practical significance of teamwork teaching methods. The evaluation results show that project-based learning has performed well in teaching financial technology workshops. The experience of this course and student reviews show that students like the practical application aspect of the system and are interested in learning. We believe that other business schools that offer similar Fintech workshop courses will also benefit from this approach."
STAFFER: Skill Training Alliance for the Future European Rail System✱,"Mouelhi, Sebti and Consilvio, Alice and Sacco, Nicola and Di Febbraro, Angela and Bugarinovic̀, Mirjana and Voss, Eckhard and Schneider, Sabine and Bregeon, Micka\""{e}l",10.1145/3578837.3578886,2023,"Rail is becoming the backbone of the European sustainable smart mobility. Nevertheless, Europe is experiencing a severe staff shortage for a rapidly developing but increasingly aging sector. In order to meet the rail development trends and to rejuvenate human resources, the STAFFER project aims to identify current and future skills requirements, and to support educational and training establishments in their efforts of curricula adaptation, and mobility programmes implementation. This paper presents the current achievements of the project namely, the identification of skills and competence needs from the perspectives of both operators and suppliers, the development of mobility and training programmes, and their validation for effectively increasing employability and career opportunities."
Digital Transformation in the Craft Sector: Identifying Digital Competencies and Attitudes for Managers and Employees,"Kocak, Seyma and Pawlowski, Jan",10.1145/3579168.3632720,2024,"Digital skills and attitudes are increasingly crucial for driving digital transformation forward. On the other hand, craft enterprises are still traditionally positioned and need more concrete digital competencies and attitudes. This study focuses on the craft sector-specific digital competencies and attitudes. To this end, a literature search was conducted initially, and mixed methods were subsequently used. The study conducted two focus group discussions with experts from the craft sector. The results show that technological competencies, professional competencies, information processing competencies, personal competencies, and digital mindset are the most critical characteristics for managers in the skilled crafts sector. Technological competencies, communication competencies, personality traits, and personal competencies are essential for employees in the craft sector. The competencies and attitudes identified for the craft sector are the first approaches for developing a digital mindset model for the craft sector."
Mining adverse drug events from patients’ disease histories via a GNN-based subgraph prediction method,"Zhou, Fangyu and Uddin, Shahadat",10.1145/3579375.3579407,2023,"Adverse drug events have been a significant concern in medication development as this issue has resulted in comparatively high risks of hospital admissions and death rates worldwide. Traditionally, these risks should have been identified through clinical trials, which could be time-consuming and cost-inefficient. At the same time, it still leaves some adverse drug events unknown due to limited samples in labs during the early phases of drug development. Therefore, various machine-learning techniques have been used in this field of study to support the discovery of adverse drug events as early as possible. This paper presents a state-of-the-art network-based approach to model each patient as a subgraph that consists of nodes of ICD-10 codes and directed edges showing the progression of their diseases. With the help of four Graph Neural Network variants, we could address three research questions 1) whether, 2) when, and 3) which ADEs would occur for a particular patient. In this short paper, we analysed the first question - how this method could be used in identifying cohorts associated with adverse drug events. The experiment showed that the GraphSage method employed on our suggested graph provided the highest accuracy - 0.8863 and the highest recall - 0.9128 for this research question. The same GNN-based framework could also be applied to the remaining research questions."
PERT-GNN: Latency Prediction for Microservice-based Cloud-Native Applications via Graph Neural Networks,"Tam, Da Sun Handason and Liu, Yang and Xu, Huanle and Xie, Siyue and Lau, Wing Cheong",10.1145/3580305.3599465,2023,"Cloud-native applications using microservice architectures are rapidly replacing traditional monolithic applications. To meet end-to-end QoS guarantees and enhance user experience, each component microservice must be provisioned with sufficient resources to handle incoming API calls. Accurately predicting the latency of microservices-based applications is critical for optimizing resource allocation, which turns out to be extremely challenging due to the complex dependencies between microservices and the inherent stochasticity. To tackle this problem, various predictors have been designed based on the Microservice Call Graph. However, Microservice Call Graphs do not take into account the API-specific information, cannot capture important temporal dependencies, and cannot scale to large-scale applications.In this paper, we propose PERT-GNN, a generic graph neural network based framework to predict the end-to-end latency for microservice applications. PERT-GNN characterizes the interactions or dependency of component microservices observed from prior execution traces of the application using the Program Evaluation and Review Technique (PERT). We then construct a graph neural network based on the generated PERT Graphs, and formulate the latency prediction task as a supervised graph regression problem using the graph transformer method. PERT-GNN can capture the complex temporal causality of different microservice traces, thereby producing more accurate latency predictions for various applications. Evaluations based on datasets generated from common benchmarks and large-scale Alibaba microservice traces show that PERT-GNN can outperform other models by a large margin. In particular, PERT-GNN is able to predict the latency of microservice applications with less than 12% mean absolute percentage error."
SMILE: Evaluation and Domain Adaptation for Social Media Language Understanding,"Bashlovkina, Vasilisa and Matthews, Riley and Kuang, Zhaobin and Baumgartner, Simon and Bendersky, Michael",10.1145/3580305.3599907,2023,"We study the ability of transformer-based language models (LMs) to understand social media language. Social media (SM) language is distinct from standard written language, yet existing benchmarks fall short of capturing LM performance in this socially, economically, and politically important domain. We quantify the degree to which social media language differs from conventional language and conclude that the difference is significant both in terms of token distribution and rate of linguistic shift. Next, we introduce a new benchmark for Social MedIa Language Evaluation (SMILE) that covers four SM platforms and eleven tasks. Finally, we show that learning a tokenizer and pretraining on a mix of social media and conventional language yields an LM that outperforms the best similar-sized alternative by 4.2 points on the overall SMILE score."
Scoping Out the Scalability Issues of Autonomous Vehicle-Pedestrian Interaction,"Tran, Tram Thi Minh and Parker, Callum and Tomitsch, Martin",10.1145/3580585.3607167,2023,"Autonomous vehicles (AVs) may use external interfaces, such as LED light bands, to communicate with pedestrians safely and intuitively. While previous research has demonstrated the effectiveness of these interfaces in simple traffic scenarios involving one pedestrian and one vehicle, their performance in more complex scenarios with multiple road users remains unclear. The scalability of AV external communication has therefore attracted increasing attention, prompting the need for further investigation. This scoping review synthesises information from 54 papers to identify seven key scalability issues in multi-vehicle and multi-pedestrian environments, with Clarity of Recipients, Information Overload, and Multi-Lane Safety emerging as the most pressing concerns. To guide future research in scalable AV-pedestrian interactions, we propose high-level design directions focused on three communication loci: vehicle, infrastructure, and pedestrian. Our work contributes the groundwork and a roadmap for designing simplified, coordinated, and targeted external AV communication, ultimately improving safety and efficiency in complex traffic scenarios."
Addressing UX Practitioners’ Challenges in Designing ML Applications: an Interactive Machine Learning Approach,"Feng, K. J. Kevin and Mcdonald, David W.",10.1145/3581641.3584064,2023,"UX practitioners face novel challenges when designing user interfaces for machine learning (ML)-enabled applications. Interactive ML paradigms, like AutoML and interactive machine teaching, lower the barrier for non-expert end users to create, understand, and use ML models, but their application to UX practice is largely unstudied. We conducted a task-based design study with 27 UX practitioners where we asked them to propose a proof-of-concept design for a new ML-enabled application. During the task, our participants were given opportunities to create, test, and modify ML models as part of their workflows. Through a qualitative analysis of our post-task interview, we found that direct, interactive experimentation with ML allowed UX practitioners to tie ML capabilities and underlying data to user goals, compose affordances to enhance end-user interactions with ML, and identify ML-related ethical risks and challenges. We discuss our findings in the context of previously established human-AI guidelines. We also identify some limitations of interactive ML in UX processes and propose research-informed machine teaching as a supplement to future design tools alongside interactive ML."
Globally-Robust Instance Identification and Locally-Accurate Keypoint Alignment for Multi-Person Pose Estimation,"Tian, Fangzheng and Kim, Sungchan",10.1145/3581783.3612525,2023,"Scenes with a large number of human instances are characterized by significant overlap of the instances with similar appearance, occlusion, and scale variation. We propose GRAPE, a novel method that leverages both Globally Robust human instance identification and locally Accurate keypoint alignment for 2D Pose Estimation. GRAPE predicts instance center and keypoint heatmaps, as global identifications of instance location and scale, and keypoint offset vectors from instance centers, as representations of accurate local keypoint positions. We use Transformer to jointly learn the global and local contexts, which allows us to robustly detect instance centers even in difficult cases such as crowded scenes, and align instance offset vectors with relevant keypoint heatmaps, resulting in refined final poses. GRAPE also predicts keypoint visibility, which is crucial for estimating centers of partially visible instances in crowded scenes. We demonstrate that GRAPE achieves state-of-the-art performance on the CrowdPose, OCHuman, and COCO datasets. The benefit of GRAPE is more apparent on crowded scenes (CrowdPose and OCHuman), where our model significantly outperforms previous methods, especially on hard examples."
Research on Intellectualization of Cost Management System of Power Transmission and Transformation Project,"Chen, Xiaoli and Chen, Chen and Wu, Fenqian and Lin, Jiawei and Liu, Mengzhe",10.1145/3582084.3582086,2023,"With the reform of China's power system, power has become a pillar industry of China's national economy, which needs to improve the competitiveness of the power industry. However, in the actual power engineering, the cost management of power transmission and transformation project (hereinafter referred to as PTATP) runs through the whole process of project construction, involving the processes of feasibility study, design, construction, change, completion and final accounts, which is a systematic problem. With the application of intelligent technology, collaborative office, BIM, smart site and big data have been applied to project management and cost management, which has been fully introduced into the digital and intelligent era. Therefore, the construction of PTATP cost management system has become the trend in the future. Through the cost management system of PTATP, we can collect, store, calculate and optimize the cost of PTATP life cycle, which can predict and warn the project cost. Therefore, this paper first analyses the application technology and construction principles of the cost management system for power transmission and transformation projects, as well as the main problems existing in the current cost management model. Then, some prediction methods, standardised data elements, validation data and integration data are analysed for the retrieved key cost data to realise the management of key cost data before the cost data collection template for transmission and substation projects, and effectively improve the key cost data matching ratio in the data collection template. Finally, this paper constructs and analyses the intelligent research results of the system, and introduces how to build a professional intelligent application platform for cost management in terms of platform architecture, technical route selection, application architecture design and data integration design. It is hoped to provide a more systematic and professional solution for the pricing management of power transmission and transformation projects."
A Sentiment Analysis Classification of Product Reviews through Convolutional Neural Networks (CNN),"Inovero, Carlo Garing and Ditablan, Gil Jeremy P. and Reyes, John Ramil C. and Tajanlangit, Renzel E.",10.1145/3582197.3582199,2023,"With the advancement of technology, the behavior of consumers turned into E-commerce, especially when the pandemic started. Defining the success and credibility of e-commerce is important as it is becoming a trend, and one of the factors that can affect it is product reviews. Product reviews build trust and loyalty, which influence the purchase decision. These product reviews can be in textual form, star rating, and emoji. Star rating is the commonly used product review but causes rating inflation. That is why word-based rating eliminates this problem which gives more accurate reviews. In this modern day, we do not only convey our sentiments in the language we are used to but also in other formats, such as using both text and emoji. Since Convolutional Neural Networks have been drawing attention to their reduced effort in feature definition, it has been gaining popularity in text classification.This leads the researchers to create a tool that aims to determine the performance measure in detecting the polarity of a text-based and text-based sentiment analysis with emoji using Convolutional Neural Networks. The researchers determined the system's performance using Confusion Matrix and derived its Precision, Recall, and F1-Score."
A Proposed Method of Literature Analysis Based on Natural Language Processing and Network Analysis,"Jiang, Yunqing and Pang, Patrick Cheong-Iao and Ang, Wen Wei and Lau, Ying",10.1145/3582197.3582202,2023,"The number of scientific publications is increasing each year, and researchers' demand for literature analysis is also growing. This study aims to propose a computer-aided literature analysis method based on natural language processing technology. Our seven-step method includes the tasks of literature search, data preprocessing, word frequency calculation, word extraction, word labeling, co-word analysis, and network visualization. Demonstrated with an empirical study for exploring the development of artificial intelligence in a medical area, this method can produce data and visualizations that help researchers to explore literature content based on their research objectives."
Data Decentralisation of LLM-Based Chatbot Systems in Chronic Disease Self-Management,"Montagna, Sara and Ferretti, Stefano and Klopfenstein, Lorenz Cuno and Florio, Antonio and Pengo, Martino Francesco",10.1145/3582515.3609536,2023,"Chronic patient self-management is crucial for maintaining physical and psychological health, reducing pressure on healthcare systems, and promoting patient empowerment. Digital technologies, particularly chatbots, have emerged as powerful tools for supporting patients in managing their chronic conditions. Large language models (LLMs), such as GPT-4, have shown potential in improving chatbot-based systems in healthcare. However, their adoption in clinical practice faces challenges, including reliability, the need for clinical trials, and privacy concerns. This paper proposes a general architecture for developing an LLM-based chatbot system that supports chronic patients while addressing privacy and security concerns. The architecture is designed to be independent of specific technologies and health conditions, focusing on data protection legislation compliance. A prototype of the system has been developed for hypertension management, demonstrating its potential for motivating patients to monitor their blood pressure and adhere to prescriptions."
Survey for Smart and Adaptative Education,"Bezerra, Rita M. A. and Dur\~{a}Es, Dalila and Novais, Paulo Novais",10.1145/3582580.3582622,2023,"Technological media has become a vital component for educators, enabling greater efficiency in lesson planning and preparation, providing active repositories of student work, as well as more personalised learning for students. The construction of intelligent systems, which encompass the use of these technologies, allow the learning process to be simpler for the student and the teaching process for the teacher to be more efficient. Thus, ambient intelligence will be the element used to connect the classroom to the intelligent classroom and used to adapt the learning process. With the rapid development of the Internet of Things (IoT) and Artificial Intelligence (AI), their applicability in education will be easy to implement and can solve some defects of the traditional education model and the computer-aided system teaching model, and at the same time make learning more motivating and enticing for the student. The aim of this research is to identify the most recent studies on the application of artificial intelligence in education and to make a comparative study of the different models and existing applications that use this technology to promote learning efficiently"
METHODS OF DETERMINING FAKE CONTENT USING ARTIFICIAL INTELLIGENCE,"Shoakhmedova, Nozima Khairullaevna and Yusupova, Dilbar Mirabidovna",10.1145/3584202.3584234,2023,"In this article, the ways and methods of detecting fake content are described by using artificial intelligence. The methods for detecting fake content using artificial intelligence are explained. The article also reveals the processes of using artificial intelligence in cases of detecting the falsity of photographs or texts."
An Artificial Neural Network Model based on Binary Particle Swarm Optimization for enhancing the efficiency of Software Defect Prediction,"Malhotra, Ruchika and Chawla, Sonali and Sharma, Anjali",10.1145/3584871.3584885,2023,"With the rise in the growth of the software industry, it is essential to identify software defects in earlier stages to save costs and improve the efficiency of the software development lifecycle process. We have devised a hybrid software defect prediction (SDP) model that integrates Binary Particle Swarm Optimization (Binary PSO), Synthetic Minority Oversampling Technique (SMOTE), and Artificial Neural Network (ANN). BPSO is applied as a wrapper feature selection process utilizing AUC as a fitness function, SMOTE handles the dataset imbalance, and ANN is used as a classification algorithm for predicting software defects. We analyze the proposed BPSO-SMOTE-ANN model's predictive capability using the AUC and G-mean performance metrics. The proposed hybrid model is found helpful in predicting software defects. The statistical results suggest the enhanced performance of the proposed hybrid model concerning AUC and G-mean values. Also, the hybrid model was found to be competitive with other machine learning(ML) algorithms in determining software defects."
How Small Businesses Transform PDF Agreements into Action,"So, Jianna and Lipka, Nedim and Siu, Alexa F. and Rossi, Ryan and Dernoncourt, Franck",10.1145/3584931.3606972,2023,"A legal agreement is a type of procedural document that describes the steps that parties must take to fulfill legal obligations. Following these steps requires human interpretation, which is often inefficient and error prone. For a Small to Medium Sized Business (SMB), this process is laborious. We conduct an interview study to understand how information in PDF agreements is currently understood, processed, and acted upon by SMB employees working in small teams of non-domain experts. Through qualitative analysis and a text highlighting activity, we observe knowledge transfer workflows in SMBs and propose design principles for using AI-extracted information to create actionable documents that address gaps in efficiency, understanding, and agency."
U.S. Master's Level Cybersecurity Programs: An Analysis of the Diversity of Content,"Nasereddin, Mahdi and Glantz, Edward and Peca, Joanne and Bartolacci, Michael and Grimes, Galen",10.1145/3585059.3611414,2023,"As the popularity of Cybersecurity-related curricula grows both among students and institutions at the higher education level, there exist many questions regarding how degree programs should be structured and what specific content should be taught in them. Many areas of study that were once thought to be somewhat removed from a networking or information security focus have created new courses, or adapted existing ones, to create degree programs based on some aspect of Cybersecurity. Whether the program is titled Information Security, Information Governance, Business Systems Security, Network Security, or a host of other variations and terms, the heart of all of them is Cybersecurity. In other words, all of these programs revolve around the securing of data, systems, networks, and their physical footprints in organization. This work seeks to review the various types of master's level programs related to Cybersecurity in the U.S."
Correlating Students' Class Performance Based on GitHub Metrics: A Statistical Study,"Cui, Jialin and Zhang, Runqiu and Li, Ruochi and Song, Yang and Zhou, Fangtong and Gehringer, Edward",10.1145/3587102.3588799,2023,"What skills does a student need to succeed in a programming class? Ostensibly, previous programming experience may affect a student's performance. Most past studies on this topic use self-reporting questionnaires to query students about their programming experience. This paper presents a novel, unified, and replicable way to measure previous programming experience using students' pre-class GitHub contributions. To our knowledge, we are the first to use GitHub contributions in this way. We conducted a comprehensive statistical study of students in an object-oriented design and development class from 2017 to 2022 (n = 751) to explore the relationships between GitHub contributions (commits, comments, pull requests, etc.) and students' performance on exams, projects, designs, etc. in the class. Several kinds of contributions were shown to have statistically significant correlations with performance in the class. A set of two-samplet -tests demonstrate statistical significance of the difference between the means of some contributions from the high-performing and low-performing groups."
Saving Bees with Computer Science: A Way to Spark Enthusiasm and Interest through Interdisciplinary Online Courses,"Marquardt, Kai and Happe, Lucia",10.1145/3587102.3588835,2023,"In computer science education (CSEd) it is a well-known challenge to create learning environments in which everyone can experience equal opportunities to identify themselves with the subject, get involved, and feel engaged. Especially for underrepresented groups such as girls or not computer enthusiasts, CSEd seems to lack sufficient opportunities at its current state. In this paper, we present a novel approach of using interdisciplinary online courses in the context of bee mortality and discuss the possibilities of such courses to enhance diverse learning in CSEd. We report summarized findings from a one-year period, including 16 workshops where over 160 secondary school students (aged 10-16) have participated in our online courses. Pre-test-post-test surveys have been conducted to gain insights into students' perceptions and attitude changes. The results show the potential of such interdisciplinary approaches to spark interest in computer science (CS) and to raise positive feelings toward programming. Particularly striking are the results from differentiated analyses of students grouped by characteristics such as low initial self-efficacy, coding aversion, or less computer affinity. We found multiple significant effects of our courses to impact students of those groups positively. Our results clearly indicate the potential of interdisciplinary CSEd to address a more diverse audience, especially traditionally underrepresented groups."
Development of IT Equipment Management Methodology based on Carbon Emission and End-of-Life Period with A Design Thinking Approach: Case Study: Bandung Institute of Technology,"Christianto, Vhydie G. and Hikmawati, Erna and Surendro, Kridanto",10.1145/3587828.3587882,2023,"Global warming and other environmental problems are becoming a worrying aspect. In the current digitalization era, various organizations are focusing on increasing the quantity of Information Technology (IT) assets to support business processes, including organizations engaged in the education sector. However, the impact arising from IT equipment has not been well thought out and managed. The same thing also happened to the Bandung Institute of Technology, which had no concerns regarding IT equipment emissions. In reality, the management of IT equipment has not been carried out properly due to the absence of an equipment management system based on environmental aspects, such as greenhouse gas emissions and end-of-life period. Therefore, this research focuses on developing a method with a design thinking approach to develop an IT equipment management system. Design thinking is used as a method of development approach so that it can be adapted to the needs of users and organizations. This method is expected to provide insight to ITB stakeholders so that they are able to design an environment-based IT equipment management system."
Integrating Real-Time Dynamic Electricity Price Forecast into Job Shop Production Scheduling Model with Multiple Machine Environments,"Krstevski, Stefan and Fatahi Valilai, Omid and Wicaksono, Hendro",10.1145/3587889.3587905,2023,"One of the challenges in the transition towards green electricity is the intermittence of power generated by renewable sources. Thus, power consumers, including the manufacturing industry, must adapt their activities and processes to green electricity supply. Real-time dynamic pricing is an approach to encourage electricity consumers to change their consumption patterns by lowering prices when the availability of green electricity in the grid is high. Due to the introduction of real-time electricity pricing, manufacturing companies must adapt their production planning by integrating dynamic price information into their production scheduling. Our research focuses on extending the basic production scheduling mathematical model by introducing real-time power pricing in the model. The prices are built based on the current proportion of green electricity in the grid represented in the green electricity index (GEI) with one-hour intervals. This paper also illustrates a scenario of how to use the model. Our future research will further extend the model addressing the flexibility of manufacturing shop floors (e.g. adding buffer, retooling, and setup time) and validate the model in two small and medium manufacturing enterprises."
Using No-code Platform to Accelerate Digital Transformation – The Study for Feasibility Analysis and Solution Development of Performance Visualization Cloud-computing Management System (PVCMS),"Hsu, Shun-Fa and Lee, Yi-In and Hsu, Ching-Tzu and Hsu, Ching-Yin",10.1145/3588155.3588159,2023,"This study is based on government's digital transformation policy and the industry's digital development trend. Micro-Small Enterprises(MSEs) are the target customers to meet the organizational productivity management needs of key performance indicators (KPI) and project management (PjM) as the research subject, combined with management by objectives (MBO) and performance appraisal organizational management practice as a methodology, using Google's No-code smart cloud-computing service as a practical platform, integrating data input/processing/analysis/ visualization (D/IPAV®) workflows to customized develop the ""Performance Visualization Cloud-computing Management System"" (PVCMS) of the organization's operational data monitoring and decision-making digital dashboard. This project sets PVCMS as the core product and develops a set of indicators construction, analysis and visual monitoring that meet the performance management needs of the industry. Diagnosis and Consultant model as a solution, to follow-up actual commercialization standard operating procedure to be followed, and quick import templates for various performance management visualization modules. Its operation process is mainly divided into five stages: (1). Define performance management requirements, (2). Establish KPIs, (3). Link Indicator with performance evaluation, (4). Build performance management system, (5). Track Indicators and decision-making application."
Design of music data analysis platform based on visualization technology,"Fan, Qiuyue",10.1145/3588155.3588161,2023,"With the advent of the Internet era, online music is booming, and the number of digital music and users is also showing an explosive growth trend. A data analysis platform based on Netease Cloud Music is designed. The main modules include data collection, data storage, data analysis and data visualization. Combined with the massive users and music resources of Netease Cloud Music, machine learning and deep learning algorithms are used to analyze their data from roughly three directions: user influence analysis, using PageRank algorithm and Triangle Count algorithm; The MLPC multilayer perceptron classifier algorithm in depth learning is mainly used for song style classification prediction; Comment data topic mining uses the BGLL algorithm of community detection based on modularity gain. It is recommended by various creators with music aesthetic experience and lovers with independent music taste, taking the song list as the main mode, including social function and interactive function. In this era of advocating personalization, NetEase Cloud Music has broken the popular recommendation of traditional music and provided music recommendation services for thousands of individuals. Therefore, it has won a large number of user resources and music resources."
Qualitative Intention-aware Attribute-based Access Control Policy Refinement,"Mitani, Shohei and Kwon, Jonghoon and Ghate, Nakul and Singh, Taniya and Ueda, Hirofumi and Perrig, Adrian",10.1145/3589608.3593841,2023,"Designing access control policies is often expensive and tedious due to the heterogeneous systems, services, and diverse user demands. Although ABAC policy and decision engine creation methods based on machine learning have been proposed, they cannot make good access decisions for applications and situations not envisioned by the decision-makers who provide training examples. It results in over-and under-permissiveness. In this paper, we propose a framework that refines pre-developed policies. It creates a decision engine that makes better decisions than those policies. Inspired by multiple criteria decision theory, our method uses the policy manager's qualitative intentions behind their judgments to guide access decisions so that more benefits are expected. In the evaluation, we prepare a coarse and relatively elaborate policy. We refine the coarse policy to obtain a decision engine that is compared for the similarity in access decisions with the elaborate policy using AUC as a measure. The results show that our method improves the coarse policy by a difference of 12-26% in AUC and outperforms the conventional machine learning methods by a difference of 3-11% in AUC."
Towards Evidence-Based Software Quality Practices for Reproducibility: Preliminary Results and Research Directions,"Milewicz, Reed and Mundt, Miranda",10.1145/3589806.3600040,2023,"In the computational science and engineering (CSE) community, there is a prevailing belief that adopting better software development practices and investing in software quality will directly lead to more robust, reproducible software. There is, however, relatively little evidence to indicate what specific aspects of software quality influence reproducibility or which practices lead to more reproducible software. To better inform this discussion, we present preliminary findings from an ongoing study of how software quality practices among CSE teams affect the reproducibility of the software they create."
Chatbot recommender systems in tourism: A systematic review and a benefit-cost analysis,"Camilleri, Mark Anthony and Troise, Ciro",10.1145/3589883.3589906,2023,"This research is focused on the utilization of artificially intelligent (AI), customer service chatbots in travel, tourism and hospitality. Rigorous criteria were used to search, screen, extract and synthesize articles on conversational, automated systems. The results shed light on the most-cited articles on the use of “chatbots” and “tourism” or “hospitality”. The researchers scrutinize the extracted articles, synthesize the findings and outline the pros and cons of using these interactive technologies. This contribution implies that there is scope for tourism businesses to continue improving their online customer services in terms of their efficiency and responsiveness to consumers and prospects. For the time being, AI chatbots are still not in a position to replace human agents in all service interactions as they cannot resolve complex queries and complaints. However, works are in progress to improve their verbal, vocal and anthropomorphic capabilities to deliver a better consumer experience."
Investigating the Availability of Child Sexual Abuse Materials in Dark Web Markets: Evidence Gathered and Lessons Learned,"Wang, Yichao and Arief, Budi and Franqueira, Virginia Nunes Leal and Coates, Anna Grace and \'{O} Ciardha, Caoilte",10.1145/3590777.3590812,2023,"Child sexual exploitation and abuse (CSEA) and the associated distribution of child sexual abuse material (CSAM) are serious offences online and offline. They are exacerbated by the increased popularity of dark web markets, in which vendors and buyers can exchange CSAM while hiding their identities. The aim of this paper is to improve our understanding of the CSEA landscape in dark web markets. We reviewed and collated four groups of keywords (a total of 198) for the detection/discovery of potential CSAM on the dark web market. This allowed us to conduct a systematic data collection (i.e., scraping) on dark web markets containing CSAM to create a new text-based dataset and perform further analysis. We found that CSAM is more public in the Chinese market, but not in the mainstream English market. To illustrate this point, we detected 724 CSAM items in the two Chinese dark web markets studied, but none in the eight English markets. While the prices of these CSAM remain low, we found that there were 3,449 sales over the 44-week observation period, implying that CSAM has been commercialised to some extent. We also noticed that mainstream cloud-based data storage services were used for the distribution and sharing of CSAM. We hope that the findings presented in this paper can help relevant stakeholders to understand the CSAM landscape in the dark web market better, which in turn may be used to devise more effective countermeasures to combat CSEA and CSAM."
A Machine Learning Model for Disease Prediction and Remote Patient Monitoring,"Naik, Kirtida Tejas and Garg, Bindu",10.1145/3590837.3590844,2023,"People's health reports, including diagnostic information and medical prescriptions, are delivered in the form of test-based case notes, making it impossible to determine a person's prior health issues or medications taken until he or she returns to the hospital later on. Storing all of a person's health information in the cloud as a soft copy, on the other hand, alleviates this issue. To accomplish this, each and every hospital, dispensary, and laboratory must have an internet connection for the registration of patient data. A unique Health Id will identify each patient, and all of the patient's data will be stored in the cloud, where the specific patient can only access it. In order to prevent and treat illness, it is critical to perform an accurate and timely analysis on any health-related problem. The ability to diagnose disease by obtaining all information from a linked Health ID combined with Machine Learning techniques will improve the system's ability to detect diseases. We believe that our diagnostic model can operate like a doctor in the earlier diagnosis of this disease, allowing for timely treatment and the preservation of life."
Text mining and Statistical Analysis based study on the effectiveness of Work Integrated Learning Program: Evidence from industry-academia collaboration between SAP Labs and BITS Pilani,"Gopal, Annapoorna and Nagpal, Gaurav",10.1145/3590837.3590935,2023,"The industry and academia need to work for bridge the gap between two of them. The significance of this is acknowledged widely in the industry as well as in academics, which has led to the birth of many collaborative education programs between the industry and academic institutions. While the intent of creating these partnerships is very noble, the extent to which they have been successful needs to be continuously measured. This study evaluates the performance of one such collaborative education program that was offered for the employees of SAP Labs, an ERP solutions provider by Birla Institute of Technology and Science- Pilani, a leading university in India. It collates and analyses the 360-degree feedback from the students, the alumni, the faculty, and the organization, through text mining and statistical analysis. It concludes that such Work Integrated Learning Program collaborations can produce significant benefits for both- the industry and the academia. Vaders Sentiment Analysis and Transformer have been used to sense the feeling of the alumni after pre-processing of text data and the word cloud has been generated."
Examining the Text-to-Image Community of Practice: Why and How do People Prompt Generative AIs?,"Sanchez, T\'{e}o",10.1145/3591196.3593051,2023,"Image generation gained popularity with machine learning (ML) models generating images from text, fuelling new online communities of practices. This work explores the sociology, motivations, and usages of AI art hobbyists. We analyzed an online questionnaire answered by 64 practitioners and a dataset of user prompts sent to the Stable Diffusion generative model. Our findings suggest that TTI generation is a recreational activity mainly conducted by narrow socio-demographic groups who use auxiliary techniques across platforms and beyond request-response interactions. Inherent model limitations and finding suitable prompt formulation are the main obstacles practitioners face. A taxonomy and a corresponding ML model capable of recognizing the semantic content of unseen prompts were created to conduct the user prompt analysis. The prompt analysis revealed that artist names are the main specifier used beside the main subject, often in sequences. We finally discuss the design and socio-technical implications of our work for creativity support."
INSIDE: an Ontology-based Data Integration System Applied to the Oil and Gas Sector,"Campos, J\'{u}lio Gon\c{c}alves and De Almeida, Vitor Pinheiro and De Armas, Elvismary Molina and Da Silva, Geiza Maria Hamazaki and Corseuil, Eduardo Thadeu and Gonzalez, Fernando Rodrigues",10.1145/3592813.3592893,2023,"Context: Data integration remains a major challenge facing organizations in the information age. Despite the advances made in recent decades, new approaches have become necessary to deal with new challenges such as Big Data. Problem: Semantic heterogeneity is a significant problem faced by companies in the oil and gas sector, as it makes it difficult to exchange information with other companies. Furthermore, there is a shortage of data integration systems that use open source technologies to deal with semantics, interoperability and scalability. Solution: INSIDE - Semantic Interoperability for Engineering Data Integration - an information system based on ontologies for data integration developed for an oil and gas company. SI Theory: This work is influenced by Representation Theory, based on the idea that an information system is a faithful representation of certain phenomena in the real world. Method: Review of state of the art on system architectures for data integration and use of methodologies for elaborating ontologies that represent the knowledge base of the information system. Summary of Results: Implementation of a prototype that allows querying heterogeneous data sources using a vocabulary familiar to the user, removing ambiguities from data with semantics. Contributions and Impact in IS area: The development of a solution for data integration using open source technologies tested with real-world data from a company in the oil and gas sector that can serve as a reference for developing new applied systems to other sectors."
On Challenges and Opportunities of Using Continuous Experimentation in the Engineering of Contemporary Software Systems,"Souza, Bruno Pedra\c{c}a and Santos, Paulo S\'{e}rgio Medeiros and Travassos, Guilherme Horta",10.1145/3592813.3592927,2023,"Context: Modern Information Systems require the use of contemporary software systems such as Cyber-Physical Systems, Embedded Systems, and Smart Cities-based Systems, eventually built under the paradigm of the Internet of Things. These Contemporary Software Systems (CSS) add new challenges for their construction, maintainability, and evolution, including the involvement of many actors with the software project and the necessary management of dependencies among hardware/things, software systems, and people. Problem: These technological challenges jeopardize the final quality of modern information systems due to the lack of adequate mechanisms supporting the engineering of CSS. Solution: Continuous Experimentation (CE) deserves some investigation regarding its suitability to mitigate and reduce engineering CSS risks. IS Theory: This research is under the General Systems Theory and is consistent with the Systems Information challenges regarding building smart cities-based systems. Method: To undertake a Structured Literature Review (StLR) supported with snowballing to reveal CE's empirical studies. Results: The StLR identified seven primary studies on CE adoption to support CSS building. Many studies are in the domain of embedded systems and CPS. Besides, the findings allowed us to conjecture a set of challenges and opportunities regarding using CE in CSS engineering. Conclusion: There are emergent technologies to support CE's execution in the context of web-based systems. However, several challenges and gaps surround CE's use for engineering CSS. Furthermore, the lack of software technologies, blueprints, or concrete guidance to promote CE in these software systems can motivate further investigations into its use in engineering the important parts of modern information systems."
On the Prediction of Software Merge Conflicts: A Systematic Review and Meta-analysis,"Graeff, C\'{e}sar Augusto and Farias, Kleinner and Carbonera, Carlos Eduardo",10.1145/3592813.3592931,2023,"Context: Predicting software merge conflicts plays a chief role in many software engineering tasks. Prediction techniques can be used to identify and anticipate potential conflicts between source code snippets that may arise when merging changes made by multiple developers. Problem: When multiple developers are working on the same source code, they may make changes to the same code snippets, which can cause conflicts when the changes are brought together. The problem is that typically these conflicts are difficult and time-consuming to resolve, and can potentially cause serious problems if not addressed properly. Solution: Predicting when conflicts will help in prioritizing or better managing the changes to be made in the code, in order to mitigate the emergence of conflicts. In this sense, developers can use tools that exam source codes to pinpoint potential snippets where conflicts are likely to occur. Theory of IS: This work was conceived under the aegis of the General Theory of Systems, in particular with regard to the interfaces between the parts of a system within its borders. In this case, the parts are themselves independent systems, called constituents, which include some information systems (IS). Method: This article focus on providing a systematic review and meta-analysis of published studies on the prediction of software conflicts. For this, we follow the PRISMA method. Contributions and Impact in the IS area: Development of a database knowledge of the historical context over predicting conflicts that will help in the development of future work and other research."
"It’s about power: What ethical concerns do software engineers have, and what do they (feel they can) do about them?","Widder, David Gray and Zhen, Derrick and Dabbish, Laura and Herbsleb, James",10.1145/3593013.3594012,2023,"How do software engineers identify and act on their ethical concerns? Past work examines how software practitioners navigate specific ethical principles such as “fairness”, but this narrows the scope of concerns to implementing pre-specified principles. In contrast, we report self-identified ethical concerns of 115 survey respondents and 21 interviewees across five continents and in non-profit, contractor, and non-tech firms. We enumerate their concerns – military, privacy, advertising, surveillance, and the scope of their concerns – from simple bugs to questioning their industry’s entire existence. We illustrate how attempts to resolve concerns are limited by factors such as personal precarity and organizational incentives. We discuss how even relatively powerful software engineers often lacked the power to resolve their ethical concerns. Our results suggest that ethics interventions must expand from helping practitioners merely identify issues to instead helping them build their (collective) power to resolve them, and that tech ethics discussions may consider broadening beyond foci on AI or Big Tech."
A Systematic Review of Ethics Disclosures in Predictive Mental Health Research,"Ajmani, Leah Hope and Chancellor, Stevie and Mehta, Bijal and Fiesler, Casey and Zimmer, Michael and De Choudhury, Munmun",10.1145/3593013.3594082,2023,"Applied machine learning (ML) has not yet coalesced on standard practices for research ethics. For ML that predicts mental illness using social media data, ambiguous ethical standards can impact peoples’ lives because of the area’s sensitivity and material consequences on health. Transparency of current ethics practices in research is important to document decision-making and improve research practice. We present a systematic literature review of 129 studies that predict mental illness using social media data and ML, and the ethics disclosures they make in research publications. Rates of disclosure are going up over time, but this trend is slow moving – it will take another eight years for the average paper to have coverage on 75% of studied ethics categories. Certain practices are more readily adopted, or ""stickier"", over time, though we found prioritization of data-driven disclosures rather than human-centered. These inconsistently reported ethical considerations indicate a gap between what ML ethicists believe ought to be and what actually is done. We advocate for closing this gap through increased transparency of practice and formal mechanisms to support disclosure."
Investigating Software Engineering Artifacts in DevOps Through the Lens of Boundary Objects,"Matthies, Christoph and Heinrich, Robert and Wohlrab, Rebekka",10.1145/3593434.3593441,2023,"Software engineering artifacts are central to DevOps, enabling the collaboration of teams involved with integrating the development and operations domains. However, collaboration around DevOps artifacts has yet to receive detailed research attention. We apply the sociological concept of Boundary Objects to describe and evaluate the specific software engineering artifacts that enable a cross-disciplinary understanding. Using this focus, we investigate how different DevOps stakeholders can collaborate efficiently using common artifacts. We performed a multiple case study and conducted twelve semi-structured interviews with DevOps practitioners in nine companies. We elicited participants’ collaboration practices, focusing on the coordination of stakeholders and the use of engineering artifacts as a means of translation. This paper presents a consolidated overview of four categories of DevOps Boundary Objects and eleven stakeholder groups relevant to DevOps. To help practitioners assess cross-disciplinary knowledge management strategies, we detail how DevOps Boundary Objects contribute to four areas of DevOps knowledge and propose derived dimensions to evaluate their use."
Code Reviewer Recommendation for Architecture Violations: An Exploratory Study,"Li, Ruiyin and Liang, Peng and Avgeriou, Paris",10.1145/3593434.3593450,2023,"Code review is a common practice in software development and often conducted before code changes are merged into the code repository. A number of approaches for automatically recommending appropriate reviewers have been proposed to match such code changes to pertinent reviewers. However, such approaches are generic, i.e., they do not focus on specific types of issues during code reviews. In this paper, we propose an approach that focuses on architecture violations, one of the most critical type of issues identified during code review. Specifically, we aim at automating the recommendation of code reviewers, who are potentially qualified to review architecture violations, based on reviews of code changes. To this end, we selected three common similarity detection methods to measure the file path similarity of code commits and the semantic similarity of review comments. We conducted a series of experiments on finding the appropriate reviewers through evaluating and comparing these similarity detection methods in separate and combined ways with the baseline reviewer recommendation approach, RevFinder. The results show that the common similarity detection methods can produce acceptable performance scores and achieve a better performance than RevFinder. The sampling techniques used in recommending code reviewers can impact the performance of reviewer recommendation approaches. We also discuss the potential implications of our findings for both researchers and practitioners."
Assessing the Impact of File Ordering Strategies on Code Review Process,"Bagirov, Farid and Derakhshanfar, Pouria and Kalina, Alexey and Kartysheva, Elena and Kovalenko, Vladimir",10.1145/3593434.3593462,2023,"Popular modern code review tools (e.g., Gerrit and GitHub) sort files in a code review in alphabetical order. A prior study (on open-source projects) shows that the changed files’ positions in the code review affect the review process. Their results show that files placed lower in the order have less chance of receiving reviewing efforts than the other files. Hence, there is a higher chance of missing defects in these files. This paper explores the impact of file order in the code review of the well-known industrial project IntelliJ IDEA. First, we verify the results of the prior study on a big proprietary software project. Then, we explore an alternative to the default Alphabetical order: ordering changed files according to their code diff. Our results confirm the observations of the previous study. We discover that reviewers leave more comments on the files shown higher in the code review. Moreover, these results show that, even with the data skewed toward Alphabetical order, ordering changed files according to their code diff performs better than standard Alphabetical order regarding placing problematic files, which needs more reviewing effort, in the code review. These results confirm that exploring various ordering strategies for code review needs more exploration."
Automatic Data-Driven Software Change Identification via Code Representation Learning,"Heri\v{c}ko, Tja\v{s}a",10.1145/3593434.3593505,2023,"Changes to a software project are inevitable as the software requires continuous adaptations, improvements, and corrections throughout maintenance. Identifying the purpose and impact of changes made to the codebase is critical in software engineering. However, manually identifying and characterizing software changes can be a time-consuming and tedious process that adds to the workload of software engineers. To address this challenge, several attempts have been made to automatically identify and demystify intents of software changes based on software artifacts such as commit change logs, issue reports, change messages, source code files, and software documentation. However, these existing approaches have their limitations. These include a lack of data, limited performance, and an inability to evaluate compound changes. This paper presents a doctoral research proposal that aims to automate the process of identifying commit-level changes in software projects using software repository mining and code representation learning models. The research background, state-of-the-art, research objectives, research agenda, and threats to validity are discussed."
Using Automatic Program Assessment in a Software Development Project Course,"Borghoff, Uwe M. and Minas, Mark and M\""{o}nch, Kim",10.1145/3593663.3593669,2023,"Education in software engineering is a must in all computer science courses. At the Institute for Software Technology, we are responsible for all teaching in this area. For many years we have developed and continuously refined a software development project course to serve these topics. The COVID-19 pandemic had a particular impact on our project course and the way we taught during that time. We describe how we adapted the course to these new conditions. In particular, we used an automated program assessment system that helped us keep the difficulty of all assignments constant for the different student teams while maintaining the motivation of the individual students on each team. In this paper, we demonstrate that our approach was robust even in an emergency remote teaching (ERT) environment, is based on a continuous improvement process, feedback evaluation, and process adaptation, and will continue at an improved level in the post-corona era. Students report that the course had been fun."
Postcards from the Future: Speculating the Future of Built Environments with Citizens,"Paraschivoiu, Irina and Dziabiola, Marta and Meschtscherjakov, Alexander",10.1145/3593743.3593784,2023,"The integration of novel technologies in built environments has increased the complexity of designing cities and spaces with citizens. Unobtrusive and invisible interfaces can lead to new forms of spatial experiences, but also form a black box which is not comprehensible to a wide audience. To address this challenge, we present the results of a study where we employed co-design fiction. By engaging 66 citizens in the critical assessment of futuristic designs of spaces, we reveal their needs and concerns about possible futures. Our findings show that citizens are concerned with the ecological limitations of urban technology, reliability, and data privacy. They also welcome the positive implications of adaptive environments and technological advancement. Participants were also reflective of how urban technology can change human experience, more generally. Our findings point to the need for flexibility, privacy, and adaptability to changing contexts, stimuli, and users in human-building interaction."
Argument Mining with Graph Representation Learning,"Zhang, Gechuan and Nulty, Paul and Lillis, David",10.1145/3594536.3595152,2023,"Argument Mining (AM) is a unique task in Natural Language Processing (NLP) that targets arguments: a meaningful logical structure in human language. Since the argument plays a significant role in the legal field, the interdisciplinary study of AM on legal texts has significant promise. For years, a pipeline architecture has been used as the standard paradigm in this area. Although this simplifies the development and management of AM systems, the connection between different parts of the pipeline causes inevitable shortcomings such as cascading error propagation.This paper presents an alternative perspective of the AM task, whereby legal documents are represented as graph structures and the AM task is undertaken as a hybrid approach incorporating Graph Neural Networks (GNNs), graph augmentation and collective classification. GNNs have been demonstrated to be an effective method for representation learning on graphs, and they have been successfully applied to many other NLP tasks. In contrast to previous pipeline-based architecture, our approach results in a single end-to-end classifier for the identification and classification of argumentative text segments. Experiments based on corpora from both the European Court of Human Rights (ECHR) and the Court of Justice of the European Union (CJEU) show that our approach achieves strong results compared to state-of-the-art baselines. Both the graph augmentation and collective classification steps are shown to improve performance on both datasets when compared to using GNNs alone."
Gesture-based Interaction for AR Systems: A Short Review,"Gavgiotaki, Despoina and Ntoa, Stavroula and Margetis, George and Apostolakis, Konstantinos C. and Stephanidis, Constantine",10.1145/3594806.3594815,2023,"Gesture-based interaction constitutes the prominent interaction modality for Augmented Reality (AR) systems. In this state-of-the-art review, we classify the recent literature on hand gesture-based AR interaction techniques, concerning the various domains where AR is gaining a foothold. Based on the conducted analysis, a taxonomy is introduced for the classification of the various reported techniques on the grounds of their application domain, the context of use, gestures recognition method, and types of gestures used. Furthermore, the discussed analysis reveals that context of use is a factor to take into account when determining the gestures that should be employed when developing an AR application. At the same time, there is a trade-off between touch-based and mid-air gestures, regarding their simplicity and user engagement entailed, which needs to be carefully considered. In all cases, it is of utmost importance that AR gesture-based systems are designed following a human-centered approach. The analysis conducted and reported in this paper aims to motivate the informed development and integration of hand gesture interaction of AR applications in various contexts and provide information on the different approaches that can be employed."
Facial Emotion Recognition in Immersive Virtual Reality: A Systematic Literature Review,"Ortmann, Thorben and Wang, Qi and Putzar, Larissa",10.1145/3594806.3594861,2023,"With the broader adoption of virtual reality (VR), objective physiological measurements to automatically assess a user’s emotional state are gaining importance. Emotions affect human behavior, perception, cognition, and decision-making. Their recognition allows analysis of VR experiences and enables systems to react to and interact with a user’s emotions. Facial expressions are one of the most potent and natural signals to recognize emotions. Automatic facial expression recognition (FER) typically relies on facial images. However, users wear head-mounted displays (HMDs) in immersive VR environments, which occlude almost the entire upper half of the face. That severely limits the capabilities of conventional FER methods. We address this emerging challenge with our systematic literature review. To our knowledge, it is the first review on FER in immersive VR scenarios where HMDs partially occlude a user’s face. We identified 256 related works and included 21 for detailed analysis. Our review provides a comprehensive overview of the state-of-the-art and draws conclusions for future research."
A Large-Scale Survey on the Usability of AI Programming Assistants: Successes and Challenges,"Liang, Jenny T. and Yang, Chenyang and Myers, Brad A.",10.1145/3597503.3608128,2024,"The software engineering community recently has witnessed widespread deployment of AI programming assistants, such as GitHub Copilot. However, in practice, developers do not accept AI programming assistants' initial suggestions at a high frequency. This leaves a number of open questions related to the usability of these tools. To understand developers' practices while using these tools and the important usability challenges they face, we administered a survey to a large population of developers and received responses from a diverse set of 410 developers. Through a mix of qualitative and quantitative analyses, we found that developers are most motivated to use AI programming assistants because they help developers reduce key-strokes, finish programming tasks quickly, and recall syntax, but resonate less with using them to help brainstorm potential solutions. We also found the most important reasons why developers do not use these tools are because these tools do not output code that addresses certain functional or non-functional requirements and because developers have trouble controlling the tool to generate the desired output. Our findings have implications for both creators and users of AI programming assistants, such as designing minimal cognitive effort interactions with these tools to reduce distractions for users while they are programming."
How to Support ML End-User Programmers through a Conversational Agent,"Arteaga Garcia, Emily Judith and Nicolaci Pimentel, Jo\~{a}o Felipe and Feng, Zixuan and Gerosa, Marco and Steinmacher, Igor and Sarma, Anita",10.1145/3597503.3608130,2024,"Machine Learning (ML) is increasingly gaining significance for enduser programmer (EUP) applications. However, machine learning end-user programmers (ML-EUPs) without the right background face a daunting learning curve and a heightened risk of mistakes and flaws in their models. In this work, we designed a conversational agent named ""Newton"" as an expert to support ML-EUPs. Newton's design was shaped by a comprehensive review of existing literature, from which we identified six primary challenges faced by ML-EUPs and five strategies to assist them. To evaluate the efficacy of Newton's design, we conducted a Wizard of Oz within-subjects study with 12 ML-EUPs. Our findings indicate that Newton effectively assisted ML-EUPs, addressing the challenges highlighted in the literature. We also proposed six design guidelines for future conversational agents, which can help other EUP applications and software engineering activities."
"Characterizing Software Maintenance Meetings: Information Shared, Discussion Outcomes, and Information Captured","Soria, Adriana Meza and Lopez, Taylor and Seero, Elizabeth and Mashhadi, Negin and Evans, Emily and Burge, Janet and Van der Hoek, Andr\'{e}",10.1145/3597503.3623330,2024,"A type of meeting that has been understudied in the software engineering literature to date is what we term the software maintenance meeting: a regularly scheduled team meeting in which emergent issues are addressed that are usually out of scope of the daily stand-up but not necessarily challenging enough to warrant an entirely separate meeting. These meetings tend to discuss a wide variety of topics and are crucial in keeping software development projects going, but little is known about these meetings and how they proceed. In this paper, we report on a single exploratory case study in which we analyzed ten consecutive maintenance meetings from a major healthcare software provider. We analyzed what kind of information is brought into the discussions held in these meetings and how, what outcomes arose from the discussions, and what information was captured for downstream use. Our findings are varied, giving rise to both practical considerations for those conducting these kinds of meetings and new research directions toward further understanding and supporting them."
BinaryAI: Binary Software Composition Analysis via Intelligent Binary Source Code Matching,"Jiang, Ling and An, Junwen and Huang, Huihui and Tang, Qiyi and Nie, Sen and Wu, Shi and Zhang, Yuqun",10.1145/3597503.3639100,2024,"While third-party libraries (TPLs) are extensively reused to enhance productivity during software development, they can also introduce potential security risks such as vulnerability propagation. Software composition analysis (SCA), proposed to identify reused TPLs for reducing such risks, has become an essential procedure within modern DevSecOps. As one of the mainstream SCA techniques, binary-to-source SCA identifies the third-party source projects contained in binary files via binary source code matching, which is a major challenge in reverse engineering since binary and source code exhibit substantial disparities after compilation. The existing binary-to-source SCA techniques leverage basic syntactic features that suffer from redundancy and lack robustness in the large-scale TPL dataset, leading to inevitable false positives and compromised recall. To mitigate these limitations, we introduce BinaryAI, a novel binary-to-source SCA technique with two-phase binary source code matching to capture both syntactic and semantic code features. First, BinaryAI trains a transformer-based model to produce function-level embeddings and obtain similar source functions for each binary function accordingly. Then by applying the link-time locality to facilitate function matching, BinaryAI detects the reused TPLs based on the ratio of matched source functions. Our experimental results demonstrate the superior performance of BinaryAI in terms of binary source code matching and the downstream SCA task. Specifically, our embedding model outperforms the state-of-the-art model CodeCMR, i.e., achieving 22.54% recall@1 and 0.34 MRR compared with 10.75% and 0.17 respectively. Additionally, BinaryAI outperforms all existing binary-to-source SCA tools in TPL detection, increasing the precision from 73.36% to 85.84% and recall from 59.81% to 64.98% compared with the well-recognized commercial SCA product Black Duck.https://www.binaryai.net"
Strengthening Supply Chain Security with Fine-grained Safe Patch Identification,"Luo, Changhua and Meng, Wei and Wang, Shuai",10.1145/3597503.3639104,2024,"Enhancing supply chain security is crucial, often involving the detection of patches in upstream software. However, current security patch analysis works yield relatively low recall rates (i.e., many security patches are missed). In this work, we offer a new solution to detect safe patches and assist downstream developers in patch propagation. Specifically, we develop SPatch to detect fine-grained safe patches. SPatch leverages fine-grained patch analysis and a new differential symbolic execution technique to analyze the functional impacts of code changes.We evaluated SPatch on various software, including the Linux kernel and OpenSSL, and demonstrated that it outperformed existing methods in detecting safe patches, resulting in observable security benefits. In our case studies, we updated hundreds of functions in modern software using safe patches detected by SPatch without causing any regression issues. Our detected safe security patches have been merged into the latest version of downstream software like ProtonVPN."
JLeaks: A Featured Resource Leak Repository Collected From Hundreds of Open-Source Java Projects,"Liu, Tianyang and Ji, Weixing and Dong, Xiaohui and Yao, Wuhuang and Wang, Yizhuo and Liu, Hui and Peng, Haiyang and Wang, Yuxuan",10.1145/3597503.3639162,2024,"High-quality defect repositories are vital in defect detection, localization, and repair. However, existing repositories collected from open-source projects are either small-scale or inadequately labeled and packed. This paper systematically summarizes the programming APIs of system resources (i.e., file, socket, and thread) in Java. Additionally, this paper demonstrates the exceptions that may cause resource leaks in the chained and nested streaming operations. A semi-automatic toolchain is built to improve the efficiency of defect extraction, including automatic building for large legacy Java projects. Accordingly, 1,094 resource leaks were collected from 321 open-source projects on GitHub. This repository, named JLeaks, was built by round-by-round filtering and cross-validation, involving the review of approximately 3,185 commits from hundreds of projects. JLeaks is currently the largest resource leak repository, and each defect in JLeaks is well-labeled and packed, including causes, locations, patches, source files, and compiled bytecode files for 254 defects. We have conducted a detailed analysis of JLeaks for defect distribution, root causes, and fix approaches. We compare JLeaks with two well-known resource leak repositories, and the results show that JLeaks is more informative and complete, with high availability, uniqueness, and consistency. Additionally, we show the usability of JLeaks in two application scenarios. Future studies can leverage our repository to encourage better design and implementation of defect-related algorithms and tools."
The Classics Never Go Out of Style: An Empirical Study of Downgrades from the Bazel Build Technology,"Alfadel, Mahmoud and McIntosh, Shane",10.1145/3597503.3639169,2024,"Software build systems specify how source code is transformed into deliverables. Keeping build systems in sync with the software artifacts that they build while retaining their capacity to quickly produce updated deliverables requires a serious investment of development effort. Enticed by advanced features, several software teams have migrated their build systems to a modern generation of build technologies (e.g., Bazel, Buck), which aim to reduce the maintenance and execution overhead that build systems impose on development. However, not all migrations lead to perceived improvements, ultimately culminating in abandonment of the build technology. While prior work has focused on upward migration towards more advanced technologies, so-called downgrades, i.e., abandonment of a modern build technology in favour of a traditional one, remains largely unexplored.In this paper, we perform an empirical study to better understand the abandonment of Bazel---a modern build technology with native support for multi-language software projects and (local/distributed) artifact caching. Our investigation of 542 projects that adopt Bazel reveals that (1) 61 projects (11.2%) have abandoned Bazel; and (2) abandonment tends to occur after investing in Bazel for a substantial amount of time (a median of 638 days). Thematic analysis reveals seven recurring reasons for abandonment, such as technical challenges, lack of platform integration, team coordination issues, and upstream trends. After abandoning Bazel, the studied projects have adopted a broad set of alternatives, spanning from language-specific tools like Go Build, to more traditional build technologies like CMake and even pure Make. These results demonstrate that choosing a build technology involves balancing tradeoffs that are not always optimized by adopting the latest technology. This paper also lays the foundation for future work on balancing the tradeoffs that are associated with build technology choice (e.g., feature richness vs. maintenance costs) and the development of tools to support migration away from modern technologies."
Uncovering the Causes of Emotions in Software Developer Communication Using Zero-shot LLMs,"Imran, Mia Mohammad and Chatterjee, Preetha and Damevski, Kostadin",10.1145/3597503.3639223,2024,"Understanding and identifying the causes behind developers' emotions (e.g., Frustration caused by 'delays in merging pull requests') can be crucial towards finding solutions to problems and fostering collaboration in open-source communities. Effectively identifying such information in the high volume of communications across the different project channels, such as chats, emails, and issue comments, requires automated recognition of emotions and their causes. To enable this automation, large-scale software engineering-specific datasets that can be used to train accurate machine learning models are required. However, such datasets are expensive to create with the variety and informal nature of software projects' communication channels.In this paper, we explore zero-shot LLMs that are pre-trained on massive datasets but without being fine-tuned specifically for the task of detecting emotion causes in software engineering: ChatGPT, GPT-4, and flan-alpaca. Our evaluation indicates that these recently available models can identify emotion categories when given detailed emotions, although they perform worse than the top-rated models. For emotion cause identification, our results indicate that zero-shot LLMs are effective at recognizing the correct emotion cause with a BLEU-2 score of 0.598. To highlight the potential use of these techniques, we conduct a case study of the causes of Frustration in the last year of development of a popular open-source project, revealing several interesting insights."
Shedding Light on Software Engineering-specific Metaphors and Idioms,"Imran, Mia Mohammad and Chatterjee, Preetha and Damevski, Kostadin",10.1145/3597503.3639585,2024,"Use of figurative language, such as metaphors and idioms, is common in our daily-life communications, and it can also be found in Software Engineering (SE) channels, such as comments on GitHub. Automatically interpreting figurative language is a challenging task, even with modern Large Language Models (LLMs), as it often involves subtle nuances. This is particularly true in the SE domain, where figurative language is frequently used to convey technical concepts, often bearing developer affect (e.g., 'spaghetti code). Surprisingly, there is a lack of studies on how figurative language in SE communications impacts the performance of automatic tools that focus on understanding developer communications, e.g., bug prioritization, incivility detection. Furthermore, it is an open question to what extent state-of-the-art LLMs interpret figurative expressions in domain-specific communication such as software engineering. To address this gap, we study the prevalence and impact of figurative language in SE communication channels. This study contributes to understanding the role of figurative language in SE, the potential of LLMs in interpreting them, and its impact on automated SE communication analysis. Our results demonstrate the effectiveness of fine-tuning LLMs with figurative language in SE and its potential impact on automated tasks that involve affect. We found that, among three state-of-the-art LLMs, the best improved fine-tuned versions have an average improvement of 6.66% on a GitHub emotion classification dataset, 7.07% on a GitHub incivility classification dataset, and 3.71% on a Bugzilla bug report prioritization dataset."
"Ten regulatory principles to scaffold the design, manufacture, and use of trustworthy autonomous systems, illustrated in a maritime context","Horne, Rachel and Law-Walsh, Caroline and Assaad, Zena and Joiner, Keith",10.1145/3597512.3599701,2023,"Autonomous systems are increasingly prevalent around the world, with the benefits related to safety, efficiency, and sustainability attractive in addition to the opportunity to establish entirely new capabilities. In order to operationalise autonomous systems technology it is critical to have in place the legal, regulatory and ethical infrastructure necessary to enable safe and trusted operation. While there is a growing body of literature regarding ethical Artificial Intelligence (AI), there is a need for more academic exploration of legal and regulatory best practice for autonomous systems used in commercial and defence contexts. This paper addresses that literature gap by considering the role of regulation and its relationship with trust in a multi-disciplinary context, before proposing 10 principles to base regulatory development and implementation on. These principles, Trust-centred; Collaborative; Risk-based; Evidence-led; Facilitate experimentation; Systems-focussed; Usable; Consistent; Adaptable and Reviewable, collectively provide a domain and technology agnostic basis for a regulatory framework development and implementation approach that supports the design, manufacture and operation of safe and trusted autonomous systems. The paper concludes by recommending next steps towards the regulation of safe and trusted autonomous systems, including a focus on collaboration and experimentation."
Ethics of Trust/worthiness in Autonomous Systems: a scoping review.,"Smith, Dr Helen and Manzini, Dr Arianna and Kennedy, Dr Mari-Rose and Ives, Prof Jonathan",10.1145/3597512.3600207,2023,"The development of adaptive autonomous systems with evolving functionality (AASEFs) differs from their technological predecessors due to their changing, rather than static, architectures and processes; subsequently, their development, deployment, and implementation creates novel ethical issues.Our scoping review surveys the literature to identify the problematic nature of AASEFs, the ethical worries that they generate, and the ethical principles affected. Our literature examination also ascertains stakeholder needs and solutions regarding the trust and trustworthiness of ASSEFs.The characteristics of non-explicability and fluctuating behaviour are predominantly problematic of AASEFs. The literature advocates for AASEF development incorporating ‘ethics by design’ with enforceable standards supported via regulatory and legal structures.."
A Large-Scale Mixed-Methods Analysis of Blind and Low-vision Research in ACM and IEEE,"Thoo, Yong-Joon and Jeanneret Medina, Maximiliano and Froehlich, Jon E. and Ruffieux, Nicolas and Lalanne, Denis",10.1145/3597638.3608412,2023,"Technologies for blind and low-vision (BLV) people have long been a focus of Human-Computer Interaction (HCI) and accessibility (ASSETS) research. To map and assess this cross-disciplinary field, prior literature reviews have focused on specific BLV research areas (e.g., navigation assistance) or study methodologies (e.g., qualitative methods). In this paper, we provide a more holistic examination, combining both quantitative bibliometric analyses with qualitative assessments. Using keyword queries of terms focused on the human (e.g., people) and their visual status (e.g., blind, low-vision), we first derived a dataset of 880 papers published between 2010-2022 from ACM and IEEE conferences and journals. We then apply a programmatic analysis of this dataset followed by a qualitative analysis of the 100 most-cited papers. Our findings highlight four major research areas: Accessibility at Home &amp; on the Go, Non-Visual Interaction, Orientation &amp; Mobility, and Education. We also capture the diversity of denominations used to refer to the BLV community and their co-occurrences, as well as computer systems targeting both blind and low-vision users with a focus on visual substitution. We close by suggesting areas for future work and hope to stimulate discussions in our field."
Virtual Reality (VR) Automated Testing in the Wild: A Case Study on Unity-Based VR Applications,"Rzig, Dhia Elhaq and Iqbal, Nafees and Attisano, Isabella and Qin, Xue and Hassan, Foyzul",10.1145/3597926.3598134,2023,"Virtual Reality (VR) is an emerging technique that provides a unique real-time experience for users. VR technologies have provided revolutionary user experiences in various scenarios (e.g., training, education, gaming, etc.). However, testing VR applications is challenging due to their nature which necessitates physical interactivity, and their reliance on specific hardware systems. Despite the recent advancements in VR technology and its usage scenarios, we still know little about VR application testing. To fill up this knowledge gap, we performed an empirical study on 314 open-source VR applications. Our analysis identified that 79% of the VR projects evaluated did not have any automatic tests, and for the VR projects that did, the median functional-method to test-method ratios were lower than those of other project types. Moreover, we uncovered tool support issues concerning the measurement of VR code coverage, and the assertion density results we were able to generate were relatively low, with an average of 17.63%. Finally, through a manual analysis of 370 test cases, we identified the different categories of test cases being used to validate VR application quality attributes. Furthermore, we extracted which of these categories are VR-attention, meaning that test writers need to pay special attention to VR characteristics when writing tests of these categories. We believe that our findings constitute a call to action for the VR development community to improve their automatic testing practices and provide directions for software engineering researchers to develop advanced techniques for automatic test case generation and test quality analysis for VR applications. Our replication package containing the dataset we used, software tools we developed, and the results we found, is accessible at ‍https://doi.org/10.6084/m9.figshare.19678938."
Third-Party Library Dependency for Large-Scale SCA in the C/C++ Ecosystem: How Far Are We?,"Jiang, Ling and Yuan, Hengchen and Tang, Qiyi and Nie, Sen and Wu, Shi and Zhang, Yuqun",10.1145/3597926.3598143,2023,"Existing software composition analysis (SCA) techniques for the C/C++ ecosystem tend to identify the reused components through feature matching between target software project and collected third-party libraries (TPLs). However, feature duplication caused by internal code clone can cause inaccurate SCA results. To mitigate this issue, Centris, a state-of-the-art SCA technique for the C/C++ ecosystem, was proposed to adopt function-level code clone detection to derive the TPL dependencies for eliminating the redundant features before performing SCA tasks. Although Centris has been shown effective in the original paper, the accuracy of the derived TPL dependencies is not evaluated. Additionally, the dataset to evaluate the impact of TPL dependency on SCA is limited. To further investigate the efficacy and limitations of Centris, we first construct two large-scale ground-truth datasets for evaluating the accuracy of deriving TPL dependency and SCA results respectively. Then we extensively evaluate Centris where the evaluation results suggest that the accuracy of TPL dependencies derived by Centris may not well generalize to our evaluation dataset. We further infer the key factors that degrade the performance can be the inaccurate function birth time and the threshold-based recall. In addition, the impact on SCA from the TPL dependencies derived by Centris can be somewhat limited. Inspired by our findings, we propose TPLite with function-level origin TPL detection and graph-based dependency recall to enhance the accuracy of TPL reuse detection in the C/C++ ecosystem. Our evaluation results indicate that TPLite effectively increases the precision from 35.71% to 88.33% and the recall from 49.44% to 62.65% of deriving TPL dependencies compared with Centris. Moreover, TPLite increases the precision from 21.08% to 75.90% and the recall from 57.62% to 64.17% compared with the SOTA academic SCA tool B2SFinder and even outperforms the well-adopted commercial SCA tool BDBA, i.e., increasing the precision from 72.46% to 75.90% and the recall from 58.55% to 64.17%."
"Replication and UpScaling of Smart Cities in Academia and Practice: Concepts, Barriers and Enablers","Soe, Ralf-Martin",10.1145/3598469.3598481,2023,"In the case of smart cities, a transference of knowledge and technologies is often assumed to be a key for leveraging successful solutions. However, previous studies and projects have indicated several barriers and enablers regarding to upscaling – a key focus of this paper that aims to contribute both to research and practice. This paper focuses on analysing the knowledge and research gap between the academic literature, via systematic literature review, compared with an empirical experience of 10 urban UpScaling projects, collected via survey among city networks, research organisations and urban stakeholders."
Legal and Ethical Implications of Data Processing in Sex-Positive Techno Parties: the case of ZusammenKommen,"Kenner, Zsofia",10.1145/3598469.3598497,2023,"This paper describes how the organizers of large sex-positive parties make use of digital systems to provide a safe and governed space for their libertine guests. It describes the legal and ethical implications of processing (including storing) person-identifying and sensible type of data about the sexuality of individuals who apply for entry into such parties. Based on the case of the so-called ZusammenKommen party series in Vienna, it describes the technical and organizational safeguards used to assure the protection of data and privacy of the guests. It further discusses which other methods could be developed and deployed to increase even further the privacy of its guests."
Designing public participation in the digital age: Lessons learned from using the policy cycle in an Austrian case study,"Edelmann, Noella and Albrecht, Valerie",10.1145/3598469.3598502,2023,"Social media, digital technologies, new online tools, and digital transformation processes in public sector organisations have led to the introduction of innovations and thus significant changes to public participation projects and processes. In this case study we study the impact of digital innovation on public participation processes by using the policy cycle phases. Multiple methods were used to collect expertise on how to develop participatory processes and the CoVID-19 lockdown in Austria represented an opportunity to test how to flexibly use digital tools in participation processes. The outcome provides an overview of lessons learned for designing participation processes flexibly drawing on different participation formats and digital tools."
Research and Development Agenda for the Use of AI in Parliaments,"von Lucke, Jorn and Fitsilis, Fotios and Etscheid, Jan",10.1145/3598469.3598517,2023,"Parliaments can use artificial intelligence (AI) and AI-based applications for their work. However, there are currently few concrete visions for the use of AI in parliamentary work. A broad research and development agenda is needed for professional use of AI in parliaments. This study takes a two-step approach that includes brainstorming with experts and workshops with selected national parliaments to gain an overview of the application areas of AI in parliaments and to create a roadmap for AI adoption. The result is a list of 210 proposals for the use of AI in parliaments that will serve as a foundation for academia and the parliamentary community to constructively shape parliamentary work with AI."
"Navigating Information Technology Challenges and Priorities: Expanding Responsibilities, Growing Roles, and Evolving Contexts for City Leaders","Buyannemekh, Battulga and Cook, Meghan E.",10.1145/3598469.3598523,2023,"Governments have adopted a range of information communication technologies (ICTs) to transform how they operate, provide services, collaborate with stakeholders, and innovate to create and add value. Some of these efforts are part of digital transformation and are influenced by diverse organizational factors. In this management paper, we focus on one of these factors, namely, the ability of leaders to adequately staff and resource their information technology (IT) departments so they can meet the challenges and priorities within their cities. In addition to a review of both practitioner and academic literature, we conducted a survey and hosted a post-survey discussion session with IT leaders in nine cities in New York State to elicit thoughts on the evolution of IT roles and the implications thereof on their respective digital transformation journeys. Based on the analysis, we propose practical considerations for government leaders to collectively address pressing issues and plan for the future in which the value of technology is maximized."
Where's the Data? Finding and Reusing Datasets in Computing Education,"Kiesler, Natalie and Impagliazzo, John and Biernacka, Katarzyna and Kapoor, Amanpreet and Kazmi, Zain and Ramagoni, Sujeeth Goud and Sane, Aamod and Tran, Keith and Taneja, Shubbhi and Wu, Zihan",10.1145/3598579.3689378,2024,"Computing education research (CER) is a rapidly advancing discipline, offering vast potential for data-driven, secondary research or replication studies. Although gathering and analyzing data for research seem straightforward, making research data publicly available to the community remains a challenge. Likewise, finding and reusing high-quality, prominent, and well-documented research data proves to be a daunting task. In this working group paper, the authors present their search for available datasets in the CER context (e.g., in databases and repositories). The available datasets are further analyzed using a newly developed metadata scheme and presented to the community as a resource. The second component of this work is a summary of the community's perspective and concerns on publishing their research data, which has been gathered through a survey among 52 computing education researchers. Based on this status quo, this report presents recommendations for measures and future steps for the community to become more accessible and establish open data practices. We thus emphasize the potential of making research data available to enhance productivity, transparency, and reproducibility in the CER community."
A Generic IoT Quantum-Safe Watchdog Timer Protocol,"Eckel, Michael and Gutsche, Tanja and Lauer, Hagen and Rein, Andr\'{e}",10.1145/3600160.3605169,2023,"This paper presents a quantum-safe watchdog timer protocol designed and implemented using various quantum-safe digital signature algorithms. The protocol is specifically tailored to be used in the context of the Internet of Things (IoT) to address the security risks posed by quantum computing to classical protocols. Our approach replaces the classical protocol with a quantum-safe watchdog timer protocol, which ensures that an IoT device’s communication channels remain secure from adversarial attacks. To demonstrate the effectiveness of our proposed protocol, we develop a proof-of-concept (PoC) implementation using an actor framework in Python. We evaluate the performance impact of the proposed protocol based on several IoT scenarios. We also compare the performance of different quantum-safe algorithms using measurements of CPU cycles, and quantitatively evaluate the results using statistical methods. Our results indicate that the performance of the tested quantum-safe algorithms is better or similar to that of the tested classical algorithms. Based on these results, we recommend a specific quantum-safe algorithm for use with the watchdog timer protocol in the IoT context. The proposed protocol and recommended quantum-safe algorithm offer an effective way to address the security risks posed by quantum computing to IoT devices, and are a significant contribution to the field of quantum-safe cryptography."
Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm Reduction,"Shelby, Renee and Rismani, Shalaleh and Henne, Kathryn and Moon, AJung and Rostamzadeh, Negar and Nicholas, Paul and Yilla-Akbari, N'Mah and Gallegos, Jess and Smart, Andrew and Garcia, Emilio and Virk, Gurleen",10.1145/3600211.3604673,2023,"Understanding the landscape of potential harms from algorithmic systems enables practitioners to better anticipate consequences of the systems they build. It also supports the prospect of incorporating controls to help minimize harms that emerge from the interplay of technologies and social and cultural dynamics. A growing body of scholarship has identified a wide range of harms across different algorithmic technologies. However, computing research and practitioners lack a high level and synthesized overview of harms from algorithmic systems. Based on a scoping review of computing research (n=172), we present an applied taxonomy of sociotechnical harms to support a more systematic surfacing of potential harms in algorithmic systems. The final taxonomy builds on and refers to existing taxonomies, classifications, and terminologies. Five major themes related to sociotechnical harms — representational, allocative, quality-of-service, interpersonal harms, and social system/societal harms — and sub-themes are presented along with a description of these categories. We conclude with a discussion of challenges and opportunities for future research."
A Systematic Review of Ethical Concerns with Voice Assistants,"Seymour, William and Zhan, Xiao and Cot\'{e}, Mark and Such, Jose",10.1145/3600211.3604679,2023,"Since Siri’s release in 2011 there have been a growing number of AI-driven domestic voice assistants that are increasingly being integrated into devices such as smartphones and TVs. But as their presence has expanded, a range of ethical concerns have been identified around the use of voice assistants, such as the privacy implications of having devices that are always listening and the ways that these devices are integrated into the existing social order of the home. This has created a burgeoning area of research across a range of fields including computer science, social science, and psychology. This paper takes stock of the foundations and frontiers of this work through a systematic literature review of 117 papers on ethical concerns with voice assistants. In addition to analysis of nine specific areas of concern, the review measures the distribution of methods and participant demographics across the literature. We show how some concerns, such as privacy, are operationalized to a much greater extent than others like accessibility, and how study participants are overwhelmingly drawn from a small handful of Western nations. In so doing we hope to provide an outline of the rich tapestry of work around these concerns and highlight areas where current research efforts are lacking."
The Ethical Implications of Generative Audio Models: A Systematic Literature Review,"Barnett, Julia",10.1145/3600211.3604686,2023,"Generative audio models typically focus their applications in music and speech generation, with recent models having human-like quality in their audio output. This paper conducts a systematic literature review of 884 papers in the area of generative audio models in order to both quantify the degree to which researchers in the field are considering potential negative impacts and identify the types of ethical implications researchers in this area need to consider. Though 65% of generative audio research papers note positive potential impacts of their work, less than 10% discuss any negative impacts. This jarringly small percentage of papers considering negative impact is particularly worrying because the issues brought to light by the few papers doing so are raising serious ethical implications and concerns relevant to the broader field such as the potential for fraud, deep-fakes, and copyright infringement. By quantifying this lack of ethical consideration in generative audio research and identifying key areas of potential harm, this paper lays the groundwork for future work in the field at a critical point in time in order to guide more conscientious research as this field progresses."
"Democratising AI: Multiple Meanings, Goals, and Methods","Seger, Elizabeth and Ovadya, Aviv and Siddarth, Divya and Garfinkel, Ben and Dafoe, Allan",10.1145/3600211.3604693,2023,"Numerous parties are calling for “the democratisation of AI”, but the phrase is used to refer to a variety of goals, the pursuit of which sometimes conflict. This paper identifies four kinds of “AI democratisation” that are commonly discussed: (1) the democratisation of AI use, (2) the democratisation of AI development, (3) the democratisation of AI profits, and (4) the democratisation of AI governance. Numerous goals and methods of achieving each form of democratisation are discussed. The main takeaway from this paper is that AI democratisation is a multifarious and sometimes conflicting concept that should not be conflated with improving AI accessibility. If we want to move beyond ambiguous commitments to “democratising AI”, to productive discussions of concrete policies and trade-offs, then we need to recognise the principal role of the democratisation of AI governance in navigating tradeoffs and risks across decisions around use, development, and profits."
Disambiguating Algorithmic Bias: From Neutrality to Justice,"Edenberg, Elizabeth and Wood, Alexandra",10.1145/3600211.3604695,2023,"As algorithms have become ubiquitous in consequential domains, societal concerns about the potential for discriminatory outcomes have prompted urgent calls to address algorithmic bias. In response, a rich literature across computer science, law, and ethics is rapidly proliferating to advance approaches to designing fair algorithms. Yet computer scientists, legal scholars, and ethicists are often not speaking the same language when using the term ‘bias.’ Debates concerning whether society can or should tackle the problem of algorithmic bias are hampered by conflations of various understandings of bias, ranging from neutral deviations from a standard to morally problematic instances of injustice due to prejudice, discrimination, and disparate treatment. This terminological confusion impedes efforts to address clear cases of discrimination. In this paper, we examine the promises and challenges of different approaches to disambiguating bias and designing for justice. While both approaches aid in understanding and addressing clear algorithmic harms, we argue that they also risk being leveraged in ways that ultimately deflect accountability from those building and deploying these systems. Applying this analysis to recent examples of generative AI, our argument highlights unseen dangers in current methods of evaluating algorithmic bias and points to ways to redirect approaches to addressing bias in generative AI at its early stages in ways that can more robustly meet the demands of justice."
What does it mean to be a responsible AI practitioner: An ontology of roles and skills,"Rismani, Shalaleh and Moon, AJung",10.1145/3600211.3604702,2023,"With the growing need to regulate AI systems across a wide variety of application domains, a new set of occupations has emerged in the industry. The so-called responsible Artificial Intelligence (AI) practitioners or AI ethicists are generally tasked with interpreting and operationalizing best practices for ethical and safe design of AI systems. Due to the nascent nature of these roles, however, it is unclear to future employers and aspiring AI ethicists what specific function these roles serve and what skills are necessary to serve the functions. Without clarity on these, we cannot train future AI ethicists with meaningful learning objectives. In this work, we examine what responsible AI practitioners do in the industry and what skills they employ on the job. We propose an ontology of existing roles alongside skills and competencies that serve each role. We created this ontology by examining the job postings for such roles over a two-year period (2020-2022) and conducting expert interviews with fourteen individuals who currently hold such a role in the industry. Our ontology contributes to business leaders looking to build responsible AI teams and provides educators with a set of competencies that an AI ethics curriculum can prioritize."
Legitimate Interest is the New Consent - Large-Scale Measurement and Legal Compliance of IAB Europe TCF Paywalls,"Morel, Victor and Santos, Cristiana and Fredholm, Viktor and Thunberg, Adam",10.1145/3603216.3624966,2023,"Cookie paywalls allow visitors of a website to access its content only after they make a choice between paying a fee or accept tracking. European Data Protection Authorities (DPAs) recently issued guidelines and decisions on paywalls lawfulness, but it is yet unknown whether websites comply with them. We study in this paper the prevalence of cookie paywalls on the top one million websites using an automatic crawler. We identify 431 cookie paywalls, all using the Transparency and Consent Framework (TCF). We then analyse the data these paywalls communicate through the TCF, and in particular, the legal grounds and the purposes used to collect personal data. We observe that cookie paywalls extensively rely on legitimate interest legal basis systematically conflated with consent. We also observe a lack of correlation between the presence of paywalls and legal decisions or guidelines by DPAs."
Augmented Reality and Machine Learning in Health: A Systematic Review,"Orji, Joseph Ufiem and Chan, Gerry and Orji, Rita",10.1145/3603421.3603430,2023,"Augmented Reality (AR) is a useful technology for providing an information-rich reality by superimposing digital objects and giving a virtual interpretation of the physical environment. AR has played a key role in reducing cognitive load and the applications of AR have been useful in various fields ranging from manufacturing, advertisement, education, military, and health. AR has also been deployed on various platforms like mobile, computer screens, and head-mounted displays (HMD). In this paper, we systematically reviewed research papers that have applied AR systems with machine learning (ML) in various health-related domains within the past 12 years (2010–2021). We present a review of the state-of-the-art AR implementation and research in the area of health by (1) identifying various AR approaches, (2) uncovering various areas of health where AR have been applied, (3) determining the current trend, gaps, and areas for future work, (4) highlighting the artificial intelligence (AI) and machine learning (ML) algorithms used in the AR systems and how they are used, and (5) comparing the different visualization modalities (web, mobile, and HMD). This review adds to the existing literature by shedding light on the common tools, successful approaches used in implementing previous AR projects, and evaluation methods. We uncover how AI and object tracking was implemented in AR for health. Finally, we identify gaps and offer recommendations for advancing research in this area."
"Implicit Smartphone Use Interventions to Promote Life-Technology Balance: An App-Market Survey, Design Space and the Case of Life-Relaunched","Terzimehi\'{c}, Na\dj{}a and Draxler, Fiona and Ahsanpour, Mariam and Schmidt, Albrecht",10.1145/3603555.3603578,2023,"The increasing emphasis on digital wellbeing (DW) underscores the significance of balancing technology use with other aspects of life. However, it remains unclear to what extent mobile DW apps support this balance by incorporating the life component. We conducted a systematic review of 152 mobile apps available on Google’s Playstore and found that current DW apps mainly focus on digital-centric actions, such as monitoring screen time and setting app limits, while overlooking everyday life activities that occur outside the smartphone. To address this limitation, we created Life-Relaunched, a smartphone launcher that adapts to users’ real-life contexts and activities implicitly. We then performed a preliminary study to examine the opportunities and challenges of incorporating implicit features into DW apps. The results of our study indicate that integrating implicit features can be advantageous in facilitating a balance between life and smartphone use. Nonetheless, it also emphasizes the challenges of user burden in establishing usage contexts initially."
Unlocking Business Value with AI-Driven End User Experience Management (EUEM),"Vijayakumar, Harsha",10.1145/3603955.3604004,2023,"ABSTRACT-Artificial Intelligence (AI) has been significant technology of the 21st century. This technology is changing every aspect of modern enterprise technology tooling, from strategies to selecting and implementing to adopting digital AI transformation. The rapid development of Artificial Intelligence has prompted many changes in the field of Information Technology (IT)’s End User Experience Management (EUEM). AI-powered end-user experience management (EUEM) involves the use of artificial intelligence and machine learning techniques to monitor and optimize the performance and user experience of an application or system. AI has empowered new vitality and addressed many challenges in EUEM. However, there is a literature gap on the Business Value Impact of Artificial intelligence (AI) Powered End User Experience Management which can help IT build optimized business resilience by creating value in complex and ever-changing environments to deliver an amazing experience to end users and elevate productivity. So, this research paper examines how EUEM creates business value and sustainability, basically how EUEM can help improve customer satisfaction, increase efficiency, enhance security, and improve decision-making. In turn, benefits can ultimately lead to a range of positive business outcomes, including increased revenue, reduced costs, and more."
Deep Learning object detection models: evolution and evaluation,"Bouraya, Sara and Belangour, Abdessamad",10.1145/3604078.3604126,2023,"Computer vision is a subfield of artificial intelligence that relies on training computers to obtain a high level of understanding of vision data. A computer vision system aims at identifying objects through the acquisition of their features such as textures, shapes, sizes, colors, spatial arrangement, to gain an exhaustive description of a video or an image. There are a lot of subfields of computer vision one of them is Object Detection. Detecting objects is a task to identify objects in a specific area. Over the last decades, object detection has gained attention due to its wide range of applications such as human motion analysis, robot navigation, event detection, anomaly detection, video surveillance, traffic analysis, and security. In this paper, we are going to introduce the different Object Detection methodologies and especially relied on Deep Learning based on two categories one stage detectors and two stage detectors. The main goal of this study is to detect, analyze and compare several detection methods and identify the best method based on different several performance metrics ranging from 2014 to 2021. The purpose of this paper to compare some of Object detection methodologies using Weighted Scoring Model (WSM). This covers, studying those algorithms, selecting relevant algorithms. The result of this comparison will show the best Object Detection methods applied on COCO dataset."
"Requirements Gathering Regarding Fine Motor Skills, Adaptive Difficulty and Executive Functions for a Game in Development for Therapy Sessions with Autistic Children to Encourage Collaboration","Azevedo, Guilherme and Gunsch, Mateus Ludwig and Lacerda, Mariane Gomes and Peixoto, Henrique De Oliveira and Borges, Luciana Correia L. F. and Nunes, Eunice P. Dos Santos",10.1145/3604321.3604345,2023,"Children with Autism Spectrum Disorder (ASD) are sensitive to external stimuli, including communication, interaction, cooperation, physical touch, unknown places, among others. Thus, daily tasks involving these situations can become stressful for a child with ASD. In order to mitigate this difficulty, therapeutic treatment is often used. Games, which can be used for many different purposes, can also be used as an Assistive Technology (AT) in the therapy process, in order to improve fine motor skills and executive functions and to encourage social communication and collaboration, which are very important skills in daily life. Furthermore, in order to be more attractive and efficient, these games can have adaptive difficulty based on Artificial Intelligence (AI) techniques. This paper aims to gather requirements about fine motor skills, adaptive difficulty and executive functions to a 3D game-based Virtual Environment (VE) under development to support therapeutic treatment of children with ASD. Therefore, a Systematic Review (SR) and a exploratory search were conducted to gather more information about the theme. The results indicate that Drag and Drop technique is the most used to practice fine motor skills in autistic children and that AI has been used in this context of games and Virtual Environments, making the games more independent in their execution and contributing with human labor. Moreover, the exploratory search conducted suggests that digital games may be a promising approach to improve fine motor skills, adaptability, and executive functions in children with ASD. The results of this paper will be used to implement features into the game mencioned early. Our research, to our knowledge, is the first review about these 3 topics (fine motor skills, adaptive difficulty and executive functions) applied together in game-based 3D VEs focusing to promote collaboration on therapy sessions of children with ASD."
Spatial Ambient Remapping,"Rocha, Henry Furquim and Braga, Gustavo Beltr\~{a}o and Braga, Pedro Luiz da Costa and Eller, Vitor Grando and Soares, Luciano Pereira",10.1145/3604479.3604494,2024,"A challenge in Spatial Computing considering markerless Augmented Reality is to anchor virtual objects relative to a physical space so that objects positions are stable across different tracking and augmented reality devices. This process must be stable in conditions that distinct devices can identify a real space by different strategies and that later the virtual objects appear in the same absolute location. This will also enable user to remotely edit an augmented environment using as reference a real environment. The goal of this research project is a platform that allows to remotely edit an augmented reality scenario referenced in a space previously digitized by a point cloud scan. Editing augmented environments allows users to add new virtual objects in a previously scanned physical space context remotely, enabling a new experience for the user who is interacting in augmented reality locally. A functional prototype of a desktop editor and a mobile application was developed that allows the visualization of environments remotely edited for Augmented Reality. Tests and simulations showed the feasibility of the proposed solution."
Identifying Gamification Project Challenges through Literature Review and Post-Mortem Documents,"Dwijaputra, Junialdi and Purwitasari, Diana and Zulaikha, Ellya",10.1145/3604571.3604574,2023,"Video game software development consists of multiple disciplines with their own unique challenges. The key to successful game development lies in effective requirements engineering, which involves managing teams with diverse roles and transitioning from preproduction to production stages. Therefore, gamification project that build by game developer also present challenges during development. The primary objective of this research is to answer research questions that investigate the obstacles encountered by game developers with regard to gamification projects. The research analyzed both literature reviews and company game postmortem evaluations to gather data on what went wrong during development. The results of the literature review indicated that there are four categories of game development challenges and eleven subcategories of challenges. The criteria are game development challenges, internal challenges, technical challenges, and schedule challenges. This study examines challenges in game development by analyzing 69 ""what went wrong"" sections from six project postmortem reports. It identifies four significant obstacles: game development, internal issues, technical aspects, and time management. 44% of the challenges were attributed to game development, followed by internal obstacles (27.5%), technical problems (18.8%), and scheduling (8.8%). A deeper investigation revealed subcategories within these primary obstacles, with lack of documentation being the most prevalent (21.7%), followed by team management issues (17.4%) and technological issues (13%). These findings are consistent with those of prior research, indicating a pattern in the categories of obstacles encountered in game development, specifically gamification initiatives. By recognizing and addressing these obstacles, game developers can navigate the complexities of the development process with greater ease."
Emotion-Aware Chatbot with Cultural Adaptation for Mitigating Work-Related Stress,"Ng, Shi Hui and Soon, Lay-Ki and Su, Tin Tin",10.1145/3604571.3604578,2023,"The accessibility to affordable and yet effective mental health support is limited due to various barriers. Given the proliferation of technology, chatbots for mental health support has been widely used. Being mindful of the users’ cultural background and the ability to respond with empathy are perceived as important factors that contribute to the usability and effective communication with chatbots. Nonetheless, cultural adaptation and emotional sensitivity in mental health chatbots are not thoroughly investigated. Hence, this work aims to design and implement an emotion-aware chatbot which incorporates cultural-adaptation that could provide effective Cognitive Behavioural Therapy (CBT) interventions to Malaysian community. The emotion detection model was developed using BERT and achieved an accuracy of 0.89. For cultural adaptation, besides localised contents, Google Cloud Translation API was used as the machine translation model between Malay to English. A user study was then carried out to assess the effectiveness of emotion sensitivity and cultural adaptation in CBT-based mental health support. The ablation study shows that CBT, cultural adaptation and emotional sensitivity have positive impact on the effectiveness and usability of mental health chatbots."
Common LeSS Transformation Patterns,"Buchalcevova, Alena and Hermanek, Jakub",10.1145/3605098.3635902,2024,"Various scaled agile frameworks have been developed to address the challenges of implementing agile methods in large-scale projects. Adoption of these frameworks is quite demanding. The paper aims to analyze selected case studies focused on the adoption of the Large Scaled Scrum (LeSS) framework through the usage of natural language processing. As a result, common patterns of LeSS transformations are presented, i.e., adoption patterns, success patterns, and challenges. So, the audience, i.e., agile coaches who work with the LeSS framework and potential LeSS adopters, can understand how they may approach the agile transformation and which mistakes to avoid."
Revisiting Technical Debt Types and Indicators for Software Systems,"Caglayan, Dilek and \""{O}zcan-Top, \""{O}zden",10.1145/3605098.3636043,2024,"Technical Debt (TD) 1 term in software systems was introduced over two decades ago and remains a critical concern in software development. It has the potential to evolve into a liability that necessitates refactoring or rewriting code over time. Regardless of its significance, there exists a notable gap in literature concerning a comprehensive list of technical debt indicators. The purpose of this study is to re-evaluate existing TD categorization and extend TD indicators and offer a complete and validated TD Type and TD Indicator list. In this study, we adopted a qualitative research approach and used mapping and expert opinion techniques as the research approach. The number of TD indicators extracted from existing formal literature was 60 which was extended to 92 by reviewing gray literature. This list was then subjected to the expert review, and with their feedback, grew by an additional 21%. Consequently, we present 10 distinct TD types, accompanied by 120 TD indicators that would aid in TD identification, resolution and minimizing the risks and costs associated with technical debt in software development."
How to Playfully Teach AI to Young Learners: a Systematic Literature Review,"Gennari, Rosella and Melonio, Alessandra and Pellegrino, Maria Angela and D'Angelo, Mauro",10.1145/3605390.3605393,2023,"Children are experiencing Artificial Intelligence (AI) devices in their daily lives. It is crucial to provide them with knowledge concerning how AI works, for enabling them to use AI responsibly and participate actively in their AI-driven future. To support motivation and engagement, playful tools are often used in technology education for K-12 children. This paper offers a systematic literature review of tools for teaching AI to K-12 learners in a playful manner. The most relevant articles are classified and analysed in terms of the nature of the tools they use, that is, whether tools are digital, partly physical and partly digital, or unplugged. Their analysis also considers the target age, the educational focus, and whether their impact is evaluated. According to the results of the review, there are tools for learners of all school grades, and digital tools are the most investigated. Moreover, several studies with tools tend to evaluate engagement and learning but in different manners. The paper concludes by discussing the evaluation aspect, general future work directions and limitations in relation to HCI and education for children."
Exploring Effects of Online and Face-to-Face Teaching Formats on Students’ Interest and Engagement,"Marquardt, Kai and Happe, Lucia",10.1145/3605468.3605499,2023,"The COVID-19 pandemic has highlighted the need for flexible teaching formats, particularly online education, as an alternative to traditional face-to-face (F2F) education. This study investigates the impact of teaching format (online vs. F2F) on secondary school students’ interest and engagement. We conducted an exploratory analysis of survey data from 16 workshops (F2F: 12, online: 4) held between January and December 2022, with 129 participants completing the survey (F2F: 76, online: 53, age: 10-16). All workshops covered the same learning topics, provided by online courses developed to raise interest in computer science (CS). Our findings show that the teaching format had a negligible effect on interest development. Still, students in the F2F setting showed higher engagement levels than those in the online setting. Additionally, an analysis of the effect of age indicates that younger students are more engaged in online settings than their older peers. We also found indications for longer-running programs having a greater effect on personal interest development compared to one-day programs. This paper discusses the results and identifies implications for teaching practice and future research. Overall, the study highlights the need to balance the advantages and limitations of both teaching formats and suggests potential strategies to improve online engagement."
Project-Based Software Engineering Curriculum for Secondary Students,"Gransbury, Isabella and Brock, Janet and Root, Emily and Catete, Veronica and Barnes, Tiffany and Grover, Shuchi and Ledeczi, Akos",10.1145/3605468.3605501,2023,"Background. Software Engineering (SE) is a new and emerging topic in secondary computer science classrooms. However, a review of the recent literature has identified an overall lack of reporting on the development of SE secondary curriculum. Previous studies also report low student engagement when teaching these concepts. Objectives. In this experience report, we discuss the development of a 9-week, project-based learning (PBL) SE curriculum for secondary students. During this curriculum, students create a socially relevant project in groups of two to three. We discuss displays of participant engagement with CS concepts through the PBL pedagogy and the SE curriculum. Method. We examine participant engagement through group artifact interviews about student experiences during a week-long, virtual summer camp that piloted activities from our curriculum. During this camp, students followed a modified SE life cycle created by the authors of the paper. Findings. Participants showed engagement with the curriculum through various aspects of PBL, such as autonomy, creativity, and personal interest in their project topic. Implications. The lessons learned from this experience report suggest that PBL pedagogy can increase student engagement when teaching CS concepts, and this pedagogy provides detail and structure for future secondary SE curriculum implementations to support educators in the classroom."
Broken Promises: Measuring Confounding Effects in Learning-based Vulnerability Discovery,"Imgrund, Erik and Ganz, Tom and H\""{a}rterich, Martin and Pirch, Lukas and Risse, Niklas and Rieck, Konrad",10.1145/3605764.3623915,2023,"Several learning-based vulnerability detection methods have been proposed to assist developers during the secure software development life-cycle. In particular, recent learning-based large transformer networks have shown remarkably high performance in various vulnerability detection and localization benchmarks. However, these models have also been shown to have difficulties accurately locating the root cause of flaws and generalizing to out-of-distribution samples. In this work, we investigate this problem and identify spurious correlations as the main obstacle to transferability and generalization, resulting in performance losses of up to 30% for current models. We propose a method to measure the impact of these spurious correlations on learning models and estimate their true, unbiased performance. We present several strategies to counteract the underlying confounding bias, but ultimately our work highlights the limitations of evaluations in the laboratory for complex learning tasks such as vulnerability discovery."
Design and Development of a Knowledge Service Platform in the Field of Computer Science: Knowledge Service Platform in Computer Science: Design and Development,"Chen, Chunyun and Zhu, Shuai and Tao, Qianwen and Wu, Qidi and Shi, Youqun",10.1145/3606094.3606108,2023,"This paper introduces a crowdsourcing-based method for expanding knowledge graphs, aiming to address the high production cost problem of building high-quality domain knowledge graphs. The method includes stages such as task assignment, task division, and incentive mechanisms, which leverage the power of crowdsourcing workers to obtain high-quality domain knowledge content and expand and refine the structure and content of knowledge graphs. Additionally, to address the problem that traditional computer teaching methods cannot meet students' learning needs, this paper also develops a computer domain knowledge service platform that is easy to read and understand, has multiple types of knowledge expression, and is interactive. The platform presents the structure and content of knowledge graphs in a visual and interactive way, providing a new way of learning computer knowledge for students and relevant personnel. It not only helps students and relevant personnel better understand computer knowledge but also expands the structure and content of knowledge graphs, providing support for the application and promotion of knowledge graphs. The method and platform presented in this paper have practical value and promotion significance and are expected to play an important role in the expansion and teaching of computer domain knowledge graphs."
Video Quality Indicators for Video-Based Learning System in Higher Education,"Zulkarnain, Novan and Prabowo, Harjanto and Gaol, Ford Lumban and Isa, Sani Muhamad",10.1145/3606150.3606155,2023,"Higher education worldwide has adopted Video-Based Learning (VBL) over the past decade. They have tried to build a VBL system to improve services to students. However, the researcher's topic was not fully explored, especially in making the video itself. What features are mandatory and must be considered in making a video lecture to increase student engagement. A systematic review of the literature (SLR) is a way to identify these features. This study provides a critical analysis of the current research in VBL over the last five years (2017 – 2021). It presents a general trend examining 1336 journals and many literary works related to VBL design in Higher Education, selected through a stepwise process based on PRISMA Framework guidelines, using pre-defined selection criteria. We carefully selected 32 journals and found 22 features referring to the Lecture Video Design succession."
Patch-based deep learning models for breast mammographic mass classification,"Xie, Wentao and Liu, Qian and Su, Yongye and Yan, Yi and Huang, Shujun and Kuang, Qin and Hu, Pingzhao",10.1145/3608164.3608167,2023,"Background: Breast cancer is one of the greatest health threats to women worldwide. Mammography is an effective and inexpensive tool for breast cancer early detection. Mammography-based breast cancer screening requires a lot of manpower from professional experts. Thus, computer-aided diagnosis tools, especially accurate classifiers which can distinguish the breast masses from the background tissues, are needed. However, since the sample size of publicly available mammography data sets is relatively small, the performance of the published breast mass identification models was not great, and the models were not well-embraced by clinical practice due to their low interpretability. Methods: In this work, using two independent and well-known mammography data sets, the CBIS-DDSM and the INbreast, we proposed a novel patch generation method for data augmentation and negative case generation. We implemented two successful deep learning models, the ResNet and the ViT, to classify the generated mass and non-mass patches. We also proposed to apply the patch-level model to the full-view mammogram screening in a sliding window manner and visualize/interpret the prediction results using a heatmap so that the clinic practice could potentially benefit from the well-trained model. Result: For the CBIS-DDSM dataset, we compared the performance of the ResNet and the ViT with and without data augmentation. The F1 score is 0.91, 0.86, 0.85, and 0.70, respectively. We also evaluated our models using other metrices such as accuracy, precision, recall, and ROC curve. The results show that the ResNet model outperforms the ViT model. And the data augmentation improves the overall performance of the models. The similar conclusions are further supported using the independent INbreast data. Furthermore, we also explored to use probability-based heatmaps to visualize the potential mass regions in mammogram images. Conclusion: The study shows that our patch-level data augmentation is effective in improving the classification performance of the deep learning models. The comparable performance on the CBIS-DDSM data and the independent INbreast data demonstrates the generalizability of our methods. The proposed heatmap visualization tool increases the interpretability of our results and could be a potential approach for clinic utilization."
Innovation and Application of University Research Management in the Context of Big Data,"Zhu, Bin and Wang, Shuai",10.1145/3608218.3608236,2023,"With the development of information technology, big data has also attracted more and more attention. The application of big data technology has played a huge role in promoting the informatization of university scientific research management, effectively improving the work efficiency of university scientific research management, and making the networking and collaborative office of university scientific research management a reality. This paper analyzes the problems existing in the management of scientific research in colleges and universities and the suggestions on the application of big data for scientific research management. Through practical application cases, the advantages of big data applied to university scientific research management are expounded from the aspects of project management classification and collaborative office."
Serious 'Slow' Game Jam - A Game Jam Model for Serious Game Design,"Abbott, Daisy and Chatzifoti, Olga and Ferguson, Jamie and Louchart, Sandy and Stals, Shenando",10.1145/3610602.3610604,2023,"The Serious ‘Slow’ Game Jam (SSGJ) is a new model for use in serious game design and research. Game jams contribute to creative, innovative and collaborative design, however, game jams for serious purposes require an alternative model that integrates domain experts within the jammer community to ensure the validity of their designs and content. Furthermore, a rigorous yet accessible design methodology is required to balance pedagogic and game aspects to support jammers, as well as to assist researchers in subsequent analysis and evaluation. A standard entertainment game jam model does not afford support for these aspects. The SSGJ model addresses these needs through an inclusive, collaborative, and creative framework for multidisciplinary teams, which includes: encouraging reflection and knowledge exchange; improving content validity; and providing continuous support and mentoring to participants. Reflection on the model highlights the importance of framing serious game jams as explicitly educational activities and embedding them into existing training contexts. The SSGJ model contributes to a collaborative serious game design methodology for the wider research community, irrespective of application domains."
Cross-cultural Online Game Jams: Fostering cultural competencies through jams in game education setting,"Park, Solip and Kultima, Annakaisa and Ono, Kenji and Choi, Buho",10.1145/3610602.3610606,2023,"This paper discusses cross-cultural online game jams in a game educational setting, drawing on the experience gained from the project ""Games Now! Online Jam"" (henceforth ""GNOJ"") conducted by Aalto University in Finland during the Covid-19 pandemic in 2021-2022. Each GNOJ lasted a week, and jammers utilized various online tools, cloud services, communication platforms, and open-source software. Ninety jammers from Finland, Sweden, South Korea, and Japan participated virtually from their home countries. Through post-survey and observation data, we found that jammers highly valued the cultural learning experience offered by jamming. Notably, they encountered unexpected surprises stemming from the diverse local game development practices and different conceptual and terminological connotations across countries during the jam. But jammers displayed proactive engagement in overcoming such cultural differences, with a heightened motivation to learn other languages, cultures, and local game development practices across the world. These findings highlight the pedagogical benefits that cross-cultural online jams can bring to game education. By fostering cultural awareness and competencies in understanding local nuances in game development and communication styles, such initiatives can help future (and current) game developers to effectively prepare multinational work environments and cooperative workflow with remote teams spanning multiple time zones."
The Institute of Coding Accreditation Standard: Exploring the Use of a Professional Skills Framework to Address the UK Skills Gap,"Bowers, David S. and Hayes, Alan and Prickett, Tom and Crick, Tom and Streater, Kevin and Sharp, Chris",10.1145/3610969.3611121,2023,"Computing comprises a broad spectrum of subjects and specialisms, with a rich variety of undergraduate courses (including Computer Engineering, Computer Science, Cybersecurity, Information Systems, Information Technology and Software Engineering) offered by universities worldwide. This breadth presents challenges for employers considering employing computing graduates and hence desiring to know both the topics studied and the skills/competencies accumulated by graduates to be able to make appropriate job offers. Small to medium enterprises (SMEs) may not have the resources to provide graduate training programmes, and therefore need ‘work-ready’ graduates. This paper explores and evaluates the feasibility of benchmarking students’ achievements against an industry-led skills framework, Skills for the Information Age (SFIA), to distinguish between what graduates know, have done or are competent in. The approach taken was evolutionary prototyping, informed by expert review. The work generated an accreditation standard that could be implemented or used as a model to enhance an existing accreditation standard. In contrast to academic approaches to competency-based education, or abstract notions of generic skills, this work focused on defining an output standard expressed in terms of employer needs and expectations captured in the SFIA skills framework. We show how a course meeting the proposed standard would satisfy the UK benchmarks for an undergraduate computing degree. By badging SFIA knowledge and competencies, such a course would enhance its learning outcomes, offering clarity for employers and career benefits to students."
"Nursing Staff's Attitudes, Needs, and Preferences for Care Robots in Assisted Living Facilities: A Systematic Literature Review","Trainum, Katie and Liu, Jiaying and Hauser, Elliott and Xie, Bo",10.1145/3610978.3640690,2024,"Care robots have been proposed in response to nursing shortages in assisted living facilities (ALF), in which the population of older adults is growing. Although the use of care robots can improve the health of these older adults, their introduction fundamentally changes the work of nursing staff and has implications for the entire healthcare system. In the development of such gerotechnology, it is important to include end-users, but so far, the perspectives of nursing staff have largely been ignored. We have conducted a systematic review to examine the literature on nursing staff's attitudes, needs, and preferences for care robots in ALFs, to guide future research. Using the PRISMA method, we identified 15 publications. We found that nursing staff desire robots that can assist with physically demanding tasks and reduce workload. Further research is needed on nursing staff's concerns and the contextual factors that influence nursing staff's perspectives of care robots."
"Social Robots in Hospital Settings: An Initial Exploration of the Services Provided, Interaction Style and in the Field Evaluation","Gasteiger, Norina and Zhu, Tingting and Broadbent, Elizabeth and Lim, Jongyoon and MacDonald, Bruce A. and Ahn, Ho Seok",10.1145/3610978.3640714,2024,"There is interest in using social robots in hospitals but little understanding of how they engage with users to fulfil their roles. Research in the field helps understand how HRI may occur naturally. We conducted a scoping review of literature on social robots in hospital settings, using the Arksey and O'Malley 5-step method. This report presents an initial synthesis of 19 studies. The robots performed various tasks, from greeting and educating visitors to social companionship, supporting healthcare delivery, carrying goods, and educating staff. Most were physically embodied, but three were embodied conversational agents (virtual robots). To engage with interaction partners, 89% used speech and 79% used motions, gestures, and facial expressions. Less commonly used was written text and tactile interaction. Further personalizing interactions, introducing creativity, and focusing on the wild aspects of HRI could help support the application of social robots in hospital settings."
BFSig: Leveraging File Significance in Bus Factor Estimation,"Haratian, Vahid and Evtikhiev, Mikhail and Derakhshanfar, Pouria and T\""{u}z\""{u}n, Eray and Kovalenko, Vladimir",10.1145/3611643.3613877,2023,"Software projects experience the departure of developers due to various reasons. As developers are one of the main sources of knowledge in software projects, their absence will inevitably result in a certain degree of knowledge depletion. Bus Factor (BF) is a metric to evaluate how this knowledge loss can affect the project’s continuity.  
Conventionally, BF is calculated as the smallest set of developers, removing over half the project knowledge upon departure. Current state-of-the-art approaches measure developers’ knowledge by the number of authored files, utilizing version control system (VCS) information. However, numerous studies have shown that files in software projects have different significance.  
In this study, we explore how weighting files according to their significance affects the performance of two prevailing BF estimators. We derive significance scores by computing five well-known graph metrics from the project’s dependency graph: PageRank, In-/Out-/All-Degree, and Betweenness Centralities. Furthermore, we introduce BFSig , a prototype of our approach. Finally, we present a new dataset comprising reported BF scores collected by surveying software practitioners from five prominent Github repositories.  
Our results indicate that BFSig outperforms the baselines by up to an 18% reduction in terms of Normalized Mean Absolute Error (NMAE). Moreover, BFSig yields 18% fewer False Negatives in identifying potential risks associated with low BF. Besides, our respondent confirmed BFSig versatility by showing its ability to assess the BF of the project’s subfolders.  
In conclusion, we believe to estimate BF from authorship, software components of higher importance should be assigned heavier weight. Currently, BFSig exclusively explores the topological characteristics of these components. Nevertheless, considering attributes such as code complexity and bug proneness could potentially enhance the performance of BFSig."
InferFix: End-to-End Program Repair with LLMs,"Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey",10.1145/3611643.3613892,2023,"Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose : a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs.  combines a Retriever – transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator – an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that  outperforms strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java. We discuss the deployment of  alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow."
"Matching Skills, Past Collaboration, and Limited Competition: Modeling When Open-Source Projects Attract Contributors","Fang, Hongbo and Herbsleb, James and Vasilescu, Bogdan",10.1145/3611643.3616282,2023,"Attracting and retaining new developers is often at the heart of open-source project sustainability and success.  
 Previous research found many intrinsic (or endogenous) project characteristics associated with the attractiveness of projects to new developers, but the impact of factors external to the project itself have largely been overlooked.  
 In this work, we focus on one such external factor, a project's labor pool, which is defined as the set of contributors active in the overall open-source ecosystem that the project could plausibly attempt to recruit from at a given time. How are the size and characteristics of the labor pool associated with a project's attractiveness to new contributors?  
 Through an empirical study of over 516,893 Python projects, we found that the size of the project's labor pool, the technical skill match, and the social connection between the project's labor pool and members of the focal project all significantly influence the number of new developers that the focal project attracts, with the competition between projects with overlapping labor pools also playing a role.  
 Overall, the labor pool factors add considerable explanatory power compared to models with only project-level characteristics."
Contextual Predictive Mutation Testing,"Jain, Kush and Alon, Uri and Groce, Alex and Le Goues, Claire",10.1145/3611643.3616289,2023,"Mutation testing is a powerful technique for assessing and improving test suite quality that artificially introduces bugs and checks whether the test suites catch them. However, it is also computationally expensive and thus does not scale to large systems and projects. One promising recent approach to tackling this scalability problem uses machine learning to predict whether the tests will detect the synthetic bugs, without actually running those tests. However, existing predictive mutation testing approaches still misclassify 33% of detection outcomes on a randomly sampled set of mutant-test suite pairs. We introduce MutationBERT, an approach for predictive mutation testing that simultaneously encodes the source method mutation and test method, capturing key context in the input representation. Thanks to its higher precision, MutationBERT saves 33% of the time spent by a prior approach on checking/verifying live mutants. MutationBERT, also outperforms the state-of-the-art in both same project and cross project settings, with meaningful improvements in precision, recall, and F1 score. We validate our input representation, and aggregation approaches for lifting predictions from the test matrix level to the test suite level, finding similar improvements in performance. MutationBERT not only enhances the state-of-the-art in predictive mutation testing, but also presents practical benefits for real-world applications, both in saving developer time and finding hard to detect mutants that prior approaches do not."
Evaluating Transfer Learning for Simplifying GitHub READMEs,"Gao, Haoyu and Treude, Christoph and Zahedi, Mansooreh",10.1145/3611643.3616291,2023,"Software documentation captures detailed knowledge about a software product, e.g., code, technologies, and design. It plays an important role in the coordination of development teams and in conveying ideas to various stakeholders. However, software documentation can be hard to comprehend if it is written with jargon and complicated sentence structure. In this study, we explored the potential of text simplification techniques in the domain of software engineering to automatically simplify GitHub README files. We collected software-related pairs of GitHub README files consisting of 14,588 entries, aligned difficult sentences with their simplified counterparts, and trained a Transformer-based model to automatically simplify difficult versions. To mitigate the sparse and noisy nature of the software-related simplification dataset, we applied general text simplification knowledge to this field. Since many general-domain difficult-to-simple Wikipedia document pairs are already publicly available, we explored the potential of transfer learning by first training the model on the Wikipedia data and then fine-tuning it on the README data. Using automated BLEU scores and human evaluation, we compared the performance of different transfer learning schemes and the baseline models without transfer learning. The transfer learning model using the best checkpoint trained on a general topic corpus achieved the best performance of 34.68 BLEU score and statistically significantly higher human annotation scores compared to the rest of the schemes and baselines. We conclude that using transfer learning is a promising direction to circumvent the lack of data and drift style problem in software README files simplification and achieved a better trade-off between simplification and preservation of meaning."
Software Merge: A Two-Decade Systematic Mapping Study,"Carbonera, Carlos and Farias, Kleinner and Graeff, C\'{e}Sar and Silva, Robson",10.1145/3613372.3613391,2023,"Software merging plays a key role in many software engineering tasks, e.g., reconciling source code developed in parallel. For this reason, many studies have been proposed in the last two decades. However, the current literature still lacks a classification of currently available approaches and research carried out considering software merge. Hence, a broad understanding of these already published works remains limited and inconclusive. Even worse, research gaps are not identified or prioritized. This study, therefore, provides a classification and thematic analysis of studies about software merge in the last two decades. We performed a systematic mapping study of the literature following PRISMA guidelines to explore nine research questions. After a careful selection process, 70 studies were selected, analyzed, and categorized (out of 308). The main results are that: (1) Most studies (71.42%) evaluated syntactic conflicts; (2) Most studies (57.14%) performed statistical analysis to evaluation software merge; (3) Most of the evaluated studies were published in the last five years (2017-2023), evidencing the importance and interest of the academic community and the software industry in this promising area of software engineering. This article benefits professionals and researchers by providing a body of knowledge about the current literature, which can be the starting point for future research. Finally, we present some worth-investigating challenges by the scientific community."
An Assessment of Machine Learning Algorithms and Models for Prediction of Change-Prone Java Methods,"Farah, Paulo Roberto and Silva, Rog\'{e}rio and Vergilio, Silvia",10.1145/3613372.3613395,2023,"Identifying which parts of code are prone to change during software evolution allows developers to prioritize and allocate resources efficiently. Having as focus a smaller scope makes easier change management and allows monitoring the type of modification and its impact. However, existing change-proneness prediction approaches are focused mainly on system classes. But the problem is that classes contain many characteristics of different software attributes and some software behaviors are more granular and better captured at the method-level. Motivated by these facts, in this paper, we empirically assess the performance of four machine learning algorithms for change-prone method prediction in seven open-source software projects. We derived and compared models obtained with three sets of independent variables (features): a set composed of structural metrics, a second set composed of evolution-based metrics, and a third that includes a combination of both kinds of metrics. The results show that, Random Forest presents the best general performance, independently of the used indicator and set of features. The model composed by both sets of metrics outperforms the other two. Two features based on the frequency of changes that happened in the evolution history of the method are point out as the most important for our problem."
Architectural Technical Debt - A Systematic Mapping Study,"Sousa, Armando and Rocha, Lincoln and Britto, Ricardo",10.1145/3613372.3613399,2023,"Architectural Technical Debt (ATD) is one of the leading Technical Debt (TD) that causes more impact in maintaining and evolving complex software systems. We conduct a Systematic Mapping Study (SMS) to discover the main aspects of identifying and monitoring ATD items to help determine what the community has been studying about it in the last ten years. We evaluated 70 studies dating from 2012 to 2022. We find out the main types of ATD, how to measure and monitor ATD, which techniques and methods stand out in this area, the most used tools, and directions on how to calculate the cost of paying for ATD items. The results of this mapping study can help identify points that still require investigations on identifying, monitoring, and calculating the effort to fix ATD items. Furthermore, we have proposed a roadmap to aid managing Architectural Technical Debt, which provides guidance for identifying and monitoring ATD items in software systems."
A Systematic Review of Ability-diverse Collaboration through Ability-based Lens in HCI,"Xiao, Lan and Bandukda, Maryam and Angerbauer, Katrin and Lin, Weiyue and Bhatnagar, Tigmanshu and Sedlmair, Michael and Holloway, Catherine",10.1145/3613904.3641930,2024,"In a world where diversity is increasingly recognised and celebrated, it is important for HCI to embrace the evolving methods and theories for technologies to reflect the diversity of its users and be ability-centric. Interdependence Theory, an example of this evolution, highlights the interpersonal relationships between humans and technologies and how technologies should be designed to meet shared goals and outcomes for people, regardless of their abilities. This necessitates a contemporary understanding of ""ability-diverse collaboration,"" which motivated this review. In this review, we offer an analysis of 117 papers sourced from the ACM Digital Library spanning the last two decades. We contribute (1) a unified taxonomy and the Ability-Diverse Collaboration Framework, (2) a reflective discussion and mapping of the current design space, and (3) future research opportunities and challenges. Finally, we have released our data and analysis tool to encourage the HCI research community to contribute to this ongoing effort."
Sicknificant Steps: A Systematic Review and Meta-analysis of VR Sickness in Walking-based Locomotion for Virtual Reality,"van Gemert, Thomas and Nilsson, Niels Christian and Hirzle, Teresa and Bergstr\""{o}m, Joanna",10.1145/3613904.3641974,2024,"Walking-based locomotion techniques in virtual reality (VR) can use redirection to enable walking in a virtual environment larger than the physical one. This results in a mismatch between the perceived virtual and physical movement, which is known to cause VR sickness. However, it is unclear if different types of walking techniques (e.g., resetting, reorientation, or self-overlapping spaces) affect VR sickness differently. To address this, we conducted a systematic review and meta-analysis of 96 papers published in 2016–2022 that measure VR sickness in walking-based locomotion. We find different VR sickness effects between types of redirection and between normal walking and redirection. However, we also identified several problems with the use and reporting of VR sickness measures. We discuss the challenges in understanding VR sickness differences between walking techniques and present guidelines for measuring VR sickness in locomotion studies."
Do We Run How We Say We Run? Formalization and Practice of Governance in OSS Communities,"Chakraborti, Mahasweta and Atkisson, Curtis and St\u{a}nciulescu, \c{S}tefan and Filkov, Vladimir and Frey, Seth",10.1145/3613904.3641980,2024,"Open Source Software (OSS) communities often resist regulation typical of traditional organizations. Yet formal governance systems are being increasingly adopted among communities, particularly through non-profit project-sponsoring foundations. Our study looks at the Apache Software Foundation Incubator program and 208 of the projects it has supported. We assemble a scalable, semantic pipeline to discover and analyze the governance behavior of projects from their mailing lists. We then investigate the relationship of such behavior to what the formal policies prescribe, through their own governance priorities and how their members internalize them. Our findings indicate that a greater amount of policy over a governed topic doesn’t elicit more governed activity on that topic, but does predict greater internalization by community members. Moreover, alignment of community operations with foundation governance, be it dedicating their governance focus or adopting policy along topics seeing greater policy-making, has limited association with project outcomes."
Unlock Life with a Chat(GPT): Integrating Conversational AI with Large Language Models into Everyday Lives of Autistic Individuals,"Choi, Dasom and Lee, Sunok and Kim, Sung-In and Lee, Kyungah and Yoo, Hee Jeong and Lee, Sangsu and Hong, Hwajung",10.1145/3613904.3641989,2024,"Autistic individuals often draw on insights from their supportive networks to develop self-help life strategies ranging from everyday chores to social activities. However, human resources may not always be immediately available. Recently emerging conversational agents (CAs) that leverage large language models (LLMs) have the potential to serve as powerful information-seeking tools, facilitating autistic individuals to tackle daily concerns independently. This study explored the opportunities and challenges of LLM-driven CAs in empowering autistic individuals through focus group interviews and workshops (N=14). We found that autistic individuals expected LLM-driven CAs to offer a non-judgmental space, encouraging them to approach day-to-day issues proactively. However, they raised issues regarding critically digesting the CA responses and disclosing their autistic characteristics. Based on these findings, we propose approaches that place autistic individuals at the center of shaping the meaning and role of LLM-driven CAs in their lives, while preserving their unique needs and characteristics."
Charting Ethical Tensions in Multispecies Technology Research through Beneficiary-Epistemology Space,"Benford, Steven David and Mancini, Clara and Chamberlain, Alan and Schneiders, Eike and Castle-Green, Simon D and Fischer, Joel E and Kucukyilmaz, Ayse and Salimbeni, Guido and Ngo, Victor Zhi Heung and Barnard, Pepita and Adams, Matt and Tandavanitj, Nick and Row Farr, Ju",10.1145/3613904.3641994,2024,"While ethical challenges are widely discussed in HCI, far less is reported about the ethical processes that researchers routinely navigate. We reflect on a multispecies project that negotiated an especially complex ethical approval process. Cat Royale was an artist-led exploration of creating an artwork to engage audiences in exploring trust in autonomous systems. The artwork took the form of a robot that played with three cats. Gaining ethical approval required an extensive dialogue with three Institutional Review Boards (IRBs) covering computer science, veterinary science and animal welfare, raising tensions around the welfare of the cats, perceived benefits and appropriate methods, and reputational risk to the University. To reveal these tensions we introduce beneficiary-epistemology space, that makes explicit who benefits from research (humans or animals) and underlying epistemologies. Positioning projects and IRBs in this space can help clarify tensions and highlight opportunities to recruit additional expertise."
The Cadaver in the Machine: The Social Practices of Measurement and Validation in Motion Capture Technology,"Harvey, Emma and Sandhaus, Hauke and Jacobs, Abigail Z. and Moss, Emanuel and Sloane, Mona",10.1145/3613904.3642004,2024,"Motion capture systems, used across various domains, make body representations concrete through technical processes. We argue that the measurement of bodies and the validation of measurements for motion capture systems can be understood as social practices. By analyzing the findings of a systematic literature review (N=278) through the lens of social practice theory, we show how these practices, and their varying attention to errors, become ingrained in motion capture design and innovation over time. Moreover, we show how contemporary motion capture systems perpetuate assumptions about human bodies and their movements. We suggest that social practices of measurement and validation are ubiquitous in the development of data- and sensor-driven systems more broadly, and provide this work as a basis for investigating hidden design assumptions and their potential negative consequences in human-computer interaction."
Augmented Reality at Zoo Exhibits: A Design Framework for Enhancing the Zoo Experience,"Syiem, Brandon Victor and Webber, Sarah and Kelly, Ryan M. and Zhou, Qiushi and Goncalves, Jorge and Velloso, Eduardo",10.1145/3613904.3642015,2024,"Augmented Reality (AR) offers unique opportunities for contributing to zoos’ objectives of public engagement and education about animal and conservation issues. However, the diversity of animal exhibits pose challenges in designing AR applications that are not encountered in more controlled environments, such as museums. To support the design of AR applications that meaningfully engage the public with zoo objectives, we first conducted two scoping reviews to interrogate previous work on AR and broader technology use at zoos. We then conducted a workshop with zoo representatives to understand the challenges and opportunities in using AR to achieve zoo objectives. Additionally, we conducted a field trip to a public zoo to identify exhibit characteristics that impacts AR application design. We synthesise the findings from these studies into a framework that enables the design of diverse AR experiences. We illustrate the utility of the framework by presenting two concepts for feasible AR applications."
Trust in AI-assisted Decision Making: Perspectives from Those Behind the System and Those for Whom the Decision is Made,"Vereschak, Oleksandra and Alizadeh, Fatemeh and Bailly, Gilles and Caramiaux, Baptiste",10.1145/3613904.3642018,2024,"Trust between humans and AI in the context of decision-making has acquired an important role in public policy, research and industry. In this context, Human-AI Trust has often been tackled from the lens of cognitive science and psychology, but lacks insights from the stakeholders involved. In this paper, we conducted semi-structured interviews with 7 AI practitioners and 7 decision subjects from various decision domains. We found that 1) interviewees identified the prerequisites for the existence of trust and distinguish trust from trustworthiness, reliance, and compliance; 2) trust in AI-integrated systems is strongly influenced by other human actors, more than the system’s features; 3) the role of Human-AI trust factors is stakeholder-dependent. These results provide clues for the design of Human-AI interactions in which trust plays a major role, as well as outline new research directions in Human-AI Trust."
Uncovering and Addressing Blink-Related Challenges in Using Eye Tracking for Interactive Systems,"Grootjen, Jesse W. and Weing\""{a}rtner, Henrike and Mayer, Sven",10.1145/3613904.3642086,2024,"Currently, interactive systems use physiological sensing to enable advanced functionalities. While eye tracking is a promising means to understand the user, eye tracking data inherently suffers from missing data due to blinks, which may result in reduced system performance. We conducted a literature review to understand how researchers deal with this issue. We uncovered that researchers often implemented their use-case-specific pipeline to overcome the issue, ranging from ignoring missing data to artificial interpolation. With these first insights, we run a large-scale analysis on 11 publicly available datasets to understand the impact of the various approaches on data quality and accuracy. By this, we highlight the pitfalls in data processing and which methods work best. Based on our results, we provide guidelines for handling eye tracking data for interactive systems. Further, we propose a standard data processing pipeline that allows researchers and practitioners to pre-process and standardize their data efficiently."
User Experience Design Professionals’ Perceptions of Generative Artificial Intelligence,"Li, Jie and Cao, Hancheng and Lin, Laura and Hou, Youyang and Zhu, Ruihao and El Ali, Abdallah",10.1145/3613904.3642114,2024,"Among creative professionals, Generative Artificial Intelligence (GenAI) has sparked excitement over its capabilities and fear over unanticipated consequences. How does GenAI impact User Experience Design (UXD) practice, and are fears warranted? We interviewed 20 UX Designers, with diverse experience and across companies (startups to large enterprises). We probed them to characterize their practices, and sample their attitudes, concerns, and expectations. We found that experienced designers are confident in their originality, creativity, and empathic skills, and find GenAI’s role as assistive. They emphasized the unique human factors of “enjoyment” and “agency”, where humans remain the arbiters of “AI alignment’’. However, skill degradation, job replacement, and creativity exhaustion can adversely impact junior designers. We discuss implications for human-GenAI collaboration, specifically copyright and ownership, human creativity and agency, and AI literacy and access. Through the lens of responsible and participatory AI, we contribute a deeper understanding of GenAI fears and opportunities for UXD."
Griffith: A Storyboarding Tool Designed with Japanese Animation Professionals,"Kato, Jun and Hara, Kenta and Hirasawa, Nao",10.1145/3613904.3642121,2024,"The “E-conte,” storyboard in English, is commonly referred to as the “blueprint” in Japanese animation (anime) production, consisting of scene illustrations, timing information, and textual descriptions. This paper introduces “Griffith,” a digital system for creating these storyboards. Due to its highly cultural and domain-specific nature, the tool design entailed an in-depth study of the E-conte process and a longitudinal collaboration with an experienced anime director and producers. The resulting system contributes not only domain knowledge, but also generalizable insights into a creativity support environment for visual storytelling, including the importance of vertical timelines and discrete yet integrated tools. To reflect on the interaction design, we presented Griffith to professionals with diverse roles in anime production. Our findings highlight the benefits of the Griffith user interface and the need for a socio-technical focus in designing creativity support tools."
"""Do You Want Me to Participate or Not?"": Investigating the Accessibility of Software Development Meetings for Blind and Low Vision Professionals","Cha, Yoonha and Figueira, Isabela and Ayala, Jessy and Edwards, Emory James and Garcia, Joshua and van der Hoek, Andr\'{e} and Branham, Stacy Marie",10.1145/3613904.3642130,2024,"Scholars have investigated numerous barriers to accessible software development tools and processes for Blind and Low Vision (BLV) developers. However, the research community has yet to study the accessibility of software development meetings, which are known to play a crucial role in software development practice. We conducted semi-structured interviews with 26 BLV software professionals about software development meeting accessibility. We found four key themes related to in-person and remote software development meetings: (1) participants observed that certain meeting activities and software tools used in meetings were inaccessible, (2) participants performed additional labor in order to make meetings accessible, (3) participants avoided disclosing their disability during meetings due to fear of career repercussions, (4) participants suggested technical, social and organizational solutions for accessible meetings, including developing their own solutions. We suggest recommendations and design implications for future accessible software development meetings including technical and policy-driven solutions."
Selenite: Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models,"Liu, Michael Xieyang and Wu, Tongshuang and Chen, Tianying and Li, Franklin Mingzhe and Kittur, Aniket and Myers, Brad A",10.1145/3613904.3642149,2024,"Sensemaking in unfamiliar domains can be challenging, demanding considerable user effort to compare different options with respect to various criteria. Prior research and our formative study found that people would benefit from reading an overview of an information space upfront, including the criteria others previously found useful. However, existing sensemaking tools struggle with the “cold-start” problem — it not only requires significant input from previous users to generate and share these overviews, but such overviews may also turn out to be biased and incomplete. In this work, we introduce a novel system, Selenite, which leverages Large Language Models (LLMs) as reasoning machines and knowledge retrievers to automatically produce a comprehensive overview of options and criteria to jumpstart users’ sensemaking processes. Subsequently, Selenite also adapts as people use it, helping users find, read, and navigate unfamiliar information in a systematic yet personalized manner. Through three studies, we found that Selenite produced accurate and high-quality overviews reliably, significantly accelerated users’ information processing, and effectively improved their overall comprehension and sensemaking experience."
"“This app said I had severe depression, and now I don’t know what to do”: the unintentional harms of mental health applications","Kang, Rachael M. and Reynolds, Tera L.",10.1145/3613904.3642178,2024,"A growing market for mental health applications and increasing evidence for the efficacy of these applications have made apps a popular mode of mental healthcare delivery. However, given the gravity of mental illnesses, the potential harms of using these applications must be continually investigated. In this study, we conducted a thematic analysis using user-comments left on depression self-management applications. We analyzed 6,253 reviews from thirty-six, systematically selected apps from the Google Play and Apple App stores. We identified four themes regarding the potential, unintentional harms caused by these applications. This study uniquely contributes to the literature by examining the reported harms to users caused by depression self-management apps and contextualizing them in an ethical framework. We provide recommendations to developers for creating ethical depression self-management apps and resources for practitioners and consumers to aid in screening apps."
Teaching artificial intelligence in extracurricular contexts through narrative-based learnersourcing,"Moore, Dylan Edward and Moore, Sophia R. R. and Ireen, Bansharee and Iskandar, Winston P. and Artazyan, Grigory and Murnane, Elizabeth L.",10.1145/3613904.3642198,2024,"Collaborative technology provides powerful opportunities to engage young people in active learning experiences that are inclusive, immersive, and personally meaningful. In particular, interactive narratives have proven to be effective scaffolds for learning, and learnersourcing has emerged as a promising student-driven approach to enable personalized education and quality control at-scale. We introduce the first synthesis of these ideas in the context of teaching artificial intelligence (AI), which is now seen as a critical component of 21st-century education. Specifically, we explore the design of a narrative-based learnersourcing platform where engagement is centered around a learner-made choose-your-own-adventure story. In grounding our approach, we draw from pedagogical literature, digital storytelling, and recent work on learnersourcing. We report on our iterative, learner-centered design process as well as our study findings that demonstrate the platform’s positive effects on knowledge gains, interest in AI concepts, and the overall user experience of narrative-based learnersourcing technology."
Conceptualising Fatness within HCI: A Call for Fat Liberation,"Sobey, Aisha",10.1145/3613904.3642199,2024,"Fatness sits at the intersection of many systems of oppression, such as race, gender, class, and (dis)ability. Anti-fat bias happens out in the open and is prevalent in Western society, yet there has been little to no consideration for the wider impact of digital systems in exacerbating, recreating, and repurposing anti-fat bias, or any engagement with designing for fat justice. Therefore, this paper argues that there needs to be a consideration for fat dignity in the design of digital systems, and an investigation of the (un)intended consequences of the datafication of fat lives. This paper offers a scoping literature review of HCI and Fat Studies to identify research gaps and argues that both disciplines would benefit from collaboration. Specifically, the standard of design justice would be increased through radical acceptance, and new questions could be asked to critique how technologies have been leveraged to exercise control over our bodies."
Parent-Child Joint Media Engagement Within HCI: A Scoping Analysis of the Research Landscape,"Yu, Junnan and Qi, Xiang and Yang, Siqi",10.1145/3613904.3642307,2024,"Parents play essential roles in children’s play and learning with various media, often leading to positive and productive engagement outcomes for both parties. As such, an increasing number of HCI research has focused on understanding parent-child joint media engagement (JME) and designing new technologies to foster productive joint media experiences for children and parents. However, we currently lack a systematic view of this emerging field, which hinders the research and design of new joint media experiences and technologies for families. In this work, we conduct a scoping review of parent-child JME research within HCI (N = 89) and analyze the included papers from three lenses: publication features, methodological features, and JME features. Based on these findings, we identify gaps and opportunities in parent-child JME research and further expand the theoretical framing of JME by developing a framework that captures different JME dimensions."
Heart and Soul: The Ethics of Biometric Capture in Immersive Artistic Performance,"Sparrow, Lucy A. and Galwey, Caiti and Loveridge, Ben and Glasser, Solange and Osborne, Margaret S. and Kelly, Ryan M.",10.1145/3613904.3642309,2024,"Biometric data plays a multifaceted role in innovative artistic endeavours. As artists continue to break new ground by integrating performers’ biometric data into live performances, others collect biometric data from audiences to measure engagement. Given the sensitive and personal nature of biometric data, particularly in relation to immersive technology, it is imperative to ethically consider how this data should be handled in performative contexts. To clarify these ethical considerations, we conducted a scoping review of sources related to immersive biometric performance in HCI, Performing Arts, and Social Sciences published over the past 30+ years. We detail how and why biometric data is being used in immersive artistic performance, identify associated ethical questions and concerns, and develop a framework of ethical considerations for artists and researchers in this space. In doing so, we emphasise an ‘ethics by design’ approach that considers values such as privacy and autonomy alongside artistic merit."
Playing with Perspectives and Unveiling the Autoethnographic Kaleidoscope in HCI – A Literature Review of Autoethnographies,"Kaltenhauser, Annika and Stefanidi, Evropi and Sch\""{o}ning, Johannes",10.1145/3613904.3642355,2024,"Autoethnography is a valuable methodological approach bridging the gap between personal experiences and academic inquiry, enabling researchers to gain deep insights into various dimensions of technology use and design. While its adoption in Human-Computer Interaction (HCI) continues to grow, a comprehensive investigation of its function and role within HCI research is still lacking. This paper examines the evolving landscape of autoethnographies within HCI over the past two decades through a systematic literature review. We identify prevalent themes, methodologies, and contributions emerging from autoethnographies by analysing a corpus of 31 HCI publications. Furthermore, we detail data collection techniques and analysis methods and describe reporting standards. Our literature review aims to inform future (HCI) researchers, practitioners, and designers. It encourages them to embrace autoethnography’s rich opportunities by providing examples across domains (e.g., embodiment or health and wellbeing) to advance our understanding of the complex relationships between humans and technology."
A Survey On Measuring Presence in Mixed Reality,"Tran, Tanh Quang and Langlotz, Tobias and Regenbrecht, Holger",10.1145/3613904.3642383,2024,"Presence is a defining element of virtual reality (VR), but it is also increasingly used when assessing mixed reality (MR) experiences. The increased interest in measuring presence in MR and recent works underpinning the specific nature of presence in MR raise the question of the current state and practice of assessing presence in MR. To address this question, we present an analysis of more than 320 studies that report on presence measurements in MR. Our analysis showed that questionnaires are the dominant measurement but also identify problematic trends that stem from the lack of a generally agreed-upon concept or measurement for presence in MR. More specifically, we show that using measurements that are not validated in MR or custom questionnaires limiting the comparability of results is commonplace and could contribute to a looming replication crisis in an increasingly relevant field."
Strategies of Product Managers: Negotiating Social Values in Digital Product Design,"Lev Ari, Eilat and Roichman, Maayan and Toch, Eran",10.1145/3613904.3642409,2024,"Product managers are central figures in digital product development, coordinating teams and prioritizing features. Despite their influence, little research explores how their decisions affect user experience, especially in integrating social values into product architecture. Employing a mixed-methods framework, we conducted semi-structured interviews with 20 product managers and an online survey with an additional 81, all based in Israel. Our study identifies four unique strategies product managers utilize to balance business goals, user satisfaction, and ethical considerations. The survey data further substantiates the prevalence of these strategies across diverse sectors, confirming they reflect industry-wide approaches in the Israeli tech sector rather than isolated practices. To conclude, we emphasize how “soft resistance” tactics, such as adjusting data interpretations based on personal values, impact digital product designs. Moreover, our findings highlight that maintaining an ethical reputation in the job market can be pivotal in shaping product design."
"Supporting Cognitive Reappraisal With Digital Technology: A Content Analysis and Scoping Review of Challenges, Interventions, and Future Directions","Kitson, Alexandra and Slovak, Petr and Antle, Alissa N.",10.1145/3613904.3642488,2024,"Cognitive reappraisal (CR) is a critical emotion regulation skill that is strongly associated with mental well-being outcomes. While CR has been well theorized psychologically and many therapeutic approaches exist, CR remains one of the toughest skills to learn and develop. We explore the design space of using technologically-mediated CR supports through a dual approach. First, we draw on a content analysis of 30 therapeutic manuals combined with five clinician interviews to understand existing CR processes and challenges in therapeutic settings. Second, we compare the identified challenges with a scoping review of 42 HCI papers on technologically-mediated CR interventions. This allowed us to identify trends and gaps in a field where digital health innovations are critically needed; and suggest four design opportunities that warrant further exploration. Together, our work contributes theoretically-derived future research opportunities, and provides researchers with concrete guidance to explore these important design spaces."
Cruising Queer HCI on the DL: A Literature Review of LGBTQ+ People in HCI,"Taylor, Jordan and Simpson, Ellen and Tran, Anh-Ton and Brubaker, Jed R. and Fox, Sarah E and Zhu, Haiyi",10.1145/3613904.3642494,2024,"LGBTQ+ people have received increased attention in HCI research, paralleling a greater emphasis on social justice in recent years. However, there has not been a systematic review of how LGBTQ+ people are researched or discussed in HCI. In this work, we review all research mentioning LGBTQ+ people across the HCI venues of CHI, CSCW, DIS, and TOCHI. Since 2014, we find a linear growth in the number of papers substantially about LGBTQ+ people and an exponential increase in the number of mentions. Research about LGBTQ+ people tends to center experiences of being politicized, outside the norm, stigmatized, or highly vulnerable. LGBTQ+ people are typically mentioned as a marginalized group or an area of future research. We identify gaps and opportunities for (1) research about and (2) the discussion of LGBTQ+ in HCI and provide a dataset to facilitate future Queer HCI research."
Heads-Up Multitasker: Simulating Attention Switching On Optical Head-Mounted Displays,"Bai, Yunpeng and Ikkala, Aleksi and Oulasvirta, Antti and Zhao, Shengdong and Wang, Lucia J and Yang, Pengzhi and Xu, Peisen",10.1145/3613904.3642540,2024,"Optical Head-Mounted Displays (OHMDs) allow users to read digital content while walking. A better understanding of how users allocate attention between these two tasks is crucial for improving OHMD interfaces. This paper introduces a computational model for simulating users’ attention switches between reading and walking. We model users’ decision to deploy visual attention as a hierarchical reinforcement learning problem, wherein a supervisory controller optimizes attention allocation while considering both reading activity and walking safety. Our model simulates the control of eye movements and locomotion as an adaptation to the given task priority, design of digital content, and walking speed. The model replicates key multitasking behaviors during OHMD reading while walking, including attention switches, changes in reading and walking speeds, and reading resumptions."
Designing a Data-Driven Survey System: Leveraging Participants' Online Data to Personalize Surveys,"Velykoivanenko, Lev and Salehzadeh Niksirat, Kavous and Teofanovic, Stefan and Chapuis, Bertil and Mazurek, Michelle L. and Huguenin, K\'{e}vin",10.1145/3613904.3642572,2024,"User surveys are essential to user-centered research in many fields, including human-computer interaction (HCI). Survey personalization—specifically, adapting questionnaires to the respondents’ profiles and experiences—can improve reliability and quality of responses. However, popular survey platforms lack usable mechanisms for seamlessly importing participants’ data from other systems. This paper explores the design of a data-driven survey system to fill this gap. First, we conducted formative research, including a literature review and a survey of researchers (N = 52), to understand researchers’ practices, experiences, needs, and interests in a data-driven survey system. Then, we designed and implemented a minimum viable product called Data-Driven Surveys (DDS), which enables including respondents’ data from online service accounts (Fitbit, Instagram, and GitHub) in survey questions, answers, and flow/logic on existing survey platforms (Qualtrics and SurveyMonkey). Our system is open source and can be extended to work with more online service accounts and survey platforms. It can enhance the survey research experience for both researchers and respondents. A demonstration video is available here: https://doi.org/10.17605/osf.io/vedbj"
From Primary Education to Premium Workforce: Drawing on K-12 Approaches for Developing AI Literacy,"Kaspersen, Magnus H\o{}holt and Musaeus, Line Have and Bilstrup, Karl-Emil Kj\ae{}r and Petersen, Marianne Graves and Iversen, Ole Sejer and Dindler, Christian and Dalsgaard, Peter",10.1145/3613904.3642607,2024,"Advances in artificial intelligence present a need for fostering AI literacy in workplaces. While there is a lack of research on how this can be achieved, there are documented successful approaches in child-computer interaction (CCI), albeit aimed at K-12 education. We present an in-vivo explorative case study of how CCI approaches can be adopted for adult professionals via a full-day workshop developed in collaboration with a trade union to upskill workers. Analyzing data from pre- and post-surveys, a follow-up survey, and materials produced by participants (n=53), we demonstrate how this increased participants’ knowledge of AI while their self-efficacy and empowerment did not improve. This is similar to findings from K-12 education, pointing to self-efficacy and empowerment as major challenges for AI literacy across sectors. We discuss the role of ambassadorships and professional organizations in addressing these issues, and indicate research directions for the CHI community."
Sitting Posture Recognition and Feedback: A Literature Review,"Krauter, Christian and Angerbauer, Katrin and Sousa Calepso, Aim\'{e}e and Achberger, Alexander and Mayer, Sven and Sedlmair, Michael",10.1145/3613904.3642657,2024,"Extensive sitting is unhealthy; thus, countermeasures are needed to react to the ongoing trend toward more prolonged sitting. A variety of studies and guidelines have long addressed the question of how we can improve our sitting habits. Nevertheless, sitting time is still increasing. Here, smart devices can provide a general overview of sitting habits for more nuanced feedback on the user’s sitting posture. Based on a literature review (N=223), including publications from engineering, computer science, medical sciences, electronics, and more, our work guides developers of posture systems. There is a large variety of approaches, with pressure-sensing hardware and visual feedback being the most prominent. We found factors like environment, cost, privacy concerns, portability, and accuracy important for deciding hardware and feedback types. Further, one should consider the user’s capabilities, preferences, and tasks. Regarding user studies for sitting posture feedback, there is a need for better comparability and for investigating long-term effects."
“That’s Kind of Sus(picious)”: The Comprehensiveness of Mental Health Application Users’ Privacy and Security Concerns,"Khoo, Yi Xuan and Kang, Rachael M. and Reynolds, Tera L. and Mentis, Helena M.",10.1145/3613904.3642705,2024,"With the increasing usage of mental health applications (MHAs), there is growing concern regarding their data privacy practices. Analyzing 437 user reviews from 83 apps, we outline users’ predominant privacy and security concerns with currently available apps. We then compare those concerns to criteria from two prominent app evaluation websites – Privacy Not Included and One Mind PsyberGuide. Our findings show that MHA users have myriad data privacy and security concerns including a user’s control over their own data, but these concerns do not often overlap with those of experts from evaluation websites who focus more on issues such as required password strength. We highlight this disconnect and propose solutions in how the mental health care ecosystem can provide better guidance to MHA users and experts from the fields of privacy and security and mental health technology in choosing and evaluating, respectively, potentially useful mental health apps."
Sensible and Sensitive AI for Worker Wellbeing: Factors that Inform Adoption and Resistance for Information Workers,"Das Swain, Vedant and Gao, Lan and Mondal, Abhirup and Abowd, Gregory D. and De Choudhury, Munmun",10.1145/3613904.3642716,2024,"Algorithmic estimations of worker behavior are gaining popularity. Passive Sensing–enabled AI (PSAI) systems leverage behavioral traces from workers’ digital tools to infer their experience. Despite their conceptual promise, the practical designs of these systems elicit tensions that lead to workers resisting adoption. This paper teases apart the monolithic representation of PSAI by investigating system components that maximize value and mitigate concerns. We conducted an interactive online survey using the Experimental Vignette Method. Using Linear Mixed-effects Models we found that PSAI systems were more acceptable when sensing digital time use or physical activity, instead of visual modes. Inferences using language were only acceptable in work-restricted contexts. Compared to insights into performance, workers preferred insights into mental wellbeing. However, they resisted systems that automatically forwarded these insights to others. Our findings provide a template to reflect on existing systems and plan future implementations of PSAI to be more worker-centered."
Charting the Future of AI in Project-Based Learning: A Co-Design Exploration with Students,"Zheng, Chengbo and Yuan, Kangyu and Guo, Bingcan and Hadi Mogavi, Reza and Peng, Zhenhui and Ma, Shuai and Ma, Xiaojuan",10.1145/3613904.3642807,2024,"Students’ increasing use of Artificial Intelligence (AI) presents new challenges for assessing their mastery of knowledge and skills in project-based learning (PBL). This paper introduces a co-design study to explore the potential of students’ AI usage data as a novel material for PBL assessment. We conducted workshops with 18 college students, encouraging them to speculate an alternative world where they could freely employ AI in PBL while needing to report this process to assess their skills and contributions. Our workshops yielded various scenarios of students’ use of AI in PBL and ways of analyzing such usage grounded by students’ vision of how educational goals may transform. We also found that students with different attitudes toward AI exhibited distinct preferences in how to analyze and understand their use of AI. Based on these findings, we discuss future research opportunities on student-AI interactions and understanding AI-enhanced learning."
Investigating the Design of Augmented Narrative Spaces Through Virtual-Real Connections: A Systematic Literature Review,"Shin, Jae-Eun and Kim, Hayun and Park, Hyerim and Woo, Woontack",10.1145/3613904.3642819,2024,"Augmented Reality (AR) is regarded as an innovative storytelling medium that presents novel experiences by layering a virtual narrative space over a real 3D space. However, understanding of how the virtual narrative space and the real space are connected with one another in the design of augmented narrative spaces has been limited. For this, we conducted a systematic literature review of 64 articles featuring AR storytelling applications and systems in HCI, AR, and MR research. We investigated how virtual narrative spaces have been paired, functionalized, placed, and registered in relation to the real spaces they target. Based on these connections, we identified eight dominant types of augmented narrative spaces that are primarily categorized by whether they virtually narrativize reality or realize the virtual narrative. We discuss our findings to propose design recommendations on how virtual-real connections can be incorporated into a more structured approach to AR storytelling."
VIVID: Human-AI Collaborative Authoring of Vicarious Dialogues from Lecture Videos,"Choi, Seulgi and Lee, Hyewon and Lee, Yoonjoo and Kim, Juho",10.1145/3613904.3642867,2024,"The lengthy monologue-style online lectures cause learners to lose engagement easily. Designing lectures in a “vicarious dialogue” format can foster learners’ cognitive activities more than monologue-style. However, designing online lectures in a dialogue style catered to the diverse needs of learners is laborious for instructors. We conducted a design workshop with eight educational experts and seven instructors to present key guidelines and the potential use of large language models (LLM) to transform a monologue lecture script into pedagogically meaningful dialogue. Applying these design guidelines, we created VIVID which allows instructors to collaborate with LLMs to design, evaluate, and modify pedagogical dialogues. In a within-subjects study with instructors (N=12), we show that VIVID helped instructors select and revise dialogues efficiently, thereby supporting the authoring of quality dialogues. Our findings demonstrate the potential of LLMs to assist instructors with creating high-quality educational dialogues across various learning stages."
“It’s the only thing I can trust”: Envisioning Large Language Model Use by Autistic Workers for Communication Assistance,"Jang, JiWoong and Moharana, Sanika and Carrington, Patrick and Begel, Andrew",10.1145/3613904.3642894,2024,"Autistic adults often experience stigma and discrimination at work, leading them to seek social communication support from coworkers, friends, and family despite emotional risks. Large language models (LLMs) are increasingly considered an alternative. In this work, we investigate the phenomenon of LLM use by autistic adults at work and explore opportunities and risks of LLMs as a source of social communication advice. We asked 11 autistic participants to present questions about their own workplace-related social difficulties to (1) a GPT-4-based chatbot and (2) a disguised human confederate. Our evaluation shows that participants strongly preferred LLM over confederate interactions. However, a coach specializing in supporting autistic job-seekers raised concerns that the LLM was dispensing questionable advice. We highlight how this divergence in participant and practitioner attitudes reflects existing schisms in HCI on the relative privileging of end-user wants versus normative good and propose design considerations for LLMs to center autistic experiences."
The Metacognitive Demands and Opportunities of Generative AI,"Tankelevitch, Lev and Kewenig, Viktor and Simkute, Auste and Scott, Ava Elizabeth and Sarkar, Advait and Sellen, Abigail and Rintel, Sean",10.1145/3613904.3642902,2024,"Generative AI (GenAI) systems offer unprecedented opportunities for transforming professional and personal work, yet present challenges around prompting, evaluating and relying on outputs, and optimizing workflows. We argue that metacognition—the psychological ability to monitor and control one’s thoughts and behavior—offers a valuable lens to understand and design for these usability challenges. Drawing on research in psychology and cognitive science, and recent GenAI user studies, we illustrate how GenAI systems impose metacognitive demands on users, requiring a high degree of metacognitive monitoring and control. We propose these demands could be addressed by integrating metacognitive support strategies into GenAI systems, and by designing GenAI systems to reduce their metacognitive demand by targeting explainability and customizability. Metacognition offers a coherent framework for understanding the usability challenges posed by GenAI, and provides novel research and design directions to advance human-AI interaction."
The Hidden Toll of Instant Messaging Use in Remote Work: Interaction Dynamics Between Subordinates and Supervisors,"Lee, Chia Hsin (Josette) and Yuan, Chien Wen (Tina)",10.1145/3613904.3642913,2024,"The research centers on examining the utilization of instant messaging (IM) in remote work interactions between supervisors and subordinates. Through a series of one-on-one in-depth interviews (n = 12), our findings unveil distinct nuances in how subordinates and supervisors perceive and engage with IM, encompassing aspects such as response strategies, interaction frequency, and interaction nature. Notably, a significant finding emerged concerning the practice of self-censorship in IM, acting as a strategy for impression management, predominantly adopted by subordinates in their interactions with supervisors. Additionally, our investigation sheds light on a contrasting viewpoint regarding the social use of IM. While supervisor participants acknowledged its potential to bolster work-related collaboration, this perspective appeared less pronounced among subordinate participants. Our study concludes by delving into the design implications for IM application design, offering insights that can shape collaborative dynamics within remote work environments."
Unblind Text Inputs: Predicting Hint-text of Text Input in Mobile Apps via LLM,"Liu, Zhe and Chen, Chunyang and Wang, Junjie and Chen, Mengzhuo and Wu, Boyu and Huang, Yuekai and Hu, Jun and Wang, Qing",10.1145/3613904.3642939,2024,"Mobile apps have become indispensable for accessing and participating in various environments, especially for low-vision users. Users with visual impairments can use screen readers to read the content of each screen and understand the content that needs to be operated. Screen readers need to read the hint-text attribute in the text input component to remind visually impaired users what to fill in. Unfortunately, based on our analysis of 4,501 Android apps with text inputs, over 76% of them are missing hint-text. These issues are mostly caused by developers’ lack of awareness when considering visually impaired individuals. To overcome these challenges, we developed an LLM-based hint-text generation model called HintDroid, which analyzes the GUI information of input components and uses in-context learning to generate the hint-text. To ensure the quality of hint-text generation, we further designed a feedback-based inspection mechanism to further adjust hint-text. The automated experiments demonstrate the high BLEU and a user study further confirms its usefulness. HintDroid can not only help visually impaired individuals, but also help ordinary people understand the requirements of input components. HintDroid demo video: https://youtu.be/FWgfcctRbfI."
Integrating measures of replicability into scholarly search: Challenges and opportunities,"Wu, Chuhao and Chakravorti, Tatiana and Carroll, John M. and Rajtmajer, Sarah",10.1145/3613904.3643043,2024,"Challenges to reproducibility and replicability have gained widespread attention, driven by large replication projects with lukewarm success rates. A nascent work has emerged developing algorithms to estimate the replicability of published findings. The current study explores ways in which AI-enabled signals of confidence in research might be integrated into the literature search. We interview 17 PhD researchers about their current processes for literature search and ask them to provide feedback on a replicability estimation tool. Our findings suggest that participants tend to confuse replicability with generalizability and related concepts. Information about replicability can support researchers throughout the research design processes. However, the use of AI estimation is debatable due to the lack of explainability and transparency. The ethical implications of AI-enabled confidence assessment must be further studied before such tools could be widely accepted. We discuss implications for the design of technological tools to support scholarly activities and advance replicability."
Software Technology Project Defect Prediction Based on AI,"Yuan, Yuan",10.1145/3616901.3616929,2024,"Abstract—In the rapidly developing information age, software development has become an important component of modern society. However, due to the complexity of the development process, project defects would inevitably occur in the software development process, leading to the extension of the software development cycle and the increase of costs. Therefore, this article explores and studies the issue of defect prediction in software technology projects based on artificial intelligence (AI). By analyzing and mining relevant data on project defects, a defect prediction model based on machine learning was established, and the effectiveness of the model was evaluated and analyzed. The results show that the average defect detection rate of strategy 3 is 0.88. The AI based software technology project defect prediction method proposed in this article can effectively improve the quality and efficiency of software development."
Phishing to improve detection,"Zheng, Sarah Ying and Becker, Ingolf",10.1145/3617072.3617121,2023,"Phishing e-mail scams continue to threaten organisations around the world. With generative artificial intelligence, conventional phishing detection advice such as looking out for linguistic errors and bad layouts will become obsolete. New approaches to improve people’s ability to detect phishing are essential. We report on promising results from two experiments (total N = 183) that engaging people with an adversarial mindset improves their ability to detect phishing e-mails compared to those who received conventional or no training. Participants who completed conventional training were nearly three times as likely to fall for a simulated phishing attack compared to those who completed the adversarial training, in which they watched a fictitious cybercriminal explain how to devise a targeted phishing e-mail, and then wrote targeted phishing e-mails themselves. Although further research is needed to examine the training’s long-term efficacy with larger sample sizes, the present findings show an encouraging alternative to conventional phishing training approaches."
Challenge Accepted? A Critique of the 2021 National Institute of Justice Recidivism Forecasting Challenge,"Jegede, Tobi and Gerchick, Marissa Kumar and Mathai, Amreeta S and Horowitz, Aaron",10.1145/3617694.3623242,2023,"In 2021, the National Institute of Justice — the research arm of the United States Department of Justice — released the “Recidivism Forecasting Challenge” (“the Challenge”) with the stated goals of “increas[ing] public safety and improv[ing] the fair administration of justice across the United States,” providing “critical information to community corrections departments...,” and ultimately “improv[ing]” the ability to forecast recidivism using person-and place-based variables” [68]. The Challenge was also designed, in part, to encourage “non-criminal justice forecasting researchers to compete against more ‘traditional’ criminal justice researchers” [68]. Challenge contestants had the opportunity to win part of the $723,000 in prize money for their submitted models. In this work, we highlight how the Challenge was underpinned by a techno-solutionist framing (emphasizing technical interventions without addressing underlying structural problems) [66] and plagued by serious ethical and methodological issues, including (1) the choice of training data and the selection of an outcome variable extracted from racially biased and inaccurate law enforcement data systems, (2) data leakage that may have seriously compromised the Challenge, (3) the choice of a faulty fairness metric, leading to the inability of submitted models to accurately surface any bias issues in the data selected for the Challenge, (4) the inclusion of candidate variables that created the potential for feedback loops, (5) a Challenge structure that arguably incentivized exploiting the metrics used to judge entrants, leading to the development of trivial solutions that could not realistically work in practice, and (6) the participation of Challenge contestants who demonstrated a lack of understanding of basic aspects of the U.S. criminal legal system’s structure and functions. We analyze the Challenge and its shortcomings through the lens of participatory design, applying emerging principles for robust participatory design practices in artificial intelligence (AI) and machine learning (ML) development to evaluate the Challenge’s structure and results. We argue that if the Challenge’s designers had adhered to these principles, the Challenge would have looked dramatically different or would not have occurred at all. We highlight several urgent needs and potential paths forward for any future efforts of this nature, recognizing the real and significant harms of recidivism prediction tools and the need to center communities directly impacted by policing and incarceration when thinking about whether to develop risk assessment tools."
Setting the Right Expectations: Algorithmic Recourse Over Time,"Fonseca, Jo\~{a}o and Bell, Andrew and Abrate, Carlo and Bonchi, Francesco and Stoyanovich, Julia",10.1145/3617694.3623251,2023,"Algorithmic systems are often called upon to assist in high-stakes decision making. In light of this, algorithmic recourse, the principle wherein individuals should be able to take action against an undesirable outcome made by an algorithmic system, is receiving growing attention. The bulk of the literature on algorithmic recourse to-date focuses primarily on how to provide recourse to a single individual, overlooking a critical element: the effects of a continuously changing context. Disregarding these effects on recourse is a significant oversight, since, in almost all cases, recourse consists of an individual making a first, unfavorable attempt, and then being given an opportunity to make one or several attempts at a later date — when the context might have changed. This can create false expectations, as initial recourse recommendations may become less reliable over time due to model drift and competition for access to the favorable outcome between individuals. In this work we propose an agent-based simulation framework for studying the effects of a continuously changing environment on algorithmic recourse. In particular, we identify two main effects that can alter the reliability of recourse for individuals represented by the agents: (1) competition with other agents acting upon recourse, and (2) competition with new agents entering the environment. Our findings highlight that only a small set of specific parameterizations result in algorithmic recourse that is reliable for agents over time. Consequently, we argue that substantial additional work is needed to understand recourse reliability over time, and to develop recourse methods that reward agents’ effort."
What Factors Affect the Build Failures Correction Time? A Multi-Project Study,"Silva, Gustavo and Bezerra, Carla and Uch\^{o}a, Anderson and Machado, Ivan",10.1145/3622748.3622753,2023,"Continuous Integration (CI) is a widely adopted practice in modern software engineering that involves integrating developers’ local changes with the project baseline daily. Despite its popularity, recent studies have revealed that integrating changes can be time-consuming, requiring significant effort to correct errors that arise. This can lead to development activities being paused, including the addition of new features and fixing bugs, while developers focus on analyzing and correcting build failures. In this study, we investigate the factors that influence the time taken to correct build failures in CI. Specifically, we analyze the impact of developer activity, project characteristics, and build complexity on build failure correction time. To conduct our analysis, we collected data from 18 industrial projects of a software company, calculating 13 metrics for each project based on the literature on build failures analysis. We used association rules, a data mining technique, to examine the relationship between the defined factors and build failure correction time. Our findings reveal significant correlations between the factors studied and the duration of build failure correction time. Specifically, we found that more experienced developers require less time to correct build failures, while build failures that originate in the early stages of the project are resolved more quickly. Additionally, we observed that build failures with more lines and modified files tend to have longer correction times. Overall, this study sheds light on the factors that impact build failure correction time in CI. By identifying these factors, our findings can help software development teams optimize their CI processes and minimize the impact of build failures on development activities."
A Tool for the Definition and Deployment of Platform-Independent Bots on Open Source Projects,"Ait, Adem and C\'{a}novas Izquierdo, Javier Luis and Cabot, Jordi",10.1145/3623476.3623524,2023,"The development of Open Source Software (OSS) projects is a collaborative process that heavily relies on active contributions by passionate developers. Creating, retaining and nurturing an active community of developers is a challenging task; and finding the appropriate expertise to drive the development process is not always easy. To alleviate this situation, many OSS projects try to use bots to automate some development tasks, thus helping community developers to cope with the daily workload of their projects. However, the techniques and support for developing bots is specific to the code hosting platform where the project is being developed (e.g., GitHub or GitLab). Furthermore, there is no support for orchestrating bots deployed in different platforms nor for building bots that go beyond pure development activities. In this paper, we propose a tool to define and deploy bots for OSS projects, which besides automation tasks they offer a more social facet, improving community interactions. The tool includes a Domain-Specific Language (DSL) which allows defining bots that can be deployed on top of several platforms and that can be triggered by different events (e.g., creation of a new issue or a pull request). We describe the design and the implementation of the tool, and illustrate its use with examples."
Arguments for and Approaches to Computing Education in Undergraduate Computer Science Programmes,"Cutts, Quintin and Kallia, Maria and Anderson, Ruth and Crick, Tom and Devlin, Marie and Farghally, Mohammed and Mirolo, Claudio and Runde, Ragnhild Kobro and Sepp\""{a}l\""{a}, Otto and Urquiza-Fuentes, Jaime and Vahrenhold, Jan",10.1145/3623762.3633494,2023,"Computing education (CE), the scientific foundation of the teaching and learning of subject matter specific to computing, has matured into a field with its own research journals and conferences as well as graduate programmes. Yet, and unlike other mature subfields of computer science (CS), it is rarely taught as part of undergraduate CS programmes. In this report, we present a gap analysis resulting from semi-structured interviews with various types of stakeholders and derive a set of arguments for teaching CE courses in undergraduate CS programmes. This analysis and the arguments highlight a number of opportunities for the discipline of CS at large, in academia, in industry, and in school education, that would be opened up with undergraduate CE courses, as well as potential barriers to implementation that will need to be overcome. We also report on the results of a Delphi process performed to elicit topics for such a course with various audiences in mind. The Delphi process yielded 19 high-level categories that encompass the subject matter CE courses should incorporate, tailored to the specific needs of their intended student audiences. This outcome underscores the extensive range of content that can be integrated into a comprehensive CE programme. Based on these two stakeholder interactions as well as a systematic literature review aiming to explore the current practices in teaching CE to undergraduate students, we develop two prototypical outlines of such a course, keeping in mind that departments may have different preferences and affordances resulting in different kinds of CE offerings. Overall, input from external stakeholders underscores the clear significance of undergraduate CE courses. We anticipate leveraging this valuable feedback to actively promote these courses on a broader scale."
Drone-Driven Running: Exploring the Opportunities for Drones to Support Running Well-being through a Review of Running and Drone Interaction Technologies,"Balasubramaniam, Aswin and Reidsma, Dennis and Heylen, Dirk",10.1145/3623809.3623831,2023,"There is an underexplored interaction space for drones that can be utilised as running interaction technology, distinct from human drone interaction that warrants foregrounding. This paper consolidates the current state of art in running interaction technology through a review of relevant studies and commercial technologies in a framework positioned using dimensions related to the form of interaction as identified in the sports ITECH framework. Our analysis highlights the unmet opportunities in running interaction technology and presents the potential of drones to further support runners. The potential of drones to support various forms of interaction are supported using exemplar research done in human-drone interaction. Through our findings, we hope to inform and expedite future research and practice in the field of running interaction technology and runner drone interaction by supporting researchers in defining and situating their contributions."
"A Study Comparing the Model Skill of LR, DF, and SVM in Determining Likelihood of Grant and Registration of Patents and Utility Models Assisted by Technology Application and Promotion Institute (DOST-TAPI) through Predictive Analytics","Tomas, John Paul Quilingking and Vidal, Janeth Cruzada",10.1145/3624288.3624291,2023,"Intellectual property (IP) assets from granted patents provide a competitive advantage to the company as these demonstrate their specialization and level of expertise in their product line. Hence, there is a need to ensure the grant of IP to the inventors-entrepreneurs to increase their capacity and success in commercialization. This work is to determine a predictive model for the likelihood of the grant of patents and registration of utility models with the available information regarding the technology, the characteristics of the inventor, and the length of time it took to file and obtain the certificate. The Logistic Regression (LR), Decision Forest (DF), and Support Vector Machine (SVM) classification methods are compared by measuring the performance of the models through some metrics such as accuracy, precision, and recall. Consequently, from the results of the models, insights and actionable recommendations can be made to develop strategies. On the side of the Institute, good predictions will aid in the appropriate management of the resources."
Overview and Tendencies of Augmented Reality Applications in Medicine,"Santos, Bruna S R and Trevisan, Daniela",10.1145/3625008.3625013,2024,"Recently, augmented reality (AR) has received increasing attention, with the development of AR devices such as head-mounted displays, haptic devices, and AR glasses. Healthcare can be considered to be one of the most effective applications of AR. In this article, we describe a literature systematic mapping conducted to investigate the state-of-the-art AR technology in order to help doctors and future doctors in a wide range of applications and opportunities. The 160 studies that were ultimately collected were categorized into 3 representative topics: main goals of using AR in healthcare, the most used technologies, and how these studies were evaluated or measured. Finally, we reflected on the biggest challenges for future research and opportunities in using AR next years. At this point, improvements in tracking techniques and appropriate assessments of the tool in a clinician’s daily routine could be essential for identifying points that were not seen in controlled testing scenarios."
Internet of Things and Industrial Business Models: Knowledge Boundaries and Practical Implications,"Del Vecchio, Vito and Menegoli, Marta",10.1145/3625078.3625089,2023,"Internet of Things (IoT) is recognised as a key enabling technology in Industry 4.0 for its ability to sense valuable data from physical things. It is also a disruptive driver for management strategies. Prior studies are mainly focused on the technological application and impacts of IoT in industries, neglecting its implications on business models. Considering such issue, based on a systematic literature review and a bibliometric analysis, this study aims to analyse the boundaries of knowledge related to the application of the IoT technology within business models, in order to reflect on and understand its most important intersection points and practical implications. Seven key topics that surround the meeting between IoT and business models emerged from the analysis, which were critically analysed in order to retrieve insights for academics, practitioners, managers."
Research on Talent Demand Analysis in Big Data Related Fields Based on Text Mining,"Yin, Jun and Zhang, Wei",10.1145/3625469.3625493,2023,"This article addresses the issue of talent shortage in the current big data industry by precisely identifying the required skills for job roles. The method involves obtaining job postings from enterprises on recruitment websites, preprocessing the text data, and combining Word2Vec to construct word vectors and K-means clustering for text mining. The results divide big data job roles into six categories with basic skill requirements divided into eight categories. By comparing and analyzing the differences in experience, education, and basic skills requirements among different job roles, this research provides management guidance for the precise training of big data talents to improve talent development efficiency and meet market demands."
Stable Diffusion in the Classroom: Deploying interactive GPU-enabled ML workloads with Open OnDemand and Kubernetes,"Jaffe, Evan and Na, Heechang and Johnson, Douglas and Dockendorf, Trey and Masri Zada, Basil",10.1145/3626203.3670526,2024,"This paper explores the deployment of Stable Diffusion instances in educational settings, particularly focusing on addressing resource allocation challenges in the context of interactive GPU-enabled machine learning (ML) workloads. Traditional approaches to providing GPU resources in High-Performance Computing (HPC) environments often involve assigning a full GPU to each user, which can be cost-prohibitive and inefficient for students in a classroom. To overcome this limitation, we deploy with a novel solution leveraging Kubernetes (k8s) and GPU sharing through Open OnDemand, a platform for accessing HPC resources via web interfaces. Our innovative approach demonstrates the feasibility and effectiveness of employing Kubernetes and Open OnDemand to deploy interactive GPU-enabled ML workloads in classroom environments."
A 'Microcredential in Advanced Computing' Program,"Fishbein, Grace Sophia and Monfared, Yashar Esfahani and Clarke, Sarah Melanie and Vermeyden, Lydia",10.1145/3626203.3670530,2024,"Over the past eighteen months, ACENET has been developing and preparing to deliver a new training program – a Microcredential in Advanced Computing – the first of its kind bringing new components to the training offered in digital skills. With support and consultation from industry partners, the skills taught in this program align with the needs in the province of Newfoundland and Labrador. Authentic assessments are incorporated throughout the program to assess both the effectiveness of the teaching and whether competencies are achieved. These progress assessments build to a final authentic assessment in the form of an independent study project at the end of the program. Upon successful completion of the program, participants will earn a documented and verifiable microcredential. The development of this program carefully considered the pedagogical approach, industry-relevant needs, curriculum development process, accessibility and user-friendliness of the learning management platform."
Cultivating Cyberinfrastructure Careers through Student Engagement at Texas A&amp;M University High Performance Research Computing,"Brashear, Wesley A. and Perez, Lisa M. and Leake, Elizabeth and Nite, Sandra and Pennings, Marinus and Stebenne, Sheri and Liu, Honggao and Chakravorty, Dhruva",10.1145/3626203.3670544,2024,"Texas A&amp;M University High Performance Research Computing (HPRC) has a rich history of fostering cyberinfrastructure (CI) careers by mentoring students. Previous student fellows have gone on to become CI professionals in academia, agencies, or in the commercial sector. The growing need for CI experts is only increasing and we need to build a robust national CI pipeline. At HPRC, students are involved with several aspects of operations, including research-facing systems support, education and training, outreach, application development, and testing novel HPC architecture. Furthermore, HPRC student researchers gain invaluable experience on using CI technologies. Here, students have benefitted from the novel opportunity to work with industry experts on two of the National Science Foundation's composable computing systems. These opportunities have prepared them to adapt to new technologies as they advance in their careers. Here, we share our framework to help other institutions deploy similar student programs."
Working with Large Code Bases: A Cognitive Apprenticeship Approach to Teaching Software Engineering,"Shah, Anshul and Yu, Jerry and Tong, Thanh and Soosai Raj, Adalbert Gerald",10.1145/3626252.3630755,2024,"Prior work has highlighted the gap between industry expectations for recent university graduates and the abilities those recent graduates possess. These works have even specifically recommended that students be given the opportunity to work on large, pre-existing code bases in their undergraduate career. This paper presents our experience teaching a newly-created course calledWorking with Large Code Bases. Guided by a Cognitive Apprenticeship approach to provide an authentic classroom experience that emphasizes the implicit processes and techniques involved in real-world software engineering, the course serves as a practical introduction to the skills and workflow involved in navigating and understanding a large code base. The goal of this experience report is to provide the motivation for key course design decisions, an overview of the course content, and a detailed description of key course components. We present student feedback indicating improved confidence in navigating a large code base and course outcomes related to specific tools and techniques students used in the course. Finally, we provide the full set of course materials we used and actionable recommendations for instructors to administer this course at their own institution, even with limited TA support."
Broadening Participation in Adult Education: A Literature Review of Computer Science Education,"Agbo, Friday Joseph",10.1145/3626252.3630797,2024,"Extending computer science (CS) education to address inclusion, diversity, and equity in all settings can broaden the participation of underrepresented groups including the adult education. Recently, studies have examined CS education at elementary and college levels, however, little is known in the context of adult education. This study systematically investigates past studies on computing education research in adult education (formal or informal) through the lenses of a literature review. The study sought to understand: (i) how research in this domain has evolved over the years; and (ii) what impact - in terms of learning outcomes - has been reported in the literature. Data were collected from three databases including the ACM digital library, Scopus, and Web of Science. Findings from this study show that despite CS in adult education had started since the 1980s, there is little scholarly progress and advancement witnessed in this domain. In addition, indicators measuring the impact of broadening participation in CS education among adults appear insignificant. Further, the learning outcomes reported in CS education research for adults includes motivation, increased interest, self-confidence, and computing knowledge. This study revealed several gaps and draws scholar's attention to broadening participation in Adults' CS education, highlighted study implications and future research agenda."
Exploring the Impact of Generative AI for StandUp Report Recommendations in Software Capstone Project Development,"Neyem, Andres and Sandoval Alcocer, Juan Pablo and Mendoza, Marcelo and Centellas-Claros, Leonardo and Gonzalez, Luis A. and Paredes-Robles, Carlos",10.1145/3626252.3630854,2024,"StandUp Reports play an important role in capstone software engineering courses, facilitating progress tracking, obstacle identification, and team collaboration. However, despite their significance, students often grapple with the challenge of creating StandUp Reports that are clear, concise, and actionable. This paper investigates the impact of the use of generative AI in producing StandUp report recommendations, aiming to assist students in enhancing the quality and effectiveness of their reports. In a semester-long capstone course, 179 students participated in 16 real-world software development projects. They submitted weekly StandUp Reports with the assistance of an AI-powered Slack, which analyzed their initial reports and provided suggestions for enhancing them using both GPT-3.5 and the early access GPT-4 API. After each submitted report, students voluntarily answered a survey about usability and suggestion preference. Furthermore, we conducted a linguistic analysis of the recommendations made by the algorithms to gauge reading ease and comprehension complexity. Our findings indicate that the AI-based recommendation system helped students improve the overall quality of their StandUp Reports throughout the semester. Students expressed a high level of satisfaction with the tool and exhibited a strong willingness to continue using it in the future. The survey reveals that students perceived a slight improvement when using GPT-4 compared to GPT-3.5. Finally, a computational linguistic analysis performed on the recommendations demonstrates that both algorithms significantly improve the alignment between the generated texts and the students' educational level, thereby improving the quality of the original texts."
Putting the Service into Service Learning: A Report on a Survey of CS Faculty,"Harrell, Robert Avery and Lentz, Sidney and Robledo Yamamoto, Fujiko and Voida, Amy and Barker, Lecia",10.1145/3626252.3630910,2024,"Service learning is an experiential pedagogy in which students learn through providing services or products for community partners. Computer and information science students can develop valuable products for community organizations. However, while service learning is shown to serve students and has potential to serve the field's diversity goals, community partners' needs are often not served. We explored this asymmetry using an exploratory survey. Faculty from across the U.S. were able to describe learning goals for students, including how they were assessed. In contrast, fewer than half of respondents had explicit partner goals; partner goals were often not assessed. Also, most respondents judged reaching student goals as more important than partner goals, with about 25% of respondents seeing benefits to partners as only a bonus. Faculty justified their choices by appealing to their mission as educators. Yet for a nontrivial partnership commitment under condition of scarce resources, the community partner may be seen as being taken advantage of, which may explain why some respondents have difficulty finding or keeping partners. Further, faculty may not accomplish civic duty goals, since students may tacitly learn that community organizations' needs are secondary. To aid faculty in making decisions and better integrating community partners' needs, we offer advice from survey respondents."
Software Engineering Education Must Adapt and Evolve for an LLM Environment,"Kirova, Vassilka D. and Ku, Cyril S. and Laracy, Joseph R. and Marlowe, Thomas J.",10.1145/3626252.3630927,2024,"In the era of artificial intelligence (AI), generative AI, and Large Language Models (LLMs) in particular, have become increasingly significant in various sectors. LLMs such as GPT expand their applications, from content creation to advanced code completion. They offer unmatched opportunities but pose unique challenges to the software engineering domain. This paper discusses the necessity and urgency for software engineering education to adapt and evolve to prepare software engineers for the emerging LLM environment. While existing literature and social media have investigated AI's integration into various educational spheres, there is a conspicuous gap in examining the specifics of LLMs' implications for software engineering education. We explore the goals of software engineering education, and changes to software engineering, software engineering education, course pedagogy, and ethics. We argue that a holistic approach is needed, combining technical skills, ethical awareness, and adaptable learning strategies. This paper seeks to contribute to the ongoing conversation about the future of software engineering education, emphasizing the importance of adapting and evolving to remain in sync with rapid advancements in AI and LLMs. It is hoped that this exploration will provide valuable insights for educators, curriculum developers, and policymakers in software engineering."
Use of AI-driven Code Generation Models in Teaching and Learning Programming: a Systematic Literature Review,"Cambaz, Doga and Zhang, Xiaoling",10.1145/3626252.3630958,2024,"The recent emergence of LLM-based code generation models can potentially transform programming education. To pinpoint the current state of research on using LLM-based code generators to support the teaching and learning of programming, we conducted a systematic literature review of 21 papers published since 2018. The review focuses on (1) the teaching and learning practices in programming education that utilized LLM-based code generation models, (2) characteristics and (3) performance indicators of the models, and (4) aspects to consider when utilizing the models in programming education, including the risks and challenges. We found that the most commonly reported uses of LLM-based code generation models for teachers are generating assignments and evaluating student work, while for students, the models function as virtual tutors. We identified that the models exhibit accuracy limitations; generated content often contains minor errors that are manageable by instructors but pose risks for novice learners. Moreover, risks such as academic misconduct and over-reliance on the models are critical when considering integrating these models into education. Overall, LLM-based code generation models can be an assistive tool for both learners and instructors if the risks are mitigated."
Application of UI/UX in Tourism Information Service Problems: A Review,"Firmansyah, Bernand and Jonathan, Markus and Andreas, Jose and Philip, Samuel and Hidayaturrahman",10.1145/3626641.3627603,2023,"Increased accessibility and efficiency in travel information services are in high demand as a result of technological developments. In order to better understand how UI/UX (User Interface/User Experience) design principles might help tourism information services, this study used the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) checklist technique to examine the existing literature. The research takes the form of a systematic review that looks at the ways in which UI/UX affects things like user participation, knowledge retention, and happiness. The results prove that putting the user first in UI/UX design increases the efficiency and happiness of travel resources for everyone involved. Improved information retrieval and user engagement can be achieved through the strategic use of visual design components, interactive features, and personal experiences. The research shows how crucial it is to cater UI/UX to the specific requirements of the intended users. The importance of using the PRISMA checklist approach to perform a thorough and open literature review is also highlighted. Based on the findings of this research, tourism service providers can better apply UI/UX design concepts to their information services, resulting in a more satisfying experience for their customers. To further improve UI/UX, studies could investigate specific design methods and technological developments in the tourism information services industry. As a whole, bringing together UI and UX design principles has the potential to completely change the way people interact with the tourism business."
"Conversational Interfaces in IoT Ecosystems: Where We Are, What Is Still Missing","Gallo, Simone and Paterno, Fabio and Malizia, Alessio",10.1145/3626705.3627775,2023,"In the last few years, text and voice-based conversational agents have become more and more popular all over the world as virtual assistants for a variety of tasks. In addition, the deployment on the market of many smart objects connected with these agents has introduced the possibility of controlling and personalising the behaviour of several connected objects using natural language. This has the potential to allow people, also those without a technical background, to effectively control and use the wide variety of connected objects and services. In this paper, we present an analysis of how conversational agents have been used to interact with smart environments (such as smart homes). For this purpose, we have carried out a systematic literature review considering publications selected from the ACM and IEEE digital libraries to investigate the technologies used to design and develop conversational agents for IoT settings, including Artificial Intelligence techniques, the purpose that they have been used for, and the level of user involvement in such studies. The resulting analysis is useful to better understand how this field is evolving and indicate the challenges still open in this area that should be addressed in future research work to allow people to completely benefit from this type of solution."
ReFer: Retrieval-Enhanced Vertical Federated Recommendation for Full Set User Benefit,"Li, Wenjie and Wang, Zhongren and Wang, Jinpeng and Xia, Shu-Tao and Zhu, Jile and Chen, Mingjian and Fan, Jiangke and Cheng, Jia and Lei, Jun",10.1145/3626772.3657763,2024,"As an emerging privacy-preserving approach to leveraging cross-platform user interactions, vertical federated learning (VFL) has been increasingly applied in recommender systems. However, vanilla VFL is only applicable to overlapped users, ignoring potential universal interest patterns hidden among non-overlapped users and suffers from limited user group benefits, which hinders its application in real-world recommenders.In this paper, we extend the traditional vertical federated recommendation problem (VFR) to a more realistic Fully-Vertical federated recommendation setting (Fully-VFR) which aims to utilize all available data and serve full user groups. To tackle challenges in implementing Fully-VFR, we propose a Retrieval-enhanced Vertical Federated recommender (ReFer), a groundbreaking initiative that explores retrieval-enhanced machine learning approaches in VFL. Specifically, we establish a general ""retrieval-and-utilization"" algorithm to enhance the quality of representations across all parties. We design a flexible federated retrieval augmentation (RA) mechanism for VFL: (i) Cross-RA to complement field missing and (ii) Local-RA to promote mutual understanding between user groups. We conduct extensive experiments on both public and industry datasets. Results on both sequential and non-sequential CTR prediction tasks demonstrate that our method achieves significant performance improvements over baselines and is beneficial for all user groups."
GuardRails: Automated Suggestions for Clarifying Ambiguous Purpose Statements,"Pawagi, Mrigank and Kumar, Viraj",10.1145/3627217.3627234,2023,"Before implementing a function, programmers are encouraged to write a purpose statement i.e., a short, natural-language explanation of what the function computes. A purpose statement may be ambiguous i.e., it may fail to specify the intended behaviour when two or more inequivalent computations are plausible on certain inputs. Our paper makes four contributions. First, we propose a novel heuristic that suggests such inputs using Large Language Models (LLMs). Using these suggestions, the programmer may choose to clarify the purpose statement (e.g., by providing a functional example that specifies the intended behaviour on such an input). Second, to assess the quality of inputs suggested by our heuristic, and to facilitate future research, we create an open dataset of purpose statements with known ambiguities. Third, we compare our heuristic against GitHub Copilot’s Chat feature, which can suggest similar inputs when prompted to generate unit tests. Fourth, we provide an open-source implementation of our heuristic as an extension to Visual Studio Code for the Python programming language, where purpose statements and functional examples are specified as docstrings and doctests respectively. We believe that this tool will be particularly helpful to novice programmers and instructors."
Evaluating Copilot on CS1 Code Writing Problems with Suppressed Specifications,"Venkatesh, Varshini and Venkatesh, Vaishnavi and Kumar, Viraj",10.1145/3627217.3627235,2023,"Code writing problems in introductory programming (CS1) courses typically ask students to write simple functions or programs based on detailed natural-language specifications. These details can be leveraged by large language models (LLMs), accessible to students via tools such as GitHub Copilot, to generate solutions that are often correct. CS1 instructors who are unwilling or unable to prohibit such usage must consider variants of traditional code writing problems that align with their learning objectives but are more difficult for LLMs to solve. Since LLMs are sensitive to the level of details in their prompts, it is natural to consider variants where details are progressively trimmed from the specifications of traditional code writing problems, and consequent ambiguities are clarified via examples. We consider an extreme variant, where all natural language is suppressed except for meaningful names of functions and their arguments. We evaluate the performance of Copilot on suppressed specification versions of 153 such problems drawn from the CodeCheck repository. If Copilot initially fails to generate a correct solution, we augment each suppressed specification with as few clarifying examples as possible to obtain a correct solution. Copilot solves 134 problems (87%) with just 0.7 examples on average, requiring no examples in 78 instances. Thus, modifying traditional code-writing problems by merely trimming specification details is unlikely to thwart sophisticated LLMs such as GitHub Copilot."
Develop an Intelligent System of Construction Safety Management using BIM and Multi-Sensor,"Zhou, Chonghua and Yang, Jianan",10.1145/3627341.3630414,2023,"Safety has always been one of the most concerned issues in the Architecture Engineering and Construction (AEC) industry, On-Site Construction Safety Management (OCSM)is adopted to ensure the safety of personnel and property during the construction process and to achieve the aim at preventing and reducing accidents.In the study, the authors will explorea new OCSM system using Building Information Modeling (BIM) and multi-sensor. The contribution to this paper is in three aspects, the first is that using multi-source heterogeneous sensors to intelligently perceiveall factors of safety managementin complex construction sites, the second is that developing a decision support platform based on BIM and the multi-sensor systems, the last is that applying the new OCSM system to improve safety management ability based on the decision supported platform.Firstly this paper summarizes the existing literatures on the application of sensors including BIM in OCSM, next the authors adopt the approach of “design thinking” to define problems and propose solutions, develop a prototype system and test the system in a demonstration construction project lastly. The findings show that the integration of BIM and multi-sensor has a great potential application in OCSM.Using the proposed solutions, it can solve the problem of data opacity in the process of OCSM, so that on-site managers and engineer can quickly master the latest and most accurate data all factors of safety management. The decision supported platformintegrating spatial and temporal dimensions of BIM and the multi-sensor system that can assist on-site safety to raise management efficiency, reduce the number of managers, and save labor costs."
Prospects and Challenges in COVID-19 Study: A Review Based on Influencing Factors and Analysis Methods,"Liu, Lisai and Zhu, Dashuang and Zheng, Tianlong and Niu, Yiheng and Zhang, Zhao and Zhang, Zihan and Zhu, Xu and Liu, Sheng",10.1145/3627377.3627402,2023,"The unprecedented Coronavirus Disease 2019 had spread and diffused in many countries and regions worldwide, posing a serious threat to people's health and lives. This review presents a systematic analysis and summary of representative research on the role of AI technology in the prediction and containment of COVID-19 and influencing factors of its transmission. To further explore the influences of multiple factors on the incidence and transmission of COVID-19, we used total cases per million (TCPM) and total deaths per million (TDPM) as dependent variables and influencing factors as explanatory variables to conduct multiple linear regression, and constructed four training models: xgboost model (XGBR), random forest regenerator (RFR), extra trees regenerator (ETR), and gradient boosting regenerator (GBR) to predict the test data. This study highlights the contribution and application prospects of AI related technologies in the study of the spread of COVID-19, which will help deepen the global understanding of the COVID-19 pandemic, and provide a reference for curbing the wanton spread of COVID-19 and other diseases again."
A Review on Data-driven Methods for Studying Hygrothermal Transfer in Building Exterior Walls,"Zhu, Yurong",10.1145/3627377.3627409,2023,"This review aims to comprehensively assess and synthesize the existing literature on the use of data-driven methods for studying hygrothermal transfer in building exterior walls. The review is conducted by an exhaustive search strategy to identify relevant articles from Web of Science and Scopus databases. There are 20 eligible studies included in this review following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) protocol. The most used data-driven methods are traditional neural networks, such as Multi-Layer Perceptrons and 2D Convolutional Neural Networks. Results suggested that neural network models hold potential for accurately predicting hygrothermal attributes of building exteriors. However, a conspicuous gap in the literature is the absence of studies drawing direct comparisons between data-driven methodologies and conventional simulation techniques."
Spam Article Detection on Social Media Platform Using Deep Learning: Enhancing Content Integrity and User Experience,"Chiawchansilp, Panuwat and Kantavat, Pittipol",10.1145/3628454.3628459,2023,"In the digital age, the widespread use of online social media corresponds with a rise in spam. These messages often appear as forceful ads, sales offer, gambling promotions, or repetitive chain messages, and they compromise users' experiences and the overall communication quality on these platforms. Various strategies have been developed to tackle this problem. Some strategies involve human reviewers, which is a straightforward yet time-consuming method. Others utilize advanced techniques like machine and deep learning. Even though a substantial amount of research has applied these methods to English-based spam, studies focusing on Thai language spam are limited. Given this context, our study seeks to develop tools based on machine and deep learning techniques to identify and classify Thai spam content effectively. The central goal of our research is to compare these techniques and determine the most effective model for classifying Thai spam content."
Learning from Learning - Design-Based Research Practices in Child-Computer Interaction,"Torgersson, Olof and Baykal, G\""{o}k\c{c}e Elif and Eriksson, Eva",10.1145/3628516.3655754,2024,"Inspired by the use of design-based research (DBR) in the learning sciences, in this paper, we discuss the promise of applying DBR in Child-Computer Interaction (CCI). As much research in CCI is related to learning interventions and educational contexts, we believe that DBR can be highly relevant for CCI, but that it has not yet reached its full potential in the field. We argue that DBR as a research approach can help mature the field, by explicitly grounding research design and interventions in theory, foster better impact beyond project completion, and bridge theory and practice through clarified knowledge contributions. Grounded in the characteristics of DBR, and based on a scoping review, this paper provides a timely snapshot of DBR literature, practices and research contributions in CCI research. Based on this, we will discuss further implications and opportunities of DBR in the CCI field."
On the Value of Code Embedding and Imbalanced Learning Approaches for Software Defect Prediction,"Thi-Mai-Anh, Bui and Nhat-Hai, Nguyen",10.1145/3628797.3628963,2023,"Automated software defect prediction aims to identify and estimate the likelihood of defects in software source code elements, seeking to enhance software quality while reducing testing costs. Previous research on software defect prediction primarily concentrated on investigating design-related features such as source code complexity and object-oriented design metrics for the purpose of classifying program elements into two categories: (i) defective and (ii) non-detective. Nevertheless, the majority of these studies have relied solely on hand-crafted software metrics, neglecting the valuable asset of source code instruction, which can play a pivotal role in detecting bugs. This study leverages the use of source code embedding techniques to extract essential information from program elements through a convolutional neural network. The likelihood of a source file element (e.g., class or method) being defective is established through the utilization of a fully connected network that incorporates both source code features and design-related attributes. Additionally, we explore specific imbalanced learning strategies to address the skewed defect data distribution issue. To assess the effectiveness of our proposed approach, we conducted experiments on the publicly available dataset, namely PROMISE. The empirical results consistently showcase the superior performance of our method, as it effectively predicts defective source files, outperforming other state-of-the-art models."
Research on the Construction of Intelligent Programming Platform Based on AI-generated Content,"Wu, Hao and Fa, Daidong and Wu, Xiaoling and Tan, Weijun and Chang, Xiwen and Gao, Ying and Weng, Jinta",10.1145/3629296.3629298,2024,"Programming education can play an important role in cultivating students' higher-order thinking ability and enhancing the competitiveness of an intelligent society. However, a series of problems still exist: the lack of high-quality programming teachers, the low degree of personalization, and the insufficient learning interaction. This study constructs a novel programming learning model based on AI-generated content technology under the guidance of constructivism learning theory and cognitive load theory. The model divides programming learning into the process of learning program development, personalized context creation, learning chain of thought execution and practice exercises, and real-time interaction throughout the process. On this basis, the platform alleviates the shortage of teachers through intelligent programming assistants, realizes personalized programming learning through learning analytics and customized learning settings, as well as using real-time interaction throughout the whole process to change the status quo of insufficient interaction. Compared with traditional programming learning and practice methods, AIGC-based programming learning platforms can provide a more personalized learning experience and more timely learning interactions, which can effectively enhance learners' interest and learning quality."
Trends and Challenges of Project-Based Learning in Computer Science and Engineering Education,"Rehman, Sadaqat Ur",10.1145/3629296.3629360,2024,"In computer science and engineering (CSE) education, project-based learning (PBL) has become a highly effective pedagogical strategy for encouraging students' teamwork, creativity, and problem-solving abilities in the real world. The current trends and difficulties with PBL implementation in computer science and engineering courses are thoroughly examined in this research. Within the context of computer science and engineering education, the evolution and integration of Problem-Based Learning (PBL) is thoroughly analyzed. Emphasis is placed on its numerous advantages such as enhanced critical thinking, elevated student engagement, and the development of essential practical skills beneficial for a successful career in the field. Thus, a number of trends—including the growing demand for industry-relevant skills, the shift towards interdisciplinary collaboration, and the growing significance of open-source contributions in academic and professional settings—have an impact on the adoption of PBL. Consideration is also given to how technology, like virtual and augmented reality, bolsters immersive and group PBL experiences. However, a number of obstacles prevent the widespread adoption and successful application of PBL. These challenges include faculty reluctance because a change in conventional teaching methods is necessary, a lack of infrastructure and resources, difficulties with assessment and evaluation, and the potential for an unequal distribution of effort among students. With an emphasis on faculty development, resource allocation, and the creation of efficient evaluation strategies, I offer solutions to these issues. Finally, I discuss PBL's future directions in computer science and engineering education, highlighting the need for ongoing development and adjustment to the always-changing technological environment. At the same time that I acknowledge the persistent difficulties that educators and institutions continue to face, I also emphasize the value of PBL in preparing students for the dynamic and challenging nature of computer science and engineering education."
The Application Landscape and Research Status of Artificial Intelligence in Language Learning: A Visual Analysis,"Chen, Xiaojiao and Zhu, Keke and Jing, Yuhui and Wang, Chengliang",10.1145/3629296.3629370,2024,"The integration of Artificial Intelligence (AI) in the education industry has emerged as a dominant trend, particularly in the field of language education and learning. This study employs advanced visualization tools, such as VOSviewer, to conduct a comprehensive analysis of the current research landscape and key areas of focus pertaining to the amalgamation of foreign language learning with AI. The findings reveal the following insights: 1) Most articles in this domain are published in journals affiliated with the educational technology and computer education disciplines, with a notable prevalence of open-access publications. 2) Prominent research themes encompass the continuous advancements in AI technology and the dynamic adaptation of language learning to embrace these transformative innovations. 3) Investigations into the application of AI in language learning primarily involve researchers from China and the United States, although collaborative networks among these scholars remain to be firmly established. 4) Employing clustering analysis, this study delineates two overarching categories within the field, namely ""intelligent tutoring systems"" and ""natural language processing."" By elucidating the current state of research and discerning future trends, this study provides valuable insights and guidance to scholars and practitioners, facilitating further empirical investigations and practical implementations. Additionally, we anticipate that the findings presented herein will serve as a theoretical benchmark for the advancement of language learning and education within the realm of AI-driven technological transformations."
"""Project Driven Practical Teaching Reform Based on U+ EEC - Taking the Course \""Engineering Training II\"" as an Example""","Wang, Xin and Du, Mingxing and Zhang, Jinglei and He, Xiaoping",10.1145/3629296.3629375,2024,"In order to promote the construction of emerging engineering practice courses, the article proposes to use the U+ Engineering Education Cloud (U+ EEC) platform to carry out project driven teaching, and combs its process in detail. Firstly, taking ""Engineering Training II"" training course as an example, this paper uses OBE achievement oriented concept to reconstruct the teaching design, and reversely designs the curriculum outline according to the graduation requirements. Then, combined with the CDIO Engineering Education Concept, each link of project driven teaching is designed. With the help of U+ EEC, project management and process evaluation are carried out to form the ""1+8"" project driven teaching mode. Finally, through the data analysis of the degree of achievement of the curriculum objectives and the questionnaire survey, the teaching reform methods proposed in this paper are effective and have great promotion value."
Digital Transformation of SMEs during the COVID-19 era: Obstructive Factors and Pathways,"Chen, Xinhui and Shi, Yuting and Chen, Jiale",10.1145/3629378.3629386,2023,"Through literature analysis on digital transformation in recent years, the connotation of digital transformation is clarified, and then the obstructive factors in digital transformation encountered by SMEs are summarized systematically. Based on digital transformation theory, the process and motivation of XH Company's digital transformation in the context of the epidemic are explored in depth, divided into two stages, and the results of the study can be used as experience and insights for the digital transformation of other SMEs."
Analyzing Cause and Effect Relationships of Obstacles to Applying Intelligent Robots in Construction Projects based on Grey-DEMATEL,"Zhang, Jiaying",10.1145/3629378.3629387,2023,"With the digital intelligence transformation of the construction industry in recent years, construction projects have faced many obstacles in applying intelligent robots, but the interaction between the obstacles is unclear. This paper aims to clarify the obstacles to applying intelligent robots in construction projects and to investigate the cause and effect relationships between the obstacles further. First, a Systematic Literature Review was used to extract the obstacles from the literature. Second, based on the ""Technology-Organization-Environment"" model, the dimensions of the obstacles were classified to establish a framework of obstacles to the application of intelligent robots in construction projects. Third, the Grey-DEMATEL was used to explore the cause and effect relationships between obstacles to applying intelligent robots in construction projects. The study found that inadequate incentives and regulatory mechanisms, immature technology and a lack of senior management and leadership are the three main causes of barriers and should be addressed first. This study helps construction projects overcome the obstacles to applying intelligent robots in an orderly manner, promotes digital intelligence transformation in the construction industry, and provide guidance for stakeholders and relevant government departments to develop reasonable policies."
Study on the Evaluation Index System of Smart Construction Site Based on DEMATEL-ANP,"Yang, Jie and Xu, Xueming",10.1145/3629378.3629449,2023,"In today's era of rapid development of intelligent information technology, with the use of technological means to upgrade the traditional construction sites have become a major trend, with the concept of smart city, Smart construction sites also respond to the changes of the times as a new management concept came into being, aimed at the whole life cycle of construction projects, the full use of BIM technology, Internet of Things technology, intelligent technology and other types of information-based intelligent technology It aims to improve the project site management level and realize Smart construction sites control by making full use of various information technology such as BIM technology, IOT technology, intelligent technology, etc., to control and manage key factors such as ""human, machine, material, method and environment"", and thus form an intelligent integration platform for interconnection and information sharing. However, the existing theory of Smart construction site is still in the exploration stage, and there is no corresponding evaluation index system to clarify the stage of the Smart construction site. Based on this, this paper will build an evaluation index system of Smart construction site with the help of scientific evaluation methods in order to better understand the stage of Smart construction site. This paper only studies the internal influencing factors of the smart construction site, so the constituent factors are the internal influencing factors. Then, with the help of DEMATEL method, the factors constituting the smart construction site system are analyzed, and the evaluation index levels are scientifically divided based on the weight of each factor. After that, ANP method is used to calculate the final weight and key factors of each indicator."
Unveiling Quality in Chatbot Conversations: Quantitative Analysis of Chatbot Requirements,"Silva, Geovana Ramos Sousa and Canedo, Edna Dias",10.1145/3629479.3629481,2023,"As conversational assistants and natural language interfaces proliferate, the demand for a precise understanding of quality software requirements for chatbots becomes increasingly critical. In this work, we adopted a quantitative methodology, scrutinizing a dataset composed of conversational requirements from a diverse range of agile projects for chatbot development, and identified meaningful patterns in the language and structure utilized. Our investigation led to significant findings, revealing the importance of structured documentation, conversation flow, and user interaction in the development of chatbots, with the most desired quality attributes being capability, naturalness, straightforwardness, and clarity. In addition, a significant emphasis was placed on feature development and meeting acceptance criteria. The research also illuminated the iterative nature of chatbot development, with a recurrent presence of verbs related to improvement or refactoring. While less pronounced, the roles of documentation and testing in ensuring chatbot quality and effectiveness were also noted. This work provides valuable insights into chatbot requirements management and the significance of quality attributes in chatbot development."
Enhancing Issue Management through the Employment of Inspection Technique: An Experience Report in The Industry,"Castro, Renata and Oliveira, Flavia and Rodrigues, Janderson and Tiago, Leonardo and Sousa, Cesar and Chaves, Lennon",10.1145/3629479.3629489,2023,"The software industry has been adopting the concept of Global Software Development to release products quicker and with fewer costs, assuring quality by employing software testing techniques which are able to detect issues early. In this context, this paper presents an experience report performed at SIDIA Institute of Science and Technology between the years of 2020 and 2022, during which a process of issue management (IM) was developed to reduce the problems in the issue report, such as: lack of cohesion, absence of confirmation tests and long learning curve time of the testing team. In order to face these challenges, the IM process employs inspection techniques in order to improve the quality of reports, and then, increase its effectiveness, i.e., the number of valid issues. In addition, to support the IM process, it was created an onboarding phase, a template for issue reports and the monitoring of issues until the confirmation test. Through the implementation of this process, there was evaluated a total amount of 11002 issues report, and as a result, it was observed that the issues’ effectiveness rate was increased from to (raising by ) in the period in which this research was performed. In sequence, it was detected that around of issues had problems related to issue report writing. Furthermore, qualitative research revealed that of the interviewed testers consider the process effective. The authors hope that lessons learned described in this paper can contribute to the academic and industry communities by regarding the employment of inspection techniques to evaluate issue reports."
Impacts of using PDCA in the requirements specification process,"Ferreira Costa, Alex Felipe and Tom\'{e} Oliveira, Vict\'{o}ria and Leit\~{a}o Dantas, Val\'{e}ria Lelli and De Sousa Santos, Ismayle and De Castro Andrade, Rossana Maria and Lopes Gomes, Rafael",10.1145/3629479.3629511,2023,"The requirements specification plays a fundamental role in the software development process, impacting the final product’s quality directly. However, many projects have faced challenges related to the requirements specification. In this context, this article presents an experience report of implementing improvements in the requirements specification process of a software project in an academia-industry partnership. This report highlights the importance of enhancing that process to address issues associated with extensive and complex requirements documentation. Thus, we used the PDCA cycle (Plan-Do-Check-Act) and apply the lessons learned from the PDCA’s Check stage to improve the requirements specification process, making it clearer, more concise, and comprehensible for the stakeholders in the project. This article provides detailed descriptions of the PDCA cycle stages such as planning improvements, implementing actions and evaluation results and lessons learned. This PDCA-based approach showed to be effective in identifying and resolving problems in requirements specification activities, promoting positive changes in the requirements process."
An Adaptive Logging System (ALS): Enhancing Software Logging with Reinforcement Learning Techniques,"Khosravi Tabrizi, Amirmahdi and Ezzati-Jivan, Naser and Tetreault, Francois",10.1145/3629526.3645033,2024,"The efficient management of software logs is crucial in software performance evaluation, enabling detailed examination of runtime information for postmortem analysis. Recognizing the importance of logs and the challenges developers face in making informed log-placement decisions, there is a clear need for a robust log-placement framework that supports developers. Existing frameworks, however, are limited by their inability to adapt to customized logging objectives, a concern highlighted by our industrial partner, Ciena, who required a system for their specific logging goals in resource-limited environments like routers. Moreover, these frameworks often show poor cross-project consistency. This study introduces a novel performance logging objective designed to uncover potential performance-bugs, categorized into three classes-Loops, Synchronization, and API Misuses-and defines 12 source code features for their detection. We present an Adaptive Logging System (ALS), based on reinforcement learning, which adjusts to specified logging objectives, particularly for identifying performance-bugs. This framework, not restricted to specific projects, demonstrates stable cross-project performance. We trained and evaluated ALS on Python source code from 17 diverse open-source projects within the Apache and Django ecosystems. Our findings suggest that ALS has the potential to significantly enhance current logging practices by providing a more targeted, efficient, and context-aware logging approach, particularly beneficial for our industry partner who requires a flexible system that adapts to varied performance objectives and logging needs in their unique operational environments."
"A Systematic Review of Biometric Monitoring in the Workplace: Analyzing Socio-technical Harms in Development, Deployment and Use","Awumey, Ezra and Das, Sauvik and Forlizzi, Jodi",10.1145/3630106.3658945,2024,"Modern advances in AI have increased employer interest in tracking workers’ biometric signals — e.g., their brainwaves and facial expressions — to evaluate and make predictions about their performance and productivity. These technologies afford managers information about internal emotional and physiological states that were previously accessible only to individual workers, raising new concerns around worker privacy and autonomy. Yet, the research literature on the impact of AI-powered biometric work monitoring (AI-BWM) technologies on workers remains fragmented across disciplines and industry sectors, limiting our understanding of its impacts on workers at large. In this paper, we sytematically review 129 papers, spanning varied disciplines and industry sectors, that discuss and analyze the impact of AI-powered biometric monitoring technologies in occupational settings. We situate this literature across a process model that spans the development, deployment, and usage phases of these technologies. We further draw on Shelby et al.’s Taxonomy of Socio-technical Harms in AI systems to systematize the harms experienced by workers across the three phases of our process model. We find that the development, deployment, and sustained use of AI-powered biometric work monitoring technologies put workers at risk of a number of the socio-technical harms specified by Shelby et al.: e.g., by forcing workers to exert additional emotional labor to avoid flagging unreliable affect monitoring systems, or through the use of these data to make inferences about productivity. Our research contributes to the field of critical AI studies by highlighting the potential for a cascade of harms to occur when the impact of these technologies on workers is not considered at all phases of our process model."
Transparency in the Wild: Navigating Transparency in a Deployed AI System to Broaden Need-Finding Approaches,"Turri, Violet and Morrison, Katelyn and Robinson, Katherine-Marie and Abidi, Collin and Perer, Adam and Forlizzi, Jodi and Dzombak, Rachel",10.1145/3630106.3658985,2024,"Transparency is a critical component when building artificial intelligence (AI) decision-support tools, especially for contexts in which AI outputs impact people or policy. Effectively identifying and addressing user transparency needs in practice remains a challenge. While a number of guidelines and processes for identifying transparency needs have emerged, existing methods tend to approach need-finding with a limited focus that centers around a narrow set of stakeholders and transparency techniques. To broaden this perspective, we employ numerous need-finding methods to investigate transparency mechanisms in a widely deployed AI-decision support tool developed by a wildlife conservation non-profit. Throughout our 5-month case study, we conducted need-finding through semi-structured interviews with end-users, analysis of the tool’s community forum, experiments with their ML model, and analysis of training documents created by end-users. We also held regular meetings with the tool’s product and machine learning teams. By approaching transparency need-finding from a broad lens, we uncover insights into end-users’ transparency needs as well as unexpected uses and challenges with current transparency mechanisms. Our study is one of the first to incorporate such diverse perspectives to reveal an unbiased and rich view of transparency needs. Lastly, we offer the FAccT community recommendations on broadening transparency need-finding approaches, contributing to the evolving field of transparency research."
A Critical Survey on Fairness Benefits of Explainable AI,"Deck, Luca and Schoeffer, Jakob and De-Arteaga, Maria and K\""{u}hl, Niklas",10.1145/3630106.3658990,2024,"In this critical survey, we analyze typical claims on the relationship between explainable AI (XAI) and fairness to disentangle the multidimensional relationship between these two concepts. Based on a systematic literature review and a subsequent qualitative content analysis, we identify seven archetypal claims from 175 scientific articles on the alleged fairness benefits of XAI. We present crucial caveats with respect to these claims and provide an entry point for future discussions around the potentials and limitations of XAI for specific fairness desiderata. Importantly, we notice that claims are often (i) vague and simplistic, (ii) lacking normative grounding, or (iii) poorly aligned with the actual capabilities of XAI. We suggest to conceive XAI not as an ethical panacea but as one of many tools to approach the multidimensional, sociotechnical challenge of algorithmic fairness. Moreover, when making a claim about XAI and fairness, we emphasize the need to be more specific about what kind of XAI method is used, which fairness desideratum it refers to, how exactly it enables fairness, and who is the stakeholder that benefits from XAI."
Monitoring Automotive Software Security Health through Trustworthiness Score,"Sapin, Etienne and Menon, Suraj and Ge, Jingquan and Habib, Sheikh Mahbub and Heymann, Maurice and Li, Yuekang and Palige, Rene and Byman, Gabriel and Liu, Yang",10.1145/3631204.3631859,2023,"The automotive industry is drastically moving towards autonomous. This trend constitutes in a fundamental change of going from mechanical and electrical engineering towards software-driven approaches. Modern vehicles can embed more than hundred electronic control units (ECUs). As autonomous vehicles require more intelligence as well as more computing power, high-performance computers (HPCs) bring the data management capabilities for cloud and IoT services to support the transition to a service-oriented vehicle system architecture. With this growing reliance on software in vehicles, software reliability and trustworthiness are increasingly critical to vehicle security. Measuring security trustworthiness in automotive software is even more valuable as cybersecurity is shifting to the left, i.e. in the early phase of development and design process. In this article, we propose a novel method for evaluating security trustworthiness of automotive software by leveraging a computational trust model. The method consists of selecting different domains contributing to software security, calculating their respective expectation value (trustworthiness score) and combining it using operators from the computational trust model. We evaluate the method using an automotive use case, i.e. over-the-air (OTA) update software. We describe a possible integration of the proposed method into a solution which would be valuable for cybersecurity stakeholders, e.g. cybersecurity managers, cybersecurity architects and software quality managers, aiming to monitor security health of automotive software throughout its development life cycle."
Evaluation of Free and Open Source Tools for Automated Software Composition Analysis,"Bottner, Laura and Hermann, Artur and Eppler, Jeremias and Th\""{u}m, Thomas and Kargl, Frank",10.1145/3631204.3631862,2023,"Vulnerable or malicious third-party components introduce vulnerabilities into the software supply chain. Software Composition Analysis (SCA) is a method to identify direct and transitive dependencies in software projects and assess their security risks and vulnerabilities. In this paper, we investigate two open source SCA tools, Eclipse Steady (ES) and OWASP Dependency Check (ODC), with respect to vulnerability detection in Java projects. Both tools use different vulnerability detection methods. ES implements a code-centric and ODC a metadata-based approach. Our study reveals that both tools suffer from false positives. Furthermore, we discover that the success of the vulnerability detection depends on the underlying vulnerability database. Especially ES suffered from false negatives because of the insufficient vulnerability information in the database. While code-centric and metadata-based approaches offer significant potential, they also come with their respective downsides. We propose a hybrid approach assuming that combining both detection methods will lead to less false negatives and false positives."
Knowledge Graph-Based Recommendation System for Personalized E-Learning,"Baig, Duaa and Nurbakova, Diana and Mbaye, Baba and Calabretto, Sylvie",10.1145/3631700.3665229,2024,"Due to the large amount of available e-learning data, identifying relevant information from e-learning data presents significant challenges. A recommendation system is a popular solution to provide relevant data to any user but it also faces challenges such as scalability, processing large volumes of data, addressing the cold start problem, predicting personalized recommendations, and providing an adaptive recommendation, etc.. In this paper, we present an efficient knowledge graph-based recommendation framework, which can provide personalized e-learning recommendations to existing or new target learners without having enough historical data of that target learner. The proposed framework includes five modules i.e. Data Module, Knowledge Graph Representation Module, Community Building Module, Graph Embedding Module, and Recommendation Module. It is based on knowledge graphs to deal with huge amounts of data and to identify the hidden relationships between data. The proposed framework aims to provide personalized recommendations to learners, it utilizes a clustering-based community generation model for better identifying the interests or preferences of learners."
Constructive Alignment in Modern Computing Education: An Open-Source Computer-Based Examination System,"Linhuber, Matthias and Bernius, Jan Philip and Krusche, Stephan",10.1145/3631802.3631818,2024,"Large-scale paper-based examinations (PBEs) in computing education frequently emphasize rote memorization, thereby misaligning instructional objectives with assessment techniques. Such incongruities hinder the preparation of students for real world challenges in both industry and academia by inadequately evaluating higher-order cognitive abilities. Often, educators are deterred from implementing comprehensive skills assessment due to the perceived complexity and resource-intensive grading processes involved. To mitigate these limitations, this paper introduces an exam mode as an integral feature of the open-source learning platform Artemis. Designed for both local and cloud-based deployment, this exam mode incorporates anti-cheating protocols, automates the grading of diverse exercise types, and features double-blind manual grading to ensure assessment integrity. It fosters the evaluation of complex cognitive skills while substantially reducing the administrative load on faculty. This paper substantiates the effectiveness of the Artemis&nbsp; exam mode through widespread institutional adoption, demonstrated by over 50 successful computer-based examinations (CBEs). An in-depth case study involving 1,700 undergraduate software engineering students offers key insights, best practices, and lessons learned. This research not only pioneers the documentation of a secure, scalable, and reliable exam system at an institutional scale but also marks a seminal contribution to modernizing assessment strategies in computing education, with a particular focus on constructive alignment."
Exploring Factors Influencing the Satisfaction of Adult Software Engineering Students with Teamwork in Distance Education,"Rahimi, Ebrahim and Passier, Harrie and Stuurman, Sylvia",10.1145/3631802.3631823,2024,"Using team-based software development assignments is a prevalent instructional strategy in software engineering (SE) education. Students utilize these development assignments as a vehicle to (co-)learn SE concepts, practice problem-solving, and develop soft skills. The satisfaction of SE students with their teamwork experience in team-based assignments is an important educational and motivational factor that contributes to increased participation in future team-based projects. There is a scarcity of research on the satisfaction of SE students with teamwork in distance and online education specifically for adult learners. This study reports on a case study conducted to identify factors influencing the satisfaction of adult SE graduate students with their teamwork experiences in team-based software design and development assignments at the Open University of the Netherlands (OUNL), a distance education university for adult learners. The self-reflection reports of 29 adult SE students, aged between 25 and 35 years, documented and self-evaluated their experiences with a team-based design and development assignment within a master SE course were analyzed using an open thematic analysis approach. The analysis of the reports revealed six categories of factors that influenced the adult online SE students’ satisfaction with their team-working experience, namely, the attitude of team members, communication, collaboration, team characteristics, tooling and technology, and learning. Additionally, we conducted a literature review to identify any similarities and differences between the results obtained from the literature and those derived from our study on this topic."
"Beyond “Awareness”: If We Teach Inclusive Design, Will Students Act On It?","Garcia, Rosalinda and Morreale, Patricia and Patel, Pankati and Guevara, Jimena Noa and Moz-Ruiz, Dahana and Satish Kumar, Sabyatha and Velhal, Prisha and Busteed, Alec and Burnett, Margaret",10.1145/3632620.3671101,2024,"Motivation: Many university CS programs have begun teaching various types of CS-related societal issues using approaches such as ethics, Responsible CS, inclusive design, and more. However, some recent research suggests that, although these programs have been able to teach awareness, students often fail to act upon this awareness. To address this problem, University X's CS program tried an unusual approach—integrating hands-on inclusive design skills in small ways across all four years of the CS major. But did it work? That is, did the students who experienced this change across the major actually build more inclusive technology than the students who did not experience it?Objectives: This paper aims to answer this through addressing two research questions: (RQ1): Did students who learned inclusive design across the curriculum act to create more inclusive software? (RQ2): How did inclusivity (or lack thereof) manifest in students’ projects?Method: To investigate these RQs, we conducted a case study of 22 term-long CS projects built by 22 teams consisting of a total of 92 3rd- and 4th-year CS students. Half of the student teams had experienced courses that had integrated inclusive design and the other half had not. The inclusive design elements University X taught were those of the GenderMag inclusive design method, so evaluating the students’ term-long projects was done by GenderMag experts—industry-experienced UX and Software professionals with real-world GenderMag experience.Results: The inclusiveness of students’ projects was higher Post-GenderMag, with fewer reports of inclusivity bugs and higher inclusivity ratings. Experts’ evaluations also revealed the ways in which bias (e.g. bias against risk-averse users) and inclusion (e.g. inclusion of users with diverse information processing styles) appeared in students’ projects.Implications: We believe this to be the first published evidence that compares student-built technology's inclusiveness before vs. after they have been taught inclusive design. These positive results suggest that teaching inclusive design across the curriculum can impact students beyond simply heightening awareness—moving them to act upon this new understanding by building technology that more inclusively serves a wider spectrum of society."
"""It's Not Exactly Meant to Be Realistic"": Student Perspectives on the Role of Ethics In Computing Group Projects","Tran, Michelle and Fiesler, Casey",10.1145/3632620.3671109,2024,"In computing education, group projects are often seen as an opportunity for students to gain experiences similar to what they will face in the workplace. As such, it is a pressing area for ethics education: practice incorporating ethics into project-based courses might provide encouragement and scaffolding for students to consider ethics when working on “real” technologies. In this work, we provide a preliminary look at the state of ethics education in project-based computing courses via semi-structured focus groups. These focus groups revealed that while not completely ignored, ethics had a very small role in our participants’ group projects, especially in software engineering courses. Furthermore, participants generally agreed that while group projects can be useful learning opportunities, they are not realistic, and this impacts their willingness to consider ethical implications while working on projects. We compiled participant feedback and ideas on how to improve ethics education in computing group projects, and present a synthesis on how this feedback can be implemented in the classroom. We also emphasize that part of simulating ""real"" tech work should include acknowledgment of larger structural problems and ideally training and practice in communication and argumentation."
An Investigation of the Drivers of Novice Programmers' Intentions to Use Web Search and GenAI,"Skripchuk, James and Bacher, John and Price, Thomas",10.1145/3632620.3671112,2024,"External help resources are frequently used by novice programmers solving classwork in undergraduate computing courses. Traditionally, these tools consisted of web resources such as tutorial websites and Q&amp;A forums. With the rise of Generative AI (GenAI), there has been increasing concern and research about how external resources should be used in the classroom. However, little work has directly contrasted student beliefs and perceptions of web resources with GenAI, has grounded these beliefs in prior psychological theory, and has investigated how demographic factors and student backgrounds influence these beliefs and intentions. We administered a vignette-style survey across two courses required for a CS major at an R1 University, a freshman (n = 152) and senior capstone course (n = 44). Students responded to likert questions aiming to measure behavioral factors related to these tools, such as intention to use, perceived attitudes, peer perceptions, and their own perceived tool competency. We primarily investigate the results of an introductory course, finding that novices have a wide range of opinions on both resources, but overall find them slightly useful and have a tendency to prefer web-search. We compare this with seniors, who have more positive perceptions of these tools, and discuss possible reasons and implications for this difference. We constructed two path models to investigate which factors strongly influence novices’ intention to use resources and find the primary factor to be their general attitudes in how these tools will result in a positive or negative outcome (e.g. perceived benefits, justifiability). We also measure the effects of student background on intention to use these resources. Finally, we discuss implications and suggestions on how instructors can use this information to approach, address, and influence resource usage in their classrooms."
Debugging for Inclusivity in Online CS Courseware: Does it Work?,"Chatterjee, Amreeta and Choudhuri, Rudrajit and Sarkar, Mrinmoy and Chattopadhyay, Soumiki and Liu, Dylan and Hedaoo, Samarendra and Burnett, Margaret and Sarma, Anita",10.1145/3632620.3671117,2024,"Online computer science (CS) courses have broadened access to CS education, yet inclusivity barriers persist for minoritized groups in these courses. One problem that recent research has shown is that often inclusivity biases (“inclusivity bugs”) lurk within the course materials themselves, disproportionately disadvantaging minoritized students. To address this issue, we investigated how a faculty member can use AID—an Automated Inclusivity Detector tool—to remove such inclusivity bugs from a large online CS1 (Intro CS) course and what is the impact of the resulting inclusivity fixes on the students’ experiences. To enable this evaluation, we first needed to (Bugs):&nbsp;investigate inclusivity challenges students face in 5 online CS courses; (Build):&nbsp;build decision rules to capture these challenges in courseware (“inclusivity bugs”) and implement them in the AID tool; (Faculty):&nbsp;investigate how the faculty member followed up on the inclusivity bugs that AID reported; and (Students):&nbsp;investigate how the faculty member’s changes impacted students’ experiences via a before-vs-after qualitative study with CS students. Our results from (Bugs) revealed 39 inclusivity challenges spanning courseware components from the syllabus to assignments. After implementing the rules in the tool (Build), our results from (Faculty) revealed how the faculty member treated AID more as a “peer” than an authority in deciding whether and how to fix the bugs. Finally, the study results with (Students) revealed that students found the after-fix courseware more approachable - feeling less overwhelmed and more in control in contrast to the before-fix version where they constantly felt overwhelmed, often seeking external assistance to understand course content."
An fMRI study of the neural mechanisms of second and third tone recognition in deaf children,"Meng, Yuan and Li, Qiang",10.1145/3632971.3632975,2024,"Vocal intonation is an important component of language that plays a key role in language comprehension and communication. However, children with hearing loss face challenges in vocal tone recognition due to hearing impairment. In this study, five deaf children and two children with normal hearing were recruited to compare the differences in second and third tone recognition tasks between deaf and normal children. The results revealed that (1) Dysfunction in certain brain regions responsible for processing vocal tones in deaf children is associated with their hearing loss. This reveals the profound impact of hearing loss on the complex neural mechanisms of vocal tone perception. (2) Deaf children may use different neural networks when processing complex information about vocal tones. This adaptation suggests that the human brain has remarkable plasticity in its efforts to compensate for the absence of auditory input. Our study reveals the intricate interplay between hearing, neural processing, and language comprehension. It highlights not only the challenges that hearing-impaired children face in deciphering vocal tones, but also the extraordinary ability of their brains to reconfigure and adapt in an effort to compensate for gaps in auditory perception. Our research contributes to the understanding of the complex world of language processing."
‘Ought’ should not assume ‘Can’? Basic Capabilities in Cybersecurity to Ground Sen’s Capability Approach,"Das Chowdhury, Partha and Renaud, Karen",10.1145/3633500.3633506,2023,"We inhabit a ‘digital first’ society, which is only viable if everyone, regardless of ability and capacity, is able to benefit from online offerings in a safe and secure way. However, disabled individuals, people living under oppressive regimes, elderly citizens and individuals fleeing conflict can be excluded, because they might not have the opportunity to implement cybersecurity hygiene measures. To reduce this potential exclusion, it is crucial to make all users’ situated realities focal variables in policy debates and provisioning efforts. This requires a validated set of basic minimum capabilities which reflect individuals’ diverse personal and social realities. In this paper, we report on a scoping literature review intended to reveal the state of play with respect to capabilities-related research in the cyber domain. We motivate our initial focus on the over 65s for this investigation. We used advice from online government cybersecurity advisories to arrive at a set of five recommended cybersecurity hygiene tasks. These fed into a survey with sixty senior citizens to elicit the barriers they could envisage someone of their age encountering, in acting upon cybersecurity hygiene advice. The final deliverable is a candidate list of basic capabilities (cybersecurity) for seniors. This enables us to start measuring security and privacy poverty, an essential step in recognising and mitigating exclusion, as well as informing threat modelling efforts."
A portfolio selection and decision-making method considering interactions and time window constraints,"Qiu, Biaobiao and Zhu, Renqi and Dou, Yajie and Chen, Ziyi and Jiang, Jiang and Dai, Yulong",10.1145/3633586.3640309,2024,"The project portfolio selection problem (PPSP) is closely tied to achieving the strategic objectives of an enterprise. However, due to limited budgetary constraints, the number of available projects is restricted. Moreover, the project portfolio selection process becomes particularly challenging as it involves considering various constraints. One of these crucial constraints is the project execution cycle, which must be taken into account. To address these complexities, this paper proposes a novel approach. It incorporates the project execution cycle constraints into the project portfolio selection process. This integration enables simultaneous scheduling of projects during the selection process. What's more, a decision method is introduced, providing a reference solution within the Pareto solution set for decision- makers. To demonstrate the effectiveness of the proposed method, an example of multi-objective portfolio selection is presented. Through this example, the paper illustrates how the new approach yields promising results, paving the way for more efficient and effective project portfolio decision-making."
An Investigation into Misuse of Java Security APIs by Large Language Models,"Mousavi, Zahra and Islam, Chadni and Moore, Kristen and Abuadbba, Alsharif and Babar, M. Ali",10.1145/3634737.3661134,2024,"The increasing trend of using Large Language Models (LLMs) for code generation raises the question of their capability to generate trustworthy code. While many researchers are exploring the utility of code generation for uncovering software vulnerabilities, one crucial but often overlooked aspect is the security Application Programming Interfaces (APIs). APIs play an integral role in upholding software security, yet effectively integrating security APIs presents substantial challenges. This leads to inadvertent misuse by developers, thereby exposing software to vulnerabilities. To overcome these challenges, developers may seek assistance from LLMs. In this paper, we systematically assess ChatGPT's trustworthiness in code generation for security API use cases in Java. To conduct a thorough evaluation, we compile an extensive collection of 48 programming tasks for 5 widely used security APIs. We employ both automated and manual approaches to effectively detect security API misuse in the code generated by ChatGPT for these tasks. Our findings are concerning: around 70% of the code instances across 30 attempts per task contain security API misuse, with 20 distinct misuse types identified. Moreover, for roughly half of the tasks, this rate reaches 100%, indicating that there is a long way to go before developers can rely on ChatGPT to securely implement security API code."
SoK: Where to Fuzz? Assessing Target Selection Methods in Directed Fuzzing,"Weissberg, Felix and M\""{o}ller, Jonas and Ganz, Tom and Imgrund, Erik and Pirch, Lukas and Seidel, Lukas and Schloegel, Moritz and Eisenhofer, Thorsten and Rieck, Konrad",10.1145/3634737.3661141,2024,"A common paradigm for improving fuzzing performance is to focus on selected regions of a program rather than its entirety. While previous work has largely explored how these locations can be reached, their selection, that is, the where, has received little attention so far. In this paper, we fill this gap and present the first comprehensive analysis of target selection methods for fuzzing. To this end, we examine papers from leading security and software engineering conferences, identifying prevalent methods for choosing targets. By modeling these methods as general scoring functions, we are able to compare and measure their efficacy on a corpus of more than 1,600 crashes from the OSS-Fuzz project. Our analysis provides new insights for target selection in practice: First, we find that simple software metrics significantly outperform other methods, including common heuristics used in directed fuzzing, such as recently modified code or locations with sanitizer instrumentation. Next to this, we identify language models as a promising choice for target selection. In summary, our work offers a new perspective on directed fuzzing, emphasizing the role of target selection as an orthogonal dimension to improve performance."
Effectiveness of the Improvement Recommendations Model for Addressing the Syntactic Ambiguity in Malay Requirements Specifications,"Zahrin, Mohd Firdaus and Osman, Mohd Hafeez and Hassan, Sa'adah and Haron, Azlena",10.1145/3634814.3634826,2024,"Malaysian government agencies have developed various citizen-service platforms and applications. However, unresolved Malay requirements specification (RS) ambiguities might disrupt the software development project's completion. Most prior research has focused on English RS but not Malay. Therefore, devising a model to recommend improvements for addressing the ambiguous Malay requirements specifications is challenging. This paper investigates the state-of-the-art approaches for establishing and validating the effectiveness of an improvement recommendations model for addressing syntactical ambiguity in Malay RS. We devised an improvement recommendations model using natural language processing (NLP), rule-based, part-of-speech (POS), subject-verb-object (SVO) patterns, and Malay boilerplate. Thirteen experts from various Malaysian public sector agencies validated the model's effectiveness based on improved Malay RS enhanced by disambiguating syntactic ambiguity terms. We surveyed the experts’ opinions on the model's effectiveness and work experience through semi-structured interviews. This study revealed that 76.9% of the experts agreed and 23.1% strongly agreed that the improvement recommendations model effectively improved Malay RS by handling ambiguity. Experts with at least six years of experience in requirements engineering can comprehensively validate the improved Malay RS. The experts recognised the necessity for a model/ tool to aid requirements engineers in validating and enhancing the Malay RS by addressing ambiguity and recommending improved Malay RS structure based on Malay boilerplate syntax."
Applying Soft System Methodology for a clearer understanding of the future Intensive Care Units,"Markopoulos, Dimitrios and Tsolakidis, Anastasios and Karanikolas, Nikitas and Marinagi, Aikaterini and Skourlas, Christos",10.1145/3635059.3635084,2024,"Intensive Care Units (ICUs) generate a large amount of valuable data related to the health status of patients. In addition, ICUs can leverage other sources of big data, such as structured data, text, video, and images. In this work, a framework for the future ICU system is proposed, which is based on the Soft System methodology (SSM) and the use of big data technology. The framework and the related activity models ensure that the ICU can have its particularities and specialties, as well as its core services and functions. The application of the framework also implies that ICU can provide ongoing expertise and training to upgrade its staff, can improve interoperability with the National Health System, and ICU staff intercommunication and remote services."
Towards exploiting BPMN and DMN in public service modeling,"Tavantzis, Theocharis and Promikyridis, Rafail and Tambouris, Efthimios",10.1145/3635059.3635092,2024,"Modeling of Public Services (PSs) is used by a large number of public authorities worldwide to improve their quality. The resulting diagrams are useful in the case of simple PSs however they can become very complex in the case of complex PSs. To address process complexity, the integration of Business Process Model and Notation (BPMN) and Decision Model and Notation (DMN) has been proposed. DMN is applied to model the decision logic of a process and has proven to provide promising results in the private sector. However, the integration of BPMN and DMN has not been studied in the case of PSs. The aim of this paper is twofold. First, to investigate the integration of BPMN and DMN when applied to complex PSs. Second, to identify a set of steps and principles for integrating BPMN and DMN to model complex PSs. For this purpose, a literature review is conducted leading to the result that there is limited research on the use of BPMN to model complex PS while research on the use of DMN is missing. In addition, a PS is selected and modeled based on selected criteria and finally, a set of steps and principles is proposed."
How Metaverse is Affecting Smart Cities Economy,"Chatzopoulou, Ioanna and Tsoutsa, Paraskevi and Fitsilis, Panos",10.1145/3635059.3635099,2024,"In recent years, the convergence of physical and digital realms within the metaverse has become a focal point of interest, particularly in the post-COVID-19 era. This exploration takes on heightened significance given the myriad opportunities the metaverse offers to the modern world, particularly in shaping a smarter, more efficient economy. The impact of metaverse ecosystems extends across various domains of the Smart City, with a pronounced emphasis on the smart economy. The metaverse's role in transforming economic interactions, transactions, and innovation is crucial. However, the metaverse city also presents new challenges concerning trust, privacy, and ensuring inclusivity, highlighting the importance of ethical and equitable economic practices. This paper puts forward a comprehensive research agenda that aims to seamlessly integrate the metaverse into Smart Cities, with a strong emphasis on fostering a smart economy. This agenda seeks to engage researchers, practitioners, and policymakers to harness the full potential of metaverse technology in a manner that drives economic growth and innovation while ensuring equitable and inclusive prosperity for all."
“My ideas come little by little”: how graphic professionals manage ideas,"Rosselli Del Turco, Emilia and Dalsgaard, Peter",10.1145/3635636.3656203,2024,"To understand and support creative work, we need to look at its manifestation in everyday professional practice. We do so by focusing on the core activity of capturing and managing material representations of ideas, i.e. idea management. Starting from previous empirical research in other domains, we devise a theoretical framework and extend it in the domain of Graphics. Combining a qualitative survey (n=48) and in-depth interviews (n=15), we look for unexpected themes that emerge only among graphic professionals. We expand the existing knowledge in four main ways: we identify what makes a graphic idea good compared to other domains, we observe that practitioners balance conflicting creative identities with consequences on their idea management needs, that graphic ideas are sculpted out of inspirational material, and that creative control is negotiated with clients and collaborators."
Infusing Indigenous Perspectives into ICT Curriculum,"Herbert, Nicole",10.1145/3636243.3636246,2024,"Australia's Information and Communication Technology (ICT) curricula have been sluggish with integrating Indigenous perspectives. Despite universities committing to this in 2017, there remains a scarcity of tangible examples showcasing the successful blending of discipline-specific material with Indigenous perspectives within ICT subjects. Particularly elusive are instances that illustrate the interconnectedness between cultural awareness and ICT professional practice. This practitioner-oriented paper takes readers on a journey through the process employed to infuse Aboriginal perspectives into a first-year ICT subject. The focal point of this exploration is a case study, collaboratively crafted by an Aboriginal Education Officer and the ICT subject expert. Over a three-year review cycle, the implementation of this case study is dissected, revealing invaluable lessons learned along the way. Key takeaways include the imperative for educators to articulate compelling reasons for the integration, the pressing need to address knowledge gaps in Indigenous history and culture to dismantle stereotypes and biases, the pivotal significance of seamlessly blending Indigenous perspectives with discipline-specific content, and the transformative power of embracing collaborative Indigenous partnerships. This paper challenges the prevailing deficit-oriented perspective often encountered in ICT, offering practical guidance for academics committed to infusing Indigenous perspectives into ICT education."
Navigating the IT Skills Gap: Cultivating Job-Ready Graduates,"Herbert, Nicole and Herbert, David and Gray, Tony",10.1145/3636243.3636251,2024,"The perpetual misalignment between the skill sets of Information Technology (IT) graduates and evolving demands of industry remains an ongoing concern. Reports indicate that IT graduates often lack the practical job-readiness that industry requires, prompting an examination of potential solutions. Work-integrated learning and experiential learning have emerged as promising solutions. In 2023, an innovative approach to job-readiness was pioneered by incorporating these approaches into an on-campus internship tailored for non-cognate international postgraduate IT students. This internship cultivates industry insight, technical proficiency, lifelong learning, and personal branding, all impacting on a graduate's employability. This practitioner paper unveils the design, encountered challenges, and lessons learned with the aim to facilitate the adoption of similar initiatives by other institutions, particularly those in regions with limited access to industry internships for postgraduate IT students."
The Sequence Matters in Learning - A Systematic Literature Review,"Valle Torre, Manuel and Oertel, Catharine and Specht, Marcus",10.1145/3636555.3636880,2024,"Describing and analysing learner behaviour using sequential data and analysis is becoming more and more popular in Learning Analytics. Nevertheless, we found a variety of definitions of learning sequences, as well as choices regarding data aggregation and the methods implemented for analysis. Furthermore, sequences are used to study different educational settings and serve as a base for various interventions. In this literature review, the authors aim to generate an overview of these aspects to describe the current state of using sequence analysis in educational support and learning analytics. The 74 included articles were selected based on the criteria that they conduct empirical research on an educational environment using sequences of learning actions as the main focus of their analysis. The results enable us to highlight different learning tasks where sequences are analysed, identify data mapping strategies for different types of sequence actions, differentiate techniques based on purpose and scope, and identify educational interventions based on the outcomes of sequence analysis."
"Have Learning Analytics Dashboards Lived Up to the Hype? A Systematic Review of Impact on Students' Achievement, Motivation, Participation and Attitude","Kaliisa, Rogers and Misiejuk, Kamila and L\'{o}pez-Pernas, Sonsoles and Khalil, Mohammad and Saqr, Mohammed",10.1145/3636555.3636884,2024,"While learning analytics dashboards (LADs) are the most common form of LA intervention, there is limited evidence regarding their impact on students’ learning outcomes. This systematic review synthesizes the findings of 38 research studies to investigate the impact of LADs on students' learning outcomes, encompassing achievement, participation, motivation, and attitudes. As we currently stand, there is no evidence to support the conclusion that LADs have lived up to the promise of improving academic achievement. Most studies reported negligible or small effects, with limited evidence from well-powered controlled experiments. Many studies merely compared users and non-users of LADs, confounding the dashboard effect with student engagement levels. Similarly, the impact of LADs on motivation and attitudes appeared modest, with only a few exceptions demonstrating significant effects. Small sample sizes in these studies highlight the need for larger-scale investigations to validate these findings. Notably, LADs showed a relatively substantial impact on student participation. Several studies reported medium to large effect sizes, suggesting that LADs can promote engagement and interaction in online learning environments. However, methodological shortcomings, such as reliance on traditional evaluation methods, self-selection bias, the assumption that access equates to usage, and a lack of standardized assessment tools, emerged as recurring issues. To advance the research line for LADs, researchers should use rigorous assessment methods and establish clear standards for evaluating learning constructs. Such efforts will advance our understanding of the potential of LADs to enhance learning outcomes and provide valuable insights for educators and researchers alike."
Quantifying Collaborative Complex Problem Solving in Classrooms using Learning Analytics,"Taylor, Megan and Barthakur, Abhinava and Azad, Arslan and Joksimovic, Srecko and Zhang, Xuwei and Siemens, George",10.1145/3636555.3636913,2024,"Complex problem solving (CPS) is a critical skill with far-reaching implications for personal success and professional development. While CPS research has made extensive progress, additional investigation is needed to explore CPS processes beyond online contexts and performance outcomes. This study, conducted with Year 9 students aged between thirteen and fourteen, focuses on collaborative CPS. It utilises audio and video recordings to capture group communications during a CPS classroom activity. To analyse these interactions, we introduce a novel CPS framework as a dynamic, cognitive and social process involving interrelated main skills, sub-skills, and indicators. Through sequential pattern mining, we identify recurring subskill patterns that reflect CPS processes in an educational environment. Our research underscores the importance of employing diverse patterns before plan execution, particularly building shared knowledge, planning, and negotiation. We uncover patterns related to groups going off-task and highlight the significance of effective communication and maintaining focus for keeping groups on track. Furthermore, we indicate patterns following the detection of emergent issues, recognising the value of cultivating clarity and adaptability among team members. Our CPS framework, combined with our research results, offers practical implications for teaching, learning, and assessment approaches in educational, professional and industry sectors."
CTAM4SRL: A Consolidated Temporal Analytic Method for Analysis of Self-Regulated Learning,"Nath, Debarshi and Gasevic, Dragan and Fan, Yizhou and Rajendran, Ramkumar",10.1145/3636555.3636926,2024,"Temporality in Self-Regulated Learning (SRL) has two perspectives: one as a passage of time and the other as an ordered sequence of events. Each of these conceptions is distinct and requires independent considerations. Only a single analytic method is not sufficient in adequately capturing both these facets of temporality. Yet, most research uses a single method in temporally-focused SRL research, and those that use multiple methods do not address both aspects of temporality. We propose CTAM4SRL, a consolidated temporal analytic method which combines advanced data visualisation, network analysis and pattern mining to capture both facets of temporality. We employ CTAM4SRL in a cohort of 36 learners engaged in a reading-writing activity. Using CTAM4SRL, we were able to provide a rich temporal explanation of the interplay of the self-regulatory processes of the learners. We were further able to identify differences in SRL behaviours in high and low performers in terms of their approach to learning comprising deep and surface strategies. High performers were able to more selectively and strategically combine deep and surface learning strategies when compared to low scorers– a behaviour which was only hypothesised in SRL literature previously, but now has empirical support provided by our consolidated analytic method."
Approximate Matrix Multiplication over Sliding Windows,"Yao, Ziqi and Li, Lianzhi and Chen, Mingsong and Wei, Xian and Chen, Cheng",10.1145/3637528.3671819,2024,"Large-scale streaming matrix multiplication is very common in various applications, sparking significant interest in develop efficient algorithms for approximate matrix multiplication (AMM) over streams. In addition, many practical scenarios require to process time-sensitive data and aim to compute matrix multiplication for most recent columns of the data matrices rather than the entire matrices, which motivated us to study efficient AMM algorithms over sliding windows. In this paper, we present two novel deterministic algorithms for this problem and provide corresponding error guarantees. We further reduce the space and time costs of our methods for sparse matrices by performing an approximate singular value decomposition which can utilize the sparsity of matrices. Extensive experimental results on both synthetic and real-world datasets validate our theoretical analysis and highlight the efficiency of our methods."
Research on the application mode of knowledge graph in education,"Hou, Yaning and Liu, Bo and Fan, Qianxi and Zhou, Jianqiang",10.1145/3637907.3637976,2024,"With the emergence of AI technology, the application of artificial intelligence in education has been a popular topic in educational technology. Knowledge maps, the latest form of knowledge engineering in the field of AI, have significant research value in education. This paper introduces fundamental research methods in knowledge maps and analyzes classic cases in the education field, using discipline knowledge graphs. Based on existing analysis and further study, three research models for knowledge maps in the education field have been developed. These models include intellectualized teaching resources, personalized teaching, and optimized educational decision-making. They are designed for research, teaching, and management levels respectively. Finally, this paper proposes specific suggestions and expectations for the reasonable implementation of knowledge map technology in the education field."
Gesture Recognition Methods Using Sensors Integrated into Smartwatches: Results of a Systematic Literature Review,"Gomes, Pedro Raphael In\'{a}cio and Castro, Murillo Santos de and Nascimento, Thamer Horbylon",10.1145/3638067.3638082,2024,"This work addresses the importance of gesture recognition in smartwatches and aims to conduct a systematic literature review to identify the most commonly used methods and sensors in this field. The selected works were obtained from renowned databases, such as ACM Digital Library, IEEE Xplore, Scopus, and ScienceDirect. Initially, 265 articles were identified, and after applying inclusion and quality criteria, 43 articles were selected for detailed analysis. The systematic review allowed for the consolidation of existing knowledge and the identification of the most relevant approaches in this area. The results revealed a diversity of algorithms employed, such as Na\""{\i}ve Bayes, Artificial Neural Network, Support Vector Machine, Dynamic Time Warping, Symbolic Aggregate approXimation, Long Short-Term Memory, Random Forest, Multiple Dimensional Dynamic Time Warping, Gated Recurrent Unit, K-nearest Neighbors, Hidden Markov Model, Latent Dirichlet Allocation, and Convolutional Neural Network. Additionally, we found that accelerometers were the most commonly used sensors, followed by gyroscopes, magnetometers, barometers, vital sensors (EMG, FMG, PPG), and infrared sensors. The findings highlight the relevance of gesture recognition in smartwatches and provide valuable insights for researchers and professionals interested in this constantly evolving field."
"Implementation of an Alarm System to Protect a Submersible Pump of a Company in Cusco, 2023","Romani Quiliche, Carlos Arcenio and Atapaucar Merma, Jos\'{e} Luis and Espinoza Rios, Elena Sonia Paula",10.1145/3638264.3638274,2024,"The implementation of an alarm system was carried out to protect a submersible pump whose purpose is to maintain the river water supply of a company in Cusco. The guidelines of the PMBOK, which is the main book of the PMI, were used. Within the development of the project, there are 5 phases: initiation, planning, execution, monitoring-control, and finally, closure. In the initial phase, interested parties will be determined, and the project charter will be drafted. In the planning phase, requirements gathering, design, analysis, risk management, and project schedule will be carried out. The execution phase will be carried out within the agreement with the interested parties, where the implementation of the project itself will be developed under the supervision of the project director. Progress in the monitoring and control phase will be monitored and controlled according to the schedule. Finally, in the Closing phase, what was requested by the interested parties can be compared with the final results. According to the results obtained in the initiation and planning phase, it is evident that there will be better remote control and monitoring of the submersible pump alarm system, minimizing unexpected stops. Therefore, it is concluded that by using Logo Soft Comfort tools such as Logo Web Editor and Logo Access Tool, monitoring and control screens can be developed and alarm reports generated to make assertive decisions, either for preventive or corrective maintenance of equipment, thus minimizing unwanted events that could affect the water supply from the river to the plant."
A Novel Two-Stage Data-mining Model Combining Gait Recognition and Temporal Sequence Mining,"Yang, Pu-Tai and Liao, Tsu-Tang and Chen, Chih-Jui Ray",10.1145/3639390.3639393,2024,"In recent years, artificial intelligence applications have been on the rise. Many enterprises have embraced digital transformation and have established new business models based on artificial intelligence and the Internet of Things, such as the telerehabilitation industry. The companies may utilize sensors or cameras to collect user data, and data mining is applied to discover insights for doctors’ aids. This paper establishes a novel two-stage data mining model combining gait recognition and sequential pattern mining. In the first stage, a particular computer vision application, gait recognition, identifies possible diseases using the subject's walking postures. The gaits in a video can be converted to a temporal sequence according to user-defined events. For example, (normal gait, Parkinsonian gait, normal gait) is a temporal sequence in which the identified gaits are arranged by temporal orders in the sequence. In the second stage, after collecting a dataset of temporal sequences, the frequent patterns are discovered by sequential pattern mining. Our preliminary experiment collected 30 samples from the real world and demonstrated the model's feasibility."
Adopting an Agile Approach for Reflective Learning and Teaching,"Leist, Eleanor and Lee, Jaejoon",10.1145/3639474.3640055,2024,"Software engineering is concerned with how best to create software in ways that promote sustainable development and maximise quality. We have been largely successful at transferring software engineering knowledge into the industry, however, many challenges in software engineering training remain. A key amongst these is how best to teach practical engineering approaches along with the theoretical concepts behind them.This paper describes our experience of adopting an agile approach for reflective learning and teaching within the context of our Software Systems Engineering module, aimed at addressing challenges identified with previous efforts to promote reflective practice. Our study attempts to strengthen the use of reflective learning approaches for our current cohort, as well as introducing reflective teaching practices, whereby we examine our teaching approach in order to improve its efficiency and effectiveness. Our analysis of student response to the module shows that it was very well-received by the students, and we were able to collect ample evidence from feedback to support this. Most of our approaches resulted in positive feedback and contributed to improvements in teaching quality, however, we also identified some key aspects in our method that could still benefit from refinement, such as the need for explicit links between learning outcomes and workshop activities, and intuitive design of feedback questions, along with feedback collection frequency. We plan to incorporate these additional updates into the revision of the module for the next academic year, and to continue collecting and analysing feedback data for further enhancement."
Bridging the Theory-Practice Gap in a Maintenance Programming Course: An Experience Report,"Ouhbi, Sofia",10.1145/3639474.3640062,2024,"This paper presents our experience in teaching a maintenance programming course with the aim of bridging the gap between theory and practice, a recurring issue in previous course offerings. To achieve this goal, we implemented active learning strategies within an active learning classroom setting and redesigned the project work. Our approach involves peer learning and teamwork activities to cover various aspects of legacy code maintenance. For the project work, we adopted an open-ended approach that allowed students to choose their legacy code projects, which could be open-source software or a previous software project they had worked on. Analysis of students' feedback and project reports highlighted the effectiveness of our approach in bridging the gap between theory and practice. We believe that our approach had the potential to enhance students' engagement and critical thinking abilities, as well as improve practical maintenance skills relevant to their future careers."
Introducing Computer Science Undergraduate Students to DevOps Technologies from Software Engineering Fundamentals,"Sarmiento-Calisaya, Edgar and Mamani-Aliaga, Alvaro and Leite, Julio Cesar Sampaio Do Prado",10.1145/3639474.3640071,2024,"The fast adoption of collaborative software development by the industry allied with the demand for a short time to market has led to a dramatic change in IT roles. New practices, tools, and environments are available to support professionals in their day-to-day activities. In this context, the demand for software engineers with these skills continues to increase, specifically those related to Extreme Programming, Agile frameworks, CI/CD, and DevOps. To match Computer Science undergraduate students' skills with existing job offers, some universities have begun to include DevOps topics in their curriculums. However, due to the wide range of courses covered in Computer Science majors, it is particularly challenging to introduce DevOps within the context of Software Engineering fundamentals, i.e., connect abstract concepts to skills needed for software engineers in the industry. This paper investigates ways of introducing Computer Science students to industry-relevant practices and technologies early from two Software Engineering fundamentals courses. Student outcomes were extremely positive, providing insights into ways to introduce students to DevOps-related practices and technologies and bridge the gap between academia and industry."
Video-based Training for Meeting Communication Skills,"Galster, Matthias and Mitrovic, Antonija and Malinen, Sanna and Iyer, Sreedevi Sankara and Musa, Ja'afaru and Holland, Jay",10.1145/3639474.3640080,2024,"Background: Discussing and sharing information in development teams is part of any software project. Therefore, software engineers spend significant time in meetings with their team. Communicating effectively and efficiently in those meetings is essential. However, software engineers often do not possess the right skills. On the other hand, training face-to-face meeting communication skills in university settings is resource- and time-consuming. Aims: Our goal is to develop and evaluate a method to support the training of face-to-face meeting communication skills. Method: We develop a method based on active video-watching. Active video-watching supports deep learning by systematically engaging students with video-based learning material. We also implement this method in an online platform for classroom use. Furthermore, we empirically develop a new measurement instrument to assess face-to-face meeting communication skills. To evaluate the training method, we used it in three instances of a second-year software engineering project course. To assess learning gain, we assessed (a) the conceptual knowledge about face-to-face meeting communication, and (b) skills based on our newly developed measurement instrument, both before and after the training. Results: Both conceptual knowledge as well as skill measurement scores based on our instrument increased. Increases are statistically significant. Conclusions: We show the effectiveness of active video-watching for training face-to-face meeting communication skills, one specific soft skill relevant for software engineers. The measurement instrument that we developed can also be used as a stand-alone tool to assess skills of students and potentially practitioners."
With Great Humor Comes Great Developer Engagement,"Tiwari, Deepika and Toady, Tim and Monperrus, Martin and Baudry, Benoit",10.1145/3639475.3640099,2024,"The worldwide collaborative effort for the creation of software is technically and socially demanding. The more engaged developers are, the more value they impart to the software they create. Engaged developers, such as Margaret Hamilton programming Apollo 11, can succeed in tackling the most difficult engineering tasks. In this paper, we dive deep into an original vector of engagement - humor - and study how it fuels developer engagement. First, we collect qualitative and quantitative data about the humorous elements present within three significant, real-world software projects: faker, which helps developers introduce humor within their tests; lolcommits, which captures a photograph after each contribution made by a developer; and volkswagen, an exercise in satire, which accidentally led to the invention of an impactful software tool. Second, through a developer survey, we receive unique insights from 125 developers, who share their real-life experiences with humor in software.Our analysis of the three case studies highlights the prevalence of humor in software, and unveils the worldwide community of developers who are enthusiastic about both software and humor. We also learn about the caveats of humor in software through the valuable insights shared by our survey respondents. We report clear evidence that, when practiced responsibly, humor increases developer engagement and supports them in addressing hard engineering and cognitive tasks. The most actionable highlight of our work is that software tests and documentation are the best locations in code to practice humor."
Impostor Phenomenon in Software Engineers,"Guenes, Paloma and Tomaz, Rafael and Kalinowski, Marcos and Baldassarre, Maria Teresa and Storey, Margaret-Anne",10.1145/3639475.3640114,2024,"The Impostor Phenomenon (IP) is widely discussed in Science, Technology, Engineering, and Mathematics (STEM) and has been recently evaluated in Computer and Data Science students. There has been no formal research conducted on IP in software engineers in general, even though its consequences may contribute to mental health disorders, such as depression and burnout. This study describes a survey that investigates the extent of impostor feelings in software engineers, considering aspects such as gender, race/ethnicity, and roles. Furthermore, we investigate the influence of IP on their perceived productivity. The survey instrument was designed using a theory-driven approach and included demographic questions, an internationally validated IP scale (CIPS), and questions for measuring perceived productivity based on the SPACE framework constructs. The survey was sent to companies operating in various business sectors. Data analysis used bootstrapping with resampling to calculate confidence intervals and Mann-Whitney statistical significance testing for assessing the hypotheses. We received responses from 624 software engineers distributed across 26 countries. The bootstrapping results reveal that a proportion of 52.7% of software engineers experience frequent to intense levels of IP and that women suffer at a significantly higher proportion (60.6%) than men (48.8%). Regarding race/ethnicity, we observed more frequent impostor feelings in Asian (67.9%) and Black (65.1%) than in White (50.0%) software engineers. We also observed that the presence of IP is less common among individuals who are married and have children. Moreover, the prevalence of IP showed a statistically significant negative effect on the perceived productivity for all SPACE framework constructs. The evidence relating IP to software engineers provides a starting point to help organizations find ways to raise awareness of the problem and improve the emotional skills of software professionals."
An Ethnographic Study on the CI of A Large Scale Project,"Wang, Zikuan and Liu, Bohan and Zhan, Zeye and Zhang, He and Li, Gongyuan",10.1145/3639477.3639750,2024,"Continuous Integration (CI) is the foundation for achieving rapid iteration and short-cycle delivery. To achieve CI, a series of best practices and solutions have been proposed, which are referred to as patterns. However, there is a natural contradiction between the speed and continuity pursued by CI and the ever-expanding project scale and complexity. Various factors such as project size, outdated system architecture, complex organizational structure, or limited server resources can all lead to deviations from patterns in CI practices, resulting in anti-patterns. We conducted an ethnographic research to investigate the current state, anti-patterns, and challenges in resolving anti-patterns of the CI process within a large communication project at a globally leading IT company. We conducted a deep observation and participation in the project for seven months and conducted multiple rounds of interviews with related developers in the company. The project adopts a CI pipeline that has a three-level hierarchical structure. We evaluated the company's software development practices based on the pattern list. We identified three anti-patterns that contradicted the patterns listed, and we also discovered three new anti-patterns that were not on the list. Further, we analyzed the challenges of solving these anti-patterns. Additionally, we found seven better practices and analyzed why they are better."
Towards Interpreting the Behavior of Large Language Models on Software Engineering Tasks,"Dipongkor, Atish Kumar",10.1145/3639478.3639798,2024,"Large Language Models (LLMs) have ushered in a significant breakthrough within the field of Natural Language Processing. Building upon this achievement, analogous language models have been developed specifically for code-related tasks, commonly referred to as Large Language Models for Code (LLMsC). Notable examples of LLMsC include CodeBERT, UnixCoder, CoPilot, among others. These models have demonstrated exceptional performance across various Software Engineering (SE) tasks, encompassing code summarization, test case generation, natural language to code conversion, bug triaging, malware detection, program repair, and more.Despite the promising results achieved by LLMsC in SE tasks, there remains fundamental questions regarding their decision-making processes. Understanding these model decision mechanisms is crucial for further enhancing the performance of LLMsC. In pursuit of this objective, my PhD dissertation aims to pioneer novel methodologies for interpreting and comprehending the behavior of LLMsC."
Artificial Intelligence Aspect Of Transportation Analysis Using Large Scale Systems,"Hu, Tiechuan and Zhu, Wenbo and Yan, Yuqi",10.1145/3639592.3639600,2024,"Problem: The problem of the finalized exploration revolved around the inadequacy of current traffic forecasting models. Despite decades of examination in many fields, existing approaches, often relying on linear models and stationary time series assumptions, struggle to accurately predict traffic under chaotic events. The identified limitation has profound consequences, evident in the significant economic losses and time inefficiencies incurred due to traffic congestion, as exemplified by the $144 billion in losses and the 34% increase in travel time for drivers in Los Angeles County in 2013. The challenge lies in the inherently unpredictable nature of traffic events, ranging from regular rush hours causing sharp declines in traffic speed to unpredictable accidents leading to unforeseen delays. Consequently, there is a pressing need for a more effective and adaptive traffic forecasting model that can reliably operate under both normal and abnormal traffic conditions, addressing the shortcomings of traditional linear models and stationary time series assumptions. Purpose: The purpose of the completed investigation was to determine whether a traffic forecasting model that incorporates machine learning and deep learning technologies can yield effective traffic forecasts based on real-time weather and traffic data. Method: The study involved the development of a traffic forecasting model as informed by existing literature. Data collection was done through simulating data similar to the traffic data from Los Angeles County that was utilized in Yu et al.’s research on deep learning and traffic prediction in extreme weather scenarios [1]. Data analysis was done through MAE and t-test. Results: The findings demonstrated that the created traffic forecasting model outperformed the current methods in its ability to provide traffic forecasts with a better degree of accuracy. Conclusion: Regardless of the traffic volume, weather, or time of day, the developed traffic forecasting algorithm can give precise real-time traffic predictions."
I2PN: Improved Image Projection Network for OCTA image segmentation,"Wang, Tianlei and Qu, Hong and Chen, Keyu and Luo, Ma and Zhai, Chao and Fang, Bopeng",10.1145/3639631.3639659,2024,"Optical coherence tomography (OCT) is one of the most significant advances in medical images, and OCT segmentation is an important task in medical-assisted diagnostics. However, existing 3D processing methods require large computational resources. To alleviate this problem, we present an improved image projection network (I2PN), which is an end-to-end OCT image segmentation model. Our key insight is to build a simple projection module (SPM) that uses several group convolutions to conduct effective feature extraction and dimension reduction concurrently. By stacking multiple SPMs, the proposed network can process 3D OCTA data into a high-level semantic feature map. Afterward, the Down-Up modules (DUM) that contain down-sampling and up-sampling are applied to align the SPM features to the OCT image segmentation target. Finally, with a category-dependent segmentation head (CDS), which can balance region selection among hierarchical feature maps, the I2PN model can handle the OCTA image segmentation tasks simultaneously, such as retinal vessel and FAZ segmentation. I2PN provides a succinct idea for the quantification of retinal indicators. Extensive experiments on the OCTA-500 dataset validate the effectiveness of the proposed method I2PN."
ModelMate: A recommender for textual modeling languages based on pre-trained language models,"Costa, Carlos Dur\'{a} and L\'{o}pez, Jos\'{e} Antonio Hern\'{a}ndez and Cuadrado, Jes\'{u}s S\'{a}nchez",10.1145/3640310.3674089,2024,"Current DSL environments lack smart editing facilities intended to enhance modeler productivity and cannot keep pace of current developments of integrated development environments based on AI. In this paper, we propose an approach to address this shortcoming through a recommender system specifically tailored for textual DSLs based on the fine-tuning of pre-trained language models. We identify three main tasks: identifier suggestion, line completion, and block completion, which we implement over the same fine-tuned model and we propose a workflow to apply these tasks to any textual DSL. We have evaluated our approach with different pre-trained models for three DSLs: Emfatic, Xtext and a DSL to specify domain entities, showing that the system performs well and provides accurate suggestions. We compare it against existing approaches in the feature name recommendation task showing that our system outperforms the alternatives. Moreover, we evaluate the inference time of our approach obtaining low latencies, which makes the system adequate for live assistance. Finally, we contribute a concrete recommender, named ModelMate, which implements the training, evaluation and inference steps of the workflow as well as providing integration into Eclipse-based textual editors."
A DSL for Testing LLMs for Fairness and Bias,"Morales, Sergio and Claris\'{o}, Robert and Cabot, Jordi",10.1145/3640310.3674093,2024,"Large language models (LLMs) are increasingly integrated into software systems to enhance them with generative AI capabilities. But LLMs may reflect a biased behavior, resulting in systems that could discriminate against gender, age or ethnicity, among other ethical concerns. Society and upcoming regulations will force companies and development teams to ensure their AI-enhanced software is ethically fair. To facilitate such ethical assessment, we propose LangBiTe, a model-driven solution to specify ethical requirements, and customize and automate the testing of ethical biases in LLMs. The evaluation can raise awareness on the biases of the LLM-based components of the system and/or trigger a change in the LLM of choice based on the requirements of that particular application. The model-driven approach makes both the requirements specification and the test generation platform-independent, and provides end-to-end traceability between the requirements and their assessment. We have implemented an open-source tool set, available on GitHub, to support the application of our approach."
Design Thinking Using Qualitative Data Analysis and Machine Learning,"Hanan, Moussa and Galal, Galal-Edeen H.",10.1145/3640429.3640437,2024,"Design Thinking is a human-centered approach that allows continuous feedback by the user through Empathizing, Defining, Testing, Ideating and Prototyping. It mainly focuses on user needs, aspirations, wishes, concerns and frustrations in attempting to solve their problems. The Persona Creation approach follows the process of collecting data from multiple sources including social media platforms or the traditional methods including interviews of different users to cover the different types of behaviors, interactions and goals, questionnaires, or surveys. Condensing gathered data using qualitative data analysis renders assessable domain models that can be shared among and modified by stakeholders, so as to agree on user needs and issues. After agreeing on useful needs and user issues, they are used to generate Personas that represent the different types of users of a specific software product. When both approaches Design Thinking and Persona Creation are incorporated during Agile software development, this would lead to the creation of a successful software product. Successful software products are ones that cover all the needs as mentioned by the product user, also known as user perspectives.A user perspective refers to the perception of a given user and how they would use the final product. Those are&nbsp;the people who would interact with the software product created and, therefore, the people for whom&nbsp;the software is designed. For this reason, if an application, a website or a functionality that does not meet the final user's needs, this would ultimately result in a failure for the business. Inducing pain points/insights (what is needed by users) should not left totally to the skills of the analyst with little guidance. A systematic and more guidance is needed in this situation. Agile Software Development lack a coherent and explicit technique or open architecture [1] that can accommodate changes mandated by experiments on the ground. In addition, there has to be a method for objectively evaluating resultant prototypes/releases/deliverables at the end of each sprint in a way that can effectively guide path adjustments.Therefore, in this research, we make use of Design Thinking with software products. Through creating a framework that includes Design Thinking as an elicitation technique. We propose a framework composed of two phases: The first phase is the use of a robust qualitative data analysis method, to achieve models that are rich, and at the same time concise and traceable to their origins. We propose the use of the Grounded Theory method in the analysis and integration of the qualitative data that can characterize user needs, pain points and system requirements, in addition to second layer requirements that are often hard to spot. Second layer requirements are those requirements that are not immediately visible or perceivable by the end-user of a system, or those working with or observing him or her, such as systems and requirements analysts. The source of data for generating grounded theoretical formulations include interviews (of whatever type), observations, online chatter, and documents relating to the immediate and wider contexts of the need phenomenon under study.The second part of our proposed framework is applying Machine Learning on the data resulting from the first phase so that we are able to automate the Persona creation using Machine learning. Automatic Persona creation via machine learning is used to represent potential users, as an attempt to enhance the requirements of software products since they will necessarily include user perspectives."
Processing Approaches for Functional Magnetic Resonance Imaging Data in a Limited Sample of Deaf Individuals,"Li, Qiang and Meng, Yuan and Shi, Rui Meng",10.1145/3640771.3640777,2024,"The challenge of small sample sizes in fMRI research, particularly in studies involving specific groups like deaf children, necessitates specialized data analysis techniques. Our study introduces two such methods: the Bootstrap and Permutation methods, tailored for small-sample fMRI data analysis. The Bootstrap method involves resampling with replacement, creating numerous pseudo-samples to estimate statistical properties. Its advantages include simplicity and minimal assumptions about data distribution, but it faces potential biases and inaccuracies in variance estimation in very small samples. The Permutation method, meanwhile, rearranges observed data to generate outcome distributions under the null hypothesis. Ideal for hypothesis testing in small samples. However, it is computationally intensive and may reduce test power. Our study provides a detailed examination of these methods, evaluating their processes, strengths, and limitations. This approach aims to equip researchers with effective tools for analyzing small-sample fMRI data, particularly in niche fields. It also seeks to bridge the gap between computer science and neuroscience, encouraging further research in this interdisciplinary area."
Computational Models for In-Vehicle User Interface Design: A Systematic Literature Review,"Lorenz, Martin and Amorim, Tiago and Dey, Debargha and Sadeghi, Mersedeh and Ebel, Patrick",10.1145/3640792.3675735,2024,"In this review, we analyze the current state of the art of computational models for in-vehicle User Interface (UI) design. Driver distraction, often caused by drivers performing Non Driving Related Tasks (NDRTs), is a major contributor to vehicle crashes. Accordingly, in-vehicle User Interfaces (UIs) must be evaluated for their distraction potential. Computational models are a promising solution to automate this evaluation, but are not yet widely used, limiting their real-world impact. We systematically review the existing literature on computational models for NDRTs to analyze why current approaches have not yet found their way into practice. We found that while many models are intended for UI evaluation, they focus on small and isolated phenomena that are disconnected from the needs of automotive UI designers. In addition, very few approaches make predictions detailed enough to inform current design processes. Our analysis of the state of the art, the identified research gaps, and the formulated research potentials can guide researchers and practitioners toward computational models that improve the automotive User Interface (UI) design process."
Improving Conversational User Interfaces for Citizen Complaint Management through enhanced Contextual Feedback,"Karren, Kai and Schmitz, Michael and Schaffer, Stefan",10.1145/3640794.3665562,2024,"As cities transform, disrupting citizens’ lives, their participation in urban development is often undervalued despite its importance. Citizen complaint systems exist but are often limited in fostering meaningful dialogue with municipalities. Meanwhile, smart cities aim to improve living standards, efficiency, and sustainability by integrating digital twins with physical infrastructures, potentially enhancing transparency and enriching communication between cities and their inhabitants with real-time data. Complementing these developments, technologies realizing Conversational User Interfaces (CUIs) are becoming more capable in providing a conversational and feedback-oriented approach such as complaint management processes. The improvement of CUIs for citizen complaint management through enhanced contextual feedback is explored in this work. The term contextual feedback has been developed and defined as all information (for example, background, conditions, explanations, timelines, and the existence of similar complaints) related to a complaint and or the underlying problem that could potentially be relevant for the user. The solution proposed in this paper gathers data from users about their issues via a CUI, which subsequently queries various data sources to obtain relevant contextual information. Following this, a Large Language Model processes the collected data to produce the corresponding feedback. In the study, a static CUI without contextual data as the baseline has been compared to a CUI that includes contextual data, analyzing their impact on pragmatic and hedonic quality, reuse intention, and potential influence on the citizens’ trust in their municipality. The study has been conducted in cooperation with the German municipality of Wadgassen. The good performance of the baseline system shows the general potential of LLMs in the citizen complaint domain even without data sources. The results show that contextual feedback performed better overall, with significant improvements in the pragmatic and hedonic quality, attractiveness, reuse intention, feeling that the complaint is taken seriously, and the citizens’ trust in their municipality."
Fusion of Graph and Natural Language Processing in Predictive Analytics for Adverse Drug Reactions,"Zhou, Fangyu and Uddin, Shahadat",10.1145/3641142.3641155,2024,"Adverse Drug Reactions (ADRs) pose a critical challenge to patient safety and healthcare economics worldwide. This study presents a novel graph-assisted machine learning algorithm applied to a comprehensive dataset provided by the Commonwealth Bank Health Society (CBHS), spanning 1976 to 2018, to predict ADRs. Utilizing the narrative-like structure of patients’ medical histories through Natural Language Processing (NLP), the research uniquely encodes International Classification of Diseases (ICD) codes into word embeddings. In a departure from traditional methods, it also employs networks analytics to transform disease histories into a knowledge graph structure that captures temporal and contextual relationships between ICD codes. This study's results demonstrated that integrating Node2Vec with Word2Vec improved model performance across various metrics, with significant enhancements in recall for K-nearest neighbors (KNN) and area under the receiver operating characteristic curve (AUROC) for all models. The findings underscore the potential of NLP in medical contexts and highlight the advantages of graph-based approaches in capturing complex, non-linear interdependencies inherent in patient data. This research marks a significant stride toward harnessing the power of NLP and graph theory in the predictive modeling of ADRs, aiming to improve patient outcomes by preempting potential adverse drug interactions."
"Compliance, Commitment, and Web Accessibility: Findings of an Organizational Study Engaging Accessibility Experts at Twenty U.S. Universities","Oswal, Sushil K. and Palmer, Zsuzsanna B.",10.1145/3641237.3691647,2024,"This research paper presents the results of an IRB-approved, qualitative study of web accessibility commitment among 20 U.S. universities through semi-structured interviews with their accessibility experts and administrators. The findings of the study suggest that while having to comply with regulations is often a starting point for universities for making their websites accessible, in some cases a culture of accessibility develops because of the commitment of accessibility practitioners to born accessible principles."
"Mind, Method, and Mastery: Assessing Design Thinking (DT) Proficiency Through Critical Incident Technique (CIT) and Quantitative Content Analysis","Al-hassan, Ma-aruf",10.1145/3641237.3691657,2024,"""Assessing Design Thinking (DT) Proficiency Through Critical Incident Technique"" employs a Criterion-referenced framework, C-K theory, and Functional-Behavior-Structure Ontology to assess proficiency in visual design, technical design, and cognitive task skills. This study investigated the construct validity by examining how evidence and theory support these findings. The findings revealed varying proficiency levels among participants, indicating the need to improve TPC pedagogy by targeting the development of DT skills. Practical applications include incorporating iterative design processes, fostering interdisciplinary collaboration, and aligning educational goals with industrial demands. Notably, the study underscores the importance of visual thinking in DT, which aligns with the existing evidence and theory in design cognition research. Future research should explore longitudinal studies to track skill development and investigate the integration of FBS ontology into TPC curriculum. Finally, exploring the cross-disciplinary applications of DT in diverse fields can enhance the understanding and practice of DT."
Towards Evidence-Based Software Quality Practices for Reproducibility: Practices and Aligned Software Qualities,"Gilbertson, Christian and Mundt, Miranda and Teves, Joshua and Toribio, Simone and Milewicz, Reed",10.1145/3641525.3663624,2024,"Among computational science and engineering (CSE) software development projects, reproducibility of results is widely understood to be essential for the software to be useful and meaningful. At present, there is a lack of empirical support for how best to design for, implement, or maintain reproducibility over the course of the CSE software lifecycle, and the relationship between reproducibility and other qualities of interest is not well-understood. In this study, we consider the role of software engineering practices and product qualities in enabling reproducibility. To build a foundation for future studies, we conducted case studies of the software engineering practices of 9 teams at Sandia National Laboratories (Sandia), and a first-in-kind online survey of 219&nbsp; developers and users of CSE software to assess their beliefs about quality and reproducibility. Among our results, we identified three practices that were universally attributed to enhancing reproducibility of research and software at Sandia: automated testing, documentation, and version control. We found that the majority of survey respondents believed that high-quality software was more likely to be reproducible than low-quality software, and that maintainability, reliability, and usability were generally seen as positively contributing to reproducibility. These preliminary findings merit follow-up studies to better understand how software quality is valued and enacted in CSE software development practice."
Exploring the Adaptability and Usefulness of Git-Truck for Assessing Software Capstone Project Development,"Neyem, Andres and Carrasco-Aravena, Jose and Fernandez-Blanco, Alison and Sandoval Alcocer, Juan Pablo",10.1145/3641554.3701798,2025,"In software engineering pedagogy, a persistent challenge is the comprehensive assessment of student contributions within software repositories. This study delves into the investigation of the Git-Truck tool, initially designed for professional software engineers, and explores its adaptability and effectiveness within an academic setting. We specifically focus on the tool's potential for educators when assessing Capstone software repositories. Our results emphasize that educators found bubble chart visualization and metrics such as ''Top Contributor'' and ''Number of Commits'' helpful in understanding group dynamics and contribution. We also discuss the tool's limitations among visual techniques and metrics used. As the educational landscape shifts towards increased virtual and remote modalities, tools like Git-Truck are poised to augment the intricacy and depth of software project evaluations. For those considering adopting or adapting such tools in similar contexts, our study offers the challenges and lessons learned from this experience."
Students' Use of GitHub Copilot for Working with Large Code Bases,"Shah, Anshul and Chernova, Anya and Tomson, Elena and Porter, Leo and Griswold, William G. and Soosai Raj, Adalbert Gerald",10.1145/3641554.3701800,2025,"Large language models (LLMs) are already heavily used by professional software engineers. An important skill for new university graduates to possess will be the ability to use such LLMs to effectively navigate and modify a large code base. While much of the prior work related to LLMs in computing education focuses on novice programmers learning to code, less work has focused on how upper-division students use and trust these tools, especially while working with large code bases. In this study, we taught students about various GitHub Copilot features, including Copilot chat, in an upper-division software engineering course and asked students to add a feature to a large code base using Copilot. Our analysis revealed a novel interaction pattern that we call one-shot prompting, in which students ask Copilot to implement the entire feature at once and spend the next few prompts asking Copilot to debug the code or asking Copilot to regenerate its incorrect response. Finally, students reported significantly more trust in the code comprehension features than code generation features of Copilot, perhaps due to the presence of trust affordances in the Copilot chat that are absent in the code generation features. Our study takes the first steps in understanding how upper-division students use Github Copilot so that our instruction can adequately prepare students for a career in software engineering."
A Course-based Undergraduate Research Experience (CURE) Focused Broadly on Research Methods in Computer Science,"Fernandez, Amanda S.",10.1145/3641554.3701805,2025,"This experience report introduces a novel, adaptable course-based undergraduate research (CURE) model designed to nurture research interest in undergraduate CS majors. Traditional CURE models often target specific disciplines. Our model allows students to explore research questions within their chosen CS subfields. A pilot study assessed the model's effectiveness. Undergraduate CS majors participated, completing pre- and post-course surveys to gauge research motivation. The course culminated in a final research project requiring students to delve into a specific CS area, propose a novel approach, and outline a research plan. Findings from this pilot will inform the model's refinement for wider implementation. Further, the developed materials are shared openly at https://github.com/amandanko/UTSA-CS-CURE."
Bridging the Community College Cybersecurity Classroom and Workplace with the CyberSim Lab,"Oden Choi, Judeth and Guttman, Rotem and Kisow, Matthew and Ros\'{e}, Carolyn and Nichols, William and Winyard, James and Li, Bruce and Branstetter, Lee and Herckis, Lauren",10.1145/3641554.3701834,2025,"Most postsecondary cybersecurity education focuses on technical knowledge and skills without commensurate attention to vital non-technical skills. In this position paper, we argue that cybersecurity education must integrate teaching and practicing of non-technical competencies alongside technical knowledge and skills to ensure that both technical and non-technical skills transfer to cybersecurity workplaces. We identify specific learning outcomes that meet these criteria and suggest research-based pedagogical approaches to support learning and transfer. We present a cybersecurity lab designed to address these learning outcomes through experiential learning, roleplay, collaborative learning, technical simulation and metacognitive engagement. The CyberSim Lab serves as a curricular bridge between the classroom and the workplace."
Sprint to Inclusion: Embedding Accessibility Sprint in a Software Engineering Course,"Aljedaani, Wajdi and Parthasarathy, P D and Eler, Marcelo Medeiros and Joshi, Swaroop",10.1145/3641554.3701838,2025,"This experience report contributes to the expanding body of literature advocating for the inclusion of accessibility topics within core computing courses rather than restricting them to specialized electives such as human-computer interaction (HCI). We integrated an accessibility-focused sprint into a 3-credit software engineering course at a large private university in the US. Throughout the course, students progressively developed a software project in groups with a dedicated sprint designed to emphasize the importance of accessibility. This sprint included lectures on accessibility principles and hands-on accessibility testing on their own projects, followed by a sprint to fix the issues found. A survey was conducted at the end of the course to gauge their learning outcomes and perceptions of accessibility. The results demonstrate that integrating accessibility into a software engineering course not only enhances students' technical skills but also boosts their commitment to creating inclusive software. We present the accessibility interventions made, assessment instruments used and our findings in this report. We also offer key insights as takeaways for the computing education community, providing guidance for educators aiming to embed accessibility into their curriculum."
Fostering Creativity: Student-Generative AI Teaming in an Open-Ended CS0 Assignment,"Filcik, Daniel and Sobiesk, Edward and Matthews, Suzanne J.",10.1145/3641554.3701853,2025,"The increasing ubiquity of web-based generative artificial intelligence technologies necessitates that all students experience teaming with such technologies -- exploring their strengths and limitations and learning how to create synergy with them. To aid in this effort, we designed an open-ended generative AI project for the freshmen taking our general-education introduction to computing course. Students were required to team with generative AI to create something beyond what they alone (or the AI alone) could accomplish. Upon completion, students submitted a short written critical analysis documenting their experiences and presented a three-minute demonstration of their project in class. Despite limited course coverage of AI and generative AI prior to this project, we were impressed by the creativity and sophistication of the submitted final products as well as the breadth of generative AI tools explored. Student reflections on the experience illustrated numerous insights into the strengths and limitations of the tools they employed. Our results underscore that students can learn about the benefits and limitations of generative AI in as little as a single assignment and that covering such topics need not require extensive amounts of course time and resources."
Integrating Socially Responsible Computing through Direct Community Engagement in CS2 to Promote Latinx Student Retention,"Sun, Yu and Dong, Qichao and Tang, Fang",10.1145/3641554.3701895,2025,"This experience report is part of an ongoing NSF-funded grant project involving an alliance of six California State University campuses, aimed at promoting Latinx student retention through community engagement in early computer science courses. The project focuses on integrating socially responsible computing (SRC) into the curriculum to transform computing culture and invite marginalized students to participate. At our campus, we integrated SRC concepts into the CS2 course on Data Structures and Algorithms. Initially, SRC concepts were introduced into assignments and projects, which showed promising results but highlighted challenges: the assignments and projects were instructor-created, leading to a gap between students and the concepts. Students passively received topics without proactive participation, resulting in a lack of perceived real-world impact. To address this, we involved the local Latinx community directly. Students visited community partners to identify real-world problems, which they then addressed through term projects, ultimately presenting their solutions to the community. Adopting a startup mindset, students interviewed partners, identified problems, developed prototypes, and delivered solutions. This hands-on approach, first implemented in Spring 2024, significantly enhanced student engagement and provided practical, impactful learning experiences. This report details the course design, implementation process, formative data collected, and reflections on the outcomes. The findings offer valuable insights and recommendations for educators aiming to foster community engagement and socially responsible computing in computer science education, with a specific focus on promoting Latinx student retention."
"The Roles, Responsibilities, and Skills of Engineers in the Era of Microservices-Based Architectures","Michael Ayas, Hamdy and Hebig, Regina and Leitner, Philipp",10.1145/3641822.3641871,2024,"Enterprises often try to tame the complexity of their software using microservices and practitioners generally perceive the impact of microservices as positive. However, different responsibilities fall in the hands of practitioners and new skill-sets are required to address the challenges and reap the benefits of microservices. The objective of this study is to gather and organize what industry requires from microservices practitioners. To achieve this, we conduct a qualitative analysis of 125 job-ads related to microservices that are gathered from 7 different countries, across 5 continents, posted during 14 consecutive days, sampled from a total of 1351 job-ads. We contribute with detailed taxonomies on roles, responsibilities, soft- and hard-skills that are necessary for microservices practitioners. Specifically, we detail 5 families of responsibilities, 3 of which are human-centered, describe 8 themes of popular soft-skills and describe 11 themes of popular hard-skills, along with how they relate to soft-skills. Our results indicate the importance of human-centered responsibilities and skills in microservices practitioners, and point to the existence of a job market for microservices software architects with a high demand on human aspects. Hence, our findings can help unravel organizational structures in microservices, improve training programmes, and understand the manifestation of human aspects in microservices."
A Virtual Reality Model for STEM Education in Thailand,"Soodtoetong, Nantinee and Rattanasiriwongwut, Montean",10.1145/3643487.3662170,2024,"The change in globalization era is cause of efficient and simple access of technologies and innovation. To improve human knowledge, skills, and capacities, everyone can readily access and employ innovation and technology. It develops a brand-new educational model that transcends the traditional classroom. Nonetheless, it can occur at anytime and anywhere. Through the combination of digital information systems, cloud and virtual reality technologies, all learners can access education and learn by themselves through these technologies. In the 21st century, the idea of STEM education has been used to learning management to help students develop their analytical and methodical differentiation skills. STEM education can be integrated with contemporary technology to create a virtual environment where students can practice skills and procedures in accordance with the model. Additionally, virtual reality technology makes learning more resemble life in the actual world. A challenge in STEM education in Thailand is that teachers are not well-versed which makes for poor outcomes. For this reason, this study investigates STEM education and virtual reality based on teaching and proposed STEM education model via virtual reality technology that is appropriate for Thai people."
Predicting the Lifetime of Flaky Tests on Chrome,"Malmir, Samaneh and Rigby, Peter Christopher",10.1145/3643656.3643899,2024,"Flaky tests not only disrupt the testing process but can also introduce significant delays in software release cycles, leading to increased costs and potential missed market opportunities. The ability to predict the lifetime class to which the test belongs to can empower development teams to allocate resources efficiently, prioritize bug fixes, and streamline the testing pipeline. Despite the critical role that this estimation plays in software development, there is a noticeable gap in the existing research literature. Bridging this gap is essential for enhancing testing efficiency and optimizing resource allocation and project planning in software development.In our investigation of the historical patterns of flaky tests in Chrome, we identified that 40% of flaky tests remain unresolved, while 38% are typically addressed within the initial 15 days of introduction. Subsequently, we developed a predictive model focused on identifying tests with quicker resolutions. Our model demonstrated a precision of 73% and a Matthews Correlation Coefficient (MCC) approaching 0.39 in forecasting the lifespan class of flaky tests."
A Framework for Microservice Organizational Structure Optimization,"Li, Xiaozhou and Albano, Michele",10.1145/3643657.3643913,2024,"With the soaring popularity of microservices, practitioners have realized that the key to success lies more in the management of people than in the architecture itself. Due to the systems' inevitable evolution, the organization shifts towards such changes where a lack of proper optimization can result in dreadful efficiency, error proneness, and toxic collaboration environments. However, despite the criticality, limited studies have proposed concrete support for optimizing microservice organizational structure through system evolution. This short paper sketches the vision of a framework to facilitate the optimization of microservice organizational structure. With this work, we aim to attract the attention of the microservice community toward the benefits and issues of the microservice organizational structure and propose a promising direction to a continuous solution."
"Software Engineering Education: Towards Ethical, Reliable, and Beautiful Software","Samarah, Mohammad and Inuganti, Aikya and Goyal, Madhuri",10.1145/3643660.3643950,2024,"In this paper, we present our experience with an innovative pedagogical approach to software engineering in a graduate-level advanced software engineering course. Our approach to software engineering and software design education relies on six dimensions: 1) restating the goal of software engineering education to say that software must be conceived of, architected, designed, developed, deployed, maintained, and managed to be ethical, reliable, and beautiful; 2) software should be engineered as a service; 3) apply proven architectural principles; 4) use sound design principles; 5) create rapid multi-modal prototyping; and 6) bring the course learning objectives together by creating a term-long project that creates a solution to a real-world problem using an iterative process. The results from students' feedback have been very positive with students citing the benefits of the course particularly a) the realignment of software engineering education goals centered on creating ethical, reliable, and beautiful software, b) the focus on clean, sound, and efficient architectures, and c) blending of IEEE SWEBOK, modern microservice architectures, and emerging approaches from software engineering research and open source. We plan to continue developing the course and enhance it in the areas of software reuse, software product design, AI and software design, design for diverse users, and design for sustainability."
Insights Towards Better Case Study Reporting in Software Engineering,"Rico, Sergio",10.1145/3643664.3648208,2024,"Case studies are a popular and noteworthy type of research study in software engineering, offering significant potential to impact industry practices by investigating phenomena in their natural contexts. This potential to reach a broad audience beyond the academic community is often undermined by deficiencies in reporting, particularly in the context description, study classification, generalizability, and the handling of validity threats. This paper presents a reflective analysis aiming to share insights that can enhance the quality and impact of case study reporting.We emphasize the need to follow established guidelines, accurate classification, and detailed context descriptions in case studies. Additionally, particular focus is placed on articulating generalizable findings and thoroughly discussing generalizability threats. We aim to encourage researchers to adopt more rigorous and communicative strategies, ensuring that case studies are methodologically sound, resonate with, and apply to software engineering practitioners and the broader academic community. The reflections and recommendations offered in this paper aim to ensure that insights from case studies are transparent, understandable, and tailored to meet the needs of both academic researchers and industry practitioners. In doing so, we seek to enhance the real-world applicability of academic research, bridging the gap between theoretical research and practical implementation in industry."
Human-AI Collaboration in Software Engineering: Lessons Learned from a Hands-On Workshop,"Hamza, Muhammad and Siemon, Dominik and Akbar, Muhammad Azeem and Rahman, Tahsinur",10.1145/3643690.3648236,2024,"This paper investigates the dynamics of human-AI collaboration in software engineering, focusing on the use of ChatGPT. Through a thematic analysis of a hands-on workshop in which 22 professional software engineers collaborated for three hours with ChatGPT, we explore the transition of AI from a mere tool to a collaborative partner. The study identifies key themes such as the evolving nature of human-AI interaction, the capabilities of AI in software engineering tasks, and the challenges and limitations of integrating AI in this domain. The findings show that while AI, particularly ChatGPT, improves the efficiency of code generation and optimization, human oversight remains crucial, especially in areas requiring complex problem-solving and security considerations. This research contributes to the theoretical understanding of human-AI collaboration in software engineering and provides practical insights for effectively integrating AI tools into development processes. It highlights the need for clear role allocation, effective communication, and balanced AI-human collaboration to realize the full potential of AI in software engineering."
"Towards stability, predictability, and quality of intelligent automation services: ECIT product journey from on-premise to as-a-service","Kedziora, Damian and Siemon, Dominik and Elshan, Edona and So\'{n}ta, Monika",10.1145/3643690.3648595,2024,"The intensive transformations of software products have been widely discussed in academia and business from various perspectives, yet with limited reference to low-code technologies, such as robotic process automation (RPA). The intensive growth in the size and importance of RPA and low-code industry puts their offering transitions from traditional on-premise to modern software-as-a-service models at the core of its paradigm evolution. Our single case study presents the transformation from 'On-Prem' to 'RaaS-P product for intelligent automation (IA) services, conducted by the leading Nordic consultancy ECIT Group, elaborating on its triggers, journey, stakeholders, as well as implications on cost, quality, and customer satisfaction. Triggered by internal experiences, cost, and market pressures, the case allowed us to discover that while RPA technology has rapidly grown to become a commodity at a wide amount of business organizations, the RPA service providers need to aim at resolving the issues and addressing challenges of their customers, not technology itself. It brings forward the 'Robot as a Service - Process Automation' product, with a novel approach to SLA, focused on availability, job stability, recovery, and transaction quality."
Code Ownership in Open-Source AI Software Security,"Wen, Jiawen and Yuan, Dong and Ma, Lei and Chen, Huaming",10.1145/3643691.3648586,2024,"As open-source AI software projects become an integral component in the AI software development, it is critical to develop a novel measurement method to ensure the security of the open-source AI projects for developers. Code ownership, pivotal in the evolution of such projects, offers insights into developer engagement and potential vulnerabilities. In this paper, we leverage the code ownership metrics to empirically investigate the correlation with the latent vulnerabilities across five prominent open-source AI software projects. The findings from the large-scale empirical study suggest a positive relationship between high-level ownership (characterised by a limited number of minor contributors) and a decrease in vulnerabilities. Furthermore, we innovatively introduce the time metrics, anchored on the project's duration, individual source code file timelines, and the count of impacted releases. These metrics adeptly categorise distinct phases of open-source AI software projects and their respective vulnerability intensities. With these novel code ownership metrics, we have implemented a Python-based command-line application to aid project curators and quality assurance professionals in evaluating and benchmarking their on-site projects. We anticipate this work will embark a continuous research development for securing and measuring open-source AI project security."
An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project,"Rasnayaka, Sanka and Wang, Guanlin and Shariffdeen, Ridwan and Iyer, Ganesh Neelakanta",10.1145/3643795.3648379,2024,"Large Language Models (LLMs) represent a leap in artificial intelligence, excelling in tasks using human language(s). Although the main focus of general-purpose LLMs is not code generation, they have shown promising results in the domain. However, the usefulness of LLMs in an academic software engineering project has not been fully explored yet. In this study, we explore the usefulness of LLMs for 214 students working in teams consisting of up to six members. Notably, in the academic course through which this study is conducted, students were encouraged to integrate LLMs into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of LLMs.In this paper, we analyze the AI-generated code, prompts used for code generation, and the human intervention levels to integrate the code into the code base. We also conduct a perception study to gain insights into the perceived usefulness, influencing factors, and future outlook of LLM from a computer science student's perspective. Our findings suggest that LLMs can play a crucial role in the early stages of software development, especially in generating foundational code structures, and helping with syntax and error debugging. These insights provide us with a framework on how to effectively utilize LLMs as a tool to enhance the productivity of software engineering students, and highlight the necessity of shifting the educational focus toward preparing students for successful human-AI collaboration."
Challenges of Processing Data Clumps within Plugin Architectures of Integrated Development Environment,"Baumgartner, Nils and Pulverm\""{u}ller, Elke",10.1145/3643796.3648444,2024,"In this study, we explore advanced strategies for enhancing software quality by detecting and refactoring data clumps, special types of code smells. Our approach transcends the capabilities of integrated development environments, utilizing a novel method that separates the detection of data clumps from the source access. This method facilitates data clump processing. We introduce a command-line interface plugin to support this novel method of processing data clumps. This research highlights the efficacy of modularized algorithms and advocates their integration into continuous workflows, promising enhanced code quality and efficient project management across various programming and integrated development environments."
Co-designing a knowledge management tool for educator communities of practice,"Fernandez-Nieto, Gloria Milena and Swiecki, Zachari and Tsai, Yi-Shan and Sha, Lele and Wei, Yinwei and Wen, Jim and Li, Yuheng and Jin, Yueqiao and Feraud, Ivan Silva and Li, Yuan-Fang and Wang, Weiqing and Chen, Guanliang and Gasevic, Dragan",10.1145/3643834.3660682,2024,"Knowledge management involves finding, expanding, and using knowledge in an organisation to achieve goals. Its role is crucial in higher education to improve problem-solving, research, and teaching by acquiring, sharing, and applying knowledge. Higher education institutions can promote knowledge management through Communities of Practice, but doing so remains challenging due to cultural, organisational, and technological reasons. We present findings of the first step of co-design workshops with authentic higher education teaching teams that sought to understand (a) their practices as a community and any motivators and impediments to their community development; (b) how they perceived the tools they use for knowledge management; and (c) the kinds of tools they believed could help them better conduct knowledge management and develop as Communities of Practice. Our findings suggested four essential design requirements and informed our development of a new tool to support the knowledge management needs of higher education teaching teams."
"""My Sense of Morality Leads to My Suffering, Battling, and Arguing"": The Role of Platform Designers in (Un)Deciding Gig Worker Issues","Ma, Shuhao and Zimmerman, John and Fox, Sarah E and Nisi, Valentina and Nunes, Nuno Jardim",10.1145/3643834.3660713,2024,"HCI and design studies have increasingly identified challenges for gig workers and advocated for designs centered around worker justice. However, there’s an existing research gap in understanding how platform designers approach gig worker issues in their practice. Our study engaged ten platform designers from food delivery and ride-hailing platforms to investigate this gap. Through semi-structured interviews, we uncovered their strategies, the extent of authority and responsibilities, and the range of obstacles they encounter in influencing decision-making that could affect gig workers’ experiences with the platforms. While platform designers were aware of gig worker issues, they confronted challenges from business goals, decision-making power, policies, and job security in promoting worker well-being. We discuss the jurisdiction of platform designers and propose that HCI research should further support them, who are deeply engaged in the gig economy and have the potential to participate in addressing social justice issues."
"Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation","Vaithilingam, Priyan and Arawjo, Ian and Glassman, Elena L.",10.1145/3643834.3661525,2024,"We ideate a future design workflow that involves AI technology. Drawing from activity and communication theory, we attempt to isolate the new value that large AI models can provide design compared to past technologies. We arrive at three affordances—dynamic grounding, constructive negotiation, and sustainable motivation—that summarize latent qualities of natural language-enabled foundation models that, if explicitly designed for, can support the process of design. Through design fiction, we then imagine a future interface as a diegetic prototype, the story of Squirrel Game, that demonstrates each of our three affordances in a realistic usage scenario. Our design process, terminology, and diagrams aim to contribute to future discussions about the relative affordances of AI technology with regard to collaborating with human designers."
Virtual Threads: A Systematic Literature Review of 10 Years of VR/AR/MR Adoption in Fashion Design,"Balsara, Delna and G\'{o}mez-Zar\'{a}, Diego",10.1145/3643834.3661575,2024,"In the past decade, HCI researchers have developed virtual reality (VR), augmented reality (AR), and mixed reality (MR) solutions to support fashion designers. Unlike traditional technologies, VR/AR/MR technologies provide immersion and physical interactions with digital garments. To comprehend the transformative impact of these technologies in fashion design, we conducted a systematic review covering 2013 to 2023, assessing the impact of VR/AR/MR adoption in fashion design. After screening 126 articles, we identified 26 articles that introduce immersive systems centered on two main themes: advancing the fashion design toolkit and enhancing fashion design processes. We discuss potential opportunities and limitations to be studied in future research and reflect on how VR/AR/MR technologies can continue to support fashion designers in fulfilling their needs and overcoming obstacles."
"Tangible Affect: A Literature Review of Tangible Interactive Systems Addressing Human Core Affect, Emotions and Moods","Zhou, Nianmei and Devleminck, Steven and Geurts, Luc",10.1145/3643834.3661608,2024,"Tangible user interfaces (TUIs) have been applied to assess, communicate, or regulate human core affect, emotions, or moods. Previous studies identified TUIs as an innovative way to serve people’s affective needs. This review examines the design and evaluation of tangible interactive systems that focus on human core affect, emotions, and moods. We provide an overview of current studies. We summarize how tangibility can be leveraged to support affective interaction, and we propose the dimensions of tangible affective interaction, deriving guidelines for design. We highlight three main challenges: understanding tangible affective interaction within real-life scenarios, utilizing embodied interaction to express or influence affective states, and establishing benchmarks for evaluating tangible affective interfaces."
"Opportunities, tensions, and challenges in computational approaches to addressing online harassment","Huang, Evey Jiaxin and Sarma, Abhraneel and Hwang, Sohyeon and Chandrasekharan, Eshwar and Chancellor, Stevie",10.1145/3643834.3661623,2024,"Given the scale at which online harassment occurs, researchers and practitioners alike have turned to computationally driven approaches to address it. However, because harassment is highly contextual and personal, designing effective solutions to this problem can be extremely challenging. This paper examines how harassment-mitigation systems studied in human-computer interaction (HCI) consider victim-centered principles in their design. Through a scoping literature review and close reading of 17 papers, we contribute—(1) a characterization of how novel and existing systems consider victims’ identity characteristics, definitions of harassment, and preferred strategies for dealing with harassment; (2) challenges faced by the systems along these dimensions to surface limitations, gaps, and tensions; (3) practical recommendations for researchers, designers, and practitioners to overcome these challenges. In doing so, we offer potential new directions to positively design computational approaches to addressing online harassment with victim-centered principles in mind."
CRSP: Emulating Human Cooperative Reasoning for Intelligible Story Point Estimation,"Han, Rui and Han, Wanjiang and Han, Zhuoyan and Tian, Yifan and Chen, Longzheng and Han, Ren",10.1145/3643916.3644417,2024,"Software effort estimation plays a critical role in software project development. Inaccurate cost estimation can impact progress and result in budget overruns. The story point estimation technique is a commonly used practice in agile software development for the estimation of software development effort. It allows for the evaluation of relative task workloads by analyzing task titles and descriptions. In previous studies, researchers have mainly focused on providing story point estimation results by task titles. However, in practical scenarios, users are often unable to provide task titles as precise as those found in the training dataset, leading to inaccurate estimation results. To address this problem, we propose a Cooperative Reasoning Story Point estimation method (CRSP). We approach the estimation problem as a question-and-answer challenge, addressing it through a framework of model construction, Monte Carlo Tree search, and model inference. In the model construction phase, we train a generator responsible for generating problem-solving reasoning paths and employ verifier to score the quality of these reasoning paths. During the Monte Carlo Tree search stage, we execute MCTS using generator and verifier to generate candidate solutions. In the final model inference phase, we employ a solver to derive the ultimate answer. To evaluate the effectiveness of CRSP, we modify and adapt the well-known JIRA dataset to make it more compatible with the input format of the question-answering model. The new JIRA dataset contains 21,082 issues from 16 open-source software projects. Across 16 open-source projects, the mean absolute error of CRSP is lower than other baseline methods. In contrast to the traditional regression and classification methods, we pioneer the use of question-and-answer method to address the issue, opening up new directions for future research."
Understanding the Impact of Branch Edit Features for the Automatic Prediction of Merge Conflict Resolutions,"Aldndni, Waad and Servant, Francisco and Meng, Na",10.1145/3643916.3644433,2024,"Developers regularly have to resolve merge conflicts, i.e., two conflicting sets of changes to the same files in different branches, which can be tedious and error-prone. To resolve conflicts, developers typically: keep the local version (KL) or the remote version (KR) of the code. They also sometimes manually edit both versions into a single one (ME). However, most existing techniques only support merging the local and remote versions (the ME strategy).We recently proposed RPredictor, a machine learning-based approach to support developers in choosing how to resolve a conflict (by KL, KR, or ME), by predicting their resolution strategy. In its original design, RPredictor uses a set of Evolution History Features (EHFs) that capture: the magnitude of the changes in conflict, their evolution, and the experience of the developers involved.In this paper, we proposed and evaluated a new set of Branch Edit Features (BEFs), that capture the fine-grained edits that were performed on each branch of the conflict. We learned multiple lessons. First, BEFs provided lower effectiveness (F-score) than the original EHFs. Second, combining BEFs with EHFs still did not improve the effectiveness of EHFs, it provided the same f-score. Third, the feature set that provided highest effectiveness in our experiments was the combination of EHFs with a subset of BEFs that captures the number of insertions performed in the local branch, but this combination only improved EHFs by 3 pp. f-score. Finally, our experiments also share the lesson that some feature sets provided higher C-score (i.e., the safety of the technique's mistakes) as a trade-off for lower f-scores. This may be valued by developers and we believe that it should be studied in the future."
How do Machine Learning Projects use Continuous Integration Practices? An Empirical Study on GitHub Actions,"Bernardo, Jo\~{a}o Helis and Da Costa, Daniel Alencar and Medeiros, S\'{e}rgio Queiroz de and Kulesza, Uir\'{a}",10.1145/3643991.3644915,2024,"Continuous Integration (CI) is a well-established practice in traditional software development, but its nuances in the domain of Machine Learning (ML) projects remain relatively unexplored. Given the distinctive nature of ML development, understanding how CI practices are adopted in this context is crucial for tailoring effective approaches. In this study, we conduct a comprehensive analysis of 185 open-source projects on GitHub (93 ML and 92 non-ML projects). Our investigation comprises both quantitative and qualitative dimensions, aiming to uncover differences in CI adoption between ML and non-ML projects. Our findings indicate that ML projects often require longer build duration, and medium-sized ML projects exhibit lower test coverage compared to non-ML projects. Moreover, small and medium-sized ML projects show a higher prevalence of increasing build duration trends compared to their non-ML counterparts. Additionally, our qualitative analysis illuminates the discussions around CI in both ML and non-ML projects, encompassing themes like CI Build Execution and Status, CI Testing, and CI Infrastructure. These insights shed light on the unique challenges faced by ML projects in adopting CI practices effectively."
Unveiling ChatGPT's Usage in Open Source Projects: A Mining-based Study,"Tufano, Rosalia and Mastropaolo, Antonio and Pepe, Federica and Dabic, Ozren and Di Penta, Massimiliano and Bavota, Gabriele",10.1145/3643991.3644918,2024,"Large Language Models (LLMs) have gained significant attention in the software engineering community. Nowadays developers have the possibility to exploit these models through industrial-grade tools providing a handy interface toward LLMs, such as OpenAI's ChatGPT. While the potential of LLMs in assisting developers across several tasks has been documented in the literature, there is a lack of empirical evidence mapping the actual usage of LLMs in software projects. In this work, we aim at filling such a gap. First, we mine 1,501 commits, pull requests (PRs), and issues from open-source projects by matching regular expressions likely to indicate the usage of ChatGPT to accomplish the task. Then, we manually analyze these instances, discarding false positives (i.e., instances in which ChatGPT was mentioned but not actually used) and categorizing the task automated in the 467 true positive instances (165 commits, 159 PRs, 143 issues). This resulted in a taxonomy of 45 tasks which developers automate via ChatGPT. The taxonomy, accompanied with representative examples, provides (i) developers with valuable insights on how to exploit LLMs in their workflow and (ii) researchers with a clear overview of tasks that, according to developers, could benefit from automated solutions."
Supporting High-Level to Low-Level Requirements Coverage Reviewing with Large Language Models,"Preda, Anamaria-Roberta and Mayr-Dorn, Christoph and Mashkoor, Atif and Egyed, Alexander",10.1145/3643991.3644922,2024,"Refining high-level requirements into low-level ones is a common task, especially in safety-critical systems engineering. The objective is to describe every important aspect of the high-level requirement in a low-level requirement, ensuring a complete and correct implementation of the system's features. To this end, standards and regulations for safety-critical systems require reviewing the coverage of high-level requirements by all its low-level requirements to ensure no missing aspects.The challenge of supporting automatic reviews for requirements coverage originates from the distinct levels of abstraction between high-level and low-level requirements, their reliance on natural language, and the often different vocabulary used. The rise of Large Language Models (LLMs), trained on extensive text corpora and capable of contextualizing both high-level and low-level requirements, opens new avenues for addressing this challenge.This paper presents an initial study to explore the performance of LLMs in assessing requirements coverage. We employed GPT-3.5 and GPT-4 to analyze requirements from five publicly accessible data sets, determining their ability to detect if low-level requirements sufficiently address the corresponding high-level requirement. Our findings reveal that GPT-3.5, utilizing a zero-shot prompting strategy augmented with the prompt of explaining, correctly identifies complete coverage in four out of five evaluation data sets. Additionally, it exhibits an impressive 99.7% recall rate in accurately identifying instances where coverage is incomplete due to removing a single low-level requirement across our entire set of evaluation data."
How I Learned to Stop Worrying and Love ChatGPT,"Przymus, Piotr and Fejzer, Miko\l{}aj and Narundefinedbski, Jakub and Stencel, Krzysztof",10.1145/3643991.3645073,2024,"In the dynamic landscape of software engineering, the emergence of ChatGPT-generated code signifies a distinctive and evolving paradigm in development practices. We delve into the impact of interactions with ChatGPT on the software development process, specifically analysing its influence on source code changes. Our emphasis lies in aligning code with ChatGPT conversations, separately analysing the user-provided context of the code and the extent to which the resulting code has been influenced by ChatGPT. Additionally, employing survival analysis techniques, we examine the longevity of ChatGPT-generated code segments in comparison to lines written traditionally. The goal is to provide valuable insights into the transformative role of ChatGPT in software development, illuminating its implications for code evolution and sustainability within the ecosystem."
Investigating the Utility of ChatGPT in the Issue Tracking System: An Exploratory Study,"Das, Joy Krishan and Mondal, Saikat and Roy, Chanchal",10.1145/3643991.3645083,2024,"Issue tracking systems serve as the primary tool for incorporating external users and customizing a software project to meet the users' requirements. However, the limited number of contributors and the challenge of identifying the best approach for each issue often impede effective resolution. Recently, an increasing number of developers are turning to AI tools like ChatGPT to enhance problem-solving efficiency. While previous studies have demonstrated the potential of ChatGPT in areas such as automatic program repair, debugging, and code generation, there is a lack of study on how developers explicitly utilize ChatGPT to resolve issues in their tracking system. Hence, this study aims to examine the interaction between ChatGPT and developers to analyze their prevalent activities and provide a resolution. In addition, we assess the code reliability by confirming if the code produced by ChatGPT was integrated into the project's codebase using the clone detection tool NiCad. Our investigation reveals that developers mainly use ChatGPT for brainstorming solutions but often opt to write their code instead of using ChatGPT-generated code, possibly due to concerns over the generation of ""hallucinated"" code, as highlighted in the literature."
Exploring the Role of Automation in Duplicate Bug Report Detection: An Industrial Case Study,"G\""{o}tharsson, Malte and Stahre, Karl and Gay, Gregory and de Oliveira Neto, Francisco Gomes",10.1145/3644032.3644450,2024,"Duplicate bug reports can increase technical debt and tester work-load in long-running software projects. Many automated techniques have been proposed to detect potential duplicate reports. However, such techniques have not seen widespread industrial adoption. Our objective in this study is to better understand how automated techniques could effectively be employed within a tester's duplicate detection workflow. We are particularly interested in exploring the potential of a human-in-the-loop scenario where tools and humans work together to make duplicate determinations.We have conducted an industrial case study where we characterize the current tester workflow. Based on this characterization, we have developed Bugle---an automated technique based on a complex language model that suggests potential duplicates to testers based on an input bug description that can be freely reformulated if the initial suggestions are irrelevant. We compare the assessments of Bugle and testers of varying experience, capturing how often---and why---opinions might differ between the two, and comparing the strengths and limitations of automated techniques to the current tester workflow. We additionally examine the influence of knowledge and biases on accuracy, the suitability of language models, and the limitations affecting duplicate detection techniques."
Synergizing Optogenetics and Artificial Intelligence: A Novel Paradigm for Advanced Neuroscience InvestigationsAI and Optogenetics: Advancing Neuroscience,"Wang, Xueru and Li, Shanshan and Jin, Xin",10.1145/3644116.3644288,2024,"Over the past decade, the application of artificial intelligence (AI) and optogenetics has significantly advanced our understanding of neural circuits, shedding light on their implications in both normal behavior and pathological states. This review presents a comprehensive overview of the application of these groundbreaking technologies, emphasizing their contributions and potential trajectories in biomedical research. Optogenetics, employing light to regulate cellular activities within live tissues, has emerged as a powerful tool in neuroscience, offering an unprecedented level of precision in modulating neuronal activity. AI, equipped with its advanced data analysis and predictive capabilities, has established itself as an essential tool for interpreting complex neural datasets. The combined use of these two technologies heralds a new era in the landscape of brain research."
Exploring Process Debt in Large-Scale Agile Software Development For Secure Telecom Solutions,"Saeeda, Hina and Ahmad, Muhammad Ovais and Gustavsson, Tomas",10.1145/3644384.3644470,2024,"Background: Agile methodologies emphasise iterative development, customer collaboration, and flexibility in software development. However, challenges arise when agile practices are adopted in larger projects. Process inefficiencies and redundancies, known as process debt, result from the compounded complexities of expanding agile processes and workflows. However, strategies to understand and tackle it remain markedly inadequate.Aims: This study investigates process debt types, causes, and effects in large-scale agile development and its connection with technical debt.Method: In this case study, we conducted fifteen semi-structured interviews with a Nordic IT company, primarily focusing on telecom-related products like 5G secure solutions, testing tools, and base station software. We performed a thematic analysis to examine the data qualitatively.Results: The thematic analysis identified five process debt types with 28 sub-types: documentation (3), roles &amp; responsibilities (5), synchronization (5), inefficiency &amp; unsuitability (12), and infrastructure debt (3) identified causes and effects of process debt and identified the correlation of process debt to technical debt based on descriptions from interview data and researchers' insights.Conclusions: Process debt, stemming from flawed agile practices in large-scale development, causes inefficiencies, reduces quality, and extends timelines, risking technical debt. Its management is essential for the success of these projects."
System Simulation of the Balanced Evolution Model of Network Education Information Ecosystem in a Data Intelligence Environment,"Ye, Lei",10.1145/3644479.3644492,2024,"In the era of data intelligence, the network education information ecosystem exhibits the characteristics of multimodality and large information capacity, which will have an impact on the balance of the network education information ecosystem. The existing information ecosystem evolution models are less involved in the field of network education. Therefore, it is necessary to construct an evolution model for the balance of the network education information ecosystem and conduct system simulation. This article mainly consists of three parts. Firstly, use Citespace software to conduct quantitative analysis of literature data on the information ecosystem. Secondly, construct an evolution model from three aspects which contain information subject, information, and information environment, establish a causal relationship diagram of the evolution model, obtain two positive feedback loops, draw a stock flow diagram of the evolution model, set the main equations for variables in the model, and assign values to constants. Thirdly, use the system dynamics software Vensim to simulate the changing trend of the evolution model."
Research on the Optimization of Supply Chain Information Sys-tems for Export Cross-border E-commerce,"Han, Chengguo and Liu, Zhibin",10.1145/3644523.3644577,2024,"With its unparalleled supply chain advantages and comprehensive industry support, China's export cross-border e-commerce industry has experienced significant and sustained growth. The performance optimization of supply chain information systems has continuously provided data-driven capabilities for export cross-border e-commerce. This paper focuses on the efficiency of order processing in cross-border exports, taking Dongguan SG Company as a case study. It explores the weights and performance of factors influencing the performance of supply chain information systems from six perspectives: database, hardware, code design, system scale, processes and operational standards, business models, and architecture. Subsequently, it proposes an upgrade and optimization solution encompassing aspects such as table splitting, read-write separation, server upgrades, and standardized operations. Furthermore, potential risks associated with information system upgrades are analyzed, and safeguard measures are suggested in terms of organization, technology, personnel, and institutional aspects. The upgraded and optimized supply chain information system exhibits a 2.5-fold improvement in performance, ensuring the sustainability and high-efficiency operation of cross-border e-commerce businesses."
"Research on Intelligent Construction under the Intelligent Management Scenario: Investigation, Simulation and Evaluation Algorithm","Yi, Yaxuan",10.1145/3644523.3644585,2024,"In recent years, with the continuous development of society, there has been rapid progress in intelligent buildings and their associated technologies. Among these, the application of intelligent construction products and new technologies can significantly enhance the efficiency of construction operations, ensure the safety of on-site personnel, and improve the accuracy of construction data. Consequently, intelligent construction products have become a focal issue for the majority of enterprises. Addressing this market demand, this paper simulates a technology company which specializing in intelligent building products, and investigates various innovative technologies and products related to intelligent construction. Simultaneously, a series of market surveys were conducted to analyze the current understanding and awareness of intelligent buildings among the public. Then we analyze construction techniques under smart management scenarios, presents the design of a smart construction management software based on BIM (Building Information Modeling) technology. Finally, this paper innovatively proposes an intelligent construction evaluation method based on genetic algorithms, integrated with BIM technology. This algorithm provides intuitive and clear forecasts for construction time and cost, and selects the most appropriate construction plan according to predefined rules."
Research on the Application of Business Data Fusion in the Engineering Management of Oil and Gas Storage and Transportation Station,"Xie, Rui and Fan, Bo and Ma, Jianlin and Pan, Meng",10.1145/3644523.3644605,2024,"Due to challenges such as complex contractor personnel, a large amount of scattered form data, a cumbersome business implementation process, low levels of digitalization and security, and difficult management in the engineering management business of oil and gas storage and transportation stations, this paper proposes a study on business data fusion application in the engineering management of oil and gas storage and transportation stations. Focused on the business processes and data of engineering management in oil and gas storage and transportation stations, this research establishes and analyzes an architecture for engineering management business in these stations. Using an Artifact-centric data fusion method, the research constructs an information model for the engineering management business, identifying the data topics for this area. Finally, the research applies and verifies the study of business data fusion through real-world business management scenarios. As researchers in the field of business data fusion, this study utilizes digital technologies and methods to enhance the overall safety management of engineering operations, thereby promoting the transformation and upgrading of engineering management business in oil and gas storage and transportation stations"
Research on Accurate Management of College Student Organizations Based on ICT Technology,"Dai, Yilin and Yan, Qilin and Song, Baoqing and Cai, Yunji",10.1145/3644523.3644683,2024,"This paper explores whether ICT technology can aid in comprehending quantitative factors of student organizations’ operational condition and the variations in their dynamic status. We created and implemented an ICT-based process for managing student organizations in high colleges. It can accurately capture the vital data of students taking part in association activities. To participate in the activities, we set up student organizations at the school. The findings of the experiment demonstrate that ICT technology may master many objective factors of the functioning of student associations and, to a certain extent, can represent the varying vitality status of various student organizations. We therefore believe that an ICT digital management method should be used in order to improve school administrators’ ability to precisely control the development situation and trend of associations due to the characteristics of short iteration cycles, greater personnel dynamics, and dispersed activities in student organizations."
What is the current state of Intelligent Systems Applications in digital pedagogy?: In the example of English for Specific Purposes,"Shakirova, Dilfuza",10.1145/3644713.3644778,2024,"Integrating systems into the realm of education has shown promise in addressing the various obstacles faced in providing personalized and adaptable learning experiences. However despite progress, in this field there remains a lack of exploration regarding the application of systems particularly in English for Specific Purposes (ESP) within digital education. To bridge this gap this study aims to conduct an analysis of existing literature on the subject matter. Through a review we delve into the role played by intelligence and intelligent tutoring systems in digital education with a specific focus on ESP. Furthermore we explore how data analysis plays a role in delivering tailored learning experiences while also discussing the challenges that must be overcome for integration of intelligent systems within educational platforms. The findings indicate that intelligent systems can effectively assist learners throughout their language learning journey by offering feedback adapting to individual learning styles and fostering an environment to self paced learning. Nonetheless despite these advantages implementing systems within digital education is not without its challenges. These challenges encompass considerations, inadequate infrastructure support and limited research on the impact of systems, on learner outcomes. In general this paper emphasizes the importance of conducting research to fully understand the capabilities of systems, in digital education. The discoveries presented in this paper add to the conversation about incorporating technology into language teaching and offer insights, for educators and policymakers looking to improve the quality of online learning experiences."
Sequential Pattern Mining: A Proposed Approach for Intrusion Detection Systems,"Lefoane, Moemedi and Ghafir, Ibrahim and Kabir, Sohag and Awan, Irfan Ullah",10.1145/3644713.3644803,2024,"Technological advancements have played a pivotal role in the rapid proliferation of the fourth industrial revolution (4IR) through the deployment of Internet of Things (IoT) devices in large numbers. COVID-19 caused serious disruptions across many industries with lockdowns and travel restrictions imposed across the globe. As a result, conducting business as usual became increasingly untenable, necessitating the adoption of new approaches in the workplace. For instance, virtual doctor consultations, remote learning, and virtual private network (VPN) connections for employees working from home became more prevalent. This paradigm shift has brought about positive benefits, however, it has also increased the attack vectors and surface, creating lucrative opportunities for cyber-attacks. Consequently, more sophisticated attacks have emerged, including Botnet attacks which typically lead to Distributed Denial of Service (DDoS). These pose a serious threat to businesses and organisations worldwide. This paper proposes a system for detecting malicious activities in network traffic using sequential pattern mining (SPM) techniques. The proposed approach utilises SPM as an unsupervised learning technique to extract intrinsic communication patterns from network traffic, enabling the discovery of rules for detecting malicious activities and generating security alerts accordingly. By leveraging this approach, businesses and organisations can enhance the security of their networks, detect malicious activities including emerging ones, and thus respond proactively to potential threats. The performance evaluation for the proposed approach reveals a True Positive Rate (TPR) of over 99% and a False Positive Rate (FPR) of 0%."
Unmasking Data Secrets: An Empirical Investigation into Data Smells and Their Impact on Data Quality,"Recupito, Gilberto and Rapacciuolo, Raimondo and Di Nucci, Dario and Palomba, Fabio",10.1145/3644815.3644960,2024,"Artificial Intelligence (AI) is rapidly advancing with a data-centered approach suitable for various domains. Nevertheless, AI faces significant challenges, particularly in data quality. Data collection from diverse sources can introduce quality issues that may threaten the development of AI-enabled systems. A growing concern in this context is the emergence of data smells - issues specific to the data used in building AI models, which can have long-term consequences. In this paper, we aim at enlarging the current body of knowledge on data smells, by proposing a two-step investigation into the matter. First, we updated an existing literature review in an effort of cataloguing the currently existing data smells and the tools to detect them. Afterward, we assess the prevalence of data smells and their correlation with data quality metrics. We identify a novel set composed of 12 data smells distributed across three additional categories. Secondly, we observe that the correlation between data smells and data quality is notably impactful, exhibiting a pronounced and substantial effect, especially in highly diffused data smell instances. This research sheds light on the complex relationship between data smells and data quality, providing valuable insights into the challenges of maintaining AI-enabled systems."
"Comprehensive Review on the Synergies and Challenges of Quantum Computing, Cloud Computing, and Fog Computing in New Era","Narang, Barkha and Virwani, Harshita and Jain, Nikita",10.1145/3647444.3652433,2024,"This paper explores the emerging technologies of quantum computing, cloud computing, and cloud computing, focusing on their principles, applications, advantages, challenges, and interactions. Quantum computing has the ability to adapt to solving problems in many areas using the principles of superposition and entanglement, but it faces problems such as de-coherence. Cloud computing provides convenience and accessibility but also faces security and data transfer issues. Fog computing is a method that increases efficiency by distributing tasks near the edge of the network and has applications in the Internet of Things, smart cities, and data processing over time. This article introduces various types of quantum and cloud computing, including gate computing, adiabatic computing, topological computing, photonic computing, and quantum dot computing. Quantum computing enables quantum computing, advanced machine learning, cryptography, and cross-sectional innovation. Cloud computing enables increased efficiency, productivity and innovation across businesses, but it also faces security, data transfer and environmental issues. The interaction of technologies in the 3D printing industry highlights their importance in shaping the future of computer technology."
Proposition Development Study- Intervention of AI in the Education Sector of UAE,"Ramakrishnan, Swamynathan and Bishnoi, Malini Mittal and V, Harish and S, Jijitha",10.1145/3647444.3652472,2024,"Artificial intelligence (AI) has been revolutionizing various fields, including education sector, therefore it is essential to learn about the potentials and challenges of the AI impact on learning and education process. As the UAE's commitment to become a leader in innovation and education, it is vital to study about the factors that impact the successful implementation of AI in education sector in the UAE. The study acknowledges the transformative potential of AI in education sector including personalized learning experiences, streamlined administrative tasks, and enhanced assessment processes. The paper examines the main elements that influence the application of AI in the UAE's education sector, drawing on previously published research. It investigates the technological foundation necessary to allow AI interventions, the accessibility and availability of data, the significance of data privacy and security, and the moral issues raised by the application of AI algorithms. The study analyses the existing literature on the intervention of AI in education sector in UAE and based on it the propositions are developed. The propositions developed in this study can serve as a foundation for further research, discussions, and decision-making related to the intervention of AI in the UAE's education sector."
Breaking Barriers: Overcoming Resistance to Curriculum Indigenisation,"Herbert, Nicole",10.1145/3649165.3690104,2024,"The representation of Aboriginal and Torres Strait Islander peoples in Australia's information technology (IT) sector remains critically low, exacerbated by their underrepresentation in IT-focused higher education. Introducing Indigenous perspectives into IT curricula could create a culturally safe learning environment, stimulate Indigenous engagement in IT education and careers, and enhance cultural competency among non-Indigenous IT students. Despite the potential benefits, the integration of Indigenous perspectives in IT curricula has progressed slowly due to challenges such as academic hesitancy, curriculum overload, and student resistance. This paper presents a comprehensive analysis of a four-year initiative aimed at integrating Aboriginal perspectives into an IT degree. Employing a quantitative research methodology, the study assesses the effectiveness of the approach in improving cultural competency, reducing resistance, and influencing perceptions of relevance. The results reveal significant improvements in all these areas. A key finding is diminishing resistance towards the inclusion of Indigenous content, facilitated by sustained exposure. Key lessons include the importance of confidently conveying the necessity of cultural competency for future practice, integrating Indigenous and IT-specific content effectively, and framing approaches to avoid perpetuating deficit views."
Investigating Students' Perspectives on the Value of Help-Seeking Resources in CS Education,"Zahn, Matthew and Heckman, Sarah and Battestilli, Lina",10.1145/3649165.3690130,2024,"The accessibility and effectiveness of help-seeking resources plays a pivotal role in contributing to the success of students in Computer Science courses. However, students do not always choose to utilize these resources, and when they do, their experiences can vary. While some students commend help-seeking resources for effectively providing clarification on assignment instructions, debugging code, and addressing questions about course concepts, others share instances where their problems were not resolved, or, in some cases, they did not receive any meaningful guidance from these resources. In this study, we examine the experiences of students enrolled in a CS2 course, all of whom had access to the course's help-seeking resources. These experiences were gathered through qualitative interviews at three time points within a semester. Our findings, derived from emergent coding, reveal thematic patterns in student encounters with help-seeking resources and contribute to a broader theme regarding help-seeking resource utilization at different phases of the semester. The findings of this investigation contribute to the wider conversation on student success and help-seeking resource utilization in Computer Science education."
"Agile Ethics: A Low Stakes, Skills-based Framework for Teaching CS Ethics","Brooks, Alexi",10.1145/3649217.3653539,2024,"Computer Science educators widely agree that ethics is a vital and underdeveloped part of the CS curriculum. Attempts to increase ethics content within undergraduate CS programs have faced challenges integrating material into the current coursework. I present an ethical framework applicable to the core Computer Science activities of programming and software development, with potential for extension into other CS subfields. The application of this framework to a specific educational intervention is reserved for future work. In this paper, I focus on the framework itself and its theoretical justification. By shifting emphasis from hard ethical quandaries and advanced CS products to mundane challenges faced by a front line software developer, this framework may allow instructors to more easily and effectively integrate ethics material into introductory CS coursework. While instructors may continue to apply prepared scenarios, the framework de-emphasizes those in favor of scaffolding student coding practices that maximize the frequency of practice with ethical skills.The Agile Ethics framework provides a structure which students and educators can use to think about (1) when during a project ethical reasoning is needed, (2) what questions need to be asked at that time, and (3) how to apply Computer Science skills and knowledge to answer each question. Frequent, low intensity instances of ethical reasoning under this framework reinforce the integration of ethics as a habitual part of the software development process."
AI-Grading Standup Updates to Improve Project-Based Learning Outcomes,"Menezes, Tyler and Egherman, Lola and Garg, Nikhil",10.1145/3649217.3653541,2024,"Integrating project-based learning (such as class projects, capstones, or internships) into a Computer Science degree helps students apply what they have learned in lectures and homework. Often, these projects involve group work, but ensuring all students are contributing and learning equally is a large challenge.This experience report describes how we built and used a chatbot to collect and publish brief answers to the ""standup"" questions used daily in the technology industry: ""What did you do yesterday? What are you doing today? Is anything blocking you?"". We share the rubric we used to grade these updates, as well as how we trained an AI tool to perform this work for us.This tool was used by several hundred students from US-based Career and Technical Colleges (CTCs) and non-R1 universities, who were largely underrepresented in Computer Science. We found that collecting and publishing standups reduced the number of students who did not significantly contribute. We also found that scoring the standups according to a rubric helped identify and reach out to any students who were still not contributing, which helped some students but not others. Finally, we found that an AI tool can be used to evaluate student updates at scale helping under-resourced schools and over-worked faculty to implement this program in their classrooms."
"Embedding Technical, Personal and Professional Competencies in Computing Degree Programmes","Prickett, Tom and Crick, Tom and Davenport, James H. and Bowers, David S. and Hayes, Alan and Irons, Alastair",10.1145/3649217.3653578,2024,"Many factors influence computing graduate employment prospects, including human capital, social capital, individual attributes, individual career-building behaviours, perceived employability, and labour market factors. Whilst most computing graduates go on to be beneficially employed, a small minority remain under-employed or unemployed. Computing curricular recommendations increasingly advocate a competency-based approach to bolster graduates' perceived employability. Hence, the discipline is evolving to incorporate competency-based approaches. However, competency-based can mean any of three different types of competency: technical, personal and professional. Technical Competency is the ability to apply acquired content knowledge and skills to develop solutions to unseen problems. Personal Competency is the personal behaviours and interpersonal skills required for success in the modern workplace. Professional Competency is Technical and Personal combined and applied in a real-world context.This position paper provides illustrative examples of how to embed all three kinds of Competency. Based on examples from representative undergraduate computing programmes at UK universities, it provides examples of embedding each kind of competency: Technical Competency (teaching programming through craft computing and approaches for developing cybersecurity competency), Personal Competency (teaching teamwork through project-based learning and creativity via problem-based learning), and Professional Competency (developing work-ready competency using industrial placements, and co-design/co-delivery with industry via degree apprenticeships), providing a valuable foundation and framing for portability and extension in other institutions and jurisdictions. Furthermore, these distinctive types of competency form a helpful taxonomy when considering how to embed competency in computing courses and are candidates for inclusion within future computing curricula guidelines."
Enhancing Student Engagement in Large-Scale Capstone Courses: An Experience Report,"Shakil, Asma and Denny, Paul",10.1145/3649217.3653580,2024,"Computer science (CS) capstone courses offer students a valuable opportunity to gain hands-on experience in software development, practice essential soft skills, and enhance their employability prospects. They are a core component in many CS undergraduate degrees and address the ACM curricula requirements of inculcating professional dispositions in students and making them aware of the broader societal implications of computing. However, coordinating a capstone course, especially for a large student cohort, can be a daunting task for academic staff. It demands considerable time and energy for planning and coordinating activities between students, academic staff, and any external stakeholders. In this experience report, we outline the iterative development and refinement of our capstone course as it grew substantially in size over a span of six consecutive sessions. We outline the pedagogies that helped us to enhance student engagement and motivation in the course as assessed by end-of-course surveys and students' written reflections. We share the lessons that we have learnt and provide recommendations to educators who are designing new capstone courses or looking to scale existing ones."
CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios,"Zeng, Zhengran and Wang, Yidong and Xie, Rui and Ye, Wei and Zhang, Shikun",10.1145/3650212.3652115,2024,"In the evolving landscape of large language models (LLMs) tailored for software engineering, the need for benchmarks that accurately reflect real-world development scenarios is paramount. Current benchmarks are either too simplistic or fail to capture the multi-tasking nature of software development. To address this, we introduce CoderUJB, a new benchmark designed to evaluate LLMs across diverse Java programming tasks that are executable and reflective of actual development scenarios, acknowledging Java's prevalence in real-world software production. CoderUJB comprises 2,239 programming questions derived from 17 real open-source Java projects and spans five practical programming tasks. Our empirical study on this benchmark investigates the coding abilities of various open-source and closed-source LLMs, examining the effects of continued pre-training in specific programming languages code and instruction fine-tuning on their performance. The findings indicate that while LLMs exhibit strong potential, challenges remain, particularly in non-functional code generation (e.g., test generation and defect detection). Importantly, our results advise caution in the specific programming languages continued pre-training and instruction fine-tuning, as these techniques could hinder model performance on certain tasks, suggesting the need for more nuanced strategies. CoderUJB thus marks a significant step towards more realistic evaluations of programming capabilities in LLMs, and our study provides valuable insights for the future development of these models in software engineering."
An Empirical Study of Static Analysis Tools for Secure Code Review,"Charoenwet, Wachiraphan and Thongtanunam, Patanamon and Pham, Van-Thuan and Treude, Christoph",10.1145/3650212.3680313,2024,"Early identification of security issues in software development is vital to minimize their unanticipated impacts. Code review is a widely used manual analysis method that aims to uncover security issues along with other coding issues in software projects. While some studies suggest that automated static application security testing tools (SASTs) could enhance security issue identification, there is limited understanding of SAST’s practical effectiveness in supporting secure code review. Moreover, most SAST studies rely on synthetic or fully vulnerable versions of the subject program, which may not accurately represent real-world code changes in the
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
code review process.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
To address this gap, we study C/C++ SASTs using a dataset of actual code changes that contributed to exploitable vulnerabilities. Beyond SAST’s effectiveness, we quantify potential benefits when changed functions are prioritized by SAST warnings. Our dataset comprises 319 real-world vulnerabilities from 815 vulnerability-contributing commits (VCCs) in 92 C and C++ projects. The result reveals that a single SAST can produce warnings in vulnerable functions of 52% of VCCs. Prioritizing changed functions with SAST warnings can improve accuracy (i.e., 12% of precision and
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
5.6% of recall) and reduce Initial False Alarm (lines of code in non-vulnerable functions inspected until the first vulnerable function) by 13%. Nevertheless, at least 76% of the warnings in vulnerable functions are irrelevant to the VCCs, and 22% of VCCs remain undetected due to limitations of SAST rules. Our findings highlight the benefits and the remaining gaps of SAST-supported secure code reviews and challenges that should be addressed in future work."
Identifying Smart Contract Security Issues in Code Snippets from Stack Overflow,"Chen, Jiachi and Chen, Chong and Hu, Jiang and Grundy, John and Wang, Yanlin and Chen, Ting and Zheng, Zibin",10.1145/3650212.3680353,2024,"Smart contract developers frequently seek solutions to developmental challenges on Q&amp;A platforms such as Stack Overflow (SO). Although community responses often provide viable solutions, the embedded code snippets can also contain hidden vulnerabilities. Integrating such code directly into smart contracts may make them susceptible to malicious attacks. We conducted an online survey and received 74 responses from smart contract developers. The results of this survey indicate that the majority (86.4%) of participants do not sufficiently consider security when reusing SO code snippets. Despite the existence of various tools designed to detect vulnerabilities in smart contracts, these tools are typically developed for analyzing fully-completed smart contracts and thus are ineffective for analyzing typical code snippets as found on SO. We introduce SOChecker, the first tool designed to identify potential vulnerabilities in incomplete SO smart contract code snippets. SOChecker first leverages a fine-tuned Llama2 model for code completion, followed by the application of symbolic execution methods for vulnerability detection. Our experimental results, derived from a dataset comprising 897 code snippets collected from smart contract-related SO posts, demonstrate that SOChecker achieves an F1 score of 68.2%, greatly surpassing GPT-3.5 and GPT-4 (20.9% and 33.2% F1 Scores respectively). Our findings underscore the need to improve the security of code snippets from Q&amp;A websites."
Practitioners’ Expectations on Automated Test Generation,"Yu, Xiao and Liu, Lei and Hu, Xing and Keung, Jacky and Xia, Xin and Lo, David",10.1145/3650212.3680386,2024,"Automated test generation can help developers craft high-quality software tests while mitigating the manual effort needed for writing test code. Despite significant research efforts in automated test generation for nearly 50 years, there is a lack of clarity about what practitioners expect from automated test generation tools and whether the existing research meets their needs. To address this issue, we follow a mixed-methods approach to gain insights into practitioners' expectations of automated test generation. We first conduct the qualitative analysis from semi-structured interviews with 13 professionals, followed by a quantitative survey of 339 practitioners from 46 countries across five continents. We then conduct a literature review of premier venue papers from 2022 to 2024 (in the last three years) and compare current research findings with practitioners' expectations. From this comparison, we outline future research directions for researchers to bridge the gap between automated test generation research and practitioners' expectations."
Interactive Home Product Design Method Using Augmented Reality Technology,"Ma, Yuwen and Chen, Guangbo and Gao, Huayun",10.1145/3650215.3650305,2024,"Augmented Reality (AR) is currently a hot research direction in the world, but its main research objects are computer vision and image processing. However, the application of AR technology in virtual reality systems is still in its infancy. Based on this, the article adopts a home interactive product design scheme based on AR technology. This article uses 3D modeling and rendering technology to realize the generation of 3D stereoscopic images of household products. On this basis, this article uses the augmented reality method to combine virtual scenes with real scenes, allowing users to view and control virtual items in real time through mobile phones and other terminal devices. Through this interactive method, users can personally experience the use of household products, and then understand the characteristics and scope of application of the products. Experimental results show that the design efficiency of the design method using augmented reality technology reaches a maximum of 85.1%, which proves that the method proposed in the article can effectively improve the efficiency of home product design, and also provides designers with a new design tool and idea."
A Practical Failure Prediction Model based on Code Smells and Software Development Metrics,"Sch\""{u}tz, Martin and Pl\""{o}sch, Reinhold",10.1145/3651640.3651644,2024,"Making errors during software development is unavoidable. Developers inevitably make errors that take additional time to fix later. Consequently, efforts for bug fixing compete with implementing new features. Typically, the later bugs are found, the higher the cost for remediation. To address this concern, software testing should start as early as possible in software development lifecycle. For this purpose, static analysis is proposed, but typically shows too many findings and hence do not support development teams appropriately. So, it would be a benefit to premature detect those findings in static analysis that will result in failures to reduce subsequent efforts notably. The purpose of the paper is to analyze failure data from issue tracking systems that are correlated to findings from static analysis. Thereupon an artificial intelligence-based approach is used to train practicable models for business environment that enables effective prediction of software faults. The results from static analysis show that predefined complexity measures encompassed the most defects. While there are commonalities in relevant defect findings in static analysis reports, meaningful prediction models cannot be expected based solely on this data. In addition to the findings of the static analysis, metrics like code changes in a time period or number of authors involved in code changes were considered for building the prediction models. Two of the developed prediction models have a high accuracy and excellent utility rate. These resulting prediction models are currently used at Raiffeisen Software GmbH for a long-term study on failure prediction based on code smells."
Transforming Tourism Experience: AI-Based Smart Travel Platform,"Ozcelik, Suayb Talha and Yondem, Meltem Turhan and Caetano, Ines and Figueiredo, Jose and Alves, Patricia and Marreiros, Goreti and Bahtiyar, Huseyin and Yuksel, Eda and Perales, Fernando and Suciu, George",10.1145/3651640.3651645,2024,"In this paper, we propose the development of a novel personalized tourism platform incorporating artificial intelligence (AI) and augmented reality (AR) technologies to enhance the smart tourism experience. The platform utilizes various data sources, including travel history, user activity, and personality assessments, combined with machine learning algorithms to generate tailored travel recommendations for individual users. We implemented fundamental requirements for the platform: secure user identification using blockchain technology and provision of personalized services based on user interests and preferences. By addressing these requirements, the platform aims to increase tourist satisfaction and improve the efficiency of the tourism industry. In collaboration with various universities and companies, this multinational project aims to create a versatile platform that can seamlessly integrate new smart tourism units, providing an engaging, educational, and enjoyable experience for users."
Analyzing Role of Artificial Intelligence in Project Management and Investment Risk: A CiteSpace Insight,"Lai, Shuang and Zhang, Shaoqian and Hassan, Abual and Mushtaq, Ray Tahir",10.1145/3651671.3651776,2024,"The current research work explores the artificial intelligence and sustainability impact on investment risk and project management based on trend analysis from 2017 to 2022. Deakin University; the University of London or other key contributors and institutions that contribute to this field is identified by a rigorous analysis of one hundred nineteen articles from Web of Science. In it, a considerable part of the world is shown with meaningful contributions from China, Korea and the United States. This research presents the changing AI phenomenon in risk management for investments and especially notes that there is increasing connection with sustainability. The results of this work emphasize the role of AI in renovating traditional risk paradigms and suggest appropriate international cooperation. The study thus not only uncovers the current state of knowledge but also positions itself as a starting point for future research in AI-driven investment risk management, providing valuable perspectives to both science and practice."
ChatGPT in Nutrition: Trends Challenges and Future Directions,"Tsiantis, Vasileios and Konstantinidis, Dimitrios and Dimitropoulos, Kosmas",10.1145/3652037.3663898,2024,"A healthy and balanced diet is of paramount importance to the physical and psychological well-being of an individual, since unhealthy dietary choices have been linked with the occurrence of non-communicable diseases. Recent technological advances has led to the development of large language models, such as ChatGPT that has been widely adopted as a convenient tool for creating meal plans and obtaining nutritional information. This work aims to provide a thorough review of the scientific work performed so far regarding the use of ChatGPT in the field of nutrition. In this survey, the advantages and limitations of ChatGPT in different aspects of nutrition are pinpointed, while the challenges from the use of ChatGPT in nutrition are identified and discussed. Future research directions are also proposed to assist researchers towards the development of more effective nutrition recommendation systems."
The use of mobile learning in special education needs and disabilities (SEND) settings: state-of-the-art classification of studies,"Muazu, Fatima A and Adedoyin, Festus A and Dogan, Huseyin and Mavengere, Nicholas and Whittington, Paul",10.1145/3652037.3663915,2024,"In developed countries, the use of mobile learning particularly has changed the delivery of teaching and learning in mainstream and special schools and evidently improved academic performance, there is still limited research on its use in underserved regions of the world. The purpose of this study is to conduct a review of existing studies on the application of mobile learning as an assistive technology in special education to enable the understanding of the depth of research in the field especially in the African context. The study adopts a systematic literature review approach to guide literature search, identification, and selection on EBSCOHOST and Scopus databases. 34 articles that were published from 2019-2024 in any language were included in this review. The review further classified these studies in terms of their years of publication, countries, aims of research, research methods and target disability the interventions were employed for. The findings revealed that there are a substantial number of studies that specifically considered the application of mobile learning in the education of special needs learners with autism spectrum disorder and intellectual disabilities and fewer studies targeted auditory, visual and communication impairments, and specific learning disabilities that included dyslexia, dyscalculia, and dysgraphia. In terms of countries/regions of research, there were more studies conducted in Asia and Europe, sub-Saharan African countries had the least representations. Quantitative research methods were the most adopted methods of research."
Teaching Model-Driven Low-Code Development Platforms,"Charles, Joel and Michael, Judith and Netz, Lukas and Rumpe, Bernhard",10.1145/3652620.3687778,2024,"We present and evaluate a method called grammar masking, which is used to guide large language models (LLMs) toward producing syntactically correct models for a given context-free grammar. Prompt engineering methods such as few-shot learning or priming can be used to improve the chances of an LLM producing correct syntax, but the more complex the grammar, the more time-consuming and less promising these methods become. Previous work is focused primarily on the usage of either language model training or prompt engineering. In this work, a method is presented that restricts the output to a given grammar using constrained decoding to ensure the output adheres to a valid syntax. We use several domain-specific languages (DSLs) built with MontiCore and task multiple LLMs to produce models with and without constrained decoding. A corresponding parser is used to confirm the syntactic correctness of each model. We show that grammar masking can dramatically improve the modeling capabilities of several LLMs, reducing the need for well-refined prompting while increasing the chance of producing correct models."
Contract-based Validation of Conceptual Design Bugs for Engineering Complex Machine Learning Software,"Meijer, Willem",10.1145/3652620.3688201,2024,"Context. Modern software systems increasingly commonly contain one or multiple machine learning (ML) components. Current development practices are generally on a trial-and-error basis, posing a significant risk of introducing bugs. One type of bug is the ""conceptual design bug,"" referring to a misunderstanding between the properties of input data and prerequisites imposed by ML algorithms (e.g., using unscaled data in a scale-sensitive algorithm). These bugs are challenging to test at design time, causing problems at runtime through crashes, noticeably poor model performance, or not at all, threatening the system's robustness and transparency. Objective. In this work, I propose the line of research I intend to pursue during my PhD, addressing conceptual design bugs in complex ML software from a prevention-oriented perspective. I intend to build open-source tooling for ML engineers that can be used to detect conceptual design bugs, enabling them to make quality assurances about their system design's robustness. Approach. We need to understand conceptual bugs beyond the status quo, identifying their types, prevalence, impacts, and structural elements in the code. We operationalize this knowledge into a tool that detects them at design time, allowing ML engineers to resolve them before running their code and wasting resources. We anticipate this tool will leverage contract-based validation applied to partial ML software models. Evaluation. We plan to evaluate the built tool two-fold using professional (industrial) ML software. First, we will study its effectiveness regarding bug detection at design time, identifying whether it fulfills its functional objective. Second, we will study its usability, identifying whether ML engineers benefit when tools like this are introduced into their ML engineering workflow."
An MBSE approach for Virtual Verification &amp; Validation of Systems with Digital Twins,"Honcak, Rene and Wooley, Ana",10.1145/3652620.3688252,2024,"The automotive industry is embracing digital transformation, with trends like electric mobility leading the way. However, how will we efficiently and sustainably develop our electrified vehicles in the future? Agile customer demands, which require shorter product development cycles, pose a challenge for the automotive supplier industry. One approach to address this challenge is the adoption of Model Based Systems Engineering (MBSE) and Digital Twins (DT) for product digitalization and virtualization. MBSE supports product development from inception to the entire product lifecycle by continuously describing the system based on a system model. DTs, which are virtual representations of physical products/systems, capture both behavior and lifespan through simulation models and workflows. The complexity of DTs will highly increase due to multi-physical interactions, requiring consistency and traceability for the cross-domain development as well as application of simulation models and workflows in virtual testing and product approvals. The foundation lies in calibrated, verified, and validated simulation models, as well as reproducible simulation workflows, with capabilities that include uncertainty assessments. Processes and methods are necessary to manage complexity, ensure systematic validation, and efficiently develop DTs. Current approaches often overlook how DTs should be specified, designed, tested, and standardized. This research highlights the advantages of introducing a V-model and MBSE for DT development within the context of digital engineering. These measures can enhance the efficiency, reusability, quality, and automation of DTs."
PromptDeck: A No-Code Platform for Modular Prompt Engineering,"Bucchiarone, Antonio and Panciera, Marco and Cicchetti, Antonio and Mana, Nadia and Castelluccio, Carlotta and Stott, Lee",10.1145/3652620.3688336,2024,"This paper introduces a no-code platform for modular prompt engineering, designed to democratize access to generative AI for nondevelopers. By integrating advanced technologies such as Node.js, Express, MongoDB, and Azure OpenAI services, the platform provides a robust and flexible environment for creating and managing AI-driven tasks. The intuitive frontend, built with React and TypeScript, enables users with minimal coding expertise to design, execute, and evaluate complex AI workflows. A key feature of the platform is its extensible plugin system, which allows users to easily incorporate additional functionalities to meet their specific needs. This no-code approach empowers a broader audience to harness the power of generative AI, fostering innovation and enabling diverse applications across various fields. By lowering the technical barriers, the platform paves the way for widespread adoption of AI technologies, driving the future of AI-enhanced solutions."
Participatory and Collaborative Modeling of Sustainable Systems: A Systematic Review,"Manellanga, Rajitha and David, Istvan",10.1145/3652620.3688557,2024,"Sustainability has become a key characteristic of modern systems. Unfortunately, the convoluted nature of sustainability limits its understanding and hinders the design of sustainable systems. Thus, cooperation among a diverse set of stakeholders is paramount to sound sustainability-related decisions. Collaborative modeling has demonstrated benefits in facilitating cooperation between technical experts in engineering problems; but fails to include non-technical stakeholders in the modeling endeavor. In contrast, participatory modeling excels in facilitating high-level modeling among a diverse set of stakeholders, often of non-technical profiles; but fails to generate actionable engineering models. To instigate a convergence between the two disciplines, we systematically survey the field of collaborative and participatory modeling for sustainable systems. By analyzing 24 primary studies (published until June 2024), we identify common challenges, cooperation models, modeling formalisms and tools; and recommend future avenues of research."
A Systematic Review on the Socio-affective Perception of IVAs' Multi-modal behaviour,"Etienne, Elodie and Ristorcelli, Marion and Saufnay, Sarah and Quilez, Aur\'{e}lien and Casanova, R\'{e}my and Schyns, Michael and Ochs, Magalie",10.1145/3652988.3673943,2024,"The multimodal behaviour of IVAs may convey different socio-affective dimensions, such as emotions, personality, or social capabilities. Several research works show that factors may impact the perception of the IVA’s behaviour. This paper proposes a systematic review, based on the PRISMA method, to investigate how the multimodal behaviour of IVAs is perceived with respect to socio-affective dimensions. To compare the results of different research works, a socio-emotional framework is proposed, considering the dimensions commonly employed in the studies. The conducted analysis of a wide array of studies ensures a comprehensive and transparent review, providing guidelines on the design of socio-affective IVAs."
Research on optimization algorithm of business system design based on graph theory and its application in digital power marketing business system design,"Xu, Yuanbin and Tian, Tian and Zhu, Pingfei and Zhang, Xiaowu and Li, Tongwei and Chen, Yuqing",10.1145/3653081.3653130,2024,"Object design is a key task in the research and development of enterprise digital systems. In the past, object design was more based on the cognitive level of the designers, and the design results could not fully meet the requirements of ""high cohesion and low coupling"" design principle, which led to a large amount of modification work when the system faced with the change of requirements, and it was difficult to support the continuous development of the enterprise's digital business. This paper proposes an object design optimization method based on graph theory, which consists of four parts: firstly, the requirements and objects are expressed as requirement diagrams and object diagrams, so that the object design can be abstracted as a transformation from the requirement diagrams to the object diagrams; secondly, quantitative evaluation indexes of cohesion and coupling are given for the object model based on the graph theory to quantify the principle of ""high cohesion, low coupling"", so as to quantify the principle of ""high cohesion, low coupling"". Secondly, based on graph theory, the quantitative evaluation indexes of aggregation and coupling degree of object model are given to quantify the principle of ""high cohesion and low coupling"", so that quantitative comparison can be made between different design solutions to guide the design optimization work; thirdly, the object groups that conform to the characteristics of a specific graph theory can be found in the object relationship graph, and designers can decide whether or not to merge them in order to form the optimized object design; fourthly, based on the requirement graph and industry knowledge, the Digital twin can be constructed heuristically, which can significantly increase the degree of object The fourth is that digital twin objects can be constructed heuristically based on the demand graph and industry knowledge, which can significantly increase the degree of object aggregation and reduce the degree of coupling between objects. The paper illustrates the design algorithm process with a simplified electric power marketing system, and gives a comparison of the design results of the actual digital marketing system."
Research on E-commerce Professional Talent Cultivation System in Colleges and Universities Based on SWOT-AHP Algorithm,"Li, Su",10.1145/3653644.3658512,2024,"Abstract: The e-commerce market is constantly changing in the new retail era, and the training of e-commerce professionals cannot keep up with the changes in market demand. In order to train new and high-quality professional e-commerce talents, a e-commerce professional talent training system is constructed based on SWOT situation analysis method and AHP analytic hierarchy process. The results show that the training strategy suitable for the current situation is SO pioneering, giving full play to the internal advantages of colleges and universities, while seizing opportunities in the external market to strengthen school-enterprise cooperation and exchanges. After the reform of the talent training system, 68% of students have engaged in professional counterpart work, and the corresponding employment rate has increased by 21%, providing a new reference for the reform of the college e-commerce professional talent training system."
"""Those don't work for us"": An Assets-Based Approach to Incorporating Emerging Technologies in Viable Hawaiian Teacher Support Tools for Culturally Relevant CS Education","Gelder, William and Baker-Ramos, Rachel and Cho, Ayoung and Kolakaluri, Jahnavi and Uchidiuno, Judith and Hester, Josiah",10.1145/3653666.3656066,2024,"Hawaiian bilingual language immersion (Kaiapuni) schools infuse curricula with place-based education to increase student connection to culture. However, stand-in teachers often lack the background and tools needed to support immersion learning, resulting in discontinuity for students in their culturally relevant education. This experience report describes a partnership between the Ka Moamoa Lab at the Georgia Institute of Technology and Ke Kula Kaiapuni 'O Pu'ohala School to design a teacher-substitute support platform via a hybrid of assets-based design methodology and emerging technology capabilities. We share insights offered by teachers and design requirements for such a platform. We also reflect on how HCI methodologies should adapt to center and respect Native Hawaiian perspectives."
Literature Review as a Service: A Human-Centered Artificial Intelligence Approach,"Le Dinh, Thang and Le, Tran Duc and Uwizeyemungu, Sylvestre and Nguyen, Chan Nam",10.1145/3655497.3655516,2024,"Nowadays, the use of generative artificial intelligence (AI), such as chatGPT and other AI-powered tools, has a significant impact on conducting research projects. Despite the advantages of this trend, it is believed that AI-powered tools should be utilized to enhance the literature review process rather than completely automating it. For this reason, this paper examines the emerging concept of ""Literature Review as a Service"" (LRaaS) based on the human-centered artificial intelligence (HCAI) perspective. Drawing on the theoretical background and service science, the paper explores a new approach to exploit the potential of AI-powered tools while putting people at the center of the research process. Through an initial prototype, the paper illustrates how AI-powered tools assist the literature review process. The paper ends with conclusions and recommendations for further research."
Supply chain innovation paths and resilience management under sudden social public events: a meta-analysis,"Li, Shengzhu and Xiang, Cong",10.1145/3656766.3656873,2024,"In the context of frequent social emergencies, more and more enterprises are actively exploring the innovation path and resilience management of supply chain driven by strong realistic needs, but there is no consistent conclusion on the relationship between supply chain innovation and supply chain resilience. Based on this, this paper adopts meta-analysis method, taking 37 independent samples from 2011 to 2022 as research objects (the total sample size is 13306), comprehensively explores the overall effect of supply chain innovation and supply chain resilience, compares the relationship of technological innovation, management innovation and Organizational innovation in supply chain innovation, and uses subgroup analysis to explore the moderating effect of moderating variables in their relationship. The results show that: (1) In terms of overall effect, there is a strong positive correlation between supply chain collaborative innovation and enterprise performance; (2) In terms of innovation path, supply chain technological innovation, management innovation and Organizational innovation can all have a positive impact on supply chain resilience management, and the effect of technological innovation path is more significant; (3) In terms of moderating effect, dynamic market environment is more conducive to the benefit of innovation behavior in supply chain resilience management; small and medium-sized enterprises implement innovation more effectively than large enterprises. This study expands the theoretical research on supply chain resilience management and provides management implications for supply chain enterprises to implement innovation."
The Innovative Application of Big Data in Chinese Media Industry,"Lan, Yanling and Liu, Sihang",10.1145/3656766.3656938,2024,"With the big data industry accelerating towards incremental, high-quality and intelligent upgrading and iteration, algorithm and computing power, data and intelligence empowerment has become the most important driving forces of Chinese media industry. This study applies literature research, in-depth interviews and case studies to systematically analyze how big data and AI technology are implemented in Chinese media institutions, such as CCTV, ByteDance, Alibaba, Tencent etc. The results show that the big data application of Chinese media industry has entered the era of intelligent advancement while the media's understanding of content, users and scenes has been improved in quantity and quality. And the renewal of ideas, integration of technologies and support from internet enterprises give Chinese media institutions more opportunities for innovation and development in the new digital era through the construction of digital intelligent media ecosystem."
Exploring ICT Solutions in Citizen-Led Communication for Public Value Enhancement,"Kazadi Katembwe, Jean-Marc and Sneiders, Eriks",10.1145/3657054.3657103,2024,"In the context of the digital age, the advent of ICT (Information and Communication Technology) has been a critical factor in reshaping the citizen-government dynamic, steering towards a Transformational Government (T-Gov) model. However, existing literature largely overlooks a nuanced examination of how ICT enhances public values regarding operational effectiveness and citizen-centric service delivery within public organizations. This research seeks to fill this the gap by focusing on the effectiveness of ICT's role in improving public values with particular attention on citizen-initiated communication, thereby contributing to a more targeted understanding of ICT's impact in the T-Gov framework. Data were gathered through a Systematic Literature Review (SLR). The findings provided a comprehensive list of ICT solutions mapped to public values they intended to facilitate the development. These ICT solutions were subsequently analyzed to assess their effectiveness in creating and developing the public values they targeted and the extent of their impact and shortcomings. The examination outcome engendered practical and innovative recommendations for public organizations, delineating ICT strategic approach and investment with special attention to progressive integration and application of technologies to improve further public values."
"Societal, Economic, Political and Environmental: A Review of Benchmarks and AI-assisted Systematic Literature Review of Impact of Open Government Data","Kao, Hao-En",10.1145/3657054.3657121,2024,"The purpose of this research is to review the impact on open government data in evaluation at world level and research in academia. First, six most renowned open government data evaluations are reviewed with their impact section. Then, a systematic literature review on the impact of open government data is conducted with test of AI-assisted software using Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) Protocol, to find the current status of realized open government data impact research.Only four out of six evaluations of open government data have covered impact explicitly, and their criteria counting toward true impact is blur at best. AI-assisted software saves around 30∼40% of reviewing time in this research. Political, social, and economic impacts are all included research topics, however, no environmental impact research. Political impact outperformed other impact, both in quality and quantity. Political capital, not social capital shows more social impact on open government data. Economic impact research of open government data is scarce, and more about the open government data helps urban innovation in China.This research contributes to the open government data research in three ways. First, it reviews impact section major open government data evaluations. Second, it tests the AI-assisted software on the impact of open government data. Third, it sheds light on new research direction for impact of open government data on case study, qualitative research and mixed method."
Towards a Privacy and Security-Aware Framework for Ethical AI: Guiding the Development and Assessment of AI Systems,"Korobenko, Daria and Nikiforova, Anastasija and Sharma, Rajesh",10.1145/3657054.3657141,2024,"As artificial intelligence (AI) continues its unprecedented global expansion, accompanied by a proliferation of benefits, an increasing apprehension about the privacy and security implications of AI-enabled systems emerges. The pivotal question of effectively controlling AI development at both jurisdictional and organizational levels has become a prominent theme in contemporary discourse. While the European Commission has taken a decisive step by reaching a political agreement on the EU AI Act, the world’s first comprehensive AI law, organizations still find it challenging to adapt to the fast-evolving AI landscape, lacking a universal tool for evaluating the privacy and security dimensions of their AI models and systems. In response to this challenge, this study conducts a systematic literature review (SLR) with a primary focus on establishing a unified definition of key concepts in AI Ethics, particularly emphasizing the domains of privacy and security. Through the synthesis of knowledge extracted from the SLR, this study presents a conceptual framework tailored for privacy- and security-aware AI systems. This framework is designed to assist diverse stakeholders, including organizations, academic institutions, and governmental bodies, in the development and critical assessment of AI systems. Essentially, the proposed framework serves as a guide for ethical decision-making, fostering an environment wherein AI is developed and utilized with a strong commitment to ethical principles. In addition, the study unravels the key issues and challenges surrounding the privacy and security dimensions, delineating promising avenues for future research, thereby contributing to the ongoing dialogue on the globalization and democratization of AI ethics."
Paradoxes in Digital Government: A Systematic Literature Review,"Wang, Yifan and Ran, Bing and Ma, Liang",10.1145/3657054.3657144,2024,"This study aims to unravel the complexities of paradoxes in digital government. Through a focused analysis of 35 rigorously selected articles, adhering to PRISMA guidelines, this research codifies seven paradoxes, categorized into ‘Internal Governance Paradoxes’ and ‘External Engagement Paradoxes’. These paradoxes arise from conflicting values among governments, employees, citizens, and businesses, reflecting both the intricacies of decision-making within government organizations and the dynamics of their interactions with external stakeholders. This paper also proposes a handling strategy that guides stakeholders in collaboratively creating public value through interventions in technology, organization, and environment (TOE). By mapping the landscape of digital government paradoxes and suggesting potential mechanisms for their resolution, this study contributes to the field, paving the way for more effective and coherent digital governance strategies."
Leadership and Transformation in the Public Sector: An Empirical Exploration of AI Adoption and Efficiency during the Fourth Industrial Revolution,"Valle-Cruz, David and Garcia-Contreras, Rigoberto and Munoz-Ch\'{a}vez, J. Patricia",10.1145/3657054.3657146,2024,"The fourth industrial revolution (4IR) demands transformative leadership, as leaders grapple with gaps and questions, particularly in optimizing Artificial Intelligence (AI). This paper seeks to comprehend public sector leaders' perceptions, identifying prevalent traits and skills amid AI adoption and efficiency. Employing the PRISMA methodology for a systematic literature review, the study reveals a dearth of research on this topic. Combining the systematic literature review with traditional leadership theory, a PLS-SEM model tests 22 statistical hypotheses for empirical analysis. Results indicate a positive correlation between leadership traits and skills with AI adoption and efficiency. This highlights the pivotal role of prepared leaders in successfully integrating AI, ensuring effective uptake and efficient utilization for optimal outcomes. Insights highlight leaders' essential engagement in supporting, preparing, and innovating, underscoring their central role in optimizing AI adoption."
An Integrated Usability Framework for Evaluating Open Government Data Portals: Comparative Analysis of EU and GCC Countries,"Molodtsov, Fillip and Nikiforova, Anastasija",10.1145/3657054.3657159,2024,"This study explores the critical role of open government data (OGD) portals in fostering transparency and collaboration between diverse stakeholders. Recognizing the challenges of usability, communication with diverse populations, and strategic value creation, this paper develops an integrated framework for evaluating OGD portal effectiveness that accommodates user diversity (regardless of their data literacy and language), evaluates collaboration and participation, and opportunities to explore and understand the data provided through them. The framework is validated by applying it to 33 national portals across European Union and Gulf Cooperation Council (GCC) countries, as a result of which we rank OGD portals, identify some good practices that lower-performing portals can learn from, and common shortcomings. The study unveils the competitive and innovative nature of GCC OGD portals, pinpointing specific improvement areas such as multilingual support and data understandability. The findings underscore the growing trend of exposing data quality metrics and advocate for enhanced two-way communication channels between users and portal representatives. Overall, the study contributes to accelerating the development of user-friendly, collaborative, and sustainable OGD portals while addressing gaps identified in previous research."
Who evaluates the algorithms? An overview of the algorithmic accountability ecosystem,"Criado, J. Ignacio and Guevara-Gomez, Ariana",10.1145/3657054.3657247,2024,"Algorithmic accountability is a concept that has been gaining interest in academic and professional circles, especially when considering the potential negative impacts of Artificial Intelligence in diverse scenarios. In this article, we present the results of a systematic literature review and in-depth interviews to advance the understanding of the algorithmic accountability definition and the dynamics within the ecosystem of such processes, with a focus on the role of the public sector. We also offer some practical recommendations for fostering an accountability ecosystem that accommodates the complexities of algorithmic systems, through multi-stakeholder collaboration, public regulation and oversight, and citizen participation."
Impact and barriers to AI in the public sector: the case of the State of Mexico,"Millan-Vargas, Adrian Osiel and Sandoval-Almazan, Rodrigo and Valle-Cruz, David",10.1145/3657054.3657249,2024,"The use and implementation of Artificial Intelligence (AI) tools for doing repetitive tasks in the public sector is a challenge, particularly in persuading bureaucrats. However, the potential benefits for citizens, such as improved process and services related to tax payments and basic services using machine learning or diffuse logic for decision making or logistic distribution, are significant. This research aims to understand the perceptions of public managers regarding the impact, functions, and barriers of AI in the context of a local government. A survey was conducted among 32 key public managers from the government of the State of Mexico in the central region to assess their perceptions of AI. The findings indicate that there is widespread concern among public administrators regarding high costs, suggesting the critical need to address financial issues to ensure sustainable implementation of AI. In terms of barriers, the results underscore the urgent necessity of addressing fundamental issues such as connectivity, financial resources, and technological capacity to enable effective integration of AI. This study is relevant as it identifies the key aspects of impact, functions, and barriers for the implementation of AI in a local government."
The use and theoretical support of emerging technologies for citizen participation in cities. A Systematic Literature Review in DGRL,"Rodr\'{\i}guez Bol\'{\i}var, Manuel Pedro and Alcaide Mu\~{n}oz, Laura and Morales Mar\'{\i}n, Miguel",10.1145/3657054.3657255,2024,"This paper examines the use and theoretical support underlying the implementation of emerging technologies for citizen participation in cities. Based on a systematic literature review in the main database of digital government (Digital Government Research Library -DGRL-), we analyze the evidence drawn by prior research concerning the situation, theoretical support, and use of these disruptive technologies in citizen participation. We seek to contribute to prior research on arising a critical debate about the initial implementation of these technologies for promoting collaborative models of governance and providing future research directions to advance&nbsp;in&nbsp;this&nbsp;topic."
The Berlin Declaration monitoring mechanism as an effective tool to monitor EU Member States' digital transformation: The Berlin Declaration monitoring mechanism: monitoring digital transformation in Europe,"Oliveira, Claudia and Custers, No\'{e}mie and Zonta, Tommaso and Miscen\`{a}, Emilia",10.1145/3657054.3657259,2024,"Since its deployment in 2021, the Berlin Declaration monitoring mechanism (BDM) has proven to be an effective tool to monitor the digital transformation of the 27 Member States of the European Union (EU). Particularly, by putting forward seven Policy Areas that tackle different aspects of the digital sphere, the BDM has supported Member States in implementing the objectives set out in the Declaration, while fostering transparency and enabling countries to learn from one another. Overall, the results of the monitoring mechanism between 2021 and 2022 vary depending on the Policy Area under consideration. Therefore, after providing an overview of the BDM methodology, this paper aims to investigate two Policy Areas that have advanced at different paces over the same period of time, namely Policy Area 3 – Foster digital empowerment and digital literacy and Policy Area 4 – Strengthen trust through security in the digital sphere, to better understand how the Berlin Declaration and its related monitoring mechanism have helped Member States to foster the digital transformation of their governments and public services. To do so, good practices from different European countries, as well as initiatives advanced by the European Commission, will be examined. In its conclusion, this paper will point out some limitations related to the BDM itself and set forth possible future developments beyond the mere application of the Declaration, taking into account the priorities of the Belgian Presidency of the Council of the European Union, which will run from January to June 2024."
Knowledge Management Diagnosis in Software Organizations: A Systematic Literature Review Extension,"Souza, Wesley and Silva, Williamson and Guedes, Gilleanes Thorwald Araujo",10.1145/3658271.3658273,2024,"Context: Effective Knowledge Management (KM) is vital for tech companies where developers frequently switch, risking the loss of valuable experience. Diagnosing its current state, assessing practices, and understanding applied methodologies in organizations is essential to succeed in KM. Problem: Given the dynamism witnessed in KM, a Systematic Literature Review (SLR) is necessary, and we found an outdated SLR on this topic until 2018. Objective: We extended the SLR to identify the current state-of-the-art KM within organizations and research niches to accurately depict the current management practices. Method: We extended the 2018 SLR, incorporating two additional databases to update this review and allow us to select new studies produced after the original review. IS Theory: This work was conceived under the auspices of the theory of organizational knowledge creation, aiming to understand the influence of KM on the development of information systems (IS) today. Summary of Results: We evidenced 29 studies on KM Diagnosis in software development organizations. Conclusion: Results show significant changes in applying learning to organizational culture through KM. Internal techniques, not tools or literature models, are now prevalent in km development. Agile methodology is a central focus in KM studies. The same holds for software architectures, with minimal application of KM in their selection and development. Contributions and Impact in the IS area: We believe in identifying KM strategies that facilitate the formation of new teams, transmitting knowledge and experiences, developing new products, and maintaining existing ISs."
Dealing with the diversity of interoperability types,"Santos, K\'{e}cia Souza Santana and Valle, Pedro Henrique Dias and Maciel, Rita Suzana Pitangueira",10.1145/3658271.3658298,2024,"Context: Interoperability is a crucial non-functional requirement for current information systems since systems can collaborate, exchange information, and use the information that has been shared. Interoperability has many types, such as semantic, legal, cultural, and organizational. Several interoperability types arose to characterize distinct information systems collaboration heterogeneity and barriers. Problem: However, various interoperability types do not have a common shared definition, leading to ambiguity and hampering developers from having a comprehensive view of the interoperability requirements they hope to achieve. Solution: We propose a conceptual model to describe and classify interoperability types reported in the scientific literature. This model aims to support developers in identifying interoperability types required in a given collaboration among information systems. IS Theory: We followed the design theory regarding the artifacts that contribute to the construction of IS. Method: The conceptual model was specified based on evidence collected from the scientific literature. It comprises two diagrams that describe and classify the interoperability types. We conducted a exploratory study with 33 information systems developers to validate our proposed conceptual model. Summary of Results: The conceptual model achieved 87.9% acceptance for the classification diagram, while the description diagram had 78.8% acceptance. Contributions and Impact in the IS area: This research work may help to comprehend how developers understand and use interoperability requirement specifications that require a particular solution to be fulfilled in an information system. In addition, this work also aims to contribute to the Full Interoperability and Systems-of-Systems Interoperability (SOSI) challenges proposed in I GrandSI-BR: 2016-2026. KEYWORDS: Interoperability; Conceptual Model"
A Case and Cluster-Based Framework for Reuse and Prioritization in Software Testing,"Silva, Luis Alvaro de Lima and Machado, Lori R. F. and Emmendorfer, Leonardo",10.1145/3658271.3658312,2024,"Context: Machine Learning (ML) based reuse and prioritization software testing techniques allow test analysts to generate and select test artifacts so that cases with a higher priority are selected and executed earlier than those with a lower priority. Problem: Even though test cases are broadly used in developing regression tests, the problem is that these software artifacts are still underused when structuring reusable software testing experiences. Solution: This work presents a Case-Based Reasoning and Clustering framework in which augmented test case representations maintain the data and knowledge from concrete instances of testing problem-solving. Then, similarity-based query answering is explored in selecting and prioritizing test cases for given testing problems. Query results’ clustering is also developed, permitting the examination of the possible cluster structures in the formed test suites and the consequent use of the identified clusters in the run-time re-prioritization of the test case executions. IS theory: Within the General Systems Theory, we investigate ML techniques with recognized explanatory capabilities to resolve software testing problems. Methods: Experiments in a real-world software project were developed. The overall goal was to assess the case and cluster methods’ effectiveness in retesting the implemented functionalities of a new target system version. Summary of results: Results with the presented techniques show improved fault detection rates positively contributing to performing regression tests in software projects. Contributions and impact in the IS area: The C2Test framework allows test analysts to better decide which items should be retested in each new version of a target system."
A Proposal of a Knowledge Graph for Digital Engineering Systems Integration for Operation and Maintenance Activities in Industrial Plants,"Molina De Armas, Elvismary and Hamazaki Da Silva, Geiza Maria and Torres Izquierdo, Yenier and Lemos, Melissa and De Lima Britto, Paulo Vin\'{\i}cius and Corseuil, Eduardo Thadeu and Souza Garcia, Robinson Luiz",10.1145/3658271.3658339,2024,"Context: Over the last years, we have observed Knowledge Graphs (KGs) being used more and more as a tool for representing knowledge, data integration and querying data. Problem: There are many distinguished yet partially-integrated information management systems used to support the life-cycle of Oil and Gas industrial plants. Our approach considers a 3D plants viewer system, a visual navigation system on platforms, and the integrated intelligent search system. However, these systems lack a semantic integration that can guide the user actions over each functionality for a unique asset. Solution: This paper presents the use of KGs to represent and help monitoring and controlling operational and maintenance activities within an Oil and Gas industrial environment. Our approach highlights the challenges and initial work required to establish a fully-integrated management domain, where the execution of the aforementioned activities can easily be managed. SI Theory: This study draws inspiration from Representation Theory, which posits that an information system faithfully mirrors specific phenomena occurring in the physical world. Method: To develop this work, it was necessary to review the literature related to the development of KGs and ontologies. The generated KG was developed using well-established standards like the Industrial Data Ontology (IDO), and the Capital Facilities Information Handover Specification (CFIHOS), complemented with the use of other ontologies. Summary of Results: A prototype of the conceptual KG was implemented, verifying the viability of our approach for data integration. Contributions and Impact in IS area: The resulted graph contains the main terms in compliance with international semantic standards for representing operational and maintenance activities data associated with facilities involved in Oil and Gas production. Finally, the KG resulting from this effort can be further extended through the incorporation of new tools and subdomains in the industrial plants life-cycle."
Generating and Reviewing Programming Codes with Large Language Models: A Systematic Mapping Study,"Albuquerque, Beatriz Ventorini Lins de and Cunha, Antonio Fernando Souza da and Souza, Leonardo and Siqueira, Sean Wolfgand Matsui and Santos, Rodrigo Pereira dos",10.1145/3658271.3658342,2024,"Context: The proliferation of technologies based on Large Language Models (LLM) is reshaping various domains, also impacting on programming code creation and review. Problem: The decision-making process in adopting LLM in software development demands an understanding of associated challenges and diverse application possibilities. Solution: This study addresses the identified challenges linked to LLM utilization in programming code processes. It explores models, utilization strategies, challenges, and coping mechanisms, focusing on the perspectives of researchers in software development. IS Theory: Drawing on Task-Technology Fit (TTF) theory, the research examines the alignment between task characteristics in code generation and review, and LLM technology attributes to discern performance impacts and utilization patterns. Method: Employing the Systematic Mapping of the Literature method, the research analyzes 19 selected studies from digital databases—IEEE Digital Library, Compendex Engineering Village, and Scopus—out of 1,257 retrieved results. Summary of Results: The research reveals 23 models, 13 utilization strategies, 15 challenges, and 14 coping mechanisms associated with LLM in programming code processes, offering a comprehensive understanding of the application landscape. Contributions to IS: Contributing to the Information Systems (IS) field, This study provides valuable insights into the utilization of LLM in programming code generation and review. The identified models, strategies, challenges, and coping mechanisms offer practical guidance for decision-making processes related to LLM technology adoption. The research aims to support the IS community in effectively navigating the complexities of integrating large language models into the dynamic software development lifecycle."
"Themes, Lenses, and Materials: Three Perspectives on HCI Program Development","Gray, Colin M. and Toombs, Austin L.",10.1145/3658619.3658622,2024,"As an inter-discipline or trans-discipline, HCI includes or references many different sources of knowledge in which students are expected to be conversant. The education of HCI practitioners requires exposure to an increasingly large number of these perspectives. However, how should this exposure be structured, with what level of depth, and through what metaphors? In this unsolved challenge, we outline the complex range of perspectives required and the limitations of typical curriculum and program design techniques. We then illustrate how HCI educators might use three different perspectives to consider and communicate program complexity to students: 1) content themes; 2) transdisciplinary lenses; and 3) design materials. We conclude with opportunities for HCI educators to leverage these insights to build courses, projects, and other program structures."
VisionGuard: Secure and Robust Visual Perception of Autonomous Vehicles in Practice,"Han, Xingshuo and Wang, Haozhao and Zhao, Kangqiao and Deng, Gelei and Xu, Yuan and Liu, Hangcheng and Qiu, Han and Zhang, Tianwei",10.1145/3658644.3670296,2024,"Modern Autonomous Vehicles (AVs) implement the Visual Perception Module (VPM) to perceive their surroundings. This VPM adopts various Deep Neural Network (DNN) models to process the data collected from cameras and LiDAR. Prior studies have shown that these models are vulnerable to physical adversarial examples (PAEs), which pose a critical safety risk to the autonomous driving task. While a few defense methods have been proposed to safeguard AVs, most of them only target a limited set of attack types and specific scenarios, making them impractical for real-world protection.In this paper, we introduce VisionGuard, a novel and practical methodology to comprehensively detect and mitigate various types of PAEs to the VPM. The key of VisionGuard is to leverage the spatiotemporal inconsistency property of PAEs to detect anomalies. It predicts the motion states from historical ones and compares them with the current driving states to identify any motion inconsistency caused by physical attacks. We evaluate 9 state-of-the-art PAEs against both camera and camera-LiDAR fusion-based object classification &amp; detection models. Experimental results in both simulation and physical world validate the effectiveness and robustness of VisionGuard. Codes, demo videos and appendix can be found on our anonymous website: https://sites.google.com/view/visionguard."
Skipping the Security Side Quests: A Qualitative Study on Security Practices and Challenges in Game Development,"Klostermeyer, Philip and Klivan, Sabrina and H\""{o}ltervennhoff, Sandra and Krause, Alexander and Busch, Niklas and Fahl, Sascha",10.1145/3658644.3690190,2024,"The video game market is one of the biggest for software products. Video game development has progressed in the last decades to complex and multifaceted endeavors. Games-as-a-Service significantly impacted distribution and gameplay, requiring providers and developers to consider factors beyond game functionality, including security and privacy. New security challenges emerged, including authentication, payment security, and user data or asset protection. However, the security community lacks in-depth insights into the security experiences, challenges, and practices of modern video game development. This paper aims to address this gap in research and highlights the criticality of considering security in the process.Therefore, we conducted 20 qualitative, semi-structured interviews with various roles of professional and skilled video game development experts, investigating awareness, priorities, knowledge, and practices regarding security in the industry through their first-hand experiences. We find that stakeholders are aware of the urgency of security and related issues. However, they often face obstacles, including a lack of money, time, and knowledge, which force them to put security issues lower in priority. We conclude our work by recommending how the game industry can incorporate security into its development processes while balancing other resources and priorities and illustrating ideas for future research."
"No Peer, no Cry: Network Application Fuzzing via Fault Injection","Bars, Nils and Schloegel, Moritz and Schiller, Nico and Bernhard, Lukas and Holz, Thorsten",10.1145/3658644.3690274,2024,"Network-facing applications are commonly exposed to all kinds of attacks, especially when connected to the internet. As a result, web servers like Nginx or client applications such as curl make every effort to secure and harden their code to rule out memory safety violations. One would expect this to include regular fuzz testing, as fuzzing has proven to be one of the most successful approaches to uncovering bugs in software. Yet, surprisingly little research has focused on fuzzing network applications. When studying the underlying reasons, we find that the interactive nature of communication, its statefulness, and the protection of exchanged messages (e.g., via encryption or cryptographic signatures) render typical fuzzers ineffective. Attempts to replay recorded messages or modify them on the fly only work for specific targets and often lead to early termination of communication.In this paper, we discuss these challenges in detail, highlighting how the focus of existing work on protocol state space promises little relief. We propose a fundamentally different approach that relies on fault injection rather than modifying messages. Effectively, we force one of the communication peers into a weird state where its output no longer matches the expectations of the target peer, potentially uncovering bugs. Importantly, this weird peer can still properly encrypt/sign the protocol message, overcoming a fundamental challenge of current fuzzers. In effect, we leave the communication system intact but introduce small corruptions. Since we can turn either the server or the client into the weird peer, our approach is the first that can effectively test client-side network applications. In an extensive evaluation of 16 targets, we show that our prototype Fuzztruction-Net significantly outperforms other fuzzers in terms of coverage and bugs found. Overall, Fuzztruction-Net uncovered 23 new bugs in well-tested software, such as the web servers Nginx and Apache HTTPd and the OpenSSH client."
Rust for Embedded Systems: Current State and Open Problems,"Sharma, Ayushi and Sharma, Shashank and Tanksalkar, Sai Ritvik and Torres-Arias, Santiago and Machiry, Aravind",10.1145/3658644.3690275,2024,"Embedded software is used in safety-critical systems such as medical devices and autonomous vehicles, where software defects, including security vulnerabilities, have severe consequences. Most embedded codebases are developed in unsafe languages, specifically C/C++, and are riddled with memory safety vulnerabilities. To prevent such vulnerabilities, Rust, a performant memory-safe systems language, provides an optimal choice for developing embedded software. Rust interoperability enables developing Rust applications on top of existing C codebases. Despite this, even the most resourceful organizations continue to develop embedded software in C/C++.  This paper performs the first systematic study to holistically understand the current state and challenges of using Rust for embedded systems. Our study is organized across three research questions. We collected a dataset of 6,408 Rust embedded software spanning various categories and 6 Static Application Security Testing (SAST) tools. We performed a systematic analysis of our dataset and surveys with 225 developers to investigate our research questions. We found that existing Rust software support is inadequate, SAST tools cannot handle certain features of Rust embedded software, resulting in failures, and the prevalence of advanced types in existing Rust software makes it challenging to engineer interoperable code. In addition, we found various challenges faced by developers in using Rust for embedded systems development."
Using AI Assistants in Software Development: A Qualitative Study on Security Practices and Concerns,"Klemmer, Jan H. and Horstmann, Stefan Albert and Patnaik, Nikhil and Ludden, Cordelia and Burton, Cordell and Powers, Carson and Massacci, Fabio and Rahman, Akond and Votipka, Daniel and Lipford, Heather Richter and Rashid, Awais and Naiakshina, Alena and Fahl, Sascha",10.1145/3658644.3690283,2024,"Following the recent release of AI assistants, such as OpenAI's ChatGPT and GitHub Copilot, the software industry quickly utilized these tools for software development tasks, e.g., generating code or consulting AI for advice. While recent research has demonstrated that AI-generated code can contain security issues, how software professionals balance AI assistant usage and security remains unclear. This paper investigates how software professionals use AI assistants in secure software development, what security implications and considerations arise, and what impact they foresee on security in software development. We conducted 27 semi-structured interviews with software professionals, including software engineers, team leads, and security testers. We also reviewed 190 relevant Reddit posts and comments to gain insights into the current discourse surrounding AI assistants for software development. Our analysis of the interviews and Reddit posts finds that, despite many security and quality concerns, participants widely use AI assistants for security-critical tasks, e.g., code generation, threat modeling, and vulnerability detection. Participants' overall mistrust leads to checking AI suggestions in similar ways to human code. However, they expect improvements and, therefore, a heavier use of AI for security tasks in the future. We conclude with recommendations for software professionals to critically check AI suggestions, for AI creators to improve suggestion security and capabilities for ethical security tasks, and for academic researchers to consider general-purpose AI in software development."
Applying ChatGPT to improve the user experience in digital libraries,"Zhang, Meisai and Zhao, Ming",10.1145/3659211.3659219,2024,"In the current information age where artificial intelligence, computer network technology and multimedia interconnections are rapidly developing, people have learned to access a wide range of information resources through the Internet and enjoy the convenience brought by the Internet+ era. With the introduction of ChatGPT, a new-generation artificial intelligence large language model, new development opportunities have been brought to the library community in China. In this study, we firstly start from the technology as well as the features of ChatGPT, and give a brief introduction to the basic concepts and characteristics of ChatGPT. Then, we delve into the application value of how ChatGPT can improve library services and carry out specific applications from four aspects in detail. Finally, we provide an overview based on the challenges encountered and the strategies to deal with them, and describe the main future trends of digital libraries."
Cross-regional Power Transmission and Transformation Engineering Field Digital Transformation Management Measures,"Dong, Yuezhou and Xie, Fangyi and Gan, Yunliang and Peng, Yupei and Wang, Hongjie and Li, Pan",10.1145/3659211.3659256,2024,"The management of cross-regional Power transmission and transformation projects requires the support of project digitization and on-site intelligent control capabilities to meet the demands of many widespread project points and high process control requirements. This paper proposes a ""digitalization and intelligent site"" management model, which realizes intelligent control over the progress, safety, quality, personnel, channel clearing, environmental protection of the project construction process. The results show that using a “point-line-surface-body” management model can effectively improve the control level of information and on-site control capabilities and risk prevention capabilities of management personnel in large transmission and transformation projects."
The Green Development of E-commerce Express Delivery by Applying System Dynamics,"Huang, Kuanwei and Wang, Xiaodan",10.1145/3659211.3659292,2024,"The rapid rise of e-commerce express delivery is closely intertwined with the essential support of logistics, further propelling the logistic industry's growth. Enterprises, typically driven by economic motives, have embraced an expansive economic development model, intensifying environmental pressures. These challenges encompass excessive resource exploitation, wasteful practices, heightened energy consumption, and substantial environmental pollution. In response, logistics companies have redirected their focus towards environmental concerns, implementing diverse strategies to preserve the environment. With the proposal of the concept of green logistics, many logistics enterprises focus on environmental benefits and protect the environment. This study applies the system dynamics method, constructing a simulation model including two subsystems that influences economic benefits and green image in e-commerce express delivery. Rigorous testing with extensive data collection allows for real-world scenario simulations. The results underscore the significance of the recycling demand, surpassing energy conservation and environmental protection investments. We suggest that for sustainable green logistics development in e-commerce express delivery, efforts should prioritize enhancing recycling demand efficiency alongside related resources investments."
Chaotic time series prediction algorithm of long-term sales volume based on machine learning,"Liu, Hongya and Noorliza, Karia and Song, Meijing and Lu, Yajing",10.1145/3659211.3659328,2024,"Long-term sales volume prediction is crucial for the development of any industry, if blindly increase the scale of production, it is easy to affect the development of the industry, resulting in a waste of resources, for this reason, this paper proposes a chaotic time series prediction algorithm for long-term sales volume based on machine learning. Analyse the factors affecting long-term sales volume and obtain chaotic time series data under this influence factor. Plan the data warehouse according to the obtained results, and extract the features of long-term sales volume chaotic time series data by using the calculation results of crossover and variation. Using the support vector machine algorithm in machine learning, the basic task of big data classification is determined, and the decision tree integration model is applied to quickly predict the long-term sales volume and realize the long-term sales volume chaotic time series prediction. The experimental results show that the correction bias of the proposed algorithm is within a reasonable range, and it can achieve the overall improvement of long-term sales volume chaotic time series prediction, the best performance of the ROC curve, better prediction performance, and higher prediction efficiency."
Research and Exploration on the Teaching Mode and Key Technologies of Comprehensive Project Practical Courses Based on Cloud Classroom,"Zeng, Wenying",10.1145/3660043.3660044,2024,"In response to meet the needs of multiple majors sources students, a wide range of professional skills, high demand for innovative design, and high difficulty in teaching implementation in the comprehensive project practical course teaching, a project practical teaching mode design and key technology research based on cloud classroom are explored and carried out. This article discusses the key technologies of cloud based classroom platforms, including the integration of cloud computing, big data, artificial intelligence, etc; Implementation strategies and approaches for project practical course teaching, including course content, teaching methods, evaluation system, etc; Cross disciplinary knowledge fusion technology involves the integration and integration of knowledge and skills from multiple disciplines; Optimize and enhance paths, etc. Research has shown that conducting comprehensive project practical course teaching based on cloud based classrooms has advantages such as resource integration, dynamic discussion, technical integration, and flexible management; At the same time, technologies such as cloud computing virtualization, data analysis, and artificial intelligence can be comprehensively utilized to reform project teaching methods and enhance students' comprehensive project design ability and professional literacy."
Reform and evaluation of comprehensive practice course of mechatronic engineering specialty based on multi-objective probability statistical model,"Jiang, Jingang and Tan, Yujian and Tang, Dedong and Bao, Yudong and Dai, Ye and Wang, Yiwen",10.1145/3660043.3660056,2024,"Aiming at the problems existing in the teaching of comprehensive practice courses in mechatronic engineering, guided by the results of the professional certification of engineering education, relying on the provincial higher education teaching reform project, and based on the concept of Outcome-Based Education (OBE) engineering education, systematically carry out the teaching reform of comprehensive practice courses in mechatronic engineering. Through the close connection between teaching projects and engineering applications and disciplinary competitions, this paper has constructed an OBE teaching mode of ""comprehensive practice"" courses oriented to the cultivation of students' comprehensive ability, with a multi-level and diversified teaching evaluation system, to cultivate students' ability to comprehensively utilize their knowledge, their engineering practice ability, and their sense of innovation to meet the new requirements of ""New Engineering"" and ""Made in China 2025"" for the electromechanical engineering majors and to promote the quality of talent cultivation of Mechatronics Engineering majors."
Blended Learning Design for Java Web Service Application Development Project Practice Course Based on OBE Concept,"Zhong, Liming and You, Zhimin",10.1145/3660043.3660061,2024,"Java web services application development project practice course applies a blended learning model based on the OBE concept for teaching design. This model is oriented towards learning outcomes, student-centered, and uses blended learning as a teaching method, which can effectively improve students' ability to apply Java web technology for practical project development. In the teaching process, the teaching objectives and unit teaching objectives of the course are designed according to the training needs of software engineering professionals, and then the evaluation criteria of the achievement degree of the course objectives are designed. Then, the corresponding teaching content is selected, and the blended teaching method is applied to implement the course teaching. Finally, the students' learning achievements are evaluated to continuously improve teachers' teaching methods and students' learning methods. Through teaching practice, this mode can effectively improve the teaching quality."
Breaststroke Training in Adolescents: A Scientometric Analysis,"Mo, Xingsheng and Shi, Zhan and Zou, Quanyu and Yang, Zhengzheng",10.1145/3660043.3660192,2024,"Objective: To methodically assess research trends in adolescent breaststroke swimming to inform future training directions.Methods: A scientometrics analysis using citespace6.2.R6 software examined literature from 2013 to 2023.Results: The quantity of literature on adolescent breaststroke training has increased. Notable English contributors include Veiga and Gonio, while Chinese literature lacks central authors. Research concentrates on land-based training, pivotal training methods, pre-competition preparation, post-intensity recovery, and starts and turns. Key terms are adolescents, physical training, swimming, performance, and competition analysis. Recent hot topics in Chinese research include physical and load training methods, while ""Elite athlete"" trends in English literature.Conclusion: Adolescent breaststroke training requires customization over standardization. Tailored plans should reflect individual physical and psychological traits, technique style, skill level, coaching, and regional factors for optimal results."
Construction of English Big Data Corpus under Artificial Intelligence Translation,"Li, Wenfang",10.1145/3660043.3660197,2024,"The construction of English big data corpora plays an important role in the field of artificial intelligence, providing key support for improving the performance of machine translation systems and training language models. This article explores the key issues and challenges of artificial intelligence in building English big data corpora, and proposes corresponding solutions. Firstly, we discussed the construction pattern of big data corpora. In view of this, the construction of big data corpora under artificial intelligence translation can be carried out from four aspects: sharing mode based on third-party open-source data, crowdsourcing translation, machine closed-loop learning mode, and human-machine collaboration mode. The future translation teaching can also rely on the construction of corpora as an important part of translation intelligence. By comprehensively considering these aspects, this article aims to provide comprehensive guidance and methods for the construction of English big data corpora, and promote the continuous development of artificial intelligence translation technology."
Target sequences model based on the results of the corelation of demographic variables with other variables: application of the sequential patternmining algorithm,"Liang, Xiangdong and Ham, Hyun-Jin",10.1145/3660395.3660474,2024,"Sequential pattern mining (SPM) is an important technique of pattern mining, which has many applications in reality. Although many efficient sequential pattern mining algorithms have been proposed, there are few studies can focus on target sequences.. This study describes an AI task-specific model application solution method with the results of the influence of demographic variables on other variables for a certain group to construct an intelligent system with improved functions. After investigating the results of the influence of demographic variables on Chinese language learning anxiety and learning engagement, the results are improved from four aspects, namely, sequence pattern mining, knowledge graph, virtual digital human and intelligent emotion monitoring system."
"""Looks Good To Me ;-)"": Assessing Sentiment Analysis Tools for Pull Request Discussions","Coutinho, Daniel and Cito, Luisa and Lima, Maria Vit\'{o}ria and Arantes, Beatriz and Alves Pereira, Juliana and Arriel, Johny and Godinho, Jo\~{a}o and Martins, Vinicius and Lib\'{o}rio, Paulo V\'{\i}tor C. F. and Leite, Leonardo and Garcia, Alessandro and Assun\c{c}\~{a}o, Wesley K. G. and Steinmacher, Igor and Baffa, Augusto and Fonseca, Baldoino",10.1145/3661167.3661189,2024,"Modern software development relies on cloud-based collaborative platforms (e.g., GitHub and GitLab). In these platforms, developers often employ a pull-based development approach, proposing changes via pull requests and engaging in communication via asynchronous message exchanges. Since communication is key for software development, studies have linked different types of sentiments embedded in the communication to their effects on software projects, such as bug-inducing commits or the non-acceptance of pull requests. In this context, sentiment analysis tools are paramount to detect the sentiment of developers’ messages and prevent potentially harmful impact. Unfortunately, existing state-of-the-art tools vary in terms of the nature of their data collection and labeling processes. Yet, there is no comprehensive study comparing the performance and generalizability of existing tools utilizing a dataset that was designed and systematically curated to this end, and in this specific context. Therefore, in this study, we design a methodology to assess the effectiveness of existing sentiment analysis tools in the context of pull request discussions. For that, we created a dataset that contains ≈ 1.8K manually labeled messages from 36 software projects. The messages were labeled by 19 experts (neuroscientists and software engineers), using a novel and systematic manual classification process designed to reduce subjectivity. By applying these existing tools to the dataset, we observed that while some tools ]perform acceptably, their performance is far from ideal, especially when classifying negative messages. This is interesting since negative sentiment is often related to a critical or unfavorable opinion. We also observed that some messages have characteristics that can make them harder to classify, causing disagreements between the experts and possible misclassifications by the tools, requiring more attention from researchers. Our contributions include valuable resources to pave the way to develop robust and mature sentiment analysis tools that capture/anticipate potential problems during software development."
LLM-Based Chatbots for Mining Software Repositories: Challenges and Opportunities,"Abedu, Samuel and Abdellatif, Ahmad and Shihab, Emad",10.1145/3661167.3661218,2024,"Software repositories have a plethora of information about software development, encompassing details such as code contributions, bug reports and code reviews. This rich source of data can be harnessed to enhance not only software quality and development velocity but also to gain insights into team collaboration and inform strategic decision-making throughout the software development lifecycle. Previous studies show that many stakeholders cannot benefit from the project information due to the technical knowledge and expertise required to extract the project data. To lower the barrier to entry by automating the process of extracting and analyzing repository data, we explored the potential of using an LLM to develop a chatbot for answering questions related to software repositories. We evaluated the chatbot on 150 software repository-related questions. We found that the chatbot correctly answered one question. This result prompted us to shift our focus to investigate the challenges in adopting LLMs for the out-of-the-box development of software repository chatbots. We identified five main challenges related to retrieving data, structuring the data, and generating the answer to the user’s query. Among these challenges, the most frequent (83.3%) is the inaccurate retrieval of data to answer questions. In this paper, we share our experience and challenges in developing an LLM-based chatbot to answer software repository-related questions within the SE community. We also provide recommendations on mitigating these challenges. Our findings will serve as a foundation to drive future research aimed at enhancing LLMs for adoption in extracting useful information from software repositories, fostering advancements in natural language understanding, data retrieval, and response generation within the context of software repository-related questions and analytics."
The development of the smart factory during the Industry 4.0 period under the study of system thinking,"Zhou, Zhenquan and Wang, Lanxin and Tian, Chenxin and Syamsunur, Deprizon",10.1145/3661638.3661681,2024,"Following introducing the Industrial Revolution 4.0 framework in 2013, many industrialised nations have implemented this new industrial paradigm. The smart factory is a fundamental concept imperative for successfully implementing the Fourth Industrial Revolution. Many scholars and specialists contend that the ongoing advancement of smart factories will transform the industrial structure. This study aims to examine the status of smart factories, exploring the development and management of the smart factory during the Industry 4.0 period under the study of system thinking. This work extensively explores the distinctive notion and presents individual anecdotes regarding the development, execution, arrangement, and administration of intelligent factories by individuals. The research employs multiple methodologies, such as system thinking of relevant literature and systematic analysis of the proposed complex project. The findings suggest that incorporating intricate system thinking, appropriate technology integration, efficient simulation and machine learning employment, and a responsive monitoring and feedback mechanism are crucial in establishing and managing intelligent manufacturing facilities. External factors and governing constraints need to be considered during the lifecycle of treating complexity. This report offers a thorough and analytical evaluation of developing the smart factory during the Fourth Industrial Revolution, serving as a scholarly resource for further exploration of related subject areas. A further investigation plans to consider deep data collection."
Design of an Integrated System for Government Invested Construction Projects,"Wang, Shuwei and Song, Sigen",10.1145/3661638.3661686,2024,"Due to the rapid development of information system, applying information system to engineering costs is currently an important issue. Because government investment construction projects have inherent characteristics and receive more attention than general projects, they have higher requirements for project management. Whether it is project application, review, or later project construction and completion acceptance, national investment projects require more efficient and scientific management. So this article attempts to design an integrated system for this type of project, providing reference for the healthy development of this type of project."
SPringBoard:AI-powered Ideation System for Technopreneurship,"Rosales, Bob Kyle Labajo and Munar, Katherin Claire Bodomo and Tulod, Charlette Vibar and Rama, Jurydel Gabunada and Laviste, Ralph Pepe",10.1145/3661904.3661918,2024,"This research introduces “SPringBoard”, a web-based application developed at the Wildcat Innovation Labs (WIL) of Cebu Institute of Technology – University. The application is designed to advance Technopreneurship among students and assist mentors in guiding them. SPringBoard employs AI, specifically integrating ChatGPT-4, to validate and monitor innovative ideas. It provides a systematic, phased progression in innovation, focusing on three lenses: customer-driven desirability, practical feasibility, and thorough viability assessment. The research aims to address the limitations of traditional idea validation methods by leveraging the capabilities of ChatGPT-4. The application provides comprehensive reports that pinpoint the strengths and weaknesses of proposed ideas, coupled with tailored recommendations for enhancement. This demonstrated its utility in aiding both students in refining their ideas and mentors in providing effective guidance. Specific improvements observed in the students’ ideas and the mentors’ guidance are discussed. The expected outcomes of this research include specific insights from the ChatGPT model in assessing the three lenses of innovation. These insights will contribute to the field of Technopreneurship by providing valuable information about the model's performance and potential as a tool for idea validation. Overall, this study represents a significant stride in linking academic theories with practical market applications. It provides a robust framework for the evaluation and mentorship of technopreneurial projects."
AI and the Future of Collaborative Work: Group Ideation with an LLM in a Virtual Canvas,"He, Jessica and Houde, Stephanie and Gonzalez, Gabriel E. and Silva Moran, Dar\'{\i}o Andr\'{e}s and Ross, Steven I. and Muller, Michael and Weisz, Justin D.",10.1145/3663384.3663398,2024,"The introduction of generative AI into multi-user applications raises novel considerations for the future of collaborative work. How might collaborative work practices change? How might we incorporate generative AI into shared tools with users’ needs at the forefront? We examine these questions in the context of a remote team conducting ideation tasks – an example of collaborative work enabled by a shared digital workspace. We conducted a user study with 17 professionals experienced with virtual group ideation workshops. Our study examined their use of the Collaborative Canvas, a virtual canvas tool with integrated generative AI capabilities that we created as a probe. Participants saw value in using generative AI to assist with group facilitation and to augment perspectives and ideas. However, they worried about losing human perspectives and critical thinking, as well as reputational harms resulting from harmful AI outputs. Participants shared suggestions for appropriate ways to incorporate generative AI capabilities within multi-user applications and identified needs for transparency of content ownership, private digital spaces, and specialized AI capabilities. Based on participants’ insights, we share implications and opportunities for the incorporation of generative AI into collaborative work in ways that place user needs at the forefront."
"Unpacking Task Management Tools, Values, and Worker Dynamics","Hu, Donghan and Bhuiyan, Md Momen and Lim, Sol and Wiese, Jason and Lee, Sang Won",10.1145/3663384.3663402,2024,"As the complexity of daily tasks grows, knowledge workers experience challenges in managing tasks and risk skipping over some. Fortunately, various task management tools have become available, ranging from traditional tools, such as sticky notes, to complex project management software. In this exploratory study, we aim to understand the landscape of task management tools that knowledge workers use and identify the value they seek from such tools. In addition, we investigate how such value relates to workers’ personality traits and job characteristics. For this purpose, we conducted a series of formative studies and an online survey (N = 248) to evaluate the perceived importance of various attributes of task-management tools, followed by an exploratory factor analysis to identify the latent structure within that. This process revealed six underlying dimensions for task management tools: communicability, structure, portability, adaptability, physicality, and visualizability. Applying regression analysis, we found connections between latent dimensions and both personality traits and job characteristics. Our findings inform the design of future task management tools with guidance on choosing features and functionality that will meet the needs of their target populations."
Bridging the Gap Between Time Management Research and Task Management App Design: A Study on the Integration of Planning Fallacy Mitigation Strategies,"Ahmetoglu, Yoana and Brumby, Duncan and Cox, Anna",10.1145/3663384.3663404,2024,"Accurate time estimations are vital for meeting deadlines and reducing work-related stress, yet individuals frequently succumb to a wide-spread cognitive bias, the planning fallacy, resulting in poor time management. This research article reports on two studies aimed at addressing this challenge. First, through a review of the psychological literature, we identify four key strategies recommended by research for supporting accurate time estimations in daily tasks. These strategies serve as the foundation for the second study, where we conduct a functionality analysis of prevalent personal task management apps to investigate their alignment with the identified strategies. Our analysis reveals a significant disparity: while research-informed strategies are recommended, they are rarely implemented to a good standard in current apps. This discrepancy emphasizes the importance of addressing this gap between theory and practice. By highlighting the need for future efforts to focus on aiding workers in task duration estimation, this study identifies opportunities for improving the design of task management software to enhance user productivity and alleviate stress."
Costs and Benefits of Machine Learning Software Defect Prediction: Industrial Case Study,"Stradowski, Szymon and Madeyski, Lech",10.1145/3663529.3663831,2024,"Context: Our research is set in the industrial context of Nokia 5G and the introduction of Machine Learning Software Defect Prediction (ML SDP) to the existing quality assurance process within the company. Objective: We aim to support or undermine the profitability of the proposed ML SDP solution designed to complement the system-level black-box testing at Nokia, as cost-effectiveness is the main success criterion for further feasibility studies leading to a potential commercial introduction. Method: To evaluate the expected cost-effectiveness, we utilize one of the available cost models for software defect prediction formulated by previous studies on the subject. Second, we calculate the standard Return on Investment (ROI) and Benefit-Cost Ratio (BCR) financial ratios to demonstrate the profitability of the developed approach based on real-world, business-driven examples. Third, we build an MS Excel-based tool to automate the evaluation of similar scenarios that other researchers and practitioners can use. Results: We considered different periods of operation and varying efficiency of predictions, depending on which of the two proposed scenarios were selected (lightweight or advanced). Performed ROI and BCR calculations have shown that the implemented ML SDP can have a positive monetary impact and be cost-effective in both scenarios. Conclusions: The cost of adopting new technology is rarely analyzed and discussed in the existing scientific literature, while it is vital for many software companies worldwide. Accordingly, we bridge emerging technology (machine learning software defect prediction) with a software engineering domain (5G system-level testing) and business considerations (cost efficiency) in an industrial environment of one of the leaders in 5G wireless technology."
Towards Realistic SATD Identification through Machine Learning Models: Ongoing Research and Preliminary Results,"Gama, Eliakim and Paixao, Matheus and Cort\'{e}s, Mariela I. and Monteiro, Lucas",10.1145/3663529.3663876,2024,"Automated identification of self-admitted technical debt (SATD) has been crucial for advancements in managing such debt. However, state-of-the-arts studies often overlook chronological factors, leading to experiments that do not faithfully replicate the conditions developers face in their daily routines. This study initiates a chronological analysis of SATD identification through machine learning models, emphasizing the significance of temporal factors in automated SATD detection. The research is in its preliminary phase, divided into two stages: evaluating model performance trained on historical data and tested in prospective contexts, and examining model generalization across various projects. Preliminary results reveal that the chronological factor can positively or negatively influence model performance and that some models are not sufficiently general when trained and tested on different projects."
Vulnerability detection tool in source code by building and leveraging semantic code graph.,"Delaitre, Sabine and Pulgar Guti\'{e}rrez, Jos\'{e} Maria",10.1145/3664476.3670942,2024,"The paper presents different vulnerability detection tools to ensure the security of software applications and container environments from a preventive and holistic approach. The solution aims to improve the quality and security of software by leveraging knowledge graph technology for a more accurate and comprehensive detection process of vulnerabilities. The ambition is to detect the vulnerabilities in the whole software supply chain and to support developers holding security as a key component over the Software Development life-cycle. We design reliable tools by building a semantic graph-based abstraction of the code from the compiler state, and we reach high accuracy by developing different static code analyzers optimizing the detection of software vulnerabilities in the source code and dependencies. In this paper, we will introduce the cybersecurity suite composed of different vulnerability detection tools to promote developer autonomy and security automation over the software supply chain. The main tool (DocSpot) detects vulnerabilities in the application source code and leverages knowledge graph technology. The second tool (DocDocker) scans for vulnerabilities in containers, and the third one (SirDocker) detects orchestration vulnerabilities, e.g. related to configuration, recommends secure best practices, and supports secure management of the containers and container images. The main contributions to the field of security automation are detailed and the first experiments and results of the tools are exposed. Finally, we describe the contributions to improving security in software and IoT applications."
"Exploring the Digital Landscape in Higher Education: Investigating the Role of Technology, Techno-invasion, Techno-overload and Tools in Student Well-being","Sok\'{o}\l{}, Igor Piotr and Ko\c{c}, Hasan",10.1145/3664934.3664936,2024,"This quantitative study investigates technostress's influence on student well-being in universities. Surveys and statistical analyses explore the complexities of technostress, techno-invasion, techno-overload, and tools on well-being and academic performance. Findings reveal a significant negative correlation between techno-invasion and well-being, especially among business students. While technostress does not predict academic performance, differentiated perceptions highlight nuanced dynamics. The study contributes insights for balancing technology benefits and challenges in higher education, fostering a conducive learning environment, and providing essential support to manage technology-related challenges."
Curriculum Reform of Information Management System Under the Background of Big Data,"Du, Zhiguo and Hu, Dahui",10.1145/3664934.3664939,2024,"ABSTRACT: This study underscores the necessity of instituting a scientific personnel cultivation program to enhance the quality of training for proficient professionals. In the backdrop of the current information revolution, the research delves into societal requisites and challenges arising from shifts in information technology. The paper advocates for realigning training objectives in information management system, thereby reconstructing the professional curriculum. The research initiates by refocusing talent development on micro facets of information system planning, design, and implementation. This approach aims to nurture individuals capable of harnessing big data for astute processing and in-depth analysis, enabling adept facilitation of complex decision-making and contributing to the data knowledge service sector. Through a comparative analysis of similar programs nationally and internationally, the study addresses concern pertinent to the era of big data, including inadequacies in teaching organization, insufficient support systems, incongruous curriculum structures, deficient practical training, and inappropriate pedagogical strategies. This culminates in a holistic curriculum system rooted in data science theory, experimental methodologies, and practical field research. Consequently, an innovative talent development model for information management system is formulated, equipping students to effectively meet evolving societal demands and ensuring the sustainable advancement of the discipline."
A Comprehensive Evaluation of Standard Data Warehousing and Data Mining Techniques in the Field of Business,"Samonte, Mary Jane C. and Angeles, Alexandra A. and Gallinera, Jeriell Emmanuel P. and Oregas, Bianca Eileen C.",10.1145/3664968.3664997,2024,"In many industries, including business, healthcare, finance, and more, decision-making no longer occurs separately from data warehousing and data mining. This meta-analysis analyzes the standard data warehousing and mining techniques to determine how well they perform and how widely they may be applied. This paper conducts a comprehensive evaluation of the effects of these methods by analyzing the findings of 30 studies from various fields, such as business intelligence, healthcare, finance, and education. The results shed light on the strengths and weaknesses of different approaches, providing important clues for further study and use. In particular, the study highlights the ethical problems accompanying the increasing adoption of data mining in healthcare and its expanding role in the industry. The investigation sheds light on the complicated interconnections between data mining methods, their practical uses, and their ability to solve challenging real-world problems. Recommendations for future studies stress the significance of working with experts from other fields, being open and honest about how the models are being used, being scalable, and having an eye toward actual, measurable results. In conclusion, this meta-analysis demonstrates the lasting value of data warehousing and mining methods in influencing well-informed decision-making, stimulating creativity, and accelerating social development."
A Balanced Approach to Project Scheduling with Project Crashing for Time-Sensitive Projects,"Uy, Jeremy Gabriel and San Juan, Jarvy Larz and Magno, Darlyn Jasmin",10.1145/3664968.3665012,2024,"Living in a world with limited resources, it is important to know how to efficiently allocate resources effectively and efficiently to successfully execute a project. While studies have shown various ways to approach the resource-constrained project scheduling problem, limited to no studies have incorporated project crashing in approaching the problem. In this study, a general mixed integer multi-objective nonlinear programming model is developed to minimize costs, environmental impact, and delays that can be used by project managers to generate project schedules for a project. Operations research techniques, particularly an algorithm that balances the optimization of the three conflicting objectives, were used for simultaneous optimization. A hypothetical case study is solved to demonstrate the capabilities and highlight the novelty of the proposed model. Finally, two scenario analyses were performed to understand the behaviors of the system under varying conditions. The results of this study show the capabilities of the model in simultaneously considering economic, environmental, and social considerations, especially for time-sensitive projects."
Breaking old Habits: On Success Factors in Software Process Improvement,"Vasylieva, Kseniia and K\""{u}pper, Steffen and Kuhrmann, Marco",10.1145/3666015.3666022,2024,"Over the years, a substantial body of knowledge of software process improvement (SPI) was accumulated that, among other things, includes numerous success factors that companies should consider when conducting improvement activities. The number of success factors is large and quite often, multiple success factors with similar names and descriptions are available to address a specific phenomenon. This raises the question whether all the success factors are unique and, if not, which ones are actually the same. In this paper, we aim to structure the body of knowledge on success factors in SPI. We conducted a systematic literature review on 103 publications that mention 1.320 success factors. A multi-staged manual and AI-supported analysis reduced the number of success factors to 124, which we categorize into 42 general success factor classes. For 20 of these general success factor classes, we observed a stable number of publications over a period of almost 30 years that, however, show only few success factors constantly studied and re-discovered. A high number of synonyms shows that this area of SPI needs consolidation for which we lay the foundation by providing a big picture and identifying the most relevant success factors as a starting point."
Ph.D. Forum: Intelligent Home Energy Management: Developing AI-Driven Systems for Sustainable Living,"Sheng, Yu",10.1145/3666025.3699665,2024,"The MAI-HOME project, funded by the Interreg initiative, addresses energy poverty and CO2 emission reduction through an AI-driven framework tailored for vulnerable populations. This research spans three years of data collection from multiple sensors installed in every room of sixteen houses across the Netherlands and Belgium. It aims to predict and promote energy-saving behaviors effectively. Utilizing an innovative blend of digital twins and robust data privacy measures, this project explores four critical areas: real-time data collection, predictive AI model development, data privacy enhancement, and behavioral intervention strategies. Initial findings suggest promising avenues for technological advancements and societal benefits in sustainable energy practices."
Application of System Engineering in Aircraft System Design,"Wang, Hao and Tang, Yu and Lv, Guangliang and Yu, Yonggang and Lan, Qingsheng and Zhou, Zhu",10.1145/3669721.3674517,2024,"The development of modern aircraft is an extremely complex system engineering. From the global cooperation of commercial aircraft to the performance requirements of advanced military aircraft, the development process, methods and tools are required to be closely and efficiently integrated. In order to promote the comprehensive application of System Engineering in China's aircraft field, a series of System Engineering standards and manuals are studied. Those standards and manuals in the field of military and civil aircraft are all published or applied by INCOSE, the Federal Aviation Administration of the United States and so forth. Taking commercial aircraft System Engineering as an example, the process, method and application cases of aircraft System Engineering at home and abroad are mainly studied. The results show that the industrial departments represented by COMAC has much System Engineering practice at the system level and subsystem level of aircraft. At the same time, domestic aircraft research and development departments are suggested to make more efforts on System Engineering standards, specifications, education and infrastructure."
A Literature Analysis on the Development of an Augmented Reality Application for Firefighting Training Seminars and Workshop,"Cagampan, Benedict R. and Delfin, Ivan Zacharia L. and Tolentino, Joey Bernard D. and Samonte, Mary Jane C. and Villaluz, Alberto C.",10.1145/3669754.3669783,2024,"The advancement of augmented reality (AR), gives a comprehensive and accurate training environment that revolutionizes firefighting courses. There are various realistic fire simulations. With markerless AR, trainees can experience realistic simulations of fire scenarios in a controlled environment, allowing them to gain situational awareness and improve their decision-making skills in a safe and effective manner. The researchers explore through using Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA). A methodological approach to examining literature systematically. Selects, obtains, and assesses the quality of the study. The growth of augmented reality continues to rise which helps the future researchers to continually improve augmented reality. This includes in different types of devices used HoloLens, Mobile Augmented Reality (MAR), and Head-Mount Displays (HMD). Augmented reality technology has the potential to revolutionize education and training. By overlaying digital information on the real world, learners can gain a deeper understanding of complex concepts. In addition, situational awareness training for disasters can be greatly enhanced through the use of augmented reality."
Utilizing Technology to Detect Bullying in The Workplace: A Systematic Review,"Wulandari, Putu Diana and Pramartha, Cokorda and Bakta, I Made and Wulanyani, Ni Made Swasti",10.1145/3669754.3669818,2024,"Bullying in the workplace has become a significant challenge for global community. Employees subjected to harassment are prone to reduced productivity as a consequence of heightened stress and unstable mental well-being. The banking sector, recognized as a pivotal component of the economic infrastructure, stands out as an industry that demands high-performance standards. It is crucial to ensure the mental well-being of banking employees. The researchers systematically conducted a literature search employing the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) framework to identify diverse methodologies for detecting workplace bullying. The results indicate that numerous methods have been experimented with for detecting workplace bullying through the utilization of technology. However, there is a noticeable dearth of focus on the banking sector. Based on the reviewed research, it is concluded that effective bullying detection necessitates a careful consideration of the nature of behavior, differentiating between offline and online manifestations. Additionally, emphasis on language nuances and the delineation of behavioral patterns in accordance with cultural and social contexts is deemed imperative."
Development of a Web-based Research Consortium Database Management System: Advancing Data-driven and Knowledge-based Project Management,"Salvador, Mitch Arkeen and Botangen, Khavee Agustus and Rabang, Mary Camille and Salinas, Ivan Christian and Naagas, Marlon and Balagot, Angelika",10.1145/3670105.3670120,2024,"The Central Luzon Agriculture, Aquatic and Natural Resources Research and Development Consortium (CLAARRDEC), comprising 29 member institutions, faces challenges in effectively monitoring and evaluating their R&amp;D activities. To address these challenges, they seek to harness digital technology for data management and real-time monitoring. This paper presents the development of a web-based database and real-time monitoring system aimed at enhancing data collection, storage, retrieval, and utilization within the consortium. The system consists of two key components: i) a data management module, designed to facilitate project data collection from member institutions, and ii) a real-time monitoring module for report generation and analytics at the CLAARRDEC main office. Successful deployment of the system not only fosters information sharing, collaboration, and informed decision-making but also empowers member institutions to monitor their own R&amp;D engagements. Furthermore, the system's potential extends beyond CLAARRDEC, as it could be utilized by other research consortia in the Philippines."
CLUE: Customizing clustering techniques using machine learning for software modularization,"Meng, Fanyi and Wang, Ying and Chong, Chun Yong and Yu, Hai and Zhu, Zhiliang",10.1145/3671016.3674816,2024,"Software clustering is often used as a remodularization and architecture recovery technique to help developers simplify software maintenance tasks and ease the burden of software comprehension. While the choice of clustering technique can significantly influence the outcomes of remodularization, it is noteworthy that existing works have yet to conduct an exhaustive exploration of the suitability of various clustering techniques for different software projects. Although many prior works introduce new clustering techniques, their validations often focus on specific domains, which may restrict the generalizability of their findings. In this paper, we conduct an empirical study aimed at understanding the impact of software features and architectural problems on the effectiveness of various software clustering techniques. Leveraging our empirical findings, we propose an approach, CLUE, which leverages Machine Learning (ML) models to customize a suitable software clustering technique for a given software. Our approach focuses on eight types of software clustering techniques and offers insights into their suitability based on features and architectural problems of software. This comprehensive analysis helps developers to select the suitable clustering technique that can achieve the best MoJoFM, c2ccvg, or TurboMQ value from the chosen pool of software clustering techniques for specific software. We evaluate CLUE by analyzing 100 open-source software projects. The experiment results demonstrate that CLUE achieves highly accurate clustering technique customization, with an accuracy exceeding 90%."
Accelerating XR Innovation through a pan-European Lab Network: An overview of the EMIL project,"Bl\""{o}nnigen, Justus and Clarke, Christopher and Dahn, Andreas and Forelli, Lisa and Gowrishankar, Ramyah and Heikura, Tuija and Helzle, Volker and Hine, Paul and Jicol, Crescent and Kreische, Alexander and Lutteroth, Christof and Mac\'{\i}a, Francisco and Moesgen, Tim and Pares, Narcis and Plichta, Leszek and Potts, Dominic and Sch\""{a}fer, Eduard and Sharma, Adwait and Spielmann, Simon and Tenhunen, Juhani and Trottnow, Jonas and Tseng, Yu-Han and Vikberg, Esa and Xiao, Yu",10.1145/3672406.3672426,2024,"European Media and Immersion Lab, or EMIL, is a pan-European network of extended reality (XR) labs consisting of 4 European academic institutions, with a mission to accelerate development of virtual, augmented and mixed reality technologies, content, services and applications. The 30-month project, which started in September 2022, has been funded by the European Union and co-funded by Innovate UK. This paper gives an overview of the project’s goals, its organization, and selected results that have been achieved."
JobSet: Synthetic Job Advertisements Dataset for Labour Market Intelligence,"Colombo, Samuele and D'Amico, Simone and Malandri, Lorenzo and Mercorio, Fabio and Seveso, Andrea",10.1145/3672608.3707718,2025,"The use of online services for advertising job positions has grown in the last decade, thanks to the ability of Online Job Advertisements (OJAs) to observe the labour market in near real-time, predict new occupation trends, identify relevant skills, and support policy and decision-making activities. Unsurprisingly, 2023 was declared the Year of Skills by the EU, as skill mismatch is a key challenge for European economies. In such a scenario, machine learning-based approaches have played a key role in classifying job ads and extracting skills according to well-established taxonomies. However, the effectiveness of ML depends on access to annotated job advertisement datasets, which are often limited and require time-consuming manual annotation. The lack of OJA annotated benchmarks representative of the real online OJA and skills distributions is currently limiting advances in skill intelligence. To deal with this, we propose JobGen, which leverages Large Language Models (LLMs) to generate synthetic OJAs. We use real OJAs collected from an EU project and the ESCO taxonomy to represent job market distributions accurately. JobGen enhances data diversity and semantic alignment, addressing common issues in synthetic data generation. The resulting dataset, JobSet, provides a valuable resource for tasks like skill extraction and job matching and is openly available to the community1."
AI-Powered Comment Triage for Efficient Collaboration and Feedback Management,"Pasam, Vamsi Krishna and Pati, Sravani and Toxtli Hernandez, Carlos",10.1145/3672608.3707835,2025,"In today's digital landscape, collaborative tools are critical for virtual teamwork, with comments as a key mechanism for communication and feedback. Our project, within the Natural Language Processing (NLP) domain, focuses on improving comment handling in collaborative environments using advanced machine learning methods. We developed a triage system that categorizes and prioritizes comments to help us efficiently address the most critical feedback. Building on previous work, we employed transformer models like BERT and RoBERTa, which showed strong performance in classifying comments when fine-tuned on our dataset. To enhance the handling of hierarchical structures, we experimented with Hierarchical Capsule Networks (HcapsNet) and Hierarchical Attention Networks (HAN). Additionally, GEMMA-2B, a large language model, demonstrated strong results in F1-score and precision while providing zero-shot and few-shot learning capabilities. The framework, tested in domains such as project management, academic collaboration, and document review, classifies and prioritizes comments based on six dimensions: urgency, importance, sentiment, actionability, resolution status, and thematic relevance.It incorporates rule-based logic alongside pre-trained NLP models, including GEMMA-2B for intent classification, Hugging Face models for sentiment analysis, and Latent Dirichlet Allocation (LDA) for topic modeling. This approach supports the efficient management of comments by prioritizing those that require immediate attention and improving the collaborative process."
"Measuring (meta)emotion, (meta)motivation, and (meta)cognition using digital trace data: A systematic review of K-12 self-regulated learning","Toomla, Kaja and Weng, Xiaojing and Kikas, Eve and Malleus-Kot\v{s}egarov, Elina and Aus, Kati and Azevedo, Roger and Hooshyar, Danial",10.1145/3672608.3707961,2025,"Artificial intelligence (AI) has demonstrated significant potential in enhancing digital learning by offering personalized and adaptive experiences that meet learners' individual needs. However, while self-regulated learning (SRL) skills are critical for success in digital environments, AI-driven learner models mainly focus on cognitive processes, with limited integration of SRL skills. This systematic review synthesizes research from 1990 to 2024, analyzing digital trace data from various learning platforms to identify which data serve as indicators of the three phases and areas of SRL in K-12 digital learning. Our findings highlight digital trace data that measure (meta)emotion, (meta)motivation, and (meta)cognition across the three SRL phases, while also revealing significant gaps in tracing (meta)motivation and (meta)emotion, particularly in the preparatory and appraisal phases. Although a variety of data traces address the (meta)cognitive area, the results underscore the challenges of meaningfully interpreting the learning process. Despite these challenges, the evolving research on trace data demonstrates substantial potential for integrating adaptive SRL scaffolding into digital learning environments."
A Genetic Algorithm with Convex Combination Crossover for Software Team Formation: Integrating Technical and Collaboration Skills,"Cunha, Felipe and Perkusich, Mirko and Albuquerque, Danyllo and Gorg\^{o}nio, Kyller and Almeida, Hyggo and Perkusich, Angelo",10.1145/3672608.3707980,2025,"In industrial settings, the Software Team Formation Problem (STFP) is often hindered by inconsistency, bias, and overlooked collaboration history in manual team formation processes. These limitations create inefficiencies in aligning technical and collaborative skills. Genetic Algorithms (GAs) have been introduced to address this, but traditional crossover methods restrict diversity and can lead to premature convergence, forming suboptimal teams. This paper introduces a GA designed to integrate both technical expertise and collaborative performance. Utilizing a graph-based fitness function and the novel Convex Combination Crossover technique, the algorithm explores team configurations more effectively than traditional methods. We validated our approach using data from 47 software projects and 149 developers across 12 simulation scenarios. The results show that the collaboration graph reliably captures team dynamics. At the same time, the Convex Combination Crossover outperforms traditional methods (Partially Mapped Crossover and One-Point Crossover) in generating more diverse and fit team structures. This validation, conducted over five rounds, demonstrated the robustness and consistency of the proposed approach. Our findings suggest that combining technical and collaborative factors leads to better decision-making in team formation, providing a more effective and scalable solution for industrial software projects."
Research on the Construction and Management Monitoring of Big Data Platform Based on Artificial Intelligence,"Zheng, Yan",10.1145/3672919.3672965,2024,"The introduction of big data technology can bring significant changes to engineering project management, greatly improving the efficiency of various aspects of project management, providing reference for scientific decision-making, and thus achieving the improvement of project benefits. In the analysis of engineering construction, the significance of monitoring in engineering construction involves the application of many new technologies, while analysis and prediction involve many interdisciplinary methods and theories. This article conducts research on the construction and management monitoring of big data platforms based on AI (Artificial Intelligence). This article applies DL (Deep Learning) to the research of this article. The research results show that the average relative error rates of interval 1 and interval 39 are 18.3%, respectively. It can be concluded that the average relative error of the quality distribution of each particle size grading in engineering filling stone mining based on AI is ≤ 25%. It can effectively optimize the blasting parameters of sand and stone mining, control the particle size grading of stone materials, and meet the mining grading requirements. It provides technical reference for the design of filling stone mining in embankment engineering and ensures the quality of embankment engineering. Through the construction and management monitoring of this platform, it is beneficial to reduce resource waste during the construction process and reduce construction costs for enterprises."
The Use of Deep Learning in the Diagnosis and Prediction of Heart Failure: A scoping review,"Alsaify, Abdel Rahman and Siam, Aisha and Hassan, Hudhaifa and Alzubaidi, Mahmood and Househ, Mowafa",10.1145/3673971.3673973,2024,"This scoping review presents a comprehensive analysis of the current implementation of deep learning techniques in heart failure diagnosis and prediction. We investigated the use of various deep learning models, focusing on their application in analyzing medical images and electronic health records. A thorough search across four electronic databases yielded 503 prospective studies, with 17 meeting our inclusion criteria. These studies predominantly originated from the United States and China and were primarily journal articles. Our review identified two main categories of deep learning models: those processing medical images and those analyzing clinical parameters from electronic health records. The most commonly used models were recurrent neural networks (RNN) for prediction and convolutional neural networks (CNN) and natural language processing (NLP) for diagnosis. The studies demonstrated a wide range of imaging modalities, with electrocardiograms being the most prevalent. Additionally, the review highlighted a variety of clinical parameters used for prediction and diagnosis, emphasizing the significance of artificial intelligence in medical research. Despite the promise shown by these models, challenges such as inconsistent performance, lack of detailed methodology, and limited geographical diversity in study sources were identified. Our findings underscore the potential of deep learning in enhancing heart failure diagnosis and prediction, but also point towards the need for more rigorous and diversified research to fully realize this technology's capabilities in healthcare."
Research on Cross domain Mutual Trust Technology of Power Communication Network for Power System Informatization Construction,"Bin, Dongmei and Xie, Ming and Yang, Chunyan and Liang, Yongjian and Li, Xin",10.1145/3674225.3674269,2024,"Due to the lack of a strategic plan for the transmission network, the lack of a specific plan for the private wireless network, and the lack of a complete system of information communication plans, the article establishes the entire structure of telecommunications. The network that defines the data supply, the creation of a power communication network planning platform for data supply, and the completion of the entire closed-loop management process of the research communication network are “ready; assets; building; follow-up assessment; development and improvement "". First, the entire architecture of the power communication network was created in consideration of data delivery. Accordingly, the sub-platform design of the power communication network planning system, the design of the sub-platform of the wireless private network planning system. , all sub-platforms of the information communication management business The design of the management system has been completed. The communication system platform plan is an important part of improving the state's power grid management level and improving the quality of electricity. Improve the development of the network, efficiency and results."
Digital Grid Investment Risk Prediction and Identification under PSO-FAHP Based Intelligent Algorithm,"Zhang, Yuhong and Du, Ying and Zhou, Ying and Liu, Hanjing and Zhou, Ping and Jiao, Jie",10.1145/3674225.3674342,2024,"Combined with the actual situation of power grid enterprise investment, this paper analyzes the main steps of investment risk management of power grid enterprise, and carries out several researches from risk identification, risk prediction, risk assessment and early warning. First of all, data mining technology was used to analyze the event from the perspective of the whole system, to find out each risk factor affecting the occurrence of the event and classify it, and to construct an index system. Improved gray correlation analysis was used to analyze the influencing factors of power grid investment risk, obtain the indicator evaluation system, analyze the role of the relationship between the influencing factors, and provide ideas for investment decision-making. Secondly, a new grid investment risk prediction method is proposed, combining qualitative and quantitative methods to comprehensively assess the risk of grid investment. The indicators are divided into qualitative and quantitative indicators, and the investment risk prediction library is constructed through a technical method with strong objectivity. A combination model of intelligent algorithms based on particle swarm, support vector machine, and neural network is established to predict the indicators such as economic rationality, and the prediction accuracy of the model is verified, so as to prepare for the subsequent assessment of risks. This report sets three levels of warning for indicators and five levels of warning for projects and enterprises; sets warning intervals and determines the risk values of different risk indicators. Finally, the use of data integration technology, through the cluster intelligence technology of intelligent algorithms summarized to form a database, and then effective identification, prediction, assessment and early warning of power grid investment risk. And relying on the sound organization, clear functions, clear division of powers and responsibilities, flexible and efficient grid investment risk management organization, relying on intelligent management and control technology, clarifying the powers and responsibilities of each grid investment risk management function by means of legalization, realizing the efficient and synergistic operation between each function when the grid investment risk occurs, and establishing the risk management organization framework for the project and the power grid enterprise."
Extending Scratch Framework to Improve the Analytical Skills of Undergraduate Students,"Akbar, Ubaid Ul and Akbar, Saeed and Ali, Mumtaz and Ullah, Rahmat and Khan, Rizwan and Lopes, Ivandro Ortet",10.1145/3674558.3674561,2024,"Scratch is an innovative and the most popular block-based Visual Programming Language designed for beginners to learn programming effectively. However, it lacks the capacity to allow users to learn and solve larger real-world problems such as the Travelling Salesman Problem (TSP). Hence, there is a need for an accessible and effective tool to assist beginners and novice programmers in learning and tackling the TSP. This paper introduces ScratchTSP, an extension of the Scratch programming Framework, specifically designed for the TSP. It offers an opportunity for beginners and novice programmers to grasp the TSP and the various algorithms used to solve it. For evaluation, we conduct a survey among undergraduate students to assess the usability and usefulness of the proposed ScratchTSP extension. Survey findings indicate that ScratchTSP offers a user-friendly and effective tool for beginners or novice programmers, aiding them in learning the TSP while developing their critical thinking and problem-solving skills."
ChatGPT application in Systematic Literature Reviews in Software Engineering: an evaluation of its accuracy to support the selection activity,"Felizardo, Katia Romero and Lima, M\'{a}rcia Sampaio and Deizepe, Anderson and Conte, Tayana Uch\^{o}a and Steinmacher, Igor",10.1145/3674805.3686666,2024,"Context: The Systematic Literature Review (SLR) process involves searching, selecting, and synthesizing relevant literature on a specific research topic for evidence-based decision-making in Software Engineering (SE). Due to the time-consuming of the SLR process, tool support is essential. Gap: ChatGPT is a significant advancement in Natural Language Processing (NLP), and it can potentially accelerate time-consuming and propone-error activities, such as the selection activity of the SLR process. Therefore, having a tool to assist in the selection process appears beneficial, and we argue that ChatGPT can facilitate the analysis of extensive studies, saving time and effort. Objective: We aim to evaluate the accuracy (i.e., studies correctly classified) of using ChatGPT–4.0 in SLR in SE, particularly to support the first stage, based on the title, abstract, and keywords. Method: We assessed the accuracy of utilizing ChatGPT for selecting studies, the first stage, to be included in two SLRs (SLR1 and SLR2), in contrast to the conventional method of reading the title and abstract. Results: The accuracy of ChatGPT supporting the initial selection activity was 75.3% (SLR1 – 101 correct selections: 48 inclusions and 53 exclusions; 33 incorrect selections: 17 inclusions and 16 exclusions) and 86.1% (SLR2 – 386 correct selections: 113 inclusions and 273 exclusions; 62 incorrect selections: 27 inclusions and 35 exclusions). Conclusions: Our accuracy results indicate that it is not advisable to completely outsource the selection process to ChatGPT. However, it could be valuable as a support tool, aiding novice researchers or even experienced ones when they are in doubt."
An Exploratory Study on Soft Skills present in Software Positions in Cyprus: a quasi-Replication Study,"Kapitsaki, Georgia and Chatzivasili, Loukas and Papoutsoglou, Maria and Galster, Matthias",10.1145/3674805.3686681,2024,"Background: Soft skills, such as the ability to communicate effectively and efficiently or work in a team, are important for software engineering practitioners. Understanding which soft skills are necessary for software professionals can assist in staff recruitment, training, and curriculum development. Various previous works have explored soft skills in job adverts in different contexts (i.e. based on what employers ask for). Aims: In this work, we rely on a study performed in New Zealand, and conduct a similar analysis in Cyprus to explore soft skills for the software industry in a different country in a different continent. Method: A manual analysis of 689 job adverts has been used to analyze job adverts from 2023 and 2024. Qualitative analysis and descriptive statistics were mainly employed for analysis purposes. Results: We have found 36 soft skills, whereas between 2023 and 2024 there are slight differences in the number of skills present per job advert. We also encountered differences in soft skills presence based on the job position category and the company size. Conclusion: The results confirm the existing findings concerning the most in demand soft skills, showing that the software industry needs are global. Nevertheless, there are differences in the soft skills popularity. The results can be used for improving job adverts concerning listing soft skills and performing curricula updates."
Threats to Validity in Software Engineering – hypocritical paper section or essential analysis?,"Lago, Patricia and Runeson, Per and Song, Qunying and Verdecchia, Roberto",10.1145/3674805.3686691,2024,"Background: In recent years, a discourse on how to systematically consider and report threats to validity started to gain momentum within the empirical software engineering community. Aims: With this study, we aim to systematically underpin the current state of threats to validity practices in software engineering research. Method: We conduct a literature review comprising 91 papers awarded with the ACM SIGSOFT Distinguished Paper Award at the ACM/IEEE International Conference on Software Engineering. Data is extracted and analyzed by considering six main facets of threats to validity, e.g., their explicit documentation, categorization, discussion of limitations, and trade-offs. Results: Results corroborate current critiques to the threats management state of the art. Threats result to be seldom discussed in depth, and are mostly considered as an enforced afterthought rather than an active concern of the research design and execution. Conclusions: To improve the observed practice, we derived items to consider for researchers, reviewers and readers, and call for a community action to increase the understanding of knowledge creation in empirical software engineering research."
An Exploratory Mixed-methods Study on General Data Protection Regulation (GDPR) Compliance in Open-Source Software,"Franke, Lucas and Liang, Huayu and Farzanehpour, Sahar and Brantly, Aaron and Davis, James C. and Brown, Chris",10.1145/3674805.3686692,2024,"Background: Governments worldwide are considering data privacy regulations. These laws, such as the European Union’s General Data Protection Regulation (GDPR), require software developers to meet privacy-related requirements when interacting with users’ data. Prior research describes the impact of such laws on software development, but only for commercial software. Although open-source software is commonly integrated into regulated software, and thus must be engineered or adapted for compliance, we do not know how such laws impact open-source software development. Aims: To understand how data privacy laws affect open-source software (OSS) development, we focus on the European Union’s GDPR, as it is the most prominent such law. We investigated how GDPR compliance activities influence OSS developer activity (RQ1), how OSS developers perceive fulfilling GDPR requirements (RQ2), the most challenging GDPR requirements to implement (RQ3), and how OSS developers assess GDPR compliance (RQ4). Method: We distributed an online survey to explore perceptions of GDPR implementations from open-source developers (N=56). To augment this analysis, we further conducted a repository mining study to analyze development metrics on pull requests (N=31,462) submitted to open-source GitHub repositories. Results: Our results suggest GDPR policies complicate OSS development and introduce challenges, primarily regarding the management of users’ data, implementation costs and time, and assessments of compliance. Moreover, we observed negative perceptions of the GDPR from OSS developers and significant increases in development activity, in particular metrics related to coding and reviewing, on GitHub pull requests related to GDPR compliance. Conclusions: Our findings provide future research directions and implications for improving data privacy policies, motivating the need for relevant resources and automated tools to support data privacy regulation implementation and compliance efforts in OSS."
Effective Inclusion of People with Disabilities in Software Development Teams,"da Rocha, Thayssa A and de Souza, Cleidson and Teran, Luciano and Mota, Marcelle",10.1145/3674805.3690749,2024,"Context: People with Disabilities (PWD) have seen the software development market as an alternative for overcoming some of the inclusion barriers they face in society. A career in software development presents itself as an opportunity for dignified and equal employment conditions. However, there is limited software engineering research about the PWD’s effective inclusion in software development teams. Objectives and Methods: This paper presents the emerging results from a systematic literature review on three Computer Science databases over the last decade (2013-2023). Our objective is to identify relevant technical and human aspects of PWD’s effective inclusion in software development teams. Results: From 1,006 initial papers, 572 were analyzed, and 39 were selected because they discuss PWD as members of software development teams. We examined them with quantitative and qualitative approaches. Our results indicate that including PWD as members of software development teams is a topic underexplored in software engineering research, with 11 papers out of the 39 discussing this topic. In addition, visual impairment is the most addressed disability, while programming is the most common role played by these professionals in software development teams. PWD’s challenges are usually technical and social, such as the accessibility of tools and mixed-ability teams’ work dynamics. We conclude by presenting the PWD’s workarounds to deal with these challenges, which are common and essential to guarantee minimally equal participation. Conclusion: Our contribution is to outline the research trajectory about including PWD as software developers over the past decade and steer new efforts towards their effective integration into software development teams."
Automatic Categorization of GitHub Actions with Transformers and Few-shot Learning,"Nguyen, Phuong T. and Di Rocco, Juri and Di Sipio, Claudio and Shakya, Mudita and Di Ruscio, Davide and Di Penta, Massimiliano",10.1145/3674805.3690752,2024,"In the GitHub ecosystem, workflows are used as an effective means to automate development tasks and to set up a Continuous Integration and Delivery (CI/CD pipeline). GitHub Actions (GHA) has been conceived to provide developers with a practical tool to create and maintain workflows, avoiding “reinventing the wheel” and cluttering the workflow with shell commands. Properly leveraging the power of GitHub Actions can facilitate the development processes, enhance collaboration, and significantly impact project outcomes. To expose actions to search engines, GitHub allows developers to assign them to one or more categories manually. These are used as an effective means to group actions sharing similar functionality. Nevertheless, while providing a practical way to execute workflows, many actions have unclear purposes, and sometimes they are not categorized. In this work, we bridge such a gap by conceptualizing Gavel, a practical solution to increasing the visibility of actions in GitHub. By leveraging the content of README.MD files for each action, we use Transformer to assign suitable categories to the action. We conducted an empirical investigation and compared Gavel with a state-of-the-art baseline. The results show that our approach can assign categories to GitHub actions effectively, thus outperforming the baseline."
MSR4SBOM: Mining Software Repositories for enhanced Software Bills of Materials,"Scanniello, Giuseppe and Di Penta, Massimiliano and Romano, Simone and Francese, Rita and Nocera, Sabato and Cassieri, Pietro and Bifolco, Daniele and Zampetti, Fiorella",10.1145/3674805.3695390,2024,"MSR4SBOM (Mining Software Repositories for enhanced Software Bills of Materials) is a project whose main goal is to deliver a framework that analyzes the content of software repositories and SBOMs to provide context-sensitive recommendations. The expected outputs are (i)&nbsp;a set of approaches and tools released as open-source projects, making them exploitable in industrial, academic, and open-source contexts; and (ii)&nbsp;replication packages of our empirical studies and repositories of datasets collected while developing, calibrating, and validating the MSR4SBOM approaches and tools."
FRINGE: context-aware FaiRness engineerING in complex software systEms,"Palomba, Fabio and Di Sorbo, Andrea and Di Ruscio, Davide and Ferrucci, Filomena and Catolino, Gemma and Giordano, Giammaria and Di Dario, Dario and Voria, Gianmario and Pentangelo, Viviana and Tortorella, Maria and Sgueglia, Arnaldo and Di Sipio, Claudio and D'Aloisio, Giordano and Di Marco, Antinisca",10.1145/3674805.3695394,2024,"Machine learning (ML) is essential in modern technology, driving complex data-driven decisions. By 2025, daily data generation will exceed 463 exabytes, increasing ML’s influence and ethical risks of data exploitation and discrimination. The European Union’s Artificial Intelligence Act highlights the need for ethical AI solutions. Project Fringe (context-aware FaiRness engineerING in complex software systEms) addresses software fairness in ML-intensive systems that collect data through interconnected devices. Fringe aims to provide software engineers, data scientists, and ML experts with methodologies and software engineering solutions to improve fairness in ML systems. The goals of the project include developing a metamodel for ML fairness, a fairness-aware monitoring infrastructure, contextual solutions for identifying fairness issues, and automated recommendation systems to design fairness properties throughout the software development lifecycle."
Preliminary Insights on Industry Practices for Addressing Fairness Debt,"de Souza Santos, Ronnie and de Lima, Luiz and Baldassarre, Maria Teresa and Sp\'{\i}nola, Rodrigo",10.1145/3674805.3695406,2024,"Context: This study explores how software professionals identify and address biases in AI systems within the software industry, focusing on practical knowledge and real-world applications. Goal: We focused on understanding the strategies employed by practitioners to manage bias and their implications for fairness debt. Method: We employed a qualitative research method, gathering insights from industry professionals through interviews and using thematic analysis to explore the collected data. Findings: Professionals identify biases through discrepancies in model outputs, demographic inconsistencies, and training data issues. They address these biases using strategies such as enhanced data management, model adjustments, crisis management, improving team diversity, and ethical analysis. Conclusion: Our paper presents initial evidence on addressing fairness debt and lays the groundwork for developing structured guidelines to manage fairness-related issues in AI systems. LAY ABSTRACT. This paper explores how software professionals tackle biases in AI systems. We discovered that they identify problems by checking if the AI’s outputs match real-world conditions, ensuring it performs well for different groups of people, and investigating biases in the training data. To address these issues, they use various strategies like improving the data quality, regularly updating the AI to adapt to new information, and involving a diverse range of people in the development process. Our findings provide a solid starting point for creating clear guidelines to manage these biases. These guidelines will help ensure that AI systems are not only technically accurate but also fair and equitable for everyone. This research is important for making sure that as AI technology advances, it benefits all users without reinforcing existing inequalities."
Microlearning-Inspired Approach for Teaching Business Modeling to Computer Engineering Students,"Popgeorgiev, Angel and Ibryamova, Elitsa and Ivanova, Galina",10.1145/3674912.3674921,2024,"The current generation of students expects education to be an interactive and engaging process that uses contemporary technologies. Teaching financial concepts for business modeling to graduate students with a computer engineering background is challenging. With the complex and technical nature of the subject matter, it is essential to find innovative and effective ways to engage these students to ensure their success. The course exemplifies complex financial concepts using authored software-implemented programming models. This approach involves dividing the theoretical part of learning into smaller segments of project-based learning. When combined with programming models, it seems promising to enhance the understanding and engagement of students in their education. This process can improve students’ learning outcomes and make learning more satisfying and effective. A study was conducted to evaluate the effectiveness of a proposed teaching method on two groups of students: those who received traditional classroom-based training and those who received training through a distance learning approach. The study involved a survey of the students to inquire about their opinions on various aspects of the course delivery, including the teaching methodology, the program models used for demonstrations, and the learners’ experience implementing the program models. Additionally, learning outcomes and final evaluations are analyzed to conclude the effectiveness of the proposed teaching approach."
"Effective Organizational Learning from Failure: Mechanism for Knowledge Accumulation &amp; Sharing, and Measurement Execution","Nagayoshi, Sanetake and Nakamura, Jun",10.1145/3675669.3675680,2024,"Organizational learning from failure is a process through which organizations learn from their past mistakes and failures to prevent similar failures in the future. This process involves four key elements: knowledge acquisition, information distribution, information interpretation, and organizational memory. However, sharing knowledge about failures within an organization can be challenging due to factors such as embarrassment, guilt, and fear of disadvantage. Intrinsic motivation and altruism can encourage employees to share their knowledge, while extrinsic motivators may not have a significant impact. Effective data withdrawal and cognition play a crucial role in organizational memory, and the persistence of organizational routines is essential for improving the performance of organizational learning from failure. This study aims to explore the process by which organizational learning from failure is effective by conducting factor analysis of quantitative data collected through questionnaires."
Research on the Quality Improvement Path of Online Open Courses Based on Learner review Text Mining,"Lin, Xiuyu and Cai, Xiaosang and Li, Runbo",10.1145/3675812.3675844,2024,"The learner review texts of online open courses are an effective material for analyzing the quality of online open courses. In order to extract valuable information from the review texts of the courses, 14 MOOC(Massive open online courses) courses were selected for this study and learners' reviews were analysed in terms of word frequency and semantic network. The results revealed that learners primarily evaluate courses based on three aspects: the main teacher, course content, and teaching method. reviews were classified into positive and negative categories based on learners' emotional tendencies. Using the BTM(Biterm Topic Model), the study identified topics from positive and negative reviews, learners' positive and negative reviews about the course focused on topics such as course content, learning experience, course assessment, learning resources, platform services and teaching methods. Through these topics, we identified the strengths and weaknesses of current online open courses and proposed a path to improve their quality."
A Predictive Framework for Failure Detection and Reallocation in Crowdsourcing Platforms,"Voleti, Sai Anushree and Tathipamula, Harini Sai and Poolla, Lakshmi Saranya and Nair, Lekshmi S",10.1145/3675888.3676114,2024,"Crowdsourcing platforms have become essential tools for managing various tasks, relying on distributed labour practices to enhance productivity and goal achievement. However, they often encounter challenges such as unemployment and low-quality task submissions, impeding project effectiveness. Timely resolution of these issues is crucial for project momentum. To address these challenges, we propose a predictive model based on machine learning techniques. Our framework aims to anticipate mistakes and optimize workload reallocation within community spaces. Leveraging state-of-the-art algorithms, our system analyzes employee behaviours, task features, and platform dynamics to detect problematic situations early. Moreover, we introduce a dynamic reallocation technique to expedite project duration while minimizing delays. This method automatically reassigns tasks to alternative tasks. The experimentation results exhibit that our framework has shown that it is better than other methods at accurately detecting failures and reassigning tasks. Furthermore, our framework’s adaptability enables its application across various crowd-sourcing fields, including data annotation and content moderation. Described as a proactive response to failures within crowd-sourcing platforms, our predictive framework offers a promising solution to enhance project efficiency."
A comparison on constrain encoding methods for quantum approximate optimization algorithm,"Liu, Yiwen and Jiao, Qingyue and Shi, Yiyu and Wan, Ke and Guo, Shangjie",10.1145/3676536.3697126,2025,"The Quantum Approximate Optimization Algorithm (QAOA) represents a significant opportunity for practical quantum computing applications, particularly in the era before error correction is fully realized. This algorithm is especially relevant for addressing constraint satisfaction problems (CSPs), which are critical in various fields such as supply chain management, energy distribution, and financial modeling. In our study, we conduct a numerical comparison of three different strategies for incorporating linear constraints into QAOA: transforming them into an unconstrained format, introducing penalty dephasing, and utilizing the quantum Zeno effect. We assess the efficiency and effectiveness of these methods using the knapsack problem as a case study. Our findings provide insights into the potential applicability of different encoding methods for various use cases."
Decoding the Privacy Policies of Assistive Technologies,"Crawford, Kirk and Khoo, Yi Xuan and Kumar, Asha and Mentis, Helena and Hamidi, Foad",10.1145/3677846.3677850,2024,"As assistive technologies (ATs) have evolved, they have become increasingly connected. However, these increasing connections pose significant privacy challenges, especially when user privacy is described using complex privacy policies. Our study decodes the privacy policies of 18 ATs to understand how data collection and processing are communicated with users. We find that (1) AT privacy policies are structured to offer legal protections to their companies and not always to protect user privacy, (2) AT privacy policies are absent protections for individuals with disabilities, (3) AT policies are inconsistent when describing data storage, handling, and security methods, (4) AT policies often do not differentiate between essential and non-essential data collection, and (5) there is often a lack of transparency in AT policies around third-party data sharing. These findings reveal that AT privacy policies overlook and underestimate a user’s acceptable privacy risks. We conclude our study by discussing AT design implications."
The Impact of Artificial Intelligence (AI) Systems on the Personalized Learning Outcomes for Senior High School Students: A Systematic Review,"Tabora, Liandro Antonio Tiongson and Ampatuan, Datu Sajid Islam Sinsuat and Castaneda, Marian Angelique Chamen and Galinato, Erylle Jerica Uy and Zulueta, Eury Ellyn Manaloto",10.1145/3678392.3678393,2024,"The implications of Artificial Intelligence (AI) in education, and academic research has generated considerable discussion and debate. Currently, AI systems attempt to address student learning by helping with obstacle alleviation that students face. With a full grasp of the technical systems involved, students ensure the optimal utilization of AI-assisted personalized learning. However, despite numerous empirical studies on its functionality and capabilities, there is a lack of understanding about AI's place in providing a personalized learning environment while assisting students, especially in the Philippines. This review aims to explore literature regarding the incorporation of AI systems for personalized learning that can be used by Senior High School (SHS) students, specifically in the Philippines. A multi-phase approach was employed, starting with a systematic literature review based on the PRISMA method, carried out in three phases to explore AI-assisted personalized learning's implementation, benefits, and challenges. Data from N=34 studies published from 2019 to 2024 were gathered and analyzed to explore the feasibility and impact of AI-assisted personalized learning. Overall findings show the positive impact of AI-assisted personalized learning, emphasizing improved learning outcomes and student motivation in various nations especially in Asia, Africa, and North America. Our findings show that AI personal learning systems, while promising, face challenges such as the need for empathetic AI systems, comprehensive teacher training, and underscores areas for further research and consideration to realize the full potential of AI personalized learning interventions."
Image-Text Multimodal Translation Based on AIGC Human-Machine Interaction,"Yang, Lixue",10.1145/3678429.3678436,2024,"The multimodal process of machine translation is studied by focusing on the development of artificial intelligence in language processing and the transformation of vocabulary into numerical representations through vectorization. The neural networks such as CBOW and Skip-gram models are applied to analyze the word vectorization. It also explores the Transformer model with self-attention mechanism, emphasizing the importance of Layer Normalization for training stability and convergence speed. The emergence of ChatGPT as a state-of-the-art conversational AI model, highlights its role in assisting translators with language understanding and generation tasks. The application of generative artificial intelligence is discussed in translation practice, where human-machine interaction maximizes human intelligence while utilizing AI capabilities. DALL·E2 is capable of generating images from text, and the integration of image with translated text plays an important role in constructing the being of the intersemiotic translated work as they maintain the existential emotions effectively through the text-image multimodal interaction."
Metaverse World Fusion of Digital Humanities in Virtual Reality Exploration,"Handoko, Bambang Leo and Pradipto, Yosef Dedy and Sundjaja, Arta Moro and Lindawati, Ang Swat Lin and Mustapha, Mazlina",10.1145/3678610.3678627,2024,"In the ever-growing digital era, the Metaverse World concept is the main highlight in this research, combining elements of Digital Humanities with virtual reality exploration. Metaverse World encapsulates a virtual space that unites users from various walks of life in a completely connected digital environment. Combining aims to explore the potential for a fusion between the Metaverse and Digital Humanities concepts, with a focus on how combining digital technology and understanding cultural context can enrich virtual experiences. This phenomenon interests us in conducting research. Our research method is literature review using Preferred Reporting Items for Systematic Reviews and Meta-Analyses or PRISMA approach. We collected literature from Scopus database then we carried out bibliometric analysis data using Vos Viewer software. We synthesized the results and combined them with the results of discussions with practitioners in the Metaverse field. Through an in-depth understanding of the human aspects of cyberspace, this research will provide new insights into how the Metaverse World can become a forum for creative expression, education, and social interaction in an ever-evolving digital era. The interaction of these two concepts provides new opportunities for research, education, and human interaction in cyberspace. However, metaverse user privacy and security are also important issues, metaverse platform providers must be able to mitigate the risk of cyber-attacks and improve cyber security."
MobGLM: A Large Language Model for Synthetic Human Mobility Generation,"Zhang, Kunyi and Pang, Yanbo and Zhang, Yurong and Sekimoto, Yoshihide",10.1145/3678717.3691311,2024,"Human mobility generation plays a critical role in urban transportation planning. Existing human mobility generation models often fall short of understanding travelers' demographics and integrating multimodal information, including activity purposes, destination choices and transport mode preferences. Recently, mobility generation models leveraging Large Language Models (LLMs) have gained significant attention, while they are limited in directly reproducing spatial information in human mobility profiles. To address these challenges, this paper proposes the Mobility Generative Language Model (MobGLM), a novel approach for generating synthetic human mobility data to support urban planning, transport management, energy consumption and epidemic control. MobGLM addresses these limitations by capturing the complex relationships between agents' mobility patterns and individual demographics. By incorporating personal information, activity types, locations and traffic modes as encoders, MobGLM uniquely identifies and replicates features of human mobility. Our framework is evaluated using a large, real-world mobility dataset and benchmarked against state-of-the-art personal mobility generation techniques. The results demonstrate the effectiveness of MobGLM in producing accurate and reliable synthetic mobility data, highlighting its potential applications in various urban mobility contexts."
The use of AI techniques in table tennis: Bibliometrics analysis,"Wu, Chih-Hung and Chen, Yi-Xiang",10.1145/3678726.3678763,2024,"This study aims at exploring the impact and trajectory of AI technology in table tennis through bibliometric analysis. We have reviewed 43 articles from the Scopus database spanning 2013 to 2023, yielding significant insights. Bibliometric analyses since 2017 show a surge in AI applications in table tennis, highlighting AI's critical role in advancing this research domain. Through thematic analysis, we identified and gauged significant research themes by examining authors' keyword clusters and their interconnections. EEG has emerged as a dominant theme, underscoring its importance in enhancing table tennis skills through technology. Simultaneously, motion analysis became a burgeoning topic, emphasizing the need for in-depth sports behavior investigations. Additional themes such as deep learning, computer vision, and neural networks point to a diverse landscape for future research. Our study has revealed a notable collaboration rate of 13.95%, indicating a high degree of interdisciplinary cooperation in AI and gesture recognition within table tennis research. In conclusion, our research provides valuable insights into the evolving landscape of AI application in table tennis and paves the way for future research."
"FlowGPT: Exploring Domains, Output Modalities, and Goals of Community-Generated AI Chatbots","Li, Xian and Han, Yuanning and Liu, Di and An, Pengcheng and Niu, Shuo",10.1145/3678884.3681875,2024,"The advent of Generative AI and Large Language Models has not only enhanced the intelligence of interactive applications but also catalyzed the formation of communities passionate about customizing these AI capabilities. FlowGPT, an emerging platform for sharing AI prompts and use cases, exemplifies this trend, attracting many creators who develop and share chatbots with a broader community. Despite its growing popularity, there remains a significant gap in understanding the types and purposes of the AI tools created and shared by community members. In this study, we delve into FlowGPT and present our preliminary findings on the domain, output modality, and goals of chatbots. We aim to highlight common types of AI applications and identify future directions for research in AI-sharing communities."
What do malware analysts want from academia? A survey on the state-of-the-practice to guide research developments,"Botacin, Marcus",10.1145/3678890.3678892,2024,"Malware analysis tasks are as fundamental for modern cybersecurity as they are challenging to perform. More than depending on any tool capability, malware analysis tasks depend on human analysts’ abilities, experiences, and practices when using the tools. Academic research has traditionally been focused on producing solutions to overcome malware analysis technical challenges, but are these solutions adopted in practice by malware analysts? Are these solutions useful? If not, how can the academic community improve its practices to foster adoption and cause a greater impact? To answer these questions, we surveyed 21 professional malware analysts working in different companies, from CSIRTs to AV companies, to hear their opinions about existing tools, practices, and the challenges they face in their daily tasks. In 31 questions, we cover a broad range of aspects, from the number of observed malware variants to the use of public sandboxes and the tools the analysts would like to exist to make their lives easier. We aim to bridge the gap between academic developments and malware practices. To do so, on the one hand, we suggest to the analysts the solutions proposed in the literature that could be integrated into their practices. On the other hand, we also point out to the academic community possible future directions to bridge existing development gaps that significantly affect malware analysis practices."
Metamorphic Testing of a Steer-by-Wire System: An Intercultural Students-as-Partners Collaboration Experience,"Zhang, Yifan and Towey, Dave and Pike, Matthew and Qiu, Rui and Jaya, Axel Tan and Huey, Sze and Zhang, Xinyi and Wu, Yuan",10.1145/3679006.3685069,2024,"This paper explores the educational and practical impacts of integrating metamorphic testing (MT) into a software engineering project conducted by an intercultural group of students. The students designed a Steer-by-Wire (SBW) system to control the steering of a model vehicle and tested using a hybrid approach that combined unit testing and MT. Four metamorphic relations (MRs) were generated and two significant violations were encountered during the testing phase. The first violation, related to steering angle consistency reported by the system, as a case of metamorphic exploration (ME), revealed a common coding mistake where the system failed to maintain consistent steering angles for equivalent inputs in opposite directions, illustrating how ME can enhance comprehension of the system and the testing process itself. It not only deepened the testers' understanding of the integration between software and mechanical systems but also represented valuable insights for others engaged in similar tasks. The second MR violation revealed issues with interruptions and delays when the system switched between manual and automated control modes, demonstrating MT's effectiveness in identifying defects and highlighting MT's importance in real-world software development scenarios. Additionally, the project examined the effectiveness of aligning MT roles to team members based on their Myers-Briggs Type Indicator (MBTI) personalities, suggesting that such alignments can enhance team dynamics and overall project efficiency. This study provides insights into the benefits of using MT in educational settings, the implications of personality-based task assignments, and the enhancement of software reliability and team performance in an intercultural context. The findings of this research reinforce the value of MT in software engineering education and support for the integration of psychological analysis in managing complex projects."
"""We Are Visual Thinkers, Not Verbal Thinkers!"": A Thematic Analysis of How Professional Designers Use Generative AI Image Generation Tools","Park, Hyerim and Eirich, Joscha and Luckow, Andre and Sedlmair, Michael",10.1145/3679318.3685370,2024,"Generative artificial intelligence (GenAI) has become increasingly popular, influencing various creative domains. However, while broader societal perspectives have been analyzed, specific examinations of how practitioners utilize GenAI tools to enhance their current workflows remain limited. To address this gap, we conducted a qualitative study involving 16 professional designers from the automotive industry. We aimed to identify their challenges with existing GenAI image generation tools in daily design practices. Thematic analysis revealed four key themes: (1) the need for visual input-centric multi-modal interfaces that extend beyond textual prompts, (2) the lack of support for the iterative nature of design processes in GenAI tools, (3) difficulties in controlling prompts to achieve desired outputs, and (4) the significance of incorporating human experiences and emotions into design. Based on our findings, we propose and discuss potential design considerations for enhancing future GenAI image generation tool interfaces."
From Seed to Compost: A Systematic Review of the Impact of Digital Technologies Across the Community Food System Lifecycle,"Pang, Feifei and Grace, Rob and Kropczynski, Jess",10.1145/3679318.3685396,2024,"Given the challenges of food insecurity and the environmental, social, and economic impacts of conventional food systems, the development of community food systems has emerged as an important research area in the field of Sustainable Human-Computer Interaction (SHCI). In this paper, we present findings from a systematic review of 48 peer-reviewed articles that examine the impact of digital technologies on sustainable food practices across different lifecycle stages of community food systems. Our comprehensive review outlines the distribution of research emphases on food production, processing, distribution, consumption, and waste management. This critical analysis not only highlights existing technology support, but also identifies research gaps and suggests opportunities for future SHCI research. By addressing these themes, we aim to contribute to a more holistic approach in SHCI studies and advocate for the development of inclusive digital technologies that connect various lifecycle stages to promote sustainability in community food systems."
Embedding caring into remote patient management systems,"Offerman, C\'{e}line and Bourgeois, Jacky and Bozzon, Alessandro",10.1145/3679318.3685399,2024,"Remote Patient Management systems (RPM) are crucial for addressing healthcare workforce shortages. These systems are often designed with a specified focus on clinical functionalities, without proper consideration for human-centric concerns. A care perspective is essential not only to acknowledge patients as people, but also to foster better quality of care and, ultimately, adoption. This highlights the gap of how RPM can embed caring. This work offers a systematic literature review aimed at developing ""Caring RPM"", a normative framework that integrates the philosophy of caring from nursing theory into RPMs. This framework underwrites the practical, moral, and relational aspects of patient care, including actionable recommendations to recalibrate RPM systems for more effective human-centric design. The framework can inspire new ways of embedding the caring dimension into HCI design practices."
More-than-Human Perspectives in Human-Computer Interaction Research: A Scoping Review,"Eriksson, Eva and Yoo, Daisy and Bekker, Tilde and Nilsson, Elisabet M.",10.1145/3679318.3685408,2024,"More-than-human perspectives are gaining ground in human-computer interaction (HCI) research, but there is not yet any shared understanding in the community of what it entails. In this paper, we present the results from a scoping review on emerging more-than-human perspectives in HCI research. Based on a search focusing on the main concept in the ACM Digital Library, and the analysis of 40 papers in the final corpus, we outline the current status of various motivations, approaches and practices addressing more-than-human perspectives in HCI research. The contribution is a snapshot of the field, illustrated by examples, a discussion focused on the role of design within the current landscape of more-than-human perspectives in HCI research, and directions for moving the field forward."
Promoting Multi-actor Collaboration for New Online Service during Public Health Emergency: Roles of an Innovation Lab at Local Government,"Yuan, Qianli",10.1145/3680127.3680145,2024,"Collaborative approach to public sector innovation becomes increasingly popular in the public sector. This paper examines the role of innovation labs in facilitating multi-actor collaboration for the development of new online service in emergency response, using the case of the Seattle Innovation and Performance Office (Seattle IP) during the COVID-19 pandemic. The study reveals that the Seattle IP acted as a crucial liaison, fostering connections among city departments, technology companies, and citizens. It aligned goals, integrated resources, and transferred collaborative innovation knowledge to other cities. Key determinants for its success included its position within the mayor's office, an adaptive structure with dedicated staff, and the urgency of the pandemic. However, financial sustainability for scaling up the online testing registration was identified as a challenge. The findings emphasize the strategic importance of innovation labs in enhancing the public sector's capacity for rapid, coordinated, and human-centric service innovation during crises."
Sustainable Tokenisation Model for the Public Sector: Selecting the most suitable business process,"Escobar, Fernando and Santos, Henrique and Pereira, Teresa",10.1145/3680127.3680166,2024,"Tokenisation is one of the most efficient ways to employ blockchain (BC) technology. Especially helpful in the public sector, tokenisation can increase trust and confidence, mitigating the lack of transparency. It may also support and encourage the achievement of the Sustainable Development Goals (SDG), promoting sustainability and trustworthiness with transparency. Process tokenisation is the possibility of adopting BC as an infrastructure for a business process. However, some processes are more amenable to tokenisation than others. To evaluate aspects that could help public sector organisations select the most suitable business process for tokenisation, considering benefits related to sustainability, this work goes towards sustainable incentives for process tokenisation in the public sector, evaluated by nineteen structured interviews, and described by the European Blockchain Services Infrastructure (EBSI) case of Education Credentials. As a result, this paper provides a novel contribution, particularly identifying eight sustainable incentives for BC adoption in the public sector: behavioural, process’ standardisation, compliance, supply chain traceability, resource management, circular economy, credit management, and SDG monitoring. It also extends the knowledge about digital technologies and sustainable development, contributes to a more sustainable society, and encourages BC adoption in the public sector with sustainability goals."
Study on Application of Software Technology of Decoration Engineering Scene 3d Model,"Wang, Ziang and Guan, Changsheng and Zhou, Liming",10.1145/3685088.3685114,2024,"With the rapid development of China's construction industry, the proportion of 3D design in the design work gradually increases. The application of a three-dimensional building information model (including BIM) provides the necessary technical support for the visualization, parameterization and collaboration of architectural design work. In order to maximize the advantages of 3D building information modeling technology and deal with the interactive relationship of collaborative design, it is of great significance to develop a more efficient design collaborative platform. This study queries the research situation of 3D collaborative design at home and abroad, examines the functional structure of multiple platforms, and pays special attention to the interaction relationship of information in the solution of 3D collaborative design, based on the concept of ""collaborative design"", proposed the solution of the 3D BIM collaborative design platform in the design process. The research results of this paper have been verified in practical applications, such as the BIM application project of CITIC Digital Intelligence and the ""3D collaborative design project management and optimization platform"" developed. In these applications, design efficiency, data security, and project management effectiveness have been significantly improved. In short, the 3D collaborative design platform proposed in this paper provides a good solution for construction industry, improves the efficiency of collaborative design and reduces the difficulty of project management. In the future, research should focus on integrating the platform with broader engineering practices and leveraging emerging technologies such as artificial intelligence and big data to further enhance the platform's capabilities."
Central Bank Digital Currency Design Concepts: A Multivocal Literature Review,"Carvalho Silva, Elcelina and Mira da Silva, Miguel",10.1145/3685243.3685250,2025,"The unstructured conceptualization of the Central Bank Digital Currency and the absence of the systematized language that can be used in the central bank’s research project is instigating ambiguities and mistakes in its understanding. This research study aims to survey the concepts used by researchers and practitioners to describe CBDC design. The research method used in this study is the multivocal literature review and the qualitative document analysis. To select the scientific papers and practitioner’s published documents, we used the inclusion, exclusion, and quality assessment criteria to select 45 studies that covered state-of-the-art and practice research about the conceptualization of the CBDC information system modeling and implementation. The findings reveal that CBDC function, governance, proposal, participant, layer, life cycle, principle, model, access, digital asset, architecture, infrastructure, type, and use case are concepts used to describe CBDC. We propose the CBDC design domain constructs, which can be used by other researchers who are working with CBDC constructs to propose a model, taxonomy, or ontology related to the CBDC domain, by CBDC project experts’ teams to create the CBDC requirement elicitation or CBDC development process methodology, and by central banks to communicate their design choice option to their stakeholders. This research contributes to improve technical language associated with CBDC design, proposing a valuable resource for countries seeking to standardize their design choices and to accelerate collaboration between experts and stakeholders across legal, social, financial, and technological dimensions of CBDC regulation."
TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs,"Wanna, Selma and Solovyev, Nicholas and Barron, Ryan and Eren, Maksim E. and Bhattarai, Manish and Rasmussen, Kim \O{}. and Alexandrov, Boian S.",10.1145/3685650.3685667,2024,"Topic modeling is a technique for organizing and extracting themes from large collections of unstructured text. Non-negative matrix factorization (NMF) is a common unsupervised approach that decomposes a term frequency-inverse document frequency (TF-IDF) matrix to uncover latent topics and segment the dataset accordingly. While useful for highlighting patterns and clustering documents, NMF does not provide explicit topic labels, necessitating subject matter experts (SMEs) to assign labels manually. We present a methodology for automating topic labeling in documents clustered via NMF with automatic model determination (NMFk). By leveraging the output of NMFk and employing prompt engineering, we utilize large language models (LLMs) to generate accurate topic labels. Our case study on over 34,000 scientific abstracts on Knowledge Graphs demonstrates the effectiveness of our method in enhancing knowledge management and document organization."
Automating Cybersecurity Compliance in DevSecOps with Open Information Model for Security as Code,"Haverinen, Henry and Janhunen, Tomi and P\""{a}iv\""{a}rinta, Tero and Lempinen, Sami and Kaartinen, Suvi and Meril\""{a}, Sami",10.1145/3685651.3686700,2024,"Software development teams meet increasing requirements to implement cybersecurity management in compliance with standards and regulations. However, adopting a compliant cybersecurity management system and DevSecOps practices as part of a software development process has turned out to be tedious and expensive in practice. Open-source communities and open ecosystems, which lack tools and realistic practices for compliant cybersecurity management, face these difficulties as well. This paper suggests a set of requirements and a solution that are based on long-term experience in adopting standard compliant DevSecOps processes in industry. The proposed solution, called Cyberismo, facilitates the adoption of compliance and cybersecurity management, improves collaboration on cybersecurity in company internal projects, cross-company projects, and open-source projects, and automates the compliance and cybersecurity management in software development by way of an open information model representation format, and an open-source tool to manage the information model. As the information model uses a simple plain text format that can be managed by automated DevSecOps tool chains, it can be understood as an instance of the Everything as Code and Security as Code paradigms. The proposed solution is designed as modular, tailorable to the organisation and its existing tools, and flexible enough to model both process- and technology-related information. It automates both the validation of how compliance requirements have been met and the gathering and archiving of evidence of compliance. The information model is mapped to a logic program conforming to the Answer Set Programming (ASP) paradigm for knowledge representation. The mapping enables flexible query evaluation and reasoning, including the calculation of performance measures and automated policy checks. However, developers, product owners and other end-users of the solution do not necessarily need to know how to write logic programs, as logic programs can be encapsulated in content modules made available for the users. By putting the ease of adoption of compliant DevSecOps processes by the practitioners in the spotlight, this paper concludes that it is both necessary and possible to meet all the proposed requirements."
Random Matrix Weighting Algorithm Based Whole Life Project Cost Management,"Peng, Zhenya and Zhao, Ya",10.1145/3686081.3686107,2024,"The application of the Randomised Matrix Weighting Algorithm (RMWA) has greatly contributed to the construction industry's quest for effective Whole Life Cycle Project Cost Management (WLPCM). This paper provides a comprehensive study of the application of RMWA to the project life cycle, including the decision, design, implementation, completion, operation, and maintenance phases. RMWA introduces a stochastic approach to the traditional deterministic cost management methodology, providing a predictive and adaptive model that addresses the uncertainty inherent in construction projects. The key to RMWA is its use of weighted a posteriori probability and Gaussian distribution models, which together enable a more nuanced and data-informed approach to cost management. The algorithm's predictive analytics help to optimize targets for specific phases and anticipate future financial challenges. This dynamic approach contrasts with traditional static models and is particularly effective in the design phase, where it promotes more cost-effective processes, and in the maintenance phase, where it more accurately predicts ongoing costs. Comparative analysis demonstrates that RMWA is superior to traditional cost management programs. It ensures consistently high cost containment rates, signaling a robust framework capable of integrating complex project parameters to achieve accurate cost forecasts. RMWA is adaptive in real-time, allowing for the continual readjustment of cost strategies, making WLPCM not only responsive but also proactive in its financial planning. This study suggests that the incorporation of RMWA into WLPCM may mark a paradigm shift towards a more flexible, data-driven approach in the construction industry. It calls for a shift in industry practice to embrace advanced analytics and demands high-quality data for algorithmic accuracy. This paper concludes that RMWA in WLPCM has the potential to be a game-changer and set new standards for the financial management of construction projects. It paves the way for future research into optimizing the parameters of the algorithm, integrating it with other predictive models, and assessing the wider implications for the sustainability and efficiency of the construction economy."
Exploration and Practice of Hybrid 5E Teaching Model Based on OBE Concept in Business Data Analytics Course,"Li, Fanghu",10.1145/3686424.3686425,2024,"In order to meet the needs of cultivating new business talents in the era of digital economy, we build a quality house model of business data analysis course by taking OBE education concept, 5E teaching model as the guideline, adopting blended and case-based teaching methods, combining with Bloom's theory of classification of educational objectives. The aim of this is to improve teaching methods, reorganize teaching content and enhance teaching quality. Students are trained to have the ability to process, analyze and present business data, to achieve the unity of knowing, doing and developing, and to realize the enhancement of students' ability and the creation of value."
Research on the Curriculum Construction of Applied Artificial Intelligence based on Outcome-Based Education Concept,"Guo, Yunying and Li, Xiaofei and Zhang, Tianyu",10.1145/3686424.3686446,2024,"Based on the results-oriented OBE education concept, in order to cultivate application oriented innovative talents, artificial intelligence and machine learning courses adopt the blended teaching mode. The online learning platform is used to combine offline classroom and online independent learning, and the practical process of blended teaching is described from before, during and after class. Through the scientific setting of KT point and the expected learning effect mapping relationship to assess the level of achievement, we evaluate the impact of blended teaching reform and reflect on our teaching practices."
Enhancing Revenue and Student Outcomes at Small Universities through Professional STEM Master's Programs,"Srivastava, Aviral and Glantz, Edward J. and Khurana, Kartik and Stager, Sarah",10.1145/3686852.3686854,2024,"This paper advocates for the strategic adoption of Professional Master's in STEM (MPS) by small U.S. universities to attract international students and increase revenue. Emphasizing practical skills over theoretical study, MPS programs are posited to enhance graduate employability in the U.S. job market significantly. We discuss innovative supports such as skill-specific student clubs, experiential learning, and enhanced career services that go beyond traditional approaches, including strategies for effective job placement. Additionally, this paper addresses the unique challenges faced by international students, such as visa restrictions and cultural adaptation, suggesting that university-led advocacy and proactive employment assistance can improve outcomes. By integrating rigorous academic training with real-world application and targeted job search support, small universities can evolve into attractive hubs for global education, fostering financial stability and strengthening their international presence. This dual approach ensures that international graduates are not only educated but also well-prepared to navigate the competitive job market."
Instructional Approaches Complementing the Use of Generative Artificial Intelligence in Higher Education,"Beaton, Catherine and Weeden, Elissa and Zilora, Stephen",10.1145/3686852.3687075,2024,"The explosion of generative artificial intelligence (AI) has created a level of chaos in higher education as both students and faculty try to determine its utility and how best to incorporate it into the learning process. Students may view generative AI as a means to an end of achieving a perfect grade, skipping important elements of the learning process, or they may view it as an opportunity to expand their creative efforts. Faculty may view it as a tool students use to circumvent plagiarism detection, may feel it potentially minimizes the role of faculty in the classroom, or they may view it as an opportunity to avail of a supplement to existing activities and assignments. Ultimately, faculty are faced with maintaining academic integrity and reinforcing the need and importance of the learning process. This paper explores the combination of three approaches: peer-supported incremental learning, master/apprentice model, and growth mindset as a way for faculty to guide appropriate student use of generative AI, while also maintaining the integrity of the learning process."
"A Collection of Standards-based Recommendations for Sustainable, Social, Accessible Robots and Systems in Public Spaces - A Systematic Review and Derivation of Unified Equality Requirement Descriptions","Kubullek, Ann-Kathrin and van Ledden, Sebastian and Dogang\""{u}n, Ayseg\""{u}l",10.1145/3687272.3688294,2024,"The development of social robots for public spaces is challenging as they need to be accessible and socially acceptable for diverse user groups. Although current standards and guidelines contain numerous requirements, there is still a lack of comprehensive analysis and a compendium of these accessibility requirements, especially for social robots. Therefore, the aim of this paper is to derive a set of requirements and recommendations based on existing standards and guidelines. This is intended to promote the development of socially sustainable, accessible robots in public spaces and ensure that the specific needs of different user groups are taken into account. For this purpose, we conducted a systematic review of 31 International, European, and German standards and guidelines relevant to the design of socially sustainable robotics in public spaces. The review focused on aspects of accessibility, usability, design, safety requirements, and risk management in relation to robots, considering the heterogeneity of use by different target groups. In particular, we emphasized the equality requirements essential for the development of socially sustainable robots. We then categorized these requirements and recommendations according to different user needs using text mining techniques. Our analysis led to the creation of an illustrative catalog of 83 equality requirements and recommendations. These requirements and recommendations are intended to guide the design process, development, implementation, and use of sustainable and equality-oriented social robots in public spaces. This catalog provides comprehensive insights into promoting accessibility and social sustainability in the design of social robotics, addressing the challenges and requirements in developing such systems, and ensuring that these systems meet the needs of a heterogeneous user group."
Research on the teaching of workshop courses for engineering management specialty by BIM-FILM training platform,"Li, Yijia and Jia, Zixuan and Wan, Xiaoli and Su, Ziyu and Liu, Shuo",10.1145/3687311.3687324,2024,"BIM-FILM training is an innovative teaching method combining building information model and film making technology, which plays an important role in the visual display in the engineering field. The workshop course teaching is practical and problem-oriented to improve students' practical ability and comprehensive ability, and then improve the quality of teaching. Taking the current situation of BIM-FILM training platform in the teaching of engineering management workshop as the research background, combined with the significant advantages of BIM-FILM software, this paper constructs the application of BIM-FILM training platform, which provides ideas for the study of BIM-FILM training platform workshop in engineering management."
Research and Implementation of Education Management Information System Based on Association Mining Technology,"Zhang, Huanhuan",10.1145/3687311.3687392,2024,"This study focuses on developing an education management information system using association mining technology. Initially, the system's comprehensive framework is crafted, and its overall design is established. Secondly, a system optimization design method is proposed, and an association mining data model is built. Finally, an independent education management database is designed through induction, analysis, and summarization. The obtained findings reveal that as the education management information system described in this study operates continuously, there is a notable rise in the concurrent connections index. By the 20th second, the simultaneous connections peak, reaching a maximum of 882 users, surpassing the count of student users engaged in experimental teaching within the school. Prove that the educational management information system based on association mining technology can meet practical applications, and the system's experimental teaching informationization recommendation effect is outstanding."
"Blockchain, IoT, and AI-based framework for traceability in carbon capture utilization storage (CCUS) supply chain","Patro, Pratyush Kumar and Acquaye, Adolf and Jayaraman, Raja and Salah, Khaled",10.1145/3688225.3688234,2024,"Carbon capture utilization and storage (CCUS) is an effective technique for capturing, storing, and utilizing CO2 from the atmosphere leading to a reduction in greenhouse gas emissions. Transparency and traceability of carbon removal from the environment are key requirements for sound system design and implementation. Currently, CCUS systems and techniques lack in maintaining transparency and traceability in supply chain processes, including the quantification of CO2 removal from the environment, and they also fall short in recommending effective accounting of carbon capture, storage, and utilization. In this paper, we propose a framework that integrates blockchain, the Internet of Things (IoT), and Artificial intelligence (AI) to enhance transparency and CCUS accounting in the supply chain. The framework uses blockchain to create an immutable and decentralized ledger for recording CCUS-related transactions and data. IoT sensors deployed at various stages facilitate the real-time collection of data concerning carbon capture, transportation, and storage, as well as environmental conditions. AI algorithms can analyze the sensor data, perform carbon capture accounting, and provide insights for optimizing CCUS supply chain operations. The integration of these technologies ensures that all stakeholders have the access to reliable and tamper-proof information about the CCUS process. The proposed framework has the potential to address the challenges of transparency and traceability in CCUS, thereby increasing trust in the technology and promoting its wider adoption. We make our smart contract publicly available on GitHub for verification."
Generative AI Meets Accessibility: Deformable Interfaces and Multimodal Solutions,"Thalhammer, Philipp Tim",10.1145/3689050.3704798,2025,"Artificial intelligence (AI), especially large language models (LLMs), has evolved into one of the most influential technologies of our century. Yet, human interaction with AI is dominated by chat-based input windows. Although there have been some developments regarding wearable AI interfaces, the field remains largely unexplored. AI has the potential to revolutionize the way we approach accessibility by automating tasks that previously required human assistance. However, most AI tools are at least partly inaccessible to people who use assistive technologies to interact with computers. The goal of my Ph.D. research is to investigate how generative AI and LLMs can be made more accessible for people with disabilities and be utilized to create new accessibility tools through the use of multimodal interactions. I approach this problem using an iterative research through design (RTD) approach focused on close engagement with the target demographic."
Tangible User Interface in Health: A Scoping Review,"Bertolo, David and Fleck, St\'{e}phanie and Lemiere, Camille and Pecci, Isabelle",10.1145/3689050.3704951,2025,"Tangible User Interfaces’ research on Healthcare is still young: it has had no comprehensive overview over the last decade. The present study undertakes a scoping review of TUIs in Healthcare to determine the targeted users, which kind of TUIs are preferred, how to evaluate them, and why using them in Healthcare. We reviewed 66 publications from 5 databases and the TEI conference proceedings (2010-2024). We examined each one according to the medical fields concerned, the targeted audience, the type of contribution and the purposes, the evaluation process, the artifacts, and their impact on Healthcare. Four key findings were: (1) The potential of TUIs has not yet been explored in all health fields; (2) The primary purposes of TUIs are facilitating diagnosis, promoting active aging, and improving the rehabilitation process; (3) The artifacts are mainly motivating, easy to use, and low-cost; (4) Familiarity is an essential criterion for the design process."
AI Integration in the IT Professional Workplace: A Scoping Review and Interview Study with Implications for Education and Professional Competencies,"Clear, Tony and Cajander, \r{A}sa and Clear, Alison and McDermott, Roger and Daniels, Mats and Divitini, Monica and Forshaw, Matthew and Humble, Niklas and Kasinidou, Maria and Kleanthous, Styliani and Kultur, Can and Parvini, Ghazaleh and Polash, Mohammad and Zhu, Tingting",10.1145/3689187.3709607,2025,"As Artificial Intelligence (AI) continues transforming workplaces globally, particularly within the Information Technology (IT) industry, understanding its impact on IT professionals and computing curricula is crucial. This research builds on joint work from two countries, addressing concerns about AI's increasing influence in IT sector workplaces and its implications for tertiary education. The study focuses on AI technologies such as generative AI (GenAI) and large language models (LLMs). It examines how they are perceived and adopted and their effects on workplace dynamics, task allocation, and human-system interaction.IT professionals, noted as early adopters of AI, offer valuable insights into the interplay between AI and work engagement, highlighting the significant competencies required for digital workplaces. This study employs a dual-method approach, combining a systematic and multi-vocal literature review and qualitative research methods. These included a thematic analysis of a set of 47 interviews conducted between March and May of 2024 with IT professionals in two countries (New Zealand and Sweden). The research aimed to understand the implications for computing students, education curricula, and the assessment of emerging professional competencies.The literature review found insufficient evidence addressing comprehensive AI practice methodologies, highlighting the need to both develop and regulate professional competencies for effective AI integration. Key interview findings revealed diverse levels of GenAI adoption, ranging from individual experimentation to institutional integration. Participants generally expressed positive attitudes toward the technology and were actively pursuing self-learning despite some concerns. The themes emerging from the interviews included AI's role in augmenting human tasks, privacy and security concerns, productivity enhancements, legal and ethical challenges, and the evolving need for new competencies in the workplace.The study underscores the critical role of competency frameworks in guiding professional development and ensuring preparedness for an AI-driven environment. Additionally, it highlights the need for educational institutions to adapt curricula to address these emerging demands effectively"
Instructors' Perspectives on Capstone Courses in Computing Fields: A Mixed-Methods Study,"Hooshangi, Sara and Shakil, Asma and Dasgupta, Subhasish and C. Davis, Karen C. and Farghally, Mohammed and Fitzpatrick, KellyAnn and Gutica, Mirela and Hardt, Ryan and Riddle, Steve and Seyam, Mohammed",10.1145/3689187.3709608,2025,"Team-based capstone courses are integral to many undergraduate and postgraduate degree programs in the computing field. They are designed to help students gain hands-on experience and practice professional skills such as communication, teamwork, and self-reflection as they transition into the real world. Prior research on capstone courses has focused primarily on the experiences of students. The perspectives of instructors who teach capstone courses have not been explored comprehensively. However, an instructor's experience, motivation, and expectancy can have a significant impact on the quality of a capstone course. In this working group, we used a mixed methods approach to understand the experiences of capstone instructors. Issues such as class size, industry partnerships, managing student conflicts, and factors influencing instructor motivation were examined using a quantitative survey and semi-structured interviews with capstone teaching staff from multiple institutions across different continents. Our findings show that there are more similarities than differences across various capstone course structures. Similarities include team size, team formation methodologies, duration of the capstone course, and project sourcing. Differences in capstone courses include class sizes and institutional support. Some instructors felt that capstone courses require more time and effort than regular lecture-based courses. These instructors cited that the additional time and effort is related to class size and liaising with external stakeholders, including industry partners. Some instructors felt that their contributions were not recognized enough by the leadership at their institutions. Others acknowledged institutional support and the value that the capstone brought to their department. Overall, we found that capstone instructors were highly intrinsically motivated and enjoyed teaching the capstone course. Most of them agree that the course contributes to their professional development. The majority of the instructors reported positive experiences working with external partners and did not report any issues with Non-Disclosure Agreements (NDAs) or disputes about Intellectual Property (IP). In most institutions, students own the IP of their work, and clients understand that. We use the global perspective that this work has given us to provide guidelines for institutions to better support capstone instructors."
"Data Systems Education: Curriculum Recommendations, Course Syllabi, and Industry Needs","Miedema, Daphne and Taipalus, Toni and Ajanovski, Vangel V. and Alawini, Abdussalam and Goodfellow, Martin and Liut, Michael and Peltsverger, Svetlana and Young, Tiffany",10.1145/3689187.3709609,2025,"Data systems have been an important part of computing curricula for decades, and an integral part of data-focused industry roles such as software developers, data engineers, and data scientists. However, the field of data systems encompasses a large number of topics ranging from data manipulation and database distribution to creating data pipelines and data analytics solutions. Due to the slow nature of curriculum development, it remains unclear (i) which data systems topics are recommended across diverse higher education curriculum guidelines, (ii) which topics are taught in higher education data systems courses, and (iii) which data systems topics are actually valued in data-focused industry roles. In this study, we analyzed computing curriculum guidelines, course contents, and industry needs regarding data systems to uncover discrepancies between them. Our results show, for example, that topics such as data visualization, data warehousing, and semi-structured data models are valued in industry, yet seldom taught in courses. This work allows professionals to further align curriculum guidelines, higher education, and data systems industry to better prepare students for their working life by focusing on relevant skills in data systems education."
An International Examination of Non-Technical Skills and Professional Dispositions in Computing -- Identifying the Present Day Academia-Industry Gap,"Garcia, Rita and Csizmadia, Andrew and Pearce, Janice L. and Alshaigy, Bedour and Glebova, Olga and Harrington, Brian and Liaskos, Konstantinos and Lunn, Stephanie J. and Mackellar, Bonnie and Nasir, Usman and Pettit, Raymond and Schulz, Sandra and Stewart, Craig and Zavaleta Bernuy, Angela",10.1145/3689187.3709610,2025,"Computing graduates are frequently reported by members of industry to lack in professional dispositions and/or non-technical skills (often referred to as ""soft skills""). In this work, we conduct a gap analysis of the alignment between academic preparation and industry expectations through a three-pronged study. First, a literature review explored the academic perspective of how fostering professional dispositions and non-technical skills occurs in tertiary computing education. Second, a literature review identifying industry's expectations of those dispositions and skills for entry-level computing professionals. Finally, a mixed-methods approach, combining a survey and structured interviews of computing industry professionals to identify their opinions on the relative importance of those skills and dispositions. In each of these prongs, we additionally consider whether and how Diversity, Equity, Inclusion, and Accessibility (DEIA) may have been approached and/or incorporated.Our work uncovers a number of gaps. Several skills and dispositions, such as leadership, ethics, and inventiveness, are over-represented in the academic literature compared to industry's expectations, while others such as lifelong learning and professionalism are under-emphasised. Furthermore, some terms such as 'ethics' and 'professionalism' are defined differently by various stakeholder groups, leading to a gap between academic training and industry expectations. Finally, several skills and dispositions, such as collaboration, teamwork, communication, and leadership show evidence of exposure in academia, but require more scaffolded instruction to meet industry expectations. We also found a dearth of coverage in the literature and a lack of focus in industry for DEIA considerations."
BKRAG : A BGE Reranker RAG for similarity analysis of power project requirements,"Guo, Jun and Chen, Bojian and Zhao, Zhichao and He, Jindong and Chen, Shichun and Hu, Donglan and Pan, Hao",10.1145/3689218.3689224,2024,"This paper proposes an innovative method called BKRAG (A BGE Reranker Retrieval Augmented Generation for similarity analysis of power project requirements), which integrates information retrieval techniques and NLP to achieve automated analysis and similarity evaluation of power project requirements. The core of the BKRAG method lies in the utilization of a Rerank model to re-rank the initially retrieved candidate documents, improving their semantic matching degree with user queries, thereby optimizing the results of requirements similarity analysis. In this paper, we elaborate on the construction principles and workflow of the BKRAG method and verify its effectiveness through a series of experiments. Results demonstrate that the BKRAG can significantly improve the retrieval accuracy of power project requirement documents and the performance of requirements similarity analysis. The research findings of this paper not only provide a new solution for the field of power project requirements analysis, but also offer new insights into the cross-application of information retrieval and natural language processing technologies."
Construction of Scientific Research Management Platform in Higher Vocational Colleges Based on Big Data Technology,"Geng, Chunxi and Zhang, Yi and Yang, Li",10.1145/3689236.3695382,2024,"Given the increasingly prominent role of quality assurance systems in the development of scientific research in vocational colleges, this study aims to explore an innovative research management platform construction plan based on big data technology to meet the needs of scientific research management reform. By comprehensively applying literature research methods to analyze the current situation and needs, and combining experimental methods to verify the feasibility of the technology, this study successfully constructed a scientific research management platform that integrates quality assurance system and big data technology. Not only has it achieved comprehensive integration, efficient governance, and intelligent analysis of scientific research data. We also utilize big data technology to deeply explore the value of scientific research data and enhance our ability to gain insights into data; Simultaneously integrating the quality assurance system, focusing on quality, activating power, and empowering high-quality development of scientific research management through diagnostic reform. This study not only provides a practical and feasible path for the digital transformation of scientific research management in vocational colleges, but also has important practical guidance significance and wide promotion value, which helps to promote the overall progress of scientific research management in higher education."
Parametric Design and Automation of Residential Buildings Based on Python,"Chen, Jiatong and Hu, Huicong and Wang, Xia",10.1145/3689236.3698035,2024,"This study focuses on the application of Python programming to automate the parametric design process for rural residential architecture. By translating traditional architectural concepts into computational logic through Python, the research demonstrates how Python code can dynamically generate flexible and diverse design solutions. Python plays a central role in the creation of site layouts, floor plans, elevations, and building forms by leveraging its libraries, such as Turtle, for visual representation. This study highlights how Python enhances not only parametric design but also the automation of architectural processes, providing new opportunities for intelligent, data-driven design. The findings suggest that Python can significantly improve the efficiency and precision of design generation. Future research should explore expanding Python's capabilities to more complex tasks such as designing public buildings or climate-responsive structures."
Talking to Objects in Natural Language: Toward Semantic Tools for Exploratory Programming,"Thiede, Christoph and Taeumel, Marcel and B\""{o}hme, Lukas and Hirschfeld, Robert",10.1145/3689492.3690049,2024,"In exploratory programming, programmers often face a semantic gap between their high-level understanding and the low-level interfaces available for interacting with objects in a system. That is, technical object structure and behavior need to be interpreted as abstract domain concepts, which then increases cognitive load and thus impedes exploration progress. We propose semantic object interfaces that bridge this gap by enabling contextual, natural-language conversations with objects. Our approach leverages an exploratory programming agent powered by a large language model (LLM) to translate natural-language questions into low-level experiments and provide high-level answers. We describe a framework for integrating semantic object interfaces into existing exploratory programming systems, including a prototype implementation in Squeak/Smalltalk using GPT-4o. We showcase the potential of semantic object interfaces through case studies and discuss their feasibility, limitations, and impact on the programming experience. While challenges remain, our approach promises to reduce mental effort and empower programmers to explore and understand systems at a higher level of abstraction for a better programming experience."
Exploring Human-Centered Approaches in Generative AI and Introductory Programming Research: A Scoping Review,"Stone, Irene",10.1145/3689535.3689553,2024,"Recent advancements in generative artificial intelligence are poised to reshape introductory programming education, challenging conventional teaching methodologies. This paper presents a scoping review that explores the current understanding of integrating generative artificial intelligence tools in the learning of introductory programming. Through an analysis of 28 selected studies, this review provides a snapshot of the landscape in mid-2024, presenting benefits, concerns, and recommendations surrounding the use of generative artificial intelligence within programming education. It finds insufficient guidance on how to implement recommended pedagogical strategies, limited consideration of student perceptions and experiences, and a predominance of short study time frames. Additionally, there is a significant research gap in second-level education, particularly in the United Kingdom and Ireland. The paper discusses how these gaps signal a need for more human-centered approaches in the current research. The paper concludes with recommendations for future research, aiming to inspire further inquiry and advance the understanding of generative artificial intelligence’s role in programming education from a human-centered perspective."
Game Model for Risk Allocation of Highway Public-Private-Partnership (PPP) Projects Considering Risk Tolerance,"Wu, Zhenyao and Zhao, Guanghui and Zhang, Yaxin",10.1145/3690407.3690455,2024,"Reasonably sharing the risks of highway PPP projects can ensure the successful operation of the project. The existing research on risk allocation in PPP projects has not yet considered the risk tolerance of project participants. This paper constructs a bargaining game model to analyze the reasonable risk allocation ratio of project participants with different risk tolerance under complete and incomplete information conditions. The research results of this paper not only supplement the shortcomings of PPP projects risk allocation research in theory, but also have very important practical significance for ensuring the construction of PPP projects on highway."
A Model for Predicting Salaries in Big Data Roles: An Integration of Random Forest and Adaboost-KNN Models,"Chen, Yuxi and Peng, Yuting and Shen, Lanlan",10.1145/3690407.3690504,2024,"Amidst the swift advancement of information technology and the widespread adoption of the Internet, the demand within the big data sector has rapidly escalated. For entities spanning various industries, expertise in big data is increasingly recognized as a pivotal asset for securing a competitive edge. Navigating such a competitive landscape necessitates a nuanced understanding of the factors influencing salary, thereby enabling job seekers to strategically navigate their career trajectories, while offering organizations a criterion for talent identification. Nonetheless, the existing body of research dedicated to the employment dynamics of big data specialists is notably limited, with a particular deficiency in comprehensive analyses and the graphical representation of demand trends. Consequently, this investigation seeks to thoroughly examine the employment market for big data professions through the aggregation of job demand data from numerous recruiting platforms, thus furnishing critical insights and benchmarks. We introduce a novel approach that synergizes machine learning techniques to conduct an exhaustive evaluation of the job demand landscape. Through the application of this model, which is adaptable to the analysis of employment demand across various professions, we have attained a predictive accuracy of 95% in forecasting salaries for big data roles. Moreover, the findings from this research equip individuals in related fields with a profound comprehension of the big data job market, enhancing their ability to refine their job search and skill enhancement strategies."
Research on Application System Integration and Service Transformation under Software Flexible Framework,"Yang, Yiping and Zhao, Liang and Yin, Yuchen",10.1145/3690407.3690514,2024,"In response to the challenges posed by changes in user business processes and the flexibility of service composition, this paper conducts in-depth analysis from two aspects: flexible software theory and data processing flow. Based on existing technologies, a service-oriented and workflow management software framework is proposed. It abstracts the business process layer and service layer, and utilizes the flexibility of workflow management to implement a flexible software framework technology, truly realizing the component assembly of software development industrialization, while supporting the flexible reorganization of data business processes. Finally, guided by the theory of flexible software, the specific implementation methods and processes of this framework were described in detail. The experimental results show that the framework achieves module reuse, reduces software development costs, and ensures the quality of software products."
Construction of investment impact index and LASSO regres-sion prediction for pumped storage power stations,"Yu, Gang and Wang, Xiuna and Han, Chao and Wang, Yanbo and Xu, Dan",10.1145/3690407.3690548,2024,"Pumped storage power stations (PSPS), as a form of energy storage technology, are deployed extensively in power systems dominated by renewable energy due to their flexible energy storage and regulation capabilities. Investment decisions for new power stations require com-prehensive consideration of cost-driving factors and estimation of total project investment. However, current cost management methods in this area remain immature. In this study, we propose a cost impact factor analysis and prediction model for PSPS. Firstly, descriptive statistics and potential relationship construction were conducted on data from 33 PSPS projects in China to preliminarily identify potential cost-driving factors. Secondly, the Grey Relational Analysis (GRA) method was employed to weight the impact factors and select significant ones. Subsequently, utilizing these significant impact factors as labels, a LASSO regression model was established to construct the relationship between the factors and unit cost, and to accomplish predictive tasks based on the constructed relationship. Finally, the predictive model was validated for its effectiveness and sensitivity analysis. The results indicate that factors such as dam volume and rated head are significant cost-driving factors for PSPS. The established LASSO regression model achieves high-quality regression prediction within an accuracy deviation of less than 5%, and the regression results show high sensitivity to factors like reservoir capacity and major material prices."
Transformer-based AI for Sentiment Analysis in Marketing,"Petratos, Pythagoras and Giannoula, Mina",10.1145/3690771.3690795,2025,"Previous research has found that sentiment analysis is the focus of Artificial Intelligence (AI). Nevertheless, a limited number of studies exist on advanced AI applications. First, this study contributes to this limited literature. It is argued that Transformer Architecture has initiated a new phase in AI and sentiment analysis. We contribute to this argument by providing evidence that transformer-based AI provides better performance and capabilities. We do so by systematically reviewing and analyzing the applications of Transformer-based models for sentiment analysis. The focus of this paper lies in the broader marketing field. In addition, marketing is one of the most essential fields in business. The choice of marketing in examining Transformer-based sentiment analysis constitutes a novelty. Using the PRISMA methodology we reviewed 481 papers. Only eight papers fulfill the criteria. It is a somewhat surprising finding that so few papers have applications of Transformer-based AI for sentiment analysis in marketing. Most papers follow older AI methods (i.e., Recurrent Neural Networks, Convolutional Neural Networks, etc.). This leaves a gap in this area, and we examine novel trends and directions for future research."
"The Intricate Connections between Digital Strategy, Absorptive Capacity, Digital Technology Use, and Digital Innovation","Cao, Guangming",10.1145/3691422.3691428,2025,"Although digital innovation has attracted much scholarly attention, little is known about the nuanced connections between digital strategy, absorptive capability, digital technology use, and digital innovation. Knowledge of these connections is essential for firms aiming to innovate successfully in the digital age. This study aims to bridge this gap by integrating perspectives from strategic management and absorptive capacity, proposing that a firm's digital innovation stems from intricate interplays among digital strategy, absorptive capacity, and digital technology use. This research employs partial least squares structural equation modeling with data from a survey of 250 Chinese firm managers. The findings reveal a positive association between a firm's digital strategy and its digital innovation, both directly and indirectly. The indirect relationship unfolds through a complex process, sequentially mediated by absorptive capacity and digital technology use, as well as simultaneously mediated by both factors. Notably, this research uncovers a curvilinear relationship between digital technology use and digital innovation. This research makes a valuable contribution to the existing literature on digital innovation by underscoring the importance of comprehending these intricate interrelationships. Furthermore, it provides actionable managerial insights for firms aiming to enhance their digital innovation initiatives."
Implementation's influence of an inventory control system with sure step methodology in Euro Peru S.A.C. Company,"Leiva Chamorro, Junior and Damian Villalobos, Michael Anmel and Picon Revolo, Luis Alexis and Infante Vidalon, Alan Miguel",10.1145/3691422.3691446,2025,"Euro Per\'{u} SAC is a company that manufactures leather shoes for women, however, the order of production of the final shoe products is not organized properly, it only has a basic file, as a result of this poorly organized process, there is a delay constant in the development of products and excessive expenses for additional purchases, affecting inventory management and the company's economy. In response to the company's problems, it is desired to implement an internal control system using the kardex methodology to improve inventory management, improving learning in systems and adapt to machine learning. For the implementation of the Kardex system, the Sure Step methodology was used and then the Kardex methodology was used, based on good practices. For the research, the quantitative approach was used with the explanatory scope, likewise, “the pre-experimental research design” was used, in addition, the sample of 8 types of leather corresponding to 3 categories of a population of 100 types of leather was defined, the pre-test and post-test was also applied with the recording technique and registration form instrument, once the kardex system was implemented, the “Shapiro-Wilk normality test” was carried out, resulting in the 4 indicators having a normal distribution. To subsequently perform the Student T test which was less than 0.05 in the 4 indicators, therefore, the implementation of an internal control system improves the management and integration of information within the organization. In the conclusion it is mentioned how the methodology influences the successful implementation of the inventory software, and it is also compared with other research where it confirms the positive influence of the methodology proposed in this study."
Efficient Detection of Test Interference in C Projects,"Eder, Florian and Winter, Stefan",10.1145/3691620.3694995,2024,"During test execution, automated software tests can interfere, i.e., their results can deviate depending on their (possibly interleaved) execution order. Such interference imposes severe restrictions on regression testing, when execution order is not or cannot be controlled for, as they can lead to non-deterministic deviations of test results giving false indication of regressions in the code base. While the phenomenon has been extensively studied for Java and Python projects, it remains unclear if or how the obtained results apply for other languages with different testing practices. Our study contributes to filling that gap by reporting results from a large-scale study on test interference in 134 C projects.To cope with the combinatorial explosion of test execution counts when testing with all possible test orders, we propose and evaluate four novel dynamic reduction strategies for test permutations, which yield massive reductions in the number of test sequences to execute. As these strategies are specific to the resources that tests interfere on, rather than the language in which the code is written, we expect them to be useful for the study of test interference in other languages beyond C.Based on the results obtained with these reductions, our results indicate that test order dependencies are far less common in C projects, compared to Java or Python, and that other aspects (concurrency, CPU time) more frequently threaten test result stability."
Three Heads Are Better Than One: Suggesting Move Method Refactoring Opportunities with Inter-class Code Entity Dependency Enhanced Hybrid Hypergraph Neural Network,"Cui, Di and Wang, Jiaqi and Wang, Qiangqiang and Ji, Peng and Qiao, Minglang and Zhao, Yutong and Hu, Jingzhao and Wang, Luqiao and Li, Qingshan",10.1145/3691620.3695068,2024,"Methods implemented in incorrect classes will cause excessive reliance on other classes than their own, known as a typical code smell symptom: feature envy, which makes it difficult to maintain increased coupling between classes. Addressing this issue, several Move Method refactoring tools have been proposed, employing a two-phase process: identifying misplaced methods to move and appropriate classes to receive, and implementing the mechanics of refactoring. These tools traditionally use hard-coded metrics to measure correlations between movable methods and target classes and apply heuristic thresholds or trained classifiers to unearth refactoring opportunities. Yet, these approaches predominantly illuminate pairwise correlations between methods and classes while overlooking the complex and complicated dependencies binding multiple code entities within these methods/classes that are prevalent in real-world cases. This narrow focus can lead to refactoring suggestions that may diverge from developers' actual needs. To bridge this gap, our paper leverages the concept of inter-class code entity dependency hypergraph to model complicated dependency relationships involving multiple code entities within various methods/classes and proposes a hypergraph learning-based approach to suggest Move Method refactoring opportunities named HMove. We first construct inter-class code entity dependency hypergraphs from training samples and assign attributes to entities with a pre-trained code model. All the attributed hypergraphs are fed into a hybrid hypergraph neural network for training. Utilizing this trained neural network alongside a large language model, we construct a refactoring suggestion system. We trained HMove on a large-scale dataset and evaluated it on two real-world datasets. The results show that demonstrates an increase of 27.8% in precision, 2.5% in recall, and 18.5% in f1-measure compared to 9 state-of-the-art refactoring tools, which is more useful for 68% of participants. The results also unveil practical suggestions and new insights that benefit existing feature envy-related refactoring techniques."
An Explainable Automated Model for Measuring Software Engineer Contribution,"Li, Yue and Zhang, He and Jin, Yuzhe and Ren, Zhong and Dong, Liming and Lyu, Jun and Yang, Lanxin and Lo, David and Shao, Dong",10.1145/3691620.3695071,2024,"Software engineers play an important role throughout the software development life-cycle, particularly in industry emphasizing quality assurance and timely delivery. Contribution measurement provides proper incentives to software engineers that motivate them to continuously improve the quality and efficiency of their work. However, existing research tends to ignore contribution measurement for software engineers in practice, relying heavily on peer review and lacking objectivity and transparency. Specifically, these studies still have two weaknesses. First, a few studies explore which metrics can be useful for contribution measurement in practice. Second, managers measure the contribution of software engineers based on their experience and lack of explainable automated tools to assist them.To this end, we conduct mixed-method studies to investigate contribution measurement in the enterprise, and then propose an explainable model for measuring the contribution of software engineers. First, we collect and synthesize 16 metrics for contribution measurement by interviewing 18 industrial practitioners. Second, we propose an explainable model, called Memento, integrating Multi-dimEnsional MEtrics for measuriNg conTributiOn. We conduct an industrial case study with a global enterprise to evaluate and refine Memento. Finally, we administer a survey to industrial practitioners to verify whether the explainable model is useful for contribution analysis, which resulted in 67 valid responses. Memento is used by the enterprise to measure the contribution of 5,174 employees. A total of 100 employees are identified as low contributors by Memento, and the five reasons for their low contribution are determined based on the results of our explainable model. The results indicate that Memento can effectively measure the contribution of software engineers, which provides a practical reference for researchers interested in contribution measurement."
ARUS: A Tool for Automatically Removing Unnecessary Stubbings from Test Suites,"Li, Mengzhen and Fazzini, Mattia",10.1145/3691620.3695355,2024,"In software testing, test doubles and stubbings are crucial for isolating the code under test from its dependencies, allowing for more controlled and reliable testing environments. However, as test suites evolve, some stubbings may become unnecessary, which should be removed to keep the test code clean, reliable, and maintainable.To address this problem, we introduce ARUS, a tool designed to automatically remove unnecessary stubbings from test suites. ARUS can be used to analyze Java projects using Maven, JUnit, and Mockito. Given a software project and its test suite, the tool executes the test suite to collect data on how stubbings are used, identifies and categorizes stubbings that are unnecessary, and removes unnecessary stubbings through different resolution strategies. We used ARUS to perform an empirical evaluation based on 128 projects. The projects contain 280 stubbing definitions that lead to 1,529 unnecessary stubbings. Overall, ARUS provides a solution for 276 of the definitions (98.6% resolution rate) and the tool's time cost is negligible. We submitted ARUS' changes to the projects through pull requests and 86 resolutions are already merged. ARUS and its dataset are publicly available at https://github.com/se-umn/arus. We provide a video demo of the tool at https://youtu.be/YcJxp5lRlFM."
Effective Unit Test Generation for Java Null Pointer Exceptions,"Lee, Myungho and Bak, Jiseong and Moon, Seokhyeon and Jhi, Yoon-Chan and Oh, Hakjoo",10.1145/3691620.3695484,2024,"In this experience paper, we share our experience on enhancing automatic unit test generation to more effectively find Java null pointer exceptions (NPEs). NPEs are among the most common and critical errors in Java applications. However, as we demonstrate in this paper, existing unit test generation tools such as Randoop and EvoSuite are not sufficiently effective at catching NPEs. Specifically, their primary strategy of achieving high code coverage does not necessarily result in triggering diverse NPEs in practice. In this paper, we detail our observation on the limitations of current state-of-the-art unit testing tools in terms of NPE detection and introduce a new strategy to improve their effectiveness. Our strategy utilizes both static and dynamic analyses to guide the test case generator to focus specifically on scenarios that are likely to trigger NPEs. We implemented this strategy on top of EvoSuite, and evaluated our tool, NpeTest, on 108 NPE benchmarks collected from 96 real-world projects. The results show that our NPE-guidance strategy can increase EvoSuite's reproduction rate of the NPEs from 56.9% to 78.9%, a 38.7% improvement. Furthermore, NpeTest successfully detected 89 previously unknown NPEs from an industry project."
ChatBR: Automated assessment and improvement of bug report quality using ChatGPT,"Bo, Lili and Ji, Wangjie and Sun, Xiaobing and Zhang, Ting and Wu, Xiaoxue and Wei, Ying",10.1145/3691620.3695518,2024,"Bug reports, containing crucial information such as the Observed Behavior (OB), the Expected Behavior (EB), and the Steps to Reproduce (S2R), can help developers localize and fix bugs efficiently. However, due to the increasing complexity of some bugs and the limited experience of some reporters, large numbers of bug reports miss this crucial information. Although machine learning (ML)-based and information retrieval (IR)-based approaches are proposed to detect and supplement the missing information in bug reports, the performance of these approaches depends heavily on the size and quality of bug report datasets.In this paper, we present ChatBR, an approach for automated assessment and improvement of bug report quality using ChatGPT. First, we fine-tune a BERT model using manually annotated bug reports to create a sentence-level multi-label classifier to assess the quality of bug reports by detecting whether existing OB, EB, and S2R. Then, we use ChatGPT in a zero-shot setup to generate missing information (OB, EB, and S2R) to improve the quality of bug reports. Finally, the output of ChatGPT are fed back into the classifier for verification until ChatGPT generates the missing information. Experimental results show that, in the task of detecting missing information in bug reports, ChatBR outperforms the state-of-the-art methods by 25.38%-29.20% in terms of precision. In the task of generating missing information in bug reports, ChatBR can achieve an average of 84.10% in terms of semantic similarity of the generated information and original information across six different projects. Furthermore, ChatBR can generate more than 99.9% of high-quality bug reports (i.e., bug reports that are full of OB, EB, and S2R) within five queries to ChatGPT."
Enhancing Automated Program Repair with Solution Design,"Zhao, Jiuang and Yang, Donghao and Zhang, Li and Lian, Xiaoli and Yang, Zitian and Liu, Fang",10.1145/3691620.3695537,2024,"Automatic Program Repair (APR) endeavors to autonomously rectify issues within specific projects, which generally encompasses three categories of tasks: bug resolution, new feature development, and feature enhancement. Despite extensive research proposing various methodologies, their efficacy in addressing real issues remains unsatisfactory. It's worth noting that, typically, engineers have design rationales (DR) on solution--- planed solutions and a set of underlying reasons---before they start patching code. In open-source projects, these DRs are frequently captured in issue logs through project management tools like Jira. This raises a compelling question: How can we leverage DR scattered across the issue logs to efficiently enhance APR?To investigate this premise, we introduce DRCodePilot, an approach designed to augment GPT-4-Turbo's APR capabilities by incorporating DR into the prompt instruction. Furthermore, given GPT-4's constraints in fully grasping the broader project context and occasional shortcomings in generating precise identifiers, we have devised a feedback-based self-reflective framework, in which we prompt GPT-4 to reconsider and refine its outputs by referencing a provided patch and suggested identifiers. We have established a benchmark comprising 938 issue-patch pairs sourced from two open-source repositories hosted on GitHub and Jira. Our experimental results are impressive: DRCodePilot achieves a full-match ratio that is a remarkable 4.7x higher than when GPT-4 is utilized directly. Additionally, the CodeBLEU scores also exhibit promising enhancements. Moreover, our findings reveal that the standalone application of DR can yield promising increase in the full-match ratio across CodeLlama, GPT-3.5, and GPT-4 within our benchmark suite. We believe that our DRCodePilot initiative heralds a novel human-in-the-loop avenue for advancing the field of APR."
Context-Aware Automated Sprint Plan Generation for Agile Software Development,"Kula, Elvan and van Deursen, Arie and Gousios, Georgios",10.1145/3691620.3695540,2024,"Sprint planning is essential for the successful execution of agile software projects. While various prioritization criteria influence the selection of user stories for sprint planning, their relative importance remains largely unexplored, especially across different project contexts. In this paper, we investigate how prioritization criteria vary across project settings and propose a model for generating sprint plans that are tailored to the context of individual teams. Through a survey conducted at ING, we identify urgency, sprint goal alignment, and business value as the top prioritization criteria, influenced by project factors such as resource availability and client type. These results highlight the need for contextual support in sprint planning. To address this need, we develop an optimization model that generates sprint plans aligned with the specific goals and performance of a team. By integrating teams' planning objectives and sprint history, the model adapts to unique team contexts, estimating prioritization criteria and identifying patterns in planning behavior. We apply our approach to real-world data from 4,841 sprints at ING, demonstrating significant improvements in team alignment and sprint plan effectiveness. Our model boosts team performance by generating plans that deliver more business value, align more closely with sprint goals, and better mitigate delay risks. Overall, our results show that the efficiency and outcomes of sprint planning practices can be significantly improved through the use of context-aware optimization methods."
Integrating Human-Centric Approaches into Undergraduate Software Engineering Education: A Scoping Review and Curriculum Analysis in the Australian Context,"Mckenzie, Sophie and Fernando, Niroshinie and Dias, Imali and Cheng, Ben and Hoang, Thuong and Liu, Xiao",10.1145/3691621.3694940,2024,"Human-Centric Software Engineering (HCSE) refers to the software engineering (SE) processes that put human needs and requirements as core practice throughout the software development life cycle. A large majority of software projects fail to cater to human needs and consequently run into budget, delivery, and usability issues. To support human-centric software engineering practices, it is important for universities to train their students on how to consider human needs. But what topics from HCSE should be provided in the undergraduate curriculum? This is still an open question. Curriculum guidelines for software engineering are available, however do not represent update to date considerations for how human-factors are included. In addition, curriculum approaches are not explored. To address this issue, this paper presents a scoping review to identify the topics and curriculum approaches suitable for teaching HCSE to undergraduate software engineering students. The scoping review was conducted according to the protocol by PRISMA-ScR (Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews). Through PRISMA-ScR, a total of 36 conference or journal papers were identified as viable for analysis, with 5 common themes found that describe topics and curriculum approaches relevant for teaching software engineering. Using the outcomes of the scoping review, this paper also analyses the Australian Software Engineering curriculum to understand the extent at which human centred software engineering topics are scaffolded into course structures. This paper concludes by suggesting topic scaffolding for the undergraduate curriculum that aligns with the software engineering process. Overall, by providing a focus on HCSE topics and curriculum approaches, the education and awareness of HCSE among current and future software engineers can increase, leading to long-term impact on the success of software projects for all stakeholders."
A First Look at Self-Admitted Miscommunications in GitHub Issues,"Hasan, Kazi Amit and Loc Mai, Vu Thanh and Wang, Cynthia and Tian, Yuan and Ding, Steven H. H.",10.1145/3691621.3694942,2024,"Effective communication is crucial for the success of open-source software development, particularly within distributed and asynchronous working environments on collaborative coding supporting platforms like GitHub. However, these environments often present significant communication challenges due to various factors, leading to project delays and wasted efforts. Despite prior research on collaboration challenges in software teams, there is a notable gap in understanding what, when, and where miscommunications occur in open-source projects. To address this gap, we mined 6,444 GitHub issues where developers explicitly admitted to miscommunication (using keywords such as ""miscommunication"") and manually analyzed the types, timing, and root causes of these self-admitted instances on a statistically significant sample set (363). Our findings are: (1) we developed a taxonomy of 12 issue types where miscommunications frequently occur, with bug reporting and feature requests being the most common; (2) we identified that most miscommunications occur before issue closure, but post-closure miscommunications, though less frequent, pose significant challenges; and (3) we uncovered five primary root causes of miscommunication, with technical misunderstandings being the most prevalent. This study provides the first comprehensive examination of miscommunication in open-source software development, offering insights and recommendations for researchers and practitioners to improve communication practices in GitHub issue discussions."
Interplay of Human Factors and Secure Architecture Design using Model-Driven Engineering,"Theveniaut, Robin and Hamid, Brahim and Jaskolka, Jason",10.1145/3691621.3694962,2024,"When developing a secure software architecture, a development team must collaborate to make critical security-related decisions. The human factors of the development team members play a vital role in secure architecture design and therefore must be considered when forming or evaluating development teams for a software project. In this paper, we present a model-driven approach for studying the interplay of human factors and secure architecture design. Specifically, we propose a conceptual model for considering direct and indirect human factors of the development team during secure software design and a set of modeling languages to represent the human factors. We also provide a questionnaire-based methodology to evaluate human factors of development team members and define team profiles. The approach enables characterizing the human factors of team members desired to achieve the protection goals of software architecture assets and to determine which team members should be participating in the decision-making for the design to achieve the goals for assets by matching the desired human factors to members belonging to team profiles. This approach can improve the confidence on the decision-making capabilities of teams when faced with critical security-related design designs. We illustrate the approach using a generic SCADA system use case."
Evaluating Usability as a Key Human Factor in Agile SPM Tools Through Expert-Based Techniques,"Alshammare, Haifa Abdullah and Alshabib, Asra Sulaiman and Alakkas, Noor Ahmad and Alshayeb, Mohammad Rabah",10.1145/3691621.3694963,2024,"Despite the importance of adopting SPM tools for supporting agile practices and the significance of usability as an essential component of human factors engineering, their usability evaluation was neglected in the literature. This study aims to identify the common usability issues in SPM tools and propose suggestions for enhancing their usability. Two of the most widely used SPM tools were selected as a case study for this study, namely Jira and Pivotal Tracker. Three independent evaluators participated in the usability evaluation, using two human factors techniques: Heuristic Evaluation and Cognitive Walkthrough. The results revealed a total frequency of 123 issues in both tools. Both tools had a nearly equal frequency of issues in each evaluation method but different severity levels. Learnability issues were the issues that were raised the most in the evaluation for both tools. Most of the issues had minor to major severity, with the following percentages of total issues: 41.33% and 46.67%, respectively, while only 13.33% were catastrophic. Considering the proposed solutions by designers and developers for this type of tool can help enhance the design of SPM tools, leading to increased user satisfaction and tool adoption."
Application of Information Audit Practice in Virtual Laboratory Simulation Environment,"Li, Wenji and Zheng, Jun",10.1145/3691720.3691761,2024,"With the rapid development of information technology, information auditing has become one of the important means to ensure corporate information security and improve business management levels. Based on the virtual laboratory simulation environment, this article discusses the practical research on the application of information auditing in virtual laboratories. Through information audit practice in a simulated environment, we can better understand potential risks in information systems and provide corresponding solutions, thereby improving an enterprise's information security risk management capabilities."
The Application Landscape and Research Status of Artificial Intelligence in Teacher Education: A Systematic Literature Review,"Jin, Sheng and Yu, Zengyi and Chen, Xinyu and Dai, Jian",10.1145/3691720.3691766,2024,"The integration of Artificial Intelligence (AI) has become a key driver in the evolution and transformation of teacher education. However, there has been no comprehensive review of AI applications in this field. In light of this, our study employs a systematic literature review, selecting 45 relevant articles from databases such as Web of Science, EBSCO, and Scopus, aiming to present a comprehensive overview of the field's development, key research themes, and propose a valuable future agenda. Findings are as follows: 1) In the realm of AI applications in teacher education, the United States emerges as a core contributor. Regarding the types of research, theoretical and empirical studies are equally prevalent; there has been a surge in publications in this field since 2021. 2) The research in this field primarily revolves around three themes: Performance Evaluation of AI Technology in Teacher Education, AI Technology's Role in Enhancing Teachers' Core Skills, and AI Assistance in Teacher Education. 3) Based on the literature review, this study offers a meaningful future agenda from both theoretical and practical perspectives. Theoretically, it involves strengthening research in teacher training and professional development, and enhancing educational decision support systems. Practically, it entails developing specialized AI applications for teacher education and promoting AI-assisted educational development."
Design and application of a web front-end development course training platform based on generative artificial intelligence and low code development,"Shan, Qiang",10.1145/3691720.3691768,2024,"With the rapid development of artificial intelligence technology, generative artificial intelligence has shown strong application potential in various fields, and low code development has become a popular choice for quickly building application systems due to its efficient and easy-to-use characteristics. With the continuous updates and iterations of web front-end development technology, how to improve learners' practical abilities and development efficiency has become an important issue in the field of vocational education. Therefore, we have designed an innovative web front-end development course training platform that combines the automation generation ability of generative artificial intelligence and the fast construction advantage of low code development. In platform design, we fully utilize the automated generation capability of generative artificial intelligence to achieve intelligent generation and layout optimization of front-end page elements. At the same time, with the help of the visual programming interface and component-based development methods of low code development platforms, the front-end development process is simplified, the technical threshold is lowered, and students can focus more on the implementation of business logic and creative expression."
Analysis on the Influence of Supply Chain Optimization and IT Development on Chinese Real Estate,"Zheng, Li and Pornsing, Choosak",10.1145/3695080.3695086,2024,"This article aims to thoroughly study the Chinese real estate industry, emphasizing its essential features, anticipated development patterns, and the use of engineering management concepts to grab potential possibilities. This research utilizes structural equation modeling and hierarchical regression analysis to examine survey data from 144 Chinese companies, aiming to determine the impact of supply chain collaborative innovation on the long-term growth of supply networks. Structural equation modeling (SEM) and confirmatory factor analysis (CFA) are conducted to validate the findings, providing strong corroboration of the results .IT and supply chain advancements have transformed the Chinese real estate industry, enhancing project efficiency, reducing costs, and improving customer satisfaction."
Project Identification and Evaluation: a Case on Business Process Reengineering of Ports Project Workflow Management,"Hernandez, Alexander A and Intal, Grace Lorraine D",10.1145/3695652.3695668,2024,"Ports project management is crucial in economic development of archipelagic country in Southeast Asia to bridge mobilization of human and transportation activities. As such, a developing country such as the Philippines needs well-defined business processes to support ports project identification and evaluation. This paper aims to present a ports project identification and evaluation applying business process reengineering tools, which aims to enhance the efficiency and effectiveness of ports project identification and evaluation processes to its stakeholders. The paper presents the problems identified and analysis, the proposed solutions, and evaluation to aid decision-making and actions towards attainable ports project management. Practical recommendations are offered to further the proposed system design and improvement."
Green transformation of China's cross-border e-commerce under the challenge of carbon tariffs,"Zhang, Shaoru and Yuan, Yan and Ma, Jialan",10.1145/3695652.3695676,2024,"As a type of green trade barrier, carbon tariffs have garnered significant attention in the context of increasingly severe global climate change. With the gradual implementation of carbon tariff policies, China's cross-border e-commerce enterprises are facing dual pressures: rising costs and declining competitiveness. This paper aims to explore green transformation strategies for China's cross-border e-commerce in response to the challenges posed by carbon tariffs. Through quantitative analysis, it is revealed that logistics transportation is a primary source of carbon emissions. The study emphasizes that the implementation of carbon tariffs will significantly increase operational costs and weaken international market competitiveness. To address these challenges, the paper suggests that the government should enhance support and develop comprehensive policies, while enterprises should accelerate their green transformation efforts by optimizing supply chain management, promoting green packaging, and adopting renewable energy solutions. The cooperation between the government and enterprises is crucial to jointly advancing the green transformation of cross-border e-commerce and achieving sustainable development."
Overview and Analysis of the Application of Digital Twin-based Human-Machine Collaboration in the Product Development Process,"Wang, Haoqi and Lou, Gaopeng and Lu, Xiaoping and Dong, Liyang",10.1145/3696474.3696485,2025,"In the era of Industry 5.0, the collaboration between humans and machines is vital for achieving smart manufacturing that is centered on human needs. This necessitates production services that are more efficient, precise, and safe. Digital twin (DT) technology plays a critical role in facilitating this collaboration by providing a virtual representation of physical systems, allowing for real-time data analysis and decision-making. This paper explores how DT-based human-machine collaboration (HMC) can significantly improve the product development process (PDP). By examining the advantages of DT-based HMC, the paper establishes correlations with key factors such as cost, time, and quality, grounded in the ten knowledge areas outlined in the Project Management Institute's Project Management Body of Knowledge. Additionally, the paper analyzes the practical applications of project management principles related to DT-based HMC within enterprises. Through the application of these project management knowledge areas, the research assesses the likelihood of successful implementation of DT-based HMC in the PDP. The findings highlight that integrating DT technology with effective project management practices can lead to substantial enhancements in manufacturing processes, aligning them more closely with human-centric objectives. Ultimately, this study underscores the transformative potential of digital twins in fostering a collaborative environment that improves product development efficiency, quality, and overall competitiveness in the industry."
Data Governance for Design Results of UHV Project,"Feng, Jieting and Si, Haiqing and Dong, Shutao and Han, Xu and Xu, Kun and He, Siyu",10.1145/3696500.3696516,2024,"By analyzing the relevant specifications and requirements of UHV project design management, a governance system for processing UHV project design results data is designed. This paper studies the data standard system suitable for the management of UHV project design results data, establishes a UHV project design data governance platform, realizes the unified and standardized management of design results data, and provides basic data and services for the digital management of various application businesses such as UHV project construction and operation."
Review of Risk Studies in Public–private partnerships (PPP) Projects in China: A Visualization Analysis based on VOSviewer Software,"Yu, Jinghan and Santoso, Djoen San",10.1145/3696500.3696537,2024,"This paper applies VOSviewer software and the Web of Science database as the main source to systematically review the journal literature on risk research of Public-private partnerships (PPP) projects in China over a 10-year period from 2014 to 2023. In this research, the selected literature is analyzed by metrological visualization, which includes the time of publication, both of domestic and international authors and institutions, and research hotspots. It can be found that the number of publications has grown rapidly in China, despite the fact that PPP-related research in China started around 2000, which is a relatively late start compared to the UK and the US. This is particularly reflected in the rapid growth of PPP projects in China since the state launched its PPP incentives in 2014, with the number of publications in 2018 almost equal to the total of the previous four years. In addition. The results show that the research topics such as risk allocation, risk management and infrastructure PPP projects as research focuses, representing the research hotspots and trends in the field of PPP project risk in China."
A Review on the Application Research of Machine Learning in the Evaluation of Scientific and Technological Achievements,"Yan, Ran and Ma, Linru and Zhang, Long",10.1145/3696500.3696580,2024,"In the era of big data, traditional evaluation methods are increasingly demonstrating limitations due to their subjectivity and inefficiency. This study conducts a comprehensive review and analysis of various methods used to evaluate scientific achievements, such as peer review, the Delphi method, and the analytic hierarchy process, highlighting their respective advantages and disadvantages. Building on this groundwork, the study explores the application of machine learning in scientific achievement evaluation, emphasizing its advantages in handling large-scale data and uncovering complex patterns. The research findings indicate that machine learning techniques significantly enhance the objectivity and efficiency of scientific achievement evaluation. There is substantial potential for applying machine learning in this context, and future research should further explore algorithm optimization, cross-domain data integration, and privacy protection mechanisms to achieve higher-level development in evaluating scientific achievements."
A Financial Budget Model Based on Blockchain and Data Mining,"Liu, Chunmiao and Yu, Mingjie",10.1145/3696952.3696994,2024,"In order to further improve the intelligence of the enterprise financial intelligent budget system, this paper constructs the financial budget system based on the concept of big data and the actual needs of the enterprise. By means of information technology, based on project management, guided by performance management, with scientific budget as the main line and internal control as the guarantee, this paper builds an integrated management system of budget performance and internal control that runs through project declaration, budget execution, performance tracking, performance evaluation and supervision, result application, and internal control. Through the comprehensive analysis of the performance test results, it can be seen that the system performance is good in most scenarios, which can meet the current intelligent needs of enterprises for financial budget systems."
The Application of Artificial Intelligence in the Field of Modern Infrastructure Investment,"Zhang, Kai and Zhang, Ting",10.1145/3697355.3697413,2024,"The use of artificial intelligence can effectively assist investment decision-making and risk control in the infrastructure field and is an important direction in the field of modern infrastructure investment. This article explores the application research of artificial intelligence in the field of modern infrastructure investment. This article analyzes the advantages of artificial intelligence in data analysis and prediction, and intelligent generation of innovative content, combined with the overall trend of intensive and efficient, intelligent and green, dynamic risk control, and value-driven modern infrastructure investment, and studies how to give play to the decision-making and analysis advantages of artificial intelligence in investment decisions, project risk control, and resource allocation, as well as how to give play to the innovative value in expanding investment value, optimizing design plans, construction stages, and investment plans. Through cases, it is confirmed that the application prospects of artificial intelligence in the field of modern infrastructure investment are broad, which can bring more intelligent and efficient investment and management models to infrastructure investment projects."
Evaluation and Improvement of the Higher Education Institution's Enrollment Information System,"Tan, Elline Tiffenie So and Tabudlong, Anna Loren Cabonce and Lomantas, Jarese Micah Echiverria and Estember, Rene Duyo",10.1145/3698062.3698084,2024,"The enrollment process is a crucial component of academic institutions, with a direct impact on student satisfaction and academic achievement. This study analyzes the enrollment process and system in the Higher Education Institution in Southern Philippines and provides recommendations to improve efficiency and effectiveness. The study reveals numerous critical recommendations concerning the Higher Education Institution's (HEI's) enrollment process, including issues with room allocation, faculty availability, and student advising. These difficulties contribute to inefficiencies such as grade release delays, erroneous course offerings, and long enrollment queues. Considering these findings, multiple recommendations are offered to improve the enrollment system of the HEI. First, Student-Faculty Advisers (SFAs) should be assigned based on departmental demands and anticipated student volumes. To maximize resource usage, modify the number of SFAs and encourage longer shifts during enrollment periods. Additionally, the utilization of an Enrollment Portal is encouraged to allow students to manage enrollment processes independently. The enrollment portal proposed for the HEI transforms the enrollment process by integrating a variety of capabilities to improve efficiency. It supports academic timetable coordination, curriculum revisions, course adjustments, and faculty feedback. Students obtain the ability to enroll, add/drop courses autonomously, and seek advising when needed. The portal provides secure access and effective task management with configurable interfaces for various offices and users. It provides students with a user-friendly dashboard for registration, billing, scholarship applications, and document uploads, increasing autonomy and convenience. Overall, the portal guarantees more efficiency, fewer manual processes, and better communication among stakeholders, resulting in a more seamless enrolling experience at the HEI."
Enhancing Cybersecurity in the Philippines Healthcare Sector: A Zero Trust Survey,"Blancaflor, Eric and Aguirre, Renz Angelo and Bernardo, Ryoji and Alcoreza, Elmer Gioseppe",10.1145/3698062.3698090,2024,"The healthcare sector in the Philippines is slowly digitizing, with electronic health records and a proliferation of connected medical devices — giving rise to new cybersecurity challenges that traditional perimeter-based models are ill-prepared for. In this paper we investigate a novel security framework referred to as Zero Trust (ZT), in contrast with traditional models ZT treats all network traffic non-trusted, which necessitate unceasing confirmation of users, devices and resources. The paper also demonstrates how the research respond to cyberattacks like the ransomware incident with PhilHealth and suggested that Zero Trust has potential in improving healthcare security. However, Zero Trust has not been widely adopted in the Philippine healthcare sector largely because of its newness and also shortage of locally customized studies. The paper concludes with recommendations for increased studies and implementation of Zero Trust in the Philippines to improve data protection and cybersecurity in healthcare."
Analyzing Use Intentions for Health-Diagnostic Chatbots: An Extended Technology Acceptance Model Approach,"Pang, Wei Ming and Liew, Tze Wei and Tan, Su-Mae and Teo, Siew Chein and Lee, Yi Yong and Lim, Tze Qing",10.1145/3698062.3698093,2024,"This study investigates the factors influencing the intention to use AI-powered health diagnostic chatbots. A research model was proposed based on the Technology Acceptance Model, subjective norms, perceived trust, perceived risk, and self-efficacy. Using Partial Least Squares Structural Equation Modeling, the study assessed the model with 274 valid responses. The results revealed that perceived usefulness, subjective norms, perceived trust, and self-efficacy significantly influence the intention to use AI-powered health diagnostic chatbots. However, perceived ease of use and perceived risk did not impact the intention to use such chatbots. The study also discusses how these findings can assist developers in promoting the long-term adoption of AI-powered health diagnostic chatbots among potential users."
Online Information Disclosure and Information Privacy Practices During Significant Life Transitions: A Scoping Review,"Gibson, Ryan Colin and Meiklem, Ramsay and Moncur, Wendy and Ruthven, Ian",10.1145/3698204.3716445,2025,"Significant life transitions are difficult and stressful periods, where personal identity and relationships undergo change. Accessing information and social support during these times requires disclosure: we must reveal information about ourselves to gain useful information. Yet, managing disclosure of information online during these periods presents great challenges and burdens to the central actor. We therefore sought to identify the information disclosure practices of people undergoing diverse selected life transitions (leaving the Armed Forces; relationship breakdown; coming out as LGBTQ+; living with cancer) with respect to online privacy and its implications for the design of information access systems. We achieved this via a scoping review, searching across nine databases to identify relevant articles, before carrying out an inductive thematic analysis of the charted data. We surface a key common disclosure strategy of multiple temporal selves, whereby people undergoing life transitions present multiple situated identities reflecting both their past and emerging identities, bound to desired levels of disclosure within specific online spaces/networks. This strategy is transacted via nuanced use of privacy features online, which calls for a high level of digital literacy. Our resulting design recommendations offer opportunities for information systems to support people in: safely partitioning their identity across new, and within existing, online spaces; creating spaces for personal reflection; and having a more informed awareness of the depth of information shared. The need for deeper understanding of the privacy requirements of two under-studied groups - leaving the Armed Forces, and living with cancer – is also highlighted."
Collaborative image data interaction in social science and humanities research tasks,"Late, Elina and Matres, In\'{e}s and Sendra, Anna and Kumpulainen, Sanna",10.1145/3698204.3716451,2025,"The purpose of this study is to investigate collaborative image data interaction in social science and humanities research tasks. Since the use of images as research data has expanded during the last decade, we necessitate investigating interactions with image data to support scholarly work better. As earlier studies have focused mostly on collaboration between scholars or collaboration during some specific activity, this study aims to provide a more holistic understanding by covering several types of collaborative activities and actors throughout the data interaction process. The mixed methods analysis is based on qualitative interviews with 21 scholars who utilize images collected from external sources. During the interviews, scholars provided accounts of image data uses in previous or ongoing research tasks. These include tasks conducted alone (n=4), in loosely (n=10) and tightly (n=7) knit groups. The findings indicate that image data interaction is collaborative by nature and takes forms of contributions, coordination, cooperation, and collaboration as presented in the model by Shah [53]. The share of contributions is high, as scholars often receive image data from study participants and organizations. Collaborative activities are done with internal academic, external academic, and non-academic actors, but their roles vary depending on the type of collaborative activity. Collaborative activities take place throughout the image data interaction process and happen particularly during the data gathering and synthesizing and reporting stages. Future research should aim to integrate collaborative activities into models of data interaction, as they represent fundamental ways of working regardless of the type of the research task."
Seeking Control: How Women Evaluate and Use Menopause Related Information,"Emter, Francesca and Chavula, Catherine",10.1145/3698204.3716455,2025,"Studies on healthy ageing have shown that access to information can improve women’s experiences during menopause. However, due to socioeconomic barriers and the quality or complexity of the available menopause information, women lack adequate information about menopause. Previous reviews have critically analysed women’s preferences for menopause information sources, the factors influencing their decision-making, and the menopause topics they seek information about. Yet, there is limited understanding on why women select certain sources or how they use them. This leaves a gap in evidence for information providers who wish to create accessible and informative menopause-related resources. In this paper, we present a thematic synthesis of 53 studies that empirically examined how women interact with information about menopause to identify information use and evaluation practices. Our synthesis identified seven distinct ways women use information about menopause and four methods that are used to evaluate these resources. We also highlight how women’s information practices are often negatively enacted upon by taboos, sexism and racism. In this light, we centrally found that women use information to gain control over their menopause experiences and evaluate the measure of control information sources provide, seeking out empowering, tailored and personalisable sources. Finally, we propose a set of recommendations that emphasises the importance of considering information practices, particularly information use when designing information access systems or creating information sources on menopause."
On the Adoption of Open Source Software Licensing - A Pattern Collection,"P\'{\i}cha, Petr and Serbout, Souhaila",10.1145/3698322.3698341,2024,"In the evolving landscape of software development, the adoption of open-source software (OSS) licensing has emerged as a pivotal trend, reshaping the way organizations, developers, and users interact with software. The notion of open source, predicated on principles of collaboration, transparency, and accessibility, stands in stark contrast to proprietary models, offering a unique set of advantages and challenges. This paper presents a collection of thirteen patterns that underpin the adoption and implementation of OSS Licensing in various contexts. Five of these patterns are described in full as a first step of forming a proper pattern language in this domain: The Open Source License Selection pattern guides choosing the right license, supported by License Education to ensure stakeholders understand the implications. Permissive Licensing promotes flexibility and broad adoption, while Copyleft Encouragement maintains the openness of derivative works. License Enforcement ensures compliance with selected licensing, safeguarding adherence to its terms. The remaining eight patterns are listed as patlets. For OSS practitioners, these patterns provide a basis for a balanced approach to open-source license management, supporting openness while maintaining necessary controls."
Community-Driven Learning: Two Case Studies,"Plummer, Tsvetelina and Sauermann, Victor and Akvardar, Berrin",10.1145/3698322.3698352,2024,"In our pattern from EuroPLoP ‘23 we’ve introduced the Community-driven Learning pattern. Since then, our experience with this pattern has evolved as we have been able to further iterate on it in different contexts. We’ve gained some new insights into how to better introduce and maintain the pattern into a company as well as how it can help during company transition times such as mergers and acquisitions. Also, we’ve gained insight into another example for this pattern in a different software company, doing its own flavour of Community-driven Learning. During the 2023 conference the main question we were asked was how we are able to introduce and establish a Community-driven Learning pattern within a company. So, given the new insights we’ve gathered, we believe it is quite valuable to shed more light on some intricacies and “secret ingredients” that make these pattern work in two Case Studies - GfK - an NIQ Company [5][29] and EPAM Systems [23][28]."
AI Future Envisioning with PLACARD,"Tedeschi, Mary and Ricaurte, Paola and Ayloo, Sridevi and Corneli, Joseph and Danoff, Charles Jeffrey and Belich, Sergio",10.1145/3698322.3698444,2024,"At EuroPLoP 2024 Mary Tedeschi led the “AI Future Envisioning with PLACARD” focus group in Germany. Three conference attendees joined in the room while Sridevi, Paola, and Charles co-facilitated remotely via a web conference. The participants were introduced to a Futures Studies technique with the goal of capturing envisionments of Artificial Intelligence (AI) going forward. To set an atmosphere a technology focused card game was used to make the session more interactive. To close everyone co-created a Project Action Review to recap of the event to capture learnings that has been summarized in this paper. The Focus Group was structured based on lessons learned over six earlier iterations."
Unrestricted Use of LLMs in a Software Project Course: Student Perceptions on Learning and Impact on Course Performance,"Korpimies, Kai and Laaksonen, Antti and Luukkainen, Matti",10.1145/3699538.3699541,2024,"Large language models (LLMs) provide round-the-clock personalized programming assistance, unlike course instructors or traditional online information sources such as Stack Overflow. While LLMs can aid in code generation, concerns about over-reliance and the impact on learning persist. This study discusses students’ experiences with LLMs in a software project course where students were allowed to use LLMs freely except for unit test generation. We conducted surveys during course instances in autumn 2023 and spring 2024. The surveys assessed the extent of LLM usage, methods of application, and perceived impact on learning. Results indicate diverse usage patterns, with many students finding LLMs beneficial for efficiency and problem-solving, though over-reliance and poor-quality outputs were noted concerns. The usage patterns can be linked to course performance and time spent on the project."
Which teamwork challenges do computing students face in a project-based learning course in research methods?,"Moalagh, Morteza and Hussain, Syed Sajid and A. Farshchian, Babak and G. Selassie, Samrawit",10.1145/3699538.3699563,2024,"Courses in research methods are crucial for developing computing students’ skills, such as critical thinking, analyzing practical evidence, and applying theoretical insights. However, traditional lecture-based approaches often result in low student motivation and engagement. Project-based learning (PBL) offers an alternative by involving students in realistic, team-based projects, yet its effects on learning processes still need to be explored. This paper critically examines the challenges of teamwork that computing students face in a PBL research methods course. Through a case study involving master’s students in computer science by reviewing reflection reports and conducting semi-structured interviews, we extracted and categorized a comprehensive list of challenges using the Holz framework. Our findings indicate that introducing PBL as a teaching approach in a research methods course may result in unique challenges for computing students. Firstly, some teams may have been following the latest technologies and trends while selecting topics for their research projects, resulting in a lack of practicality. Secondly, since some team members may have been more interested in developing software solutions instead of using a practical problem to build an empirical research project, their teams may have selected incorrect research strategies. Thirdly, some teams used an individual incremental approach instead of a collective incremental and iterative approach in their research projects, which may have negatively affected their team activities. We also proposed adding a new layer, ""How to do a research project in a team?"" to the existing Holz framework to address planning and collaborating challenges in the PBL-driven research methods course."
Personalized STEM Learning Pathways in Higher Education with Recommendation Systems: A Case Study of Signals and Systems in Biomedical Engineering,"Xi, Jianing and Shi, Wen",10.1145/3700297.3700312,2024,"The reliance on imported high-end medical equipment limits our country's technological independence and the availability of advanced medical services. Traditional medical education lacks essential STEM components, leaving students unprepared for today's advanced medical technologies, and failing to address diverse student backgrounds and interests. This paper introduces ""Personalized STEM Learning Pathways,"" a personalized, case-based teaching approach that tailors lessons to students' needs through the use of intelligent recommendation systems and integrates real-world biomedical engineering cases. The recommendation system customizes learning materials and resources, significantly improving academic performance, motivation, and engagement, with the experimental group outperforming the control group in all metrics. By fostering interdisciplinary application abilities, deep thinking, and innovation, this method enhances educational quality and student satisfaction. The findings support broader implementation of personalized, case-based teaching methods in biomedical engineering education, promoting educational equity and fostering a more inclusive society."
A Systematic Review on Integration and Innovation of Virtual Reality (VR) Technology in Higher Art Education,"Xie, Tianyi",10.1145/3700297.3700322,2024,"Objective With developments of technology, Virtual Reality (VR) technology has provided innovative teaching methods and learning experiences. This systematic review aims to explore in-depth the application of VR technology in higher arts education and its influence on teaching, training, research and artistic practice. Methods&nbsp;The multiple applications of VR technology in higher arts education were identified. These include enhancing learning experiences, simulating real environments and boosting creativity through a comprehensive analysis of studies published from 2020 to2024 in the databases Web of Science, ProQuest, Wiley Online Library and Scopus. The literature search strategy employed Boolean operators to combine the keywords “Virtual Reality”, “Arts Education” and “Higher Education”. Inclusion and exclusion criteria were developed based on the PRISMA guidelines. Results&nbsp;The results indicated that the application of VR technology in the educational sector is gaining increasing attention, but its usage in higher arts education programs remained relatively limited. VR technology showed a positive influence on the development of higher arts education in higher institutions, particularly in stimulating student interest and improving learning efficiency. However, the application of immersive VR technology in arts education was still in its early stages, indicating a vast potential for research and application in this field. Conclusion&nbsp;This review emphasizes the importance of VR technology in arts education and highlights its potential to enhance students’ learning experiences and creative abilities. It is recommended that higher education institutions increase their investment in VR technology and provide necessary training and support for arts educators. Additionally, the study reveals existing challenges in the application of VR technology, such as technical issues cost limitations, and student adaptability, which require further exploration in future research."
Score evaluation of graduation projects using fuzzy Petri-net,"Zhou, Jianfeng",10.1145/3700297.3700340,2024,"Graduation project is an important stage before the graduation of the students in most Chinese technology universities. In view of the arbitrariness and unilateralism in graduation project performance evaluation, a novel evaluation and scoring approach using fuzzy Petri-net is proposed. In accordance with the requirements of graduation project evaluation in a university of China, factors impacting on the score of graduation projects are discussed, and relationships of these factors are modeled by the weighted fuzzy Petri-net. The evaluation result can be obtained through the fuzzy reasoning of WFPN, and the graduation project score is determined by the defuzzification. The proposed approach takes into account the graduation project process, the graduation thesis and the graduation defence. It links the qualitative evaluation of daily work with the final quantitative results, so that it can fully reflect the students' performance and achievements in a graduation project, and can be used to determine a reasonable score of the graduation project. An example illustrates the proposed graduation project evaluation approach."
Opportunities and Challenges in the Cultivation of Software Development Professionals in the Context of Large Language Models,"Chen, Ping and Alias, Syazwina Binti",10.1145/3700297.3700342,2024,"In the context of the rapid development of Large Language Models (LLMs), the field of software development has undergone significant transformations presenting both opportunities and challenges for software development professional cultivation. This study systematically analyzes the applications of LLMs in software development and their impact on this cultivation, exploring the opportunities and challenges in enhancing programming efficiency, promoting personalized learning, improving interdisciplinary skills, and addressing over-reliance on LLMs and related tools. Through literature analysis, this study reviews the impact of LLMs on programming efficiency, code quality, and project management and evaluates the requirements and directions for professional cultivation in response to these changes. The research results indicate that while LLMs bring numerous opportunities, they also pose challenges such as rapid technological updates and a tendency toward over-reliance on tools. Therefore, this study proposes a series of optimized professional cultivation strategies to adapt to the technological developments and industry demands of the new era, thereby enhancing the capability of higher education institutions to cultivate software professionals who meet future needs."
Research on the Teaching Mode of Laptop Computer,"Zeng, Feng",10.1145/3700297.3700347,2024,"Most computer-related courses in higher education institutions are taught using the traditional teaching model of theory classes and lab classes separated. Students rely solely on memory during theory classes, and then practice their skills during lab classes. This leads to difficulties in understanding and applying theoretical knowledge. In response to this situation, this paper proposes a teaching model based on laptop computer classroom, which combines theory classes and lab classes. This ""learn and practice immediately"" teaching method is beneficial for improving student's learning efficiency and helps teachers understand the actual grasp of classroom knowledge by students and adjust teaching content and progress based on feedback. In the ""Computer-Aided Design"" coursers, laptop computer teaching mode have been practiced for many years, which can effectively enhance student's interest and participation, provide more efficient and comprehensive learning experiences for students, and bring comprehensive improvements to the quality of college teaching."
Ethical Dilemmas in the Integration of Artificial Intelligence in ESL Education Within Chinese College Settings: A Systematic Review,"Shi, Jingjing and Narasuman, Suthagar and Ning, Huichun and Grigoryan, Gevorg and Ren, Wenxuan",10.1145/3700297.3700357,2024,"The research on the moral conundrums linked with using AI in ESL instruction in Chinese higher education is checked critically in this systematic review. Web of Science, Science Direct, and Springer were among the data sources for this review. The paper summarizes research on the difficulties AI has while reading ESL text, emphasizing the need for high-quality training data and algorithm development. Although AI has the ability to provide individualized learning experiences, there are unknowns when it comes to data that is outside of its training set. The review highlights the need for more thorough research by exposing a lack of high-quality evidence on the results of AI integration. While AI is recognized for its ability to track student progress and provide feedback, the effectiveness of these efforts depends on the choice of suitable AI-driven learning models. The findings suggest an imperative for enhanced methodologies and a call for educators to judiciously implement AI in ESL education, addressing ethical considerations and ensuring the quality of student learning experiences."
The Impact of Business Simulation Software on Students’ Perceived Learning Outcomes,"Cai, Zhaoying and Phongsatha, Satha",10.1145/3700297.3700364,2024,"The development of enterprises is shaped by numerous management decisions at all levels, requiring specific skills. In response to these needs, new courses are being designed that utilize more effective tools and simulations. This study aims to examine the determinants that influence students’ perceived learning outcomes using business simulation software in a management accounting course. Data were collected from management accounting students at a private university in Guangdong, using an attitude test questionnaire as the research instrument. The study found that factors such as experience creation, theoretical comprehension, skills enhancement, and emotional assessment positively influence students’ perceived learning outcomes. This research will assist educators and academics in identifying factors that enhance student learning outcomes in management classes using simulation software, particularly during the coronavirus pandemic."
Advancing AI-Driven BETC Models for Transformative Educational Data Mining and Learning Analytics,"Chen, Zhiqiang and Chen, Yanru",10.1145/3700297.3700382,2024,"This research paper presents an innovative integration of Artificial Intelligence (AI) with the British Technology Education Council (BETC) method, specifically tailored for civil engineering education. The study introduces an AI-driven BETC model that integrates real-world engineering data, industry standards, and virtual construction technologies to significantly enhance learning efficiency and teaching feedback precision. The model was rigorously evaluated through quantitative metrics, demonstrating a substantial increase in pre-class task completion rates from 30% to 87%, accuracy of achievements from 38% to 82%, and student satisfaction from 56% to 97%. These results not only validate the effectiveness of the AI-BETC model but also highlight the transformative potential of AI in educational data mining and learning analytics within the civil engineering sector. The study's findings underscore the importance of adopting advanced technologies in educational practices to improve outcomes and prepare students for the challenges of the future."
Exploration and realization of the digital transformation of the whole process of digital ecological business LTC,"Yu, Kefeng and Zhang, Liping",10.1145/3700706.3700726,2025,"The digital revolution brings opportunities and challenges. Operators are the builders and operators of network infrastructure and are at the forefront of digital transformation. In order to adapt to the development of the digital economy era, various operators have launched the marketing of digital ecological products, and proprietary service products have become the key to sustainable growth. This paper proposes digital transformation based on the existing LTC full-process management. Taking the digital ecological business as an example, a full-service LTC full-process digital platform is designed to promote the transformation of full-service marketing. This article mainly discusses the application of digital and intelligent technologies in all stages of the LTC process, such as business opportunity capture, pre-sales support, in-sales support, rapid project delivery, and after-sales service experience optimization, etc., to help operators deeply understand users' expectations for professional products. Demand, provide solutions that are close to user needs, maximize the potential of professional products, and achieve rapid value growth."
An In-Depth Analysis of Artificial Intelligence on Service Capabilities of Humanoid Robots,"Samonte, Mary Jane C. and Viera, Rafaello Jose M. and Tupas, Jan Edgar E. and Sabilala, Allen Kyle D. and Tejada, Ervin C.",10.1145/3700706.3700730,2025,"Artificial intelligence (AI) emulates human intelligence in digital devices and machines. A humanoid robot is designed to imitate the look and form of a human body and is primarily used for humanitarian assistance and interaction. AI is applied to humanoid robots to allow intelligent behavior and decision-making, resulting in the ability to execute service activities and achieve goals intelligently in the human-robot interaction environment. The paper is a systematic literature review of studies published in the last four years that presents and assesses how AI techniques and methods have been applied to humanoid robots to implement and enhance service capabilities. The research papers used for the systematic literature review were carefully selected under the directives of the PRISMA method. Eighty studies were screened, and eighteen were used for the review. The results show that the primary AI technique applied in humanoid robots is the Convolutional Neural Network (CNN). The results show that the effective AI techniques used in humanoid robots based on a specific area are Hybrid Convolutional Neural Network (CNN) – Long-Short Term Memory (LSTM), Artificial Neural Network (ANN), Convolutional Neural Network (CNN), Deep Convolutional Neural Network DCNN), Recurrent Neural Network (RNN), and Grow-when-required Network (GWR). The study provides AI techniques and methods utilized in modern studies that can be used as a reference for future studies, research, and innovation."
Barriers to transformation in the South African banking sector,"Kasse Kengne, Sophie and Twum-Darko, Michael and Oluwaseyi Tokosi, Temitope",10.1145/3700794.3700795,2025,"Transformation was looked at from the socio-economic, political, and technological perspectives as directed by the literature and the views of senior bankers from banking institutions established in South Africa were taken. More specifically, this paper presented the findings based on qualitative inquiry from primary data (interviews), and secondary sources. The secondary sources of data were collected from annual reports, transformation reports, and announcements from the financial institutions, as well as updates from regulatory authorities such as the FSCA and the B-BBEE commission. A pilot study was first undertaken before the main study was conducted, with its findings reported. Seven (7) managers were further interviewed after 4 participated in the pilot study. The study is a combination of data collected from both the pilot and main study. The responses from the managers interviewed and documentary evidence both at the pilot and main study phases reflected the feelings of banks towards the role that BANKSETA play, so these feelings must not be ignored. Identified barriers to transformation included economic inequality, slow ownership transformation, regulatory complexity, resistance to change, skills shortages, ICT infrastructure limitations, and insufficient collaboration. Solutions to facilitate effective transformation involved collaboration between the banking industry and educational institutions to produce quality graduates. Others include enhancing education and customer service, adopting better technology, and addressing consumer behaviour. Proper implementation of banking regulations, such as the EE Act, profit maximisation, and customer satisfaction, were also vital for accelerating banking transformation."
Research on Power Grid Infrastructure Business Management Platform Based on Data Layer Analysis Method,"Song, Baosong and Zhang, Chenyu and Ding, Lei",10.1145/3700906.3700981,2024,"In order to carry out the concept of green development and respond to the construction of digital China, it is imperative to transform the energy industry into digital intelligent development. The power grid infrastructure project is regional and complex, and the traditional information architecture is difficult to support its data management and value realization, the application of multi-source and visual data analysis combining with power grid infrastructure engineering data, geological data, meteorological data and so on is developing gradually. The research of power grid infrastructure business management platform based on data layer analysis method, firstly analyzes the power grid infrastructure business demand, then carries out the application design and the construction function, finally carries on the example study. It realizes the unified structured management and visual display of various types of power grid infrastructure resources, breaks the data barriers, makes the heterogeneous data get fusion analysis and application, releases the value of power grid data elements and helps new power systems construct and develop."
"Integrated Design Method of Development, Operation and Maintenance Security in Public Computing Environment","Yu, Liebin and Wang, Yan and Li, Nanhua",10.1145/3700906.3700982,2024,"The public computing environment is the key technology of the new generation of ship system. The efficiency and quality of its software development will seriously affect the development of combat system. This paper compares the software development models such as waterfall, agile, and DevSecOps, analyzes the benefits of DevSecOps to public computing environment software, gives the life cycle model of public computing environment software development, and selects the tools involved. Finally, the software pipeline of public computing service equipment and display and control equipment is proposed, which provides a reference for the effective landing of DevSecOps in weapons and equipment."
Research on the Impact of Large-Scale Development of Distributed Photovoltaic on Distribution Network Planning and Management and the Innovation of Theoretical Countermeasures,"chen, chi and Tang, Guangrui and zeng, bingxin and zhang, qian",10.1145/3700906.3700986,2024,"The large-scale development of distributed photovoltaic has an important impact on the development planning, construction and transformation, mobilization and operation, safety production and power supply quality, so it is necessary to comprehensively analyze the above impact and do a good job in management response strategies. In order to better cope with the impact of the large-scale development of distributed photovoltaic, this paper puts forward countermeasures from the following aspects, including releasing the open capacity of the distribution network, standardizing the principles of distributed photovoltaic grid connection technology, and formulating the typical distributed photovoltaic network connection scheme."
Design Decision Support System for Big Data Analysis in EPC Projects,"Zhang, Dayong and Lu, Jinlong",10.1145/3701100.3707126,2025,"This study focuses on the testing and validation process of a design decision support system for EPC (Engineering, Procurement, and Construction) projects. A distributed cluster simulating a production environment was constructed to comprehensively test the system's functionality and performance. The functionality tests covered four core modules: data collection, storage, analysis, and decision support. The results indicated that each module's performance exceeded the expected targets. Performance tests simulated different scenarios with concurrent users, verifying the system's scalability and stability. The results showed that the system responded quickly under standard load and remained stable under extreme load. Optimization techniques significantly improved the system's performance in handling complex analytical tasks. User experience tests also received positive feedback from professional designers. Overall, the test results confirmed that the system can effectively support the design decision-making process in large-scale EPC projects, providing reliable assurance for improving design quality and efficiency. This study offers crucial technical support and empirical evidence for the system's actual deployment and application."
Low-Code Quality Metrics for Agile Software Development,"Domingues, Renato Cavalcanti and Silva, Mario J. and Marinho, Marcelo Luiz Monteiro",10.1145/3701625.3701626,2024,"We present a quality metrics monitoring and analysis platform, which has been developed to support the main goal of delivering customer value at competitive costs without sacrificing quality in the context of fast digital transformation using a low-code platform and agile development methods. The quality platform collects project data and enables the automatic computation of metrics tailored to our software process that development teams can use for self-assessment. Since introducing our platform two years ago, our teams have been using it constantly, demonstrating its effectiveness and importance for our software development process."
Software Development Practices and Tools for University-Industry R&amp;D projects,"Cruz, Dhyego Tavares and Almeida, Erlon Pereira and Santos, Jander Pereira and Paix\~{a}o, Felipe de Sant’Anna and de Santana, Enio Garcia and Gomes e Souza, Rodrigo Rocha and Iwamoto, H\'{e}rsio Massanori and Dur\~{a}o, Frederico Ara\'{u}jo and Serafim Prazeres, C\'{a}ssio Vinicius and Machado, Ivan do Carmo and Figueiredo, Gustavo Bittencourt and Maciel Peixoto, Maycon Leone and de Almeida, Eduardo Santana",10.1145/3701625.3701627,2024,"Research and development (R&amp;D) projects involving universities and industry drive innovation by bringing scientific knowledge closer to practical problems. Nevertheless, the partnership comes with challenges, such as high developer turnover, team members with diverse backgrounds and experience levels, and part-time contributors. In this paper, we report experiences in a large R&amp;D project in the smart home field, in which we proposed software development practices and tools with the goal of promoting short feedback cycles and dissemination of best practices. By surveying the developers of this project, we discovered that the practices and tools were generally well accepted and also determined specific areas that need improvement. The insights collected in this study can be used by other teams conducting R&amp;D projects."
Continuous Improvement of the Requirements Engineering Process in a R&amp;D Project,"Silva, Lu\~{a} and Portela, Carlos and Lisb\^{o}a, Rodrigo and Souza, Alinne and Souza, Francisco and Cordeiro, Thiago and Lobato, C\'{e}sar and F\""{u}lber, Heleno",10.1145/3701625.3701628,2024,"This study reports on the experience of improving Requirements Engineering (RE) of a Research and Development (R&amp;D) project aimed at developing a platform for text analysis using Artificial Intelligence (AI). Initially, some issues were identified, including unclear requirements, ineffective communication among stakeholders, and inadequate documentation. To address these challenges, we implemented the expected results of the Reference Model for Software Process Improvement (MR-MPS-SW) to guarantee quality in the RE practices followed. The results indicate a significant reduction in requirement changes, increased stakeholder satisfaction, and improved clarity of documentation. Finally, it is concluded that the implemented improvements contributed decisively to the project’s success, providing valuable insights for future R&amp;D projects."
Accelerating the software development process through the application of MVP and monolithic architecture,"Lima, Rayfran Rocha and da Silva, Alexandre Teixeira",10.1145/3701625.3701632,2024,"CONTEXT: The growing demand for efficiency in software development and rapid value delivery has led organizations to explore diverse architectural and methodological approaches. In particular, adopting Minimum Viable Product (MVP) strategies and monolithic architecture has shown promise in accelerating the development process, especially in environments where resources and time are constrained. GOAL: This paper presents a detailed experience report on the implementation of MVP and monolithic architecture in a software development guild within a global software company. The main objective is to evaluate the impact of these approaches on development speed, team productivity and overall quality of the production of complementary software, which can be reused via API by other systems within the organization. METHOD: The evolution of the architectural and methodological approaches went through three phases, spanning over 18 months, involving the transition from unstructured development practices to standardized MVP and monolithic methodologies. Data were collected through direct observations, document analysis, and interviews with team members and users. RESULTS: MVP and monolithic architecture implementation significantly reduced development time and improved team productivity. The structured approach facilitated better resource management and enabled faster iteration based on user feedback. However, challenges related to system scalability and maintainability were noted. CONCLUSION: The findings suggest that while MVP and monolithic architecture can accelerate the software development process and enhance team efficiency, careful consideration must be given to the long-term implications on system scalability and maintainability."
Increasing Test Coverage by Automating BDD Tests in Proofs of Concepts (POCs) using LLM,"Santos, Shexmo Richarlison Ribeiro dos and Fernandes, Raiane Eunice S. and Santos, Marcos Cesar Barbosa dos and Soares, Michel S. and Rocha, Fabio Gomes and Marczak, Sabrina",10.1145/3701625.3701637,2024,"In today’s landscape, software manufacturers must deliver quickly and with high quality to remain competitive, especially in the cybersecurity sector. Recognizing this need, we have recently implemented several strategies to accelerate our time to market without compromising quality. We introduced the Proof of Concept (POC) and Proof of Value (POV) stages in the Enterprise Architecture team before initiating inception processes for Minimum Viable Products (MVP) and new features. Initially, these proofs focused only on concept validation. However, since 2023, due to the growing need for rapid and high-quality innovation, POCs/POVs have begun to include robust implementations. We adopted the Behaviour-Driven Development (BDD) approach to define user stories and acceptance criteria, which provided a solid evaluation of POC/POV quality and involved the implementation team from the outset. To prevent products from incurring technical debt, we implemented AutoDevSuite, which uses LLM to generate tests based on user stories and acceptance criteria automatically. We used AutoDevSuite in a POC/POV of a cybersecurity product, and the results showed a significant expansion in test coverage, aligned with the acceptance criteria, demonstrating the tool’s effectiveness in automating and improving test quality."
Towards Role Definition in Agile AI-based System Development: Perspectives and Reflections,"Souza, Francisco Carlos Monteiro and Souza, Alinne Cristinne Corr\^{e}a and Amorim, Bruno Faustino and Cordeiro, Thiago Damasceno",10.1145/3701625.3701661,2024,"With the evolution of AI, software engineering practices must adapt to the unique demands of AI-based systems, ranging from standalone intelligent agents to complex machine learning applications in fields like healthcare and autonomous vehicles. This paper investigates the roles required for AI-based system development from literature and professionals’ perception and how we can integrate these roles with Scrum teams focusing on intelligent agents for multi-agent systems, pattern recognition systems, and evolutionary computation for optimization problems. Initially, a integrative review was conducted, and a survey was applied with 11 AI professionals allocated to different organizations. Our results indicate partial alignment of AI-based system development teams with existing literature regarding roles and responsibilities, highlighting the need for formalized collaborations."
How Software Industry Specifies Requirements Compliant with Data Protection Laws: a survey-based study,"Netto, Dorgival Pereira da Silva and Silva, Carla and Ara\'{u}jo, Jo\~{a}o",10.1145/3701625.3701663,2024,"[Context] There are few studies focused on discovering the state of practice related to how Information Technology (IT) industry achieves legal compliance in software requirements activities. A previous work reported an interview-based study with seven practitioners from seven IT companies tackling with legal compliance in software requirements specification (SRS). As a result, a initial theory emerged from the interviews and explains a set of factors influencing the work practices used by public and private companies to achieve requirements specification compliance with data protection laws. [Objective] This study reviews and improves the initial theory with information obtained from 39 practitioners regarding how they produce requirements specifications compliant with data protection laws. [Method] We designed a survey protocol that contains an questionnaire composed of a set of propositions inferred from the previous interview-based study and the related literature. [Results] Findings reveal that legal requirements are specified textually and the techniques that help achieve legal compliance are basic knowledge about law for software engineers, training in ambiguity identification techniques, assigning a person for tracing laws and legal regulations, identifying relevant laws and legal regulations to be analysed by lawyers and defining a glossary for all domain-specific concepts and acronyms. [Conclusion] The factors and actions that emerged in this study can be used by researchers and practitioners to leverage the methods and tools they develop or use to specify system requirements that must comply with data protection laws."
Mining Discussions on Software Migration: A study of the Boost mailing list regarding C++ code evolution,"Carvalho, Pedro V. R. de and Bonif\'{a}cio, Rodrigo and Lucas, Walter and Mota, Alana Paula Barbosa",10.1145/3701625.3701665,2024,"Programming languages are evolving faster than ever before. New versions of mainstream programming languages (e.g., C++, Java, and JavaScript) are being released with increasing frequency, posing an elevated challenge for software developers as their systems are more easily affected by obsolescence. Software migration is far from trivial. Although there is literature on software migration methods and how developers deal with the software aging and obsolescence, little research exists on how developers perceive and are affected by rapid programming language evolution. To understand how C++ developers discuss these issues and the nature of their discussions, we mined the mailing lists of the Boost organization—one of the most important C++ open-source communities. We found that software migration is a significant concern for this community, with a lasting presence in their message boards. Furthermore, most discussions related to the challenges of the migration process, with many conflicting opinions on related matters, suggesting these issues are not easily solvable."
Technical Debt Management in Agile Software Development: A Systematic Mapping Study,"Leite, Gilberto de Sousa and Vieira, Ricardo Eug\^{e}nio Porto and Cerqueira, Lidiany and Maciel, Rita Suzana Pitangueira and Freire, S\'{a}vio and Mendon\c{c}a, Manoel",10.1145/3701625.3701669,2024,"Context: Agile methodologies use continuous delivery and adaptability to develop software that meets the needs of its users. However, such methods are prone to accumulate technical debt (TD). Agile teams must balance the benefits and risks of incurring debt by managing TD items. Knowing how TD management is conducted in agile software teams can help agile practitioners increase their ability to handle TD items. Aims: To investigate, based on the state of the art, how agile software teams have managed existing TD items in their projects. Method: We carried out a systematic mapping study covering 39 articles from 2010 to 2023. Results: Among the agile methodologies, Scrum was the most used for TD management. Regarding practices, agile teams employ mostly user stories and sprint backlogs to identify TD items. Sprints and sprint backlogs are used to monitor debt, while refactoring is applied to prevent and repay TD items. Conclusion: This study maps current knowledge on TD management in agile methodologies, serving as a starting point for new investigations in the area."
Using Extension Projects to Improve Software Engineering Education and Software Quality: The Experience of the “Ricardo de Almeida Falbo” Software Engineering Practices Laboratory,"Barcellos, Monalessa P. and Silva Souza, V\'{\i}tor E. and Costa, Patr\'{\i}cia Dockhorn and Aguiar, Camila Zacche de",10.1145/3701625.3701680,2024,"Software Quality (SQ) is a fundamental aspect of Software Engineering (SE) and is approached in different ways and disciplines in Computer Science undergraduate courses. Considering the nature of SQ, its learning should be based on practical experiences. Recently, the curriculum guidelines for undergraduate courses in Brazil were changed to require that 10% of the course workload be dedicated to extension activities. Extension plays a crucial role in students’ education, as it aims to provide benefits for society and contribute to a more comprehensive education, including social aspects in addition to technical ones. Considering this scenario, the “Ricardo de Almeida Falbo” Software Engineering Practices Laboratory (LabES) was set up at DI/UFES. It aims to apply SE methods, techniques, and practices to enable students to produce quality software, bringing their education closer to the needs of the various productive sectors and producing results for society. In this paper, we introduce LabES, explain how it works, present some of the produced artifacts and projects, and share lessons learned."
Integrating ChatGPT in Project Management Education: Benefits and Challenges in the Academic Environment,"Oran, Ana Carolina and Montenegro, Let\'{\i}cia Braga and Schuster, Hellmut Alencar and Duarte, Jos\'{e} Carlos and Silva, Williamson and Lima, Rayfran Rocha",10.1145/3701625.3701684,2024,"CONTEXT: Teaching project management is complex, and students often do not feel engaged or motivated. Professors can use many initiatives to improve the teaching and learning process. Tools like ChatGPT, when integrated into education, have generated considerable interest due to their potential to enrich students’ learning experiences. GOAL: This paper analyzes the impacts of using ChatGPT as a complementary tool in teaching Project Management in the Software Engineering course, highlighting its benefits and challenges. METHOD: We performed an exploratory study to identify the effects of using ChatGPT in teaching project management, evaluating learning, productivity, teamwork, student perceptions, and future expectations. RESULTS: The results indicate that ChatGPT contributed to improving content comprehension, developing critical skills, accelerating production, improving collaboration and communication, and increasing student engagement. However, challenges related to misuse and dependence on the tool were also identified. CONCLUSION: The integration of ChatGPT in teaching project management has shown promise, promoting a richer and more collaborative learning experience. The insights obtained provide directions for future implementations and research on the use of AI in project management education."
Industry Academia and Government Collaboration to Reduce Gaps in Software Engineering: Applications for Students and Professionals in Career Transition,"Marques, Denis and Rocha, Richard and Santos, Bruna and Pacheco, Felipe and Rodrigues, Cleyton Mario de Oliveira and Santos, Wylliams Barbosa",10.1145/3701625.3701690,2024,"The development of skills in Software Engineering disciplines often faces challenges when applied to real projects in the software industry. An effective practice to overcome these difficulties is collaboration between industry and academia. This collaboration aims to strengthen the exchange of knowledge between the two contexts, promoting the advancement of industry through Research and Development (R&amp;D), and in academia, applying knowledge in real contexts and even funding research. This study explores application development and the perceptions of undergraduate, master’s and doctoral students in Computer Engineering, as well as professionals in career transition (with Now and Low Code profiles), about projects with real clients and collaboration policies between industry and academia."
A Multimethod Study of Test Smells: Cataloging Removal and New Types,"Soares, Elvys and Ribeiro, M\'{a}rcio and Santos, Andr\'{e}",10.1145/3701625.3701699,2024,"Test smells are signs in the test code that can indicate potential design or implementation issues. Despite being the subject of extensive academic literature, several questions about their impact in the industry remain unanswered. Some key areas lacking clarity include the absence of a publicly available catalog compiling their types, the lack of correspondence between new test framework features and their ability to prevent or refactor test smells, and the limited knowledge about test smells in manual test suites. To address these knowledge gaps, a multimethod study was conducted. It involved a Multivocal Literature Review, surveys with software testing professionals, studying new test framework features, contributing to popular open-source software, and analyzing manual and automatic test suites. The study resulted in: (i) a catalog that consolidates 480 distinct test smells, (ii) confirmation that new test framework features can refactor and prevent smells, and (iii) eight new smells for manual tests, (iv) a tool based on Natural Language Processing to identify such smells, and (v) their frequency in the tests of the Brazilian Eletronic Voting Machine, the Ubuntu OS and a large smartphones manufacturer. The findings of this work also provide guidance for further study fronts on test smells."
GranDIHC-BR 2025-2035 - GC1: New Theoretical and Methodological Approaches in HCI✱,"da Silva Junior, De\'{o}genes P. and Alves, Daniel Domingos and Carneiro, Nayana and Matos, Ecivaldo de Souza and Baranauskas, M. Cec\'{\i}lia C. and Mendoza, Yusseli Lizeth M\'{e}ndez",10.1145/3702038.3702054,2024,"This Grand Challenge highlights that the HCI field is going through a time when it needs to revisit and find new theoretical and methodological foundations to account for the phenomena it encompasses. Thus, it addresses philosophical aspects of the field, encompasses studies that aim to inform and enrich the understanding of human-computer interaction, and highlights the need to bring, experiment with, and conceive new ways of understanding HCI and acting in it. In addition to the application of multidisciplinary theories from anthropology, linguistics, psychology, design, sociology, and other areas, this Grand Challenge exposes the need to investigate other philosophical positions that have been little explored until now, such as those of a phenomenological nature, the creation of theoretical models, the application of existing theories in new ways, or the synthesis of several theories from a new perspective aiming to describe, explain, or inform the field. Consequently, it includes new methods and techniques for data collection and analysis, design approaches, or evaluation strategies that can be applied to HCI research and practice. This paper is presented bilingually in English and Portuguese as a testament to its situated and universal implications."
Nudge Evidence Briefing: A Proposal for Transferring Scientific Knowledge about Nudges,"Gomes, Vinicius and Cunha, Jos\'{e} Adson and Lima, K\'{a}ssio and Moura, Hermano Perrelli",10.1145/3702038.3702073,2024,"A nudge is a concept from Behavioral Economics and Psychology that refers to any small change or intervention designed to influence people's behavior predictably, without restricting their options or significantly altering their incentives. The research follows the Design Science Research methodology, introducing Nudge Evidence Briefing (NEB) to facilitate the understanding, access, application of academic findings on nudges for non-academic professionals, considering the gap between academic research on nudges and their practical application. Leveraging insights from the Evidence-Based Medicine framework, NEBs distill key findings from primary research into concise, accessible documents. Through a systematic review of the literature on nudge integration into software privacy and security, 12 primary studies were selected and the data extracted from them was formatted into NEBs. Participants, specialists and non-specialists, were invited to evaluate the NEB through online questionnaires. Feedback highlighted the clarity and structured format of the NEB, with particular praise for its ability to communicate complex scientific evidence in an accessible way. Overall, the NEB demonstrates significant promise in making nudge-related research more accessible and feasible. Ongoing refinements based on participant feedback will be crucial to realizing its full potential, contributing to the advancement of Human-Computer Interaction (HCI) and the practical application of nudges in professional environments. Future work will focus on evaluating the practical applicability of the NEB with non-academic professionals, exploring more reliable alternatives for generating NEBs through LLM, and developing a comprehensive repository of NEBs."
Revisiting visual accessibility with non-textual content: challenges and solutions for human-computer interaction,"Vargas, Pablo Nunes and Trevisan, Daniela Gorski",10.1145/3702038.3702105,2024,"Content like complex graphics, diagrams, and infographics poses significant challenges in terms of description, comprehension, and communication for individuals with visual limitations. Addressing these challenges requires innovative solutions and a deep understanding of the needs of visually impaired individuals. In this context, a systematic literature review was undertaken to explore previous approaches aimed at making visual information accessible to blind individuals, both for understanding and creating such content. The primary objective of this study was to categorize the various types of representations, including graphs, diagrams, geometric shapes, maps, and other visual forms utilized in interactions with blind individuals. The methodology adopted consisted of a mixed-method approach, presenting a selection of studies conducted between 2015 and 2023, following a structured protocol for research selection in major academic databases. Consequently, 82 studies were classified according to type of supported tasks, type of representation, sensory modality, and assistive technologies. As a result, an analysis of co-occurrence was presented through a representative network depicting the primary sub-dimensions. Additionally, a matrix of certainties, suppositions, and doubts was constructed, highlighting the state of the art, through the main findings and challenges reported in the selected studies. The expectation is to collaborate with the academic community by showing possibilities among dimensions less addressed in studies with blind individuals in the non-textual context, and then pointing out issues to be addressed in future research."
The Learning Labyrinth: Integrating Learning Theories in VR,"Jaganov, Timur and Nnadi, Chukwualuka Leonard and Watanobe, Yutaka",10.1145/3702138.3702156,2025,"This study explores how virtual reality (VR) can enhance education by seamlessly integrating the critical principles of behaviorism, cognitivism, and constructivism within the “Learning Labyrinth” immersive learning environment. It aims to create an inclusive, engaging, and captivating educational platform that caters to diverse learning needs and styles by fully leveraging the transformative potential of VR technology. The platform comprises three main interoperable modules: Kiro, an AI-powered voice assistant that provides personalized guidance, support, and feedback to learners; THE Maze LXP, a dynamic content integration system that seamlessly incorporates educational resources from third-party developers; and the HIRO VR Viewer, which offers profoundly immersive viewing and learning experiences. This study showcases the practical application of established learning theories on a cutting-edge VR platform and inspires the vast potential to significantly enhance student engagement, understanding, knowledge retention, and overall learning outcomes. The findings of this research underscore the profound advantages of VR in education and provide valuable insights into the practical implementation of learning theories in technologically advanced, adaptive, and personalized learning environments."
XR Technologies in vocational education and training research (2000-2024): A Bibliometric Review,"Xu, Ye and Mao, Decheng and Wang, Chengliang",10.1145/3702163.3702174,2025,"Extended reality (XR) technology plays a crucial role in vocational education and training in the smart era, and it has a significant impact on the cultivation of talents with innovative and comprehensive skills. We can identify current research hotspots and assess the depth and breadth of existing research so that we can fill the gaps in research in a timely manner. To achieve this goal, we used two software tools, Vosviewer and Citespace, to conduct a detailed multidimensional visualisation and analysis of 240 documents in the Web of Science (WoS) database since the 21st century. Our study reveals the following key findings: 1) The publication volume of the literature has shown a steady growth trend since 2000 to 2019, and has witnessed a significant increase between 2019 and 2024. Meanwhile, author groups such as Cattaneo, Alberto and Hamalainen, Raija are gradually becoming more representative scholars producing more articles.2) The field has clearly fostered three dominant research foci, which are: exploring VET based on an educational psychology perspective, approaching VET from a computer science and technology aspect, and focusing on the use of XR technology devices. 3) XR technology in vocational education and training shows a clear stage-by-stage evolution and is moving towards a deeper level of technological application. Some researchers have begun to focus on the construction of student-oriented educational models as well as teachers' acceptance and use of the technology."
The Role of Artificial Intelligence Tools in Knowledge Generation: Implications for Education,"Pe\~{n}afiel, Myriam Guadalupe and V\'{a}squez-Pe\~{n}afiel, Mar\'{\i}a-Stefanie and Pe\~{n}afiel, Diego Alberto V\'{a}squez",10.1145/3702163.3702180,2025,"The research delves into the transformative potential of artificial intelligence (AI) tools in knowledge generation and their pivotal role in the educational process. It illuminates which AI tools can revolutionize educational activities, harnessing their power through the myriad AI tools available for diverse educational functions and the most popular AI tools that amplify student learning. Insights from both public and private higher education institutions, encompassing students from various semesters, have unveiled significant revelations. A key finding was the students' awareness of the available AI tools and their preferences. These findings underscore the imperative of effectively integrating diverse AI tools within the educational environment to bolster knowledge generation among students, thereby highlighting areas for improvement and potential in educational practices related to AI."
Enhancing Student Engagement in global campuses: A Software Solution,"Pilapitiya, Umanga and Fernando, Ann",10.1145/3702163.3702421,2025,"This study critically addresses the imperative to foster meaningful connections and facilitate collaborative learning experiences in diverse global student communities. We present a specialized Blackboard plugin, employing the Scrum methodology and incremental delivery, to surmount geographical barriers and empower students worldwide. Released in three phases, the software integrates personalized profiles, user-friendly search tools, vibrant discussion forums, and gamification elements in Phase 1. Phase 2 further enriches the platform with an event calendar, multilingual support, and robust reporting features. The comprehensive solution is realized in Phase 3, incorporating an AI tool for content monitoring, feedback implementation, and access to training resources. Rigorous testing and positive student feedback substantiate its outstanding performance, marking a significant stride in global education. Beyond enhancing the educational landscape, this collaborative software signifies the potential for transformative global collaborative learning experiences. Its multifaceted features not only bridge geographical divides but also contribute to shaping a future where collaborative learning thrives on a global scale."
Application of Collaborative Learning in Digital Preservation of Intangible Cultural Heritage—A Case Study of Innovative Practical Courses for University Students,"Zhu, Zhu and Cheng, Li and Guan, Mandan",10.1145/3702163.3702454,2025,"This paper explores the application of collaborative learning in the digital preservation of intangible cultural heritage. It focuses on how modern educational technologies can protect and transmit intangible cultural heritage through innovative practical courses for college students. Conducted over four years (2021-2024), the study employs a “four-dimensional collaborative learning model” that integrates inquiry-based learning, scaffolded instruction, situated teaching, and cooperative learning. This model promotes in-depth student learning and practical application. The data include survey questionnaires and interview materials. Results indicate that collaborative learning enhances students' understanding and interest in intangible cultural heritage and improves their ability to solve real-world problems using modern technologies. The study evaluates the effectiveness of this approach in educational practice and its potential for protecting intangible cultural heritage."
From Traditional to Agile Methodologies in Software Project Management Education: A Case Study,"Santos, Mickael Fonseca and Filipe, Ricardo Angelo and Cunha, Joao Carlos",10.1145/3702163.3702460,2025,"Nowadays, agile methodologies are being increasingly adopted in the software development industry, replacing traditional methodologies. In this way, software engineering courses have been following the industry, and are therefore increasingly teaching students to follow agile methodologies and practices rather than traditional ones. This paper describes and analyzes this transition in a Software Project Management course at a higher-education institution. This experiment took place over two academic years, with Waterfall being used in the first year and Scrum in the second. The Learning Outcomes in both years are the same: to gain competences in managing software projects in small teams; but the steps to reach these competences changed according to the current trends in the area. The results obtained by the students show that, by following Scrum, the students demonstrated being more capable of developing software in teams, focused on the clients, and acquired more knowledge in fundamental areas of software development."
Framework of a PBL tutor system based on the triple helix model in environmental science,"Han, Chong and Li, Fenghua and Wang, Mei and Zhou, Chunfang and Liu, Zhongqiu and Yang, Che",10.1145/3702163.3702463,2025,"The convergence of three factors – namely, the global recession, the emergence of the college aspiration fulfilment industry and the urgent need for technological innovation in the field of national strategy – presents considerable obstacles to the advancement of environmental science. In other words, the demand for environmental protection services is declining as a result of the global economic downturn. The advent of the internet has rendered environmental science an unfavorable choice for those pursuing higher education. National strategies that demand innovation have led to the reconstruction of professional curriculum systems. In order to address the aforementioned challenges, a framework for a PBL tutor system based on the triple helix model in environmental science within the School of Metallurgy at Northeastern University is proposed. This approach will be implemented across disciplinary boundaries. Furthermore, the particular issues or projects may be supplied by industry partners, reflecting the pressing necessity for strategic innovation at the national level. It is projected that the introduction of a problem/project-based learning tutoring system will greatly improve students' ability to oversee projects, especially when it comes to longer-term vocational training. This is consistent with the knowledge production theory of the triple helix model and meets the requirements of undergraduate employment skills."
Research and Application of MSC Integrated Intelligent Platform for Software Development and Operation in Large-scale Software Engineering,"Gao, Jinming and Zhu, Pingfei and Xiong, Genxin and Shu, Yanfei",10.1145/3702191.3702193,2025,"With the deepening development of digital transformation, the power marketing business is showing diversified expansion and development trends, and the system architecture is shifting from chimney systems to platformization, ecologicalization, and centralization. To solve the problems of difficult collaboration, R&amp;D management, and unified technical architecture in large-scale software engineering organizations, this article proposes the design and construction of a large-scale enterprise digital project R&amp;D and production tool MSC based on a domestic cloud platform, which integrates business needs, product design, software development, and operation and maintenance management. MSC has shared R&amp;D capabilities and supports collaborative operations in multiple locations. MSC efficiently supports the full stack R&amp;D of various digital products, meets the real-time unified management requirements of a set of code and versions for the entire software network, and effectively solves the problem of same root and same origin from large-scale software engineering design to software development."
Pick and Pack: revitalizing mandalas through digital artistry,"Sarkar, Tusita and Abhishek, Tushar and Bhowmick, Partha",10.1145/3702250.3702262,2025,"This paper presents a novel approach to computational art focusing on mandalas—an iconic heritage of Indian art that has proliferated significantly in recent times. Our innovative software allows users to input a handcrafted mandala and select specific motifs for error rectification. The rectification leverages vector information for geometric discretization alongside a simple GCD rule, adhering to traditional mandala principles. The rectified image, being in vector form, allows various real-time operations in the vector space, such as insertion, deletion, or modification of motifs or layers, facilitating enhancements, enlargements, or creation of new compositions. We demonstrate the software’s merit and versatility through various examples, highlighting its potential for special applications such as digital artistry with mandalas, including teaching and training. This work not only advances the field of computational art but also promises to preserve and enhance the rich tradition of mandala art through modern technology."
Artificial Intelligence and Gene Therapy in Ophthalmic Diseases: Current Landscape and Future Directions,"Lu, Wenjia and Li, Shanshan and Jin, Xin",10.1145/3703847.3703883,2024,"A wide range of ophthalmic diseases, from prevalent conditions like age-related macular degeneration to rare genetic retinal disorders, present considerable challenges to global health. This review explores the groundbreaking possibilities of combining artificial intelligence (AI) with gene therapy in ophthalmology, highlighting recent progress, existing challenges, and prospects. AI technologies have shown exceptional precision in diagnostic imaging and predictive analytics, improving early detection and personalized treatment approaches for retinal diseases. Gene therapy has demonstrated encouraging outcomes in clinical trials for inherited retinal disorders. The synergistic potential of AI and gene therapy is evident in optimizing patient selection, monitoring therapeutic outcomes, and enabling adaptive treatment protocols. Emerging trends, including real-time adaptive treatments and combination therapies, underscore the necessity for interdisciplinary collaboration and investment in infrastructure. To implement these technologies responsibly, it is important to focus on regulatory and ethical concerns, including data privacy and equitable access. By leveraging the potential of artificial intelligence and gene therapy, ophthalmology is poised to enter a transformative era marked by precision, personalization, and innovation, which is anticipated to greatly improve patient outcomes and overall quality of life."
The Development and Alleviation of Humanistic Artificial Intelligence in the Digital Age Education: A Content Analysis research,"Chen, Xiaojiao and Hu, Zhebing and Jing, Yuhui and Wang, Chengliang",10.1145/3704137.3704189,2025,"Human-Centered Artificial Intelligence (HCAI) is flourishing in the education sector in the digital intelligence era. This study employs content analysis, selecting 21 papers related to HCAI applications in education from six databases as the research subjects. The study finds that both domestic and international research share common focal points but differ in implementation strategies and application areas. Through content analysis, the study discusses in detail the research themes, intrinsic values, and risk limitations of HCAI in the education field, aiming to provide reference and guidance for future educational policy and technology development."
Application of Digital Human Technology in Vocational College English Teaching: Enhancing Student Learning Experience and Practical Skills,"Gu, Jing-Qian",10.1145/3704217.3704233,2025,"This study explores the implementation of digital human technology to enhance English teaching in vocational and technical colleges. It outlines a comprehensive plan involving needs analysis, technology selection and procurement, teacher training, course design and development, and classroom practice and feedback. The evaluation of effectiveness is conducted through various criteria, including learning outcomes, student engagement, and teacher feedback. Continuous improvement mechanisms ensure the ongoing refinement of the implementation. The findings highlight significant improvements in student engagement and language proficiency, although technical challenges and resource constraints were noted. Recommendations for future research and practical steps for effective technology integration in vocational education are provided."
Digital Open Data Governance: Enhancing E-Government Accountability and Transparency,"Vargas-Murillo, Alfonso Renato and Pari-Bedoya, Ilda Nadia Monica de la Asuncion and Gordillo Bedoya, Shirley Mabel and Arcos Flores, Ysaac Marcelino and Trujillo Pajuelo, Michael Lincold and Morales Cauti, Guisseppi Paul",10.1145/3704217.3704236,2025,"Open data governance has emerged as a paradigm to enhance government transparency and accountability in the digital age. This literature review examines research on open data policies, management processes and oversight mechanisms advancing ethical aims through internet-based technologies. A systematic search yielded 26 studies analyzing open data's effects on accountability and transparency in the context of digital governance. Findings reveal potential exposure of corruption through online platforms, improved civic awareness via digital tools, and increased scrutiny of administrative decisions enabled by internet-accessible data. However, evidence gaps persist regarding aggregate accountability improvements attributable to open data technologies. Moreover, appropriate digital indicators and pathways linking online data disclosure to ethical governance require elucidation, spanning technological, institutional and political dimensions. In essence, purposeful digital governance, capable usage of online tools, and contextual insight into internet-based solutions, not merely data access, critically determine transparency and accountability gains in the era of e-government."
"Enhancing Pretrained Multilingual Machine Translation Model with Code-Switching: A Study on Chinese, English and Malay Language","Liu, Haijing and Seman, Noraini",10.1145/3704323.3704346,2025,"In the field of multilingual machine translation, many pretrained language models have achieved the inspiring results. However, the results based on pretrained models are not yet very satisfactory for low-resource languages. This paper investigates how to leverage code-switching data to fine-tune pretrained multilingual machine translation model, in order to boost the performance of few-shot low resource machine translation. By utilizing a multilingual mixed corpus, the code-switching method can enhance the cross-linguistic generalization ability of the model and improve its overall understanding of the language. In this paper, on the smaller size model of FLORES-101 benchmark, we use the code-switching data augmentation method to achieve the results of benchmark's larger model on six direction pairs of three languages, Chinese, English and Malay. This paper studied various corpus mixture mechanisms to construct the data in code-switching, and the experimental findings show that the results using the code-switching fine-tuning model improve the spBLEU score by an average of 2 to 3 points over the results without code-switching."
Collaborative Game-based Learning Analytics: Predicting Learning Outcomes from Game-based Collaborative Problem Solving Behaviors,"Acosta, Halim and Hong, Daeun and Lee, Seung and Min, Wookhee and Mott, Bradford and Hmelo-Silver, Cindy and Lester, James",10.1145/3706468.3706522,2025,"Skills in collaborative problem solving (CPS) are essential for the 21st century, enabling students to solve complex problems effectively. As the demand for these skills rises, understanding their development and manifestation becomes increasingly important. To address this need, we present a data-driven framework that identifies behavioral patterns associated with CPS practices and can assess students’ learning outcomes. It provides explainable insights into the relationship between students’ behaviors and learning performance. We employ embedding and clustering techniques to categorize similar trace logs and apply Latent Dirichlet allocation to generate meaningful descriptors. To capture the temporal evolution of student behaviors, we introduce a graph-based representation of transitions between behavior patterns extracted using constraint-based pattern mining. We map behavioral patterns to a CPS ontology by analyzing how action sequences correspond to specific CPS practices. Analysis of semi-structured trace log data from 61 middle school students engaged in collaborative game-based learning reveals that the extracted behavioral patterns significantly predict student learning gains using generalized additive models. Our analysis identifies patterns that provide insights into the relationship between student use of CPS practices and learning outcomes."
Pattern analysis of ambitious science talk between preservice teachers and AI-powered student agents,"Barrett, Alex and Ke, Fengfeng and Zhang, Nuodi and Dai, Chih-Pu and Bhowmik, Saptarshi and Yuan, Xin",10.1145/3706468.3706570,2025,"New frontiers in simulation-based teacher training have been unveiled with the advancement of artificial intelligence (AI). Integrating AI into virtual student agents increases the accessibility and affordability of teacher training simulations, but little is known about how preservice teachers interact with AI-powered student agents. This study analyzed the discourse behavior of 15 preservice teachers who undertook simulation-based training with AI-powered student agents. Using a framework of ambitious science teaching, we conducted a pattern analysis of teacher and student talk moves, looking for evidence of academically productive discourse. Comparisons are made with patterns found in real classrooms with professionally trained science teachers. Results indicated that preservice teachers generated academically productive discourse with AI-powered students by using ambitious talk moves. The pattern analysis also revealed coachable moments where preservice teachers succumbed to cycles of unproductive discourse. This study highlights the utility of analyzing classroom discourse to understand human-AI communication in simulation-based teacher training."
A Scoping Review of Gender Stereotypes in Artificial Intelligence,"Duan, Wen and Li, Lingyuan and Freeman, Guo and McNeese, Nathan",10.1145/3706598.3713093,2025,"People often apply gender stereotypes to Artificial Intelligence (AI), and AI design frequently reinforces these stereotypes, perpetuating traditional gender ideologies in state-of-the-art technology. Despite growing interests in investigating this phenomenon, there is little conceptual clarity or consistency regarding what actually constitutes a ""gender stereotype"" in AI. Therefore, it is critical to provide a more comprehensive image of existing understandings and ongoing discussions of gender stereotypes of AI to guide AI design that reduces the harmful effects of these stereotypes. In doing so, this paper presents a scoping review of over 20 years of research across HCI, HRI and various social science disciplines on how gender stereotypes are applied to AI. We outline the methods and contexts of this growing body of work, develop a typology to clarify these stereotypes, highlight under-explored approaches for future research, and offer guidelines to improve rigor and consistency in this field that may inform responsible AI design in the future."
VisUnit: Literate Visualisation Studies Assembled from Reusable Test-Suites,"Jianu, Radu and Slingsby, Aidan and Laksono, Dany and Okoe, Mershack",10.1145/3706598.3713104,2025,"We make four contributions to lower the overhead of conducting visualisation user studies and promote the reuse and extension of their materials. (i) A declarative Javascript specification lets experimenters describe how studies are assembled from tested visualisations, datasets, tasks and chosen evaluation strategies. (ii) A VisUnit library translates these into sequences of visual stimuli and delivers them to participants. We move away from monolithic evaluation stimuli typical of previous work and construct studies around three ingredients – visual encodings, datasets, and tasks – that can be developed independently and recombined flexibly. (iii) This paves the way for developing benchmark data+tasks test-suites as independent, reusable resources to support multiple studies. (iv) Structuring user studies as “literate” visualisation notebooks brings together in the open all ingredients necessary for replication and scrutiny: formal design specification; underlying materials; participant-facing views; and narratives justifying design and supporting reuse."
The Making of Performative Accuracy in AI Training: Precision Labor and Its Consequences,"Zhang, Ben Zefeng and Yang, Tianling and Miceli, Milagros and Haimson, Oliver L. and Thomas, Michaelanne",10.1145/3706598.3713112,2025,"Accuracy and precision are central values in the AI communities and the technology sector. This paper provides empirical evidence on the construction and organizational management of technical accuracy, demonstrating how technology companies’ preoccupation with such values leads to harm. Drawing on nine months of multi-sited ethnographic fieldwork in China, we document how AI trainers’ everyday work practices, challenges, and harms stem from clients’ demands for high levels of technical accuracy. We introduce the concept of precision labor to unpack the labor dimension of constructing and performing accuracy in AI training. This concept highlights the hidden and excessive labor required to reconcile the ambiguity and uncertainty involved in this process. We argue that precision labor offers a new lens to illuminate three critical aspects of AI training: 1) the negative health and financial impacts of hidden and excessive labor on AI workers; 2) emerging harms, including workers’ subordinate roles to machines and financial precarity; and 3) a conceptual contribution to contexts beyond AI training. This contribution re-centers arbitrariness in technical production, highlights the excessive demands of precision labor, and examines the legitimization of labor and harm. Our study also contributes to existing scholarship on the prevailing values and invisible labor in AI production, underscoring accuracy as performative rather than self-evident and unambiguous. A precision labor lens challenges the legitimacy and sustainability of relentlessly pursuing technical accuracy, raising new questions about its consequences and ethical implications. We conclude by proposing recommendations and alternative approaches to enhance worker agency and well-being."
Behavior Change Interventions Combating Online Misinformation: A Scoping Review,"Konstantinou, Loukas and Karapanos, Evangelos",10.1145/3706598.3713127,2025,"It is increasingly acknowledged that simply presenting users with corrective information is unlikely to produce the desired effects against misinformation. As such, the need for systematic use of behavioral theory is increasingly acknowledged, and behavioral interventions against misinformation are rising. This paper presents a scoping review of digital behavioral interventions countering misinformation, inquiring into their behavioral objectives, theoretical foundations, design and evaluation practices, and the factors that were empirically proven, or speculated, to contribute to interventions’ failure. Among others, we identify 17 distinct behavioral objectives, organized into three stages of the online news cycle: composition, amplification and consumption, 24 theoretical frameworks employed in designing these interventions, and nine reasons of failure. We synthesize the findings into a set of design cards with the goal of guiding intervention designers during concept ideation and refinement, and highlight areas for future research."
Making the Write Connections: Linking Writing Support Tools with Writer Needs,"Zhao, Zixin and Masson, Damien and Kim, Young-Ho and Penn, Gerald and Chevalier, Fanny",10.1145/3706598.3713161,2025,"This work sheds light on whether and how creative writers’ needs are met by existing research and commercial writing support tools (WST). We conducted a need finding study to gain insight into the writers’ process during creative writing through a qualitative analysis of the response from an online questionnaire and Reddit discussions on r/Writing. Using a systematic analysis of 115 tools and 67 research papers, we map out the landscape of how digital tools facilitate the writing process. Our triangulation of data reveals that research predominantly focuses on the writing activity and overlooks pre-writing activities and the importance of visualization. We distill 10 key takeaways to inform future research on WST and point to opportunities surrounding underexplored areas. Our work offers a holistic and up-to-date account of how tools have transformed the writing process, guiding the design of future tools that address writers’ evolving and unmet needs."
A Critical Analysis of Machine Learning Eco-feedback Tools through the Lens of Sustainable HCI,"G\""{o}r\""{u}c\""{u}, Sinem and Morais, Luiz A. and Panagiotidou, Georgia",10.1145/3706598.3713198,2025,"In light of machine learning’s increasing computational needs, developers created energy and carbon-reporting tools to calculate and communicate their models’ environmental impact. These tools use modeling parameters as inputs and respond with expected or incurred energy requirements or carbon emissions. This work critically and systematically analyses them regarding their content, form, and design process. Besides their noble intentions, many of the shortcomings of early sustainable HCI eco-feedback tools are still being propagated in these tools. Moreover, their design and development have limited inclusion of potential stakeholders. We argue the need for a next generation of approaches to ML eco-feedback that (a) further support rematerialization, (b) use participatory approaches in their design and development to support collaborative team environments and go beyond individual persuasion, (c) consider complexities of ML models and processes, and more broadly, (d) re-center around sufficiency rather than only efficiency."
"""Should I choose a smaller model?': Understanding ML Model Selection and Its Impact on Sustainability","Ben chaaben, Eya and Koch, Janin and Mackay, Wendy E.",10.1145/3706598.3713240,2025,
"""Why do we do this?"": Moral Stress and the Affective Experience of Ethics in Practice","Rattay, Sonja and Vakkuri, Ville and Rozendaal, Marco C. and Shklovski, Irina",10.1145/3706598.3713264,2025,"A plethora of toolkits, checklists, and workshops have been developed to bridge the well-documented gap between AI ethics principles and practice. Yet little is known about effects of such interventions on practitioners. We conducted an ethnographic investigation in a major European city organization that developed and works to integrate an ethics toolkit into city operations. We find that the integration of ethics tools by technical teams destabilises their boundaries, roles, and mandates around responsibilities and decisions. This lead to emotional discomfort and feelings of vulnerability, which neither toolkit designers nor the organization had accounted for. We leverage the concept of moral stress to argue that this affective experience is a core challenge to the successful integration of ethics tools in technical practice. Even in this best case scenario, organisational structures were not able to deal with moral stress that resulted from attempts to implement responsible technology development practices."
How to Design with Ambiguity: Insights from Self-tracking Wearables,"Di Lodovico, Chiara and Houben, Steven and Colombo, Sara",10.1145/3706598.3713267,2025,"Nearly 20 years ago, Gaver et al. introduced ambiguity as a design resource, proposing tactics to reflect everyday uncertainty into interactive systems. This approach is especially relevant for self-tracking wearables, which often obscure the inherent ambiguity of system design and tracked phenomena with seemingly clear, prescriptive data and insights. Although scholars recognize the importance of ambiguity, its practical application in the design process remains underexplored. To address this, we conducted a two-week workshop with 60 designers, examining the application of Gaver et al.’s tactics into 11 design concepts, and performed interviews with 16 participants. Our findings reveal eight relevant ambiguity tactics for self-tracking and offer insights into participants’ experiences with designing using ambiguity. We discuss prescription and overlooked ambiguity as levers for the operationalization of ambiguity, the potential benefits and downsides of ambiguity tactics for users, future directions for HCI research and practice, and the study limitations."
Incorporating Sustainability in Electronics Design: Obstacles and Opportunities,"Englhardt, Zachary and H\""{a}hnlein, Felix and Mei, Yuxuan and Lin, Tong and Sun, Connor Masahiro and Zhang, Zhihan and Patel, Shwetak and Schulz, Adriana and Iyer, Vikram",10.1145/3706598.3713299,2025,"Life cycle assessment (LCA) is a methodology for holistically measuring the environmental impact of a product from initial manufacturing to end-of-life disposal. However, the extent to which LCA informs the design of computing devices remains unclear. To understand how this information is collected and applied, we interviewed 17 industry professionals with experience in LCA or electronics design, systematically coded the interviews, and investigated common themes. These themes highlight the challenge of LCA data collection and reveal distributed decision-making processes where responsibility for sustainable design choices—and their associated costs—is often ambiguous. Our analysis identifies opportunities for HCI technologies to support LCA computation and its integration into the design process to facilitate sustainability-oriented decision-making. While this work provides a nuanced discussion about sustainable design in the information and communication technologies (ICT) hardware industry, we hope our insights will also be valuable to other sectors."
The Dilemma of Building Do-It-Yourself (DIY) Solutions For Workplace Accessibility,"Cha, Yoonha and Jackson, Victoria and Kohl, Karina and Prikladnicki, Rafael and van der Hoek, Andr\'{e} and Branham, Stacy",10.1145/3706598.3713302,2025,"Existing commercial and in-house software development tools are often inaccessible to blind and low vision software professionals (BLVSPs), hindering their participation and career growth at work. Building on existing research on Do-It-Yourself (DIY) assistive technologies and customized tools made by programmers, we shed light on the currently unexplored intersection of how DIY tools built and used by BLVSPs support accessible software development. Through semi-structured interviews with 30 BLVSPs, we found that such tools serve many different purposes and are driven by motivations such as desiring to maintain a professional image and a sense of dignity at work. These tools had significant impacts on workplace accessibility and revealed a need for a more centralized community for sharing tools, tips, and tricks. Based on our findings, we introduce the “Double Hacker Dilemma” and highlight a need for developing more effective peer and organizational platforms that support DIY tool sharing."
CT4ALL: Towards Putting Teachers in the Loop to Advance Automated Computational Thinking Metric Assessments in Game-Based Learning,"Troiano, Giovanni M and Cassidy, Michael and Morales, Daniel Escobar and Pons, Guillermo and Abdollahi, Amir and Robles, Gregorio and Puttick, Gillian and Harteveld, Casper",10.1145/3706598.3713368,2025,"Computational thinking (CT) is essential for the 21st century learner. Yet, assessing CT remains challenging. This is particularly challenging in constructionist learning, where individual idiosyncrasies may clash with one-size-fits-all assessments. Tools like Dr. Scratch offer CT metrics that show promise for effective and scalable CT assessments, particularly in constructionist game-based learning (GBL). Prior work has advanced the design of automated CT metrics but hardly included teachers in the process. We extend Dr. Scratch to improve automated CT assessments for GBL and put teachers in the loop to assess its novel features. Specifically, we interviewed seven middle school teachers employing GBL in STEM curricula and asked them to provide feedback on the newly designed CT metrics. Teachers view the new CT metrics positively, underscoring their potential for adaptive CT assessments despite hindrances. We advance automated CT assessments via teacher evaluation toward design-sensitive CT metrics and CT for all."
Text Entry for XR Trove (TEXT): Collecting and Analyzing Techniques for Text Input in XR,"Bhatia, Arpit and Mughrabi, Moaaz Hudhud and Abdlkarim, Diar and Di Luca, Massimiliano and Gonzalez-Franco, Mar and Ahuja, Karan and Seifi, Hasti",10.1145/3706598.3713382,2025,"Text entry for extended reality (XR) is far from perfect, and a variety of text entry techniques (TETs) have been proposed to fit various contexts of use. However, comparing between TETs remains challenging due to the lack of a consolidated collection of techniques, and limited understanding of how interaction attributes of a technique (e.g., presence of visual feedback) impact user performance. To address these gaps, this paper examines the current landscape of XR TETs by creating a database of 176 different techniques. We analyze this database to highlight trends in the design of these techniques, the metrics used to evaluate them, and how various interaction attributes impact these metrics. We discuss implications for future techniques and present TEXT: Text Entry for XR Trove, an interactive online tool to navigate our database."
Animals' Entanglement with Technology: a Scoping Review,"Kleinberger, Rebecca and Ashooh, Lena and Farsad, Keavan and Hirskyj-Douglas, Ilyena",10.1145/3706598.3713384,2025,"Animals living alongside humans are navigating a world increasingly filled with technology, yet little is known about how they interface with these systems, whether designed for, with, or around them. Anchored in HCI and ranging across diverse fields, this scoping review analyzes nearly 800 research works to explore the diverse realities of animal-technology research, examining the who, what, why, and how of animal-technology entanglements. Our analysis revealed 11 research objectives and eight types of technologies across six animal contexts. By categorizing the literature based on authors’ aims and intended beneficiaries, we highlight trends, gaps, and ethical considerations. We find that most systems involve animals with limited potential for direct engagement or sense-making. We propose a framework to understand animals as users versus subjects of interactive systems, focusing on feedback, empirical testing, and projected animal benefits. Our findings offer a foundation to understand current and future animal technology research and the diversity of animal user experience."
"Participatory Design in Human-Computer Interaction: Cases, Characteristics, and Lessons","Qi, Xiang and Yu, Junnan",10.1145/3706598.3713436,2025,"Participatory Design (PD) has become increasingly prevalent in Human-Computer Interaction (HCI) research. However, there remains a lack of comprehensive understanding of how PD has been used by HCI scholars. To bridge this gap, we sampled PD application cases (N = 185) from the SIGCHI conferences over the past decade and examined these cases through the dimensions of application features (e.g., contexts and functions of PD) and PD principles (e.g., its political commitment and mutual learning principle). Our analysis reveals the various ways PD has been applied in HCI and how its core features have been or have not been manifested in these cases. Based on these findings, we reflect on the conceptual understanding of PD within the HCI community and discuss potential misconceptions. Ultimately, we hope this work can serve as a useful reference for HCI researchers and beyond who are interested in incorporating PD into their design and research practices."
Technologies for Children's AI Learning: Design Features and Future Opportunities,"Jia, Kaiyue and Yu, Junnan",10.1145/3706598.3713443,2025,"With the growing integration of AI into daily life, various technologies have been developed to teach children about AI. However, differences in their designs highlight the need for a thorough understanding of these tools to make the most of current technological resources and guide the effective development of future learning tools. Through a systematic search, we identified 64 different AI learning tools for children and analyzed their design features, including both static design features (i.e., presentation formats and learning content) and interactive design features (i.e., learning activity types and design features that potentially enhance the effectiveness of the activities). Our findings reveal the current trends and gaps in the design of children’s AI learning technologies. Based on these insights, we reflect on future design opportunities and provide recommendations for creating new, effective learning technologies to advance AI education for the next generations."
How Do HCI Researchers Study Cognitive Biases? A Scoping Review,"Boonprakong, Nattapat and Tag, Benjamin and Goncalves, Jorge and Dingler, Tilman",10.1145/3706598.3713450,2025,"Computing systems are increasingly designed to adapt to users’ cognitive states and mental models. Yet, cognitive biases affect how humans form such models and, therefore, they can impact their interactions with computers. To better understand this interplay, we conducted a scoping review to chart how Human-Computer Interaction (HCI) researchers study cognitive biases. Our findings show that computing systems not only have the potential to induce and amplify cognitive biases but also can be designed to steer users’ behaviour and decision-making by capitalising on biases. We describe how HCI researchers develop algorithms and sensing methods to detect and quantify the effects of cognitive biases and discuss how we can use their understanding to inform system design. In this paper, we outline a research agenda for more theory-grounded research and highlight ethical issues when researching and designing computing systems with cognitive biases in mind as they affect real-world behaviour."
Digital technology for supporting Aged Care Services: A Scoping Review,"Theopilus, Yansen and Davis, Hilary and Pedell, Sonja",10.1145/3706598.3713498,2025,"Digital technology has great potential to support human health, including the complex needs of older adults in aged care services and treatments. This scoping review aims to explore the current state and roles of digital technology in supporting aged care following the PRISMA-ScR guidelines. We included 47 studies from the last 10 years covering five databases discussing the development, implementation, evaluation, or review of digital technology in aged care services and the broader clinical system. Seven key roles of digital technology were identified, including health monitoring and assessment, remote healthcare services, assistive technology to support treatment, self-care management, social technology to facilitate interaction, clinical decision support, and aged care quality measurement carried out by twelve technology types. Our findings show the potential of digital technology in enhancing the capability and efficiency of aged care services for developing or improving socio-technical aged care systems. We conclude with six recommendations for future research."
What is User Engagement?: A Systematic Review of 241 Research Articles in Human-Computer Interaction and Beyond,"Jansen, Bernard J and Guan, Kathleen W and Salminen, Joni and Aldous, Kholoud Khalil and Jung, Soon-Gyo",10.1145/3706598.3713505,2025,"User engagement (UE) is widely discussed in HCI articles, but its definition, reliability, and application remain elusive. This research conducts a systematic literature review of 241 articles from 1993 to 2023 to analyze how UE is defined and measured within the domain of HCI. Our findings reveal significant definitional inconsistencies that hinder UE's practical application in HCI research and system design. Based on our findings, we recommend using UE as a categorical label rather than a unified construct until more systematic frameworks are established. We also highlight the need for divergent views of UE across HCI research communities as a valuable avenue to pursue. This divergent view approach can help HCI researchers focus on specific, measurable aspects of UE that align with specific community practices and norms. Our findings also suggest that until such a framework emerges, researchers should be aware of its limitations when using UE as a research construct."
Which Contributions Deserve Credit? Perceptions of Attribution in Human-AI Co-Creation,"He, Jessica and Houde, Stephanie and Weisz, Justin D.",10.1145/3706598.3713522,2025,"AI systems powered by large language models can act as capable assistants for writing and editing. In these tasks, the AI system acts as a co-creative partner, making novel contributions to an artifact-under-creation alongside its human partner(s). One question that arises in these scenarios is the extent to which AI should be credited for its contributions. We examined knowledge workers’ views of attribution through a survey study (N=155) and found that they assigned different levels of credit across different contribution types, amounts, and initiative. Compared to a human partner, we observed a consistent pattern in which AI was assigned less credit for equivalent contributions. Participants felt that disclosing AI involvement was important and used a variety of criteria to make attribution judgments, including the quality of contributions, personal values, and technology considerations. Our results motivate and inform new approaches for crediting AI contributions to co-created work."
Trusting Autonomous Teammates in Human-AI Teams - A Literature Review,"Duan, Wen and Flathmann, Christopher and McNeese, Nathan and Scalia, Matthew J and Zhang, Ruihao and Gorman, Jamie and Freeman, Guo and Zhou, Shiwen and Hauptman, Allyson Ivy and Yin, Xiaoyun",10.1145/3706598.3713527,2025,"As autonomous AI agents become increasingly integrated into human teams, the level of trust humans place in these agents - both as a piece of technology and increasingly viewed as teammates - significantly impacts the success of human-AI teams (HATs). This work presents a literature review of the HAT research that investigates humans’ trust in their AI teammates. In this review, we first identify the ways in which trust was conceptualized and operationalized, which underscores the pressing need for clear definitions and consistent measurements. Then, we categorize and quantify the factors found to influence trust in an AI teammate, highlighting that agent-related factors (such as transparency, reliability) have the strongest impacts on trust in HAT research. We also identify under-explored factors related to humans, teams, and environments, and gaps for future HAT research and design."
Designing Conversational AI for Aging: A Systematic Review of Older Adults' Perceptions and Needs,"Huang, Yuanhui and Zhou, Quan and Piper, Anne Marie",10.1145/3706598.3713578,2025,"Recent advances in conversational AI and the ubiquity of related devices and applications—from robots to smart speakers to chatbots—has led to extensive research on designing and studying conversational systems with older adults. Despite a growing literature on this topic, many studies examine small groups of older adults and specific devices, neglecting a holistic understanding of how diverse groups of older adults perceive conversational interaction more broadly. We present a systematic review that synthesizes older adults’ perceptions of the challenges and opportunities for interacting with these systems. We highlight their vision for future AI-based conversational systems, emphasizing a desire for more human-like interactions, personalization, and greater control over their information. We discuss the implications for future research and design of conversational AI systems for older adults."
Breaking the Binary: A Systematic Review of Gender-Ambiguous Voices in Human-Computer Interaction,"De Cet, Martina and Obaid, Mohammad and Torre, Ilaria",10.1145/3706598.3713608,2025,"Voice interfaces come in many forms in Human-Computer Interaction (HCI), such as voice assistants and robots. These are often gendered, i.e. they sound masculine or feminine. Recently, there has been a surge in creating gender-ambiguous voices, aiming to make voice interfaces more inclusive and less prone to stereotyping. In this paper, we present the first systematic review of research on gender-ambiguous voices in HCI literature, with an in-depth analysis of 36 articles. We report on the definition and availability of gender-ambiguous voices, creation methods, user perception and evaluation techniques. We conclude with several concrete action points: clarifying key terminology and definitions for terms such as gender-ambiguous, gender-neutral, and non-binary; conducting an initial acoustic analysis of gender-ambiguous voices; taking initial steps toward standardising evaluation metrics for these voices; establishing an open-source repository of gender-ambiguous voices; and developing a framework for their creation and use. These recommendations provide important insights for fostering the development and adoption of inclusive voice technologies."
"Sustainability, Development, and Human–Computer Interaction","Sharma, Vishal and Kumar, Neha",10.1145/3706598.3713663,2025,"Researchers in Human–Computer Interaction (HCI) have studied the design and use of technologies for sustainability and development, contributing to the subfields of Sustainable HCI and HCI for Development. Increasingly, there have been calls within and outside HCI for a more integrated approach to sustainable development. To identify the potential of such an approach, we present a comprehensive review of HCI scholarship on sustainability and development, combined with an analysis of interviews with researchers working in and across both subfields. Using the lens of political economy, we uncover understandings, critiques, tensions, and considerations toward advancing scholarship at the intersections of sustainability, development, and HCI. We conclude by inviting the larger Special Interest Group on Computer-Human Interaction (SIGCHI) community to join us in collectively devising pathways for technology-mediated sustainable development."
Expression-in-action and Expression-on-action: A Systematic Review of Mediums for Expression in Mental Health,"Xu, Tian and Cook, Sid and Semaan, Bryan and Voida, Stephen",10.1145/3706598.3713669,2025,"Expression facilitates the externalization of personal experiences and inner states (e.g., thoughts and emotions) embedded in everyday life. Yet, in mental health contexts, expression is often marginalized or systematically and structurally excluded from technology and system design. While HCI scholarship has explored expression in specific settings to advance user experience, a comprehensive understanding of expression remains limited. This limitation constrains the design space for future technologies that support and embrace expression, resulting in a potential lack of authentic experiential data reflecting individuals’ lived experiences embedded in everyday management of mental health. Through a systematic review of n = 105 studies, we explore the mediums, populations, and practices of expression in mental health contexts. Our study contributes a nuanced sociotechnical understanding of expression for the HCI community, developing two concepts for enriching expression and offering insights for designing experience-rich technologies that better support expression for everyday mental health management."
“Bring them back to life”: LifeLink Application for Caregivers Dealing with Suicidality,"Jha, Smriti and Chan, Gerry and Jewer, Seana and Agyapong, Vincent I.O. and Orji, Rita",10.1145/3706598.3713699,2025,"Suicide is a complex phenomenon wherein, in addition to the individual impacted, its effects seep into many lives including those of their caregivers. Caregivers seek help everywhere but face unique challenges including limited access to timely resources and personal mental health struggles. Mobile health apps offer a promising solution, but addressing caregivers’ specific needs remains a concern. We present LifeLink, a persuasive mobile app to support caregivers of individuals experiencing suicidal thoughts. The app was developed in three stages. First, we reviewed 80 existing suicide prevention apps. Second, we designed a low-fidelity prototype of LifeLink using the Persuasive System Design model and refined it through a study conducted with 45 caregivers. Finally, incorporating evidence-based strategies and caregiver feedback, we developed and evaluated LifeLink in another study with 50 caregivers. Results show that LifeLink is user-friendly, engaging, elicits positive user experience and effectively empowers caregivers. LifeLink usage was associated with improved mental wellbeing, increased mental health literacy, and a more supportive environment. Our findings highlight the importance of involving caregivers in the design process. We offer recommendations for designers and researchers developing impactful persuasive technology for suicide prevention and for those working in related areas."
Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at CHI through a Systematic Literature Review,"Pang, Rock Yuren and Schroeder, Hope and Smith, Kynnedy Simone and Barocas, Solon and Xiao, Ziang and Tseng, Emily and Bragg, Danielle",10.1145/3706598.3713726,2025,"Large language models (LLMs) have been positioned to revolutionize HCI, by reshaping not only the interfaces, design patterns, and sociotechnical systems that we study, but also the research practices we use. To-date, however, there has been little understanding of LLMs’ uptake in HCI. We address this gap via a systematic literature review of 153 CHI papers from 2020-24 that engage with LLMs. We taxonomize: (1) domains where LLMs are applied; (2) roles of LLMs in HCI projects; (3) contribution types; and (4) acknowledged limitations and risks. We find LLM work in 10 diverse domains, primarily via empirical and artifact contributions. Authors use LLMs in five distinct roles, including as research tools or simulated users. Still, authors often raise validity and reproducibility concerns, and overwhelmingly study closed models. We outline opportunities to improve HCI research with and on LLMs, and provide guiding questions for researchers to consider the validity and appropriateness of LLM-related work."
"""I Don't Know Why I Should Use This App"": Holistic Analysis on User Engagement Challenges in Mobile Mental Health","Jin, Seungwan and Kim, Bogoan and Han, Kyungsik",10.1145/3706598.3713732,2025,"Over the past decade, mobile apps have been widely adopted as a digital intervention method for mental health support, offering scalable and accessible solutions to address the growing global mental health challenges. However, sustaining user engagement in real-world settings remains a major challenge in the development of these applications. This study systematically examines factors that hinder user engagement in existing mobile mental health support systems through a scoping review of the literature. After an initial identification of 1,267 papers, we conducted a final analysis of 111 empirical studies using mobile app-based mental health support systems. The study investigates the main factors that negatively affect user engagement from user and system perspectives. Based on these findings, we propose guidelines for sustaining and enhancing user engagement and for structuring personalized emotional interaction design along three dimensions: adaptive, continuous, and multimodal interactions. Furthermore, we discuss the potential for integration with advanced AI methods (e.g., LLM-based generative AI agents) as a way to achieve these design implications and suggestions. Our results provide critical insights for enhancing long-term user engagement in the development of future mental health support systems."
Owning the (Virtual) World: A Systematic Review of Psychological Ownership of Interactive Virtual Objects and Environments,"Krauss, Jana and Wienrich, Carolin",10.1145/3706598.3713750,2025,"In this systematic review, we analyze the literature on psychological ownership of virtual objects and environments according to the PRISMA statement. Psychological ownership describes the feelings of possession towards an object which are independent of legal possession. The construct stems from organizational management literature, but is gaining in importance in Human-Computer-Interaction as users invest billions to own virtual objects. The analysis of 21 research papers reveals how and why ownership emerges and presents the dimensions and consequences of such feelings. In addition, we relate these variables to the classic psychological ownership motives of self-efficacy, self-identity, and belonging, as well as the routes of control, identity transfer, and intimate knowledge. We outline why designers should pay attention to the phenomenon and how it can be utilized in different contexts. Finally, the paper concludes by outlining why and what research will be needed in the future."
Of Ironies and Agency: Energy Professionals' Views on Digital Interventions and Their Users,"Bremer, Christina and Knowles, Bran and Friday, Adrian",10.1145/3706598.3713754,2025,"The efficacy of digital solutions to increase energy efficiency, including technical optimisations and behavioural influence, has long been a subject of debate within sustainable HCI (SHCI). While the viewpoints of policymakers and academics are frequently published (and often contradictory), less is known about the views of those on the ground. In this paper we ask: What are energy professionals’ views of digital energy-saving interventions and their users? What are the challenges they face implementing these interventions? Based on a university campus case study with twelve semi-structured interviews and a focus group with energy and facilities’ professionals, we illustrate how they strongly advocate digital efficiency as a pathway to sustainability; yet, this optimism is in apparent tension with key barriers they identify to realising ‘their seamless visions’, particularly the complexities of the human behaviour they are seeking to optimise. These findings underscore the seductiveness of techno-optimism and the need for more systemic change."
Let's Talk Futures: A Literature Review of HCI's Future Orientation,"Sanchez, Camilo and Wang, Sui and Savolainen, Kaisa and Epp, Felix Anand and Salovaara, Antti",10.1145/3706598.3713759,2025,"HCI is future-oriented by nature: it explores new human–technology interactions and applies the findings to promote and shape vital visions of society. Still, the visions of futures in HCI publications seem largely implicit, techno-deterministic, narrow, and lacking in roadmaps and attention to uncertainties. A literature review centered on this problem examined futuring and its forms in the ACM Digital Library’s most frequently cited HCI publications. This analysis entailed developing the four-category framework SPIN, informed by futures studies literature. The results confirm that, while technology indeed drives futuring in HCI, a growing body of HCI research is coming to challenge techno-centric visions. Emerging foci of HCI futuring demonstrate active exploration of uncertainty, a focus on human experience, and contestation of dominant narratives. The paper concludes with insight illuminating factors behind techno-centrism’s continued dominance of HCI discourse, as grounding for five opportunities for the field to expand its contribution to futures and anticipation research."
Bridging Simulation and Reality: Augmented Virtuality for Mass Casualty Triage Training - From Landscape Analysis to Empirical Insights,"Chen, Yang and Fennedy, Katherine and Zhang, Jiayi and Sim, Yong Jie and Zheng, Clement and Yen, Ching Chiuan",10.1145/3706598.3713794,2025,"Live drills are the gold standard for mass casualty incident (MCI) training but are often too resource-intensive for widespread implementation. Immersive technologies offer a promising alternative, but can they deliver comparable fidelity and effectiveness? Working with a local disaster response academy, this paper investigated the potential of Augmented Virtuality (AV) in MCI training through two phases. First, we conducted a landscape analysis of 126 papers across the virtuality continuum, revealing trends in population, training focus, and evaluation metrics. Second, we empirically evaluated an AV system for mass casualty triage training against traditional role-playing and Virtual Reality (VR) approaches, involving 60 trainees in an operational curriculum. Results indicated that both AV and VR surpassed traditional simulations, with AV’s tactile integration significantly enhancing physical engagement, satisfaction, and triage accuracy. Through the lens of triage, we discussed the broader practical implications of integrating immersive technologies like AV into real-world MCI education."
Generative AI Uses and Risks for Knowledge Workers in a Science Organization,"Wagman, Kelly B. and Dearing, Matthew T. and Chetty, Marshini",10.1145/3706598.3713827,2025,"Generative AI could enhance scientific discovery by supporting knowledge workers in science organizations. However, the real-world applications and perceived concerns of generative AI use in these organizations are uncertain. In this paper, we report on a collaborative study with a US national laboratory with employees spanning Science and Operations about their use of generative AI tools. We surveyed 66 employees, interviewed a subset (N=22), and measured early adoption of an internal generative AI interface called Argo lab-wide. We have four findings: (1) Argo usage data shows small but increasing use by Science and Operations employees; Common current and envisioned use cases for generative AI in this context conceptually fall into either a (2) copilot or (3) workflow agent modality; and (4) Concerns include sensitive data security, academic publishing, and job impacts. Based on our findings, we make recommendations for generative AI use in science and other organizations."
"Translating HCI Research to Broader Audiences: Motivation, Inspiration, and Critical Factors on Alternative Research Outcomes","Yoo, MinYoung and Ppali, Sophia and Odom, William and Zhuang, Yumeng and Kritika, Kritika and Olson, Wyatt and Wieczorek, Catherine and Biggs, Heidi and Berger, Arne and Desjardins, Audrey and Wakkary, Ron and Ringland, Kathryn E.",10.1145/3706598.3713884,2025,"Alternative Research Outcomes (AROs) go beyond traditional academic publications, taking diverse forms such as documentaries, DIY tutorials, or exhibitions. With growing recognition of the need for more inclusive and contextually appropriate research dissemination, AROs are particularly relevant in HCI and design research. Yet, little has been discussed on why it is important to work on AROs. What are key qualities of AROs? How can the HCI community benefit from learning more about creating AROs? By analyzing six case studies, we propose four qualities of AROs and demonstrate how they emerge in the timeline of a research project. We argue AROs can be adapted to diverse audience needs and share research insights that may extend beyond the original research goals. Our work contributes to a deeper understanding of how AROs can support inclusive research dissemination practices, enabling HCI researchers to engage broader audiences and extend the relevance of their work."
"What Do We Design for When We Design ""Smart Buildings""? - A Scoping Review of Human Experience Design Research in Buildings","Rao, Shruti and Rogers, Katja and Good, Judith and Alavi, Hamed",10.1145/3706598.3713903,2025,"Built environments increasingly incorporate new forms of intelligence, creating opportunities for enhancing human interactive experiences with and within building spaces. This scoping review examines design interventions and discourses within the domain of “Smart Buildings”. The goal is to identify and characterise the type of human experiences that research in this domain aims to address. Using a hybrid deductive-inductive coding approach, we analysed 192 papers related to human experiences and smart buildings from ACM Digital Library and Scopus published between 1996 and 2024. Our analysis revealed 11 distinct “targeted human experiences”, 20 commonly used “design mechanisms” to achieve those design goals, as well as two typologies of “technological interventions”. Our findings create a foundation for understanding building design research and the range of human experience they entail."
MedAI-SciTS: Enhancing Interdisciplinary Collaboration between AI Researchers and Medical Experts,"Cao, Chen and Wu, Yu and Fang, Xiao Zoe and Liang, Zhenwen and Mamykina, Lena and Sbaffi, Laura and Xu, Xuhai",10.1145/3706598.3713926,2025,"Integrating AI in healthcare requires effective interdisciplinary collaboration, yet challenges like methodological differences, terminology barriers, and divergent objectives persist. To address the issues, we introduce MedAI-SciTS, a structured approach combining a theoretical framework and a toolkit to improve collaboration across disciplines. The framework builds on a formative study (N=12) and literature review, identifying the key challenges and potential solutions in medical-AI projects. We further develop an innovative toolkit with twelve tools, featuring an AI-enhanced research glossary with personalized analogies, an agile co-design platform, and an integrated resource management system. A three-month case study involving AI and medical professionals (N=16 total) applying a segmentation algorithm for adrenal CT images confirmed the toolkit’s effectiveness in enhancing team engagement, communication, trust, and collaboration outcomes. We envision MedAI-SciTS could potentially be applied to a wide range of medical applications and facilitate broader medical-AI collaboration."
Understanding VR Accessibility Practices of VR Professionals,"Wang, Yi and Liu, Xiao and Arora, Chetan and Grundy, John and Hoang, Thuong",10.1145/3706598.3713927,2025,"Accessibility is a crucial concept in Virtual Reality (VR), pivotal for meeting the needs of users, including those with disabilities. In recent years, there has been an increasing focus of VR products on enhancing the accessibility of a diverse range of digital content. Despite this growing attention from the VR community, there is a serious lack of empirical research on how VR practitioners consider VR accessibility. This includes their understanding of and insights into VR accessibility challenges and practices in the VR software development life cycle. In this paper, we aim to address these gaps using a mixed-methods approach. Specifically, we conducted interviews with 21 VR practitioners (incl. 3D modelers, developers, technical directors, and product managers) and surveyed 202 VR practitioner respondents from VR-related industries. Our findings outline the insights and challenges they face concerning VR accessibility practices in the software development life cycle. Furthermore, our findings shed light on the challenges faced by practitioners concerning VR accessibility and the reasons why it often goes unconsidered. As far as we know, this is the first comprehensive report about the understanding of accessibility in the VR software development life cycle from practitioners’ perspectives. We hope this paper will help VR practitioners better understand the practices, challenges, and potential solutions related to VR accessibility."
Co-designing Large Language Model Tools for Project-Based Learning with K12 Educators,"Ravi, Prerna and Masla, John and Kakoti, Gisella and Lin, Grace C. and Anderson, Emma and Taylor, Matt and Ostrowski, Anastasia K. and Breazeal, Cynthia and Klopfer, Eric and Abelson, Hal",10.1145/3706598.3713971,2025,"The emergence of generative AI, particularly large language models (LLMs), has opened the door for student-centered and active learning methods like project-based learning (PBL). However, PBL poses practical implementation challenges for educators around project design and management, assessment, and balancing student guidance with student autonomy. The following research documents a co-design process with interdisciplinary K-12 teachers to explore and address the current PBL challenges they face. Through teacher-driven interviews, collaborative workshops, and iterative design of wireframes, we gathered evidence for ways LLMs can support teachers in implementing high-quality PBL pedagogy by automating routine tasks and enhancing personalized learning. Teachers in the study advocated for supporting their professional growth and augmenting their current roles without replacing them. They also identified affordances and challenges around classroom integration, including resource requirements and constraints, ethical concerns, and potential immediate and long-term impacts. Drawing on these, we propose design guidelines for future deployment of LLM tools in PBL."
Exploring Assumptions about Sustainability: Towards a Constructive Framework for Action in Sustainable HCI,"Laurell Thorslund, Minna and Leifler, Ola",10.1145/3706598.3714001,2025,"The global environmental crises continue to get worse, fast approaching various irreversible thresholds. While a vast array of approaches to solving sustainability problems are found under the umbrella of Sustainable HCI, their contributions are sometimes hard to compare. In this essay, we describe a set of assumptions that influence what is considered meaningful and important areas of sustainability research, along four dimensions of sustainability: 1) the depth and nature of the sustainability challenges; 2) the role of technological innovation in sustainability; 3) what gets defined as ""externalities"" to a design or system; and 4) the time perspective used to consider sustainability. We argue that what one assumes within each of these dimensions directly influences what one means by the term ""sustainability"", which is then reflected in the questions that are asked, the methods chosen, the proposed solutions and the developed systems. By describing these assumptions and some of their commensurate actions, we offer a framework that may enable members of the SHCI community to reflect on and better position their own work and that of others in the field. Our intention is for the framework to lead to better transparency and more constructive conversations about where we might collectively direct our efforts moving forward."
Vision-Based Multimodal Interfaces: A Survey and Taxonomy for Enhanced Context-Aware System Design,"Hu, Yongquan ‘Owen’ and Tang, Jingyu and Gong, Xinya and Zhou, Zhongyi and Zhang, Shuning and Elvitigala, Don Samitha and Mueller, Florian ‘Floyd’ and Hu, Wen and Quigley, Aaron J.",10.1145/3706598.3714161,2025,"The recent surge in artificial intelligence, particularly in multimodal processing technology, has advanced human-computer interaction, by altering how intelligent systems perceive, understand, and respond to contextual information (i.e., context awareness). Despite such advancements, there is a significant gap in comprehensive reviews examining these advances, especially from a multimodal data perspective, which is crucial for refining system design. This paper addresses a key aspect of this gap by conducting a systematic survey of data modality-driven Vision-based Multimodal Interfaces (VMIs). VMIs are essential for integrating multimodal data, enabling more precise interpretation of user intentions and complex interactions across physical and digital environments. Unlike previous task- or scenario-driven surveys, this study highlights the critical role of the visual modality in processing contextual information and facilitating multimodal interaction. Adopting a design framework moving from the whole to the details and back, it classifies VMIs across dimensions, providing insights for developing effective, context-aware systems."
How CO2STLY Is CHI? The Carbon Footprint of Generative AI in HCI Research and What We Should Do About It,"Inie, Nanna and Falk, Jeanette and Selvan, Raghavendra",10.1145/3706598.3714227,2025,"The energy cost of developing and deploying Generative AI (GenAI) models has exploded with their mass adoption, as has the ensuing carbon emissions. The climate impact of this is currently unknown. In Human-Computer Interaction, GenAI models are rarely trained but often used. Based on detailed review of 282 papers, we estimate this footprint from energy consumption of the total use of GenAI for CHI 2024 research as between 10,769.63 and 10,925.12 kg CO2e — equal to driving a car for more than 100,000 km. We show that in CHI research, GenAI is most often used for Prototyping, Evaluation &amp; User studies, and that Data Collection and Fine-tuning models incurs the highest CO2st.1 We find that CHI submissions are unlikely to report GenAI use transparently, which makes precise calculations difficult. By measuring the usage of a subset of the papers on local hardware, we obtain estimations of the energy consumption and carbon footprint. Based on this evidence, we discuss and demonstrate ways to mitigate the issues of GenAI carbon footprint and lack of transparency."
"Digital Technologies for Deaf and Hard of Hearing Children: a Systematic Review, Critical Reflections, and Future Research Directions","Zhao, Jing and Neto, Isabel and Pires, Ana Cristina and Tom\'{e}-Pires, Catarina and Nicolau, Hugo",10.1145/3706598.3714241,2025,"Digital technologies in Human-Computer Interaction (HCI) have the potential to support the development and well-being of Deaf and Hard of Hearing (DHH) children. Yet, there has yet to be a systematic review of the field. A shared understanding of current research is needed to develop a future vision. In this review, we analyzed 42 papers from the ACM Digital Library and the top 20 HCI Conferences and Journals, spanning the past 24 years, to investigate the trends, methods, and the level of inclusion of DHH children. Our review reveals that sign language learning platforms dominate the current technological effort. Moreover, children are not yet fully involved in the design process of these technologies and are mostly considered users and testers.We also capture a gap in integrating Deaf culture and child development in prior research. We conclude by critically examining literature gaps and offering guidance for future research."
Relatedness Technologies: An Online Compendium and Systematic Review,"Wenhart, Christiane and Ringfort-Felner, Ronda and Wallbaum, Torben and Amidi, Maryam and Albers, Ruben and Hassenzahl, Marc",10.1145/3706598.3714260,2025,"Over the past decades, numerous concepts and prototypes for fostering emotional connections across distance (relatedness technologies) have been proposed. This has made it challenging for researchers and designers in Human-Computer Interaction (HCI) to maintain a comprehensive overview and effectively build on previous work. To address this, we conducted a systematic literature search (PRISMA) and collected 241 concepts and prototypes (2010-2024). We organized this corpus according to key aspects: (1) target population, (2) theoretical grounding, (3) design, (4) evaluation, and (5) ethics. Based on this, we developed the “COmpendium of RElatedness Technologies” (CORE), an open-access, searchable online database that provides researchers and practitioners with a reliable repository to inform future work. In addition, we present a systematic review of the corpus, revealing that despite its long tradition work on relatedness technologies remains characterized by limited theoretical grounding, lack of robust empirical evidence of effects, and insufficient attention to ethical considerations."
"A Concept at Work: A Review of Motivations, Operationalizations, and Conclusions in VR Research about Presence","Xiao, Cleo and Yu, Difeng and Hornb\ae{}k, Kasper and Bergstr\""{o}m, Joanna",10.1145/3706598.3714279,2025,"Presence appears an important concept for virtual reality (VR): It is frequently measured with questionnaires, and theory and methods about it have been discussed in numerous works. Yet, it is unclear how to actually work with this concept: Why is presence important to measure, how to choose an appropriate questionnaire, and what to conclude about it based on findings? To answer these questions, we review how the concept is put to work in 288 VR papers from 2023 measuring presence with questionnaires. Our findings include that measuring presence is often motivated by another construct, such as user experience; the reasons for choosing a specific questionnaire are often weak or not reported at all; and high presence values are frequently used simply to validate an interaction technique. We propose recommendations for working with presence and formulate questions to direct future research."
How To Draw Commands? An Elicitation Study for Sketching on Spreadsheets,"Hesenius, Marc and Krvavac, Mak and Valbj\""{o}rnsson, Valbj\""{o}rn J\'{o}n and Theresia Mita Erika and Book, Matthias",10.1145/3706598.3715269,2025,"Sketching is one of the oldest techniques humans use to express themselves. We sketch to visualize concepts, externalize memory, and communicate ideas. However, we barely use sketching to interact with computers. Given how naturally sketching comes to humans, we believe untapped potential exists in being able to simply draw commands onto a user interface. In this paper, we present results of an elicitation study about expressing common operations in spreadsheets through sketching. Spreadsheets are an interesting class of applications because they are widely used, support complex data and operations, and are available on touch-enabled devices. Our results show that despite considerable variation in syntactic details, participants gravitate towards recurring patterns (e.g., enclosures and arrows, examples and cross-references, and temporal sequences of strokes). The sketch patterns we identified can be a first step towards developing interpreters of sketched commands, and thus enable new means of interacting with spreadsheets and other applications."
Walk in Their Shoes to Navigate Your Own Path: Learning About Procrastination Through A Serious Game,"Zhang, Runhua and Gan, Jiaqi and Gao, Shangyuan and Chen, Siyi and Wu, Xinyu and Chen, Dong and Tian, Yulin and Wang, Qi and An, Pengcheng",10.1145/3706598.3715271,2025,"Procrastination, the voluntary delay of tasks despite potential negative consequences, has prompted numerous time and task management interventions in the HCI community. While these interventions have shown promise in addressing specific behaviors, psychological theories suggest that learning about procrastination itself may help individuals develop their own coping strategies and build mental resilience. However, little research has explored how to support this learning process through HCI approaches. We present ProcrastiMate, a text adventure game where players learn about procrastination’s causes and experiment with coping strategies by guiding in-game characters in managing relatable scenarios. Our field study with 27 participants revealed that ProcrastiMate facilitated learning and self-reflection while maintaining psychological distance, motivating players to integrate newly acquired knowledge in daily life. This paper contributes empirical insights on leveraging serious games to facilitate learning about procrastination and offers design implications for addressing psychological challenges through HCI approaches."
A Full-Process Closed-Loop Dynamic Management System for Research and Teaching in Hospital,"Ding, Wanfu and Yang, Yao and Tian, Qian and Wang, Yiqun and Zhang, Yuming and Wu, Zilin",10.1145/3706890.3706903,2025,"With the rapid development of information technology in hospitals, there is a growing demand for more intelligent, refined, and scientific management. As essential components of hospital operations, residency training and clinical teaching require intelligent implementation to ensure smart management and efficient operation. This paper proposes and implements an integrated system for medical research and education, which has been piloted in a hospital setting. Experimental results demonstrate that the system effectively reduces labor costs, strengthens the management of research data throughout the entire process, and comprehensively enhances the efficiency, quality, and overall governance of medical research and education management in hospitals."
Research on Refined Management System of Patient Complaints and Disputes to Build Harmonious Doctor-patient Relationship,"Ding, Wanfu and Yang, Yao and Tian, Qian and Wang, Yiqun and Zhang, Yuming and Wu, Zilin",10.1145/3706890.3707033,2025,"With the continuous deepening of medical reform and the increasing investment in informatization in China, the market-oriented characteristics of medical services have become more prominent. It is far from enough to simply improve the level and quality of medical technology. We must also attach importance to harmonious doctor-patient relationships and listen to the voices and needs of patients. Whether we can timely and effectively meet patients' medical treatment and health needs, and transform their potential needs into real ones, is related to the survival and development of hospitals. At present, there are traditional manual recording models for complaint handling in hospitals to varying degrees. Information-based management of complaint handling is an urgent problem to be solved. This paper analyzes this issue in depth from three aspects: petition business, complaint management channels, and the application of information technology. By empowering hospitals with new generation information technology, the service quality and work efficiency can be greatly improved."
Analysis of the Research Status and Development Trend of Urban Transportation System in the Era of Artificial Intelligence,"Deng, Tingyin and Jiang, Xuemei and Wang, Meiling and Luo, Jingwen",10.1145/3707292.3707372,2025,"The swift growth of industrial urban areas urgently calls for a shift from conventional to intelligent urban centers. In this context, transportation emerges as a primary entry point and fundamental pillar for this transformation, poised to revolutionize urban management and service models. To uncover the evolutionary trends of urban transportation systems in the age of artificial intelligence, this paper utilizes the Web of Science core database as its retrieval source, employing tools such as Citespace 6.2R6 and Vosviewer for visual analysis of the retrieved literature. The research findings indicate that the overall number of publications in this field has shown an upward trend from 2005 to 2024, passing through three stages: emergence, exploration, and stabilization. China stands out as the leading force in this research domain, with ZHANG J identified as a key author exerting significant influence. The research hotspots converge around four themes: intelligent transportation management, deep intelligent models, intelligent transportation systems(ITS) and smart transportation frameworks. In summary, ITS have witnessed substantial development, and their role in future urban transportation management is set to become increasingly important. With continuous technological advancements and sustained policy support, it is anticipated that ITS will achieve broader applications and deeper research in the coming years."
R&amp;D and application of whole-process control platform for engineering projects based on EPC and big data,"Xi, Yixiang and Huang, Baixue and Liao, Xiaohang",10.1145/3708036.3708044,2025,"The digital platform can realise the detailed management of project progress, quality, safety and resources, and provide three-dimensional guidance for project construction. By providing progress management function modules, it enables managers to track and supervise the progress of the project and identify and deal with problems that may arise in a timely manner. Through the quality control of the project, the quality indexes of each project can be managed in a standardised manner, and the quality of the project can be guaranteed to meet the predetermined requirements through testing and evaluation of the project. Through the safety management of the project, the safety norms and operation procedures of the EPC (Engineering Procurement Construction) project are formulated, and the hazards of the project are recorded and analysed, so as to ensure the safety of the project. With the sustained and steady growth of China's economy in recent years and the accelerating pace of urbanisation, the people's living standards continue to improve; at the same time, due to China's large population base, the per capita housing area is small, so that China's large and medium-sized cities, commercial housing, sheltered housing and public building demand remains high, the construction industry development is booming. Construction not only need to invest a lot of labour and machinery, there are a large number of building materials, semi-finished products production, transport, storage and supply work, to make people, materials, machinery and other factors of production in an orderly manner to play its role, the difficulty and complexity is increasing. Advanced and reasonable schedule management methods can enable construction enterprises to achieve the optimal balance of construction cost and construction period while realising project quality objectives’. Project schedule control is very important for the success or failure of project management, on the basis of meeting the requirements of project quality, the optimisation of cost and duration determines the success or failure of project management. Therefore, it is of far-reaching significance to explore the project schedule management techniques."
Evaluation Method for Standardized Operation Quality of the Project Management System Based on Fuzzy Analytic Hierarchy Process,"Huang, Binxi and Li, Xin and Mai, Hongsheng and Jie, Zhidan and Mai, Xiaohui",10.1145/3708036.3708133,2025,"This study presents a novel method for evaluating the standardized operation quality of project management system using the Fuzzy Analytic Hierarchy Process (FAHP). By integrating quantitative and qualitative factors, FAHP addresses uncertainties inherent in traditional methods. The method was applied to a project management system in the electric power industry, revealing strong management procedures, high process consistency, and clear evaluation standards, with room for improvement in result traceability. Compared to traditional methods, FAHP provided a more balanced and reliable evaluation. The findings offer insights for optimizing project management practices and highlight the method's potential for continuous monitoring and improvement across various industries."
Venture Capital and AI Transformation: Evidence from China's A-Share Listed Companies,"Jiang, Lingfang and Huang, Qingcheng and Yuan, Gonglin and Tang, Jiahuan",10.1145/3708036.3708179,2025,"In the era of rapid development of the digital economy, the implementation of AI transformation is regarded as a key means of enhancing corporate competitiveness. As a major driver of technological innovation, whether venture capital (VC) can facilitate the AI transformation of companies remains underexplored. This study analyzes a sample of non-financial A-share listed companies in China. The results show: (1) VC promotes AI transformation in companies; (2) Mechanism tests demonstrate that VC supports AI transformation by improving the company's financing structure and reducing short-term decision-making by management. (3) Heterogeneity tests show that the impact of VC in driving AI transformation is more significant in companies operating in high-tech industries and those with similar business models. (4) Economic consequence tests indicate that VC-driven AI transformation reduces performance volatility and enhances total factor productivity. The conclusions of this study offer corporate managers and policymakers a more detailed understanding of VC's role in promoting AI transformation in companies, providing both theoretical foundations and practical guidance for developing AI-supportive policies."
Design and implementation of engineering integrated practical training management system under the background of engineering education,"Xue, Tianbao and Lan, Quanxiang",10.1145/3708036.3708215,2025,"With the promotion of global economic integration, internationalised engineering education has become an important way for colleges and universities to improve their teaching quality and international competitiveness. In order to cultivate students' ability to solve complex engineering problems, many colleges and universities adopt the mode of integrated practical training projects. However, this model has the characteristics of long duration, multi-stage tasks, multi-dimensional assessment, etc. The use of traditional management methods will lead to management chaos, low teaching efficiency, and is difficult to adapt to modern teaching needs. For this reason, this paper designs and develops an intelligent, automation engineering integrated practical training management system based on B/S architecture. The system adopts SpringBoot, Vue, MySQL and Redis technologies to optimize the management process of the practical training project by automating the management of the project, marking scheme and grade analysis. In addition, the system has real-time performance analysis and feedback functions, which improves the efficiency of teaching management and provides strong support for universities to cultivate composite talents with innovative ability and comprehensive engineering literacy."
Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks,"Peng, Yingzhe and Qin, Xiaoting and Zhang, Zhiyang and Zhang, Jue and Lin, Qingwei and Yang, Xu and Zhang, Dongmei and Rajmohan, Saravan and Zhang, Qi",10.1145/3708359.3712093,2025,"The rise of large language models (LLMs) has revolutionized user interactions with knowledge-based systems, enabling chatbots to synthesize vast amounts of information and assist with complex, exploratory tasks. However, LLM-based chatbots often struggle to provide personalized support, particularly when users start with vague queries or lack sufficient contextual information. This paper introduces the Collaborative Assistant for Personalized Exploration (CARE), a system designed to enhance personalization in exploratory tasks by combining a multi-agent LLM framework with a structured user interface. CARE’s interface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling iterative query refinement and dynamic solution generation. The multi-agent framework collaborates to identify both explicit and implicit user needs, delivering tailored, actionable solutions. In a within-subject user study with 22 participants, CARE was consistently preferred over a baseline LLM chatbot, with users praising its ability to reduce cognitive load, inspire creativity, and provide more tailored solutions. Our findings highlight CARE’s potential to transform LLM-based systems from passive information retrievers to proactive partners in personalized problem-solving and exploration. The code will be made available at https://aka.ms/chatbot-care."
Technologies Supporting Self-Reflection on Social Interactions: A Systematic Review,"Hao, Chenxu and Matej Hrkalovic, Tiffany and Balliet, Daniel and Hung, Hayley and Dudzik, Bernd",10.1145/3708359.3712158,2025,"As intelligent technology and applications have become an integral part of nearly all aspects of people’s daily lives, many intelligent systems have been designed to help people navigate the complex space of social interactions. One prominent strategy for such intelligent support is providing meaningful Ad Hoc Interventions (ADI), e.g., through timely notifications. An alternative is Technology-Supported Reflection (TSR), e.g., by offering information about activities in one’s past for personal insights. In contrast to straight-up interventions, the aim of the latter strategy is not to directly augment human skills but instead support learning and personal growth over time. However, while TSR has seen widespread interest in applications in some areas, such as physical fitness and mental health, its use for improving human social interactions has not yet been systematically explored. Concretely, it is currently unclear 1) what forms of self-reflection systems intend to support, 2) how their different technological components (e.g., data collection, information integration) are involved in providing support, and 3) what common limitations and design challenges they face. In this article, we present the results of a systematic literature review focusing on these questions to provide a structured foundation for targeted research. Concretely, we identified and analysed a collection of 23 relevant papers, each describing a system deploying TSR to support humans with elements of social interactions.We constructed a framework with a set of features to comprehensively describe and analyze the systems that support self-reflection, including their application domains, how they fit into the existing design framework, how they facilitate learning through reflection, how adaptive they are to individual users, and how they were evaluated. Finally, we propose a direction for designing systems that support individual’s social interactions through self-reflection in an adaptive manner."
Automated Test Case Output Generation Using Seq2Seq Models,"Ozer, Edipcan and Akcayol, Muhammet Ali",10.1145/3708635.3708644,2025,"The aim of this paper is to present a creative approach to generate test case outputs for a given input automatically for software testing. Sequence-to-sequence (seq2seq) model is applied. Our approach aims to address the challenge of creating meaningful test case outputs for input variations in software testing, improving efficiency and accuracy in test automation. With the help of natural language processing techniques, the model is trained on an original dataset of test inputs and their corresponding outputs, predicting the output for a given test case input. We employ evaluation metrics including BLEU, ROUGE, and JACCARD similarity scores to assess the quality of generated outputs, comparing them against reference outputs. Our initial results show that the seq2seq model has a huge potential of producing accurate test case outputs, significantly reducing manual effort in test case generation. This work demonstrates the potential for integrating Recurrent Neural Network techniques into software testing and providing a scalable solution for automated test case output generation."
A review of practices suitable for ethics-aware software engineering,"Marebane, Senyeki Milton and Mnkandla, Ernest",10.1145/3708635.3708651,2025,"Research shows that the lack of ethical practices for regulating human behaviour and ethical judgment in software development is a concern.  The lack of ethical practices leads to the development of software which is not in line with the ethical needs of software stakeholders. The consideration of the ethical needs of stakeholders and their integration into the software process requires knowledge of the relevant practices.  This study sought to identify and synthesise ethical practices suitable for supporting ethics-aware software engineering.  To achieve the objective of this study literature review was conducted based on search keywords.  This study determined that ethics awareness, enforcement, decision-making, evaluation, and governance are integral ethical practices for achieving ethics-aware software engineering practice to cater for stakeholder ethical needs.  Although the findings of this study are based on literature sources, future empirical studies should be conducted to validate the combination of these ethical practices to determine how they can support ethics-aware software engineering practice."
Sentiment Analysis-Based Review of Sentiment Polarity in Translations,"Almarzoqi, Arwa and Alsuhaibani, Mohammed",10.1145/3709026.3709092,2025,"Translation process has risen as a pivotal component in implementing sentiment analysis for low-resource languages. Despite the usefulness of the translation process, it suffers from certain challenges and impacts on the sentiment level. This includes the potential of altering sentiment polarity and producing translations that fail to preserve the original sentiment of the source texts. As the translation process is increasingly used across various contexts, analyzing the dimensions of sentiment polarity in translations has become valuable. A number of studies examining sentiment polarity in translations have been published, highlighting the need for a comprehensive review. Therefore, this paper presents a PRISMA-based review that analyzes three lines of research, including sentiment analysis using translation, challenges and impacts of translation on sentiment polarity, and sentiment preservation in translations. This paper analyzes the included papers comprehensively using key points, including main focus, approach, model, and outcomes. It contributes to the field of sentiment analysis by providing in-depth insights into the current trends concerning sentiment polarity in translations, paving the way for future investigations in this area."
Education Relationship in Online Education: A Scoping Review on the Phenomenological Pedagogy Perspective,"Wang, Jiaojiao and Pek, Lim Seong",10.1145/3711403.3711423,2025,"The development of Internet digital technology has given rise to online education. To meet the needs of social development and lifelong learning, the widespread application of online education has attracted many researchers to study it. Undoubtedly, due to the development of online education, the nature and structure of educational relationships have changed significantly.&nbsp;Using the PRISMA method, this study screened 538 existing relevant studies in three literature resource libraries, Scopus, WOS,&nbsp;and ProQuest. Explore the impact of AI-based educational technology on educational relationships from a phenomenological perspective. Finally, 8 closely related studies that meet the criteria were screened out, and it was found that there are 5 core elements in the research of this topic: educational technology, phenomenology, embodied learning, online education, and online educational relationships. It is pointed out that these 5 elements provide a systematic research direction for future research to further optimize educational relationships in online education and improve the effectiveness and quality of online learning."
"A Systematic Literature Review Study of Digital Transformation in Higher Education: Technologies, Challenges and Trends","Hu, Xuting and Xu, Yuanyuan and Liao, Han and Ma, Jiqiao",10.1145/3711403.3711424,2025,"The digital transformation of higher education is occurring rapidly around the globe, leaving an indelible mark on improving the quality of education, expanding the accessibility of educational resources, and realizing innovation and reform in teaching and research. The extant research on digital transformation in higher education is relatively limited in scope and has certain inherent limitations. Therefore, this paper employs the method of systematic literature review (SLR) to provide a comprehensive analysis of the digital transformation of higher education from three perspectives: digital technology, existing challenges, and future trends. This analysis aims to identify potential correlations between these three dimensions and offer valuable insights for those engaged in the education industry and researchers. Furthermore, this study seeks to deepen our understanding of digital transformation and to provide guidance for future research directions."
A Review of the Role and Impact of Generative Artificial Intelligence on Education,"Yang, Qi",10.1145/3711403.3711435,2025,"In order to ensure quality development in the age of intelligence, it is crucial to integrate intelligent technology with education. Artificial Intelligence (AI) and Generative Artificial Intelligence (GAI) are disruptive technologies in the area of education. While online education brings significant advantages in enhancing educational quality, promoting educational equity, and improving educational efficiency, it has also raised concerns among scholars around the world regarding students' moral ethics, cultivation of emotional values, technological dependence, thinking deprivation, privacy, and policy making. Using Cite Space software to analyze more than 50 articles from core journals in the field of educational technology at home and abroad, this paper comprehensively summarizes the role and impact of generative AI in education up to 2023, suggests the limitations of generative AI in empowering education at present, and predicts the direction scholars will tend to research in this field in the future."
Literature Review of Personalizing Learning Recommendation Systems Using Machine Learning in Chinese Higher Education,"Huang, Mingjing and Cheong, Ngai and Liu, Jiaqi and Zhang, Zhuofan",10.1145/3711403.3711441,2025,"With the rapid development of information technology, personalized education recommendation systems have gained widespread attention and rapid development in China's education sector. These systems provide customized learning resources and course recommendations to meet the individual needs of different students by analyzing their learning behaviors, preferences, and performance. This study comprehensively searched 45,623 literatures from 2002 to 2022, focusing on the current status, key technologies, and application practices of educational recommendation systems in China. It first investigates the basic deep learning recommendation methods used in current personalized learning recommendation systems, and then summarizes the current status of machine learning techniques applied in personalized learning recommendation systems for higher education in China. Finally, through a comprehensive analysis of the existing literature, we discuss the current challenges and future development direction of China's educational recommendation systems, and put forward improvement suggestions for domestic educational recommendation systems, aiming to fill the research gaps and promote technological progress and innovation."
The Research on the Application Modes of Intelligent Instructional Design Generation in Primary and Secondary Schools Supported by Large Models,"Lin, Jian and Wang, Xiaoyi and Li, Bingjun and He, Musheng and Luo, Yufei",10.1145/3711403.3711452,2025,"As the development of large language models and generative artificial intelligence technologies continues to advance, the application of digital teaching resources is exhibiting a new trend of human-machine collaboration and co-creation. As an important digital teaching resource for the implementation of smart teaching, the application of instructional design documents has been significantly influenced by the development of intelligent technologies, especially in the field of primary and secondary education, which holds significant research value. However, there is a clear gap between the value claims and actual effects of traditional instructional design, which limits the professional development of teachers and hinders the comprehensive development of students. This research aims to combine ChatGPT-4 models and generative artificial intelligence technology to construct an intelligent instructional design system platform, achieving the intelligent transformation and upgrading of instructional design."
Research on Blended teaching of Artificial Intelligence and Machine Learning,"Guo, Yunying and Li, Xiaofei and Zhang, Tianyu",10.1145/3711403.3711480,2025,"Based on the results-oriented OBE education concept, in order to cultivate application oriented innovative talents, artificial intelligence and machine learning courses adopt the blended teaching mode. The online learning platform is used to combine offline classroom and online independent learning, and the practical process of blended teaching is described from before, during and after class. Through the scientific setting of KT point and the expected learning effect mapping relationship to assess the level of achievement, we evaluate the impact of blended teaching reform and reflect on our teaching practices. Blended teaching in artificial intelligence courses presents the best scenario for learning, as it combines traditional and online teaching to enhance the understanding and engagement. It offers more personal and flexible learning, more interactions, and practical opportunities for the students to learn."
Creating Agile Development Practices in Virtual Workspaces: Exploring the Potential of VR Technologies,"Pu, Hongjian",10.1145/3711496.3711499,2025,"This study explores the integration of Virtual Reality (VR) technology into Agile development practices, focusing on analyzing the effectiveness of core practices such as virtual user story walls, pair programming, and daily stand-up meetings within a virtual environment. By developing a Unity-based virtual office, this research aims to evaluate how VR technology can improve team immersion, enhance communication efficiency, and support more effective collaboration, particularly for geographically distributed teams. Participant feedback indicated that integrating Agile practices in VR environments not only accelerated development but also improved visualization of task management and strengthened team member interaction. Furthermore, the immersive environment of VR allowed teams to experience more intuitive and immediate forms of communication, which promoted a stronger sense of presence and cohesion among members. However, notable challenges include high equipment costs, technical complexity, and the learning curve associated with user adaptation to immersive technologies. This study summarizes both the opportunities and challenges presented by VR, offering valuable insights and recommendations for future research and practice, particularly regarding improving the scalability and accessibility of VR in Agile development contexts."
Research on Storage Strategies for Multi-source Heterogeneous Subway Monitoring Data and Anomaly Detection Technologies,"Wang, Xinling and Sun, Jinglai and Zhang, Zhuxin and Su, Yue and Fang, Hui and Liu, Hao",10.1145/3711618.3711643,2025,"This paper presents a comprehensive study on efficient storage strategies and anomaly detection techniques tailored for multi-source heterogeneous subway monitoring data. By leveraging advanced deep learning models, we aim to address the challenges posed by the vast amount, diversity, and complexity of data generated by modern subway systems. A novel framework is introduced, which integrates optimized data storage solutions with a deep neural network-based anomaly detection system. Experimental results on synthetic subway dataset demonstrate the effectiveness of our approach in terms anomaly detection accuracy. The proposed method achieves significant improvements over traditional methods, highlighting its potential for monitoring and predictive maintenance in subway systems."
Research on the Military Application and Development Suggestions of Artificial Intelligence,"Li, Shilong and Zhang, Chenyi and Yang, Zhihan",10.1145/3714334.3714336,2025,"With the rapid development of artificial intelligence (AI) technology, various countries have actively promoted the development of military intelligence in an attempt to seize the initiative in the military intelligence revolution. This paper first sorts out and discusses the current application status of AI in the military field, systematically analyzing the application of AI technology in areas such as situation awareness and intelligence analysis, intelligent decision-making and decision support, intelligent development of weapon systems, and intelligent offensive and defensive capabilities in cyber warfare. Subsequently, the paper delves into the developmental experiences of the United States and Russia in the militarization of AI from multiple perspectives, including strategic layout, technological research and development, talent cultivation, and military-civilian integration. Finally, based on the aforementioned analysis, this paper proposes specific recommendations for the militarization of AI in China from the perspectives of national top-level planning, investment and financing channels, military-civilian collaboration, and international cooperation. The research aims to promote the healthy development of AI militarization applications in China and provide support for safeguarding national security."
Broadening Data Science Education: An Experience Report,"Deb, Debzani and Betz, Scott and Fuad, Muztaba",10.1145/3716640.3716641,2025,"Data Science is an essential concept for the twenty-first-century workforce. As a result, the need to incorporate core data-intensive skills into nearly every discipline has recently gained increased attention. This paper details our experiences developing a framework where CS and non-CS faculties collaborate on developing contextual data science modules tailored to the needs of multiple disciplines, academic levels, and student and instructor preparedness and integrate them into existing undergraduate courses. A quantitative and qualitative analysis approach is used to explore the following research question: To what extent a common framework be utilized to integrate data science-based learning objectives into multiple science, social, and health science courses to generate more data science-informed graduates? A significant number of underrepresented and minority (URM) students were exposed to data science knowledge and skills throughout this intervention during the last three years. The student performance and survey results show that most students understood the concepts and could use related tools to organize and use data to support their claims and conclusions to a certain extent. Further analysis of instructor perspectives identified commonalities, such as the benefits of incorporating data science concepts into disciplinary contexts through hands-on exercises using real-world data, as well as differences, such as student activity types based on instructor and student preparedness and usage and rigor of data science tools."
Exploring Gender Disparities and Collaborative Learning in IT Education,"Yeom, Soonja and Herbert, Nicole and Ryu, Riseul",10.1145/3716640.3716645,2025,"Despite efforts to foster gender diversity, women remain underrepresented in Information Technology (IT). Existing research indicates that women often underestimate their abilities in comparison to men. This study investigates this perceived performance gap hypothesis. This study is unique for its extended scope spanning six years and its focus on postgraduate IT students with 39% of women. This study first examines performance disparities between genders in introductory programming. Findings reveal minimal differences, suggesting that women perform comparably to men. However, to encourage broader participation, there is a need for initiatives that enhance the learning environment for women. This study also explores the influence of collaborative learning on performance. No significant improvements in project performance were identified and no significant performance differences were found between genders in group-based projects. The findings, which reveal the intricate interplay among gender, performance, and collaborative learning, are significant for teaching practice, especially at the postgraduate level."
An Experience Report on a Conversion Masters Degree Program in Computer Science for Non-CS Majors,"Islam Molla, Md Tahmidul and Krenz, Gary and Kaczmarek, Thomas and Moyer, John",10.1145/3716640.3716653,2025,"Computing is a fast-growing profession with excellent job prospects. However, for individuals who have already earned a non-computing related baccalaureate degree, the time, effort, financial challenges, and education required to enter computing can be daunting and even intimidating for academically talented, low-income, post-baccalaureate career changers. In this work, we propose a model that provides a relatively quick turn-around for students with no computing background to obtain an MS in Computer and Information Science (CIS). The model offers a highly focused bridge course combined with a follow-up curriculum that allows people without undergraduate CS-related degrees to merge quickly and efficiently into a professional MS in CIS program. We present the structure of our conversion program, the context that motivates it, and our seven years of experience providing the program. To evaluate the effectiveness of our conversion program, we conducted surveys with thirty conversion students, thirteen recent graduates, and twenty-seven CIS instructors from 2017 to 2024. Our data shows that conversion students perform comparably to traditional students in terms of GPA. The survey results from both students and instructors indicate that the program was successful in transitioning students from non-CS majors to CIS and in preparing them for CIS-related jobs. The study findings emphasize the importance of fostering cohort camaraderie and the need for supplementary assistance to address conversion students’ unique needs. We hope that our experience with the conversion program and promising results will encourage other institutions to introduce MS in CIS programs for post-baccalaureate individuals with a non-CS background."
An Intelligent Aids to Navigation Sysetm for Waterway Transportation,"Qu, Yuwei and Wu, Fawei",10.1145/3716895.3716896,2025,"Aids to navigation is one of the indispensable links in water transport. Aids to navigation refers to a sign that provides position and direction information for navigation, which can guide ships to pass through waters safely, and is an indispensable and important part of water transport. This paper emphasizes the importance of aids to navigation task in water transportation from the aspects of definition and function of aids to navigation task, types and layout of aids to navigation, maintenance and task of aids to navigation, and put forward relevant suggestions. Recent advancements, including autonomous systems and AI integration, signify promising directions. Intelligent Aids to navigation Systems (INMS) bind AI, data analytics, and predictive maintenance for hazard detection, route optimization, traffic flow, and congestion reduction. A detailed experimentation validates INMS superiority over existing systems, highlighting its potential to transform waterway safety and efficiency."
Identification and Evaluation Optimization of Comprehensive Risk Factors for Pumped Storage Power Stations Based on DEMATEL-AISM,"Ding, Shuo and Zhou, Ziyuan and Guan, Kai and Jiang, Yali",10.1145/3716895.3716954,2025,"As the construction of pumped storage power plant enters into a rapid development stage, the comprehensive risk analysis and prevention and control for each stage of its project implementation becomes crucial. This paper identifies 18 key risk factors based on literature analysis and expert questionnaire survey, analyzes the influence level of key risk factors of pumped storage power plant using DEMATEL-AISM method, and studies the influence relationship and causal attributes between each risk factor. The results show that: the comprehensive risk factors of pumped storage power plants can be divided into 6 basic levels and 3 causal levels; 8 cause factors and 10 effect factors are analyzed and identified, among which 10 factors such as changes in construction plan, insufficient construction technical ability and high operation and maintenance cost are important risks affecting pumped storage power plant projects; 3 dimensions of preventing fundamental risks, cutting off risk propagation and monitoring direct risks are given. Corresponding comprehensive risk prevention and control measures for pumped storage power plants are given to lay a theoretical foundation for the risk management of the whole life cycle of pumped storage power plant construction."
Discussion on Digital Delivery of Highway Projects based on BIM,"Li, Xiangyong and Shao, Yunqi",10.1145/3716895.3716974,2025,"While BIM technology has been widely applied in the design stage of highway engineering, the application of digital delivery standards and methods lags behind, and has become a bottleneck restricting the effective transmission of engineering information from the design stage to the construction stage and maintenance stage. By summarizing the digital delivery methods used in relevant industries both domestically and internationally, and taking into account the industry's own characteristics, key technical issues in digital delivery of highway engineering were identified. Combined with the implementation of Nanjing Wuhu Expressway Project, a new method for digital delivery of highway engineering was proposed."
Leveraging Big Data Analytics for Carbon Neutrality: A Review on Green Supply Chain and Sustainable Project Management,"Shen, Yutong and Deng, Bowei and Xu, Zizhen",10.1145/3716895.3717010,2025,"This literature review underscores the transformative potential of Big Data Analytics (BDA) in Green Supply Chain Management (GSCM) and Sustainable Project Management (SPM), positioning it as a critical tool for achieving carbon neutrality. By synthesizing the findings from recent studies, it clarifies how BDA improves supply chain visibility, helps resource optimization, and reduces environmental footprint, while also identifying gaps in existing frameworks and calling for future research on integrating BDA in project management. Additionally, through practical recommendations for real-time sustainability metrics and decision-making strategies, this research contributes to bridging the gap between theoretical advances and practical implementation in engineering contexts, ensuring that project managers across industries can leverage BDA to meet their sustainability objectives effectively."
Planning method of underground space layout in new urban area based on spatiotemporal data,"Fan, Chao",10.1145/3716895.3717023,2025,"This paper analyzes the data-driven requirements of underground space planning under the guidance of composite goals, summarizes the basic characteristics of multi-source spatio-temporal data and the application scenarios of underground space planning. Facing the development requirements of national space planning and governance in the new era, this paper interprets the transformation of underground space planning paradigm in multi-source spatio-temporal data environment from the perspectives of technology methods, forms of results, management and control modes, and management and control elements. According to the current situation of data-driven underground space planning, the corresponding improvement strategy is proposed, and the data-driven underground space planning framework is constructed."
A Bibliometric Analysis of the Current Status and Trends in Regional Cultural and Creative Design,"Deng, Tingyin and Luo, Jingwen and Jiang, Xuemei and Wang, Meiling and Wei, Liting",10.1145/3716895.3717026,2025,"Geographical cultural creativity has disrupted the rigid design and production patterns of traditional products from a market perspective, emerging as a crucial means to enhance industrial economic efficiency and market competitiveness. To delve into the evolutionary trends of the Regional Cultural Creative Design (RCCD) field, this study utilizes the Web of Science core database(WOSCD) as its data source and employs tools such as CiteSpace (6.2R6) and VOSviewer for visual analysis of the retrieved literature. The results reveal that the publication volume in this field has shown an overall upward trend from 2004 to 2024, specifically undergoing three developmental stages: emergence, exploration, and surge. The primary research forces are concentrated in the USA, China, and the UK, with author ZHANG X standing out as a core and influential figure in the field, having published seven papers and thus being recognized as a key contributor. The research hotspots are centered around four themes: Innovative Design, Design Cognition, Cultural Health, and Creative Economy. In conclusion, the rapid development of the RCCD field is poised to bring new growth opportunities to the global cultural and creative industries (CCI) and contribute to sustainable societal development."
Adoption of RMVRVM Paradigm in Industrial Setting: An Empirical Study,"Singh, Lavneet and Tiwari, Saurabh",10.1145/3717383.3717390,2025,"RMVRVM (Remote-Model View Remote-View-Model) creates energy-efficient cloud-connected user applications. The paradigm replaces the MVVM paradigm in developing cloud-connected UI-heavy applications that run on battery-operated devices. Since the paradigm significantly affects the architecture of cloud-connected applications, adopting the RMVRVM paradigm could be challenging for IT professionals. In this paper, we present the results of the empirical study conducted to assess the acceptability and challenges of adopting the RMVRVM paradigm in industrial settings. We carried out the empirical study in two phases. In Phase 1, we conducted an online survey with the industry professionals and captured their experiences with the paradigm. In Phase 2, we solicited the opinions of domain experts by conducting focused interview sessions. The qualitative and quantitative analysis of the data shows that the proposed paradigm is acceptable among IT professionals and could be adopted in the industry, provided certain challenges are addressed before adoption."
Research on enterprise-level management decision-making assistant technology based on improved SRL algorithm,"Yu, Jiayin and Li, Zizhen",10.1145/3717664.3717697,2025,"In order to improve the efficiency of enterprise management, the research on enterprise-level management decision-making assistant technology based on improved SRL algorithm is proposed. Under the improved SRL algorithm, the enterprise-level management decision-making assistance levels are divided into initial cognitive level, basic analysis level, deep insight level and intelligent decision-making support level, from which enterprise-level management decision-making assistance indicators are constructed, and these indicators are analyzed to construct the relationship between enterprise-level management decision-making assistance businesses and realize enterprise-level management decision-making assistance process design. The experimental results show that the improved SRL algorithm has significantly higher computational efficiency than existing models in high complexity business scenarios, while maintaining a high level of decision accuracy."
GitHub Repository Complexity Leads to Diminished Web Archive Availability,"Calano, David and Nelson, Michael and Weigle, Michele",10.1145/3717867.3717920,2025,"Software is often developed using versioned controlled software, such as Git, and hosted on centralized Web hosts, such as GitHub and GitLab. These Web hosted software repositories are made available to users in the form of traditional HTML Web pages for each source file and directory, as well as a presentational home page and various descriptive pages. We examined more than 12,000 Web hosted Git repository project home pages, primarily from GitHub, to measure how well their presentational components are preserved in the Internet Archive, as well as the source trees of the collected GitHub repositories to assess the extent to which their source code has been preserved. We found that more than 31% of the archived repository home pages examined exhibited some form of minor page damage and 1.6% exhibited major page damage. We also found that of the source trees analyzed, less than 5% of their source files were archived, on average, with the majority of repositories not having source files saved in the Internet Archive at all. The highest concentration of archived source files available were those linked directly from repositories’ home pages at a rate of 14.89% across all available repositories and sharply dropping off at deeper levels of a repository’s directory tree."
"Research on risk management of international engineering “investment, construction and operation integration” infrastructure project based on SEM","Jiang, Yali and Zhao, Miaomiao and Li, Meng and Guo, Ping and Ding, Shuo",10.1145/3718491.3718590,2025,"Since China proposed the “One Belt, One Road” initiative, the Chinese foreign contracting engineering industry has undergone significant transformation and upgrading, with integrated investment, construction, and operation projects emerging as a new trend in global infrastructure development. Through a literature review and analysis of the characteristics of overseas integrated investment, construction, and operation projects, a risk index system comprising 26 secondary indicators across 8 dimensions (politics, law, economy, society, financing, construction, operation, and cooperation) was developed. A structural equation model for project risks was then developed and tested. The results indicate that partner trust risks, lack of project experience, and project management risks significantly affect the risks associated with “going global” integrated investment, construction, and operation projects, making them key areas for risk supervision. Recommendations are also provided to assist in project decision-making."
Design and Implementation of a Medical Insurance System Based on Blockchain Smart Contract Technology,"Geng, Guanhao",10.1145/3718677.3718715,2025,"With the breakout of blockchain production, its decentralized, unfaltering, and obvious characteristics are progressive within the area of medical insurance. This paper aims to provide for the growth and implementation of a blockchain-based health insurance system smart contract addressing issues such as inadequate record security, data asymmetry, and low operational efficiency in traditional health insurance systems. Using a detailed investigation of blockchain technology and its ismart contract functions, this paper builds a blockchain-based medical insurance system, which is made up of policy management, automatic claims processing, and data sharing. It also offers a thorough discussion on the architecture design of the system, the main algorithms of the smart contracts, and the actual development process. Experimental results show that the developed system greatly improves data transparency, security, and operational efficiency. Research shows that blockchain technology can be used in the health insurance field and it contains clear connections of such with the newly afforded technical benchmarks. The conclusions of this study produce new points and technical explanations for the application of blockchain technology in health insurance."
Practice and Exploration of Interactive and Collaborative English Teaching Based on Online Collaboration Models,"Zhang, Lin",10.1145/3718751.3718809,2025,"This paper explores the importance of interaction and collaboration in English teaching, leveraging expertise in computer science to practice and investigate based on online collaboration models. Through literature review, case analysis, and empirical research, it discusses the selection of online collaboration tools, online teaching design, implementation strategies, and the challenges faced. It summarizes the advantages of online collaboration models in enhancing students' language proficiency, promoting active learning, and improving teamwork skills. The study indicates that online collaboration models have broad application prospects in English teaching but highlights the need for attention to technical support, teacher training, and instructional design."
Research on Intelligent Management Method of Assembled Construction Project Scheduling Based on BIM+Digital Twin Technology,"Wang, Yijia",10.1145/3718751.3718814,2025,"The conventional intelligent management method of prefabricated construction project scheduling mainly uses the STC (Starting Time Criticality) decentralized buffer method to generate robust construction plans, which is vulnerable to the impact of the delay of penalty cost calculation, resulting in high deviation rate of management costs. Therefore, a management method based on BIM+digital twin technology is proposed. That is to identify the risk factors of intelligent management of prefabricated construction project scheduling, build an intelligent management model of prefabricated construction project scheduling using BIM+digital twin technology, and achieve intelligent management of prefabricated construction project scheduling. The case analysis results show that the designed intelligent management method of prefabricated building BIM+digital twin project scheduling has low deviation rate of management cost, short scheduling cycle, reliability and certain economic value, and has made certain contributions to reducing energy consumption of prefabricated buildings and meeting the development requirements of green buildings."
An evaluation of commonly used Kubernetes security scanning tools,"Kapetanidou, Ioanna Angeliki and Nizamis, Alexandros and Votis, Konstantinos",10.1145/3721889.3721924,2025,"With the advent of the edge-cloud continuum, Kubernetes (K8s) has emerged as the prominent solution for service orchestration. In this context, ensuring deployment security is essential. To identify vulnerabilities and misconfigurations in a timely and efficient manner, security scanning tools are employed. In this work, we evaluate six widely used security tools for workload and static code analysis. We consider several factors, including scan time, the number of detected threats, and resource usage. This evaluation seeks not only to demonstrate the performance trade-offs of these tools but also to contribute additional insights into their practical capabilities."
Knowledge Map Analysis of Artificial intelligence and Higher Education Based on Citespace,"Cai, Lihong",10.1145/3722237.3722244,2025,"With the advent of the artificial intelligence era, more and more scholars have launched relevant research on the application of artificial intelligence in higher education. With the help of CiteSpace software, this paper conducts econometric analysis on 347 documents related to artificial intelligence and higher education in the core collection of Web of Science. It is found that summarizing the annual publication volume of related research can be divided into three stages, the preliminary exploration stage (2014-2018), the rapid development stage (2019-2021), and the centralized outbreak stage (2022 to present); the hot topics of research on AI and higher education are the AI to higher education The hot topics of research on AI and higher education are the impact of AI on higher education, the system construction and resource construction of AI in higher education, and the application of AI-related technologies in higher education."
Project-guided Teaching Reform of Image Processing and Machine Vision: A Case Study of Intelligent Diagnosis of Pulmonary Nodules,"Zhang, Guobin and Wang, Kexin and Wang, Lewen and Zhang, Runfeng and Yang, Lu and Wang, Min and Liu, Zhenzhong",10.1145/3722237.3722281,2025,"This paper explores the innovation of teaching mode of image processing and machine vision courses under project orientation. Traditional image processing courses always lack a proper practical application, leading to some difficulties for students to understand the theoretical knowledge and apply the course content. Therefore, in order to better develop students' abilities in various aspects, this study takes the intelligent diagnosis of lung nodules as an example, and organically combines the theoretical knowledge in the textbook with the practical application in the project. In this paper we firstly discuss the content and importance of the image Processing and Machine Vision course, and review its research progress in related fields. Then we describe the design and implementation of the innovative model, and demonstrate the actual teaching effect under the new model through case studies. The final results show that this project-oriented teaching mode enhances students' learning interest, innovative thinking, practical and teamwork ability, which provides an important reference value for undergraduate talent cultivation."
Cluster Analysis of Teacher Competence in Vocational Colleges,"Sun, Shufen and Gao, You and Wen, Tingting and Jia, Binghai",10.1145/3722237.3722318,2025,"This study surveyed 631 vocational college teachers through questionnaires to establish a competency scale. An improved FCM clustering algorithm was applied to analyze the samples. The results showed that vocational college teachers can be divided into three major types: “Positive”, “Balanced” and “Passive”. The results showed that vocational college teachers can be divided into three major types: “Positive”, “Balanced” and “Passive”. Significant differences were found among the three types of teachers in attributes, competence dimensions, and perception of influencing factors. Significant differences were found among the three types of teachers in attributes, competency dimensions, and perception of influencing factors. This study proposes customized cultivation strategies for different types of teachers in an attempt to stimulate their potential. This study proposes customized cultivation strategies for different types of teachers in an attempt to stimulate their potential. A theoretical framework of factors influencing competence was constructed, which provides quantitative basis for optimizing the cultivation and management of vocational college teachers with great reference value."
Using Explainable AI (XAI) to Identify and Intervene with Students in Need: A Review,"Liu, Bingxue and Li, Caiqin and Wan, Ziqian",10.1145/3722237.3722348,2025,"This review discusses the use of Explainable artificial intelligence (XAI) in education, identifying and supporting students who require extra support. The application of XAI to artificial intelligence models strengthens the ties between human values and objectives, making it possible to introduce transparency and stability into educators’ decisions related to actions and to help them make such decisions based on the knowledge of their impact on the education process. From 2019 to 2024 literature on the application, goals, outcomes, and primary methods of XAI in education are reviewed. LIME, SHAP, and Captum are being used as key technologies, to predict academic success, identify at-risk students, and provide personalized interventions. Enabling human-centered AI in data-driven education decision-making is the comment focus. At the same time, the paper stresses the requirement to maintain human values, to be transparent, and to offer responsible explanations while tailoring interventions. However, some challenges remain in these areas. Hopefully, future research plans will focus on developing more sophisticated intervention strategies and explanatory models, by integrating people-centered methods with data analysis. The review suggests the potential of XAI to improve educational outcomes for increasing transparency, personalization, and trust, and to enhance accountability in AI-integrated educational systems."
Exploration and Practice of AI Application-oriented Talent Training Model Based on OBE Concept,"Zhu, Yan and Li, Xiangju",10.1145/3722237.3722361,2025,"With the acceleration of digital transformation, Artificial Intelligence (AI) has become a key technology to promote social progress. This paper proposes a set of teaching reform strategies based on the concept of Outcome-Based Education (OBE) for talent cultivation in the field of Artificial Intelligence. This study was implemented in the School of Electronics and Computer Science, Southeast University Chengxian College, aiming to cultivate AI application talents with solid theoretical foundation, proficient technical application ability and good professional quality through teaching reform. The reform strategies include the construction and optimization of the curriculum system, the reorganization and innovation of teaching content, the diversified application of teaching methods, the in-depth practice of project-driven teaching, and the exploration of online and offline hybrid teaching models. Special emphasis is placed on case and project driven teaching methods to realize the OBE concept and promote the development of students' practical skills and innovative thinking. Moreover, this paper also explores the reform of the teaching evaluation system and establishes a multi-dimensional and process-based evaluation system to comprehensively examine students' learning outcomes. The reform results show that students have significantly improved in course learning, academic competitions, college students’ innovation and entrepreneurship training program, and the quality of graduation design, and students' comprehensive literacy has been comprehensively developed. The teaching reform practice of this study provides new ideas and methods for application-oriented talent cultivation in the field of artificial intelligence. It is expected to provide more high-quality application-oriented talents for the industry and provide a reference for education reform."
Wireless Sensing of Gait for Neurodegenerative Disease Assessment: A Scoping Review,"Wu, Patrick and Dong, Yiwen and Xu, Chenhan and Geil, Mark and Akintomide, Modupe and Zhang, Xinyue and Xie, Zongxing",10.1145/3722570.3726893,2025,"Early diagnosis of neurodegenerative diseases is a significant public health challenge, yet crucial for better prognoses. Gait analysis accomplishes this task by detecting abnormal motion or declining motor control, which are strong early indicators of neurodegeneration. A new healthcare paradigm, shifting from clinical settings to home-based approaches, promotes less intrusive and privacy-preserving monitoring solutions. In this context, wireless gait analysis methods, such as radars and commercial cameras, are well-suited for home-based neurodegenerative disease assessment. These non-contact sensing technologies offer a less cumbersome alternative to traditional methods while maintaining reliability. We conducted this scoping review study to examine wireless sensing solutions for gait analysis in neurodegeneration assessment. In accordance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses for Scoping Review (PRISMA-ScR) guidelines, searches were conducted on the Scopus, PubMed, IEEE, and ACM databases. 16 of 139 articles were included in this review, which were evaluated for sensor choices, gait features, means of analysis, and the investigated condition. Studies indicated that the RGB/depth cameras and radars were effective means of capturing real-time gait data. Step length and speed were found to be the most accurate discriminators among several gait parameters for data analysis in this study, regardless of the sensing approach chosen. Various descriptive statistical methods and data-driven models have been explored as the analytical tools for assessing neurodegeneration-affected gait. By evaluating various approaches to wireless sensing-based gait analysis in this scoping review, we discuss and highlight challenges and opportunities for future research."
Research on the Influence Mechanism of Different Generations of Employees' Nonwork Orientation Relative to career on Turnover Intention Based on the Moderated Mediation Model,"Ma, Yu and Wang, Yuqing",10.1145/3723366.3723367,2025,"Purpose: The increasingly integrated external environment and the pursuing of work life balance make it increasingly difficult for people to separate their work and non-work life, especially the new generation of employees. It may lead to turnover and make loss for the organization who fails to meet their non-work needs. The questionnaire surveyed 664 employees to explore the mechanism of the influence of nonwork orientations relative to career on turnover intention. This study aims to provides empirical support for integration research in the work-nonwork and career areas and suggests that organizations need to pay attention to employees’ non-work orientations relative to career. Method: This study used structural equation modeling to test for simple mediation effects and group regression analysis to test for moderation effects. Results: (1) Only family orientation has a significant negative effect on turnover intention. (2) work→non-work conflict plays a mediating role between family orientation and turnover intention(B=0.1, p&lt;0.01), but has no significant mediating role between personal life orientation, community service orientation and turnover intention. (3) The generational differences moderates the process of the influence of nonwork orientations relative to career on turnover intention under the mediating effect of work→non-work conflict. Conclusion: This study suggests that organizations can reduce the turnover intention of employees by meeting the non-work needs of employees of different generations."
Evaluation of Global Universities' Sustainable Development Capabilities from an Efficiency Perspective: An Analysis Based on the Super-Efficiency SBM-DEA Model,"Wang, Zhisheng and Xu, Jiaqi and Chu, Xiaoyan and Zhai, Xuesong",10.1145/3723366.3723396,2025,"Higher education institutions (HEIs) play a crucial role in advancing the United Nations' 17 Sustainable Development Goals (SDGs). International higher education research bodies such as Times Higher Education (THE) and Quacquarelli Symonds (QS) have integrated SDGs or sustainability concepts into university ranking systems. However, a critical issue remains: how can higher education institutions maximize sustainable development efficiency under resource constraints? Addressing current challenges such as the difficulty in distinguishing efficient decision-making units (DMU) and the lack of specific regional evaluation, this paper constructs a sustainable development evaluation model centered on efficiency transformation. Using the super-efficiency SBM-DEA model, data on the inputs and outputs of sustainable development from 700 higher education institutions across 71 countries were collected. The study analyzes the differences in sustainable development efficiency from both global and China's perspectives based on regional categories. The results indicate that the average efficiency value of global universities is 0.57, with Asia having the highest average efficiency, followed by North America. In China, universities have an average efficiency of 0.78, with the central region having the highest average efficiency, followed by “Hong Kong, Macau, and Taiwan” region, the northeastern region and eastern regions, while the western region has the lowest average efficiency. Consequently, this study suggests improving the sustainable development evaluation system, dynamically adjusting fiscal resource allocations, drawing on the advanced experiences of universities in regions with high efficiency, and establishing global centers for specific sustainable development centers."
Understanding Game Art Practice Beyond Technical Expertise: A Qualitative Study,"Hawey, Dave",10.1145/3723498.3723724,2025,"Although they contribute a great deal to what players see on the screen, there is a marked absence in the literature of direct studies of artists working in digital game development. This is why we stress the need to understand game art practice in real-world industry settings, and particularly professional practice beyond technical expertise. Referring to design theory (i.e., Sch\""{o}n and others), professional artistry/knowledge in game art practice is understood as design-like process and skills. This qualitative study is based on ethnographic results from our doctoral research that have not yet been published in English. Cross-synthesis of three case studies is reported (each case comprises one experienced artist shadowed in a Montr\'{e}al indie game studio during preproduction, between 2016-2018). Findings give insights on the ‘design-like’ professional artistry of game art practice, in terms of complex situations, common point of view on game experience and development, and core skillset. Referring to the theme of the conference, the findings point to ways of stimulating the growth of game art students’ design-like professional artistry in terms of interdisciplinary and sustainable collaboration in game development. Specifically, they strengthen the importance of humanistic skills (e.g. collaborative, ethical), combined with creative and technical ones, in professional practice of game art."
Computational Tools for Table-Top Role-Playing Games: A Scoping Review,"Shyne, Fiona and Cooper, Seth",10.1145/3723498.3723816,2025,"Table-top role-playing games (TTRPGs) are a form of gameplay that often requires a variety of complex tasks to be completed both in preparation and throughout gameplay: from tracking game state to the creation of fictional worlds. This has presented an opportunity for computational assistance in TTRPG sessions, both in the creation of artifacts and throughout the gameplay. We investigate the current research in computational tools for TTRPGs through a scoping review of academic works and present the major trends and opportunities from these works. We screened over one thousand works sourced from three different academic databases: ACM Digital Library, IEEE-Xplore, and Google Scholar. Papers were included based on relevance to TTRPGs, computational interface, and academic venue. In total, we evaluated 46 works in terms of produced artifacts, computational methods, evaluation, and outcomes. These papers include a diverse set of produced artifacts and computational methods, with an emphasis on tangible interfaces and generative AI systems. However, we found an opportunity for future work in terms of long-term studies, mixed-initiative methods, and different aspects of gameplay."
Comparison of kinematics and dynamics estimation of lower extremity in basketball players running up and down using markerless and marker-based motion capture systems,"Wei, Linyu and Yang, Changzhi and Xu, Yanjia and Huang, Yuxiang and Zhou, Mingqi and Hu, Zhe",10.1145/3723936.3723961,2025,"Background: Traditional marker-based motion capture systems (MB systems) have been widely validated for their reliability in capturing kinematic and kinetic data by placing markers on athletes. However, the application of markerless motion capture systems (ML systems) in sports scenarios requires further validation, particularly in high-speed and highly dynamic movements such as basketball players' approach and touch high jumps.Objective: This study aims to compare the accuracy and reliability of MB and ML systems in estimating lower limb kinematics and kinetics during the approach and touch high jumps of basketball players, in order to assess the performance differences between the two systems in practical sports analysis.Method: We enrolled 12 amateur male basketball players in a running vertical jump (RVJ) study. Kinematic and dynamic data for the RVJ were acquired using both marker-based and markerless motion capture systems, complemented by force plate measurements. Statistical analyses, including correlation analysis and root mean square difference (RMSD), were conducted to evaluate the concordance between the data obtained from these two methodologies.Result: The joint center estimated by ML and MB system in RVJ was 1.9±0.3cm, 2.2±0.7cm, 0.6±0.3cm for hip, knee and ankle, and 0.999±0.003, 0.999±0.004, 0.999±0.001, respectively. In RVJ, RMSD of hip, knee and ankle were 3.6±0.5°, 4.7±1.2°, 4.3±0.9°, and Rxy were 0.998±0.016, 0.998±0.018, 0.988±0.012, respectively. In RVJ, RMSD of hip, knee and ankle were 3.6±0.5°, 4.7±1.2°and 4.3±0.9°, and Rxy were 0.998±0.016, 0.998±0.018 and 0.988±0.012, respectively. In RVJ, RMSD of hip, knee and ankle were 1.64±0.13W/kg, 0.82±0.3W/kg, 0.52±0.09W/kg, and Rxy were 0.993±0.015, 0.980±0.012, 0.984±0.012, respectively.Conclusion: Markerless motion capture systems demonstrated equivalent accuracy and reliability in estimating kinematic and dynamic parameters during basketball players' running vertical jumps (RVJ) when compared with marker-based systems. This study's outcomes robustly endorse the potential use of ML systems in the analysis of basketball RVJ and provide substantial evidence for subsequent investigative endeavors and practical applications."
Design and Implementation of Engineering Document Management Information System,"Yu, Jingqin",10.1145/3724154.3724177,2025,"In order to optimize engineering document management, this paper studies the design and implementation of engineering document management information system. The research process includes the overall structure design, implementation and application effect analysis of the system. The overall structure design of the system covers the basic structure design (divided from data processing order and management level) and the boundary and connection (associated with other subsystems). The implementation part introduces the architecture (using B/S architecture), system flow (including multiple modules) and document management design. Application effect analysis analyzes the key functions through system testing, and expounds the actual application effect, such as solving the function dispersion of document management software, breaking the information island and meeting the system performance requirements. The research results show that the functions of the system meet the design objectives and meet the needs of users. The conclusion is that the system can effectively improve the efficiency and level of engineering document management and promote the organization and management."
Analysis of Digital Transformation with Random Neural Network Algorithm in Commercial Bank,"Ding, Yixi",10.1145/3724154.3724308,2025,"Focusing on the digital transformation of commercial bank, and basing on the case data produced by the digital transformation project, the factors analysis is conducted to explain commercial bank digital transformation. Specifically, by analyzing the 73,555 daily interaction data from the digital project, the text analysis algorithm is applied to get interaction content feature, interaction emotion feature, and interaction experience feature. In order to get the most important features, a random neural network algorithm is employed to get the optimal feature subset. Finally, the feature analysis experiment is constructed to explain the connection between the user's interaction and bank digital transformation. The analysis results provide the management insights for the commercial bank on the digital transformation project application."
A Study on the Impact of Blended Teaching Modes on Student Engagement Based on SPSS Statistical Analysis,"Chen, Xiaoru and Liang, Yibo",10.1145/3724504.3724532,2025,"This research work studies the impact of blended teaching modes on student engagement in the horticulture course. This work intends to address the challenges of low participation, lack of interaction, and monotonous teaching methods in vocational education. This work aims to improve student engagement and knowledge using a three-phase mode such as pre-class exploration, in-class guidance, and post-class expansion. This work performs an empirical analysis by comparing student engagement levels before and after the implementation of the blended teaching model. The pre- and post-test data were collected, and the statistical analysis is performed using SPSS. The outcome of the results indicates the significant improvements in pre-class, in-class engagement (p &lt; 0.01), and the overall involvement. Though, post-class engagement showed limited enhancement. This recommends the necessity for additional refinement of the post-class development step. Through the correlation analysis, the improvement in the performance of the students were demonstrated. Regression analysis comparison illustrates that decision tree regression outperforms well in terms of Root Mean Squared Error (RMSE), and the R2 values."
Research on Teaching Reform of AIGC Empowering C Language Programming Course,"Xie, Nan and Chen, Weimin",10.1145/3724504.3724535,2025,"The new idea development has become more and more broadly in the new generation of information technology education. And the computer related general education courses for non-computer majors in Chinese universities are constantly undergoing deeper teaching research and exploration, which including the courses’ curriculum design, teaching mode, teaching methods, teaching design, teaching system, etc., at the same time, the changes in educational reform policies for computer general education courses in universities are all following. The C Language programming course is a compulsory course of non-computer major general education courses in many undergraduate universities. Through studying, the course can not only cultivate students' computer thinking and innovative thinking, but also continuously improve their logical thinking ability and programming debugging practical skills and application skills. What's more, it can expand students’ multidimensional cognition and knowledge seeking ability in the world and help students fully understand the current new generation of information technology foundation, comprehensively master the key, difficult and programming skills of C language. AIGC empowering C course teaching mode has been proposed in response to several explicit issues in the current C programming course teaching, such as poor personalized learning experience, weak practical ability, and slow problem-solving yield. This teaching model builds studying framework based on the multi-source data fusion aims to promote personalized learning for students, deepen their understanding of relevant knowledge, and provide immediate problem feedback. In addition, the model can align the course teaching objects through much training to stimulate better students' learning motivation, improve their comprehensive quality, and cultivate outstanding talents. This paper would provide a new path and reference for the teaching and reform ideas of general education courses for applied undergraduate programs in current undergraduate universities. The research also provides a new implementation path and teaching application reference for the C course teaching and reforming ideas of general courses for non-computer majors in applied undergraduate universities."
Research on the Teaching System of Electrical “1+X” Certificate Based on Blockchain,"Peng, Chao and Qin, Xiaobin and Ge, Pengdan",10.1145/3724504.3724609,2025,"This paper focuses on the teaching system of electrical “1+X” certificate based on blockchain. It introduces in detail the composition and training mode of the electrical “1+X” certificate system, including online and offline learning, practical operation, enterprise internship, and assessment and evaluation. The design and implementation of the teaching system are carried out, starting from the demand analysis, covering user support, information security, learning certification, certificate management, and data application. The functional modules including user management, course management, learning and certification, certificate management, and data analysis and report are designed. Through system testing and result analysis, the effectiveness of the system is verified from four dimensions: function, boundary value, security, and performance."
Review of AI-Based Mental Health Apps,"Alotaibi, Abeer and Sas, Corina",10.14236/ewic/BCSHCI2023.27,2024,"The last decade has seen a significant growth of HCI research in mental health technologies while Artificial intelligence raises both challenges and opportunities to better support symptom identification or personalization of interventions. There has been also a growth of commercial AI-based mobile apps for mental health. Despite emerging HCI work on reviewing mental health apps, those that are AI-based have received limited attention. To address this gap, we report a functionality review of 13 apps selected from 127 apps from the Apple Store. The selection criteria involved a minimum rating of 4 out of 5. After eliminating duplicates, irrelevant, and low-rated apps, an expert evaluation and auto-ethnography approach were used to explore apps’ functionalities. Findings indicate that apps support functions such as tracking and detecting emotions and moods, providing recommendations for therapy and well-being interventions, and supporting talking therapy through conversational agents powered by Natural Language Processing models. A critical finding is apps’ limited support for AI literacy and explainability, as well as limited consideration for ethical concerns regarding personal data, its reliability, and algorithmic biases. Our paper concludes with three design implications for AI-based mental health apps towards developing conversational agents to support Cognative Behavoural Therapy interventions based on tracked multimodal data, addressing the ethics of NLP biases, and user exploration of AI-based models and their explainability."
Engineering events in CPS: experiences and lessons learned,"Ollesch, Julius and Hesenius, Marc and Gruhn, Volker",,2017,"Event-based control paradigms are vital enablers for adaptive analytical control mechanisms needed in smart CPS. We describe the challenges of engineering events in CPS grounded in experience gathered by designing and implementing a prototype of an indoor location-aware public library system. Furthermore, we review existing event modeling and specification methodologies and infer shortcomings with regard to the lessons learned. We address those by enhancing an existing best-practice methodology on how to engineer event-based software for smart CPS."
Characterization of the underlying mechanisms of vulnerability in complex projects using dynamic network simulation,"Zhu, Jin and Mostafavi, Ali",,2017,"The objective of this study was to investigate the underlying mechanisms of vulnerability in complex construction projects using simulation experiments. Specifically, two hypotheses related to project vulnerability were tested: (1) project schedule performance is negatively correlated with vulnerability; (2) the level of project vulnerability is positively correlated with project exposure to uncertainty and organizational complexity. In the proposed dynamic network simulation methodology, construction projects are modeled as heterogeneous meta-networks. Project vulnerability is assessed by the decrease in meta-network efficiency due to uncertainty-induced perturbations. Project schedule deviation is used as a measure for quantifying the impacts of vulnerability on project performance outcomes. The proposed simulation methodology was implemented in three case studies of real-world construction projects. Monte-Carlo simulation experiments were conducted under different simulation scenarios consisting of varying levels of uncertainty and project planning strategies to test the hypotheses."
Human activity recognition and mobile sensing for construction simulation,"Nath, Nipun D. and Shrestha, Prabhat and Behzadan, Amir H.",,2017,"Construction industry has been constantly lagging behind in terms of efficiency and productivity growth. Simulation modeling can be used to improve the productivity of construction workflow processes through modeling uncertainties and stochastic events that may negatively impact project cost and schedule. In the research presented in this paper, mobile sensors coupled with machine learning techniques are used for ubiquitous data collection and human activity recognition (HAR), which will constitute the key input parameters of process simulation modeling. To assess the designed methodology, an experiment is carried out which replicates a warehouse quality control operation. Smartphones mounted on human bodies are used to collect multi-modal time-motion data. Support vector machine (SVM) is then applied to classify workers' and inspectors' activities, and activity durations are subsequently extracted. Finally, a simulation model is built using the output of the HAR phase and rigorously validated and used to analyze workflow processes, productivity, and bottlenecks."
A study of discrete event simulation project data and provenance information management in an automotive manufacturing plant,"Barrera-Diaz, Carlos A. and Oscarsson, Jan and Lidberg, Simon and Sellgren, Tommy",,2017,"Discrete Event Simulation (DES) project data management is a complex and important engineering activity which impacts on an organization's efficiency. This efficiency could be decreased by the lack of provenance information or the unreliability of existing information regarding previous simulation projects, all of which complicates the reusability of the existing data. This study presents an analysis of the management of simulation projects and their provenance data, according to the different types of scenarios usually found at a manufacturing plant. A survey based on simulation projects at an automotive manufacturing plant was conducted, in order to categorize the information regarding the studied projects, map the available provenance data and standardize its management. This study also introduces an approach that demonstrates how a structured framework based on the specific data involved in the different types of scenarios could allow an improvement of the management of DES projects."
Correlation-robust analysis of single item auction,"Bei, Xiaohui and Gravin, Nick and Lu, Pinyan and Tang, Zhihao Gavin",,2019,"We investigate the problem of revenue maximization in single-item auction within the new correlation-robust framework proposed by Carroll [2017] and further developed by Gravin and Lu [2018]. In this framework the auctioneer is assumed to have only partial information about marginal distributions, but does not know the dependency structure of the joint distribution. The auctioneer's revenue is evaluated in the worst-case over the uncertainty of possible joint distribution.For the problem of optimal auction design in the correlation robust-framework we observe that in most cases the optimal auction does not admit a simple form like the celebrated Myerson's auction for independent valuations. We analyze and compare performances of several DSIC mechanisms used in practice. Our main set of results concern the sequential posted-price mechanism (SPM). We show that SPM achieves a constant (4.78) approximation to the optimal correlation-robust mechanism. We also show that in the symmetric (anonymous) case when all bidders have the same marginal distribution, (i) SPM has almost matching worst-correlation revenue as any second price auction with common reserve price, and (ii) when the number of bidders is large, SPM converges to optimum. In addition, we extend some results on approximation and computational tractability for lookahead auctions to the correlation-robust framework."
Tight revenue gaps among simple mechanisms,"Jin, Yaonan and Lu, Pinyan and Tang, Zhihao Gavin and Xiao, Tao",,2019,"We consider a fundamental problem in microeconomics: Selling a single item among a number of buyers whose values are drawn from known independent and regular distributions. There are four widely-used and widely-studied mechanisms in this literature: Anonymous Posted-Pricing (AP), Second-Price Auction with Anonymous Reserve (AR), Sequential Posted-Pricing (SPM), and Myerson Auction (OPT). Myerson Auction is optimal but complicated, which also suffers a few issues in practice such as fairness; AP is the simplest mechanism, but its revenue is also the lowest among these four; AR and SPM are of intermediate complexity and revenue. We study the revenue gaps among these four mechanisms, which is defined as the largest ratio between revenues from two mechanisms. We establish two tight ratios and one tighter bound:1. SPM/AP. This ratio studies the power of discrimination in pricing schemes. We obtain the tight ratio of roughly 2.62, closing the previous known bounds [e/(e − 1),e].2. AR/AP. This ratio studies the relative power of auction vs. pricing schemes, when no discrimination is allowed. We get the tight ratio of π2/6 ≈ 1.64, closing the previous known bounds [e/(e − 1), e].3. OPT/AR. This ratio studies the power of discrimination in auctions. Previously, the revenue gap is known to be in interval [2, e], and the lower-bound of 2 is conjectured to be tight [38, 37, 4]. We disprove this conjecture by obtaining a better lower-bound of 2.15."
Simulation analysis of a deep reinforcement learning approach for task selection by autonomous material handling vehicles,"Li, Maojia Patrick and Sankaran, Prashant and Kuhl, Michael E. and Ganguly, Amlan and Kwasinski, Andres and Ptucha, Raymond",,2018,"The use of autonomous vehicles is a growing trend in the material handling and warehousing. Some challenges that face material handling include the navigation within a warehouse, precision localization and movement, and task selection decisions. In this paper, we address the issue of task selection. In particular, we develop a deep reinforcement learning methodology to enable a vehicle to select from among multiple tasks and move to the closest task in the context of material handling in a warehouse. To evaluate the deep reinforcement learning methodology, we conduct a simulation-based experiment to generate scenarios to first train and then test the capabilities of the method. The results of the experiment show that the method performs well under the given conditions."
Enabling intelligent processes in simulation utilizing the TensorFlow deep learning resources,"De la Fuente, Rodrigo and Erazo, Ignacio and Smith, Raymond L.",,2018,"Availability of large data sets and increased computing performance have contributed to many improvements in productivity and decision-making. Simulation can exploit these by incorporating data mining capabilities, such as machine learning, in the modeling and analysis process. This paper demonstrates the integration of discrete event simulation with a deep learning resource, known as TensorFlow, to enable intelligent decision making in the form of smart processes. A bank credit approval process is modeled using these smart processes to evaluate customer credit worthiness based on 20 reported features. Comparison of three models is made where credit worthiness is (1) known, (2) randomly assigned, or (3) evaluated based on customer features. Additionally, the experiment compares results under conditions where the process is perturbed by an unexpected surge in customer arrivals. The presented models and results demonstrate the feasibility of enabling smart processes in discrete event simulation software and the improved decision-making fidelity."
Strategic supply chain design for an austrian winter road service provider,"Felberbauer, Thomas and H\""{u}bl, Alexander and Altendorfer, Klaus and Gattringer, Josef",,2018,"Snowplow operations are critical for public safety and economic success in countries where difficult driving conditions occur in winter. Specifically, the salt supply ensuring good driving conditions is a crucial factor. In this paper, the strategic supply chain design of a winter service provider in Austria is investigated. Two research directions on the influence of bigger and fewer salt silos per depot and the logistic costs for a unique summer salt purchasing strategy are addressed applying two independent solution approaches. On the same data basis, a simulation model is developed and a mixed integer linear problem is applied to answer the respective research questions. The first study shows that the current depot availability is quite good but that bigger and fewer salt silos per depot could be a risk. Finally, the second study shows the logistic costs for the unique summer salt purchasing strategy and the optimal salt warehouse locations."
A stepwise implementation of the virtual factory in manufacturing industry,"Dalstam, Amanda and Engberg, Marcus and N\r{a}fors, Daniel and Johansson, Bj\""{o}rn and Sundblom, Anneli",,2018,"A big challenge for manufacturers today is to create a flexible and efficient production system. One way of managing this challenge is to establish a virtual factory, a virtual model of the production unit. Working smarter by using the advantages that digitalization implies enables production of personalized products at increasing speed. This paper explores how to implement such a concept by stepwise increasing the maturity of the virtual factory. Evaluated at a large-scale Swedish manufacturer, local needs and enabling technologies benchmarked at industry leaders have been identified and strategically mapped to their corresponding maturity step. This paper shows that the implementation of a virtual factory relies on standardized work procedures, ensuring its use as a decision aid throughout the company. Implementing a virtual factory in this manner will facilitate user-driven development and more accurate decision making, generating support for efficient production systems."
A multi-level modeling approach for simulation-based capacity planning and scheduling of aircraft maintenance projects,"Fabig, Christian and Winter, Elias",,2018,"The aim of this contribution is to provide a modeling approach for multi-project manufacturing. Existing approaches for capacity planning and detailed scheduling in those complex environments are using separate models that are linked by instruction and feedback slopes. In contrast, we include multiple levels hierarchically into one simulation model. Thus, the possibilities to propagate restrictions such as precedence dependencies and starting times from gross to detailed levels in a consistent manner are established. The approach is implemented in Java, extending the modeling capabilities of a simulation-based optimization framework. A real-life application to project-oriented aircraft maintenance is presented to highlight the practicality and efficiency of the integrated model for simulation-based capacity planning using work packages as well as detailed scheduling using activities of work plans."
Serious 3D game over a cluster computing for situated learning of traffic signals,"Proa\~{n}o, Carlos and Villac\'{\i}s, C\'{e}sar and Proa\~{n}o, V\'{\i}ctor and Fuertes, Walter and Almache, Mario and Zambrano, Margarita and Gal\'{a}rraga, Fernando",,2020,"this research presents a serious 3D game over a computer cluster that allows the learning of traffic signals so that the child is a responsible driver and pedestrian. For this purpose, we have applied an agile methodology to be able to produce a driving and pedestrian simulation system as a serious game, which includes a three-layer hierarchical architecture (orientation layer, management layer, and control layer). This architecture can be used to simulate three environments that are: a) Driver's environment; b) Pedestrian environment; c) Vehicle traffic observer environment. All these environments are controlled by intelligent agents that allow the user to handle the autonomous longitudinal control, the relative distance, the speed of the cars, the collisions between vehicles, the pedestrian collision, excess of the speed limit, circulation in the opposite direction, and respect of traffic signals. Our solution to traffic problems has been validated by the execution of several tests in public schools, and the results demonstrate that this simulation application encourages learning in road safety education for children between 9 and 12 years old, using a distributed environment with a computational cluster in the cloud."
Advanced tutorial: networking simulations across platforms and enterprises with virtual SIPs,"Savage, Sam L and Doheney, Shaun and Smith, Colin",,2020,"SIPmath represents uncertainties as coherent arrays of realizations called SIPs, which may be shared between diverse simulation applications across the enterprise. This allows simulations to be linked together to form networks in which the output distributions of one simulation become the input distributions of another simulation. Furthermore, the outputs of stochastic models in packages such as R, Python or discrete event simulations may be shared with managers using interactive dashboards in native Microsoft Excel. Two recent open source advances in simulation modeling and analysis have yielded great efficiencies in this approach. Metalog distributions can fit virtually any continuous distribution of data with an analytical F-Inverse function much like a Taylor's series. The HDR Portable Uniform Random Number Generator produces identical results on all platforms including a single cell formula in Excel. Participants are encouraged to bring their laptops for a hands-on learning experience."
The activity-entity-impact method: understanding bottleneck behavior of simulation models demonstrated by an emergency department model,"Furian, Nikolaus and Gutschi, Clemens and Neubacher, Dietmar and Walker, Cameron and O'Sullivan, Michael",,2020,"Simulation models are often used to gain a better understanding of a system's sensitivity to changes in the input parameters. Data gathered during simulation runs is aggregated to Key Performance Indicators (KPIs) that allow one to assess a model's or system's performance. KPIs do not provide a deeper understanding of the causes of the observed output because this is not their primary objective. By contrast, dynamic bottleneck methods both identify elements that yield the largest gain in productivity with increased availability and also visualize these elements over time to enable bottlenecks to be better understood. In this paper we discuss whether dynamic bottleneck detection methods can be utilized to identify, measure, and visualize causes of observed behavior in complex models. We extend standard bottleneck detection methods, and introduce the Activity-Entity-Impact-Method. The practicality of the method is demonstrated by an example model of a typical Emergency Department setting."
Report on evaluation experiments using different machine learning techniques for defect prediction,"Grigoriou, Marios-Stavros and Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris",,2020,"With the emergence of AI, it is of no surprise that the application of Machine Learning techniques has attracted the attention of numerous software maintenance groups around the world. For defect proneness classification in particular, the use of Machine Learning classifiers has been touted as a promising approach. As a consequence, a large volume of research works has been published in the related research literature, utilizing either proprietary data sets or the PROMISE data repository which, for the purposes of this study, focuses only on the use of source code metrics as defect prediction training features. It has been argued though by several researchers, that process metrics may provide a better option as training features than source code metrics. For this paper, we have conducted a detailed extraction of GitHub process metrics from 148 open source systems, and we report on the findings of experiments conducted by using different Machine Learning classification algorithms for defect proneness classification. The main purpose of the paper is not to propose yet another Machine Learning technique for defect proneness classification, but to present to the community a very large data set using process metrics as opposed to source code metrics, and draw some initial interesting conclusions from this statistically significant data set."
Symbolic computer algebra and SAT based information forwarding for fully automatic divider verification,"Scholl, Christoph and Konrad, Alexander",,2020,"During the last few years Symbolic Computer Algebra (SCA) delivered excellent results in the verification of large integer and finite field multipliers at the gate level. In contrast to those encouraging advances, SCA-based divider verification has been still in its infancy and awaited a major breakthrough. In this paper we analyze the fundamental reasons that prevented the success for SCA-based divider verification so far and present SAT Based Information Forwarding (SBIF). SBIF enhances SCA-based backward rewriting by information propagation in the opposite direction. We successfully apply the method to the fully automatic formal verification of large non-restoring dividers."
"A simulation-based decision-support system for reducing duration, cost, and environmental impacts of earthmoving operations","Pourrahimian, Elyar and Hattab, Malak Al and Ead, Rana and Labban, Ramzi Roy and AbouRizk, Simaan",,2021,"Earthmoving operations are equipment-intensive processes that rely heavily on the proper selection of the equipment fleet and proper scheduling of associated tasks. Early equipment planning decisions have direct implications on schedules, costs, and more importantly, the environmental performance of such operations. While traditional planning of earthmoving works is ad-hoc and based on planners' experiences, ensuring favorable performance requires advanced analytical techniques that consider multiple variables and competing objectives. Accordingly, this study develops a discrete-event simulation-based decision-support system (DES-DSS) for selecting the optimal equipment fleet, while considering the trade-offs between time, cost, and environmental impacts. The model's results from a case study reveal how different fleet mixes and sizes can considerably impact associated emissions, durations, and costs. The DES-DSS can aid planners in making informed decisions during early planning stages and be used as a control feedback mechanism to continuously enhance operations in real-time while reducing emissions."
A discrete-event heuristic for makespan optimization in multi-server flow-shop problems with machine re-entering,"Juan, Angel A. and Copado, Pedro and Panadero, Javier and Laroque, Christoph and de la Torre, Rocio",,2021,"Modern Manufacturing, known as Industrial Internet or Industry 4.0, is more than ever determined by customer-specific products, that are to be manufactured and delivered in given lead times and due-dates. Many of these manufacturing systems can be modeled as flow-shops where some of the processes can handle jobs on parallel machines. In addition, complex manufacturing environments contain specific machine loops or re-entry cycles where jobs might re-enter specific processes at some point of the flow-shop chain. A specific server is assigned to a job the first time it visits a machine, and it is quite usual that this job has to be processed by exactly the same server if it re-visits the machine due to quality issues. With the goal of minimizing the makespan, this paper analyzes this complex flow-shop setting and proposes an original discrete-event heuristic for solving it in short computing times. Our algorithm combines biased (non-uniform) randomization strategies with the use of a discrete-event list, which iteratively processes as the simulation clock advances. A series of computational experiments contribute to illustrate the performance of our methodology."
Neural functional analysis in virtual reality simulation: example of a human-robot collaboration tasks,"Zhu, Qi and Du, Jing",,2021,"Human-robot collaboration has gained its popularity with the fast evolution of the Industry 4.0. One of the challenges of HRC is human-robot interface design that adapts to the personalized needs. This paper presents a method of using Virtual Reality (VR) simulation as a testbed and data collector for examining and modeling personal reactions to different human-robot interface designs. To obtain real-time leading indicator of human performance, this study focuses on the neural functional analysis in VR. An integrated system is presented using eye-tracking and force input data as event makers for Neuroimaging technique, i.e., Functional Near Infrared Spectroscopy (fNIRS). The real-time hemodynamic responses in subjects' brains are analyzed based on the general linear model (GLM) for modeling neural functional changes under different levels of haptic designs. Our results indicate that the neurobehavioral data collected from the VR environment can be used directly as a personalized model for human-robot interface optimization."
On the use of simulation-optimization in sustainability aware project portfolio management,"Saiz, Miguel and Lostumbo, Marisa Andrea and Juan, Angel A. and Lopez-Lopez, David",,2021,"Among other variables, uncertainty and limitation of resources make real-life project portfolio management a complex activity. Simulation-optimization is considered an appropriate technique to face stochastic problems like this one. The main objective of this paper is to develop a hybrid model, which combines optimization with Monte Carlo simulation, to deal with stochastic project portfolio management. A series of computational experiments illustrate how these hybrid approach can include uncertainty into the model, and how this is an essential contribution for informed decision making. A relevant novelty is the inclusion of a sustainability dimension, which allows managers to select and prioritize projects not only based on their monetary profitability but also taking into account the associated environmental and/or social impact. This additional criterion can be necessary when evaluating projects in areas such as civil engineering, building and construction, or urban transformation."
Supporting experiential learning through exploring central topics in ICT project team leadership: the rhea.framework knowledge base,"Haselberger, David",,2022,"This article describes central topics of team leadership in ICT projects within a socio-technical system supporting experiential learning in team leadership: the rhea.framework. These were iteratively elaborated in a hermeneutic, machine-learning supported process based on a literature survey and the collection of interrelated experience descriptions - including organizational patterns - concerning team leadership. The four central topics - or core flows - distinguished within the rhea.framework knowledge base are abstractions of clusters of connected experience descriptions. They are anchors in personal reflection and experiential team learning processes."
Last-mile delivery of pharmaceutical items to heterogeneous healthcare centers with random travel times and unpunctuality fees,"Herrera, Erika and Panadero, Javier and Juan, Angel A. and Neroni, Mattia and Bertolini, Massimo",,2022,"This paper analyzes a real-life distribution problem that is related to a pharmaceutical supplier in Spain. Every day, a fleet of vehicles has to deliver the previously requested items to a large set of pharmacies. The distribution has to be conducted with (i) the total distance and time incurred by the entire fleet being reasonably low and (ii) the time of the delivery meeting the specified time windows or, if that is not possible and some delays occur, the total fee incurred by these unpunctualities being minimized. Unpunctuality fees depend upon how important is the customer for the distributor, and the size of the tardiness gap. To include even more realistic details, travel times are modeled as random variables, which also makes the problem more challenging to solve by employing traditional optimization methods. To solve this stochastic variant of the problem, a simheuristic algorithm is proposed and evaluated."
Combining simulation with reliability analysis in supply chain project management under uncertainty: a case study in healthcare,"Lostumbo, Marisa A. and Saiz, Miguel and Calvet, Laura and Juan, Angel A. and Lopez-Lopez, David",,2022,"Many projects involving supply networks can be logically represented by multiple processing paths. When the supply chain is working under deterministic conditions, computing the total time requested by each path is a trivial task. However, this computation becomes troublesome when processing times in each stage are subject to uncertainty. In this paper, we assume the existence of historical data that allow us to model each stage's processing time as a random variable. Then, we propose a methodology combining Monte Carlo simulation with reliability analysis in order to (i) estimate the project survival function and (ii) the most likely 'bottleneck' path. Identifying these critical paths facilitates reducing the project makespan by investing the available budget in improving the performance of some stages along the path, e.g., by modifying the transportation mode at one particular stage in order to speed up the process. A numerical example is employed to illustrate these concepts."
A biased-randomized discrete-event heuristic for the hybrid flow shop problem with batching and multiple paths,"Laroque, Christoph and Lei\ss{}au, Madlene and Copado, Pedro and Panadero, Javier and Juan, Angel A. and Schumacher, Christin",,2022,"Based on a real-life use-case, this paper discusses a manufacturing scenario where different jobs be processed by a series of machines. Depending on its type, each job must follow a pre-defined route in the hybrid flow shop, where the aggregation of jobs in batches might be required at several points of a route. This process can be modeled as a hybrid flow shop problem with several additional but realistic restrictions. The objective is to find a good permutation of jobs (solution) that minimizes the makespan. Discrete-event simulation can be used to obtain the makespan value associated with any given permutation. However, to obtain high-quality solutions to the problem, simulation needs to be combined with an optimization component, e.g., a discrete-event heuristic. The proposed approach can find solutions that significantly outperform those provided by employing simulation only and can easily be extended to a simheuristic to account for random processing times."
An automated framework for generating synthetic point clouds from as-built BIM with semantic annotation for scan-to-BIM,"Ma, Jong Won and Han, Bing and Leite, Fernanda",,2022,"Data scarcity is a major constraint which hinders Scan-to-BIM's generalizability in unseen environments. Manual data collection is not only time-consuming and laborious but especially achieving the 3D point clouds is in general very limited due to indoor environment characteristics. In addition, ground-truth information needs to be attached for the effective utilization of the achieved dataset which also requires considerable time and effort. To resolve these issues, this paper presents an automated framework which integrates the process of generating synthetic point clouds and semantic annotation from as-built BIMs. A procedure is demonstrated using commercially available software systems. The viability of the synthetic point clouds is investigated using a deep learning semantic segmentation algorithm by comparing its performance with real-world point clouds. Our proposed framework can potentially provide an opportunity to replace real-world data collection through the transformation of existing as-built BIMs into synthetic 3D point clouds."
Analyzing impact of semi-productive work hours in scheduling and budgeting labor-intensive projects: simulation-based approach,"Zahedi, Leila and Lu, Ming and Collister, Todd",,2022,"This research investigates labor productivity based on resource-constrained project scheduling simulation models in order to render analytical decision support in planning crew size and worker-activity allocation for steel girder fabrication projects. In the dynamic environment of a structural steel fabrication facility, each laborer (journeyman) is part of teams temporarily formed at particular workstations to conduct various material-handling and connection activities. Discrete-event-simulation-based resource-constrained scheduling analysis is instrumental in analyzing semi-productive work hours resulting from labor transferring between activities and crew matching. In the case study, semi-productive work hours can be lowered from about one half of the total working time to a third by fine-tuning the crew size and work sequencing based on the simulation model, thereby resulting in enhancements on the time and cost performances of the entire project."
Gender Neutrality in Robots: An Open Living Review Framework,"Seaborn, Katie and Pennefather, Peter",,2022,"Gender is a primary characteristic by which people organize themselves. Previous research has shown that people tend to unknowingly ascribe gender to robots based on features of their embodiment. Yet, robots are not necessarily ascribed the same, or any, gender by different people. Indeed, robots may be ascribed non-human genders or used as ""genderless"" alternatives. This underlies the notion of gender neutrality in robots: neither masculine nor feminine but somewhere in between or even beyond gender. Responding to calls for gender as a locus of study within robotics, we offer a framework for conducting an open living review to be updated periodically as work emerges. Significantly, we provide an open, formalized submission process and open access dataset of research on gender neutrality in robots. This novel and timely approach to consensus-building is expected to pave the way for similar endeavours on other key topics within human-robot interaction research."
"Neither ""Hear"" Nor ""Their"": Interrogating Gender Neutrality in Robots","Seaborn, Katie and Pennefather, Peter",,2022,"Gender is a social framework through which people organize themselves-and non-human subjects, including robots. Research stretching back decades has found evidence that people tend to gender artificial agents unwittingly, even with the slightest cue of humanlike features in voice, body, role, and other social features. This has led to the notion of gender neutrality in robots: ways in which we can avoid gendering robots in line with human models, as well explorations of extra-human genders. This rapid review critically surveyed the literature to capture the state of art on gender neutrality in robots that interact with people. We present findings on theory, methods, results, and reflexivity. We interrogate the very idea that robot gender/ing can be neutral and explore alternate ways of approaching gender/ing through the design and study of robots interacting with people."
Are analytical techniques worthwhile for analog IC placement?,"Lin, Yishuang and Li, Yaguang and Fang, Donghao and Madhusudan, Meghna and Sapatnekar, Sachin S. and Harjani, Ramesh and Hu, Jiang",,2022,"Analytical techniques have long been a prevailing approach to digital IC placement due to their advantage in handling large-sized problems. Recently, they have been adopted for analog IC placement, an area where prior methods were mostly based on simulated annealing. However, a comparative study between the two classes of approaches is lacking. Moreover, the effectiveness of different analytical techniques is not clear. This work attempts to shed light on both issues by studying existing methods and developing a new analytical technique. Since prior analytical methods have not addressed circuit performance, a critical concern for automated analog layout, this work also extends the new analytical placer for performance-driven placement. Experiments on various test circuits show that for a conventional performance-oblivious formulation, the proposed analytical technique achieves 55X speedup and 12% wirelength reduction compared to simulated annealing. For performance-driven placement, the proposed technique outperforms simulated annealing in terms of circuit performance, area, and runtime. Moreover, the proposed technique generally provides better solution quality than an alternative analytical technique."
Emotion Contagion in Agent-based Simulations of Crowds: A Systematic Review,"van Haeringen, Erik and Gerritsen, Charlotte and Hindriks, Koen",,2023,"Emotions are known to spread among people, a process called emotion contagion. Both positive and negative emotions are believed to be contagious, but the mass spread of negative emotions has attracted the most attention due to its danger to society. The use of agent-based techniques to simulate emotion contagion in crowds has grown over the last decade and a range of contagion mechanisms and applications have been considered. With this review we aim to give a comprehensive overview of agent-based methods to implement emotion contagion in crowd simulations. We took a systematic approach and collected studies from Web of Science, Scopus, IEEE and ACM that propose agent-based models that include a process of emotion contagion in crowds. We classify the models in three categories based on the mechanism of emotion contagion and analyse the contagion mechanism, application and findings of the studies. Additionally, a broad overview is given of other agent characteristics that are commonly considered in the models. We conclude that there are fundamental theoretical differences among the mechanisms of emotion contagion that reflect a difference in view on the contagion process and its application, although findings from comparative studies are inconclusive. Further, while large theoretical progress has been made in recent years, empirical evaluation of the proposed models is lagging behind due to the complexity of reliably measuring emotions and context in large groups. We make several suggestions on a way forward regarding validation to eventually justify the application of models of emotion contagion in society."
A Prompt-based Few-shot Learning Approach to Software Conflict Detection,"Helmeczi, Robert K. and Cevik, Mucahit and Y1ldmm, Savas",,2022,"A software requirement specification (SRS) document is an essen­tial part of the software development life cycle which outlines the requirements that a software program in development must satisfy. This document is often specified by a diverse group of stakeholders and is subject to continual change, making the process of maintain­ing the document and detecting conflicts between requirements an essential task in software development. Notably, projects that do not address conflicts in the SRS document early on face consider­able problems later in the development life cycle. These problems incur substantial costs in terms of time and money, and these costs often become insurmountable barriers that ultimately result in the termination of a software project altogether. As a result, early detec­tion of SRS conflicts is critical to project sustainability. The conflict detection task is approached in numerous ways, many of which require a significant amount of manual intervention from devel­opers, or require access to a large amount of labeled, task-specific training data. In this work, we propose using a prompt-based learn­ing approach to perform few-shot learning for conflict detection. We compare our results to supervised learning approaches that use pretrained language models, such as BERT and its variants. Our results show that prompting with just 32 labeled examples can achieve a similar level of performance in many key metrics to that of supervised learning on training sets that are magnitudes larger in size. In contrast to many other conflict detection approaches, we make no assumptions about the type of underlying requirements, al­lowing us to analyze pairings of both functional and non-functional requirements. This allows us to omit the potentially expensive task of filtering out non-functional requirements from our dataset."
Workshop: Machine Learning in Software Quality,"Azim, Akramul and Smith, Kevin",,2022,"The workshop focuses on talks related to advancing the software quality paradigm using machine learning (ML) and/or artificial intelligence (AI). Software testing is an essential part of develop-ment that can be further advanced with the help of automation and effectively leveraging historical information. Continuous inte-gration environments enable large-scale software testing and the large volume of data generated in the process promotes the use of data analytics and ML techniques. Moreover, the historical infor-mation is useful to improve the next version of the software from models to code, which are essential components of software engi-neering. The workshop aims to cover the topics on software testing using ML, software testing in continuous integration environments, test case prioritization using ML, software quality assurance and requirements engineering using AI/ML."
A Self-Adaptive Search Space Reduction Approach for Offshore Wind Farm Installation Using Multi-Installation Vessels,"Peng, Shengrui and Szczerbicka, Helena",,2023,"As an important part of renewable energy resources, offshore wind energy has great potential compared to its onshore counterpart despite the vast developments in recent decades. However, due to the more complex environmental condition and physical restrictions the installation of an offshore wind farm is hard to plan and predict, which often results in delays. This paper focuses on the scheduling problem in the installation phase of an offshore wind farm. We propose an adaptive search strategy based on the Apriori property and information entropy. The purpose is to prune the search space effectively and intelligently to realize an agile and swift rescheduling according to the environmental changes. For the numerical experiments, we use the environmental data obtained from the German North Sea from the year 1958 to 2007 in hourly resolution."
How Does Imaging Impact Patient Flow in Emergency Departments?,"Prabhu, Vishnunarayan Girishan and Taaffe, Kevin and Shehan, Marisa and Pirrallo, Ronald and Jackson, William and Ramsay, Michael and Hobbs, Jessica",,2023,"Emergency Department (ED) overcrowding continues to be a public health issue as well as a patient safety issue. The underlying factors leading to ED crowding are numerous, varied, and complex. Although lack of in-hospital beds is frequently attributed as the primary reason for crowding, ED's dependencies on other ancillary resources, including imaging, consults, and labs, also contribute to crowding. Using retrospective data associated with imaging, including delays, processing time, and the number of image orders, from a large tier 1 trauma center, we developed a discrete event simulation model to identify the impact of the imaging delays and bundling image orders on patient time in the ED. Results from sensitivity analysis show that reducing the delays associated with imaging and bundling as few as 10% of imaging orders for certain patients can significantly (p-value &lt; 0.05) reduce the time a patient spends in the ED."
Discrete-Event Simulation and Machine Learning for Prototype Composites Manufacture Lead Time Predictions,"Smith, Jamie Karl and Dickinson, Calum",,2023,"The article looks to generate synthetic data for machine learning algorithms using discrete-event simulation (DES). The case study used for the DES model was the Composite Centre at the AMRC, where prototype composites products are manufactured. The machine learning algorithm was used to predict the lead times of composite products based on the current state of the system. The machine learning algorithm can calculate the lead times much faster than a simulation model and does not require the expertise of a simulation engineer to execute. Three different types of composites materials and their manufacturing process were initially modelled: dry fiber, prepreg and thermoplastic. The accuracies of three machine learning algorithms were compared. The algorithms chosen were: Artificial Neural Network (ANN), Recurrent Neural Network (RNN) and linear regression. It was found that the RNN provided the most accurate predictions and the linear regression algorithm was the worst performing algorithm."
A Tool-Based Approach to Assess Simulation Worthiness and Specify Sponsor Needs for SMEs,"Bicalho-Hoch, Ana Luiza and \""{O}zkul, Felix and Wittine, Nicolas and Wenzel, Sigrid",,2023,"Many small and medium-sized enterprises (SMEs) still refrain from using discrete-event simulation (DES) to plan, implement and operate their manufacturing systems. Given the increasing relevance of DES - e.g., as the basis for digital twins - a need for action is therefore identified. While the reasons for the seeming aversion to DES in the context of SMEs are well-researched, the following work's main objective is to present a tool-based approach that aims at supporting SMEs overcome the hurdles of the early stages within a structured simulation study. This is done through the assistance of users in identifying issues that are simulation-worthy as well as approaching the possible tendering and development of accurate problem specifications. Furthermore, the research methodology and the results of the commenced Delphi study are outlined. A critical reflection and an outlook on the topic are provided as well."
A Biased-Randomized Simheuristic for a Hybrid Flow Shop with Stochastic Processing Times in the Semiconductor Industry,"Ammouriova, Majsa and Panadero, Javier and Lei\ss{}au, Madlene and Laroque, Christoph and Schumacher, Christin and Juan, Angel A.",,2023,"Compared to other industries, production systems in semiconductor manufacturing have an above-average level of complexity. Developments in recent decades document increasing product diversity, smaller batch sizes, and a rapidly changing product range. At the same time, the interconnections between equipment groups increase due to rising automation, thus making production planning and control more difficult. This paper discusses a hybrid flow shop problem with realistic constraints, such as stochastic processing times and priority constraints. The primary goal of this paper is to find a solution set (permutation of jobs) that minimizes the production makespan. The proposed algorithm extends our previous work by combining biased-randomization techniques with a discrete-event simulation heuristic. This simulation-optimization approach allows us to efficiently model dependencies caused by batching and by the existence of different flow paths. As shown in a series of numerical experiments, our methodology can achieve promising results even when stochastic processing times are considered."
Real-Time Activity Duration Extraction of Crane Works for Data-Driven Discrete Event Simulation,"Jungmann, Manuel and Ungureanu, Lucian and Hartmann, Timo and Posada, Hector and Chacon, Rolando",,2023,"The construction industry is struggling with low productivity rates because of a low level of digitalization, dynamic interactions, and uncontrollable circumstances on sites, which make the planning process complex. Usage of the digital twin construction paradigm enables to facilitate construction management and leverage the sector's unexploited potential. This research addresses current shortcomings by real-time discrete event simulation. During crane operations, kinematic data were collected, which were classified by machine learning algorithms for activity recognition and duration extraction. Based on the identified durations, Goodness-of-Fit techniques determined suitable probability density functions. The resulting probability density functions were used as input parameters in stochastic discrete event simulations. It was shown that with enriched data collection, probability density functions have to be updated. The data-driven discrete event simulation facilitates decision-making processes by providing more reliable real-time information for the planning of upcoming construction works. Thus, data-based instead of experience-based management can be enabled."
Evaluating District-based Election Surveys with Synthetic Dirichlet Likelihood,"Mitra, Adway and Dey, Palash",,2024,"In district-based multi-party elections, electors cast votes in their respective districts. In each district, the party with maximum votes wins the corresponding ""seat"" in the governing body. Election Surveys try to predict the election outcome (vote shares and seat shares of parties) by querying a random sample of electors. However, the survey results are often inconsistent with the actual results, which could be due to multiple reasons. The aim of this work is to estimate a posterior distribution over the possible outcomes of the election, given one or more survey results. This is achieved using a prior distribution over vote shares, election models to simulate the complete election from the vote share, and survey models to simulate survey results from a complete election. The desired posterior distribution over the space of possible outcomes is constructed using Synthetic Dirichlet Likelihoods, whose parameters are estimated from Monte Carlo sampling of elections using the election models. We further show the same approach can also use be used to evaluate the surveys - whether they were biased or not, based on the true outcome once it is known. Our work offers the first-ever probabilistic model to analyze district-based election surveys. We illustrate our approach with extensive experiments on real and simulated data of district-based political elections in India."
A Review of Agent-Based Modeling Applications in Substance Abuse Policy Research,"Zhong, Xiang and Li, Xuanjing and Mangoni, Samantha",,2024,"This study provides a systematic review of existing studies that used agent-based modeling (ABM) to inform substance abuse policies and identifies future research directions. The detailed review included 20 articles, among which, tobacco, alcohol, cannabis, opioids, and heroin substance abuse were studied. These studies examined substance abuse interventions and the associations between substance use and social behavior, such as peer interaction and selection. Effective interventions included retailer density reduction policies, restriction of trading hours of licensed venues, ecstasy pill-testing and passive-alert detection dogs by police at public venues, and a mass-media drug prevention education policy. ABM can capture the dynamic interactions among and between agents and environments, making it appropriate to model complex substance abuse behaviors. Limitations in current studies include a lack of ABM validation efforts and generalizable data. Future studies should use generalizable and abundant information to inform their ABM, as well as have an explicit validation method."
"Symbiotic Use of Digital Twin, Simulation and Design Thinking Approach for Resilient Enterprise","Barat, Souvik and Lobo, Sylvan and Korabu, Reshma and Thogaru, Himabindu and Mahamuni, Ravi",,2024,"Enterprises are increasingly facing the need to be resilient in the face of uncertainty and dynamism. Simulatable digital twins have become critical aids for analyzing and adapting complex systems. Design thinking and service design methodologies, in contrast, are gaining momentum for ideation, subjective evaluation, and innovation. A systematic application of these methodologies to explore innovative ideas and a faithful virtual environment to test and fine-tune those ideas without impacting real systems could be transformational. This paper presents an approach that establishes a symbiotic relationship between these two approaches to introduce precision and innovativeness to make enterprises resilient. We describe the key characteristics of resilient enterprises, present our approach, and illustrate its effectiveness with a case study focusing on a transformation toward a new normal to address the Covid-19 pandemic induced disruptions in the IT industry."
Forecasting Patient Arrivals and Optimizing Physician Shift Scheduling in Emergency Departments,"Prabhu, Vishnunarayan Girishan and Taaffe, Kevin and Pirrallo, Ronald and Jackson, William and Ramsay, Michael and Hobbs, Jessica",,2024,"Emergency Departments (EDs) are the primary access points for millions of patients seeking medical care. The increasing patient demand and lack of long-term dynamic planning strain the EDs in providing timely patient care, leading to crowding. While a well-recognized problem, ED crowding is still prevalent, where suboptimal resource allocation is one significant contributing factor. In this research, we developed an end-to-end solution that first forecasted the patient arrivals to the partner ED and then used an optimization model to develop an optimal physician staffing schedule to minimize the combined cost of patient wait times, handoffs, and physician shifts. Finally, the new schedule was tested using the validated simulation model to evaluate the ED performance. By generating shift schedules based on forecasts and testing them in the validated simulation model, we observed that patient time in the ED and handoffs could be reduced by 5.6% and 9.2% compared to current practices."
Simulating the Impact of Forecast Related Overbooking and Underbooking Behavior on MRP Planning and a Reorder Point System,"Seiringer, Wolfgang and Altendorfer, Klaus and Felberbauer, Thomas",,2024,"Production Planning and its parameterization is critical to fulfil customer demands and to successfully react on changes in high volatile markets. Therefore, demand updates should be considered to improve production planning. In this paper the performance of two production planning methods MRP (Material Requirements Planning) and RPS (Reorder Point System) are compared in a multi-item single stage system where customer orders are updated in a rolling horizon manner. Applying a simulation study, we investigate the performance of MRP and RPS for biased and unbiased forecast information and discuss the difference in the optimal planning parameters. The study shows that for a production system with underbooking and low demand uncertainty, RPS method is superior, in all other scenarios MRP outperforms RPS. For overbooking scenarios, the results show MRP leads to overall cost improvements ranging from 8 to 30 %."
Towards an Automatic Construction of Simulation Scenarios: A Systematic Review,"Davis, Christopher W. H. and Jetter, Antonie J. and Giabbanelli, Philippe J.",,2024,"A predictive simulation is built on a conceptual model (e.g., to identify relevant constructs and relationships) and serves to estimate the potential effects of 'what-if' scenarios. Developing the conceptual model and plausible scenarios has long been a time-consuming activity, often involving the manual processes of identifying and engaging with experts, then performing desk research, and finally crafting a compelling narrative about the potential futures captured as scenarios. Automation could speed-up these activities, particularly through text mining. We performed the first review on automation for simulation scenario building. Starting with 420 articles published between 1995 and 2022, we reduced them to 11 relevant works. We examined them through four research questions concerning data collection, extraction of individual elements, connecting elements of insight and (degree of automation of) scenario generation. Our review identifies opportunities to guide this growing research area by emphasizing consistency and transparency in the choice of datasets or methods."
Applying Civil Information Modeling and Augmented Reality to the Construction of Underground Pipelines,"Cui, Andy and Liang, Man",,2024,"Municipal construction projects are often challenging and risk-prone due to unexpected underground conditions. Access to As-Built and As-Design data is essential to avoid budget overruns, schedule delays, and other construction disputes. However, coordinating field conditions with construction drawings can be difficult and lead to discrepancies. Traditional methods of denoting information onto the ground by surveyors and field workers have been limited in their ability to provide relevant information and support scaling up. These methods also create restrictions in data sharing and communication among workers and engineering teams. With the development and use of AR technology, our study proposes an augmented reality tool leveraging Google ARCore to assist construction engineers in a straightforward and efficient manner by displaying utility information, including pipe direction, type, slope, diameter, and material. The campus area of the University of Maryland College Park is used as a case study to demonstrate our approach."
A Review of Trends and Practices in Using Visual Data for Construction-Related Machine Learning Models,"Mohammadi, Abbas and Golazad, SeyedeZahra and Rashidi, Abbas",,2025,"This paper systematically reviews image-based analysis in the construction industry, examining 136 articles through 2023. The findings reveal a marked increase in the use of machine learning (ML), deep learning (DL), and reinforcement learning (RL) models, which utilize image and video data to enhance worker safety, monitor construction progress, and improve project management. The study identifies a significant shift towards integrating real and synthetic data, enhancing model robustness. It also highlights the rising adoption of data-sharing practices, with an increase in publicly available datasets. However, the review highlights underexplored areas such as synthetic data use and advanced privacy-preserving methods. These gaps suggest opportunities for further research to leverage technology more effectively in the construction sector."
Cross-Training Policies for Enhanced Resilience in Emergency Departments,"Abdelwanis, Moustafa and Ouda, Eman and Sleptchenko, Andrei and Gabor, Adriana F. and Simsekler, Mecit Can Emre and Omar, Mohammed",,2025,"This paper investigates cross-training policies to enhance emergency department resilience in managing patient demand surges. We evaluated operational performance under varying patient flows using a simulation model based on an emergency department in Abu Dhabi, UAE. Results demonstrated a significant increase in patient length of stay in the triage room due to a shortage of available nurses during surges. Conversely, nurses in other areas (adult zone, pediatrics, and fast-track) were less affected. Two cross-training policies were investigated: pooling nurses from the triage and adult zones and expanding the pool to include the pediatrics section. The first policy reduced patient length of stay in the triage by 91.71%. The second policy further improved flexibility, leading to reductions in length of stay. Additional experiments revealed the limitations of the second policy when subjected to higher surge levels; therefore, further research is needed to explore a broader range of policies and contexts."
Investigating the Impact of Pandemic on the Perioperative Healthcare Workers Availability: An Agent-Based Approach,"Prasad, Shweta and Prabhu, Vishnunarayan Girishan and Hand, William",,2025,"Protecting healthcare workers (HCWs) during a pandemic is critical to provide timely medical care for patients. Although prior studies have investigated HCW unavailability during the COVID-19 pandemic, the studies have not investigated the impact of parameters such as patient census, vaccination rates, transmission rates, and multiple hospital locations on HCW availability. This research considers a high-risk HCW group of perioperative staff to investigate the impact of segregating and rotating staffing policies on HCW unavailability during a pandemic in a health system with multiple locations. An agent-based model with a SEIR compartmental model was developed to simulate various scenarios. Simulated findings indicate that segregating and rotating policies significantly (p-value &lt;0.01) reduced the peak weekly unavailability of HCWs and the total percentage of HCWs getting infected by as much as 25% and 60% when vaccination rates were lower (&lt;75%). However, these benefits diminished when the vaccination rates increased to 75%."
A Maturity Model for Digital Twins in Healthcare,"Mustafee, Navonil and Harper, Alison and Viana, Joe and Monks, Thomas",,2025,"Digital models, digital shadows, and digital twins (DTs) are increasingly used in manufacturing/Industry 4.0 to represent levels of integration between physical systems and their digital counterparts; data-flow mechanisms are the enablers of such integration. Healthcare operations management has also witnessed rising interest in hybrid models that use real-time data to increase situational awareness (SA) and enable short-term decision-making. In M&amp;S literature, such models are referred to as Real-time Simulations (RtS) and DTs. Healthcare organizations can realize a heightened state of SA by transitioning from conventional modeling to RtS/DTs. The paper presents a Maturity Model for DTs to contextualize the increasing levels of healthcare Information Systems/Information Technology (IS/IT) integration with real-time models that such a shift will necessitate. The higher the Maturity Level of IS/IT integration, the greater the opportunity to develop modeling artifacts that realize the potential of real-time data and enable organizations to attain higher levels of SA."
How Do Different Minds Shape Performance in Construction? An Agent-Based Modelling Approach,"Shehab, Lynn and Hamzeh, Farook",,2025,"In the dynamic construction industry, project performance crucially depends on the cognitive diversity of team members. The varied cognitive abilities across teams significantly influence their adaptability and efficiency, driving the success of construction projects in unpredictable and dynamics environments. This paper employs an Agent-Based Modelling approach to explore how variations in cognitive abilities, improvisation, collaboration, and physiology-related, affect project outcomes. By simulating real-world construction scenarios, the model examines the effects of cognitive trait diversity on decision-making and team dynamics. This study aims to uncover the complex relationship between cognitive diversity and project performance, highlighting its impact on the efficacy of solutions and collaborative efforts. The findings provide valuable insights into optimizing team composition and enhancing decision-making processes in construction projects. Ultimately, this research advances our understanding of how cognitive factors influence project success, offering strategies to foster more resilient and effective construction teams."
Disengagement through Algorithms: How Traditional Organizations Aim for Experts' Satisfaction,"Poiroux, J\'{e}r\'{e}mie",,2025,"This study examines the use of algorithmic tools in traditional organizational decision-making processes. Through forty semi-structured interviews with managers, engineers, and (expert) users across six European projects, we suggest that initiators deploy algorithms not to automate actions or replace users, but to disengage themselves from prescriptive decision-making. Consequently, the responsibility to choose, select, and decide falls upon the users; they become engaged. Therefore, algorithm evaluation is oriented towards utility, interpretability, and, more broadly, user satisfaction. Further research is encouraged to analyze the advent of a 'satisfaction regime', from platforms to traditional organizations."
Repairing Trust in Robots?: A Meta-analysis of HRI Trust Repair Studies with A No-Repair Condition,"Esterwood, Connor and Robert, Lionel P.",,2025,"As robots become more integrated into various sectors, understanding human-robot interaction (HRI) dynamics, particularly trust repair, is crucial for successful collaboration. For this paper, the authors conducted a meta-analysis of 22 HRI trust repair studies with 3,763 participants to evaluate the effectiveness of strategies for restoring trust after breaches relative to offering no repair. The analysis identified three key findings: (1) strategies are differentially effective, showing limited success in restoring trustworthiness; (2) the overall impact on repairing trust is marginal, with a small effect size; and (3) apologies and explanations are the most effective strategies for trust repair. These insights enrich HRI literature by providing a comprehensive evaluation of trust repair mechanisms, offering valuable guidance for future research and practical improvements in human-robot collaboration."
Virtually the Same or Realistically Different?: A Meta-analysis of Real vs. 'Not So Real' Robots,"Esterwood, Connor and Guan, Ruijia and Ye, Xin and Robert, Lionel P.",,2025,"This study examined an important debate in Human-Robot Interaction (HRI) research: the suitability of non-physically non-collocated robots instead of physically collocated robots for HRI research. This meta-analysis (N=34 studies) examined the equivalence of physically and non-physically collocated robots in HRI research, focusing on anthropomorphism, social presence, and user engagement. No significant differences were found, suggesting that non-physical representations are viable alternatives. However, observed heterogeneity indicates potential moderating factors (e.g., task complexity, user characteristics, design features) warranting further investigation. These findings inform choices in resource-constrained environments."
NeuroEngage: A Multimodal Dataset Integrating fMRI for Analyzing Conversational Engagement in Human-Human and Human-Robot Interactions,"Torubarova, Ekaterina and Arvidsson, Caroline and Berrebi, Jonathan and Udd\'{e}n, Julia and Pereira, Andre",,2025,"This study aimed to deepen our understanding of the behavioral and neurocognitive processes involved in human-human and human-robot communication in a more ecologically valid setting compared to the traditional neurolinguistic paradigms. We collected a novel open-source dataset (N=30 for human-human and N=20 for human-robot interactions), that includes fMRI, eye-tracking, segmented audio, video, and behavioral data, resulting in 30 minutes of free conversations per participant. To enable unrestricted, spontaneous robot behavior, we employed a novel VR-mediated teleoperation system. Our mixed design allowed us to compare participants' perception of humans and robots across three within-subject conditions of conversational engagement: Engaged Communicator, Active Listener, and Passive Listener. We provide an open-access dataset, replicable code for the teleoperation system, and an initial analysis of fMRI, behavioral, and speech data. We observed distinct neural profiles: speaking to the human agent recruited more higher-level frontal regions associated with socio-pragmatic processes, while listening to the robot recruited more sensory areas, including auditory and visual regions. Engagement levels and agent types also affected speech and behavioral patterns, offering valuable insights into conversational dynamics in human-human and human-robot interactions."
Power Dynamics and Autonomy: Engaging Employees around the Design of Autonomous Agents,"Ostrowski, Anastasia K. and Gunther, Lucy and DiPaola, Daniella and Breazeal, Cynthia",,2025,"Power dynamics through the lens of autonomy in human-robot interaction (HRI) has largely been considered through the robot's persuasion and its impact on users. Power found in hierarchies and relationships between users and robots has been less investigated. Through a co-design workshop with industry employees, we investigate how employees design autonomous agents (AA) for a risk management and safety task, focusing on the designed agent interactions and personalities (robots and chatbots). The qualitative analysis of the storyboards and personalities revealed how employees designed autonomy and power dynamics between the workers and AAs, mirroring power dynamics in social systems. These results revealed how power dynamics are interconnected with anthropomorphization of AAs, demonstrating what has been previously theorized by HRI scholars. Overall, this work considers autonomy and power in design representations and unifies varying conceptualizations of power in HRI to support a holistic perspective of how power is explored through HRI design."
"Staging, Accommodating or Caring: Reviewing the Human Labor Involved in Shaping Robots into Agents","Stedtler, Samantha",,2025,"This review examines the hidden and invisible labor humans perform to enable robots to function as agents within Human-Robot Interaction (HRI). While robot agency is often framed as internal and autonomous, this perspective overlooks the relational dynamics that sustain robot functionality in real-world settings. Drawing on feminist theory, the review highlights how humans frequently share the responsibility of managing robot limitations through scaffolding, mediation, and care work, often unnoticed or undervalued. The review compares various studies that investigate this labor, focusing on the roles of human agents and their contributions in ensuring robots succeed in tasks. It identifies gaps in the current literature, including a limited focus on domestic environments and the lack of frameworks to conceptualize and make visible the often unrecognized labor in HRI. The findings call for more attention to the invisible labor integral to robot agency and suggest future research directions that incorporate both qualitative and quantitative methods to better understand and value this critical work."
