ID,ENTRYTYPE,title,author,year,journal,booktitle,volume,number,pages,doi,url,note,bibsource,biburl,timestamp,publisher,editor,series,howpublished,month,eprint,eprinttype,urn,Abstract
DBLP:journals/fgcs/EcclesWV26,article,Mosaic: Composite projection pruning for resource-efficient LLMs,"Bailey J. Eccles and
Leon Wong and
Blesson Varghese",2026,Future Gener. Comput. Syst.,,175,,108056,10.1016/J.FUTURE.2025.108056,https://doi.org/10.1016/j.future.2025.108056,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/fgcs/EcclesWV26.bib,"Thu, 11 Sep 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/jss/WysockiO26,article,"Leveraging LLM-based data augmentation for automatic classification
of recurring tasks in software development projects","Wlodzimierz Wysocki and
Miroslaw Ochodek",2026,J. Syst. Softw.,,231,,112641,10.1016/J.JSS.2025.112641,https://doi.org/10.1016/j.jss.2025.112641,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/jss/WysockiO26.bib,"Tue, 07 Oct 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/access/AboukadriOMA25,article,"Leveraging {RAG} and LLMs for Access Control Policy Extraction From
User Stories in Agile Software Development","Sara Aboukadri and
Aafaf Ouaddah and
Abdellatif Mezrioui and
Ikram El Asri",2025,{IEEE} Access,,13,,116462--116472,10.1109/ACCESS.2025.3586203,https://doi.org/10.1109/ACCESS.2025.3586203,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/access/AboukadriOMA25.bib,"Sat, 09 Aug 2025 01:00:00 +0200",,,,,,,,,"Agile development has become increasingly popular among software development teams due to its capacity to deliver and update software rapidly while accommodating evolving requirements. Within this dynamic context, access control policies are critical for ensuring the security of systems by defining who can access specific resources under given conditions. However, identifying and documenting these policies often rely on manual, time-intensive processes prone to errors and oversight. This paper proposes an innovative framework leveraging Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) to automate the extraction and organization of access control policies from user stories and software documentation. The framework focuses on the early stages of the development lifecycle, capturing access control requirements as expressed in natural language artifacts. It comprises two core components: 1) a pipeline for extracting and categorizing access control policies, enabling precise mappings between roles, actions, and resources, and 2) an interactive chatbot designed to support Security Operations Center (SOC) analysts in evaluating suspicious access requests by providing contextualized insights into access policies. By integrating advanced natural language processing techniques with retrieval-based augmentation, the framework aims to reinforce access control mechanisms by improving visibility, and providing contextualized insights for security analysts."
DBLP:journals/ase/LiuJZNLL25,article,"Exploring the potential of general purpose LLMs in automated software
refactoring: an empirical study","Bo Liu and
Yanjie Jiang and
Yuxia Zhang and
Nan Niu and
Guangjie Li and
Hui Liu",2025,Autom. Softw. Eng.,,32,1,26,10.1007/S10515-025-00500-0,https://doi.org/10.1007/s10515-025-00500-0,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/ase/LiuJZNLL25.bib,"Thu, 31 Jul 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/cgf/SevastjanovaGSE25,article,"\emph{LayerFlow}: Layer-wise Exploration of {LLM} Embeddings using
Uncertainty-aware Interlinked Projections","Rita Sevastjanova and
Robin Gerling and
Thilo Spinner and
Mennatallah El{-}Assady",2025,Comput. Graph. Forum,,44,3,,10.1111/CGF.70123,https://doi.org/10.1111/cgf.70123,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/cgf/SevastjanovaGSE25.bib,"Thu, 11 Sep 2025 01:00:00 +0200",,,,,,,,,"<jats:title>Abstract</jats:title>Large language models (LLMs) represent words through contextual word embeddings encoding different language properties like semantics and syntax. Understanding these properties is crucial, especially for researchers investigating language model capabilities, employing embeddings for tasks related to text similarity, or evaluating the reasons behind token importance as measured through attribution methods. Applications for embedding exploration frequently involve dimensionality reduction techniques, which reduce high‚Äêdimensional vectors to two dimensions used as coordinates in a scatterplot. This data transformation step introduces uncertainty that can be propagated to the visual representation and influence users' interpretation of the data. To communicate such uncertainties, we present <jats:bold>LayerFlow</jats:bold> ‚Äì a visual analytics workspace that displays embeddings in an interlinked projection design and communicates the transformation, representation, and interpretation uncertainty. In particular, to hint at potential data distortions and uncertainties, the workspace includes several visual components, such as convex hulls showing 2D and HD clusters, data point pairwise distances, cluster summaries, and projection quality metrics. We show the usability of the presented workspace through replication and expert case studies that highlight the need to communicate uncertainty through multiple visual components and different data perspectives."
DBLP:journals/cluster/ChandraKPA25,article,"Decision support system for Forest fire management using Ontology
with Big Data and LLMs","Ritesh Chandra and
Shashi Shekhar Kumar and
Rushil Patra and
Sonali Agarwal",2025,Clust. Comput.,,28,8,548,10.1007/S10586-025-05383-0,https://doi.org/10.1007/s10586-025-05383-0,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/cluster/ChandraKPA25.bib,"Sun, 07 Sep 2025 01:00:00 +0200",,,,,,,,,"Forests are crucial for ecological balance, but wildfires, a major cause of forest loss, pose significant risks. Fire weather indices, which assess wildfire risk and predict resource demands, are vital. With the rise of sensor networks in fields like healthcare and environmental monitoring, semantic sensor networks are increasingly used to gather climatic data such as wind speed, temperature, and humidity. However, processing these data streams to determine fire weather indices presents challenges, underscoring the growing importance of effective forest fire detection. This paper discusses using Apache Spark for early forest fire detection, enhancing fire risk prediction with meteorological and geographical data. Building on our previous development of Semantic Sensor Network (SSN) ontologies and Semantic Web Rules Language (SWRL) for managing forest fires in Monesterial Natural Park, we expanded SWRL to improve a Decision Support System (DSS) using a Large Language Models (LLMs) and Spark framework. We implemented real-time alerts with Spark streaming, tailored to various fire scenarios, and validated our approach using ontology metrics, query-based evaluations, LLMs score precision, F1 score, and recall measures."
DBLP:journals/cm/MekracheMKBV25,article,"On Combining {XAI} and LLMs for Trustworthy Zero-Touch Network and
Service Management in 6G","Abdelkader Mekrache and
Mohamed Mekki and
Adlen Ksentini and
Bouziane Brik and
Christos V. Verikoukis",2025,{IEEE} Commun. Mag.,,63,4,154--160,10.1109/MCOM.002.2400276,https://doi.org/10.1109/MCOM.002.2400276,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/cm/MekracheMKBV25.bib,"Wed, 23 Apr 2025 01:00:00 +0200",,,,,,,,,"Zero-touch network and service management (ZSM) is a key pillar of 6G networks. It allows the 6G management and orchestration framework to operate the networks without external (e.g., human) intervention. To effectively achieve ZSM, advanced network management procedures are required to detect and resolve anomalies within the 6G network autonomously, which usually requires artificial intelligence (AI) and machine learning (ML) models. However, relying solely on AI can raise concerns about trust due to their lack of explainability. Indeed, as these models are not explainable, it is difficult to understand and trust their decisions. To overcome this limitation, this article introduces a novel pipeline for ensuring trustworthy ZSM in 6G networks by combining AI for detecting anomalies; eXplainable AI (XAI) to identify the root causes of anomalies using feature importance analysis; and large language models (LLMs) to generate user-friendly explanations and suggest/apply corrective actions to resolve anomalies. A use case is presented using XGBoost as AI, SHAP as XAI, and Llama2 as LLM to address service level agreement (SLA) latency violations within cloud-native 6G microservices. Evaluation results obtained through real experiments demonstrate the framework's efficiency in scaling cloud resources to prevent SLA violations while providing understandable explanations to users, thereby enhancing trust in the system."
DBLP:journals/computer/Vieira25,article,"Leveraging LLMs for Trustworthy Software Engineering: Insights and
Challenges",Marco Vieira,2025,Computer,,58,7,79--90,10.1109/MC.2025.3546204,https://doi.org/10.1109/MC.2025.3546204,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/computer/Vieira25.bib,"Sat, 09 Aug 2025 01:00:00 +0200",,,,,,,,,"Large language models (LLMs) are transforming software engineering by accelerating development, reducing complexity, and cutting costs. If fully integrated into the software lifecycle they will have the potential to drive design, development, and deployment. However, LLM-driven trustworthy software engineering requires addressing multiple challenges."
DBLP:journals/fcomp/HemmatSRLT25,article,"Research directions for using {LLM} in software requirement engineering:
a systematic review","Arshia Hemmat and
Mohammadreza Sharbaf and
Shekoufeh Kolahdouz Rahimi and
Kevin Lano and
Sobhan Yassipour Tehrani",2025,Frontiers Comput. Sci.,,7,,,10.3389/FCOMP.2025.1519437,https://doi.org/10.3389/fcomp.2025.1519437,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/fcomp/HemmatSRLT25.bib,"Sun, 27 Apr 2025 01:00:00 +0200",,,,,,,,,"Natural Language Processing (NLP) and Large Language Models (LLMs) are transforming the landscape of software engineering, especially in the domain of requirement engineering. Despite significant advancements, there is a notable lack of comprehensive survey papers that provide a holistic view of the impact of these technologies on requirement engineering. This paper addresses this gap by reviewing the current state of NLP and LLMs in requirement engineering.We analyze trends in software requirement engineering papers, focusing on the application of NLP and LLMs. The review highlights their effects on improving requirement extraction, analysis, and specification, and identifies key patterns in the adoption of these technologies.The findings reveal an upward trajectory in the use of LLMs for software engineering tasks, particularly in requirement engineering. The review underscores the critical role of requirement engineering in the software development lifecycle and emphasizes the transformative potential of LLMs in enhancing precision and reducing ambiguities in requirement specifications.This paper identifies a growing interest and significant progress in leveraging LLMs for various software engineering tasks, particularly in requirement engineering. It provides a foundation for future research and highlights key challenges and opportunities in this evolving field."
DBLP:journals/frai/DyachenkoHSSN25,article,{LLM} services in the management of social communications,"Yuriy Dyachenko and
Oleksandra Humenna and
Oleg Soloviov and
Inna Skarga{-}Bandurova and
Nayden Nenkov",2025,Frontiers Artif. Intell.,,8,,,10.3389/FRAI.2025.1474017,https://doi.org/10.3389/frai.2025.1474017,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/frai/DyachenkoHSSN25.bib,"Thu, 01 May 2025 01:00:00 +0200",,,,,,,,,"This paper proposes enhancing social communication management with a behavioral economics approach through artificial intelligence instruments. The research aims to explore the influence of social communication on citizens‚Äô behavior using large language model services and assess its effectiveness. The paper builds on Daniel Kahneman‚Äôs dual-process theory, highlighting the intuitive system (System 1) and the rational system (System 2) in decision-making. The author introduces a third system, System 3, representing rooted in identity socially conditioned behavior influenced by societal norms and self-awareness. On this theoretical basis, the paper emphasizes automating communication management through large language model services, freeing up citizens‚Äô potential for self-determination and self-organization. By leveraging these services, messages can be crafted to support social transformation while respecting historical, cultural, and political contexts. Based on the preconditions and restrictions described above, we use GPT-4 model to generate messages based on these narratives. The experiment will use an observational study design with virtual persons. To compare the impact of original and modified messages according to the addressee‚Äôs mentality, we used the Claude 3.5 Sonnet model. We can see that the potential activity of respondents after perceiving the changed message does not change much, and the original message is perceived. Modifying messages by LLM services crafted to support social transformation while respecting historical, cultural, and political contexts cause attitudes to become substantially more negative (2.5‚ÄØunits downward shift in median); the intentions showed a slight positive increase (0.2‚ÄØunits upward change in median)."
DBLP:journals/ijmms/ZhaQHLGX25,article,"Designing child-centric {AI} learning environments: Insights from
an LLM-powered creative project-based learning study","Siyu Zha and
Yuehan Qiao and
Qingyu Hu and
Zhongsheng Li and
Jiangtao Gong and
Yingqing Xu",2025,Int. J. Hum. Comput. Stud.,,204,,103602,10.1016/J.IJHCS.2025.103602,https://doi.org/10.1016/j.ijhcs.2025.103602,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/ijmms/ZhaQHLGX25.bib,"Mon, 06 Oct 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/infsof/AhlgrenSKN25,article,"Assisting early-stage software startups with LLMs: Effective prompt
engineering and system instruction design","Thea Lovise Ahlgren and
Helene F{\o}nstelien Sunde and
Kai{-}Kristian Kemell and
Anh Nguyen{-}Duc",2025,Inf. Softw. Technol.,,187,,107832,10.1016/J.INFSOF.2025.107832,https://doi.org/10.1016/j.infsof.2025.107832,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/infsof/AhlgrenSKN25.bib,"Sat, 09 Aug 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/infsof/GaoCZ25,article,"{SVA-ICL:} Improving LLM-based software vulnerability assessment via
in-context learning and information fusion","Chaoyang Gao and
Xiang Chen and
Guangbei Zhang",2025,Inf. Softw. Technol.,,186,,107803,10.1016/J.INFSOF.2025.107803,https://doi.org/10.1016/j.infsof.2025.107803,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/infsof/GaoCZ25.bib,"Sat, 09 Aug 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/iotj/SuWKWSPZH25,article,"Hybrid RAG-Empowered Multimodal {LLM} for Secure Data Management in
Internet of Medical Things: {A} Diffusion-Based Contract Approach","Cheng Su and
Jinbo Wen and
Jiawen Kang and
Yonghua Wang and
Yuanjia Su and
Hudan Pan and
Zishao Zhong and
M. Shamim Hossain",2025,{IEEE} Internet Things J.,,12,10,13428--13440,10.1109/JIOT.2024.3521425,https://doi.org/10.1109/JIOT.2024.3521425,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/iotj/SuWKWSPZH25.bib,"Fri, 04 Jul 2025 01:00:00 +0200",,,,,,,,,"Secure data management and effective data sharing have become paramount in the rapidly evolving healthcare landscape, especially with the growing demand for the Internet of Medical Things (IoMT) integration. The advent of generative artificial intelligence (GenAI) has further elevated multimodal large language models (MLLMs) as essential tools for managing and optimizing healthcare data in IoMT. MLLMs can handle multimodal inputs and generate different kinds of data by utilizing large-scale training on massive multimodal datasets. Nevertheless, significant challenges remain in developing medical MLLMs, especially security and data freshness concerns, which impact the quality of MLLM outputs. To this end, this article proposes a hybrid Retrieval-Augmented Generation (RAG)-empowered medical MLLM framework for healthcare data management. The proposed framework enables secure data training by utilizing a hierarchical cross-chain design. Furthermore, it improves the output quality of MLLMs by using hybrid RAG that filters different unimodal RAG results using multimodal metrics and integrates these retrieval results as additional inputs for MLLMs. Furthermore, we utilize the age of information (AoI) to indirectly assess the influence of data freshness on MLLMs and apply contract theory to motivate healthcare data stakeholders to disseminate their current data, thereby alleviating information asymmetry in the data-sharing process. Finally, we employ a generative diffusion model-based deep reinforcement learning (DRL) technique to find the optimal contract for efficient data sharing. Numerical results show the effectiveness of the proposed approach in achieving secure and efficient healthcare data management."
DBLP:journals/iotj/YangZLK25,article,"ChatDL: An LLM-Based Defect Localization Approach for Software in
IIoT Flexible Manufacturing","Haiyang Yang and
Yulu Zhou and
Tian Liang and
Li Kuang",2025,{IEEE} Internet Things J.,,12,16,32333--32343,10.1109/JIOT.2025.3531512,https://doi.org/10.1109/JIOT.2025.3531512,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/iotj/YangZLK25.bib,"Thu, 11 Sep 2025 01:00:00 +0200",,,,,,,,,"With the rapid advancement of flexible manufacturing in the Industrial Internet of Things (IIoT), there has been a significant increase in the number of IIoT devices and application software aimed at meeting various needs. The software defects may lead to delays or crashes in flexible manufacturing system, thereby affecting the production schedule. Automated software defect localization based on code changes can significantly reduce development and maintenance time costs, thereby maintaining the competitive edge of flexible manufacturing in the IIoT. Current efforts in software defect localization are primarily based on deep learning models or information retrieval models. This article investigates the performance of large language models (LLMs) in software defect localization and optimizes localization accuracy by combining it with an information retrieval model. Our empirical study reveals that GPT, given a software defect description, is unable to determine whether specific code changes are relevant. The model is unable to provide accurate answers, which aligns with the generative nature of LLMs where responses are generated according to probability distributions. However, the combined framework of LLMs and information retrieval models proposed in this article outperforms the current state-of-the-art models on public datasets. We conclude that LLMs can enhance localization performance when used as side information in conjunction with existing information retrieval models. The effectiveness of the framework has been validated through experiments conducted on publicly available datasets and in practical applications within IIoT projects. This offers valuable insights into the application and development of LLMs for defect localization in the software development and maintenance processes in the IIoT flexible manufacturing."
DBLP:journals/jbd/GregoriLLMT25,article,"An LLM-guided platform for multi-granular collection and management
of data provenance","Luca Gregori and
Pasquale Leonardo Lazzaro and
Marialaura Lazzaro and
Paolo Missier and
Riccardo Torlone",2025,J. Big Data,,12,1,187,10.1186/S40537-025-01209-3,https://doi.org/10.1186/s40537-025-01209-3,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/jbd/GregoriLLMT25.bib,"Mon, 08 Sep 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/jss/CaiLWLS25,article,"Demystifying issues, causes and solutions in {LLM} open-source projects","Yangxiao Cai and
Peng Liang and
Yifei Wang and
Zengyang Li and
Mojtaba Shahin",2025,J. Syst. Softw.,,227,,112452,10.1016/J.JSS.2025.112452,https://doi.org/10.1016/j.jss.2025.112452,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/jss/CaiLWLS25.bib,"Wed, 11 Jun 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/jss/DilCD25,article,"Towards higher quality software vulnerability data using LLM-based
patch filtering","Charlie Dil and
Hui Chen and
Kostadin Damevski",2025,J. Syst. Softw.,,230,,112581,10.1016/J.JSS.2025.112581,https://doi.org/10.1016/j.jss.2025.112581,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/jss/DilCD25.bib,"Sun, 07 Sep 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/kbs/VitoFA25,article,{HELIOT:} LLM-Based {CDSS} for adverse drug reaction management,"Gabriele De Vito and
Filomena Ferrucci and
Athanasios Angelakis",2025,Knowl. Based Syst.,,328,,114184,10.1016/J.KNOSYS.2025.114184,https://doi.org/10.1016/j.knosys.2025.114184,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/kbs/VitoFA25.bib,"Sat, 06 Sep 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/pacmmod/ZhaoZFNLYPJLXTC25,article,"{MEMO:} Fine-grained Tensor Management For Ultra-long Context {LLM}
Training","Pinxue Zhao and
Hailin Zhang and
Fangcheng Fu and
Xiaonan Nie and
Qibin Liu and
Fang Yang and
Yuanbo Peng and
Dian Jiao and
Shuaipeng Li and
Jinbao Xue and
Yangyu Tao and
Bin Cui",2025,Proc. {ACM} Manag. Data,,3,1,53:1--53:28,10.1145/3709703,https://doi.org/10.1145/3709703,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/pacmmod/ZhaoZFNLYPJLXTC25.bib,"Tue, 08 Apr 2025 01:00:00 +0200",,,,,,,,,"Nowadays, Large Language Models (LLMs) have been trained using extended context lengths to foster more creative applications. However, long context training poses great challenges considering the constraint of GPU memory. It not only leads to substantial activation memory consumption during training, but also incurs considerable memory fragmentation. To facilitate long context training, existing frameworks have adopted strategies such as recomputation and various forms of parallelisms. Nevertheless, these techniques rely on redundant computation or extensive communication, resulting in low Model FLOPS Utilization (MFU). In this paper, we propose MEMO, a novel LLM training framework designed for fine-grained activation memory management. Given the quadratic scaling of computation and linear scaling of memory with sequence lengths when using FlashAttention, we offload memory-consuming activations to CPU memory after each layer's forward pass and fetch them during the backward pass. To maximize the swapping of activations without hindering computation, and to avoid exhausting limited CPU memory, we implement a token-wise activation recomputation and swapping mechanism. Furthermore, we tackle the memory fragmentation issue by employing a bi-level Mixed Integer Programming (MIP) approach, optimizing memory reuse across transformer layers. Empirical results demonstrate that MEMO achieves an average of 1.97x and 1.80x MFU compared to Megatron-LM and DeepSpeed, respectively. This improvement is attributed to MEMO's ability to minimize memory fragmentation, reduce recomputation and intensive communication, and circumvent the delays associated with the memory reorganization process due to fragmentation. By leveraging fine-grained activation memory management, MEMO facilitates efficient training of 7B LLM with 1 million sequence length on just 8 A800 GPUs, achieving an MFU of 52.30%."
DBLP:journals/pacmse/BouzeniaP25,article,"You Name It, {I} Run It: An {LLM} Agent to Execute Tests of Arbitrary
Projects","Islem Bouzenia and
Michael Pradel",2025,Proc. {ACM} Softw. Eng.,,2,{ISSTA},1054--1076,10.1145/3728922,https://doi.org/10.1145/3728922,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/pacmse/BouzeniaP25.bib,"Thu, 11 Sep 2025 01:00:00 +0200",,,,,,,,,"The ability to execute the test suite of a project is essential in many scenarios, e.g., to assess code quality and code coverage, to validate code changes made by developers or automated tools, and to ensure compatibility with dependencies. Despite its importance, executing the test suite of a project can be challenging in practice because different projects use different programming languages, software ecosystems, build systems, testing frameworks, and other tools. These challenges make it difficult to create a reliable, universal test execution method that works across different projects. This paper presents ExecutionAgent, an automated technique that prepares scripts for building an arbitrary project from source code and running its test cases. Inspired by the way a human developer would address this task, our approach is a large language model (LLM)-based agent that autonomously executes commands and interacts with the host system. The agent uses meta-prompting to gather guidelines on the latest technologies related to the given project, and it iteratively refines its process based on feedback from the previous steps. Our evaluation applies ExecutionAgent to 50 open-source projects that use 14 different programming languages and many different build and testing tools. The approach successfully executes the test suites of 33/50 projects, while matching the test results of ground truth test suite executions with a deviation of only 7.5%. These results improve over the best previously available technique by 6.6x. The costs imposed by the approach are reasonable, with an execution time of 74 minutes and LLM costs of USD¬†0.16, on average per project. We envision ExecutionAgent to serve as a valuable tool for developers, automated programming tools, and researchers that need to execute tests across a wide variety of projects."
DBLP:journals/pacmse/GuanBL25,article,"CrossProbe: LLM-Empowered Cross-Project Bug Detection for Deep Learning
Frameworks","Hao Guan and
Guangdong Bai and
Yepang Liu",2025,Proc. {ACM} Softw. Eng.,,2,{ISSTA},2430--2452,10.1145/3728984,https://doi.org/10.1145/3728984,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/pacmse/GuanBL25.bib,"Thu, 11 Sep 2025 01:00:00 +0200",,,,,,,,,"Deep Learning (DL) models may introduce reliability challenges in the underlying DL frameworks. These frameworks may be prone to bugs that can lead to crash or wrong results, particularly when involving complex model architectures and substantial computational demands. Such framework bugs can disrupt DL applications, impacting customer experience and potentially causing financial losses. Traditional approaches to testing DL frameworks face limitations in adapting to the vast search space of model structures, diverse APIs, and the complexity of hybrid programming and hardware environments. Recent advancements using Large Language Models (LLMs) have improved DL framework fuzzing, but their efficacy depends heavily on the quality and diversity of input prompts, which are often constructed using single-framework data.
 
 In this paper, we propose an innovative approach for enhancing test generation for DL frameworks by leveraging ‚Äúmirroring issues‚Äù‚Äîanalogous bugs identified across different frameworks with common functionalities. Our approach is inspired by the fact that DL frameworks, such as PyTorch and TensorFlow, often share common bugs due to dependencies, developer errors, or edge-case inputs. We develop CrossProbe that utilizes LLMs to effectively learn from existing issues of one framework and transfer the acquired knowledge to generate test cases for finding mirroring issues in another framework, thus enabling cross-framework bug detection. To overcome the challenges of test case generation arising from the incompatible functionalities and different implementations between frameworks, we introduce three processes:
 alignment
 ,
 screening
 , and
 distinction
 . These processes help mitigate transfer errors by establishing API pair databases, filtering unsuitable cases, and highlighting cross-framework distinctions. Experiments demonstrate that CrossProbe is efficient by saving 36.3% iterations of generation, and achieves a 25.0% higher success rate in issue transferring compared to existing state-of-the-art LLM-based testing techniques. CrossProbe detects 24 unique bugs using its transferred knowledge. Out of them, 19 are previously unknown and each requires cross-framework knowledge in deep learning for identification.
"
DBLP:journals/pacmse/WangGGFCX25,article,"Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge
in Software Engineering","Ruiqi Wang and
Jiyu Guo and
Cuiyun Gao and
Guodong Fan and
Chun Yong Chong and
Xin Xia",2025,Proc. {ACM} Softw. Eng.,,2,{ISSTA},1955--1977,10.1145/3728963,https://doi.org/10.1145/3728963,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/pacmse/WangGGFCX25.bib,"Thu, 11 Sep 2025 01:00:00 +0200",,,,,,,,,"Recently, large language models (LLMs) have been deployed to tackle various software engineering (SE) tasks like code generation, significantly advancing the automation of SE tasks. However, assessing the quality of these LLM-generated code and text remains challenging. The commonly used Pass@k metric necessitates extensive unit tests and configured environments, demands a high labor cost, and is not suitable for evaluating LLM-generated text. Conventional metrics like BLEU, which measure only lexical rather than semantic similarity, have also come under scrutiny. In response, a new trend has emerged to employ LLMs for automated evaluation, known as LLM-as-a-judge. These LLM-as-a-judge methods are claimed to better mimic human assessment than conventional metrics without relying on high-quality reference answers. Nevertheless, their exact human alignment in SE tasks remains unexplored.
 In this paper, we empirically explore LLM-as-a-judge methods for evaluating SE tasks, focusing on their alignment with human judgments. We select seven LLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs specifically fine-tuned for evaluation. After generating and manually scoring LLM responses on three recent SE datasets of code translation, code generation, and code summarization, we then prompt these methods to evaluate each response. Finally, we compare the scores generated by these methods with human evaluation. The results indicate that output-based methods reach the highest Pearson correlation of 81.32 and 68.51 with human scores in code translation and generation, achieving near-human evaluation, noticeably outperforming ChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such output-based methods prompt LLMs to output judgments directly, and exhibit more balanced score distributions that resemble human score patterns. Finally, we provide insights and implications, concluding that current state-of-the-art LLM-as-a-judge methods can potentially replace human evaluations in certain SE tasks."
DBLP:journals/pacmse/XiaDDZ25,article,Demystifying LLM-Based Software Engineering Agents,"Chunqiu Steven Xia and
Yinlin Deng and
Soren Dunn and
Lingming Zhang",2025,Proc. {ACM} Softw. Eng.,,2,{FSE},801--824,10.1145/3715754,https://doi.org/10.1145/3715754,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/pacmse/XiaDDZ25.bib,"Thu, 11 Sep 2025 01:00:00 +0200",,,,,,,,,"Recent advancements in large language models (LLMs) have significantly advanced the automation of software development tasks, including code synthesis, program repair, and test generation. More recently, researchers and industry practitioners have developed various autonomous LLM agents to perform end-to-end software development tasks. These agents are equipped with the ability to use tools, run commands, observe feedback from the environment, and plan for future actions. However, the complexity of these agent-based approaches, together with the limited abilities of current LLMs, raises the following question: Do we really have to employ complex autonomous software agents? To attempt to answer this question, we build Agentless ‚Äì an agentless approach to automatically resolve software development issues. Compared to the verbose and complex setup of agent-based approaches, Agentless employs a simplistic three-phase process of localization, repair, and patch validation, without letting the LLM decide future actions or operate with complex tools. Our results on the popular SWE-bench Lite benchmark show that surprisingly the simplistic Agentless is able to achieve both the highest performance (32.00%, 96 correct fixes) and low cost ($0.70) compared with all existing open-source software agents at the time of paper submission! Agentless also achieves more than 50% solve rate when using Claude 3.5 Sonnet on the new SWE-bench Verified benchmark. In fact, Agentless has already been adopted by OpenAI as the go-to approach to showcase the real-world coding performance of both GPT-4o and the new o1 models; more recently, Agentless has also been used by DeepSeek to evaluate their newest DeepSeek V3 and R1 models. Furthermore, we manually classified the problems in SWE-bench Lite and found problems with exact ground truth patches or insufficient/misleading issue descriptions. As such, we construct SWE-bench Lite-ùëÜ by excluding such problematic issues to perform more rigorous evaluation and comparison. Our work highlights the currently overlooked potential of a simplistic, cost-effective technique in autonomous software development. We hope Agentless will help reset the baseline, starting point, and horizon for autonomous software agents, and inspire future work along this crucial direction. We have open-sourced Agentless at: https://github.com/OpenAutoCoder/Agentless"
DBLP:journals/pacmse/YuZWNZY25,article,"CXXCrafter: An LLM-Based Agent for Automated {C/C++} Open Source Software
Building","Zhengmin Yu and
Yuan Zhang and
Ming Wen and
Yinan Nie and
Wenhui Zhang and
Min Yang",2025,Proc. {ACM} Softw. Eng.,,2,{FSE},2618--2640,10.1145/3729386,https://doi.org/10.1145/3729386,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/pacmse/YuZWNZY25.bib,"Thu, 11 Sep 2025 01:00:00 +0200",,,,,,,,,"Project building is pivotal to support various program analysis tasks, such as generating intermediate representation code for static analysis and preparing binary code for vulnerability reproduction. However, automating the building process for C/C++ projects is a highly complex endeavor, involving tremendous technical challenges, such as intricate dependency management, diverse build systems, varied toolchains, and multifaceted error handling mechanisms. Consequently, building C/C++ projects often proves to be difficult in practice, hindering the progress of downstream applications. Unfortunately, research on facilitating the building of C/C++ projects remains to be inadequate. The emergence of Large Language Models (LLMs) offers promising solutions to automated software building. Trained on extensive corpora, LLMs can help unify diverse build systems through their comprehension capabilities and address complex errors by leveraging tacit knowledge storage. Moreover, LLM-based agents can be systematically designed to dynamically interact with the environment, effectively managing dynamic building issues. Motivated by these opportunities, we first conduct an empirical study to systematically analyze the current challenges in the C/C++ project building process. Particularly, we observe that most popular C/C++ projects encounter an average of five errors when relying solely on the default build systems. Based on our study, we develop an automated build system called CXXCrafter to specifically address the above-mentioned challenges, such as dependency resolution. Our evaluation on open-source software demonstrates that CXXCrafter achieves a success rate of 78% in project building. Specifically, among the Top100 dataset, 72 projects are built successfully by both CXXCrafter and manual efforts, 3 by CXXCrafter only, and 14 manually only. Despite the slightly lower performance,CXXCrafter can save tremendous manual efforts and can also be easily applied to a wider range of applications automatically."
DBLP:journals/pvldb/ChangG25,article,"SagaLLM: Context Management, Validation, and Transaction Guarantees
for Multi-Agent {LLM} Planning","Edward Y. Chang and
Longling Geng",2025,Proc. {VLDB} Endow.,,18,12,4874--4886,,https://www.vldb.org/pvldb/vol18/p4874-chang.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/pvldb/ChangG25.bib,"Thu, 25 Sep 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/sigops/YangG25,article,"{DREAM:} Distributed Regional Efficient Agent Management with LLMs
for Online Multi-Agent Pathfinding","Rui Yang and
Rajiv Gupta",2025,{ACM} {SIGOPS} Oper. Syst. Rev.,,59,1,24--33,10.1145/3759441.3759446,https://doi.org/10.1145/3759441.3759446,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/sigops/YangG25.bib,"Fri, 05 Sep 2025 01:00:00 +0200",,,,,,,,,"In this paper, we introduce DREAM, Distributed Regional Efficient Agent Management, a novel method using Large Language Models (LLMs) to solve Multi-Agent Pathfinding (MAPF) problems in complicated environments. Our approach splits up the area into various local regions and an LLM agent handles each one of them intelligently in reasoning and decision making. We present some novel designs in our system: 1) Adaptive region management and allocation to regions, supporting the dynamic partitioning of different complexity or density areas. 2) The multi-level LLM-driven agents collaboration framework that enables peer-peer, interLLM coordination and controls for effective monitoring intelligence across a hierarchical path planning organization hierarchy level ensures autonomy whilst improving overall understanding among LLM agents, leading to more accurate planning decisions from real-time analysis. (3) Failurereflection- replanning mechanism integrated within an individual LLM's management scope eventually results continual improvement. (4) LLM agents can do function calling to interact with the typical algorithms also. Our system successfully processes complex and large-scale MAPF scenarios by merging the higher-orderality of reasoning capabilities in LLMs with this novel distributed framework. For instance, the distributed and hierarchical nature of this approach helps to break a high-dimensional MAPF problem into several groups of smaller dimension. As such, this approach also opens up the development of AI language models in more complex robotics and logistics scenarios, potentially changing how multi-agent coordination is done for actual situations."
DBLP:journals/softx/AldanaMartinBM25,article,eidos: {A} modular approach to external function integration in LLMs,"Jos{\'{e}} F. Aldana{-}Mart{\'{\i}}n and
Antonio Ben{\'{\i}}tez{-}Hidalgo and
Jos{\'{e}} Francisco Aldana Montes",2025,SoftwareX,,31,,102290,10.1016/J.SOFTX.2025.102290,https://doi.org/10.1016/j.softx.2025.102290,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/softx/AldanaMartinBM25.bib,"Sun, 07 Sep 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/softx/ForootaniAT25,article,"Bio-Eng-LLM {AI} Assist: {A} modular chatbot platform for interdisciplinary
research and education","Ali Forootani and
Danial Esmaeili Aliabadi and
Daniela Thr{\""{a}}n",2025,SoftwareX,,31,,102260,10.1016/J.SOFTX.2025.102260,https://doi.org/10.1016/j.softx.2025.102260,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/softx/ForootaniAT25.bib,"Tue, 05 Aug 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/softx/PentangeloTLGP25,article,"{SENEM-AI:} Leveraging LLMs for student behavior simulation in virtual
learning environments","Viviana Pentangelo and
Luigi Turco and
Stefano Lambiase and
Carmine Gravino and
Fabio Palomba",2025,SoftwareX,,31,,102278,10.1016/J.SOFTX.2025.102278,https://doi.org/10.1016/j.softx.2025.102278,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/softx/PentangeloTLGP25.bib,"Sun, 07 Sep 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/softx/PeriniAGPC25,article,"BrickLLM: {A} Python library for generating Brick-compliant {RDF}
graphs using LLMs","Marco Perini and
Daniele Antonucci and
Rocco Giudice and
Marco Savino Piscitelli and
Alfonso Capozzoli",2025,SoftwareX,,30,,102121,10.1016/J.SOFTX.2025.102121,https://doi.org/10.1016/j.softx.2025.102121,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/softx/PeriniAGPC25.bib,"Fri, 04 Jul 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/softx/SalimHJBB25,article,"{LLM} based {QA} chatbot builder: {A} generative AI-based chatbot
builder for question answering","Md. Shahidul Salim and
Sk. Imran Hossain and
Tanim Jalal and
Dhiman Kumer Bose and
Mohammad Jahid Ibna Basher",2025,SoftwareX,,29,,102029,10.1016/J.SOFTX.2024.102029,https://doi.org/10.1016/j.softx.2024.102029,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/softx/SalimHJBB25.bib,"Wed, 08 Oct 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/softx/TohmaOGAY25,article,"SmartControl: Interactive {PID} controller design powered by {LLM}
agents and control system expertise","Kadir Tohma and
Halil Ibrahim Okur and
Handan G{\""{u}}rsoy{-}Demir and
Merve Nilay Aydin and
Celaleddin Yeroglu",2025,SoftwareX,,31,,102194,10.1016/J.SOFTX.2025.102194,https://doi.org/10.1016/j.softx.2025.102194,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/softx/TohmaOGAY25.bib,"Tue, 05 Aug 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/stt/HippargiKN25,article,"Evaluating the Capabilities of LLMs in Traceability Maintenance for
Automotive System and Software Requirements: Three Case Studies","Vibhashree Hippargi and
Erik Kamsties and
J{\""{u}}rgen Naumann",2025,Softwaretechnik-Trends,,45,1,48--49,,https://fb-swt.gi.de/fileadmin/FB/SWT/Softwaretechnik-Trends/Verzeichnis/Band\_45\_Heft\_1/Evaluating\_the\_Capabilities\_of\_LLMs\_in\_Traceability\_Maintenance\_for\_Automotive\_System\_and\_Software\_Requirements.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/stt/HippargiKN25.bib,"Mon, 31 Mar 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/tasm/PowersUBCEHPG25,article,What's It Like to Trust an {LLM:} The Devolution of Trust Psychology?,"Simon T. Powers and
Neil Urquhart and
Chloe M. Barnes and
Theodor Cimpeanu and
Anik{\'{o}} Ek{\'{a}}rt and
The Anh Han and
Jeremy Pitt and
Michael Guckert",2025,{IEEE} Technol. Soc. Mag.,,44,3,30--37,10.1109/MTS.2025.3583233,https://doi.org/10.1109/MTS.2025.3583233,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tasm/PowersUBCEHPG25.bib,"Sun, 21 Sep 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/tosem/HeTL25,article,"LLM-Based Multi-Agent Systems for Software Engineering: Literature
Review, Vision, and the Road Ahead","Junda He and
Christoph Treude and
David Lo",2025,{ACM} Trans. Softw. Eng. Methodol.,,34,5,124:1--124:30,10.1145/3712003,https://doi.org/10.1145/3712003,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tosem/HeTL25.bib,"Thu, 11 Sep 2025 01:00:00 +0200",,,,,,,,,"Integrating Large Language Models (LLMs) into autonomous agents marks a significant shift in the research landscape by offering cognitive abilities that are competitive with human planning and reasoning. This article explores the transformative potential of integrating Large Language Models into Multi-Agent (LMA) systems for addressing complex challenges in software engineering (SE). By leveraging the collaborative and specialized abilities of multiple agents, LMA systems enable autonomous problem-solving, improve robustness, and provide scalable solutions for managing the complexity of real-world software projects. In this article, we conduct a systematic review of recent primary studies to map the current landscape of LMA applications across various stages of the software development lifecycle (SDLC). To illustrate current capabilities and limitations, we perform two case studies to demonstrate the effectiveness of state-of-the-art LMA frameworks. Additionally, we identify critical research gaps and propose a comprehensive research agenda focused on enhancing individual agent capabilities and optimizing agent synergy. Our work outlines a forward-looking vision for developing fully autonomous, scalable, and trustworthy LMA systems, laying the foundation for the evolution of Software Engineering 2.0."
DBLP:journals/tse/ChenSYSWCSZ25,article,"NumScout: Unveiling Numerical Defects in Smart Contracts Using LLM-Pruning
Symbolic Execution","Jiachi Chen and
Zhenzhe Shao and
Shuo Yang and
Yiming Shen and
Yanlin Wang and
Ting Chen and
Zhenyu Shan and
Zibin Zheng",2025,{IEEE} Trans. Software Eng.,,51,5,1538--1553,10.1109/TSE.2025.3555622,https://doi.org/10.1109/TSE.2025.3555622,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tse/ChenSYSWCSZ25.bib,"Wed, 11 Jun 2025 01:00:00 +0200",,,,,,,,,"In recent years, the Ethereum platform has witnessed a proliferation of smart contracts, accompanied by exponential growth in total value locked (TVL). High-TVL smart contracts often require complex numerical computations, particularly in mathematical financial models used by many decentralized applications (DApps). Improper calculations can introduce numerical defects, posing potential security risks. Existing research primarily focuses on traditional numerical defects like integer overflow, and there is currently a lack of systematic research and effective detection methods targeting new types of numerical defects. In this paper, we identify five new types of numerical defects through the analysis of 1,199 audit reports by utilizing the open card method. Each defect is defined and illustrated with a code example to highlight its features and potential consequences. We also propose NumScout, a symbolic execution-based tool designed to detect these five defects. Specifically, the tool combines information from source code and bytecode, analyzing key operations such as comparisons and transfers, to effectively locate defects and report them based on predefined detection patterns. Furthermore, NumScout uses a large language model (LLM) to prune functions which are unrelated to numerical operations. This step allows symbolic execution to quickly enter the target function and improve runtime speed by 28.4%. We run NumScout on 6,617 real-world contracts and evaluated its performance based on manually labeled results. We find that 1,774 contracts contained at least one of the five defects, and the tool achieved an overall precision of 89.7%."
DBLP:journals/tse/CrupiTVMPB25,article,On the Effectiveness of LLM-as-a-Judge for Code Generation and Summarization,"Giuseppe Crupi and
Rosalia Tufano and
Alejandro Velasco and
Antonio Mastropaolo and
Denys Poshyvanyk and
Gabriele Bavota",2025,{IEEE} Trans. Software Eng.,,51,8,2329--2345,10.1109/TSE.2025.3586082,https://doi.org/10.1109/TSE.2025.3586082,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tse/CrupiTVMPB25.bib,"Thu, 11 Sep 2025 01:00:00 +0200",,,,,,,,,"Large Language Models (LLMs) have been recently exploited as judges for complex natural language processing tasks, such as Q&A (Question & Answer). The basic idea is to delegate to an LLM the assessment of the ‚Äúquality‚Äù of the output provided by an automated technique (often another LLM) for tasks for which: (i) quantitative metrics would only tell part of the story, and; (ii) a large-scale human-based evaluation would be too expensive. LLMs-as-a-judge, if proven effective for a specific task, can also unlock new possibilities for automation, with several LLMs proposing a solution for a given instance of the task (<italic>e.g.,</italic> an answer to a question) and others judging and deciding what is the best output to show the user. We study the effectiveness of LLMs-as-a-judge for two code-related tasks, namely <italic>code generation</italic> and <italic>code summarization</italic>. The rationale for choosing these tasks is two-fold. First, quantitative metrics are usually not enough for the assessment of code summarizers/generators. For example, it is well documented that metrics such as BLEU are quite weak proxies for the quality of the generated summaries. Second, even state-of-the-art techniques still struggle with handling complex instances of these tasks (<italic>e.g.,</italic> summarizing a quite long / complex function), making them good candidates for benefiting from more advanced solutions envisioning collaboration among LLMs. For <italic>code generation</italic>, we check whether eight LLMs are able to judge the correctness of 1,405 Java methods and 1,281 Python functions generated by the same LLMs or implemented by humans. For <italic>code summarization</italic>, we compare the judgment of five LLMs to those provided by ninehumans for <inline-formula><tex-math notation=""LaTeX"">$\sim$</tex-math><alternatives><mml:math><mml:mo>‚àº</mml:mo></mml:math><graphic position=""float"" orientation=""portrait"" xlink:href=""bavota-ieq1-3586082.gif""/></alternatives></inline-formula> 1.2k summaries, related to both Java and Python functions. Our findings show that GPT-4-turbo is the best LLM in terms of judging capabilities for both tasks, with ‚Äúsmaller‚Äù LLMs featuring tens of billions parameters not being able to cope with judging tasks. However, even the best-performing LLM frequently misjudges the correctness of the code and summary quality."
DBLP:journals/tse/DengTYZZZ25,article,"{TARGET:} Traffic Rule-Based Test Generation for Autonomous Driving
via Validated LLM-Guided Knowledge Extraction","Yao Deng and
Zhi Tu and
Jiaohong Yao and
Mengshi Zhang and
Tianyi Zhang and
James Xi Zheng",2025,{IEEE} Trans. Software Eng.,,51,7,1950--1968,10.1109/TSE.2025.3569086,https://doi.org/10.1109/TSE.2025.3569086,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tse/DengTYZZZ25.bib,"Sat, 09 Aug 2025 01:00:00 +0200",,,,,,,,,"Recent incidents with autonomous vehicles highlight the need for rigorous testing to ensure safety and robustness. Constructing test scenarios for autonomous driving systems (ADSs), however, is labor-intensive. We propose TARGET, an end-to-end framework that automatically generates test scenarios from traffic rules. To address complexity, we leverage a Large Language Model (LLM) to extract knowledge from traffic rules. To mitigate hallucinations caused by large context during input processing, we introduce a domain-specific language (DSL) designed to be syntactically simple and compositional. This design allows the LLM to learn and generate test scenarios in a modular manner while enabling syntactic and semantic validation for each component. Based on these validated representations, TARGET synthesizes executable scripts to render scenarios in simulation. Evaluated seven ADSs with 284 scenarios derived from 54 traffic rules, TARGET uncovered 610 rule violations, collisions, and other issues. For each violation, TARGET generates scenario recordings and detailed logs, aiding root cause analysis. Two identified issues were confirmed by ADS developers: one linked to an existing bug report and the other to limited ADS functionality."
DBLP:journals/tse/FerragBTJMALTMDC25,article,"SecureFalcon: Are We There Yet in Automated Software Vulnerability
Detection With LLMs?","Mohamed Amine Ferrag and
Ammar Battah and
Norbert Tihanyi and
Ridhi Jain and
Diana Maimut and
Fatima Alwahedi and
Thierry Lestable and
Narinderjit Singh Thandi and
Abdechakour Mechri and
M{\'{e}}rouane Debbah and
Lucas C. Cordeiro",2025,{IEEE} Trans. Software Eng.,,51,4,1248--1265,10.1109/TSE.2025.3548168,https://doi.org/10.1109/TSE.2025.3548168,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tse/FerragBTJMALTMDC25.bib,"Fri, 09 May 2025 01:00:00 +0200",,,,,,,,,"Software vulnerabilities can cause numerous problems, including crashes, data loss, and security breaches. These issues greatly compromise quality and can negatively impact the market adoption of software applications and systems. Traditional bug-fixing methods, such as static analysis, often produce false positives. While bounded model checking, a form of Formal Verification (FV), can provide more accurate outcomes compared to static analyzers, it demands substantial resources and significantly hinders developer productivity. Can Machine Learning (ML) achieve accuracy comparable to FV methods and be used in popular instant code completion frameworks in near real-time? In this paper, we introduce SecureFalcon, an innovative model architecture with only 121 million parameters derived from the Falcon-40B model and explicitly tailored for classifying software vulnerabilities. To achieve the best performance, we trained our model using two datasets, namely the FormAI dataset and the FalconVulnDB. The FalconVulnDB is a combination of recent public datasets, namely the SySeVR framework, Draper VDISC, Bigvul, Diversevul, SARD Juliet, and ReVeal datasets. These datasets contain the top 25 most dangerous software weaknesses, such as CWE-119, CWE-120, CWE-476, CWE-122, CWE-190, CWE-121, CWE-78, CWE-787, CWE-20, and CWE-762. SecureFalcon achieves 94% accuracy in binary classification and up to 92% in multiclassification, with instant CPU inference times. It outperforms existing models such as BERT, RoBERTa, CodeBERT, and traditional ML algorithms, promising to push the boundaries of software vulnerability detection and instant code completion frameworks."
DBLP:journals/tse/HayetSd25,article,ChatAssert: LLM-Based Test Oracle Generation With External Tools Assistance,"Ishrak Hayet and
Adam Scott and
Marcelo d'Amorim",2025,{IEEE} Trans. Software Eng.,,51,1,305--319,10.1109/TSE.2024.3519159,https://doi.org/10.1109/TSE.2024.3519159,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tse/HayetSd25.bib,"Mon, 03 Mar 2025 00:00:00 +0100",,,,,,,,,"Test oracle generation is an important and challenging problem. Neural-based solutions have been recently proposed for oracle generation but they are still inaccurate. For example, the accuracy of the state-of-the-art technique teco is only 27.5% on its dataset including 3,540 test cases. We propose ChatAssert, a prompt engineering framework designed for oracle generation that uses dynamic and static information to iteratively refine prompts for querying large language models (LLMs). ChatAssert uses code summaries and examples to assist an LLM in generating candidate test oracles, uses a lightweight static analysis to assist the LLM in repairing generated oracles that fail to compile, and uses dynamic information obtained from test runs to help the LLM in repairing oracles that compile but do not pass. Experimental results using an independent publicly-available dataset show that ChatAssert improves the state-of-the-art technique, teco, on key evaluation metrics. For example, it improves Acc@1 by 15%. Overall, results provide initial yet strong evidence that using external tools in the formulation of prompts is an important aid in LLM-based oracle generation."
DBLP:journals/tse/LiSYLLZL25,article,"Improving Co-Decoding Based Security Hardening of Code LLMs Leveraging
Knowledge Distillation","Dong Li and
Shanfu Shu and
Meng Yan and
Zhongxin Liu and
Chao Liu and
Xiaohong Zhang and
David Lo",2025,{IEEE} Trans. Software Eng.,,51,9,2634--2650,10.1109/TSE.2025.3591791,https://doi.org/10.1109/TSE.2025.3591791,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tse/LiSYLLZL25.bib,"Mon, 06 Oct 2025 01:00:00 +0200",,,,,,,,,"Large Language Models (LLMs) have been widely adopted by developers in software development. However, the massive pretraining code data is not rigorously filtered, allowing LLMs to learn unsafe coding patterns. Several prior studies have demonstrated that code LLMs tend to generate code with potential vulnerabilities. The widespread adoption of intelligent programming assistants poses a significant threat to the software development process. Existing approaches to mitigating this risk primarily involve constructing secure data that are free of vulnerabilities and then retraining or fine-tuning the models. However, such an effort is resource intensive and requires significant manual supervision. When the model parameters are too large (e.g., more than 1 billion) or multiple models with the same parameter scale have the same optimization needs (e.g., to avoid outputting vulnerable code), the above work will become unaffordable. To address this challenge, in previous work, we proposed CoSec, an approach to improve the security of code LLMs with different parameters by utilizing an independent and very small parametric security model as a decoding navigator. Despite CoSec‚Äôs excellent performance, we found that there is still room for improving: 1) its ability to maintain the functional correctness of hardened targets, and 2) the security of the generated code. To address the above issues, we propose CoSec+, a hardening framework consisting of three phases: 1) Functional Correctness Alignment, which improves the functional correctness of the security base with knowledge disstillation; 2) Security Training, which yields an independent, but much smaller security model; and 3) Co-decoding, where the security model iteratively reasons about the next token along with the target model. Due to the higher confidence that a well-trained security model places in secure and correct tokens, it guides the target base model to generate more secure code, even as it improves the functional correctness of the target base model. We have conducted extensive experiments in several code LLMs (i.e., CodeGen, StarCoderBase, DeepSeekCoder and Qwen2.5-Coder), and the results show that our approach is effective in improving the functional correctness and security of the models. The evaluation results show that CoSec+ can deliver a 0.8% to 37.7% improvement in security across models of various parameter sizes and families; moreover, it preserves the functional correctness of the target base models‚Äîachieving functional-correctness gains of 0.7% to 51.1% for most of those models."
DBLP:journals/tse/QinWLDWLM25,article,"SoapFL: {A} Standard Operating Procedure for LLM-Based Method-Level
Fault Localization","Yihao Qin and
Shangwen Wang and
Yiling Lou and
Jinhao Dong and
Kaixin Wang and
Xiaoling Li and
Xiaoguang Mao",2025,{IEEE} Trans. Software Eng.,,51,4,1173--1187,10.1109/TSE.2025.3543187,https://doi.org/10.1109/TSE.2025.3543187,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tse/QinWLDWLM25.bib,"Tue, 13 May 2025 01:00:00 +0200",,,,,,,,,"Fault Localization (FL) is an essential step during the debugging process. With the strong capabilities of code comprehension, the recent Large Language Models (LLMs) have demonstrated promising performance in diagnosing bugs in the code. Nevertheless, due to LLMs‚Äô limited performance in handling long contexts, existing LLM-based fault localization remains on localizing bugs within a small code scope (i.e., a method or a class), which struggles to diagnose bugs for a large code scope (i.e., an entire software system). To address the limitation, this paper presents SoapFL, which builds an LLM-driven standard operating procedure (SOP) to automatically localize buggy methods from the entire software. By simulating the behavior of a human developer, SoapFL models the FL task as a three-step process, which involves comprehension, navigation, and confirmation. Within specific steps, SoapFL provides useful test behavior or coverage information to LLM through program analysis. Particularly, we adopt a series of auxiliary strategies such as Test Behavior Tracking, Document-Guided Search, and Multi-Round Dialogue to overcome the challenges in each step. The evaluation on the widely used Defects4J-V1.2.0 benchmark shows that SoapFL can localize 175 out of 395 bugs within Top-1, which outperforms the other LLM-based approaches and exhibits complementarity to the state-of-the-art learning-based techniques. Additionally, we confirm the indispensability of the components in SoapFL with the ablation study and demonstrate the usability of SoapFL through a user study. Finally, the cost analysis shows that SoapFL spends an average of only 0.081 dollars and 92 seconds for a single bug."
DBLP:journals/tse/TipBS25,article,LLMorpheus: Mutation Testing Using Large Language Models,"Frank Tip and
Jonathan Bell and
Max Sch{\""{a}}fer",2025,{IEEE} Trans. Software Eng.,,51,6,1645--1665,10.1109/TSE.2025.3562025,https://doi.org/10.1109/TSE.2025.3562025,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tse/TipBS25.bib,"Sun, 06 Jul 2025 01:00:00 +0200",,,,,,,,,"In mutation testing, the quality of a test suite is evaluated by introducing faults into a program and determining whether the program‚Äôs tests detect them. Most existing approaches for mutation testing involve the application of a fixed set of mutation operators, e.g., replacing a ‚Äú+‚Äù with a ‚Äú-‚Äù, or removing a function‚Äôs body. However, certain types of real-world bugs cannot easily be simulated by such approaches, limiting their effectiveness. This paper presents a technique for mutation testing where placeholders are introduced at designated locations in a program‚Äôs source code and where a Large Language Model (LLM) is prompted to ask what they could be replaced with. The technique is implemented in LLMorpheus, a mutation testing tool for JavaScript, and evaluated on 13 subject packages, considering several variations on the prompting strategy, and using several LLMs. We find LLMorpheus to be capable of producing mutants that resemble existing bugs that cannot be produced by StrykerJS, a state-of-the-art mutation testing tool. Moreover, we report on the running time, cost, and number of mutants produced by LLMorpheus, demonstrating its practicality."
DBLP:journals/tse/VitoMFGP25,article,"LLM-Based Automation of {COSMIC} Functional Size Measurement From
Use Cases","Gabriele De Vito and
Sergio Di Martino and
Filomena Ferrucci and
Carmine Gravino and
Fabio Palomba",2025,{IEEE} Trans. Software Eng.,,51,5,1500--1523,10.1109/TSE.2025.3554562,https://doi.org/10.1109/TSE.2025.3554562,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tse/VitoMFGP25.bib,"Wed, 11 Jun 2025 01:00:00 +0200",,,,,,,,,"COmmon Software Measurement International Consortium (COSMIC) Functional Size Measurement is a method widely used in the software industry to quantify user functionality and measure software size, which is crucial for estimating development effort, cost, and resource allocation. COSMIC measurement is a manual task that requires qualified professionals and effort. To support professionals in COSMIC measurement, we propose an automatic approach, CosMet, that leverages Large Language Models to measure software size starting from use cases specified in natural language. To evaluate the proposed approach, we developed a web tool that implements CosMet using GPT-4 and conducted two studies to assess the approach quantitatively and qualitatively. Initially, we experimented with CosMet on seven software systems, encompassing 123 use cases, and compared the generated results with the ground truth created by two certified professionals. Then, seven professional measurers evaluated the analysis achieved by CosMet and the extent to which the approach reduces the measurement time. The first study's results revealed that CosMet is highly effective in analyzing and measuring use cases. The second study highlighted that CosMet offers a transparent and interpretable analysis, allowing practitioners to understand how the measurement is derived and make necessary adjustments. Additionally, it reduces the manual measurement time by 60-80%."
DBLP:journals/tse/ZhangLSWLTL25,article,"ACFix: Guiding LLMs With Mined Common {RBAC} Practices for Context-Aware
Repair of Access Control Vulnerabilities in Smart Contracts","Lyuye Zhang and
Kaixuan Li and
Kairan Sun and
Daoyuan Wu and
Ye Liu and
Haoye Tian and
Yang Liu",2025,{IEEE} Trans. Software Eng.,,51,9,2512--2532,10.1109/TSE.2025.3590108,https://doi.org/10.1109/TSE.2025.3590108,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tse/ZhangLSWLTL25.bib,"Thu, 02 Oct 2025 01:00:00 +0200",,,,,,,,,"Smart contracts are susceptible to various security issues, among which access control (AC) vulnerabilities are particularly critical. While existing research has proposed multiple detection tools, automatic and appropriate repair of AC vulnerabilities in smart contracts remains a challenge. Unlike commonly supported vulnerability types by existing repair tools, such as reentrancy, which are usually fixed by template-based approaches, the main obstacle of repairing AC vulnerabilities lies in identifying the appropriate roles or permissions amid a long list of non-AC-related source code to generate proper patch code, a task that demands human-level intelligence. In this paper, we employ the state-of-the-art GPT-4 model and enhance it with a novel approach called ACFix. The key insight is that we can mine common AC practices for major categories of code functionality and use them to guide LLMs in fixing code with similar functionality. To this end, ACFix involves offline and online phases. In the offline phase, ACFix mines a taxonomy of common Role-based Access Control practices from 344,251 on-chain contracts, categorizing 49 role-permission pairs from the top 1,000 unique samples. In the online phase, ACFix tracks AC-related elements across the contract and uses this context information along with a Chain-of-Thought pipeline to guide LLMs in identifying the most appropriate role-permission pair for the subject contract and subsequently generating a suitable patch. To evaluate ACFix, we built the first benchmark dataset of 118 real-world AC vulnerabilities, and our evaluation revealed that ACFix successfully repaired 94.92% of them, a major improvement compared to the baseline GPT-4 at only 52.54%. We also conducted a human study to understand the value of ACFix‚Äôs repairs and their differences from human repairs."
DBLP:journals/tvcg/LiKLCLZLQL25,article,"Save It for the ""Hot"" Day: An LLM-Empowered Visual Analytics System
for Heat Risk Management","Haobo Li and
Wong Kam{-}Kwai and
Yan Luo and
Juntong Chen and
Chengzhong Liu and
Yaxuan Zhang and
Alexis Kai{-}Hon Lau and
Huamin Qu and
Dongyu Liu",2025,{IEEE} Trans. Vis. Comput. Graph.,,31,10,8928--8943,10.1109/TVCG.2025.3586689,https://doi.org/10.1109/TVCG.2025.3586689,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tvcg/LiKLCLZLQL25.bib,"Fri, 26 Sep 2025 01:00:00 +0200",,,,,,,,,"The escalating frequency and intensity of heat-related climate events, particularly heatwaves, emphasize the pressing need for advanced heat risk management strategies. Current approaches, primarily relying on numerical models, face challenges in spatial-temporal resolution and in capturing the dynamic interplay of environmental, social, and behavioral factors affecting heat risks. This has led to difficulties in translating risk assessments into effective mitigation actions. Recognizing these problems, we introduce a novel approach leveraging the burgeoning capabilities of Large Language Models (LLMs) to extract rich and contextual insights from news reports. We hence propose an LLM-empowered visual analytics system, Havior, that integrates the precise, data-driven insights of numerical models with nuanced news report information. This hybrid approach enables a more comprehensive assessment of heat risks and better identification, assessment, and mitigation of heat-related threats. The system incorporates novel visualization designs, such as ‚Äúthermoglyph‚Äù and news glyph, enhancing intuitive understanding and analysis of heat risks. The integration of LLM-based techniques also enables advanced information retrieval and semantic knowledge extraction that can be guided by experts‚Äô analytics needs. We conducted an experiment on information extraction, a case study on the 2022 China Heatwave, and an expert survey & interview collaborated with six domain experts, demonstrating the usefulness of our system in providing in-depth and actionable insights for heat risk management."
DBLP:journals/tvcg/WengWLFLFHC25,article,"InsightLens: Augmenting LLM-Powered Data Analysis With Interactive
Insight Management and Navigation","Luoxuan Weng and
Xingbo Wang and
Junyu Lu and
Yingchaojie Feng and
Yihan Liu and
Haozhe Feng and
Danqing Huang and
Wei Chen",2025,{IEEE} Trans. Vis. Comput. Graph.,,31,6,3719--3732,10.1109/TVCG.2025.3567131,https://doi.org/10.1109/TVCG.2025.3567131,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tvcg/WengWLFLFHC25.bib,"Sun, 06 Jul 2025 01:00:00 +0200",,,,,,,,,"The proliferation of large language models (LLMs) has revolutionized the capabilities of natural language interfaces (NLIs) for data analysis. LLMs can perform multi-step and complex reasoning to generate data insights based on users‚Äô analytic intents. However, these insights often entangle with an abundance of contexts in analytic conversations such as code, visualizations, and natural language explanations. This hinders efficient recording, organization, and navigation of insights within the current chat-based LLM interfaces. In this paper, we first conduct a formative study with eight data analysts to understand their general workflow and pain points of insight management during LLM-powered data analysis. Accordingly, we introduce InsightLens, an interactive system to overcome such challenges. Built upon an LLM-agent-based framework that automates insight recording and organization along with the analysis process, InsightLens visualizes the complex conversational contexts from multiple aspects to facilitate insight navigation. A user study with twelve data analysts demonstrates the effectiveness of InsightLens, showing that it significantly reduces users‚Äô manual and cognitive effort without disrupting their conversational data analysis workflow, leading to a more efficient analysis experience."
DBLP:journals/ws/EricksonSPMM25,article,"{LLM} experimentation through knowledge graphs: Towards improved management,
repeatability, and verification","John S. Erickson and
Henrique Santos and
Vl{\'{a}}dia Pinheiro and
Jamie P. McCusker and
Deborah L. McGuinness",2025,J. Web Semant.,,85,,100853,10.1016/J.WEBSEM.2024.100853,https://doi.org/10.1016/j.websem.2024.100853,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/ws/EricksonSPMM25.bib,"Sat, 25 Jan 2025 00:00:00 +0100",,,,,,,,,
DBLP:conf/IEEEscc/TamilselvamS25,inproceedings,"Agentic Multi-Modal LLMs for Software Comprehension: Structuring Code
Summarization with Business Process Awareness","Srikanth Tamilselvam and
Ashita Saxena",2025,,"{IEEE} International Conference on Software Services Engineering,
{SSE} 2025, Helsinki, Finland, July 7-12, 2025",,,128--131,10.1109/SSE67621.2025.00024,https://doi.org/10.1109/SSE67621.2025.00024,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/IEEEscc/TamilselvamS25.bib,"Wed, 10 Sep 2025 14:09:49 +0200",{IEEE},"Rong N. Chang and
Carl K. Chang and
Jingwei Yang and
Nimanthi Atukorala and
Dan Chen and
Sumi Helal and
Sasu Tarkoma and
Qiang He and
Tevfik Kosar and
Claudio A. Ardagna and
Javier Berrocal and
Kaoutar El Maghaouri and
Yanchun Sun",,,,,,,"Application Understanding task aims to help users comprehend an application's capabilities by systematically analyzing its artifacts. Ideally, such summaries should align with how the application is used in practice, highlighting essential workflows and functional modules in a structured manner. However, existing automated approaches often fall short of this expectation. Lack of application-specific background and domain knowledge limits the system's ability to present functionalities meaningfully. To address these challenges, we propose a novel agentic approach leveraging multimodal LLMs that integrate code analysis, textual artifacts, and domain knowledge to identify key business flow entities-such as programs and tables-within a repository and infer application workflows. This work opens new avenues in LLM-guided software comprehension, bridging the gap between code-centric insights and high-level business process understanding."
DBLP:conf/aaai/AggarwalCDSMKMB25,inproceedings,"ScriptSmith: {A} Unified {LLM} Framework for Enhancing {IT} Operations
via Automated Bash Script Generation, Assessment, and Refinement","Pooja Aggarwal and
Oishik Chatterjee and
Ting Dai and
Suranjana Samanta and
Prateeti Mohapatra and
Debanjana Kar and
Ruchi Mahindru and
Steve Barbieri and
Eugen Postea and
Brad Blancett and
Arthur De Magalhaes",2025,,"AAAI-25, Sponsored by the Association for the Advancement of Artificial
Intelligence, February 25 - March 4, 2025, Philadelphia, PA, {USA}",,,28829--28835,10.1609/AAAI.V39I28.35147,https://doi.org/10.1609/aaai.v39i28.35147,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/aaai/AggarwalCDSMKMB25.bib,"Thu, 17 Apr 2025 01:00:00 +0200",{AAAI} Press,"Toby Walsh and
Julie Shah and
Zico Kolter",,,,,,,"In the rapidly evolving landscape of site reliability engineering (SRE), the demand for efficient and effective solutions to manage and resolve issues in site and cloud applications is paramount. This paper presents an innovative approach to action automation using large language models (LLMs) for script generation, assessment, and refinement. By leveraging the capabilities of LLMs, we aim to significantly reduce the human effort involved in writing and debugging scripts, thereby enhancing the productivity of SRE teams. Our experiments focus on Bash scripts, a commonly used tool in SRE, and involve the CodeSift dataset of 100 tasks and the InterCode dataset of 153 tasks. The results show that LLMs can automatically assess and refine scripts efficiently, reducing the need for script validation in an execution environment. Results demonstrate that the framework shows an overall improvement of 7-10% in script generation."
DBLP:conf/aaai/ChenWGLG25,inproceedings,"Practical Offloading for Fine-Tuning {LLM} on Commodity {GPU} via
Learned Sparse Projectors","Siyuan Chen and
Zhuofeng Wang and
Zelong Guan and
Yudong Liu and
Phillip B. Gibbons",2025,,"AAAI-25, Sponsored by the Association for the Advancement of Artificial
Intelligence, February 25 - March 4, 2025, Philadelphia, PA, {USA}",,,23614--23622,10.1609/AAAI.V39I22.34531,https://doi.org/10.1609/aaai.v39i22.34531,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/aaai/ChenWGLG25.bib,"Mon, 21 Jul 2025 01:00:00 +0200",{AAAI} Press,"Toby Walsh and
Julie Shah and
Zico Kolter",,,,,,,"Fine-tuning large language models (LLMs) requires significant memory, often exceeding the capacity of a single GPU. A common solution to this memory challenge is offloading compute and data from the GPU to the CPU. However, this approach is hampered by the limited bandwidth of commodity hardware, which constrains communication between the CPU and GPU, and by slower matrix multiplications on the CPU.

In this paper, we present an offloading framework, LSP-Offload, that enables near-native speed LLM fine-tuning on commodity hardware through learned sparse projectors. Our data-driven approach involves learning efficient sparse compressors that minimize communication with minimal precision loss. Additionally, we introduce a novel layer-wise communication schedule to maximize parallelism between communication and computation. As a result, our framework can fine-tune a 1.3 billion parameter model on a 4GB laptop GPU and a 6.7 billion parameter model on an NVIDIA RTX 4090 GPU with 24GB memory. Compared to state-of-the-art offloading frameworks, our approach reduces end-to-end fine-tuning time by 33.1%-62.5% when converging to the same accuracy."
DBLP:conf/aaai/HuangSZRK25,inproceedings,"Gradient Weight-normalized Low-rank Projection for Efficient {LLM}
Training","Jia{-}Hong Huang and
Yixian Shen and
Hongyi Zhu and
Stevan Rudinac and
Evangelos Kanoulas",2025,,"AAAI-25, Sponsored by the Association for the Advancement of Artificial
Intelligence, February 25 - March 4, 2025, Philadelphia, PA, {USA}",,,24123--24131,10.1609/AAAI.V39I23.34587,https://doi.org/10.1609/aaai.v39i23.34587,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/aaai/HuangSZRK25.bib,"Sat, 31 May 2025 01:00:00 +0200",{AAAI} Press,"Toby Walsh and
Julie Shah and
Zico Kolter",,,,,,,"Large Language Models (LLMs) have shown remarkable performance across various tasks, but the escalating demands on computational resources pose significant challenges, particularly in the extensive utilization of full fine-tuning for downstream tasks. To address this, parameter-efficient fine-tuning (PEFT) methods have been developed, but they often underperform compared to full fine-tuning and struggle with memory efficiency. In this work, we introduce Gradient Weight-Normalized Low-Rank Projection (GradNormLoRP), a novel approach that enhances both parameter and memory efficiency while maintaining comparable performance to full fine-tuning. GradNormLoRP normalizes the weight matrix to improve gradient conditioning, facilitating better convergence during optimization. Additionally, it applies low-rank approximations to the weight and gradient matrices, significantly reducing memory usage during training. Extensive experiments demonstrate that our 8-bit GradNormLoRP reduces optimizer memory usage by up to 89.5\% and enables the pre-training of large LLMs, such as LLaMA 7B, on consumer-level GPUs like the NVIDIA RTX 4090, without additional inference costs. Moreover, GradNormLoRP outperforms existing low-rank methods in fine-tuning tasks. For instance, when fine-tuning the RoBERTa model on all GLUE tasks with a rank of 8, GradNormLoRP achieves an average score of 80.65, surpassing LoRA's score of 79.23. These results underscore GradNormLoRP as a promising alternative for efficient LLM pre-training and fine-tuning."
DBLP:conf/aaai/Yang25,inproceedings,"Advancing Intelligent Software Development and Trustworthy Models
Through the Synergy of Software Engineering and LLMs",Guanqun Yang,2025,,"AAAI-25, Sponsored by the Association for the Advancement of Artificial
Intelligence, February 25 - March 4, 2025, Philadelphia, PA, {USA}",,,29313--29314,10.1609/AAAI.V39I28.35234,https://doi.org/10.1609/aaai.v39i28.35234,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/aaai/Yang25.bib,"Thu, 17 Apr 2025 01:00:00 +0200",{AAAI} Press,"Toby Walsh and
Julie Shah and
Zico Kolter",,,,,,,"Integrating Large Language Models (LLMs) into software engineering unlocks new opportunities to automate manual processes but raises challenges around reliability, safety, and scalability. My research centers on this synergy, with two key objectives: first, harnessing LLMs to solve software engineering tasks traditionally dependent on labor-intensive, domain-specific methods, and second, applying robust software engineering principles to improve LLM safety and performance. This dual focus creates a powerful feedback loop, where LLMs drive innovation while engineering rigor ensures these systems meet the high standards required for real-world applications."
DBLP:conf/acl/CocchieriRITM25,inproceedings,"""What do you call a dog that is incontrovertibly true? Dogma"": Testing
{LLM} Generalization through Humor","Alessio Cocchieri and
Luca Ragazzi and
Paolo Italiani and
Giuseppe Tagliavini and
Gianluca Moro",2025,,"Proceedings of the 63rd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), {ACL} 2025, Vienna, Austria,
July 27 - August 1, 2025",,,22922--22937,,https://aclanthology.org/2025.acl-long.1117/,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/acl/CocchieriRITM25.bib,"Wed, 24 Sep 2025 15:22:07 +0200",Association for Computational Linguistics,"Wanxiang Che and
Joyce Nabende and
Ekaterina Shutova and
Mohammad Taher Pilehvar",,,,,,,
DBLP:conf/acl/HabbaAIPBCSS25,inproceedings,"{DOVE:} {A} Large-Scale Multi-Dimensional Predictions Dataset Towards
Meaningful {LLM} Evaluation","Eliya Habba and
Ofir Arviv and
Itay Itzhak and
Yotam Perlitz and
Elron Bandel and
Leshem Choshen and
Michal Shmueli{-}Scheuer and
Gabriel Stanovsky",2025,,"Findings of the Association for Computational Linguistics, {ACL} 2025,
Vienna, Austria, July 27 - August 1, 2025",,,11744--11763,,https://aclanthology.org/2025.findings-acl.611/,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/acl/HabbaAIPBCSS25.bib,"Mon, 28 Jul 2025 15:13:54 +0200",Association for Computational Linguistics,"Wanxiang Che and
Joyce Nabende and
Ekaterina Shutova and
Mohammad Taher Pilehvar",,,,,,,
DBLP:conf/acl/LiangZ0LLXCZZZC25,inproceedings,Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?,"Qingyuan Liang and
Zhao Zhang and
Zeyu Sun and
Zheng Lin and
Qi Luo and
Yueyi Xiao and
Yizhou Chen and
Yuqun Zhang and
Haotian Zhang and
Lu Zhang and
Chenbin Chenbin and
Yingfei Xiong",2025,,"Findings of the Association for Computational Linguistics, {ACL} 2025,
Vienna, Austria, July 27 - August 1, 2025",,,15640--15653,,https://aclanthology.org/2025.findings-acl.807/,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/acl/LiangZ0LLXCZZZC25.bib,"Tue, 29 Jul 2025 01:00:00 +0200",Association for Computational Linguistics,"Wanxiang Che and
Joyce Nabende and
Ekaterina Shutova and
Mohammad Taher Pilehvar",,,,,,,
DBLP:conf/acl/MomenSM25,inproceedings,"Filling the Temporal Void: Recovering Missing Publication Years in
the Project Gutenberg Corpus Using LLMs","Omar Momen and
Manuel Schaaf and
Alexander Mehler",2025,,"Findings of the Association for Computational Linguistics, {ACL} 2025,
Vienna, Austria, July 27 - August 1, 2025",,,17318--17334,,https://aclanthology.org/2025.findings-acl.890/,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/acl/MomenSM25.bib,"Mon, 28 Jul 2025 01:00:00 +0200",Association for Computational Linguistics,"Wanxiang Che and
Joyce Nabende and
Ekaterina Shutova and
Mohammad Taher Pilehvar",,,,,,,
DBLP:conf/acl/SheffieldMPDML25,inproceedings,"Is It {JUST} Semantics? {A} Case Study of Discourse Particle Understanding
in LLMs","William Berkeley Sheffield and
Kanishka Misra and
Valentina Pyatkin and
Ashwini Deo and
Kyle Mahowald and
Junyi Jessy Li",2025,,"Findings of the Association for Computational Linguistics, {ACL} 2025,
Vienna, Austria, July 27 - August 1, 2025",,,21704--21715,,https://aclanthology.org/2025.findings-acl.1117/,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/acl/SheffieldMPDML25.bib,"Mon, 28 Jul 2025 01:00:00 +0200",Association for Computational Linguistics,"Wanxiang Che and
Joyce Nabende and
Ekaterina Shutova and
Mohammad Taher Pilehvar",,,,,,,
DBLP:conf/acl/TraininA25,inproceedings,"T{\unicode{8309}}Score: {A} Methodology for Automatically Assessing
the Quality of {LLM} Generated Multi-Document Topic Sets","Itamar Trainin and
Omri Abend",2025,,"Findings of the Association for Computational Linguistics, {ACL} 2025,
Vienna, Austria, July 27 - August 1, 2025",,,26347--26375,,https://aclanthology.org/2025.findings-acl.1351/,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/acl/TraininA25.bib,"Mon, 28 Jul 2025 01:00:00 +0200",Association for Computational Linguistics,"Wanxiang Che and
Joyce Nabende and
Ekaterina Shutova and
Mohammad Taher Pilehvar",,,,,,,
DBLP:conf/acl/WangLSJ25,inproceedings,Continual Gradient Low-Rank Projection Fine-Tuning for LLMs,"Chenxu Wang and
Yilin Lyu and
Zicheng Sun and
Liping Jing",2025,,"Proceedings of the 63rd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), {ACL} 2025, Vienna, Austria,
July 27 - August 1, 2025",,,14815--14829,,https://aclanthology.org/2025.acl-long.721/,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/acl/WangLSJ25.bib,"Thu, 24 Jul 2025 01:00:00 +0200",Association for Computational Linguistics,"Wanxiang Che and
Joyce Nabende and
Ekaterina Shutova and
Mohammad Taher Pilehvar",,,,,,,
DBLP:conf/acl/ZengLYSL25,inproceedings,It's Not Bragging If You Can Back It Up: Can LLMs Understand Braggings?,"Jingjie Zeng and
Huayang Li and
Liang Yang and
Yuanyuan Sun and
Hongfei Lin",2025,,"Proceedings of the 63rd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), {ACL} 2025, Vienna, Austria,
July 27 - August 1, 2025",,,17542--17560,,https://aclanthology.org/2025.acl-long.858/,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/acl/ZengLYSL25.bib,"Thu, 24 Jul 2025 01:00:00 +0200",Association for Computational Linguistics,"Wanxiang Che and
Joyce Nabende and
Ekaterina Shutova and
Mohammad Taher Pilehvar",,,,,,,
DBLP:conf/aied/GajewskaWBC25,inproceedings,"Leveraging a Multi-agent LLM-Based System to Educate Teachers in Hate
Incidents Management","Ewelina Gajewska and
Michal Wawer and
Katarzyna Budzynska and
Jaroslaw A. Chudziak",2025,,"Artificial Intelligence in Education - 26th International Conference,
{AIED} 2025, Palermo, Italy, July 22-26, 2025, Proceedings, Part {V}",15881,,332--339,10.1007/978-3-031-98462-4\_42,https://doi.org/10.1007/978-3-031-98462-4\_42,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/aied/GajewskaWBC25.bib,"Tue, 05 Aug 2025 01:00:00 +0200",Springer,"Alexandra I. Cristea and
Erin Walker and
Yu Lu and
Olga C. Santos and
Seiji Isotani",Lecture Notes in Computer Science,,,,,,
DBLP:conf/asplos/JeongA25,inproceedings,"Accelerating {LLM} Serving for Multi-turn Dialogues with Efficient
Resource Management","Jinwoo Jeong and
Jeongseob Ahn",2025,,"Proceedings of the 30th {ACM} International Conference on Architectural
Support for Programming Languages and Operating Systems, Volume 2,
{ASPLOS} 2025, Rotterdam, Netherlands, 30 March 2025 - 3 April 2025",,,1--15,10.1145/3676641.3716245,https://doi.org/10.1145/3676641.3716245,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/asplos/JeongA25.bib,"Thu, 01 May 2025 01:00:00 +0200",{ACM},"Lieven Eeckhout and
Georgios Smaragdakis and
Katai Liang and
Adrian Sampson and
Martha A. Kim and
Christopher J. Rossbach",,,,,,,"Although there have been significant efforts to make LLM serving efficient, we observe two limitations of current state-of-the-art serving frameworks in handling multi-turn dialogues between users and assistants, particularly in chat scenarios. First, existing LLM frameworks incur substantial computational overhead in recomputing attention keys and values (KVs) for understanding context across multiple turns of user queries. Second, as the prompt length of user queries is amplified due to multi-turns, a first-come-first-served (FCFS) scheduling policy often causes head-of-line blocking issues, leading to underutilization of GPU resources. To address these limitations, we present FlashGen to rapidly complete multi-turn queries by efficiently utilizing the compute and memory resources of GPUs as well as the host hardware (e.g., DRAM and SSD). We introduce a multi-level KV cache comprised of GPU, CPU, and SSD, to efficiently retain attention KVs from prior turns. Our approach employs low-cost cache restoration techniques to avoid the recomputation burden. Further, we propose a request reordering technique to effectively utilize GPU memory. This scheduling technique carefully adjusts the request order without compromising fairness. Our proposed techniques outperform the vLLM framework in terms of both latency and throughput. For OPT 30B and Llama-2 70B models with the ShareGPT dataset, we achieve 1.63x and 2.85x better throughput, respectively while in a similar latency boundary."
DBLP:conf/asplos/PrabhuNMRP25,inproceedings,vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention,"Ramya Prabhu and
Ajay Nayak and
Jayashree Mohan and
Ramachandran Ramjee and
Ashish Panwar",2025,,"Proceedings of the 30th {ACM} International Conference on Architectural
Support for Programming Languages and Operating Systems, Volume 1,
{ASPLOS} 2025, Rotterdam, The Netherlands, 30 March 2025 - 3 April
2025",,,1133--1150,10.1145/3669940.3707256,https://doi.org/10.1145/3669940.3707256,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/asplos/PrabhuNMRP25.bib,"Fri, 07 Mar 2025 00:00:00 +0100",{ACM},"Lieven Eeckhout and
Georgios Smaragdakis and
Kaitai Liang and
Adrian Sampson and
Martha A. Kim and
Christopher J. Rossbach",,,,,,,"PagedAttention is a popular approach for dynamic memory allocation in LLM serving systems. It enables on-demand allocation of GPU memory to mitigate KV cache fragmentation - a phenomenon that crippled the batch size (and consequently throughput) in prior systems. However, in trying to allocate physical memory at runtime, PagedAttention ends up changing the virtual memory layout of the KV cache from contiguous to non-contiguous. Such a design leads to non-trivial programming and performance overheads. We present vAttention - an approach that mitigates fragmentation in physical memory while retaining the virtual memory contiguity of the KV cache. We achieve this by decoupling the allocation of virtual and physical memory using CUDA virtual memory management APIs. We also introduce various LLM-specific optimizations to address the limitations of CUDA virtual memory support. Overall, vAttention is a simpler, portable, and performant alternative to PagedAttention: it supports various attention kernels out-of-the-box and improves LLM serving throughput by up to 1.23√ó compared to the use of PagedAttention-based kernels of FlashAttention-2 and FlashInfer."
DBLP:conf/caise/Casciani25,inproceedings,"Integrating LLMs and Symbolic Reasoning for Framed Autonomy in AI-Augmented
Business Process Management",Angelo Casciani,2025,,"Intelligent Information Systems - CAiSE 2025 Forum and Doctoral Consortium,
Vienna, Austria, June 16-20, 2025, Proceedings",557,,277--285,10.1007/978-3-031-94590-8\_33,https://doi.org/10.1007/978-3-031-94590-8\_33,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/caise/Casciani25.bib,"Fri, 04 Jul 2025 01:00:00 +0200",Springer,"Luise Pufahl and
Kristina Rosenthal and
Sergio Espa{\~{n}}a and
Selmin Nurcan",Lecture Notes in Business Information Processing,,,,,,
DBLP:conf/case/ChoudharyUPKSKTM25,inproceedings,"A Retrieval-Augmented Generation (RAG)-Based {LLM} for Modern Warehouse
Automation and Management","Kabita Choudhary and
Sheeba Uruj and
Abhinav Pathak and
V. Kalaichelvi and
Sujala D. Shetty and
R. Karthikeyan and
Tarek Taha and
Rajkumar Muthusamy",2025,,"21st {IEEE} International Conference on Automation Science and Engineering,
{CASE} 2025, Los Angeles, CA, USA, August 17-21, 2025",,,1688--1694,10.1109/CASE58245.2025.11164106,https://doi.org/10.1109/CASE58245.2025.11164106,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/case/ChoudharyUPKSKTM25.bib,"Tue, 30 Sep 2025 09:03:32 +0200",{IEEE},,,,,,,,"Robotics automation and integration is quickly becoming a pillar of Industry 4.0 in modern warehouse management systems, but existing solutions fail to seamlessly integrate autonomous robotics into operational workflows, that leads to inefficiencies and scalability challenges. As the dependency on robotics grows in warehouses, there is the need for more reliable data and adaptable systems. The traditional system is not flexible enough to support dynamically evolving warehouse infrastructure that leads to inefficiencies and operational challenges. To address these issues, an efficient and trustworthy automation system is highly needed. This paper proposes a Modern Warehouse Management System (MWMS) framework that integrates Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs). The system uses an advanced framework to make it possible for robots to scan inventory on their own, sort packages in real time, plan their paths, assist people in pickup and place items, and to work together on tasks. A mock-up warehouse environment is developed to test and demonstrate the system‚Äôs capabilities in robotics assistance in a dynamic environment. The framework is evaluated using three LLMs, demonstrating its ability to interact with real-time data and enhance operational efficiency, accuracy, and trustworthiness."
DBLP:conf/chi/GoldiRU25,inproceedings,"Efficient Management of LLM-Based Coaching Agents' Reasoning While
Maintaining Interaction Quality and Speed","Andreas G{\""{o}}ldi and
Roman Rietsche and
Lyle H. Ungar",2025,,"Proceedings of the 2025 {CHI} Conference on Human Factors in Computing
Systems, {CHI} 2025, YokohamaJapan, 26 April 2025- 1 May 2025",,,992:1--992:18,10.1145/3706598.3713606,https://doi.org/10.1145/3706598.3713606,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/chi/GoldiRU25.bib,"Sat, 09 Aug 2025 01:00:00 +0200",{ACM},"Naomi Yamashita and
Vanessa Evers and
Koji Yatani and
Sharon Xianghua Ding and
Bongshin Lee and
Marshini Chetty and
Phoebe O. Toups Dugas",,,,,,,"LLM-based agents improve upon standalone LLMs, which are optimized for immediate intent-satisfaction, by allowing the pursuit of more extended objectives, such as helping users over the long term. To do so, LLM-based agents need to reason before responding. For complex tasks like personalized coaching, this reasoning can be informed by adding relevant information at key moments, shifting it in the desired direction. However, the pursuit of objectives beyond interaction quality may compromise this very quality. Moreover, as the depth and informativeness of reasoning increase, so do the number of tokens required, leading to higher latency and cost. This study investigates how an LLM-based coaching agent can adjust its reasoning depth using a discrepancy mechanism that signals how much reasoning effort to allocate based on how well the objective is being met. Our discrepancy-based mechanism constrains reasoning to better align with alternative objectives, reducing cost roughly tenfold while minimally impacting interaction quality."
DBLP:conf/chi/JiangHZLA25,inproceedings,"Visiobo Demo: Augmenting Static Prints with Projection-based Visual
Cueing and Concept Mapping via {LLM} Reasoning","Jiaqi Jiang and
Kexin Huang and
Hanqing Zhou and
Huiying Lu and
Pengcheng An",2025,,"Proceedings of the Extended Abstracts of the {CHI} Conference on Human
Factors in Computing Systems, {CHI} {EA} 2025, Yokohama, Japan, 26
April 2025- 1 May 2025",,,753:1--753:5,10.1145/3706599.3721169,https://doi.org/10.1145/3706599.3721169,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/chi/JiangHZLA25.bib,"Sun, 25 May 2025 20:50:59 +0200",{ACM},"Naomi Yamashita and
Vanessa Evers and
Koji Yatani and
Sharon Xianghua Ding",,,,,,,"Posters are widely used for information dissemination, but their static nature and limited interactivity often hinder engagement and personalized learning experiences. We introduce a guide agent Visiobo designed to enhance poster reading by providing personalized support and dynamic visual augmentations. Visiobo overlays cueing (e.g., highlights, arrows) to help users quickly locate key information and employs concept mapping to address the content limitations of static posters by offering structured, relevant information. The user study with 12 participants demonstrated that by dynamically adapting to user interests and seamlessly integrating voice interaction with visual overlay cueing and concept mapping, Visiobo provides an engaging and enriched poster reading experience."
DBLP:conf/chi/KretzerKBPM25,inproceedings,"Closing the Loop between User Stories and {GUI} Prototypes: An LLM-Based
Assistant for Cross-Functional Integration in Software Development","Felix Kretzer and
Kristian Kolthoff and
Christian Bartelt and
Simone Paolo Ponzetto and
Alexander Maedche",2025,,"Proceedings of the 2025 {CHI} Conference on Human Factors in Computing
Systems, {CHI} 2025, YokohamaJapan, 26 April 2025- 1 May 2025",,,879:1--879:19,10.1145/3706598.3713932,https://doi.org/10.1145/3706598.3713932,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/chi/KretzerKBPM25.bib,"Fri, 09 May 2025 01:00:00 +0200",{ACM},"Naomi Yamashita and
Vanessa Evers and
Koji Yatani and
Sharon Xianghua Ding and
Bongshin Lee and
Marshini Chetty and
Phoebe O. Toups Dugas",,,,,,,"Graphical user interfaces (GUIs) are at the heart of almost every software we encounter. GUIs are often created through a collaborative effort involving UX designers, product owners, and software developers, constantly facing changing requirements. Historically, problems in GUI development include a fragmented, poorly integrated tool landscape and high synchronization efforts between stakeholders. Recent approaches suggest using large language models (LLMs) to recognize requirements fulfillment in GUIs and automatically propose new GUI components. Based on ten interviews with practitioners, this paper proposes an LLM-based assistant as a Figma plug-in that bridges the gap between user stories and GUI prototyping. We evaluated the prototype with 40 users and 40 crowd-workers, showing that the effectiveness of GUI creation is improved by using LLMs to detect requirements‚Äô completion and generate new GUI components. We derive design rationales to support cross-functional integration in software development, ensuring that our plug-in integrates well into established processes."
DBLP:conf/chi/NeupaneDG025,inproceedings,"Wearable Meets {LLM} for Stress Management: {A} Duoethnographic Study
Integrating Wearable-Triggered Stressors and {LLM} Chatbots for Personalized
Interventions","Sameer Neupane and
Poorvesh Dongre and
Denis Gracanin and
Santosh Kumar",2025,,"Proceedings of the Extended Abstracts of the {CHI} Conference on Human
Factors in Computing Systems, {CHI} {EA} 2025, Yokohama, Japan, 26
April 2025- 1 May 2025",,,588:1--588:8,10.1145/3706599.3720197,https://doi.org/10.1145/3706599.3720197,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/chi/NeupaneDG025.bib,"Fri, 09 May 2025 01:00:00 +0200",{ACM},"Naomi Yamashita and
Vanessa Evers and
Koji Yatani and
Sharon Xianghua Ding",,,,,,,"We use a duoethnographic approach to study how wearable-integrated LLM chatbots can assist with personalized stress management, addressing the growing need for immediacy and tailored interventions. Two researchers interacted with custom chatbots over 22 days, responding to wearable-detected physiological prompts, recording stressor phrases, and using them to seek tailored interventions from their LLM-powered chatbots. They recorded their experiences in autoethnographic diaries and analyzed them during weekly discussions, focusing on the relevance, clarity, and impact of chatbot-generated interventions. Results showed that even though most events triggered by the wearable were meaningful, only one in five warranted an intervention. It also showed that interventions tailored with brief event descriptions were more effective than generic ones. By examining the intersection of wearables and LLM, this research contributes to developing more effective, user-centric mental health tools for real-time stress relief and behavior change."
DBLP:conf/chi/SchirraVB25,inproceedings,"""It's Something to Polish Your Own Thoughts, Rather than Create Thoughts
for You"": Understanding Participants' Use of Chatbots and LLMs During
Online Research Participation","Steven Schirra and
Sasha G. Volkov and
Frank Bentley",2025,,"Proceedings of the Extended Abstracts of the {CHI} Conference on Human
Factors in Computing Systems, {CHI} {EA} 2025, Yokohama, Japan, 26
April 2025- 1 May 2025",,,20:1--20:6,10.1145/3706599.3720027,https://doi.org/10.1145/3706599.3720027,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/chi/SchirraVB25.bib,"Thu, 01 May 2025 01:00:00 +0200",{ACM},"Naomi Yamashita and
Vanessa Evers and
Koji Yatani and
Sharon Xianghua Ding",,,,,,,"There are growing discussions within the research community about how to adapt study design given the widespread availability of Generative Artificial Intelligence (GenAI), including Large Language Models (LLMs). While much prior research has focused on LLM use from a researcher perspective (e.g. detecting and screening for LLM use) we present a complementary study from the perspective of participants who use LLMs during their research participation. In this exploratory interview study with 17 participants, we found a range of LLM use cases, from sourcing studies, to generating or modifying responses, to asking clarification questions about studies. We also explored participants‚Äô ethical considerations, finding that participants considered researchers‚Äô needs for authentic data when setting ethical boundaries. Participants also discussed how attempts to thwart their LLM use have negatively impacted their everyday participant experience. We propose a set of recommendations that researchers can incorporate into their studies to proactively address participant LLM use."
DBLP:conf/chi/ShenYZLNF25,inproceedings,"""It Warned Me Just at the Right Moment"": Exploring LLM-based Real-time
Detection of Phone Scams","Zitong Shen and
Sineng Yan and
Youqian Zhang and
Xiapu Luo and
Grace Ngai and
Eugene Yujun Fu",2025,,"Proceedings of the Extended Abstracts of the {CHI} Conference on Human
Factors in Computing Systems, {CHI} {EA} 2025, Yokohama, Japan, 26
April 2025- 1 May 2025",,,18:1--18:7,10.1145/3706599.3720263,https://doi.org/10.1145/3706599.3720263,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/chi/ShenYZLNF25.bib,"Fri, 09 May 2025 01:00:00 +0200",{ACM},"Naomi Yamashita and
Vanessa Evers and
Koji Yatani and
Sharon Xianghua Ding",,,,,,,"Despite living in the era of the internet, phone-based scams remain one of the most prevalent forms of scams. These scams aim to exploit victims for financial gain, causing both monetary losses and psychological distress. While governments, industries, and academia have actively introduced various countermeasures, scammers also continue to evolve their tactics, making phone scams a persistent threat. To combat these increasingly sophisticated scams, detection technologies must also advance. In this work, we propose a framework for modeling scam calls and introduce an LLM-based real-time detection approach, which assesses fraudulent intent in conversations, further providing immediate warnings to users to mitigate harm. Through experiments, we evaluate the method‚Äôs performance and analyze key factors influencing its effectiveness. This analysis enables us to refine the method to improve precision while exploring the trade-off between recall and timeliness, paving the way for future directions in this critical area of research."
DBLP:conf/chi/ZhuYYTYS25,inproceedings,"AutoPBL: An LLM-powered Platform to Guide and Support Individual Learners
Through Self Project-based Learning","Yihao Zhu and
Zhoutong Ye and
Yichen Yuan and
Wenxuan Tang and
Chun Yu and
Yuanchun Shi",2025,,"Proceedings of the 2025 {CHI} Conference on Human Factors in Computing
Systems, {CHI} 2025, YokohamaJapan, 26 April 2025- 1 May 2025",,,584:1--584:26,10.1145/3706598.3714261,https://doi.org/10.1145/3706598.3714261,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/chi/ZhuYYTYS25.bib,"Mon, 28 Apr 2025 01:00:00 +0200",{ACM},"Naomi Yamashita and
Vanessa Evers and
Koji Yatani and
Sharon Xianghua Ding and
Bongshin Lee and
Marshini Chetty and
Phoebe O. Toups Dugas",,,,,,,"Self project-based learning (SPBL) is a popular learning style where learners follow tutorials and build projects by themselves. SPBL combines project-based learning‚Äôs benefit of being engaging and effective with the flexibility of self-learning. However, insufficient guidance and support during SPBL may lead to unsatisfactory learning experiences and outcomes. While LLM chatbots (e.g., ChatGPT) could potentially serve as SPBL tutors, we have yet to see an SPBL platform with responsible and systematic LLM integration. To address this gap, we present AutoPBL, an interactive learning platform for SPBL learners. We examined human PBL tutors‚Äô roles through formative interviews to inform our design. AutoPBL features an LLM-guided learning process with checkpoint questions and in-context Q&A. In a user study where 29 beginners learned machine learning through entry-level projects, we found that AutoPBL effectively improves learning outcomes and elicits better learning behavior and metacognition by clarifying current priorities and providing timely assistance."
DBLP:conf/ci2/Fukumura025,inproceedings,"Can LLM-Powered Multi-Agent Systems Augment Human Creativity? Evidence
from Brainstorming Tasks","Kazuma Fukumura and
Takayuki Ito",2025,,"Proceedings of the {ACM} Collective Intelligence Conference, {CI}
2025, San Diego, CA, USA, August 4-6, 2025",,,20--29,10.1145/3715928.3737479,https://doi.org/10.1145/3715928.3737479,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ci2/Fukumura025.bib,"Thu, 25 Sep 2025 15:30:56 +0200",{ACM},"Steven P. Dow and
Joshua Becker and
Besmira Nushi and
Lisa O'Bryan and
Saiph Savage",,,,,,,
DBLP:conf/compsac/ManakinaL25,inproceedings,"Designing Reusable LLM-Enhanced Assignments: {A} Quality-Oriented
Framework for Software Engineering Education","Olga Manakina and
Chung{-}Horng Lung",2025,,"49th {IEEE} Annual Computers, Software, and Applications Conference,
{COMPSAC} 2025, Toronto, ON, Canada, July 8-11, 2025",,,2212--2217,10.1109/COMPSAC65507.2025.00310,https://doi.org/10.1109/COMPSAC65507.2025.00310,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/compsac/ManakinaL25.bib,"Fri, 05 Sep 2025 21:24:29 +0200",{IEEE},"Hossain Shahriar and
Kazi Shafiul Alam and
Hiroyuki Ohsaki and
Stelvio Cimato and
Miriam A. M. Capretz and
Shamem Ahmed and
Sheikh Iqbal Ahamed and
AKM Jahangir Alam Majumder and
Munirul Haque and
Tomoki Yoshihisa and
Alfredo Cuzzocrea and
Michiharu Takemoto and
Nazmus Sakib and
Marwa Elsayed",,,,,,,"Large Language Models (LLMs) are increasingly prevalent in software engineering (SE) practice, yet their integration into education remains improvised and lacks theoretical grounding. This paper presents a quality-oriented framework for LLM integration in programming curricula, drawing on Cognitive Load Theory and Constructivism. We demonstrate its utility by using an LLM (Claude Sonnet 3.7) to redesign an Operating Systems assignment, followed by expert evaluation. Results show that LLMs can serve as tools for curriculum design and aids for student learning. For instructors, LLMs streamline the redesign of assignments while maintaining quality. For learners, they provide structured guidance and promote metacognitive development. We position LLMs as reusable educational components that can improve learning outcomes for students and streamline material design for instructors, while preparing both for professional environments where Artificial Intelligence (AI) collaboration is increasingly expected."
DBLP:conf/compsac/SunYKCL25,inproceedings,"StuLAC: An Adaptive LLM-Driven Framework for Scalable Student Feedback
Analysis in Software-Driven Educational Systems","Yicheng Sun and
Hi Kuen Yu and
Jacky Keung and
Yuchen Cao and
Yihan Liao",2025,,"49th {IEEE} Annual Computers, Software, and Applications Conference,
{COMPSAC} 2025, Toronto, ON, Canada, July 8-11, 2025",,,121--130,10.1109/COMPSAC65507.2025.00024,https://doi.org/10.1109/COMPSAC65507.2025.00024,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/compsac/SunYKCL25.bib,"Tue, 07 Oct 2025 01:00:00 +0200",{IEEE},"Hossain Shahriar and
Kazi Shafiul Alam and
Hiroyuki Ohsaki and
Stelvio Cimato and
Miriam A. M. Capretz and
Shamem Ahmed and
Sheikh Iqbal Ahamed and
AKM Jahangir Alam Majumder and
Munirul Haque and
Tomoki Yoshihisa and
Alfredo Cuzzocrea and
Michiharu Takemoto and
Nazmus Sakib and
Marwa Elsayed",,,,,,,"With the growing scalability challenges in higher education, automated student feedback analysis has become crucial for course evaluation and pedagogical improvements. However, traditional methods struggle to handle mixed sentiments, adapt to evolving feedback trends, and maintain computational efficiency. To address these challenges, we propose StuLAC, a Software Engineering-driven framework that integrates Large Language Models (LLMs) with Adaptive Template-Based Caching (ATC). StuLAC employs hierarchical matching for fine-grained classification and dynamically updates feedback templates through context-aware cache refinement. Empirical results on 80,000 student feedback entries demonstrate that StuLAC-generated summaries improve overall quality by 10.5% compared to manually generated reports, while also achieving faster processing times. Additionally, StuLAC attains an 86.4% accuracy and an 86.24% F1-score in sentiment detection. StuLAC‚Äôs Feedback Summary Generation provides actionable insights that enhance data-driven decision-making in educational settings. These findings establish StuLAC as a scalable and adaptive solution for improving AI-driven educational feedback systems."
DBLP:conf/compsac/XieJGS25,inproceedings,"Empowering {AI} to Generate Better {AI} Code: Guided Generation of
Deep Learning Projects with LLMs","Chen Xie and
Mingsheng Jiao and
Xiaodong Gu and
Beijun Shen",2025,,"49th {IEEE} Annual Computers, Software, and Applications Conference,
{COMPSAC} 2025, Toronto, ON, Canada, July 8-11, 2025",,,1394--1399,10.1109/COMPSAC65507.2025.00175,https://doi.org/10.1109/COMPSAC65507.2025.00175,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/compsac/XieJGS25.bib,"Fri, 05 Sep 2025 01:00:00 +0200",{IEEE},"Hossain Shahriar and
Kazi Shafiul Alam and
Hiroyuki Ohsaki and
Stelvio Cimato and
Miriam A. M. Capretz and
Shamem Ahmed and
Sheikh Iqbal Ahamed and
AKM Jahangir Alam Majumder and
Munirul Haque and
Tomoki Yoshihisa and
Alfredo Cuzzocrea and
Michiharu Takemoto and
Nazmus Sakib and
Marwa Elsayed",,,,,,,"While large language models (LLMs) have been widely applied to code generation, they struggle with generating entire deep learning projects, which are characterized by complex structures, longer functions, and stronger reliance on domain knowledge than general-purpose code. An open-domain LLM often lacks coherent contextual guidance and domain expertise for specific projects, making it challenging to produce complete code that fully meets user requirements. In this paper, we propose a novel planning-guided code generation method, DLCodeGen, tailored for generating deep learning projects. DLCodeGen predicts a structured solution plan, offering global guidance for LLMs to generate the project. The generated plan is then leveraged to retrieve semantically analogous code samples and subsequently abstract a code template. To effectively integrate these multiple retrieval-augmented techniques, a comparative learning mechanism is designed to generate the final code. We validate the effectiveness of our approach on a dataset we build for deep learning code generation. Experimental results demonstrate that DLCodeGen outperforms other baselines, achieving improvements of 9.7% in CodeBLEU and 3.6% in human evaluation metrics."
DBLP:conf/cscwd/NobregaM0BASXS25,inproceedings,"Towards a Knowledge Management Framework for LLM-Generated Personas
in Collaborative Systems","Lucas N{\'{o}}brega and
Luiz Felipe Martinez and
Yuri Lima and
Carlos Eduardo Barbosa and
Matheus Margarido Arg{\^{o}}lo and
Herbert Salazar and
Geraldo Xex{\'{e}}o and
Jano Moreira de Souza",2025,,"28th International Conference on Computer Supported Cooperative Work
in Design, {CSCWD} 2025, Compiegne, France, May 5-7, 2025",,,539--545,10.1109/CSCWD64889.2025.11033648,https://doi.org/10.1109/CSCWD64889.2025.11033648,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/cscwd/NobregaM0BASXS25.bib,"Fri, 11 Jul 2025 07:46:05 +0200",{IEEE},"Weiming Shen and
Marie{-}H{\'{e}}l{\`{e}}ne Abel and
Nada Matta and
Jean{-}Paul A. Barth{\`{e}}s and
Junzhou Luo and
Jinghui Zhang and
Haibin Zhu and
Kunkun Peng",,,,,,,"With the advancement of Artificial Intelligence (AI) technologies, the world has been increasingly reshaped across multiple domains - healthcare, finance, education, and creative fields. This expansion transforms individual tasks and facilitates complex collaborations where humans and machines work to- gether in unprecedented ways. Through machine learning and, more specifically, large language models (LLMs), AI now plays a crucial role in enhancing productivity. Recent technological advancements have significantly boosted the text generation capabilities of LLMs, enabling them to simulate personas that can represent groups or specific well-known individuals. By simulating expert perspectives, LLMs provide insights to guide human decision-making across various collaborative frameworks in Computer-Supported Cooperative Work (CSCW). Personas, as simulated identities, have proven helpful in group brainstorming sessions, consensus-building exercises, and virtual advisory boards, which help represent diverse viewpoints and facilitate group decisions. Among these CSCW methods, the Delphi method is a well-established approach for reaching consensus among experts through iterative feedback and structured discussions. This work introduces a framework to manage and evaluate LLM-generated personas simulating expert input in Delphi studies. Our contributions include a framework designed for evaluating LLM personas in Delphi studies using a text similarity validation technique comparing real and simulated expert opinions and an evaluation of a Delphi about Brazilian higher education, assessing the LLMs' reliability in real-world scenarios. Findings show LLMs can effectively replicate expert perspectives, enhancing AI integration in specialized decision-making."
DBLP:conf/csedu/AnandL25,inproceedings,"iReflect: Enhancing Reflective Learning with LLMs: {A} Study on Automated
Feedback in Project Based Courses","Bhojan Anand and
Quek Sze Long",2025,,"Proceedings of the 17th International Conference on Computer Supported
Education, {CSEDU} 2025, Porto, Portugal, April 1-3, 2025, Volume
2",,,395--403,10.5220/0013435800003932,https://doi.org/10.5220/0013435800003932,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/csedu/AnandL25.bib,"Fri, 02 May 2025 12:52:52 +0200",{SCITEPRESS},"Benedict du Boulay and
Tania Di Mascio and
Edmundo Tovar and
Christoph Meinel",,,,,,,": Reflective learning in education offers various benefits, including a deeper understanding of concepts, increased self-awareness, and higher-quality project work. However, integrating reflective learning into the syllabus presents challenges, such as the difficulty of grading and the manual effort required to provide in-dividualised feedback. In this paper, we explore the use of Large Language Models (LLMs) to automate formative feedback on student reflections. Our study is conducted in the CS4350 Game Development Project course, where students work in teams to develop a game through multiple milestone assessments over the semester. As part of the reflective learning process, students write reflections at the end of each milestone to prepare for the next. Students are given the option to use our automated feedback tool to improve their submissions. These reflections are graded by Teaching Assistants (TAs). We analyse the impact of the tool by comparing students‚Äô initial reflection drafts with their final submissions and surveying them on their experience with automated feedback. In addition, we assess students‚Äô perceptions of the usefulness of reflective writing in the game development process. Our findings indicate that students who revised their reflections after using the tool showed an improvement in their overall reflection scores, suggesting that automated feedback improves reflection quality. Furthermore, most of the students reported that reflective writing improved their learning experience, citing benefits such as increased self-awareness, better project and time management, and enhanced technical skills."
DBLP:conf/csr2/KalafatidisKP25,inproceedings,"C-Shield: {A} Holistic Solution for Secure End-to-End Kubernetes Multi-Cluster
Management and Online Threat Mitigation using LLMs","Sarantis Kalafatidis and
George Kitsos and
Nikos Papageorgopoulos",2025,,"{IEEE} International Conference on Cyber Security and Resilience,
{CSR} 2025, Chania, Crete, Greece, August 4-6, 2025",,,846--853,10.1109/CSR64739.2025.11130174,https://doi.org/10.1109/CSR64739.2025.11130174,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/csr2/KalafatidisKP25.bib,"Tue, 02 Sep 2025 18:25:51 +0200",{IEEE},,,,,,,,"This paper presents C-Shield, a holistic security framework for the secure creation, management, and protection of Kubernetes clusters among geographically distributed nodes. C-Shield aims to address the challenges of secure node registration, encrypted inter-node communication, runtime threat detection, and automated threat mitigation. The framework provides end-users with ready-to-use secure clusters, reducing the need for complex, manual deployment processes. It utilizes WireGuardbuilt overlay networks to establish strong encrypted communication among distributed Kubernetes nodes, while centralized monitoring collects network and system telemetry across multiple clusters. In order to improve threat detection and response, C Shield integrates Large Language Models for contextual security analysis, alert explanation, false positive reduction, and automated policy generation. We evaluated the effectiveness of C Shield subsystems through realistic experimental deployments and demonstrated their ability to provide end-to-end security for distributed Kubernetes infrastructures."
DBLP:conf/csr2/MuthM25,inproceedings,"An Approach for a Supporting Multi-LLM System for Automated Certification
Based on the German IT-Grundschutz","Lea R. Muth and
Marian Margraf",2025,,"{IEEE} International Conference on Cyber Security and Resilience,
{CSR} 2025, Chania, Crete, Greece, August 4-6, 2025",,,482--489,10.1109/CSR64739.2025.11130171,https://doi.org/10.1109/CSR64739.2025.11130171,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/csr2/MuthM25.bib,"Tue, 02 Sep 2025 01:00:00 +0200",{IEEE},,,,,,,,"This paper presents a novel approach to perform semi-automated BSI IT-Grundschutz certification using a MultiLarge Language Model system (MLS) with Hybrid RetrievalAugmented Generation (HybridRAG). Facing the challenges of the Network and Information Security Directive 2 (NIS2) directive, a shortage of specialists, and high implementation costs, our MLS architecture aims to increase efficiency, reduce costs, and support certifiers in maintaining the quality of security concepts while meeting the increased demand for certifications of newly affected companies. The system combines Large Language Models (LLMs) and Knowledge Graphs (KGs) to support different phases of the certification process, including protection needs assessment, modeling, IT-Grundschutz check, measure consolidation, and subsequent realization. Our architecture addresses the growing demand for security concepts and offers an approach to handle the digital security challenges introduced by NIS2."
DBLP:conf/cvpr/KhlaisamniangKV25,inproceedings,"Decomposing Food Images for Better Nutrition Analysis: {A} Nutritionist-Inspired
Two-Step Multimodal {LLM} Approach","Pitikorn Khlaisamniang and
Kun Kerdthaisong and
Supasate Vorathammathorn and
Nutchanon Yongsatianchot and
Hirunkul Phimsiri and
Amrest Chinkamol and
Teermade Thitseesaeng and
Kanyakorn Veerakanjana and
Kaisorn Kachai and
Piyalitt Ittichaiwong and
Tossaporn Saengja",2025,,"{IEEE/CVF} Conference on Computer Vision and Pattern Recognition Workshops,
{CVPR} Workshops 2025, Nashville, TN, USA, June 11-15, 2025",,,482--491,,https://openaccess.thecvf.com/content/CVPR2025W/MTF/html/Khlaisamniang\_Decomposing\_Food\_Images\_for\_Better\_Nutrition\_Analysis\_A\_Nutritionist-Inspired\_Two-Step\_CVPRW\_2025\_paper.html,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/cvpr/KhlaisamniangKV25.bib,"Thu, 24 Jul 2025 16:40:50 +0200",Computer Vision Foundation / {IEEE},,,,,,,,
DBLP:conf/dac/LiZLS25,inproceedings,"BlockPIM: Optimizing Memory Management for PIM-enabled Long-Context
{LLM} Inference","Zhichun Li and
Jun Zhou and
Xueqi Li and
Ninghui Sun",2025,,"62nd {ACM/IEEE} Design Automation Conference, {DAC} 2025, San Francisco,
CA, USA, June 22-25, 2025",,,1--7,10.1109/DAC63849.2025.11133193,https://doi.org/10.1109/DAC63849.2025.11133193,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/dac/LiZLS25.bib,"Tue, 23 Sep 2025 01:00:00 +0200",{IEEE},,,,,,,,"Processing-In-Memory (PIM) architectures alleviate the memory bottleneck in the decode phase of large language model (LLM) inference by performing operations like GEMV and Softmax in memory. However, the fragmented data layout in current PIM architectures limits end-to-end acceleration for long-context LLMs. In this paper, we propose BlockPIM, a cross-channel block memory layout strategy that maximizes memory utilization and eliminates the context length constraint. Additionally, we introduce a cross-channel attention computation scheme that is compatible with the current architecture to support distributed attention operations on BlockPIM. Experimental results demonstrate that our approach achieves a 62% average throughput increase compared to existing state-of-the-art PIM solutions, enabling efficient and scalable deployment of large language models on PIM architectures."
DBLP:conf/date/XieWXHSZX25,inproceedings,"FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained Mixed-Precision
Quantization of LLMs","Xilong Xie and
Liang Wang and
Limin Xiao and
Meng Han and
Lin Sun and
Shuai Zheng and
Xiangrong Xu",2025,,"Design, Automation {\&} Test in Europe Conference, {DATE} 2025, Lyon,
France, March 31 - April 2, 2025",,,1--7,10.23919/DATE64628.2025.10993129,https://doi.org/10.23919/DATE64628.2025.10993129,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/date/XieWXHSZX25.bib,"Thu, 07 Aug 2025 01:00:00 +0200",{IEEE},,,,,,,,"Large language models (LLMs) have significantly advanced the natural language processing paradigm but impose substantial demands on memory and computational resources. Quantization is one of the most effective ways to reduce memory consumption of LLMs. However, advanced single-precision quantization methods experience significant accuracy degradation when quantizing to ultra-low bits. Existing mixed-precision quantization methods are quantized by groups with coarse granularity. Employing high precision for group data leads to substantial memory overhead, whereas low precision severely impacts model accuracy. To address this issue, we propose FineQ, software-hardware co-design for low-bit fine-grained mixed-precision quantization of LLMs. First, FineQ partitions the weights into finer-grained clusters and considers the distribution of outliers within these clusters, thus achieving a balance between model accuracy and memory overhead. Then, we propose an outlier protection mechanism within clusters that uses 3 bits to represent outliers and introduce an encoding scheme for index and data concatenation to enable aligned memory access. Finally, we introduce an accelerator utilizing temporal coding that effectively supports the quantization algorithm while simplifying the multipliers in the systolic array. FineQ achieves higher model accuracy compared to the SOTA mixed-precision quantization algorithm at a close average bit-width. Meanwhile, the accelerator achieves up to 1.79x energy efficiency and reduces the area of the systolic array by 61.2%."
DBLP:conf/ddecs/ZhangHD25,inproceedings,"LLM-assisted Performance Estimation of Embedded Software on {RISC-V}
Processors","Weiyan Zhang and
Muhammad Hassan and
Rolf Drechsler",2025,,"28th {IEEE} International Symposium on Design and Diagnostics of Electronic
Circuits and Systems, {DDECS} 2025, Lyon, France, May 5-7, 2025",,,7--12,10.1109/DDECS63720.2025.11006767,https://doi.org/10.1109/DDECS63720.2025.11006767,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ddecs/ZhangHD25.bib,"Tue, 10 Jun 2025 12:22:49 +0200",{IEEE},,,,,,,,"In this paper, we present a methodology that combines a Large Language Model (LLM) with a traditional Machine Learning (ML) approach to estimate the performance of embedded software on RISC-V processors across different microarchitectures. In particular, we employ a Retrieval-Augmented Generation (RAG)-based LLM to extract performance-related information from processor specifications and source code. Additionally, we leverage the predictive capabilities of ML models to create Predictive Models (PMs) for RISC-V processors. To demonstrate the effectiveness of our hybrid approach, we present results on the performance estimation of open-source benchmarks using the generated PMs, with open-source RISC-V-based Register Transfer Level (RTL) implementations as reference models. Our results demonstrate that our proposed LLM-assisted methodology provides highly accurate predictions, with Mean Absolute Percentage Errors (MAPEs) of only 2.50% for SweRV core and 11.90% for RSD core, respectively. In comparison with the state-of-the-art methodology, our approach achieves significant improvements, reducing the MAPE by 61.54% for SweRV and 36.02% for RSD."
DBLP:conf/deem/CaoAK25,inproceedings,Buffer Management for Out-of-GPU {LLM} Execution,"Jiashen Cao and
Joy Arulraj and
Hyesoon Kim",2025,,"Proceedings of the Workshop on Data Management for End-to-End Machine
Learning, {DEEM} 2025, Berlin, Germany, June 22-27, 2025",,,1:1--1:5,10.1145/3735654.3735937,https://doi.org/10.1145/3735654.3735937,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/deem/CaoAK25.bib,"Sun, 06 Jul 2025 01:00:00 +0200",{ACM},,,,,,,,"The rapid advancement of large language models (LLMs) has caused their parameter sizes to grow beyond the memory capacity of a single GPU. Although distributed inference across multiple GPUs is a solution in enterprise settings, it remains inaccessible for most non-commercial users. Thus, there is a growing demand to run LLMs on a single GPU when the model does not fit entirely in GPU memory. A common approach is to offload parts of the model from the GPU to the CPU during inference. However, repeatedly transferring parameters between these devices incurs significant overhead. To address this challenge, we propose a new buffer management policy, LIRS-M, which maximizes buffer hits and minimizes data transfer. Experimental results show that our approach achieves a 2.0√ó speedup compared to StoA offloading techniques while delivering robust buffer-hit performance."
DBLP:conf/deem/HoseiniHQ25,inproceedings,"End-To-End {ML} with LLMs and Semantic Data Management: Experiences
from Chemistry 4.0","Sayed Hoseini and
Vincent Herrmann and
Christoph Quix",2025,,"Proceedings of the Workshop on Data Management for End-to-End Machine
Learning, {DEEM} 2025, Berlin, Germany, June 22-27, 2025",,,6:1--6:10,10.1145/3735654.3735942,https://doi.org/10.1145/3735654.3735942,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/deem/HoseiniHQ25.bib,"Sun, 06 Jul 2025 01:00:00 +0200",{ACM},,,,,,,,"Machine Learning (ML) in industrial chemistry is often hindered by the complexity of preprocessing heterogeneous datasets. In this proof-of-concept study, we explore the use of semantic data management to support LLM-driven automation of end-to-end ML pipelines in a real-world Chemistry 4.0 setting. A semantic model is used to capture domain knowledge and metadata in a machine-readable form, guiding LLMs through natural language prompts to generate complete data wrangling and ML modeling code. We evaluate several state-of-the-art LLMs on their ability to autonomously produce functionally correct Python code for preprocessing and Gaussian Process modeling. Our results show that, when guided by structured semantic context, larger LLMs can reliably generate accurate pipelines, significantly reducing the need for manual intervention. These findings provide an encouraging starting point for further exploration toward leveraging the semantic model to improve the robustness of code generation by systematically integrating relevant information into the generation process, rather than relying solely on the raw intelligence of the LLM."
DBLP:conf/dexa/AlamAC25,inproceedings,"Improving Software Security Through a LLM-Based Vulnerability Detection
Model","Syeda Sadia Alam and
Mst. Shapna Akter and
Alfredo Cuzzocrea",2025,,"Database and Expert Systems Applications - 36th International Conference,
{DEXA} 2025, Bangkok, Thailand, August 25-27, 2025, Proceedings, Part
{I}",16046,,122--129,10.1007/978-3-032-02049-9\_9,https://doi.org/10.1007/978-3-032-02049-9\_9,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/dexa/AlamAC25.bib,"Sat, 13 Sep 2025 16:18:22 +0200",Springer,"Robert Wrembel and
Gabriele Kotsis and
A Min Tjoa and
Ismail Khalil",Lecture Notes in Computer Science,,,,,,
DBLP:conf/dsn/ImperadeiroA025,inproceedings,Rethinking {BFT:} Leveraging Diverse Software Components with LLMs,"Jo{\~{a}}o Imperadeiro and
Ana Nunes Alonso and
Jos{\'{e}} Pereira",2025,,"2025 55th Annual {IEEE/IFIP} International Conference on Dependable
Systems and Networks, {DSN} 2025 - Supplemental Volume, Naples, Italy,
June 23-26, 2025",,,216--220,10.1109/DSN-S65789.2025.00067,https://doi.org/10.1109/DSN-S65789.2025.00067,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/dsn/ImperadeiroA025.bib,"Mon, 21 Jul 2025 09:34:56 +0200",{IEEE},,,,,,,,"Diversity is crucial in systems that tolerate Byzantine faults. Traditionally, system builders have relied on standardized interfaces (e.g., POSIX for operating systems) to obtain off-the-shelf components or on n-version programming for custom functionality. Unfortunately, standardized alternatives are rare, and the independent development of multiple versions of the same software is costly and justified only on the most critical applications. In this paper, we show that a limited and focused use of LLMs for translation opens up the possibility of leveraging the existing diversity in functionally equivalent but non-standardized components. Specifically, we show that LLMs can produce functionally correct database query translations with minimal guidance and adapt to diverse data models and query contexts, enabling the use of radically different database models, both SQL and NoSQL, together in a Byzantine fault-tolerant replicated system. We outline an approach to achieve this in practice and discuss future research directions."
DBLP:conf/ecis/MaichleSP25,inproceedings,"What can we learn from LLMs? Building a Foundation Model for Inventory
Management","Magnus Josef Maichle and
Nikolai Stein and
Richard Pibernik",2025,,"33rd European Conference on Information Systems - Co-Creating Value
for an Intelligent Future, {ECIS} 2025, Amman, Jordan, June 16-18,
2025",,,,,https://aisel.aisnet.org/ecis2025/bus\_analytics/bus\_analytics/11,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ecis/MaichleSP25.bib,"Tue, 30 Sep 2025 11:31:12 +0200",,"Ahmad Ghazawneh and
Sirkka Jarvenpaa and
Omar El Sawy and
Hanna Krasnova and
Johann Kranz and
Rikard Lindgren",,,,,,,
DBLP:conf/ecsa/AmalfitanoLSPF25,inproceedings,"Automated Software Architecture Design Recovery from Source Code Using
LLMs","Domenico Amalfitano and
Marco De Luca and
Tiziano Santilli and
Patrizio Pelliccione and
Anna Rita Fasolino",2025,,"Software Architecture - 19th European Conference, {ECSA} 2025, Limassol,
Cyprus, September 15-19, 2025, Proceedings",15929,,73--89,10.1007/978-3-032-02138-0\_5,https://doi.org/10.1007/978-3-032-02138-0\_5,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ecsa/AmalfitanoLSPF25.bib,"Sat, 13 Sep 2025 16:30:41 +0200",Springer,"Vasilios Andrikopoulos and
Cesare Pautasso and
Nour Ali and
Jacopo Soldani and
Xiwei Xu",Lecture Notes in Computer Science,,,,,,
DBLP:conf/ecsa/KlinakuLB25,inproceedings,"LLM-Based Explainability at Design Time: Detecting Elasticity Antipatterns
in Software Architectures","Floriment Klinaku and
Jonas Lammert and
Steffen Becker",2025,,"Software Architecture. {ECSA} 2025 Tracks and Workshops - Limassol,
Cyprus, September 15-19, 2025, Proceedings",15982,,141--154,10.1007/978-3-032-04403-7\_14,https://doi.org/10.1007/978-3-032-04403-7\_14,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ecsa/KlinakuLB25.bib,"Mon, 06 Oct 2025 01:00:00 +0200",Springer,"Domenico Bianculli and
Hassan Sartaj and
Vasilios Andrikopoulos and
Cesare Pautasso and
Tommi Mikkonen and
Jennifer P{\'{e}}rez and
Tom{\'{a}}s Bures and
Martina De Sanctis and
Henry Muccini and
Elena Navarro and
Mohamed Soliman and
Uwe Zdun",Lecture Notes in Computer Science,,,,,,
DBLP:conf/ecsa/OliveiraM25,inproceedings,"LLM-Based Quality Assessment of Software Architecture Diagrams: {A}
Preliminary Study with Four Open-Source Projects","Glauber Queiroz de Oliveira and
Nabor C. Mendon{\c{c}}a",2025,,"Software Architecture - 19th European Conference, {ECSA} 2025, Limassol,
Cyprus, September 15-19, 2025, Proceedings",15929,,116--123,10.1007/978-3-032-02138-0\_8,https://doi.org/10.1007/978-3-032-02138-0\_8,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ecsa/OliveiraM25.bib,"Sat, 13 Sep 2025 01:00:00 +0200",Springer,"Vasilios Andrikopoulos and
Cesare Pautasso and
Nour Ali and
Jacopo Soldani and
Xiwei Xu",Lecture Notes in Computer Science,,,,,,
DBLP:conf/ecsa/SolimanAAKV25,inproceedings,"LLMs for Software Architecture Knowledge: {A} Comparative Analysis
Among Seven LLMs","Mohamed Soliman and
Elia Ashraf and
Kamel M. K. Abdelsalam and
Jan Keim and
Ashwin Prasad Shivarpatna Venkatesh",2025,,"Software Architecture - 19th European Conference, {ECSA} 2025, Limassol,
Cyprus, September 15-19, 2025, Proceedings",15929,,99--115,10.1007/978-3-032-02138-0\_7,https://doi.org/10.1007/978-3-032-02138-0\_7,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ecsa/SolimanAAKV25.bib,"Sat, 13 Sep 2025 01:00:00 +0200",Springer,"Vasilios Andrikopoulos and
Cesare Pautasso and
Nour Ali and
Jacopo Soldani and
Xiwei Xu",Lecture Notes in Computer Science,,,,,,
DBLP:conf/ectel/ThomasBBGGK25,inproceedings,"LLM-Generated Feedback Supports Learning if Learners Choose to Use
it","Danielle R. Thomas and
Conrad Borchers and
Shambhavi Bhushan and
Erin Gatz and
Shivang Gupta and
Kenneth R. Koedinger",2025,,"Two Decades of {TEL.} From Lessons Learnt to Challenges Ahead - 20th
European Conference on Technology Enhanced Learning, {EC-TEL} 2025,
Newcastle upon Tyne and Durham, UK, September 15-19, 2025, Proceedings,
Part {I}",16063,,489--503,10.1007/978-3-032-03870-8\_33,https://doi.org/10.1007/978-3-032-03870-8\_33,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ectel/ThomasBBGGK25.bib,"Sat, 13 Sep 2025 16:55:51 +0200",Springer,"Kairit Tammets and
Sergey A. Sosnovsky and
Rafael Ferreira Mello and
Gerti Pishtari and
Tanya Nazaretsky",Lecture Notes in Computer Science,,,,,,
DBLP:conf/edcc/Jananloo25,inproceedings,Software Trustworthiness Assessment via Large Language Models (LLMs),Saeed Javani Jananloo,2025,,"20th European Dependable Computing Conference, {EDCC} 2025 - Companion
Proceedings, Lisbon, Portugal, April 8-11, 2025",,,72--75,10.1109/EDCC-C66476.2025.00034,https://doi.org/10.1109/EDCC-C66476.2025.00034,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/edcc/Jananloo25.bib,"Tue, 16 Sep 2025 21:07:01 +0200",{IEEE},,,,,,,,"Nowadays, almost everything in our daily lives is handled by software. From transport systems and autonomous systems to healthcare and financial contracts. So, trustworthiness has become critical. Software trustworthiness refers to the degree to which a software system can be trusted, in the aspects of reliability and security. It means the ability to behave as expected even when facing faults, attacks, or unexpected conditions. Static Code Analysis (SCA) tools can be used to identify security issues, such as software vulnerabilities. Though, SCA tools have a great number of False Positives (FPs) and False Negatives (FNs). Consequently, developers do not use them or spend a lot of time analyzing the alerts reported by such tools without knowing that all vulnerabilities are identified. On the other hand, Large Language Models (LLMs) are being used in software engineering tasks, such as code generation, code review, code documentation, and even bug detection. These capabilities have attracted the interest of researchers to their potential for automating Software Trustworthiness Assessment (STA). As the performance of LLMs in STA is not satiable yet, there is an emerging need to obtain the best results out of their great potential. Although there are many works about the use of LLMs for the Software Vulnerability Detection (SVD), to the best of our knowledge, no other works have evaluated their ability to perform STA. This Ph.D. has 4 goals: 1. To evaluate the ability of LLMs to do the STA, 2. To develop a Software Characterization Mechanism based on LLMs, 3. To develop the Software Compliance and Risk Assessment Process supported by LLMs, and 4. To create an LLM-based tool for STA."
DBLP:conf/educon/MushtaqNG0H025,inproceedings,"Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving:
{A} Framework for Senior Design Projects","Abdullah Mushtaq and
Muhammad Rafay Naeem and
Ibrahim Ghaznavi and
Muhammad Imran Taj and
Imran Hashmi and
Junaid Qadir",2025,,"{IEEE} Global Engineering Education Conference, {EDUCON} 2025, London,
United Kingdom, April 22-25, 2025",,,1--10,10.1109/EDUCON62633.2025.11016653,https://doi.org/10.1109/EDUCON62633.2025.11016653,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/educon/MushtaqNG0H025.bib,"Fri, 13 Jun 2025 11:57:25 +0200",{IEEE},,,,,,,,"Multi-Agent Large Language Models (LLMs) are gaining attention for their ability to harness collective intelligence in complex problem-solving, decision-making, and planning tasks. This aligns with the wisdom of crowds concept, where diverse agents collectively generate effective solutions, making them well-suited for educational settings. Senior design projects, pivotal in engineering education, integrate theoretical knowledge with practical application, fostering critical thinking, teamwork, and real-world problem-solving skills. These projects often involve multidisciplinary considerations and conflicting objectives, such as optimizing technical performance while addressing ethical, social, and environmental concerns. In this paper, we explore a framework where distinct LLM agents embody expert perspectives, including problem formulation, system complexity, societal and ethical considerations, and project management. These agents engage in rich, collaborative dialogues, leveraging multi-agent system principles like coordination, cooperation, and negotiation. Prompt engineering is employed to create diverse personas, simulating human engineering teams and incorporating swarm AI principles to balance contributions efficiently. To evaluate the framework, we analyzed six senior capstone project proposals from engineering and computer science, comparing Multi-Agent and single-agent LLMs using metrics developed with engineering faculty and widely used NLP-based measures. These metrics assess technical quality, ethical considerations, social impact, and feasibility, aligning with the educational objectives of engineering design. Our findings suggest that Multi-Agent LLMs can provide a richer, more inclusive problem-solving environment compared to single-agent systems with 89% alignment with engineering-faculty scores, offering a promising tool for enhancing the educational experience of engineering and computer science students by simulating the complexity and collaboration of real-world engineering and computer science practice. By supporting senior design projects, this tool not only aids in achieving academic excellence but also prepares students for the multifaceted challenges they will face in their professional engineering careers. We have open-sourced our framework for further development and adaptation on GitHub11Copilot is available at GitHub Repository: https://github.com/AbdullahMushtaq78/Multi-Agent-SDP-Copliot."
DBLP:conf/eenergy/HewageIRB25,inproceedings,"Aging-aware {CPU} Core Management for Embodied Carbon Amortization
in Cloud {LLM} Inference","Tharindu B. Hewage and
Shashikant Ilager and
Maria Rodriguez Read and
Rajkumar Buyya",2025,,"Proceedings of the 16th {ACM} International Conference on Future and
Sustainable Energy Systems, E-Energy 2025, Rotterdam, The Netherlands,
June 17-20, 2025",,,43--55,10.1145/3679240.3734608,https://doi.org/10.1145/3679240.3734608,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/eenergy/HewageIRB25.bib,"Sun, 06 Jul 2025 01:00:00 +0200",{ACM},,,,,,,,"Broad adoption of Large Language Models (LLM) demands rapid expansions of cloud LLM inference clusters, leading to the accumulation of embodied carbon ‚àí the emissions from manufacturing and supplying IT assets ‚àí that mostly concentrate on inference server CPU. This paper delves into the challenges of sustainable growth of cloud LLM inference, emphasizing extended amortization of CPU embodied over an increased lifespan. Given the reliability risks of silicon aging, we propose an aging-aware CPU core management technique to delay CPU aging effects, allowing the cluster operator to safely increase CPU life. Our technique exploits CPU underutilization patterns that we uncover in cloud LLM inference by halting aging in unused cores and even-outing aging in active cores via selective deep idling and aging-aware inference task allocation. Through extensive simulations using real-world Azure inference traces and an extended LLM cluster simulator from Microsoft, we show superior performance of our technique over existing methods with an estimated 37.67% reduction in yearly embodied carbon emissions through p99 performance of managing CPU aging effects, a 77% reduction in CPU underutilization, and less than 10% impact to the inference service quality."
DBLP:conf/epia/PintoFSO25,inproceedings,"Prompting LLMs for Relation Classification in Portuguese: Is It Worth
It?","Tom{\'{a}}s Pinto and
Bruno Ferreira and
Catarina Silva and
Hugo Gon{\c{c}}alo Oliveira",2025,,"Progress in Artificial Intelligence - 24th {EPIA} Conference on Artificial
Intelligence, {EPIA} 2025, Faro, Portugal, October 1-3, 2025, Proceedings,
Part {II}",16122,,249--261,10.1007/978-3-032-05179-0\_19,https://doi.org/10.1007/978-3-032-05179-0\_19,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/epia/PintoFSO25.bib,"Tue, 07 Oct 2025 01:00:00 +0200",Springer,"Jos{\'{e}} Valente de Oliveira and
Jo{\~{a}}o Leite and
Jo{\~{a}}o Rodrigues and
Jo{\~{a}}o Dias and
Pedro Cardoso",Lecture Notes in Computer Science,,,,,,
DBLP:conf/europar/AngelopoulosMOS25,inproceedings,Cache Management for Mixture-of-Experts LLMs,"Spyros Angelopoulos and
Loris Marchal and
Adrien Obrecht and
Bertrand Simon",2025,,"Euro-Par 2025: Parallel Processing - 31st European Conference on Parallel
and Distributed Processing, Dresden, Germany, August 25-29, 2025,
Proceedings, Part {III}",15902,,18--32,10.1007/978-3-031-99872-0\_2,https://doi.org/10.1007/978-3-031-99872-0\_2,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/europar/AngelopoulosMOS25.bib,"Thu, 11 Sep 2025 01:00:00 +0200",Springer,"Wolfgang E. Nagel and
Diana Goehringer and
Pedro C. Diniz",Lecture Notes in Computer Science,,,,,,
DBLP:conf/europar/XiHZ25,inproceedings,CacheC: LLM-Based {GPU} Cache Management to Enhance Kernel Concurrency,"Mengyue Xi and
Jingyi He and
Xianwei Zhang",2025,,"Euro-Par 2025: Parallel Processing - 31st European Conference on Parallel
and Distributed Processing, Dresden, Germany, August 25-29, 2025,
Proceedings, Part {II}",15901,,118--131,10.1007/978-3-031-99857-7\_9,https://doi.org/10.1007/978-3-031-99857-7\_9,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/europar/XiHZ25.bib,"Thu, 11 Sep 2025 01:00:00 +0200",Springer,"Wolfgang E. Nagel and
Diana Goehringer and
Pedro C. Diniz",Lecture Notes in Computer Science,,,,,,
DBLP:conf/gi-ws/BourasMP25,inproceedings,Llm-Assisted Crossover in Genetic Improvement of Software,"Dimitrios Stamatios Bouras and
Sergey Mechtaev and
Justyna Petke",2025,,"{IEEE/ACM} International Workshop on Genetic Improvement, GI@ICSE
2025, Ottawa, ON, Canada, April 27, 2025",,,19--26,10.1109/GI66624.2025.00012,https://doi.org/10.1109/GI66624.2025.00012,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/gi-ws/BourasMP25.bib,"Tue, 05 Aug 2025 01:00:00 +0200",{IEEE},,,,,,,,"This study explores the use of Large Language Models to improve the crossover process in genetic programming, as applied in the genetic improvement domain. Traditional crossover techniques typically combine parent variants by selecting modifications uniformly or even randomly, without consideration of contextual relevance, often resulting in inefficient searches and suboptimal solutions due to incompatible or redundant modifications. In contrast, our LLM-assisted crossover leverages context to select and combine edits from parent solutions that are more likely to work well together, with the goal of producing higher quality variants, accelerating optimization. We implemented this approach within MAGPIE, a unified genetic improvement framework. We evaluated against five traditional crossover methods across seven benchmarks, measuring performance on four key metrics: average ranking, best variant execution time, efficiency in reaching performance milestones, and viable variant count. Results show that LLM-assisted crossover achieved an average ranking of 2.27 (on a scale where 1 is best and 6 is worst), making it the top-performing method across benchmarks based on the quality of the optimal variants produced. The LLM-based approach also improved the fitness (execution time) by an average of 8.5 % over the best variant produced by the traditional methods. In terms of efficiency, the LLM-assisted crossover required on average 25.6 % fewer variants to reach $25 \%, 50 \%, 75 \%$, and 100 % of the final performance improvement compared to the traditional methods. Additionally, the LLM-assisted crossover produced 4.8 % more viable variants across scenarios, including both source code modification and parameter tuning cases. These findings suggest that LLMs can significantly enhance genetic improvement by guiding the crossover process toward more effective and viable solutions, providing motivation for further research in LLM-assisted search algorithms."
DBLP:conf/hci/WenVW25,inproceedings,"LLM-Assisted Collaborative Change Specification of Industrial Control
Software","Ziming Wen and
Birgit Vogel{-}Heuser and
Jan Wilch",2025,,"{HCI} in Business, Government and Organizations - 12th International
Conference, {HCIBGO} 2025, Held as Part of the 27th {HCI} International
Conference, {HCII} 2025, Gothenburg, Sweden, June 22-27, 2025, Proceedings,
Part {I}",15804,,242--255,10.1007/978-3-031-92823-9\_16,https://doi.org/10.1007/978-3-031-92823-9\_16,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/hci/WenVW25.bib,"Sun, 06 Jul 2025 01:00:00 +0200",Springer,"Keng Leng Siau and
Fiona Fui{-}Hoon Nah",Lecture Notes in Computer Science,,,,,,
DBLP:conf/icaiic/KomanduriEAYFRD25,inproceedings,"Optimizing {LLM} Prompts for Automation of Network Management: {A}
User's Perspective","Vishnu Komanduri and
Sebastian Estropia and
Scott Alessio and
Gokhan Yerdelen and
Tyler Ferreira and
Geovanny Palomino Roldan and
Ziqian Dong and
Roberto Rojas{-}Cessa",2025,,"International Conference on Artificial Intelligence in Information
and Communication, {ICAIIC} 2025, Fukuoka, Japan, February 18-21,
2025",,,958--963,10.1109/ICAIIC64266.2025.10920709,https://doi.org/10.1109/ICAIIC64266.2025.10920709,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icaiic/KomanduriEAYFRD25.bib,"Fri, 11 Jul 2025 09:24:55 +0200",{IEEE},,,,,,,,"Although being linguistic assemblers and decomposers, large language models (LLMs) have found their use in analysis and generation of code, art, and design of electronic circuitry, industrial parts, and network design. However, the probabilistic nature of generative LLMs makes network design and implementation scenarios prone to errors. The complexity levels of design may exacerbate the number of inaccuracies included in an LLM's response. Therefore, it is necessary to identify the features that make prompts generate effective and error-free responses as users. To reduce the error rate of the responses, we test prompt specificity in text and schematic descriptions. As various degrees of specificity, we compare highly intuitive to highly specific prompting. The responses are expressed as network schematics and router configuration commands that are evaluated with our proposed scoring policy. Our tests include networks with three levels of complexity and multiple levels of specificity in text and graphic prompts. The results show the trade-offs on the text and graphic modes and the degrees of specificity."
DBLP:conf/icccn/NeupaneKPPCQ25,inproceedings,NetPrompt: LLM-driven Programmable Network Policy Management and Optimization,"Kiran Neupane and
Kevin Kostage and
Sean Peppers and
Ashish Pandey and
Prasad Calyam and
Chengyi Qu",2025,,"34th International Conference on Computer Communications and Networks,
{ICCCN} 2025, Tokyo, Japan, August 4-7, 2025",,,1--9,10.1109/ICCCN65249.2025.11133886,https://doi.org/10.1109/ICCCN65249.2025.11133886,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icccn/NeupaneKPPCQ25.bib,"Tue, 16 Sep 2025 21:07:04 +0200",{IEEE},,,,,,,,"Software-Defined Networking (SDN) requires adaptive policy generation to ensure satisfactory Quality of Service (QoS) and Quality of Experience (QoE) expectations under dynamic network conditions. While generative AI can potentially automate the optimization of network configuration, there is a lack of methods for AI-driven policy automation and enforcement, particularly in translating high-level network intent into suitable service function chains using P4 switch configurations without misconfigurations. In this paper, we present a novel framework, viz., NetPrompt, that uses Large Language Models (LLMs) for automated and intent-driven policy generation in SDN in the context of a video streaming application. By integrating prompt engineering and structured model refinement, pre-trained NetPrompt adaptively selects the appropriate LLM configuration to generate suitable P4 scripts that align with user requirements, such as dynamic QoS adaptation. We validate NetPrompt in network emulators and advanced compute/network testbed environments, including Mininet, Chameleon Cloud, and FABRIC, to construct practical network topologies for evaluation against key performance metrics such as latency reduction, throughput improvement, and error rate minimization. Our experimental results demonstrate that NetPrompt reduces misconfigurations significantly, showcasing its potential in dynamic policy management of programmable networks."
DBLP:conf/iccel/SeoJHLN25,inproceedings,"Effective Activation Scratchpad Management in NPUs for LLMs via Inter-
and Intra-Decoder Allocation","Minseok Seo and
Seongho Jeong and
Jungi Hyun and
Hyuk{-}Jae Lee and
Xuan Truong Nguyen",2025,,"{IEEE} International Conference on Consumer Electronics, {ICCE} 2025,
Las Vegas, NV, USA, January 11-14, 2025",,,1--3,10.1109/ICCE63647.2025.10930181,https://doi.org/10.1109/ICCE63647.2025.10930181,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iccel/SeoJHLN25.bib,"Fri, 06 Jun 2025 12:29:37 +0200",{IEEE},,,,,,,,"To accelerate emerging Large Language Models (LLMs), modern Neural Processing Units (NPUs) typically adopt a scratchpad memory for activations as on-chip memory. However, managing an activation scratchpad in NPUs becomes challenge, as the size of LLMs and their context sequence lengths continue to grow. Suboptimal activation memory management may cause low on-chip memory usage and excessive off-chip memory accesses due to large activation data, leading to significant performance degradation during LLM inference on NPUs. This challenge necessitates careful methods to maximize on-chip memory utilization. To address the problem, this work proposes an effective activation scratchpad management in NPUs for LLMs via inter-and intra-decoder allocation. Specifically, inter-decoder optimization facilitates the sharing of on-chip memory across stacked decoders, and intra-decoder one interleaves on-chip memory usage among operations in a decoder. We evaluate our approach with OPT models. The experimental results demonstrate that the proposed approach achieves 56‚Äì79 √ó reduction in memory usage to na√Øve on-chip memory management methods."
DBLP:conf/iccnc/AlamS25,inproceedings,"Enhancing Network Intelligence with LLM-Based {IBN} and {DRL:} {A}
Dynamic Approach for {SAGIN} Resource Management","Sajid Alam and
Wang{-}Cheol Song",2025,,"International Conference on Computing, Networking and Communications,
{ICNC} 2025, Honolulu, HI, USA, February 17-20, 2025",,,723--727,10.1109/ICNC64010.2025.10994008,https://doi.org/10.1109/ICNC64010.2025.10994008,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iccnc/AlamS25.bib,"Wed, 28 May 2025 11:02:27 +0200",{IEEE},,,,,,,,"We propose an innovative Intent-Based Networking (IBN) system that leverages Large Language Models (LLMs) to translate high-level user intents into actionable network policies. By integrating an LLM-based translation engine with a Deep Reinforcement Learning (DRL) agent, our system enables dynamic and intelligent resource allocation within Space-Air-Ground Integrated Networks (SAGINs). The LLM interprets user requests expressed in natural language to generate precise network configurations, which the DRL agent uses to optimize resource utilization while meeting Quality of Service (QoS) requirements. Simulations demonstrate that our approach effectively reduces latency, enhances bandwidth utilization, minimizes QoS violations, and increases Virtual Network Request (VNR) acceptance rates compared to static allocation strategies. These results highlight the potential of combining LLMs with DRL in IBN systems to improve network adaptability, efficiency, and user satisfaction."
DBLP:conf/icde/KhanWZTO25,inproceedings,"{LLM} + Vector Data: Coupling of Large Language Models with Vector
Data Management for Enhancing Data Science","Arijit Khan and
Yuxiang Wang and
Weixi Zhang and
Yao Tian and
M. Tamer {\""{O}}zsu",2025,,"41st {IEEE} International Conference on Data Engineering, {ICDE} 2025
- Workshops, Hong Kong, May 19-23, 2025",,,93--96,10.1109/ICDEW67478.2025.00018,https://doi.org/10.1109/ICDEW67478.2025.00018,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icde/KhanWZTO25.bib,"Wed, 10 Sep 2025 01:00:00 +0200",{IEEE},,,,,,,,
DBLP:conf/icde/LuNLPZZCZDCZ25,inproceedings,"DataSculpt: {A} Holistic Data Management Framework for Long-Context
LLMs Training","Keer Lu and
Xiaonan Nie and
Zheng Liang and
Da Pan and
Shusen Zhang and
Keshi Zhao and
Weipeng Chen and
Zenan Zhou and
Guosheng Dong and
Bin Cui and
Wentao Zhang",2025,,"41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
Hong Kong, May 19-23, 2025",,,4234--4247,10.1109/ICDE65448.2025.00316,https://doi.org/10.1109/ICDE65448.2025.00316,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icde/LuNLPZZCZDCZ25.bib,"Fri, 05 Sep 2025 21:24:36 +0200",{IEEE},,,,,,,,"In recent years, foundation models, particularly large language models (LLMs), have demonstrated significant improvements across a variety of tasks. One of their most important features is long-context capability, which enables them to generate extended text with high semantic coherence, retrieving relevant information, and handling tasks with substantial amounts of text efficiently. The key to improving long-context performance lies in effective data organization and management strategies that integrate data from multiple domains and optimize the context window during training. Through extensive experimental analysis, we identified three key challenges in designing effective data management strategies that enable the model to achieve long-context capability without sacrificing performance in other tasks: (1) a shortage of long documents across multiple domains, (2) effective construction of context windows, and (3) efficient organization of large-scale datasets. To address these challenges, we introduce DataSculpt, a novel data management framework designed for long-context training. We first formulate the organization of training data as a multi-objective optimization problem, focusing on attributes including the relevance among documents within the same training sequence, the quantity of concatenated instances, individual document integrity, and computational cost. Specifically, our approach utilizes a coarse-to-fine method to optimize training data organization effectively. We begin by clustering the data based on semantic similarity (coarse), followed by a multi-objective greedy search within each cluster to score and concatenate documents into various context windows (fine). We have deployed DataSculpt as the data management backend for long-context training in Baichuan Inc. Extensive experiments with diverse downstream tasks show that DataSculpt enhances the model's long-context performance by an average of 15.73%, while maintaining the general capabilities with a 4.63% improvement."
DBLP:conf/icde/SuiWCXHZZYSP25,inproceedings,"Bridging the Gap: LLM-Powered Transfer Learning for Log Anomaly Detection
in New Software Systems","Yicheng Sui and
Xiaotian Wang and
Tianyu Cui and
Tong Xiao and
Chenghao He and
Shenglin Zhang and
Yuzhi Zhang and
Xiao Yang and
Yongqian Sun and
Dan Pei",2025,,"41st {IEEE} International Conference on Data Engineering, {ICDE} 2025,
Hong Kong, May 19-23, 2025",,,4414--4427,10.1109/ICDE65448.2025.00331,https://doi.org/10.1109/ICDE65448.2025.00331,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icde/SuiWCXHZZYSP25.bib,"Fri, 05 Sep 2025 01:00:00 +0200",{IEEE},,,,,,,,
DBLP:conf/iceis/StergiopoulosVT25,inproceedings,"Conference Management System Utilizing an LLM-Based Recommendation
System for the Reviewer Assignment Problem","Vaios Stergiopoulos and
Michael Vassilakopoulos and
Eleni Tousidou and
Spyridon Kavvathas and
Antonio Corral",2025,,"Proceedings of the 27th International Conference on Enterprise Information
Systems, {ICEIS} 2025, Porto, Portugal, April 4-6, 2025, Volume 1",,,1004--1011,10.5220/0013482600003929,https://doi.org/10.5220/0013482600003929,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iceis/StergiopoulosVT25.bib,"Fri, 02 May 2025 13:02:57 +0200",{SCITEPRESS},"Joaquim Filipe and
Michal Smialek and
Alexander Brodsky and
Slimane Hammoudi",,,,,,,": One of the most important tasks of a conference organizer is to assign reviewers to papers. The peer review process of the submitted papers is a crucial step in determining the conference agenda, quality, and success. However, this is not an easy task; large conferences often assign hundreds of papers to hundreds of reviewers, making it impossible for a single person to complete the task due to hard time constraints. We propose a Conference Management System that embodies a Large Language Model (LLM) in its core. The LLM is utilized as a Recommendation System which applies Content-based Filtering and automates the task of reviewers-to-papers assignment for a conference. The LLM we select to use is the Bidirectional Encoder Representations from Transformers (BERT), in two specific variants, BERT-tiny and BERT-large."
DBLP:conf/iclr/PatnaikABK25,inproceedings,"It Helps to Take a Second Opinion: Teaching Smaller LLMs To Deliberate
Mutually via Selective Rationale Optimisation","Sohan Patnaik and
Milan Aggarwal and
Sumit Bhatia and
Balaji Krishnamurthy",2025,,"The Thirteenth International Conference on Learning Representations,
{ICLR} 2025, Singapore, April 24-28, 2025",,,,,https://openreview.net/forum?id=NHxwxc3ql6,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iclr/PatnaikABK25.bib,"Thu, 10 Jul 2025 01:00:00 +0200",OpenReview.net,,,,,,,,
DBLP:conf/iclr/SunAZMVRK025,inproceedings,How new data permeates {LLM} knowledge and how to dilute it,"Chen Sun and
Renat Aksitov and
Andrey Zhmoginov and
Nolan Andrew Miller and
Max Vladymyrov and
Ulrich Rueckert and
Been Kim and
Mark Sandler",2025,,"The Thirteenth International Conference on Learning Representations,
{ICLR} 2025, Singapore, April 24-28, 2025",,,,,https://openreview.net/forum?id=NGKQoaqLpo,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iclr/SunAZMVRK025.bib,"Thu, 15 May 2025 01:00:00 +0200",OpenReview.net,,,,,,,,
DBLP:conf/iclr/SunZJ25,inproceedings,"Empowering Users in Digital Privacy Management through Interactive
LLM-Based Agents","Bolun Sun and
Yifan Zhou and
Haiyun Jiang",2025,,"The Thirteenth International Conference on Learning Representations,
{ICLR} 2025, Singapore, April 24-28, 2025",,,,,https://openreview.net/forum?id=FEpAUnS7f7,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iclr/SunZJ25.bib,"Thu, 15 May 2025 01:00:00 +0200",OpenReview.net,,,,,,,,
DBLP:conf/iclr/WangCG25,inproceedings,"SqueezeAttention: 2D Management of KV-Cache in {LLM} Inference via
Layer-wise Optimal Budget","Zihao Wang and
Bin Cui and
Shaoduo Gan",2025,,"The Thirteenth International Conference on Learning Representations,
{ICLR} 2025, Singapore, April 24-28, 2025",,,,,https://openreview.net/forum?id=9HK2rHNAhd,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iclr/WangCG25.bib,"Thu, 15 May 2025 01:00:00 +0200",OpenReview.net,,,,,,,,
DBLP:conf/icsa/BucaioniWHL025,inproceedings,A Functional Software Reference Architecture for LLM-Integrated Systems,"Alessio Bucaioni and
Martin Weyssow and
Junda He and
Yunbo Lyu and
David Lo",2025,,"22nd {IEEE} International Conference on Software Architecture, {ICSA}
- Companion, Odense, Denmark, March 31 - April 4, 2025",,,1--5,10.1109/ICSA-C65153.2025.00006,https://doi.org/10.1109/ICSA-C65153.2025.00006,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icsa/BucaioniWHL025.bib,"Thu, 12 Jun 2025 13:20:19 +0200",{IEEE},,,,,,,,"The integration of large language models into software systems is transforming capabilities such as natural language understanding, decision-making, and autonomous task execution. However, the absence of a commonly accepted software reference architecture hinders systematic reasoning about their design and quality attributes. This gap makes it challenging to address critical concerns like privacy, security, modularity, and interoperability, which are increasingly important as these systems grow in complexity and societal impact. In this paper, we describe our emerging results for a preliminary functional reference architecture as a conceptual framework to address these challenges and guide the design, evaluation, and evolution of large language model-integrated systems. We identify key architectural concerns for these systems, informed by current research and practice. We then evaluate how the architecture addresses these concerns and validate its applicability using three open-source large language model-integrated systems in computer vision, text processing, and coding."
DBLP:conf/icsa/TagliaferroCG25,inproceedings,"Leveraging LLMs to Automate Software Architecture Design from Informal
Specifications","Alberto Tagliaferro and
Simone Corbo and
Bruno Guindani",2025,,"22nd {IEEE} International Conference on Software Architecture, {ICSA}
- Companion, Odense, Denmark, March 31 - April 4, 2025",,,291--299,10.1109/ICSA-C65153.2025.00049,https://doi.org/10.1109/ICSA-C65153.2025.00049,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icsa/TagliaferroCG25.bib,"Sat, 06 Sep 2025 01:00:00 +0200",{IEEE},,,,,,,,"Designing a software architecture starting from specifications and requirements is a time-consuming and errorprone process that demands domain expertise. Automating this process has become a significant research focus in software engineering. Traditional approaches rely on rule-based mechanisms to translate manually derived, standardized requirements into the desired architecture. However, these methods struggle to identify implicit patterns without expert intervention. Recently, approaches leveraging Large Language Models (LLMs) have gained attention. This study evaluates the performance of LLMs in generating software architecture blueprints, specifically UML component diagrams, from informal natural-language specifications. We develop a formal characterization of component diagrams to derive quantitative metrics for analyzing LLM-generated diagrams, comparing them against expert-drawn ground truths associated with the specifications. Our findings indicate that while LLM-based approaches show promise in addressing the flaws of rule-based methods, they currently lack the accuracy needed for deployment in real-world scenarios."
DBLP:conf/icse-seip/NaharKBPZB25,inproceedings,"Beyond the Comfort Zone: Emerging Solutions to Overcome Challenges
in Integrating LLMs into Software Products","Nadia Nahar and
Christian K{\""{a}}stner and
Jenna L. Butler and
Chris Parnin and
Thomas Zimmermann and
Christian Bird",2025,,"47th {IEEE/ACM} International Conference on Software Engineering:
Software Engineering in Practice, SEIP@ICSE 2025, Ottawa, ON, Canada,
April 27 - May 3, 2025",,,516--527,10.1109/ICSE-SEIP66354.2025.00051,https://doi.org/10.1109/ICSE-SEIP66354.2025.00051,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icse-seip/NaharKBPZB25.bib,"Sun, 07 Sep 2025 20:27:02 +0200",{IEEE},,,,,,,,"Large Language Models (LLMs) are increasingly embedded into software products across diverse industries, en-hancing user experiences, but at the same time introducing numerous challenges for developers. Unique characteristics of LLMs force developers, who are accustomed to traditional software development and evaluation, out of their comfort zones as the LLM components shatter standard assumptions about software systems. This study explores the emerging solutions that software developers are adopting to navigate the encountered challenges. Leveraging a mixed-method research, including 26 interviews and a survey with 182 responses, the study identifies 19 emerging solutions regarding quality assurance that practitioners across several product teams at Microsoft are exploring. The findings provide valuable insights that can guide the development and evaluation of LLM-based products more broadly in the face of these challenges."
DBLP:conf/icse/BanoGH25,inproceedings,"What Does a Software Engineer Look Like? Exploring Societal Stereotypes
in LLMs","Muneera Bano and
Hashini Gunatilake and
Rashina Hoda",2025,,"47th {IEEE/ACM} International Conference on Software Engineering:
Software Engineering in Society, {ICSE-SEIS} 2025, Ottawa, ON, Canada,
April 27 - May 3, 2025",,,173--184,10.1109/ICSE-SEIS66351.2025.00023,https://doi.org/10.1109/ICSE-SEIS66351.2025.00023,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icse/BanoGH25.bib,"Mon, 30 Jun 2025 21:48:33 +0200",{IEEE},,,,,,,,"Large language models (LLMs) have rapidly gained popularity and are being embedded into professional applications due to their capabilities in generating human-like content. However, unquestioned reliance on their outputs and recommendations can be problematic as LLMs can reinforce societal biases and stereotypes. This study investigates how LLMs, specifically OpenAI's GPT-4 and Microsoft Copilot, can reinforce gender and racial stereotypes within the software engineering (SE) profession through both textual and graphical outputs. We used each LLM to generate 300 profiles, consisting of 100 gender-based and 50 gender-neutral profiles, for a recruitment scenario in SE roles. Recommendations were generated for each profile and evaluated against the job requirements for four distinct SE positions. Each LLM was asked to select the top 5 candidates and subsequently the best candidate for each role. Each LLM was also asked to generate images for the top 5 candidates, providing a dataset for analysing potential biases in both text-based selections and visual representations. Our analysis reveals that both models preferred male and Caucasian profiles, particularly for senior roles, and favoured images featuring traits such as lighter skin tones, slimmer body types, and younger appearances. These findings highlight underlying societal biases influence the outputs of LLMs, contributing to narrow, exclusionary stereotypes that can further limit diversity and perpetuate inequities in the SE field. As LLMs are increasingly adopted within SE research and professional practices, awareness of these biases is crucial to prevent the reinforcement of discriminatory norms and to ensure that AI tools are leveraged to promote an inclusive and equitable engineering culture rather than hinder it."
DBLP:conf/icse/GuerraE25,inproceedings,Assessing LLMs for Front-end Software Architecture Knowledge,"Luiz Pedro Franciscatto Guerra and
Neil A. Ernst",2025,,"{IEEE/ACM} International Workshop on Designing Software, Designing@ICSE
2025, Ottawa, ON, Canada, April 27-28, 2025",,,6--10,10.1109/DESIGNING66910.2025.00007,https://doi.org/10.1109/Designing66910.2025.00007,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icse/GuerraE25.bib,"Mon, 30 Jun 2025 21:48:33 +0200",{IEEE},,,,,,,,"Large Language Models (LLMs) have demonstrated significant promise in automating software development tasks, yet their capabilities with respect to software design tasks remains largely unclear. This study investigates the capabilities of an LLM in understanding, reproducing, and generating structures within the complex VIPER architecture, a design pattern for iOS applications. We leverage Bloom's taxonomy to develop a comprehensive evaluation framework to assess the LLM's performance across different cognitive domains such as remembering, understanding, applying, analyzing, evaluating, and creating. Experimental results, using ChatGPT 4 Turbo 2024-04-09, reveal that the LLM excelled in higher-order tasks like evaluating and creating, but faced challenges with lower-order tasks requiring precise retrieval of architectural details. These findings highlight both the potential of LLMs to reduce development costs and the barriers to their effective application in real-world software design scenarios. This study proposes a benchmark format for assessing LLM capabilities in software architecture, aiming to contribute toward more robust and accessible AI-driven development tools."
DBLP:conf/icse/KimKCCMLP25,inproceedings,"Multi-Modal LLM-Based Fully-Automated Training Dataset Generation
Software Platform for Mathematics Education","Minjoo Kim and
Taehyun Kim Kim and
Jaehyun Chung and
Hyunseok Choi and
Seokhyeon Min and
Joon{-}Ho Lim and
Soohyun Park",2025,,"47th {IEEE/ACM} International Conference on Software Engineering:
Software Engineering in Society, {ICSE-SEIS} 2025, Ottawa, ON, Canada,
April 27 - May 3, 2025",,,1--10,10.1109/ICSE-SEIS66351.2025.00022,https://doi.org/10.1109/ICSE-SEIS66351.2025.00022,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icse/KimKCCMLP25.bib,"Mon, 30 Jun 2025 01:00:00 +0200",{IEEE},,,,,,,,"Due to the academic and commercial successes in large-language model (LLM) software research and development, there are a lot of activities to utilize this technology. Accordingly, many successful software have been released and developed for various social applications. Among them, mathematics education is one of emerging social applications which is obviously helpful for social welfare. Aligned with the development directions of LLM technologies, the use of direct preference optimization (DPO) is considered. However, one of the biggest hurdles is the lack of training dataset. Therefore, this research introduces fullyautomated training dataset generation using the advanced form of LLM, i.e., multi-modal LLM. Based on various generation results based on our multi-modal LLM, various discussions and analysis results are provided. Lastly, it has to be noted that our proposed platform can contribute to providing fair education opportunities for diverse human beings without discrimination, which is definitely beneficial for social welfare."
DBLP:conf/icse/MougoueiAFMDKRKA25,inproceedings,"A First Look at {AI} Trends in Value-Aligned Software Engineering
Publications: Human-LLM Insights","Davoud Mougouei and
Ahmad Azarnik and
Mahdi Fahmideh and
Elahe Mougouei and
Hoa Khanh Dam and
Arif Ali Khan and
Saima Rafi and
Javed Ali Khan and
Aakash Ahmad",2025,,"47th {IEEE/ACM} International Conference on Software Engineering:
Software Engineering in Society, {ICSE-SEIS} 2025, Ottawa, ON, Canada,
April 27 - May 3, 2025",,,82--93,10.1109/ICSE-SEIS66351.2025.00014,https://doi.org/10.1109/ICSE-SEIS66351.2025.00014,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icse/MougoueiAFMDKRKA25.bib,"Mon, 30 Jun 2025 01:00:00 +0200",{IEEE},,,,,,,,"Recent criticism of social media platforms by the U.S. Senate Judiciary Committee for neglecting child safety exemplifies how software can undermine human values. This is further complicated by the growing integration of Artificial Intelligence (AI) in software, which introduces inherent challenges such as biases and limited transparency. However, AI also presents opportunities to embed human values into software. To explore these opportunities, we have utilized the reasoning abilities of ChatGPT, a large language model (LLM), in combination with human expertise, to study the use of AI in publications that address human values, across some of the leading software engineering (SE) venues from 2022 to 2023. Our findings confirm the use of AI concepts - mainly General Machine Learning - in around 33 % of these valuealigned publications. The value alignments largely concern pragmatic aspects of Achievement and (personal) Security, while the majority of the values receive less attention. The socially focused values of Conformity and Tradition and the personally focused value of Hedonism are rarely addressed in the SE publications."
DBLP:conf/icse/Shao0S00025,inproceedings,Are LLMs Correctly Integrated into Software Systems?,"Yuchen Shao and
Yuheng Huang and
Jiawei Shen and
Lei Ma and
Ting Su and
Chengcheng Wan",2025,,"47th {IEEE/ACM} International Conference on Software Engineering,
{ICSE} 2025, Ottawa, ON, Canada, April 26 - May 6, 2025",,,1178--1190,10.1109/ICSE55347.2025.00204,https://doi.org/10.1109/ICSE55347.2025.00204,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icse/Shao0S00025.bib,"Mon, 30 Jun 2025 11:40:51 +0200",{IEEE},,,,,,,,"Large language models (LLMs) provide effective solutions in various application scenarios, with the support of retrieval-augmented generation (RAG). However, developers face challenges in integrating LLM and RAG into software systems, due to lacking interface specifications, various requirements from software context, and complicated system management. In this paper, we have conducted a comprehensive study of 100 open-source applications that incorporate LLMs with RAG support, and identified 18 defect patterns. Our study reveals that 77 % of these applications contain more than three types of integration defects that degrade software functionality, efficiency, and security. Guided by our study, we propose systematic guidelines for resolving these defects in software life cycle. We also construct an open-source defect library HYDRANGEA [1]."
DBLP:conf/icse/Wang25,inproceedings,"Identifying Performance-Sensitive Configurations in Software Systems
with LLM-Driven Agents",Zehao Wang,2025,,"47th {IEEE/ACM} International Conference on Software Engineering,
{ICSE} 2025 - Companion Proceedings, Ottawa, ON, Canada, April 27
- May 3, 2025",,,222--223,10.1109/ICSE-COMPANION66252.2025.00069,https://doi.org/10.1109/ICSE-Companion66252.2025.00069,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icse/Wang25.bib,"Tue, 24 Jun 2025 15:49:19 +0200",{IEEE},,,,,,,,"Configuration settings are essential for tailoring software behavior to meet specific performance requirements. However, incorrect configurations are common, and identifying those that impact system performance is challenging due to the vast number and complexity of possible settings. In this work, we present PerfAware, a lightweight framework that leverages Large Language Models (LLMs) to efficiently identify performance-sensitive configurations with minimal overhead. PerfAware employs LLM agents to simulate interactions between developers and performance engineers using advanced prompting techniques such as prompt chaining and retrieval-augmented generation (RAG). Our evaluation of seven open-source Java systems demon-strates that PerfAware achieves an average accuracy of 70.55% in classifying performance-sensitive configurations, outperforming both our LLM baseline (58.47%) and the previous state-of-the-art method (60.87%). Notably, our prompt chaining technique improves precision significantly while maintaining similar recall levels. In summary, PerfAware significantly reduces manual effort in classifying performance-sensitive configurations and offers valuable insights for future LLM-based code analysis research."
DBLP:conf/icse/WangYFP25,inproceedings,"Automating a Complete Software Test Process Using LLMs: An Automotive
Case Study","Shuai Wang and
Yinan Yu and
Robert Feldt and
Dhasarathy Parthasarathy",2025,,"47th {IEEE/ACM} International Conference on Software Engineering,
{ICSE} 2025, Ottawa, ON, Canada, April 26 - May 6, 2025",,,373--384,10.1109/ICSE55347.2025.00211,https://doi.org/10.1109/ICSE55347.2025.00211,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icse/WangYFP25.bib,"Mon, 30 Jun 2025 01:00:00 +0200",{IEEE},,,,,,,,"Vehicle API testing verifies whether the interactions between a vehicle's internal systems and external applications meet expectations, ensuring that users can access and control various vehicle functions and data. However, this task is inherently complex, requiring the alignment and coordination of API systems, communication protocols, and even vehicle simulation systems to develop valid test cases. In practical industrial scenarios, inconsistencies, ambiguities, and interde-pendencies across various documents and system specifications pose significant challenges. This paper presents a system designed for the automated testing of in-vehicle APIs. By clearly defining and segmenting the testing process, we enable Large Language Models (LLMs) to focus on specific tasks, ensuring a stable and controlled testing workflow. Experiments conducted on over 100 APIs demonstrate that our system effectively automates vehicle API testing. The results also confirm that LLMs can efficiently handle mundane tasks requiring human judgment, making them suitable for complete automation in similar industrial contexts."
DBLP:conf/icsoft/CramerM25,inproceedings,"Verifying LLM-Generated Code in the Context of Software Verification
with Ada/SPARK","Marcos Cramer and
Lucian McIntyre",2025,,"Proceedings of the 20th International Conference on Software Technologies,
{ICSOFT} 2025, Bilbao, Spain, June 10-12, 2025",,,39--50,10.5220/0013461900003964,https://doi.org/10.5220/0013461900003964,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icsoft/CramerM25.bib,"Thu, 17 Jul 2025 16:42:43 +0200",{SCITEPRESS},"Massimo Mecella and
Arend Rensink and
Leszek A. Maciaszek",,,,,,,
DBLP:conf/icst/BaldonadoBB25,inproceedings,"Towards a Probabilistic Framework for Analyzing and Improving LLM-Enabled
Software","Juan Manuel Baldonado and
Flavia Bonomo{-}Braberman and
V{\'{\i}}ctor A. Braberman",2025,,"{IEEE} International Conference on Software Testing, Verification
and Validation, {ICST} 2025 - Workshops, Naples, Italy, March 31 -
April 4, 2025",,,418--422,10.1109/ICSTW64639.2025.10962470,https://doi.org/10.1109/ICSTW64639.2025.10962470,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icst/BaldonadoBB25.bib,"Wed, 30 Apr 2025 09:20:45 +0200",{IEEE},,,,,,,,"Ensuring the reliability and verifiability of large language model (LLM)-enabled systems remains a significant challenge in software engineering. We propose a probabilistic framework for systematically analyzing and improving these systems by modeling and refining distributions over clusters of semantically equivalent outputs. This framework facilitates the evaluation and iterative improvement of Transference Models‚Äì key software components that utilize LLMs to transform inputs into outputs for downstream tasks. To illustrate its utility, we apply the framework to the autoformalization problem, where natural language documentation is transformed into formal program specifications. Our case illustrates how distribution-aware analysis enables the identification of weaknesses and guides focused alignment improvements, resulting in more reliable and interpretable outputs. This principled approach offers a foundation for addressing critical challenges in the development of robust LLM-enabled systems."
DBLP:conf/ieaaie/HajjarzadehAF25,inproceedings,"LLM-Based MaSE, {A} Software Development Framework for Developing
Multi-agent Systems","Sahar Hajjarzadeh and
Zahra Shakeri Hossein Abad and
Behrouz H. Far",2025,,"Advances and Trends in Artificial Intelligence. Theory and Applications
- 38th International Conference on Industrial, Engineering and Other
Applications of Applied Intelligent Systems, {IEA/AIE} 2025, Kitakyushu,
Japan, July 1-4, 2025, Proceedings, Part {I}",15706,,432--443,10.1007/978-981-96-8889-0\_37,https://doi.org/10.1007/978-981-96-8889-0\_37,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ieaaie/HajjarzadehAF25.bib,"Thu, 17 Jul 2025 14:25:02 +0200",Springer,"Hamido Fujita and
Yutaka Watanobe and
Moonis Ali and
Yinglin Wang",Lecture Notes in Computer Science,,,,,,
DBLP:conf/ieeecai/QinYK25,inproceedings,Graph LLM-Based Portfolio Management Algorithm,"Dayu Qin and
Yi Yan and
Ercan E. Kuruoglu",2025,,"{IEEE} Conference on Artificial Intelligence, {CAI} 2025, Santa Clara,
CA, USA, May 5-7, 2025",,,157--160,10.1109/CAI64502.2025.00032,https://doi.org/10.1109/CAI64502.2025.00032,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ieeecai/QinYK25.bib,"Thu, 09 Oct 2025 18:08:37 +0200",{IEEE},,,,,,,,"This paper explores the integration of large language models (LLMs) with graph-based financial networks for quantitative trading. By leveraging GPT-3 for stock network return predictions, we develop a Graph-LLM trading strategy. Experimental results demonstrate that the proposed strategy achieves lower volatility and more stable performance than traditional baselines. Our findings highlight the potential of combining LLMs with financial complex networks to enhance quantitative trading strategies."
DBLP:conf/ieeecai/SunH25,inproceedings,"Engineering Environmental Legislative-Guided {LLM} to Support Waste
Management Learning","Qiming Sun and
I{-}Han Hsiao",2025,,"{IEEE} Conference on Artificial Intelligence, {CAI} 2025, Santa Clara,
CA, USA, May 5-7, 2025",,,364--369,10.1109/CAI64502.2025.00066,https://doi.org/10.1109/CAI64502.2025.00066,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ieeecai/SunH25.bib,"Thu, 09 Oct 2025 01:00:00 +0200",{IEEE},,,,,,,,"This study presents Waste Genie+, an AI-infused web-based educational technology to improve sustainability awareness and waste management skills. Waste Genie+ uses Large Language Models (LLMs) to transform complex environmental regulations and waste management information into accessible and digestible content. The platform features AI-infused content, interactive quizzes, and various sustainability awareness simulations. We utilized LLM chain-of-thought reasoning to implement a systematic prompt-based evaluation of the decomposed legislative content quality. Following the content and intelligent agent evaluation, a 10-day user study with 10 participants was designed and conducted to evaluate the effectiveness of Waste Genie+ in improving users' understanding of waste management practices and regulations. Results demonstrated that our Environmental Legislative-guided LLM successfully extracted coherent, highly readable content while preserving the original legal information. The user study uncovered significant improvement in the participants' waste management skills. It also revealed that users found that the quizzes were engaging and the AI-generated bite-sized content was more digestible and easier to understand compared to the original bills or articles. These findings contributed to our understanding of how AI-enhanced educational technologies can promote environmental stewardship and facilitate public awareness of sustainability practices and policies."
DBLP:conf/ijcai/BaiXWY0025,inproceedings,"{POLO:} An LLM-Powered Project-Level Code Performance Optimization
Framework","Jiameng Bai and
Ruoyi Xu and
Sai Wu and
Dingyu Yang and
Junbo Zhao and
Gang Chen",2025,,"Proceedings of the Thirty-Fourth International Joint Conference on
Artificial Intelligence, {IJCAI} 2025, Montreal, Canada, August 16-22,
2025",,,7319--7328,10.24963/IJCAI.2025/814,https://doi.org/10.24963/ijcai.2025/814,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ijcai/BaiXWY0025.bib,"Wed, 24 Sep 2025 17:45:28 +0200",ijcai.org,,,,,,,,"Program performance optimization is essential for achieving high execution efficiency, yet it remains a challenging task that requires expertise in both software and hardware.

Large Language Models (LLMs), trained on high-quality code from platforms like GitHub and other open-source sources, have shown promise in generating optimized code for simple snippets. However, current LLM-based solutions often fall short when tackling project-level programs due to the complexity of call graphs and the intricate interactions among functions. In this paper, we emulate the process a human expert might follow when optimizing project-level programs and introduce a three-phase framework POLO (PrOject-Level Optimizer) to address this limitation. 

First, we profile the program to identify performance bottlenecks using an iterative weighting algorithm. 

Next, we conduct structural analysis by scanning the project and generating a graph that represents the program's structure. 

Finally, two LLM agents collaborate in iterative cycles to rewrite and optimize the code at these hotspots, gradually improving performance. 

We conduct experiments on open-source and proprietary projects. The results demonstrate that POLO accurately identifies performance bottlenecks and successfully applies optimizations. Under the O3 compilation flag, the optimized programs achieved speedups ranging from 1.34x to 21.5x."
DBLP:conf/iri/PuACF25,inproceedings,"Information Integration in Social Science Research: Advances with
LLMs in the {NAIP} Project","Calton Pu and
Anmol Agarwal and
Tianyu Chen and
Lewis Faulk",2025,,"25th {IEEE} International Conference on Information Reuse and Integration
and Data Science, {IRI} 2025, San Jose, CA, USA, August 6-8, 2025",,,110--115,10.1109/IRI66576.2025.00027,https://doi.org/10.1109/IRI66576.2025.00027,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iri/PuACF25.bib,"Wed, 24 Sep 2025 20:46:46 +0200",{IEEE},,,,,,,,
DBLP:conf/iri/TalukderAA25,inproceedings,"Leveraging LLMs for Automatic Feature Extraction in Embedded Systems
to Support Software Reuse","A. A. Talha Talukder and
Omar Alam and
Akramul Azim",2025,,"25th {IEEE} International Conference on Information Reuse and Integration
and Data Science, {IRI} 2025, San Jose, CA, USA, August 6-8, 2025",,,1--6,10.1109/IRI66576.2025.00009,https://doi.org/10.1109/IRI66576.2025.00009,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iri/TalukderAA25.bib,"Wed, 24 Sep 2025 01:00:00 +0200",{IEEE},,,,,,,,
DBLP:conf/isca/MoWW0CMJCX0025,inproceedings,"{LUT} Tensor Core: {A} Software-Hardware Co-Design for LUT-Based Low-Bit
{LLM} Inference","Zhiwen Mo and
Lei Wang and
Jianyu Wei and
Zhichen Zeng and
Shijie Cao and
Lingxiao Ma and
Naifeng Jing and
Ting Cao and
Jilong Xue and
Fan Yang and
Mao Yang",2025,,"Proceedings of the 52nd Annual International Symposium on Computer
Architecture, {ISCA} 2025, Tokyo, Japan, June 21-25, 2025",,,514--528,10.1145/3695053.3731057,https://doi.org/10.1145/3695053.3731057,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/isca/MoWW0CMJCX0025.bib,"Wed, 06 Aug 2025 01:00:00 +0200",{ACM},,,,,,,,"Large Language Model (LLM) inference becomes resource-intensive, prompting a shift toward low-bit model weights to reduce the memory footprint and improve efficiency. Such low-bit LLMs necessitate the mixed-precision matrix multiplication (mpGEMM), an important yet underexplored operation involving the multiplication of lower-precision weights with higher-precision activations. Off-the-shelf hardware does not support this operation natively, leading to indirect, thus inefficient, dequantization-based implementations. In this paper, we study the lookup table (LUT)-based approach for mpGEMM and find that a conventional LUT implementation fails to achieve the promised gains. To unlock the full potential of LUT-based mpGEMM, we propose LUT Tensor Core, a software-hardware co-design for low-bit LLM inference. LUT Tensor Core differentiates itself from conventional LUT designs through: 1) software-based optimizations to minimize table precompute overhead and weight reinterpretation to reduce table storage; 2) a LUT-based Tensor Core hardware design with an elongated tiling shape to maximize table reuse and a bit-serial design to support diverse precision combinations in mpGEMM; 3) a new instruction set and compilation optimizations for LUT-based mpGEMM. LUT Tensor Core significantly outperforms existing pure software LUT implementations and achieves a 1.44 √ó improvement in compute density and energy efficiency compared to previous state-of-the-art LUT-based accelerators."
DBLP:conf/iwpc/KumarGC25,inproceedings,"LLM2FedLLM - {A} Tool for Simulating Federated LLMs for Software Engineering
Tasks","Jahnavi Kumar and
Siddhartha Gandu and
Sridhar Chimalakonda",2025,,"33rd {IEEE/ACM} International Conference on Program Comprehension,
ICPC@ICSE 2025, Ottawa, ON, Canada, April 27-28, 2025",,,414--418,10.1109/ICPC66645.2025.00052,https://doi.org/10.1109/ICPC66645.2025.00052,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iwpc/KumarGC25.bib,"Mon, 23 Jun 2025 14:59:17 +0200",{IEEE},,,,,,,,"The paper introduces LLM2FedLLM, a tool designed for Software Engineering (SE) researchers to simulate fine-tuning Large Language Models (LLMs) within a federated learning (FL) framework. Unlike existing FL frameworks that facilitate real client collaboration, our simulator provides a controlled environment for experimenting with FL scenarios on a single machine. The LLM2FedLLM Simulator addresses SE code tasks, such as code summarization, code review, and code translation, within a federated learning framework by first partitioning the selected code dataset into heterogeneous subsets for multiple clients. It then fine-tunes the chosen LLM and evaluates its performance against vanilla, centralized, and individual client models using various metrics. The tool supports several federated aggregation methods and PEFT for supervised learning, with the flexibility to easily integrate additional techniques. The evaluation of our tool on Python code summarization showed that FedLLM performs comparably to centralized models and outperforms individual clients, particularly in low-data scenarios. Our tool aims to facilitate research advances in secure collaborative training simulations within the SE community. https://youtu.be/-byKkaiBchw."
DBLP:conf/llm4code/AgrawalSWRI25,inproceedings,Analysis of Student-LLM Interaction in a Software Engineering Project,"Naman Agrawal and
Ridwan Shariffdeen and
Guanlin Wang and
Sanka Rasnayaka and
Ganesh Neelakanta Iyer",2025,,"{IEEE/ACM} International Workshop on Large Language Models for Code,
LLM4Code@ICSE 2025, Ottawa, ON, Canada, May 3, 2025",,,112--119,10.1109/LLM4CODE66737.2025.00019,https://doi.org/10.1109/LLM4Code66737.2025.00019,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/llm4code/AgrawalSWRI25.bib,"Tue, 05 Aug 2025 01:00:00 +0200",{IEEE},,,,,,,,"Large Language Models (LLMs) are becoming increasingly competent across various domains, educators are showing a growing interest in integrating these LLMs into the learning process. Especially in software engineering, LLMs have demonstrated qualitatively better capabilities in code summarization, code generation, and debugging. Despite various research on LLMs for software engineering tasks in practice, limited research captures the benefits of LLMs for pedagogical advancements and their impact on the student learning process. To this extent, we analyze 126 undergraduate students‚Äô interaction with an AI assistant during a 13-week semester to understand the benefits of AI for software engineering learning. We analyze the conversations, code generated, code utilized, and the human intervention levels to integrate the code into the code base.Our findings suggest that students prefer ChatGPT over CoPilot. Our analysis also finds that ChatGPT generates responses with lower computational complexity compared to CoPilot. Furthermore, conversational-based interaction helps improve the quality of the code generated compared to auto-generated code. Early adoption of LLMs in software engineering is crucial to remain competitive in the rapidly developing landscape. Hence, the next generation of software engineers must acquire the necessary skills to interact with AI to improve productivity."
DBLP:conf/llm4code/MunsonGC25,inproceedings,"{\unicode{9834}} With a Little Help from My {(LLM)} Friends: Enhancing
Static Analysis with LLMs to Detect Software Vulnerabilities","Amy Munson and
Juanita Gomez and
Alvaro A. C{\'{a}}rdenas",2025,,"{IEEE/ACM} International Workshop on Large Language Models for Code,
LLM4Code@ICSE 2025, Ottawa, ON, Canada, May 3, 2025",,,25--32,10.1109/LLM4CODE66737.2025.00008,https://doi.org/10.1109/LLM4Code66737.2025.00008,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/llm4code/MunsonGC25.bib,"Mon, 30 Jun 2025 01:00:00 +0200",{IEEE},,,,,,,,"This paper explores the integration of Large Language Models (LLMs) with static analysis tools, specifically Semgrep, to enhance vulnerability detection in Java applications. Through a series of experiments, we evaluate the performance of various LLMs in triaging security weaknesses identified by Semgrep. We also study how LLMs perform across different types of vulnerabilities and assess the impact of various prompt engineering strategies. Our results reveal that while some LLM models reduce the accuracy of baseline results with static analysis, they show a consistent improvement with each new model released. In particular, o1-mini significantly outperformed others in our experiments in terms of their accuracy and false positive reduction. Although LLMs might not be ready for prime time in vulnerability detection yet, this study highlights their growing potential to complement existing tools and paves the way for future research to further optimize LLM-based vulnerability detection systems."
DBLP:conf/miccai/PunneshettyIAMA25,inproceedings,"An Explainable Multimodal Framework with {LLM} Agents for Intracranial
Hemorrhage Detection","Shashwath Punneshetty and
Dhyey Italiya and
Vinti Agarwal and
Chandresh Maurya and
Amit Agrawal",2025,,"{AI} for Clinical Applications - First International Workshops, Agentic
{AI} 2025, {CREATE} 2025, and Clinical MLLMs 2025, Held in Conjunction
with {MICCAI} 2025, Daejeon, South Korea, September 23 and 27, 2025,
Proceedings",16147,,3--12,10.1007/978-3-032-06004-4\_1,https://doi.org/10.1007/978-3-032-06004-4\_1,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/miccai/PunneshettyIAMA25.bib,"Mon, 06 Oct 2025 20:23:21 +0200",Springer,"Jianing Qiu and
Jinlin Wu and
Curtis Langlotz and
Baoru Huang and
Zhen Lei and
Honghan Wu and
Hongbin Liu and
Weidi Xie",Lecture Notes in Computer Science,,,,,,
DBLP:conf/msr/AhmedDTP25,inproceedings,Can LLMs Replace Manual Annotation of Software Engineering Artifacts?,"Toufique Ahmed and
Premkumar T. Devanbu and
Christoph Treude and
Michael Pradel",2025,,"22nd {IEEE/ACM} International Conference on Mining Software Repositories,
MSR@ICSE 2025, Ottawa, ON, Canada, April 28-29, 2025",,,526--538,10.1109/MSR66628.2025.00086,https://doi.org/10.1109/MSR66628.2025.00086,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/msr/AhmedDTP25.bib,"Mon, 23 Jun 2025 14:11:52 +0200",{IEEE},,,,,,,,"Experimental evaluations of software engineering innovations, e.g., tools and processes, often include human-subject studies as a component of a multi-pronged strategy to obtain greater generalizability of the findings. However, human-subject studies in our field are challenging, due to the cost and difficulty of finding and employing suitable subjects, ideally, professional programmers with varying degrees of experience. Meanwhile, large language models (LLMs) have recently started to demonstrate human-level performance in several areas. This paper explores the possibility of substituting costly human subjects with much cheaper LLM queries in evaluations of code and coderelated artifacts. We study this idea by applying six state-of-theart LLMs to ten annotation tasks from five datasets created by prior work, such as judging the accuracy of a natural language summary of a method or deciding whether a code change fixes a static analysis warning. Our results show that replacing some human annotation effort with LLMs can produce inter-rater agreements equal or close to human-rater agreement. To help decide when and how to use LLMs in human-subject studies, we propose model-model agreement as a predictor of whether a given task is suitable for LLMs at all, and model confidence as a means to select specific samples where LLMs can safely replace human annotators. Overall, our work is the first step toward mixed human-LLM evaluations in software engineering."
DBLP:conf/msr/SahaRS25,inproceedings,"MaLAware: Automating the Comprehension of Malicious Software Behaviours
using Large Language Models (LLMs)","Bikash Saha and
Nanda Rani and
Sandeep Kumar Shukla",2025,,"22nd {IEEE/ACM} International Conference on Mining Software Repositories,
MSR@ICSE 2025, Ottawa, ON, Canada, April 28-29, 2025",,,169--173,10.1109/MSR66628.2025.00037,https://doi.org/10.1109/MSR66628.2025.00037,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/msr/SahaRS25.bib,"Mon, 23 Jun 2025 01:00:00 +0200",{IEEE},,,,,,,,"Current malware (malicious software) analysis tools focus on detection and family classification but fail to provide clear and actionable narrative insights into the malignant activity of the malware. Therefore, there is a need for a tool that translates raw malware data into human-readable descriptions. Developing such a tool accelerates incident response, reduces malware analysts‚Äô cognitive load, and enables individuals having limited technical expertise to understand malicious software behaviour. With this objective, we present MaLAware, which automatically summarizes the full spectrum of malicious activity of malware executables. MaLAware processes Cuckoo Sandbox-generated reports using large language models (LLMs) to correlate malignant activities and generate concise summaries explaining malware behaviour. We evaluate the tool‚Äôs performance on five opensource LLMs. The evaluation uses the human-written malware behaviour description dataset as ground truth. The model‚Äôs performance is measured using 11 extensive performance metrics, which boosts the confidence of MaLAware‚Äôs effectiveness. The current version of the tool, i.e., MaLAware, supports Qwen2.5-7B, Llama2-7B, Llama3.1-8B, Mistral-7B, and Falcon-7B, along with the quantization feature for resource-constrained environments. MaLAware lays a foundation for future research in malware behavior explanation, and its extensive evaluation demonstrates LLMs‚Äô ability to narrate malware behavior in an actionable and comprehensive manner."
DBLP:conf/msr/YeLB25,inproceedings,"LLMSecConfig: An LLM-Based Approach for Fixing Software Container
Misconfigurations","Ziyang Ye and
Triet Huynh Minh Le and
M. Ali Babar",2025,,"22nd {IEEE/ACM} International Conference on Mining Software Repositories,
MSR@ICSE 2025, Ottawa, ON, Canada, April 28-29, 2025",,,629--641,10.1109/MSR66628.2025.00099,https://doi.org/10.1109/MSR66628.2025.00099,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/msr/YeLB25.bib,"Mon, 23 Jun 2025 01:00:00 +0200",{IEEE},,,,,,,,"Security misconfigurations in Container Orchestrators (COs) can pose serious threats to software systems. While Static Analysis Tools (SATs) can effectively detect these security vulnerabilities, the industry currently lacks automated solutions capable of fixing these misconfigurations. The emergence of Large Language Models (LLMs), with their proven capabilities in code understanding and generation, presents an opportunity to address this limitation. This study introduces LLMSecConfig, an innovative framework that bridges this gap by combining SATs with LLMs. Our approach leverages advanced prompting techniques and Retrieval-Augmented Generation (RAG) to automatically repair security misconfigurations while preserving operational functionality. Evaluation of 1,000 real-world Kubernetes configurations achieved a 94% success rate while maintaining a low rate of introducing new misconfigurations.Our work makes a promising step towards automated container security management, reducing the manual effort required for configuration maintenance."
DBLP:conf/naacl/BalepurGRFBR25,inproceedings,"Reverse Question Answering: Can an {LLM} Write a Question so Hard
(or Bad) that it Can't Answer?","Nishant Balepur and
Feng Gu and
Abhilasha Ravichander and
Shi Feng and
Jordan Lee Boyd{-}Graber and
Rachel Rudinger",2025,,"Proceedings of the 2025 Conference of the Nations of the Americas
Chapter of the Association for Computational Linguistics: Human Language
Technologies, {NAACL} 2025 - Volume 2: Short Papers, Albuquerque,
New Mexico, April 29 - May 4, 2025",,,44--64,10.18653/V1/2025.NAACL-SHORT.5,https://doi.org/10.18653/v1/2025.naacl-short.5,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/naacl/BalepurGRFBR25.bib,"Thu, 14 Aug 2025 11:28:45 +0200",Association for Computational Linguistics,"Luis Chiruzzo and
Alan Ritter and
Lu Wang",,,,,,,
DBLP:conf/naacl/MondshinePT25,inproceedings,"Beyond English: The Impact of Prompt Translation Strategies across
Languages and Tasks in Multilingual LLMs","Itai Mondshine and
Tzuf Paz{-}Argaman and
Reut Tsarfaty",2025,,"Findings of the Association for Computational Linguistics: {NAACL}
2025, Albuquerque, New Mexico, USA, April 29 - May 4, 2025",,,1331--1354,10.18653/V1/2025.FINDINGS-NAACL.73,https://doi.org/10.18653/v1/2025.findings-naacl.73,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/naacl/MondshinePT25.bib,"Thu, 14 Aug 2025 11:29:01 +0200",Association for Computational Linguistics,"Luis Chiruzzo and
Alan Ritter and
Lu Wang",,,,,,,
DBLP:conf/naacl/SandovalACTD25,inproceedings,My {LLM} might Mimic {AAE} - But When Should It?,"Sandra Sandoval and
Christabel Acquaye and
Kwesi A. Cobbina and
Mohammad Nayeem Teli and
Hal Daum{\'{e}} III",2025,,"Proceedings of the 2025 Conference of the Nations of the Americas
Chapter of the Association for Computational Linguistics: Human Language
Technologies, {NAACL} 2025 - Volume 1: Long Papers, Albuquerque, New
Mexico, USA, April 29 - May 4, 2025",,,5277--5302,10.18653/V1/2025.NAACL-LONG.273,https://doi.org/10.18653/v1/2025.naacl-long.273,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/naacl/SandovalACTD25.bib,"Thu, 14 Aug 2025 11:28:41 +0200",Association for Computational Linguistics,"Luis Chiruzzo and
Alan Ritter and
Lu Wang",,,,,,,
DBLP:conf/naacl/WangLX25a,inproceedings,"CVE-Bench: Benchmarking LLM-based Software Engineering Agent's Ability
to Repair Real-World {CVE} Vulnerabilities","Peiran Wang and
Xiaogeng Liu and
Chaowei Xiao",2025,,"Proceedings of the 2025 Conference of the Nations of the Americas
Chapter of the Association for Computational Linguistics: Human Language
Technologies, {NAACL} 2025 - Volume 1: Long Papers, Albuquerque, New
Mexico, USA, April 29 - May 4, 2025",,,4207--4224,10.18653/V1/2025.NAACL-LONG.212,https://doi.org/10.18653/v1/2025.naacl-long.212,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/naacl/WangLX25a.bib,"Fri, 13 Jun 2025 01:00:00 +0200",Association for Computational Linguistics,"Luis Chiruzzo and
Alan Ritter and
Lu Wang",,,,,,,
DBLP:conf/naacl/ZhangASS25,inproceedings,Personalize Your {LLM:} Fake it then Align it,"Yijing Zhang and
Dyah Adila and
Changho Shin and
Frederic Sala",2025,,"Findings of the Association for Computational Linguistics: {NAACL}
2025, Albuquerque, New Mexico, USA, April 29 - May 4, 2025",,,7287--7301,10.18653/V1/2025.FINDINGS-NAACL.407,https://doi.org/10.18653/v1/2025.findings-naacl.407,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/naacl/ZhangASS25.bib,"Fri, 13 Jun 2025 01:00:00 +0200",Association for Computational Linguistics,"Luis Chiruzzo and
Alan Ritter and
Lu Wang",,,,,,,
DBLP:conf/nlbse/BusanyHRRMSAA25,inproceedings,"Automating Benchmark Generation for LLMs in Software Engineering:
Challenges and Opportunities","Nimrod Busany and
Hananel Hadad and
Gil Rosenblum and
Ramya Ramachandran and
Zofia Maszlanka and
Rohit Shashank Shelke and
Okhaide Akhigbe and
Daniel Amyot",2025,,"{IEEE/ACM} International Workshop on Natural Language-Based Software
Engineering, NLBSE@ICSE 2025, Ottawa, ON, Canada, April 27-28, 2025",,,25--26,10.1109/NLBSE66842.2025.00011,https://doi.org/10.1109/NLBSE66842.2025.00011,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/nlbse/BusanyHRRMSAA25.bib,"Mon, 30 Jun 2025 21:18:26 +0200",{IEEE},,,,,,,,"As Large Language Models (LLMs) become increasingly integral to software engineering tasks, the need for extensive evaluation benchmarks grows. Yet, creating these benchmarks manually is costly and time-consuming, posing a significant barrier to the effective testing and deployment of LLM-based systems. In this position paper, we highlight the challenges associated with manual benchmark creation using LLMs and discuss the potential of using LLMs themselves to automate this process. We explore critical issues such as ensuring dataset quality and comparability to human-crafted benchmarks, assisting users in validating generated examples, reducing the need for manual feedback, lowering associated costs, and facilitating customization across diverse tasks. By examining these challenges and sharing insights from our preliminary tool development, we aim to spark discussion and collaboration within the community to develop scalable solutions for benchmark generation."
DBLP:conf/noms/DzeparoskaL25a,inproceedings,{KPI} Assurance and LLMs for Intent-Based Management,"Kristina Dzeparoska and
Alberto Leon{-}Garcia",2025,,"{NOMS} 2025 {IEEE} Network Operations and Management Symposium, Honolulu,
HI, USA, May 12-16, 2025",,,1--9,10.1109/NOMS57970.2025.11073595,https://doi.org/10.1109/NOMS57970.2025.11073595,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/noms/DzeparoskaL25a.bib,"Tue, 29 Jul 2025 10:18:18 +0200",{IEEE},,,,,,,,"Intent-Based Management provides a shift in network management by automating the alignment of network operations with business objectives. However, primary challenges include: 1) intent processing (translate, decompose and identify the logic to fulfill the intent), and 2) ensuring intent conformance (ongoing adaptation of the logic to ensure the intent is met, considering dynamic conditions). We use a 3-tier Large Language Model (LLM) pipeline to convert intents into Policy Trees, that are then executed using closed control loop automation. In this paper, we focus on assurance that is tasked with continuous monitoring, verification, and validation of the operational state, and corrective actions to ensure conformance with the target objectives. To do so, we use a generic LLM (OpenAI's GPT) with in-context learning and well-established decision-making algorithms (feedback controllers) to determine assurance actions and remediate intent deviation. We show that AI-driven policies can support intent fulfillment and assurance, and we discuss current limitations, benefits, and future directions to address critical challenges in using AI for network management, towards improved generalizability, scalability, and overall trustworthiness."
DBLP:conf/ofc/LiuQZCYHZ25,inproceedings,"First Field Trial of LLM-Powered {AI} Agent for Lifecycle Management
of Autonomous Driving Optical Networks","Xiaomin Liu and
Qizhi Qiu and
Yihao Zhang and
Yuming Cheng and
Lilin Yi and
Weisheng Hu and
Qunbi Zhuge",2025,,"Optical Fiber Communications Conference and Exhibition, {OFC} 2025,
San Francisco, CA, USA, March 30 - April 3, 2025",,,1--3,,https://ieeexplore.ieee.org/document/11047189,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ofc/LiuQZCYHZ25.bib,"Mon, 21 Jul 2025 09:42:58 +0200",{IEEE},,,,,,,,
DBLP:conf/ofc/WangYZWST25,inproceedings,"LLM-centric Transport Network Configuration Management Framework and
Demonstration","Cen Wang and
Noboru Yoshikane and
Chenxiao Zhang and
Yuta Wakayama and
Daiki Soma and
Takehiro Tsuritani",2025,,"Optical Fiber Communications Conference and Exhibition, {OFC} 2025,
San Francisco, CA, USA, March 30 - April 3, 2025",,,1--3,,https://ieeexplore.ieee.org/document/11046780,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ofc/WangYZWST25.bib,"Mon, 21 Jul 2025 01:00:00 +0200",{IEEE},,,,,,,,
DBLP:conf/ofc/ZaidSSJBF25,inproceedings,Multi-Agent Design for LLM-assisted Network Management,"Hussein Zaid and
Pooyan Safari and
Behnam Shariati and
Aydin Jafari and
Mihail Balanici and
Johannes Karl Fischer",2025,,"Optical Fiber Communications Conference and Exhibition, {OFC} 2025,
San Francisco, CA, USA, March 30 - April 3, 2025",,,1--3,,https://ieeexplore.ieee.org/document/11046468,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ofc/ZaidSSJBF25.bib,"Mon, 21 Jul 2025 01:00:00 +0200",{IEEE},,,,,,,,
DBLP:conf/pacmi/LamprouKRV25,inproceedings,Guarding LLM-aided Software Transformation Tasks via Component Exoskeletons,"Evangelos Lamprou and
Christian Gram Kalhauge and
Martin C. Rinard and
Nikos Vasilakis",2025,,"Proceedings of the 4th Workshop on Practical Adoption Challenges of
{ML} for Systems, {PACMI} 2025, Seoul, Republic of Korea, October
13-16, 2025",,,13--18,10.1145/3766882.3767171,https://doi.org/10.1145/3766882.3767171,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/pacmi/LamprouKRV25.bib,"Thu, 02 Oct 2025 14:59:23 +0200",{ACM},,,,,,,,
DBLP:conf/refsq/FranchGPQS25,inproceedings,"Leveraging Requirements Elicitation through Software Requirement Patterns
and LLMs","Xavier Franch and
Stefania Gnesi and
Federico Paccosi and
Carme Quer and
Laura Semini",2025,,"Requirements Engineering: Foundation for Software Quality - 31st International
Working Conference, {REFSQ} 2025, Barcelona, Spain, April 7-10, 2025,
Proceedings",15588,,261--276,10.1007/978-3-031-88531-0\_19,https://doi.org/10.1007/978-3-031-88531-0\_19,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/refsq/FranchGPQS25.bib,"Fri, 09 May 2025 01:00:00 +0200",Springer,"Anne Hess and
Angelo Susi",Lecture Notes in Computer Science,,,,,,
DBLP:conf/refsq/HippargiKN25,inproceedings,"Evaluating the Capabilities of LLMs in Traceability Maintenance for
Automotive System and Software Requirements","Vibhashree Hippargi and
Erik Kamsties and
J{\""{u}}rgen Naumann",2025,,"Joint Proceedings of {REFSQ} 2025 Workshops, Doctoral Symposium, Posters
{\&} Tools Track, and Education and Training Track co-located with
31st International Conference on Requirements Engineering: Foundation
for Software Quality {(REFSQ} 2025), Barcelona, Spain, April 7-10,
2025",3959,,,,https://ceur-ws.org/Vol-3959/NLP4RE-short2.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/refsq/HippargiKN25.bib,"Wed, 04 Jun 2025 17:15:40 +0200",CEUR-WS.org,"Anne Hess and
Angelo Susi and
Eduard C. Groen and
Marcela Ruiz and
Muhammad Abbas Khan and
Fatma Basak Aydemir and
Maya Daneva and
Renata S. S. Guizzardi and
Jens Gulden and
Andrea Herrmann and
Jennifer Horkoff and
Sylwia Kopczynska and
Patrick Mennig and
Marc Oriol and
Elda Paja and
Anna Perini and
Alexander Rachmann and
Kurt Schneider and
Laura Semini and
Paola Spoletini and
Andreas Vogelsang",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/refsq/IbtashamBAHSC25,inproceedings,"ReqRAG: Enhancing Software Release Management through Retrieval-Augmented
LLMs: An Industrial Study","Md Saleh Ibtasham and
Sarmad Bashir and
Muhammad Abbas and
Zulqarnain Haider and
Mehrdad Saadatmand and
Antonio Cicchetti",2025,,"Requirements Engineering: Foundation for Software Quality - 31st International
Working Conference, {REFSQ} 2025, Barcelona, Spain, April 7-10, 2025,
Proceedings",15588,,277--292,10.1007/978-3-031-88531-0\_20,https://doi.org/10.1007/978-3-031-88531-0\_20,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/refsq/IbtashamBAHSC25.bib,"Thu, 01 May 2025 01:00:00 +0200",Springer,"Anne Hess and
Angelo Susi",Lecture Notes in Computer Science,,,,,,
DBLP:conf/rv/CohenHPG25,inproceedings,The Power of Reframing: Using LLMs in Synthesizing {RV} Monitors,"Itay Cohen and
Klaus Havelund and
Doron Peled and
Yoav Goldberg",2025,,"Runtime Verification - 25th International Conference, {RV} 2025, Graz,
Austria, September 15-19, 2025, Proceedings",16087,,202--212,10.1007/978-3-032-05435-7\_12,https://doi.org/10.1007/978-3-032-05435-7\_12,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/rv/CohenHPG25.bib,"Thu, 25 Sep 2025 17:58:49 +0200",Springer,"Bettina K{\""{o}}nighofer and
Hazem Torfah",Lecture Notes in Computer Science,,,,,,
DBLP:conf/saner/BoronatM25,inproceedings,"{MDRE-LLM:} {A} Tool for Analyzing and Applying LLMs in Software Reverse
Engineering","Artur Boronat and
Jawad Mustafa",2025,,"{IEEE} International Conference on Software Analysis, Evolution and
Reengineering, {SANER} 2025, Montreal, QC, Canada, March 4-7, 2025",,,850--854,10.1109/SANER64311.2025.00090,https://doi.org/10.1109/SANER64311.2025.00090,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/saner/BoronatM25.bib,"Tue, 10 Jun 2025 13:56:07 +0200",{IEEE},,,,,,,,"Understanding and maintaining software systems often requires extracting high-level abstractions, such as domain models, from source code. MDRE-LLM addresses this challenge by integrating Large Language Models (LLMs) with traditional Model-Driven Reverse Engineering (MDRE) techniques, offering an innovative approach to automate and enhance domain model recovery. The tool supports flexible granularity strategies and validates LLM -generated models against deterministic baselines. MDRE-LLM addresses diverse use cases, including analyzing legacy systems with minimal documentation, rapidly compre-hending large-scale codebases, and validating LLM performance in reverse engineering tasks. These capabilities have the potential to improve software analysis and refactoring while advance AI-driven research and education by fostering systematic experimentation and collaboration. The tool and a webcast are available at https://zenodo.org/uploads/14072106."
DBLP:conf/saner/MillikenKY25,inproceedings,"Beyond pip Install: Evaluating {LLM} Agents for the Automated Installation
of Python Projects","Louis Milliken and
Sungmin Kang and
Shin Yoo",2025,,"{IEEE} International Conference on Software Analysis, Evolution and
Reengineering, {SANER} 2025, Montreal, QC, Canada, March 4-7, 2025",,,1--11,10.1109/SANER64311.2025.00009,https://doi.org/10.1109/SANER64311.2025.00009,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/saner/MillikenKY25.bib,"Tue, 10 Jun 2025 01:00:00 +0200",{IEEE},,,,,,,,"Many works have recently proposed the use of Large Language Model (LLM) based agents for performing ‚Äòrepository level‚Äô tasks, loosely defined as a set of tasks whose scopes are greater than a single file. This has led to speculation that the orchestration of these repository-level tasks could lead to software engineering agents capable of performing almost independently of human intervention. However, of the suite of tasks that would need to be performed by this autonomous software engineering agent, we argue that one important task is missing, which is to fulfil project level dependency by installing other repositories. To investigate the feasibility of this repository level installation task, we introduce a benchmark of of repository installation tasks curated from 40 open source Python projects, which includes a ground truth installation process for each target repository. Further, we propose Installamatic, an agent which aims to perform and verify the installation of a given repository by searching for relevant instructions from documentation in the repository. Empirical experiments reveal that that 55% of the studied repositories can be automatically installed by our agent at least one out of ten times. Through further analysis, we identify the common causes for our agent's inability to install a repository, discuss the challenges faced in the design and implementation of such an agent and consider the implications that such an agent could have for developers."
DBLP:conf/saner/NunesFSNFS25,inproceedings,"Evaluating the Effectiveness of LLMs in Fixing Maintainability Issues
in Real-World Projects","Henrique Gomes Nunes and
Eduardo Figueiredo and
Larissa Rocha Soares and
Sarah Nadi and
Fischer Ferreira and
Geanderson E. dos Santos",2025,,"{IEEE} International Conference on Software Analysis, Evolution and
Reengineering, {SANER} 2025, Montreal, QC, Canada, March 4-7, 2025",,,669--680,10.1109/SANER64311.2025.00069,https://doi.org/10.1109/SANER64311.2025.00069,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/saner/NunesFSNFS25.bib,"Sat, 06 Sep 2025 01:00:00 +0200",{IEEE},,,,,,,,"Large Language Models (LLMs) have gained attention for addressing coding problems, but their effectiveness in fixing code maintainability remains unclear. This study evaluates LLMs capability to resolve 127 maintainability issues from 10 GitHub repositories. We use zero-shot prompting for Copilot Chat and Llama 3.1, and few-shot prompting with Llama only. The LLM-generated solutions are assessed for compilation errors, test failures, and new maintainability problems. Llama with few-shot prompting successfully fixed 44.9 % of the methods, while Copilot Chat and Llama zero-shot fixed 32.29 % and 30 %, respectively. However, most solutions introduced errors or new maintainability issues. We also conducted a human study with 45 participants to evaluate the readability of 51 LLM-generated solutions. The human study showed that 68.63 % of participants observed improved readability. Overall, while LLMs show potential for fixing maintainability issues, their introduction of errors highlights their current limitations."
DBLP:conf/saner/SialaL25,inproceedings,"Towards Using LLMs in the Reverse Engineering of Software Systems
to Object Constraint Language","Hanan Abdulwahab Siala and
Kevin Lano",2025,,"{IEEE} International Conference on Software Analysis, Evolution and
Reengineering, {SANER} 2025, Montreal, QC, Canada, March 4-7, 2025",,,1--6,10.1109/SANER64311.2025.00096,https://doi.org/10.1109/SANER64311.2025.00096,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/saner/SialaL25.bib,"Tue, 10 Jun 2025 01:00:00 +0200",{IEEE},,,,,,,,"Using reverse engineering to extract semantic representations from software systems is beneficial for the understanding of these systems, and can facilitate their maintenance and evolution. In particular, extracting semantically-precise specifications from systems is useful for re-engineering of systems to functionally-equivalent versions in different programming languages. Large language models (LLMs) are a type of machine learning (ML) technique that has been utilized in various domains, including software engineering and program translation. Yet, abstracting precise Object Constraint Language (OCL) specifications from source code using LLMs has not gained attention in reverse engineering approaches. In this paper, we present a new reverse engineering approach, named LLM4Models, to abstract OCL specifications from Java and Python programs, using LLMs."
DBLP:conf/satml/MeeusSJFRM25,inproceedings,"SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and
How to Fix It)","Matthieu Meeus and
Igor Shilov and
Shubham Jain and
Manuel Faysse and
Marek Rei and
Yves{-}Alexandre de Montjoye",2025,,"{IEEE} Conference on Secure and Trustworthy Machine Learning, SaTML
2025, Copenhagen, Denmark, April 9-11, 2025",,,385--401,10.1109/SATML64287.2025.00028,https://doi.org/10.1109/SaTML64287.2025.00028,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/satml/MeeusSJFRM25.bib,"Sat, 11 Oct 2025 01:00:00 +0200",{IEEE},,,,,,,,"Whether Large Language models (LLMs) memorize their training data and what this means, from measuring privacy leakage to detecting copyright violations, has become a rapidly growing area of research. In the last few months, more than 10 new methods have been proposed to perform Membership Inference Attacks (MIAs) against LLMs. Contrary to traditional MIAs which rely on fixed-but randomized-records or models, these methods are mostly trained and tested on datasets collected post-hoc. Sets of members and non-members, used to evaluate the MIA, are constructed using informed guesses after the release of a model. This lack of randomization raises concerns of a distribution shift between members and non-members. In this work, we first extensively review the literature on MIAs against LLMs and show that, while most work focuses on sequence-level MIAs evaluated in post-hoc setups, a range of target models, motivations and units of interest are considered. We then quantify distribution shifts present in 6 datasets used in the literature, ranging from books to papers using a model-less bag of word classifier and compare the model-less results to the MIA. Our analysis shows that all of these datasets constructed post-hoc suffer from strong distribution shifts. These shifts invalidate the claims of LLMs memorizing strongly in real-world scenarios and, potentially, also the methodological contributions of the recent papers based on these datasets. Yet, all hope might not be lost. In the second part of this work, we introduce important considerations to properly evaluate MIAs against LLMs and discuss, in turn, potential ways forwards: randomized test splits, injections of randomized (unique) sequences, randomized finetuning, and several post-hoc control methods. While each option comes with its advantages and limitations, we believe they collectively provide solid grounds to guide the development of MIA methods and study LLM memorization. We conclude with an overview of recommended approaches to benchmark sequence-level and document-level MIAs against LLMs."
DBLP:conf/satrends/BecattiniVV25,inproceedings,{SALLMA:} {A} Software Architecture for LLM-Based Multi-Agent Systems,"Marco Becattini and
Roberto Verdecchia and
Enrico Vicario",2025,,"{IEEE/ACM} International Workshop New Trends in Software Architecture,
SATrends@ICSE 2025, Ottawa, ON, Canada, April 29, 2025",,,5--8,10.1109/SATRENDS66715.2025.00006,https://doi.org/10.1109/SATrends66715.2025.00006,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/satrends/BecattiniVV25.bib,"Mon, 30 Jun 2025 22:34:47 +0200",{IEEE},,,,,,,,"As a new and disruptive technology, the introduction of large language models (LLMs) may be the first step into a paradigm shift of how we develop and deploy software-intensive systems. While the capabilities of LLM agents for software engineering and architecture tasks are currently explored, how to architect LLM-based systems appears to be to date an uncharted territory. Software architectures based on a single LLM agent face inherent challenges, such as lack of task customization, lack of memory, and limited access to ground truth. These challenges become especially pressing in real-world contexts that demand persistent context, validated information, and task-specific flexibility. As a potential solution to overcome these challenges, multiple LLM-agents can be adopted for specialized tasks within a single software-intensive system. In this contribution, we open the discourse on architecting LLM-intensive software products by presenting SALLMA, a Software Architecture for LLMbased Multi-Agent systems. SALLMA leverages two core layers, namely (i) the Operational Layer, responsible for request intent management, handling real-time task execution and dynamic orchestration of agents, and (ii) the Knowledge Layer, used to to store and manage metamodels and configurations for workflows and agents. To primarily assess the viability of SALLMA, we develop a proof of concept leveraging as key technologies Docker, Kubernetes, Python, LangChain, Hugging Face, Mistral, LLaMA, and SQL and NoSQL databases. Currently, SALLMA is deployed to provide information on behalf of public administration offices, and is currently utilized in a business simulation scenario."
DBLP:conf/se/DongHHV25,inproceedings,On the logical (in)consistency of code-generating LLMs,"Ke Dong and
William Hsu and
Pascal Hitzler and
Eugene Y. Vasserman",2025,,"Software Engineering 2025 Companion Proceedings, Fachtagung des GI-Fachbereichs
Softwaretechnik, Karlsruhe, Germany, February 24-28, 2025",,,12,10.18420/SE2025-WS-12,https://doi.org/10.18420/se2025-ws-12,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/se/DongHHV25.bib,"Fri, 04 Jul 2025 01:00:00 +0200","Gesellschaft f{\""{u}}r Informatik e.V.","Kevin Feichtinger and
Lisa Sonnleithner and
Hamideh Hajiabadi",,,,,,,
DBLP:conf/sigcomm/WangLYGYZHBPKY025,inproceedings,"Intent-Driven Network Management with Multi-Agent LLMs: The Confucius
Framework","Zhaodong Wang and
Samuel Lin and
Guanqing Yan and
Soudeh Ghorbani and
Minlan Yu and
Jiawei Zhou and
Nathan Hu and
Lopa Baruah and
Sam Peters and
Srikanth Kamath and
Jerry Yang and
Ying Zhang",2025,,"Proceedings of the {ACM} {SIGCOMM} 2025 Conference, {SIGCOMM} 2025,
S{\~{a}}o Francisco Convent, Coimbra, Portugal, September 8-11, 2025",,,347--362,10.1145/3718958.3750537,https://doi.org/10.1145/3718958.3750537,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigcomm/WangLYGYZHBPKY025.bib,"Thu, 09 Oct 2025 13:11:12 +0200",{ACM},"Mar{\'{\i}}lia Curado and
Christian Esteve Rothenberg and
George Porter and
Srikanth Kandula",,,,,,,
DBLP:conf/sigcse/AljedaaniEP25,inproceedings,"Enhancing Accessibility in Software Engineering Projects with Large
Language Models (LLMs)","Wajdi Aljedaani and
Marcelo Medeiros Eler and
P. D. Parthasarathy",2025,,"Proceedings of the 56th {ACM} Technical Symposium on Computer Science
Education V. 1, {SIGCSE} {TS} 2025, Pittsburgh, PA, USA, 26 February
2025 - 1 March 2025",,,25--31,10.1145/3641554.3701841,https://doi.org/10.1145/3641554.3701841,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigcse/AljedaaniEP25.bib,"Fri, 07 Mar 2025 00:00:00 +0100",{ACM},"Jeffrey A. Stone and
Timothy T. Yuen and
Libby Shoop and
Samuel A. Rebelsky and
James Prather",,,,,,,"Digital accessibility ensures that digital products and services are usable by a diverse range of users, regardless of their physical or cognitive abilities. While numerous standards and guidelines have been established to aid developers in creating accessible content, studies reveal a persistent lack of accessibility in many web and mobile applications. This gap is often attributed to barriers such as lack of awareness, insufficient knowledge, absence of specific requirements, time constraints, and lack of executive support. In this context, we aim to address the lack of awareness and knowledge challenges by proposing a hands-on approach that leverages the capabilities of Large Language Models (LLMs) like ChatGPT to enhance students' accessibility awareness, knowledge, and practical skills. We engaged software engineering students in tasks involving website development and accessibility evaluation using checker tools, and we utilized ChatGPT 3.5 to fix identified accessibility issues. Our findings suggest that practical assignments significantly enhance learning outcomes, as interactions with LLMs allow students to develop a deeper understanding of accessibility concepts. This approach not only reinforces theoretical knowledge but also highlights the real-world impact of their work. The results indicate that combining practical assignments with AI-driven support effectively improves students' proficiency in web accessibility."
DBLP:conf/sigcse/BrockenbroughFS25,inproceedings,"Exploring LLMs Impact on Student-Created User Stories and Acceptance
Testing in Software Development","Allan Brockenbrough and
Henry Feild and
Dominic Salinas",2025,,"Proceedings of the 56th {ACM} Technical Symposium on Computer Science
Education V. 2, {SIGCSE} {TS} 2025, Pittsburgh, PA, USA, 26 February
2025 - 1 March 2025",,,1401--1402,10.1145/3641555.3705183,https://doi.org/10.1145/3641555.3705183,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigcse/BrockenbroughFS25.bib,"Fri, 07 Mar 2025 00:00:00 +0100",{ACM},"Jeffrey A. Stone and
Timothy T. Yuen and
Libby Shoop and
Samuel A. Rebelsky and
James Prather",,,,,,,"In Agile software development methodology, a user story describes a new feature or functionality from an end user's perspective. The user story details may also incorporate acceptance testing criteria, which can be developed through negotiation with users. When creating stories from user feedback, the software engineer may maximize their usefulness by considering story attributes, including scope, independence, negotiability, and testability. This study investigates how LLMs (large language models), with guided instructions, affect undergraduate software engineering students' ability to transform user feedback into user stories. Students, working individually, were asked to analyze user feedback comments, appropriately group related items, and create user stories following the principles of INVEST, a framework for assessing user stories. We found that LLMs help students develop valuable stories with well-defined acceptance criteria. However, students tend to perform better without LLMs when creating user stories with an appropriate scope."
DBLP:conf/sigmod/BukhshCCMWY25,inproceedings,{LLM-DPM} - Workshop on Large Language Models for Data Process Management,"Faiza Allah Bukhsh and
Paolo Ceravolo and
Xu Chu and
Samira Maghool and
Eugene Wu and
Cong Yu",2025,,"Companion of the 2025 International Conference on Management of Data,
{SIGMOD/PODS} 2025, Berlin, Germany, June 22-27, 2025",,,872--873,10.1145/3722212.3724492,https://doi.org/10.1145/3722212.3724492,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigmod/BukhshCCMWY25.bib,"Sun, 06 Jul 2025 01:00:00 +0200",{ACM},"Volker Markl and
Joseph M. Hellerstein and
Azza Abouzied",,,,,,,"The LLM-DPM Workshop investigates the transformative impact of Large Language Models (LLMs) and Explainable AI (XAI) on Data and Process Management. With organizations facing growing dependence on intricate data-driven workflows, there is an urgent demand for systems that prioritize not only efficiency but also transparency, reliability, and equity. This workshop serves as a dedicated platform to explore the convergence of LLMs, process mining, and database technologies, tackling critical issues such as enhancing data integrity, refining query understanding, forecasting process outcomes, and optimizing system performance. Featuring technical discussions, keynote speeches, and collaborative panels, LLM-DPM seeks to bridge the gap between academic research and industry applications, inspiring innovative approaches and advancing the creation of accountable, AI-powered solutions for process management challenges."
DBLP:conf/sigsoft/ChomatekPNP25,inproceedings,Decoding {CI/CD} Practices in Open-Source Projects with {LLM} Insights,"Lukasz Chomatek and
Jakub Papuga and
Przemyslaw Nowak and
Aneta Poniszewska{-}Maranda",2025,,"Proceedings of the 33rd {ACM} International Conference on the Foundations
of Software Engineering, {FSE} Companion 2025, Clarion Hotel Trondheim,
Trondheim, Norway, June 23-28, 2025",,,1638--1644,10.1145/3696630.3728699,https://doi.org/10.1145/3696630.3728699,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigsoft/ChomatekPNP25.bib,"Sat, 09 Aug 2025 01:00:00 +0200",{ACM},"Leonardo Montecchi and
Jingyue Li and
Denys Poshyvanyk and
Dongmei Zhang",,,,,,,
DBLP:conf/sigsoft/HarmanOS25,inproceedings,"Harden and Catch for Just-in-Time Assured LLM-Based Software Testing:
Open Research Challenges","Mark Harman and
Peter W. O'Hearn and
Shubho Sengupta",2025,,"Proceedings of the 33rd {ACM} International Conference on the Foundations
of Software Engineering, {FSE} Companion 2025, Clarion Hotel Trondheim,
Trondheim, Norway, June 23-28, 2025",,,1--17,10.1145/3696630.3734199,https://doi.org/10.1145/3696630.3734199,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigsoft/HarmanOS25.bib,"Sat, 09 Aug 2025 01:00:00 +0200",{ACM},"Leonardo Montecchi and
Jingyue Li and
Denys Poshyvanyk and
Dongmei Zhang",,,,,,,
DBLP:conf/sigsoft/LarsenEHDJ25,inproceedings,An {LLM} Assistant for Software Project Onboarding,"Knud Ronau Larsen and
Magnus Edvall and
Truong Ho{-}Quang and
Felix Dobslaw and
Rodi Jolak",2025,,"Proceedings of the 33rd {ACM} International Conference on the Foundations
of Software Engineering, {FSE} Companion 2025, Clarion Hotel Trondheim,
Trondheim, Norway, June 23-28, 2025",,,1345--1352,10.1145/3696630.3728719,https://doi.org/10.1145/3696630.3728719,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigsoft/LarsenEHDJ25.bib,"Sat, 09 Aug 2025 01:00:00 +0200",{ACM},"Leonardo Montecchi and
Jingyue Li and
Denys Poshyvanyk and
Dongmei Zhang",,,,,,,"New developers commonly join software teams for projects in the advanced stages of development. They frequently face barriers, especially in software comprehension, that impede their ability to make early contributions. To aid newcomers in understanding software projects, we have developed an LLM-based tool called SPAC-B that can answer software project-specific questions. In this study, a case study is conducted to investigate the accuracy of SPAC-B‚Äôs answers to common developer questions when resolving issues on two open-source projects regarding relevance, completeness, and correctness. On a Likert scale (1-5), answers from SPAC-B scored a mean of 4.6 in relevance and 4.3 in completeness and correctness. An experiment with ten software developers is performed to further examine SPAC-B‚Äôs ability to help newcomers formulate plans for resolving real-life open-source issues. Results show that the participants could make a better implementation plan with the use of SPAC-B and 8/10 participants found the tool to be helpful."
DBLP:conf/sigsoft/LiuZX0L25,inproceedings,Towards the Versioning of LLM-Agent-Based Software,"Chengwei Liu and
Lyuye Zhang and
Xiufeng Xu and
Wenbo Guo and
Yang Liu",2025,,"Proceedings of the 33rd {ACM} International Conference on the Foundations
of Software Engineering, {FSE} Companion 2025, Clarion Hotel Trondheim,
Trondheim, Norway, June 23-28, 2025",,,1619--1622,10.1145/3696630.3728714,https://doi.org/10.1145/3696630.3728714,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigsoft/LiuZX0L25.bib,"Sat, 09 Aug 2025 01:00:00 +0200",{ACM},"Leonardo Montecchi and
Jingyue Li and
Denys Poshyvanyk and
Dongmei Zhang",,,,,,,
DBLP:conf/sigsoft/Poniszewska-Maranda25,inproceedings,"Autonomous agents in software development for information retrieval
using {LLM} models","Aneta Poniszewska{-}Maranda and
Maciej Kopa",2025,,"Proceedings of the 33rd {ACM} International Conference on the Foundations
of Software Engineering, {FSE} Companion 2025, Clarion Hotel Trondheim,
Trondheim, Norway, June 23-28, 2025",,,1240--1241,10.1145/3696630.3731432,https://doi.org/10.1145/3696630.3731432,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigsoft/Poniszewska-Maranda25.bib,"Sat, 09 Aug 2025 01:00:00 +0200",{ACM},"Leonardo Montecchi and
Jingyue Li and
Denys Poshyvanyk and
Dongmei Zhang",,,,,,,
DBLP:conf/sigsoft/Ronanki25,inproceedings,"Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent
System oriented Software Engineering",Krishna Ronanki,2025,,"Proceedings of the 33rd {ACM} International Conference on the Foundations
of Software Engineering, {FSE} Companion 2025, Clarion Hotel Trondheim,
Trondheim, Norway, June 23-28, 2025",,,1333--1337,10.1145/3696630.3728717,https://doi.org/10.1145/3696630.3728717,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigsoft/Ronanki25.bib,"Tue, 05 Aug 2025 01:00:00 +0200",{ACM},"Leonardo Montecchi and
Jingyue Li and
Denys Poshyvanyk and
Dongmei Zhang",,,,,,,
DBLP:conf/sigsoft/TrinkenreichCHB25,inproceedings,"Get on the Train or be Left on the Station: Using LLMs for Software
Engineering Research","Bianca Trinkenreich and
Fabio Calefato and
Geir Hanssen and
Kelly Blincoe and
Marcos Kalinowski and
Mauro Pezz{\`{e}} and
Paolo Tell and
Margaret{-}Anne D. Storey",2025,,"Proceedings of the 33rd {ACM} International Conference on the Foundations
of Software Engineering, {FSE} Companion 2025, Clarion Hotel Trondheim,
Trondheim, Norway, June 23-28, 2025",,,1503--1507,10.1145/3696630.3731666,https://doi.org/10.1145/3696630.3731666,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigsoft/TrinkenreichCHB25.bib,"Sat, 09 Aug 2025 01:00:00 +0200",{ACM},"Leonardo Montecchi and
Jingyue Li and
Denys Poshyvanyk and
Dongmei Zhang",,,,,,,
DBLP:conf/sigsoft/WivestadB25,inproceedings,"Attitudes Towards {LLM} Use Among Software Engineering Researchers:
Results From {A} Two-Phase Survey Study","Viggo Tellefsen Wivestad and
Astri Moksnes Barbala",2025,,"Proceedings of the 33rd {ACM} International Conference on the Foundations
of Software Engineering, {FSE} Companion 2025, Clarion Hotel Trondheim,
Trondheim, Norway, June 23-28, 2025",,,1531--1535,10.1145/3696630.3731671,https://doi.org/10.1145/3696630.3731671,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigsoft/WivestadB25.bib,"Sat, 09 Aug 2025 01:00:00 +0200",{ACM},"Leonardo Montecchi and
Jingyue Li and
Denys Poshyvanyk and
Dongmei Zhang",,,,,,,
DBLP:conf/sigsoft/ZhangZJHD025,inproceedings,"AgentFM: Role-Aware Failure Management for Distributed Databases with
LLM-Driven Multi-Agents","Lingzhe Zhang and
Yunpeng Zhai and
Tong Jia and
Xiaosong Huang and
Chiming Duan and
Ying Li",2025,,"Proceedings of the 33rd {ACM} International Conference on the Foundations
of Software Engineering, {FSE} Companion 2025, Clarion Hotel Trondheim,
Trondheim, Norway, June 23-28, 2025",,,525--529,10.1145/3696630.3728492,https://doi.org/10.1145/3696630.3728492,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigsoft/ZhangZJHD025.bib,"Mon, 06 Oct 2025 01:00:00 +0200",{ACM},"Leonardo Montecchi and
Jingyue Li and
Denys Poshyvanyk and
Dongmei Zhang",,,,,,,"Distributed databases are critical infrastructures for today's large-scale software systems, making effective failure management essential to ensure software availability. However, existing approaches often overlook the role distinctions within distributed databases and rely on small-scale models with limited generalization capabilities. In this paper, we conduct a preliminary empirical study to emphasize the unique significance of different roles. Building on this insight, we propose AgentFM, a role-aware failure management framework for distributed databases powered by LLM-driven multi-agents. AgentFM addresses failure management by considering system roles, data roles, and task roles, with a meta-agent orchestrating these components. Preliminary evaluations using Apache IoTDB demonstrate the effectiveness of AgentFM and open new directions for further research."
DBLP:conf/sosp/ZhangDLKMWLYLLZ25,inproceedings,Jenga: Effective Memory Management for Serving {LLM} with Heterogeneity,"Chen Zhang and
Kuntai Du and
Shu Liu and
Woosuk Kwon and
Xiangxi Mo and
Yufeng Wang and
Xiaoxuan Liu and
Kaichao You and
Zhuohan Li and
Mingsheng Long and
Jidong Zhai and
Joseph Gonzalez and
Ion Stoica",2025,,"Proceedings of the {ACM} {SIGOPS} 31st Symposium on Operating Systems
Principles, {SOSP} 2025, Lotte Hotel World, Seoul, Republic of Korea,
October 13-16, 2025",,,446--461,10.1145/3731569.3764823,https://doi.org/10.1145/3731569.3764823,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sosp/ZhangDLKMWLYLLZ25.bib,"Mon, 06 Oct 2025 01:00:00 +0200",{ACM},"Youjip Won and
Youngjin Kwon and
Ding Yuan and
Rebecca Isaacs",,,,,,,
DBLP:conf/soups/WangZWHCFH025,inproceedings,"Can You Walk Me Through It? Explainable {SMS} Phishing Detection using
LLM-based Agents","Yizhu Wang and
Haoyu Zhai and
Chenkai Wang and
Qingying Hao and
Nick A. Cohen and
Roopa Foulger and
Jonathan A. Handler and
Gang Wang",2025,,"Twenty-First Symposium on Usable Privacy and Security, {SOUPS} 2025,
Seattle, WA, USA, August 10-12, 2025",,,37--56,,https://www.usenix.org/conference/soups2025/presentation/wang,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/soups/WangZWHCFH025.bib,"Fri, 22 Aug 2025 15:03:50 +0200",{USENIX} Association,"Patrick Gage Kelley and
Mainack Mondal and
Kami Vaniea",,,,,,,
DBLP:conf/splc/ZineQR25,inproceedings,LLM-based Co-Evolution of Configurable Software Systems,"Nada Zine and
Cl{\'{e}}ment Quinton and
Romain Rouvoy",2025,,"Proceedings of the 29th {ACM} International Systems and Software Product
Line Conference - Volume A, {SPLC-A} 2025, {A} Coru{\~{n}}a, Spain,
September 1-5, 2025",,,27--38,10.1145/3744915.3748460,https://doi.org/10.1145/3744915.3748460,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/splc/ZineQR25.bib,"Fri, 26 Sep 2025 08:04:32 +0200",{ACM},"Miguel R. Luaces and
Tirso V. Rodeiro and
Sandra Greiner and
Jos{\'{e}} {\'{A}}ngel Galindo Duarte and
Tao Yue and
Kentaro Yoshimura and
Laura Semini and
Maxime Cordy and
Maider Azanza and
Jacob Kr{\""{u}}ger and
Gilles Perrouin and
Sophie Fortz and
Iris Groher and
Daniel{-}Jesus Munoz and
Klaus Schmid and
Francisca P{\'{e}}rez and
Jessie Galasso{-}Carbonnel and
Jos{\'{e}} Miguel Horcas and
Kevin Feichtinger",,,,,,,
DBLP:conf/tma/TalibzadeBD25,inproceedings,On {LLM} Embeddings for Vulnerability Management,"Rustam Talibzade and
Francesco Bergadano and
Idilio Drago",2025,,"9th Network Traffic Measurement and Analysis Conference, {TMA} 2025,
Copenhagen, Denmark, June 10-13, 2025",,,1--4,10.23919/TMA66427.2025.11097007,https://doi.org/10.23919/TMA66427.2025.11097007,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/tma/TalibzadeBD25.bib,"Wed, 20 Aug 2025 20:49:22 +0200",{IEEE},,,,,,,,"Effective vulnerability management is fundamental for cybersecurity, requiring significant manual effort to identify, classify, and prioritize reports. Automating this process could reduce analyst workload and improve response to threats. We explore the use of Large Language Models (LLMs) for representing and analyzing Common Vulnerabilities and Exposures (CVEs), specifically their ability to generate semantic embeddings that capture the nature of vulnerabilities from textual descriptions. Using a dataset of 4,710 CVEs, we generate vector embeddings with multiple LLMs. We then apply both unsupervised clustering and supervised classification to evaluate the quality of the embeddings. Our preliminary results show that some LLMs ‚Äì in particular Llama 3.2 ‚Äì map similar vulnerabilities together in the embedding space. Embeddings seen to position related vulnerabilities in nearby regions, with certain clusters showing strong correspondence to specific categories like SQL Injection (CWE-89) and Path Traversal (CWE-22). A simple KNN classifier using only the embeddings achieves around 50% accuracy when categorizing CVEs. This is a remarkable result, considering the very high number of classes in the problem. Our initial findings show potential for the use of LLMs in vulnerability management processes, calling for deeper analysis on how to better learn representations from vulnerability reports."
DBLP:conf/uai/GaoSWY25,inproceedings,"A Fast Optimization View: Reformulating Single Layer Attention in
{LLM} Based on Tensor and {SVM} Trick, and Solving It in Matrix Multiplication
Time","Yeqi Gao and
Zhao Song and
Weixin Wang and
Junze Yin",2025,,"Conference on Uncertainty in Artificial Intelligence, Rio Othon Palace,
Rio de Janeiro, Brazil, 21-25 July 2025",286,,1381--1452,,https://proceedings.mlr.press/v286/gao25a.html,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/uai/GaoSWY25.bib,"Mon, 18 Aug 2025 15:30:39 +0200",{PMLR},"Silvia Chiappa and
Sara Magliacane",Proceedings of Machine Learning Research,,,,,,
DBLP:conf/uist/ChenWLLSZQ25,inproceedings,"CoGrader: Transforming Instructors' Assessment of Project Reports
through Collaborative {LLM} Integration","Zixin Chen and
Jiachen Wang and
Yumeng Li and
Haobo Li and
Chuhan Shi and
Rong Zhang and
Huamin Qu",2025,,"Proceedings of the 38th Annual {ACM} Symposium on User Interface Software
and Technology, {UIST} 2025, Busan, Korea, 28 September 2025 - 1 October
2025",,,129:1--129:18,10.1145/3746059.3747670,https://doi.org/10.1145/3746059.3747670,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/uist/ChenWLLSZQ25.bib,"Tue, 30 Sep 2025 16:23:37 +0200",{ACM},"Andrea Bianchi and
Elena L. Glassman and
Wendy E. Mackay and
Shengdong Zhao and
Jeeeun Kim and
Ian Oakley",,,,,,,"Grading project reports are increasingly significant in today's educational landscape, where they serve as key assessments of students'comprehensive problem-solving abilities. However, it remains challenging due to the multifaceted evaluation criteria involved, such as creativity and peer-comparative achievement. Meanwhile, instructors often struggle to maintain fairness throughout the time-consuming grading process. Recent advances in AI, particularly large language models, have demonstrated potential for automating simpler grading tasks, such as assessing quizzes or basic writing quality. However, these tools often fall short when it comes to complex metrics, like design innovation and the practical application of knowledge, that require an instructor's educational insights into the class situation. To address this challenge, we conducted a formative study with six instructors and developed CoGrader, which introduces a novel grading workflow combining human-LLM collaborative metrics design, benchmarking, and AI-assisted feedback. CoGrader was found effective in improving grading efficiency and consistency while providing reliable peer-comparative feedback to students. We also discuss design insights and ethical considerations for the development of human-AI collaborative grading systems."
DBLP:conf/um/GhoochaniSFDOE25,inproceedings,"From Feedback to Formative Guidance: Leveraging LLMs for Personalized
Support in Programming Projects","Fatemeh Ghoochani and
Jonas Scharfenberger and
Burkhardt Funk and
Raoul Doublan and
Mayur Jakharabhai Odedra and
Bennet Etsiwah",2025,,"Adjunct Proceedings of the 33rd {ACM} Conference on User Modeling,
Adaptation and Personalization, {UMAP} Adjunct 2025, New York City,
NY, USA, June 16-19, 2025",,,398--403,10.1145/3708319.3733808,https://doi.org/10.1145/3708319.3733808,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/um/GhoochaniSFDOE25.bib,"Sun, 06 Jul 2025 01:00:00 +0200",{ACM},,,,,,,,"Large Language Models (LLMs) offer scalable opportunities to personalize feedback in education, yet their trustworthiness and effectiveness remain underexplored. We present a study conducted in an introductory programming and data science course with approximately 1,400 first-year university students. A subset of these students received both peer and LLM-generated feedback on their individual programming projects. Our results show that 56% of students preferred the LLM feedback, and 52% could not reliably distinguish it from human-written feedback. Student ratings suggest that LLM feedback is perceived as helpful, constructive, and relevant, though it often lacks personalized depth and motivational nuance. These findings underline the potential of LLMs to support scalable, personalized education, while pointing to key areas for responsible improvement. Based on these insights, we outline the future roadmap for the course in which LLM-generated feedback supports students in their learning journey but also instructors through monitoring student performance and helping to allocate instructional resources more effectively. Given limited human resources this approach enables personalized instructor feedback to be scaled to a large group of students."
DBLP:conf/wcnc/HabibIOEBGE25,inproceedings,"LLM-Based Intent Processing and Network Optimization Using Attention-Based
Hierarchical Reinforcement Learning","Md Arafat Habib and
Pedro Enrique Iturria{-}Rivera and
Yigit Ozcan and
Medhat H. M. Elsayed and
Majid Bavand and
Raimundas Gaigalas and
Melike Erol{-}Kantarci",2025,,"2025 {IEEE} Wireless Communications and Networking Conference (WCNC),
Milan, Italy, March 24-27, 2025",,,1--6,10.1109/WCNC61545.2025.10978505,https://doi.org/10.1109/WCNC61545.2025.10978505,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/wcnc/HabibIOEBGE25.bib,"Mon, 26 May 2025 09:41:54 +0200",{IEEE},,,,,,,,"Intent-based network automation is a promising tool that enables easier network management; however, certain challenges must be addressed effectively. These are: 1) processing intents, i.e., identification of logic and necessary parameters to fulfill an intent, 2) validating an intent to align it with current network status, and 3) satisfying intents via network optimizing applications. This paper addresses these points via a three-fold strategy to introduce intent-based automation for modern 5G architectures. First, intents are processed via a lightweight Large Language Model (LLM). Secondly, once an intent is processed, it is validated against future incoming traffic volume profiles (high or low). Finally, a series of network optimization applications has been developed. With their machine learning-based functionalities, they can improve certain key performance indicators such as throughput, delay, and energy efficiency. In the final stage, using an attention-based hierarchical reinforcement learning algorithm, these applications are optimally initiated to satisfy the intent of an operator. Our simulations show that the proposed method can achieve at least a 12% increase in throughput, a 17.1% increase in energy efficiency, and a 26.5% decrease in network delay compared to the baseline algorithms."
DBLP:conf/wer/AlmeidaCOABFML25,inproceedings,"From Elicitation Interviews to Software Requirements: Evaluating {LLM}
Performance in Requirement Generation","Camila Almeida and
Isaque Copque and
Alvaro Oliveira and
Murilo Guerreiro Arouca and
Adriano Barbosa and
S{\'{a}}vio Freire and
Manoel G. Mendon{\c{c}}a and
Julio C{\'{e}}sar Sampaio P. Leite",2025,,"28th Workshop on Requirements Engineering, {WER} 2025, Rio de Janeiro,
Brazil, 20-22 August 2025",,,,10.29327/1588952.28-12,https://doi.org/10.29327/1588952.28-12,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/wer/AlmeidaCOABFML25.bib,"Fri, 22 Aug 2025 15:03:50 +0200","Even3, Brazil","Marcela N. Ridao and
M{\'{a}}rcia Lucena and
Rodrigo Pereira dos Santos",,,,,,,"Recent advancements in artificial intelligence (AI), particularly in large language models (LLMs), offer new possibilities for automating requirements generation from elicitation interviews. This study compares the performance of ChatGPT-4 and DeepSeek-V3 in generating software requirements based on transcribed stakeholder interviews. Using two case studies, the LLMs were tasked with identifying functional and non-functional requirements. The results indicate that ChatGPT-4 performed better in extracting precise requirements, particularly nonfunctional ones, while DeepSeek-V3 demonstrated advantages in efficiency. However, both models exhibited limitations in handling ambiguity and properly categorizing requirements. This study highlights the potential of LLMs in Requirements Engineering while emphasizing the need for improved prompt/dialogues techniques and human supervision. Future research should explore hybrid AI-human approaches and domain-specific fine-tuning to enhance requirement extraction accuracy."
DBLP:conf/wsese/FelizardoDCGMGS25,inproceedings,"On the Difficulties of Conducting and Replicating Systematic Literature
Reviews Studies Using LLMs in Software Engineering","K{\'{a}}tia Romero Felizardo and
Anderson Deizepe and
Daniel Coutinho and
Genildo Gomes and
Maria A. C. Meireles and
Marco Aur{\'{e}}lio Gerosa and
Igor Steinmacher",2025,,"{IEEE/ACM} International Workshop on Methodological Issues with Empirical
Studies in Software Engineering, WSESE@ICSE 2025, Ottawa, ON, Canada,
May 3, 2025",,,20--23,10.1109/WSESE66602.2025.00010,https://doi.org/10.1109/WSESE66602.2025.00010,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/wsese/FelizardoDCGMGS25.bib,"Sat, 06 Sep 2025 01:00:00 +0200",{IEEE},,,,,,,,"The Software Engineering (SE) community has adopted Systematic Literature Reviews (SLRs) to summarize the state-of-the-art in specific research topics. SLRs offer benefits such as synthesizing evidence from diverse studies to generate auditable results following a reproducible approach, and identifying research gaps for future exploration. However, the process is effort-intensive, prone to errors, and lays various challenges during their conduction. To overcome some of these issues, there is a growing belief that Large Language Models (LLMs) can support systematic literature reviews. While the literature has shown promising results in social sciences, more evidence of its accuracy is needed in technical fields like SE. In this context, studies and replications are essential in verifying the benefits and drawbacks of applying LLMs in systematic literature reviews. This paper discusses the difficulties in conducting and replicating studies that adopt LLMs to support systematic literature in SE. As an implication, we identified the challenges of adopting LLM in SLRs and offered a list of open issues for future research."
DBLP:conf/wsese/MartinoCPFM25,inproceedings,"A Framework for Using LLMs for Repository Mining Studies in Empirical
Software Engineering","Vincenzo De Martino and
Joel Casta{\~{n}}o and
Fabio Palomba and
Xavier Franch and
Silverio Mart{\'{\i}}nez{-}Fern{\'{a}}ndez",2025,,"{IEEE/ACM} International Workshop on Methodological Issues with Empirical
Studies in Software Engineering, WSESE@ICSE 2025, Ottawa, ON, Canada,
May 3, 2025",,,6--11,10.1109/WSESE66602.2025.00008,https://doi.org/10.1109/WSESE66602.2025.00008,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/wsese/MartinoCPFM25.bib,"Thu, 24 Jul 2025 01:00:00 +0200",{IEEE},,,,,,,,"Context: The emergence of Large Language Mod-els (LLMs) has significantly transformed Software Engineering (SE) by providing innovative methods for analyzing software repositories. Objectives: Our objective is to establish a practical framework for future SE researchers needing to enhance the data collection and dataset while conducting software repository mining studies using LLMs. Method: This experience report shares insights from two previous repository mining studies, focusing on the methodologies used for creating, refining, and validating prompts that enhance the output of LLMs, particularly in the context of data collection in empirical studies. Results: Our research packages a framework, coined Prompt Refinement and Insights for Mining Empirical Software repositories (PRIMES), consisting of a checklist that can improve LLM usage performance, enhance output quality, and minimize errors through iterative processes and comparisons among different LLMs. We also emphasize the significance of reproducibility by implementing mechanisms for tracking model results. Conclusion: Our findings indicate that standardizing prompt engineering and using PRIMES can enhance the reliability and reproducibility of studies utilizing LLMs. Ultimately, this work calls for further research to address challenges like hallucinations, model biases, and cost-effectiveness in integrating LLMs into workflows."
DBLP:conf/www/MaiHCPL0D025,inproceedings,"You Can't Eat Your Cake and Have It Too: The Performance Degradation
of LLMs with Jailbreak Defense","Wuyuao Mai and
Geng Hong and
Pei Chen and
Xudong Pan and
Baojun Liu and
Yuan Zhang and
Haixin Duan and
Min Yang",2025,,"Proceedings of the {ACM} on Web Conference 2025, {WWW} 2025, Sydney,
NSW, Australia, 28 April 2025- 2 May 2025",,,872--883,10.1145/3696410.3714632,https://doi.org/10.1145/3696410.3714632,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/www/MaiHCPL0D025.bib,"Fri, 09 May 2025 01:00:00 +0200",{ACM},"Guodong Long and
Michale Blumestein and
Yi Chang and
Liane Lewin{-}Eytan and
Zi Helen Huang and
Elad Yom{-}Tov",,,,,,,"With the rise of generative large language models (LLMs) like LLaMA and ChatGPT, these models have significantly transformed daily life and work by providing advanced insights. However, as jailbreak attacks continue to circumvent built-in safety mechanisms, exploiting carefully crafted scenarios or tokens, the safety risks of LLMs have come into focus. While numerous defense strategies-such as prompt detection, modification, and model fine-tuning-have been proposed to counter these attacks, a critical question arises: do these defenses compromise the utility and usability of LLMs for legitimate users? Existing research predominantly focuses on the effectiveness of defense strategies without thoroughly examining their impact on performance, leaving a gap in understanding the trade-offs between LLM safety and performance. Our research addresses this gap by conducting a comprehensive study on the utility degradation, safety elevation, and exaggerated-safety escalation of LLMs with jailbreak defense strategies. We propose USEBench, a novel benchmark designed to evaluate these aspects, along with USEIndex, a comprehensive metric for assessing overall model performance. Through experiments on seven state-of-the-art LLMs, we found that mainstream jailbreak defenses fail to ensure both safety and performance simultaneously. Although model-finetuning performs the best overall, their effectiveness varies across LLMs. Furthermore, vertical comparisons reveal that developers commonly prioritize performance over safety when iterating or fine-tuning their LLMs."
DBLP:conf/www/YuHFDCYS25,inproceedings,BiasNavi: LLM-Empowered Data Bias Management,"Junliang Yu and
Jay Thai Duong Huynh and
Shaoyang Fan and
Gianluca Demartini and
Tong Chen and
Hongzhi Yin and
Shazia Sadiq",2025,,"Companion Proceedings of the {ACM} on Web Conference 2025, {WWW} 2025,
Sydney, NSW, Australia, 28 April 2025 - 2 May 2025",,,2939--2942,10.1145/3701716.3715169,https://doi.org/10.1145/3701716.3715169,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/www/YuHFDCYS25.bib,"Wed, 11 Jun 2025 01:00:00 +0200",{ACM},"Guodong Long and
Michale Blumestein and
Yi Chang and
Liane Lewin{-}Eytan and
Zi Helen Huang and
Elad Yom{-}Tov",,,,,,,"Bias in datasets undermines the fairness, transparency, and reliability of AI systems, presenting critical challenges across applications. Existing tools for managing data bias often remain inaccessible to non-experts or struggle with the complexities of domain-specific datasets. In this work, we introduce BiasNavi, an large language models (LLM)-empowered toolkit for data bias management. BiasNavi features an autonomous agent that seamlessly integrates with modules for bias identification, measurement, surfacing, and adaptation, reasoning over data and interactions to adaptively guide users through the bias management pipeline. With intuitive and personalized interfaces, BiasNavi empowers users to customize their workflows and address data bias effectively. A case study with the COMPAS dataset demonstrates how BiasNavi, by leveraging the advanced reasoning capabilities of LLMs, democratizes responsible AI practices, making bias management both accessible and effective for users with varying levels of expertise. BiasNavi is available at: https://github.com/CIRES-Hub/BiasNavi."
DBLP:data/11/PaivaC25,misc,LLM-Generated Software Requirements from GitHub Issues (Version 4),"Guilherme Pereira Paiva and
Edna Dias Canedo",2025,,,,,,10.5281/ZENODO.15003631,https://doi.org/10.5281/zenodo.15003631,Accessed on YYYY-MM-DD.,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/data/11/PaivaC25.bib,"Wed, 27 Aug 2025 01:00:00 +0200",Zenodo,,,\url{https://doi.org/10.5281/zenodo.15003631},March,,,,
DBLP:journals/corr/abs-2501-00217,article,"The Potential of LLMs in Automating Software Testing: From Generation
to Reporting","Betim Sherifi and
Khaled Slhoub and
Fitzroy Nembhard",2025,CoRR,,abs/2501.00217,,,10.48550/ARXIV.2501.00217,https://doi.org/10.48550/arXiv.2501.00217,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2501-00217.bib,"Thu, 06 Feb 2025 00:00:00 +0100",,,,,,2501.00217,arXiv,,"Having a high quality software is essential in software engineering, which requires robust validation and verification processes during testing activities. Manual testing, while effective, can be time consuming and costly, leading to an increased demand for automated methods. Recent advancements in Large Language Models (LLMs) have significantly influenced software engineering, particularly in areas like requirements analysis, test automation, and debugging. This paper explores an agent-oriented approach to automated software testing, using LLMs to reduce human intervention and enhance testing efficiency. The proposed framework integrates LLMs to generate unit tests, visualize call graphs, and automate test execution and reporting. Evaluations across multiple applications in Python and Java demonstrate the system's high test coverage and efficient operation. This research underscores the potential of LLM-powered agents to streamline software testing workflows while addressing challenges in scalability and accuracy."
DBLP:journals/corr/abs-2501-00655,article,Finding Missed Code Size Optimizations in Compilers using LLMs,"Davide Italiano and
Chris Cummins",2025,CoRR,,abs/2501.00655,,,10.48550/ARXIV.2501.00655,https://doi.org/10.48550/arXiv.2501.00655,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2501-00655.bib,"Mon, 10 Feb 2025 00:00:00 +0100",,,,,,2501.00655,arXiv,,"Compilers are complex, and significant effort has been expended on testing them. Techniques such as random program generation and differential testing have proved highly effective and have uncovered thousands of bugs in production compilers. The majority of effort has been expended on validating that a compiler produces correct code for a given input, while less attention has been paid to ensuring that the compiler produces performant code. In this work we adapt differential testing to the task of identifying missed optimization opportunities in compilers. We develop a novel testing approach which combines large language models (LLMs) with a series of differential testing strategies and use them to find missing code size optimizations in C / C++ compilers. The advantage of our approach is its simplicity. We offload the complex task of generating random code to an off-the-shelf LLM, and use heuristics and analyses to identify anomalous compiler behavior. Our approach requires fewer than 150 lines of code to implement. This simplicity makes it extensible. By simply changing the target compiler and initial LLM prompt we port the approach from C / C++ to Rust and Swift, finding bugs in both. To date we have reported 24 confirmed bugs in production compilers, and conclude that LLM-assisted testing is a promising avenue for detecting optimization bugs in real world compilers."
DBLP:journals/corr/abs-2501-00826,article,LLM-Powered Multi-Agent System for Automated Crypto Portfolio Management,"Yichen Luo and
Yebo Feng and
Jiahua Xu and
Paolo Tasca and
Yang Liu",2025,CoRR,,abs/2501.00826,,,10.48550/ARXIV.2501.00826,https://doi.org/10.48550/arXiv.2501.00826,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2501-00826.bib,"Mon, 05 May 2025 01:00:00 +0200",,,,,,2501.00826,arXiv,,"Cryptocurrency investment is inherently difficult due to its shorter history compared to traditional assets, the need to integrate vast amounts of data from various modalities, and the requirement for complex reasoning. While deep learning approaches have been applied to address these challenges, their black-box nature raises concerns about trust and explainability. Recently, large language models (LLMs) have shown promise in financial applications due to their ability to understand multi-modal data and generate explainable decisions. However, single LLM faces limitations in complex, comprehensive tasks such as asset investment. These limitations are even more pronounced in cryptocurrency investment, where LLMs have less domain-specific knowledge in their training corpora. To overcome these challenges, we propose an explainable, multi-modal, multi-agent framework for cryptocurrency investment. Our framework uses specialized agents that collaborate within and across teams to handle subtasks such as data analysis, literature integration, and investment decision-making for the top 30 cryptocurrencies by market capitalization. The expert training module fine-tunes agents using multi-modal historical data and professional investment literature, while the multi-agent investment module employs real-time data to make informed cryptocurrency investment decisions. Unique intrateam and interteam collaboration mechanisms enhance prediction accuracy by adjusting final predictions based on confidence levels within agent teams and facilitating information sharing between teams. Empirical evaluation using data from November 2023 to September 2024 demonstrates that our framework outperforms single-agent models and market benchmarks in classification, asset pricing, portfolio, and explainability performance."
DBLP:journals/corr/abs-2501-01205,article,"Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving:
{A} Framework for Senior Design Projects","Abdullah M. Almasoud and
Muhammad Rafay Naeem and
Ibrahim Ghaznavi and
Muhammad Imran Taj and
Imran Hashmi and
Junaid Qadir",2025,CoRR,,abs/2501.01205,,,10.48550/ARXIV.2501.01205,https://doi.org/10.48550/arXiv.2501.01205,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2501-01205.bib,"Tue, 11 Mar 2025 00:00:00 +0100",,,,,,2501.01205,arXiv,,
DBLP:journals/corr/abs-2501-02688,article,"Decoding specialised feature neurons in LLMs with the final projection
layer",Harry J. Davies,2025,CoRR,,abs/2501.02688,,,10.48550/ARXIV.2501.02688,https://doi.org/10.48550/arXiv.2501.02688,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2501-02688.bib,"Mon, 17 Feb 2025 00:00:00 +0100",,,,,,2501.02688,arXiv,,
DBLP:journals/corr/abs-2501-03569,article,"What Does a Software Engineer Look Like? Exploring Societal Stereotypes
in LLMs","Muneera Bano and
Hashini Gunatilake and
Rashina Hoda",2025,CoRR,,abs/2501.03569,,,10.48550/ARXIV.2501.03569,https://doi.org/10.48550/arXiv.2501.03569,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2501-03569.bib,"Tue, 18 Feb 2025 00:00:00 +0100",,,,,,2501.03569,arXiv,,
DBLP:journals/corr/abs-2501-06370,article,"Towards a Probabilistic Framework for Analyzing and Improving LLM-Enabled
Software","Juan Manuel Baldonado and
Flavia Bonomo{-}Braberman and
V{\'{\i}}ctor A. Braberman",2025,CoRR,,abs/2501.06370,,,10.48550/ARXIV.2501.06370,https://doi.org/10.48550/arXiv.2501.06370,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2501-06370.bib,"Wed, 19 Feb 2025 00:00:00 +0100",,,,,,2501.0637,arXiv,,
DBLP:journals/corr/abs-2501-09866,article,"Fine-grained Testing for Autonomous Driving Software: a Study on Autoware
with LLM-driven Unit Testing","Wenhan Wang and
Xuan Xie and
Yuheng Huang and
Renzhi Wang and
An Ran Chen and
Lei Ma",2025,CoRR,,abs/2501.09866,,,10.48550/ARXIV.2501.09866,https://doi.org/10.48550/arXiv.2501.09866,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2501-09866.bib,"Mon, 22 Sep 2025 01:00:00 +0200",,,,,,2501.09866,arXiv,,"Testing autonomous driving systems (ADS) is critical to ensuring their reliability and safety. Existing ADS testing works focuses on designing scenarios to evaluate system-level behaviors, while fine-grained testing of ADS source code has received comparatively little attention. To address this gap, we present the first study on testing, specifically unit testing, for ADS source code. Our study focuses on an industrial ADS framework, Autoware. We analyze both human-written test cases and those generated by large language models (LLMs). Our findings reveal that human-written test cases in Autoware exhibit limited test coverage, and significant challenges remain in applying LLM-generated tests for Autoware unit testing. To overcome these challenges, we propose AwTest-LLM, a novel approach to enhance test coverage and improve test case pass rates across Autoware packages."
DBLP:journals/corr/abs-2501-11086,article,Can {LLM} Generate Regression Tests for Software Commits?,"Jing Liu and
Seongmin Lee and
Eleonora Losiouk and
Marcel B{\""{o}}hme",2025,CoRR,,abs/2501.11086,,,10.48550/ARXIV.2501.11086,https://doi.org/10.48550/arXiv.2501.11086,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2501-11086.bib,"Fri, 14 Mar 2025 00:00:00 +0100",,,,,,2501.11086,arXiv,,"Large Language Models (LLMs) have shown tremendous promise in automated software engineering. In this paper, we investigate the opportunities of LLMs for automatic regression test generation for programs that take highly structured, human-readable inputs, such as XML parsers or JavaScript interpreters. Concretely, we explore the following regression test generation scenarios for such programs that have so far been difficult to test automatically in the absence of corresponding input grammars: $\bullet$ Bug finding. Given a code change (e.g., a commit or pull request), our LLM-based approach generates a test case with the objective of revealing any bugs that might be introduced if that change is applied. $\bullet$ Patch testing. Given a patch, our LLM-based approach generates a test case that fails before but passes after the patch. This test can be added to the regression test suite to catch similar bugs in the future. We implement Cleverest, a feedback-directed, zero-shot LLM-based regression test generation technique, and evaluate its effectiveness on 22 commits to three subject programs: Mujs, Libxml2, and Poppler. For programs using more human-readable file formats, like XML or JavaScript, we found Cleverest performed very well. It generated easy-to-understand bug-revealing or bug-reproduction test cases for the majority of commits in just under three minutes -- even when only the code diff or commit message (unless it was too vague) was given. For programs with more compact file formats, like PDF, as expected, it struggled to generate effective test cases. However, the LLM-supplied test cases are not very far from becoming effective (e.g., when used as a seed by a greybox fuzzer or as a starting point by the developer)."
DBLP:journals/corr/abs-2501-12210,article,"You Can't Eat Your Cake and Have It Too: The Performance Degradation
of LLMs with Jailbreak Defense","Wuyuao Mai and
Geng Hong and
Pei Chen and
Xudong Pan and
Baojun Liu and
Yuan Zhang and
Haixin Duan and
Min Yang",2025,CoRR,,abs/2501.12210,,,10.48550/ARXIV.2501.12210,https://doi.org/10.48550/arXiv.2501.12210,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2501-12210.bib,"Wed, 26 Feb 2025 00:00:00 +0100",,,,,,2501.1221,arXiv,,
DBLP:journals/corr/abs-2501-12904,article,A Functional Software Reference Architecture for LLM-Integrated Systems,"Alessio Bucaioni and
Martin Weyssow and
Junda He and
Yunbo Lyu and
David Lo",2025,CoRR,,abs/2501.12904,,,10.48550/ARXIV.2501.12904,https://doi.org/10.48550/arXiv.2501.12904,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2501-12904.bib,"Tue, 25 Feb 2025 00:00:00 +0100",,,,,,2501.12904,arXiv,,
DBLP:journals/corr/abs-2501-15829,article,"Aging-aware {CPU} Core Management for Embodied Carbon Amortization
in Cloud {LLM} Inference","Tharindu B. Hewage and
Shashikant Ilager and
Maria Rodriguez Read and
Rajkumar Buyya",2025,CoRR,,abs/2501.15829,,,10.48550/ARXIV.2501.15829,https://doi.org/10.48550/arXiv.2501.15829,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2501-15829.bib,"Wed, 26 Feb 2025 00:00:00 +0100",,,,,,2501.15829,arXiv,,
DBLP:journals/corr/abs-2501-16155,article,"{CITYWALK:} Enhancing LLM-Based {C++} Unit Test Generation via Project-Dependency
Awareness and Language-Specific Knowledge","Yuwei Zhang and
Qingyuan Lu and
Kai Liu and
Wensheng Dou and
Jiaxin Zhu and
Li Qian and
Chunxi Zhang and
Zheng Lin and
Jun Wei",2025,CoRR,,abs/2501.16155,,,10.48550/ARXIV.2501.16155,https://doi.org/10.48550/arXiv.2501.16155,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2501-16155.bib,"Tue, 08 Apr 2025 01:00:00 +0200",,,,,,2501.16155,arXiv,,"
 Unit testing plays a pivotal role in the software development lifecycle, as it ensures code quality. However, writing high-quality unit tests remains a time-consuming task for developers in practice. More recently, the application of large language models (LLMs) in automated unit test generation has demonstrated promising results. Existing approaches primarily focus on interpreted programming languages (e.g., Java), while mature solutions tailored to compiled programming languages like C++ are yet to be explored. The intricate language features of C++, such as pointers, templates, and virtual functions, pose particular challenges for LLMs in generating both executable and high-coverage unit tests. To tackle the aforementioned problems, this paper introduces
 CITYWALK
 , a novel LLM-based framework for C++ unit test generation.
 CITYWALK
 enhances LLMs by providing a comprehensive understanding of the dependency relationships within the project under test via program analysis. Furthermore,
 CITYWALK
 incorporates language-specific knowledge about C++ derived from project documentation and empirical observations, significantly improving the correctness of the LLM-generated unit tests. We implement
 CITYWALK
 by employing the widely popular LLM GPT-4o. The experimental results show that
 CITYWALK
 outperforms current state-of-the-art approaches on a collection of ten popular C++ projects. Our findings demonstrate the effectiveness of
 CITYWALK
 in generating high-quality C++ unit tests.
"
DBLP:journals/corr/abs-2502-00350,article,OrcaLoca: An {LLM} Agent Framework for Software Issue Localization,"Zhongming Yu and
Hejia Zhang and
Yujie Zhao and
Hanxian Huang and
Matrix Yao and
Ke Ding and
Jishen Zhao",2025,CoRR,,abs/2502.00350,,,10.48550/ARXIV.2502.00350,https://doi.org/10.48550/arXiv.2502.00350,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-00350.bib,"Fri, 07 Mar 2025 00:00:00 +0100",,,,,,2502.0035,arXiv,,"Recent developments in Large Language Model (LLM) agents are revolutionizing Autonomous Software Engineering (ASE), enabling automated coding, problem fixes, and feature improvements. However, localization -- precisely identifying software problems by navigating to relevant code sections -- remains a significant challenge. Current approaches often yield suboptimal results due to a lack of effective integration between LLM agents and precise code search mechanisms. This paper introduces OrcaLoca, an LLM agent framework that improves accuracy for software issue localization by integrating priority-based scheduling for LLM-guided action, action decomposition with relevance scoring, and distance-aware context pruning. Experimental results demonstrate that OrcaLoca becomes the new open-source state-of-the-art (SOTA) in function match rate (65.33%) on SWE-bench Lite. It also improves the final resolved rate of an open-source framework by 6.33 percentage points through its patch generation integration."
DBLP:journals/corr/abs-2502-01273,article,Analysis of Student-LLM Interaction in a Software Engineering Project,"Naman Agrawal and
Ridwan Shariffdeen and
Guanlin Wang and
Sanka Rasnayaka and
Ganesh Neelakanta Iyer",2025,CoRR,,abs/2502.01273,,,10.48550/ARXIV.2502.01273,https://doi.org/10.48550/arXiv.2502.01273,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-01273.bib,"Tue, 05 Aug 2025 01:00:00 +0200",,,,,,2502.01273,arXiv,,
DBLP:journals/corr/abs-2502-02009,article,"LLMSecConfig: An LLM-Based Approach for Fixing Software Container
Misconfigurations","Ziyang Ye and
Triet Huynh Minh Le and
M. Ali Babar",2025,CoRR,,abs/2502.02009,,,10.48550/ARXIV.2502.02009,https://doi.org/10.48550/arXiv.2502.02009,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-02009.bib,"Mon, 10 Mar 2025 00:00:00 +0100",,,,,,2502.02009,arXiv,,
DBLP:journals/corr/abs-2502-02368,article,"Evaluating the Effectiveness of LLMs in Fixing Maintainability Issues
in Real-World Projects","Henrique Gomes Nunes and
Eduardo Figueiredo and
Larissa Rocha Soares and
Sarah Nadi and
Fischer Ferreira and
Geanderson E. dos Santos",2025,CoRR,,abs/2502.02368,,,10.48550/ARXIV.2502.02368,https://doi.org/10.48550/arXiv.2502.02368,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-02368.bib,"Sat, 06 Sep 2025 01:00:00 +0200",,,,,,2502.02368,arXiv,,
DBLP:journals/corr/abs-2502-02675,article,"Exploring LLMs Impact on Student-Created User Stories and Acceptance
Testing in Software Development","Allan Brockenbrough and
Henry Feild and
Dominic Salinas",2025,CoRR,,abs/2502.02675,,,10.48550/ARXIV.2502.02675,https://doi.org/10.48550/arXiv.2502.02675,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-02675.bib,"Tue, 11 Mar 2025 00:00:00 +0100",,,,,,2502.02675,arXiv,,
DBLP:journals/corr/abs-2502-03964,article,"""It Warned Me Just at the Right Moment"": Exploring LLM-based Real-time
Detection of Phone Scams","Zitong Shen and
Sineng Yan and
Youqian Zhang and
Xiapu Luo and
Grace Ngai and
Eugene Yujun Fu",2025,CoRR,,abs/2502.03964,,,10.48550/ARXIV.2502.03964,https://doi.org/10.48550/arXiv.2502.03964,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-03964.bib,"Wed, 12 Mar 2025 00:00:00 +0100",,,,,,2502.03964,arXiv,,
DBLP:journals/corr/abs-2502-04008,article,"Automating a Complete Software Test Process Using LLMs: An Automotive
Case Study","Shuai Wang and
Yinan Yu and
Robert Feldt and
Dhasarathy Parthasarathy",2025,CoRR,,abs/2502.04008,,,10.48550/ARXIV.2502.04008,https://doi.org/10.48550/arXiv.2502.04008,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-04008.bib,"Wed, 12 Mar 2025 00:00:00 +0100",,,,,,2502.04008,arXiv,,
DBLP:journals/corr/abs-2502-04564,article,My {LLM} might Mimic {AAE} - But When Should it?,"Sandra Sandoval and
Christabel Acquaye and
Kwesi A. Cobbina and
Mohammad Nayeem Teli and
Hal Daum{\'{e}} III",2025,CoRR,,abs/2502.04564,,,10.48550/ARXIV.2502.04564,https://doi.org/10.48550/arXiv.2502.04564,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-04564.bib,"Wed, 12 Mar 2025 00:00:00 +0100",,,,,,2502.04564,arXiv,,"We examine the representation of African American English (AAE) in large language models (LLMs), exploring (a) the perceptions Black Americans have of how effective these technologies are at producing authentic AAE, and (b) in what contexts Black Americans find this desirable. Through both a survey of Black Americans ($n=$ 104) and annotation of LLM-produced AAE by Black Americans ($n=$ 228), we find that Black Americans favor choice and autonomy in determining when AAE is appropriate in LLM output. They tend to prefer that LLMs default to communicating in Mainstream U.S. English in formal settings, with greater interest in AAE production in less formal settings. When LLMs were appropriately prompted and provided in context examples, our participants found their outputs to have a level of AAE authenticity on par with transcripts of Black American speech. Select code and data for our project can be found here: https://github.com/smelliecat/AAEMime.git"
DBLP:journals/corr/abs-2502-05310,article,"Oracular Programming: {A} Modular Foundation for Building LLM-Enabled
Software","Jonathan Laurent and
Andr{\'{e}} Platzer",2025,CoRR,,abs/2502.05310,,,10.48550/ARXIV.2502.05310,https://doi.org/10.48550/arXiv.2502.05310,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-05310.bib,"Wed, 12 Mar 2025 00:00:00 +0100",,,,,,2502.0531,arXiv,,"Large Language Models have proven surprisingly effective at solving a wide range of tasks from just a handful of examples. However, their lack of reliability and modularity limits their capacity to tackle large problems that require many steps of reasoning. In response, researchers have proposed advanced pipelines that leverage domain-specific knowledge to chain smaller prompts, provide intermediate feedback and improve performance through search. However, the current complexity of writing, tuning, maintaining and improving such pipelines has limited their sophistication. We propose oracular programming, a foundational paradigm for building LLM-enabled applications that lets domain experts express high-level problem-solving strategies as programs with unresolved choice points. These choice points are resolved at runtime by LLMs, which generalize from user-provided examples of correct and incorrect decisions. An oracular program is composed of three orthogonal components: a strategy that consists in a nondeterministic program with choice points that can be reified into a search tree, a policy that specifies how to navigate this tree with the help of LLM oracles, and a set of demonstrations that describe successful and unsuccessful search tree navigation scenarios across diverse problem instances. Each component is expressed in a dedicated programming language and can be independently improved or substituted. We address the key programming language design challenges of modularly composing oracular programs and enforcing consistency between their components as they evolve."
DBLP:journals/corr/abs-2502-06193,article,"Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge
in Software Engineering","Ruiqi Wang and
Jiyu Guo and
Cuiyun Gao and
Guodong Fan and
Chun Yong Chong and
Xin Xia",2025,CoRR,,abs/2502.06193,,,10.48550/ARXIV.2502.06193,https://doi.org/10.48550/arXiv.2502.06193,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-06193.bib,"Mon, 05 May 2025 01:00:00 +0200",,,,,,2502.06193,arXiv,,
DBLP:journals/corr/abs-2502-06215,article,"LessLeak-Bench: {A} First Investigation of Data Leakage in LLMs Across
83 Software Engineering Benchmarks","Xin Zhou and
Martin Weyssow and
Ratnadira Widyasari and
Ting Zhang and
Junda He and
Yunbo Lyu and
Jianming Chang and
Beiqi Zhang and
Dan Huang and
David Lo",2025,CoRR,,abs/2502.06215,,,10.48550/ARXIV.2502.06215,https://doi.org/10.48550/arXiv.2502.06215,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-06215.bib,"Wed, 12 Mar 2025 00:00:00 +0100",,,,,,2502.06215,arXiv,,"Large Language Models (LLMs) are widely utilized in software engineering (SE) tasks, such as code generation and automated program repair. However, their reliance on extensive and often undisclosed pre-training datasets raises significant concerns about data leakage, where the evaluation benchmark data is unintentionally ``seen'' by LLMs during the model's construction phase. The data leakage issue could largely undermine the validity of LLM-based research and evaluations. Despite the increasing use of LLMs in the SE community, there is no comprehensive study that assesses the extent of data leakage in SE benchmarks for LLMs yet. To address this gap, this paper presents the first large-scale analysis of data leakage in 83 SE benchmarks concerning LLMs. Our results show that in general, data leakage in SE benchmarks is minimal, with average leakage ratios of only 4.8\%, 2.8\%, and 0.7\% for Python, Java, and C/C++ benchmarks, respectively. However, some benchmarks exhibit relatively higher leakage ratios, which raises concerns about their bias in evaluation. For instance, QuixBugs and BigCloneBench have leakage ratios of 100.0\% and 55.7\%, respectively. Furthermore, we observe that data leakage has a substantial impact on LLM evaluation. We also identify key causes of high data leakage, such as the direct inclusion of benchmark data in pre-training datasets and the use of coding platforms like LeetCode for benchmark construction. To address the data leakage, we introduce \textbf{LessLeak-Bench}, a new benchmark that removes leaked samples from the 83 SE benchmarks, enabling more reliable LLM evaluations in future research. Our study enhances the understanding of data leakage in SE benchmarks and provides valuable insights for future research involving LLMs in SE."
DBLP:journals/corr/abs-2502-06556,article,"ProjectTest: {A} Project-level {LLM} Unit Test Generation Benchmark
and Impact of Error Fixing Mechanisms","Yibo Wang and
Congying Xia and
Wenting Zhao and
Jiangshu Du and
Chunyu Miao and
Zhongfen Deng and
Philip S. Yu and
Chen Xing",2025,CoRR,,abs/2502.06556,,,10.48550/ARXIV.2502.06556,https://doi.org/10.48550/arXiv.2502.06556,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-06556.bib,"Wed, 12 Mar 2025 00:00:00 +0100",,,,,,2502.06556,arXiv,,"Unit test generation has become a promising and important use case of LLMs. However, existing evaluation benchmarks for assessing LLM unit test generation capabilities focus on function- or class-level code rather than more practical and challenging project-level codebases. To address such limitation, we propose ProjectTest, a project-level benchmark for unit test generation covering Python, Java, and JavaScript. ProjectTest features 20 moderate-sized and high-quality projects per language. We evaluate nine frontier LLMs on ProjectTest and the results show that all frontier LLMs tested exhibit moderate performance on ProjectTest on Python and Java, highlighting the difficulty of ProjectTest. We also conduct a thorough error analysis, which shows that even frontier LLMs, such as Claude-3.5-Sonnet, have significant basic yet critical errors, including compilation and cascade errors. Motivated by this observation, we further evaluate all frontier LLMs under manual error-fixing and self-error-fixing scenarios to assess their potential when equipped with error-fixing mechanisms. Our code and dataset is available at \href{https://github.com/YiboWANG214/ProjectTest}{ProjectTest}."
DBLP:journals/corr/abs-2502-07049,article,"LLMs in Software Security: {A} Survey of Vulnerability Detection Techniques
and Insights","Ze Sheng and
Zhicheng Chen and
Shuning Gu and
Heqing Huang and
Guofei Gu and
Jeff Huang",2025,CoRR,,abs/2502.07049,,,10.48550/ARXIV.2502.07049,https://doi.org/10.48550/arXiv.2502.07049,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-07049.bib,"Fri, 14 Mar 2025 00:00:00 +0100",,,,,,2502.07049,arXiv,,"Large Language Models (LLMs) are emerging as transformative tools for software vulnerability detection. Traditional methods, including static and dynamic analysis, face limitations in efficiency, false-positive rates, and scalability with modern software complexity. Through code structure analysis, pattern identification, and repair suggestion generation, LLMs demonstrate a novel approach to vulnerability mitigation.
 This survey examines LLMs in vulnerability detection, analyzing problem formulation, model selection, application methodologies, datasets, and evaluation metrics. We investigate current research challenges, emphasizing cross-language detection, multimodal integration, and repository-level analysis. Based on our findings, we propose solutions addressing dataset scalability, model interpretability, and low-resource scenarios.
 Our contributions include: (1) a systematic analysis of LLM applications in vulnerability detection; (2) a unified framework examining patterns and variations across studies; and (3) identification of key challenges and research directions. This work advances the understanding of LLM-based vulnerability detection. The latest findings are maintained at https://github.com/OwenSanzas/LLM-For-Vulnerability-Detection"
DBLP:journals/corr/abs-2502-07728,article,"Verifying LLM-Generated Code in the Context of Software Verification
with Ada/SPARK","Marcos Cramer and
Lucian McIntyre",2025,CoRR,,abs/2502.07728,,,10.48550/ARXIV.2502.07728,https://doi.org/10.48550/arXiv.2502.07728,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-07728.bib,"Wed, 12 Mar 2025 00:00:00 +0100",,,,,,2502.07728,arXiv,,"Large language models (LLMs) have demonstrated remarkable code generation capabilities, but the correctness of the generated code cannot be inherently trusted. This paper explores the feasibility of using formal software verification, specifically the SPARK framework for Ada, to ensure the reliability of LLM-generated code. We present Marmaragan, a tool that leverages an LLM in order to generate SPARK annotations for existing programs, enabling formal verification of the code. The tool is benchmarked on a curated set of SPARK programs, with annotations selectively removed to test specific capabilities. The performance of Marmaragan with GPT-4o on the benchmark is promising, with correct annotations having been generated for 50.7% of the benchmark cases. The results establish a foundation for future work on combining the power of LLMs with the reliability of formal software verification."
DBLP:journals/corr/abs-2502-09331,article,"Beyond English: The Impact of Prompt Translation Strategies across
Languages and Tasks in Multilingual LLMs","Itai Mondshine and
Tzuf Paz{-}Argaman and
Reut Tsarfaty",2025,CoRR,,abs/2502.09331,,,10.48550/ARXIV.2502.09331,https://doi.org/10.48550/arXiv.2502.09331,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-09331.bib,"Wed, 12 Mar 2025 00:00:00 +0100",,,,,,2502.09331,arXiv,,"Despite advances in the multilingual capabilities of Large Language Models (LLMs) across diverse tasks, English remains the dominant language for LLM research and development. So, when working with a different language, this has led to the widespread practice of pre-translation, i.e., translating the task prompt into English before inference. Selective pre-translation, a more surgical approach, focuses on translating specific prompt components. However, its current use is sporagic and lacks a systematic research foundation. Consequently, the optimal pre-translation strategy for various multilingual settings and tasks remains unclear. In this work, we aim to uncover the optimal setup for pre-translation by systematically assessing its use. Specifically, we view the prompt as a modular entity, composed of four functional parts: instruction, context, examples, and output, either of which could be translated or not. We evaluate pre-translation strategies across 35 languages covering both low and high-resource languages, on various tasks including Question Answering (QA), Natural Language Inference (NLI), Named Entity Recognition (NER), and Abstractive Summarization. Our experiments show the impact of factors as similarity to English, translation quality and the size of pre-trained data, on the model performance with pre-translation. We suggest practical guidelines for choosing optimal strategies in various multilingual settings."
DBLP:journals/corr/abs-2502-12115,article,"SWE-Lancer: Can Frontier LLMs Earn {\textdollar}1 Million from Real-World
Freelance Software Engineering?","Samuel Miserendino and
Michele Wang and
Tejal Patwardhan and
Johannes Heidecke",2025,CoRR,,abs/2502.12115,,,10.48550/ARXIV.2502.12115,https://doi.org/10.48550/arXiv.2502.12115,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-12115.bib,"Wed, 19 Mar 2025 00:00:00 +0100",,,,,,2502.12115,arXiv,,"We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \$1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from \$50 bug fixes to \$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development."
DBLP:journals/corr/abs-2502-12964,article,"Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs","Adi Simhi and
Itay Itzhak and
Fazl Barez and
Gabriel Stanovsky and
Yonatan Belinkov",2025,CoRR,,abs/2502.12964,,,10.48550/ARXIV.2502.12964,https://doi.org/10.48550/arXiv.2502.12964,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-12964.bib,"Wed, 19 Mar 2025 00:00:00 +0100",,,,,,2502.12964,arXiv,,
DBLP:journals/corr/abs-2502-14145,article,LLM-Enhanced Dialogue Management for Full-Duplex Spoken Dialogue Systems,"Hao Zhang and
Weiwei Li and
Rilin Chen and
Vinay Kothapally and
Meng Yu and
Dong Yu",2025,CoRR,,abs/2502.14145,,,10.48550/ARXIV.2502.14145,https://doi.org/10.48550/arXiv.2502.14145,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-14145.bib,"Mon, 07 Apr 2025 01:00:00 +0200",,,,,,2502.14145,arXiv,,"Achieving full-duplex communication in spoken dialogue systems (SDS) requires real-time coordination between listening, speaking, and thinking. This paper proposes a semantic voice activity detection (VAD) module as a dialogue manager (DM) to efficiently manage turn-taking in full-duplex SDS. Implemented as a lightweight (0.5B) LLM fine-tuned on full-duplex conversation data, the semantic VAD predicts four control tokens to regulate turn-switching and turn-keeping, distinguishing between intentional and unintentional barge-ins while detecting query completion for handling user pauses and hesitations. By processing input speech in short intervals, the semantic VAD enables real-time decision-making, while the core dialogue engine (CDE) is only activated for response generation, reducing computational overhead. This design allows independent DM optimization without retraining the CDE, balancing interaction accuracy and inference efficiency for scalable, next-generation full-duplex SDS."
DBLP:journals/corr/abs-2502-14541,article,LLM-based User Profile Management for Recommender System,"Seunghwan Bang and
Hwanjun Song",2025,CoRR,,abs/2502.14541,,,10.48550/ARXIV.2502.14541,https://doi.org/10.48550/arXiv.2502.14541,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-14541.bib,"Wed, 19 Mar 2025 00:00:00 +0100",,,,,,2502.14541,arXiv,,"The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users'purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing this gap, we propose PURE, a novel LLM-based recommendation framework that builds and maintains evolving user profiles by systematically extracting and summarizing key information from user reviews. PURE consists of three core components: a Review Extractor for identifying user preferences and key product features, a Profile Updater for refining and updating user profiles, and a Recommender for generating personalized recommendations using the most current profile. To evaluate PURE, we introduce a continuous sequential recommendation task that reflects real-world scenarios by adding reviews over time and updating predictions incrementally. Our experimental results on Amazon datasets demonstrate that PURE outperforms existing LLM-based methods, effectively leveraging long-term user information while managing token limitations."
DBLP:journals/corr/abs-2502-16399,article,"Ensemble ToT of LLMs and Its Application to Automatic Grading System
for Supporting Self-Learning","Yuki Ito and
Qiang Ma",2025,CoRR,,abs/2502.16399,,,10.48550/ARXIV.2502.16399,https://doi.org/10.48550/arXiv.2502.16399,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-16399.bib,"Thu, 20 Mar 2025 00:00:00 +0100",,,,,,2502.16399,arXiv,,"Providing students with detailed and timely grading feedback is essential for self-learning. While existing LLM-based grading systems are promising, most of them rely on one single model, which limits their performance. To address this, we propose Ensemble Tree-of-Thought (ToT), a framework that enhances LLM outputs by integrating multiple models. Using this framework, we develop a grading system. Ensemble ToT follows three steps: (1) analyzing LLM performance, (2) generating candidate answers, and (3) refining them into a final result. Based on this, our grading system first evaluates the grading tendencies of LLMs, then generates multiple results, and finally integrates them via a simulated debate. Experimental results demonstrate our approach's ability to provide accurate and explainable grading by effectively coordinating multiple LLMs."
DBLP:journals/corr/abs-2502-17441,article,"Renaissance of Literate Programming in the Era of LLMs: Enhancing
LLM-Based Code Generation in Large-Scale Projects","Wuyang Zhang and
Yansong Li and
Zeyu Dong and
Yu Wu and
Yingyao Zhou and
Duolei Wang and
Songsirou Xing and
Chichun Zhou and
Da Shen",2025,CoRR,,abs/2502.17441,,,10.48550/ARXIV.2502.17441,https://doi.org/10.48550/arXiv.2502.17441,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-17441.bib,"Wed, 16 Apr 2025 01:00:00 +0200",,,,,,2502.17441,arXiv,,"Large Language Models (LLMs) have helped programmers increase efficiency through code generation, comprehension, and repair. However, their application to large-scale projects remains challenging due to complex interdependencies and the extensive size of modern codebases. Although Knuth's concept of Literate Programming (LP) combines code and natural language to convey logic and intent, its potential for enhancing relationships in large projects has not been fully explored. In this study, we introduce the idea of Interoperable LP (ILP), which leverages literate programming principles to enhance the development of both small-scale documents and large-scale projects with LLMs. We investigate how LLMs perform under ILP-style instructions for both document-oriented tasks and entire projects. Recognizing that many researchers rely on well-structured templates to guide LLMs, we propose a concise prompt engineering method to write LP documents so LLMs can better be involved in code generation. We also examine the capacity of various LLMs to generate Scheme and Python code on the RepoBench benchmark, illustrating the advantages of our approach. Our findings indicate that ILP with LLMs can enhance LLM-based code generation in large-scale project development."
DBLP:journals/corr/abs-2502-17650,article,"Wearable Meets {LLM} for Stress Management: {A} Duoethnographic Study
Integrating Wearable-Triggered Stressors and {LLM} Chatbots for Personalized
Interventions","Sameer Neupane and
Poorvesh Dongre and
Denis Gracanin and
Santosh Kumar",2025,CoRR,,abs/2502.17650,,,10.48550/ARXIV.2502.17650,https://doi.org/10.48550/arXiv.2502.17650,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-17650.bib,"Fri, 21 Mar 2025 00:00:00 +0100",,,,,,2502.1765,arXiv,,
DBLP:journals/corr/abs-2502-18449,article,"{SWE-RL:} Advancing {LLM} Reasoning via Reinforcement Learning on
Open Software Evolution","Yuxiang Wei and
Olivier Duchenne and
Jade Copet and
Quentin Carbonneaux and
Lingming Zhang and
Daniel Fried and
Gabriel Synnaeve and
Rishabh Singh and
Sida I. Wang",2025,CoRR,,abs/2502.18449,,,10.48550/ARXIV.2502.18449,https://doi.org/10.48550/arXiv.2502.18449,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-18449.bib,"Mon, 07 Apr 2025 01:00:00 +0200",,,,,,2502.18449,arXiv,,"The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data -- the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified -- a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data."
DBLP:journals/corr/abs-2502-18468,article,"{SOK:} Exploring Hallucinations and Security Risks in AI-Assisted
Software Development with Insights for {LLM} Deployment","Mohd Ariful Haque and
Sunzida Siddique and
Md. Mahfuzur Rahman and
Ahmed Rafi Hasan and
Laxmi Rani Das and
Marufa Kamal and
Tasnim Masura and
Kishor Datta Gupta",2025,CoRR,,abs/2502.18468,,,10.48550/ARXIV.2502.18468,https://doi.org/10.48550/arXiv.2502.18468,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-18468.bib,"Thu, 20 Mar 2025 00:00:00 +0100",,,,,,2502.18468,arXiv,,"The integration of Large Language Models (LLMs) such as GitHub Copilot, ChatGPT, Cursor AI, and Codeium AI into software development has revolutionized the coding landscape, offering significant productivity gains, automation, and enhanced debugging capabilities. These tools have proven invaluable for generating code snippets, refactoring existing code, and providing real-time support to developers. However, their widespread adoption also presents notable challenges, particularly in terms of security vulnerabilities, code quality, and ethical concerns. This paper provides a comprehensive analysis of the benefits and risks associated with AI-powered coding tools, drawing on user feedback, security analyses, and practical use cases. We explore the potential for these tools to replicate insecure coding practices, introduce biases, and generate incorrect or non-sensical code (hallucinations). In addition, we discuss the risks of data leaks, intellectual property violations and the need for robust security measures to mitigate these threats. By comparing the features and performance of these tools, we aim to guide developers in making informed decisions about their use, ensuring that the benefits of AI-assisted coding are maximized while minimizing associated risks."
DBLP:journals/corr/abs-2502-18851,article,"Marking Code Without Breaking It: Code Watermarking for Detecting
LLM-Generated Code","Jungin Kim and
Shinwoo Park and
Yo{-}Sub Han",2025,CoRR,,abs/2502.18851,,,10.48550/ARXIV.2502.18851,https://doi.org/10.48550/arXiv.2502.18851,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-18851.bib,"Thu, 20 Mar 2025 00:00:00 +0100",,,,,,2502.18851,arXiv,,"Identifying LLM-generated code through watermarking poses a challenge in preserving functional correctness. Previous methods rely on the assumption that watermarking high-entropy tokens effectively maintains output quality. Our analysis reveals a fundamental limitation of this assumption: syntax-critical tokens such as keywords often exhibit the highest entropy, making existing approaches vulnerable to logic corruption. We present STONE, a syntax-aware watermarking method that embeds watermarks only in non-syntactic tokens and preserves code integrity. For its rigorous assessment, we also introduce STEM, a comprehensive framework that balances three critical dimensions: correctness, detectability, and imperceptibility. Across Python, C++, and Java, STONE preserves correctness, sustains strong detectability, and achieves balanced performance with minimal overhead. Our implementation is available at https://anonymous.4open.science/r/STONE-watermarking-AB4B/."
DBLP:journals/corr/abs-2502-19135,article,"A Temporal Planning Framework for Multi-Agent Systems via LLM-Aided
Knowledge Base Management","Enrico Saccon and
Ahmet Tikna and
Davide De Martini and
Edoardo Lamon and
Luigi Palopoli and
Marco Roveri",2025,CoRR,,abs/2502.19135,,,10.48550/ARXIV.2502.19135,https://doi.org/10.48550/arXiv.2502.19135,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-19135.bib,"Thu, 20 Mar 2025 00:00:00 +0100",,,,,,2502.19135,arXiv,,"This paper presents a novel framework, called PLANTOR (PLanning with Natural language for Task-Oriented Robots), that integrates Large Language Models (LLMs) with Prolog-based knowledge management and planning for multi-robot tasks. The system employs a two-phase generation of a robot-oriented knowledge base, ensuring reusability and compositional reasoning, as well as a three-step planning procedure that handles temporal dependencies, resource constraints, and parallel task execution via mixed-integer linear programming. The final plan is converted into a Behaviour Tree for direct use in ROS2. We tested the framework in multi-robot assembly tasks within a block world and an arch-building scenario. Results demonstrate that LLMs can produce accurate knowledge bases with modest human feedback, while Prolog guarantees formal correctness and explainability. This approach underscores the potential of LLM integration for advanced robotics tasks requiring flexible, scalable, and human-understandable planning."
DBLP:journals/corr/abs-2502-19413,article,"Project Alexandria: Towards Freeing Scientific Knowledge from Copyright
Burdens via LLMs","Christoph Schuhmann and
Gollam Rabby and
Ameya Prabhu and
Tawsif Ahmed and
Andreas Hochlehnert and
Huu Nguyen and
Nick Akinci Heidrich and
Ludwig Schmidt and
Robert Kaczmarczyk and
S{\""{o}}ren Auer and
Jenia Jitsev and
Matthias Bethge",2025,CoRR,,abs/2502.19413,,,10.48550/ARXIV.2502.19413,https://doi.org/10.48550/arXiv.2502.19413,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-19413.bib,"Fri, 21 Mar 2025 00:00:00 +0100",,,,,,2502.19413,arXiv,,"Paywalls, licenses and copyright rules often restrict the broad dissemination and reuse of scientific knowledge. We take the position that it is both legally and technically feasible to extract the scientific knowledge in scholarly texts. Current methods, like text embeddings, fail to reliably preserve factual content, and simple paraphrasing may not be legally sound. We propose a new idea for the community to adopt: convert scholarly documents into knowledge preserving, but style agnostic representations we term Knowledge Units using LLMs. These units use structured data capturing entities, attributes and relationships without stylistic content. We provide evidence that Knowledge Units (1) form a legally defensible framework for sharing knowledge from copyrighted research texts, based on legal analyses of German copyright law and U.S. Fair Use doctrine, and (2) preserve most (~95\%) factual knowledge from original text, measured by MCQ performance on facts from the original copyrighted text across four research domains. Freeing scientific knowledge from copyright promises transformative benefits for scientific research and education by allowing language models to reuse important facts from copyrighted text. To support this, we share open-source tools for converting research documents into Knowledge Units. Overall, our work posits the feasibility of democratizing access to scientific knowledge while respecting copyright."
DBLP:journals/corr/abs-2502-19518,article,Accessing LLMs for Front-end Software Architecture Knowledge,"Luiz Pedro Franciscatto Guerra and
Neil A. Ernst",2025,CoRR,,abs/2502.19518,,,10.48550/ARXIV.2502.19518,https://doi.org/10.48550/arXiv.2502.19518,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2502-19518.bib,"Sun, 29 Jun 2025 01:00:00 +0200",,,,,,2502.19518,arXiv,,
DBLP:journals/corr/abs-2503-01048,article,Personalize Your {LLM:} Fake it then Align it,"Yijing Zhang and
Dyah Adila and
Changho Shin and
Frederic Sala",2025,CoRR,,abs/2503.01048,,,10.48550/ARXIV.2503.01048,https://doi.org/10.48550/arXiv.2503.01048,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-01048.bib,"Tue, 08 Apr 2025 01:00:00 +0200",,,,,,2503.01048,arXiv,,"Personalizing large language models (LLMs) is essential for delivering tailored interactions that improve user experience. Many existing personalization methods require fine-tuning LLMs for each user, rendering them prohibitively expensive for widespread adoption. Although retrieval-based approaches offer a more compute-efficient alternative, they still depend on large, high-quality datasets that are not consistently available for all users. To address this challenge, we propose CHAMELEON, a scalable and efficient personalization approach that uses (1) self-generated personal preference data and (2) representation editing to enable quick and cost-effective personalization. Our experiments on various tasks, including those from the LaMP personalization benchmark, show that CHAMELEON efficiently adapts models to personal preferences, improving instruction-tuned models and outperforms two personalization baselines by an average of 40% across two model architectures."
DBLP:journals/corr/abs-2503-01319,article,{ABFS:} Natural Robustness Testing for LLM-based {NLP} Software,"Mingxuan Xiao and
Yan Xiao and
Shunhui Ji and
Yunhe Li and
Lei Xue and
Pengcheng Zhang",2025,CoRR,,abs/2503.01319,,,10.48550/ARXIV.2503.01319,https://doi.org/10.48550/arXiv.2503.01319,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-01319.bib,"Tue, 08 Apr 2025 01:00:00 +0200",,,,,,2503.01319,arXiv,,"Owing to the exceptional performance of Large Language Models (LLMs) in Natural Language Processing (NLP) tasks, LLM-based NLP software has rapidly gained traction across various domains, such as financial analysis and content moderation. However, these applications frequently exhibit robustness deficiencies, where slight perturbations in input (prompt+example) may lead to erroneous outputs. Current robustness testing methods face two main limitations: (1) low testing effectiveness, limiting the applicability of LLM-based software in safety-critical scenarios, and (2) insufficient naturalness of test cases, reducing the practical value of testing outcomes. To address these issues, this paper proposes ABFS, a straightforward yet effective automated testing method that, for the first time, treats the input prompts and examples as a unified whole for robustness testing. Specifically, ABFS formulates the testing process as a combinatorial optimization problem, employing Best-First Search to identify successful test cases within the perturbation space and designing a novel Adaptive control strategy to enhance test case naturalness. We evaluate the robustness testing performance of ABFS on three datasets across five threat models. On Llama2-13b, the traditional StressTest achieves only a 13.273% success rate, while ABFS attains a success rate of 98.064%, supporting a more comprehensive robustness assessment before software deployment. Compared to baseline methods, ABFS introduces fewer modifications to the original input and consistently generates test cases with superior naturalness. Furthermore, test cases generated by ABFS exhibit stronger transferability and higher testing efficiency, significantly reducing testing costs."
DBLP:journals/corr/abs-2503-01622,article,"{DOVE:} {A} Large-Scale Multi-Dimensional Predictions Dataset Towards
Meaningful {LLM} Evaluation","Eliya Habba and
Ofir Arviv and
Itay Itzhak and
Yotam Perlitz and
Elron Bandel and
Leshem Choshen and
Michal Shmueli{-}Scheuer and
Gabriel Stanovsky",2025,CoRR,,abs/2503.01622,,,10.48550/ARXIV.2503.01622,https://doi.org/10.48550/arXiv.2503.01622,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-01622.bib,"Tue, 08 Apr 2025 01:00:00 +0200",,,,,,2503.01622,arXiv,,
DBLP:journals/corr/abs-2503-02246,article,From Code to Courtroom: LLMs as the New Software Judges,"Junda He and
Jieke Shi and
Terry Yue Zhuo and
Christoph Treude and
Jiamou Sun and
Zhenchang Xing and
Xiaoning Du and
David Lo",2025,CoRR,,abs/2503.02246,,,10.48550/ARXIV.2503.02246,https://doi.org/10.48550/arXiv.2503.02246,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-02246.bib,"Tue, 08 Apr 2025 01:00:00 +0200",,,,,,2503.02246,arXiv,,"Recently, Large Language Models (LLMs) have been increasingly used to automate SE tasks such as code generation and summarization. However, evaluating the quality of LLM-generated software artifacts remains challenging. Human evaluation, while effective, is very costly and time-consuming. Traditional automated metrics like BLEU rely on high-quality references and struggle to capture nuanced aspects of software quality, such as readability and usefulness. In response, the LLM-as-a-Judge paradigm, which employs LLMs for automated evaluation, has emerged. Given that LLMs are typically trained to align with human judgment and possess strong coding abilities and reasoning skills, they hold promise as cost-effective and scalable surrogates for human evaluators. Nevertheless, LLM-as-a-Judge research in the SE community is still in its early stages, with many breakthroughs needed. This forward-looking SE 2030 paper aims to steer the research community toward advancing LLM-as-a-Judge for evaluating LLMgenerated software artifacts, while also sharing potential research paths to achieve this goal. We provide a literature review of existing SE studies on LLM-as-a-Judge and envision these frameworks as reliable, robust, and scalable human surrogates capable of evaluating software artifacts with consistent, multi-faceted assessments by 2030 and beyond. To validate this vision, we analyze the limitations of current studies, identify key research gaps, and outline a detailed roadmap to guide future developments of LLM-as-a-Judge in software engineering. While not intended to be a definitive guide, our work aims to foster further research and adoption of LLM-as-a-Judge frameworks within the SE community, ultimately improving the effectiveness and scalability of software artifact evaluation methods."
DBLP:journals/corr/abs-2503-02400,article,Promptware Engineering: Software Engineering for {LLM} Prompt Development,"Zhenpeng Chen and
Chong Wang and
Weisong Sun and
Guang Yang and
Xuanzhe Liu and
Jie M. Zhang and
Yang Liu",2025,CoRR,,abs/2503.02400,,,10.48550/ARXIV.2503.02400,https://doi.org/10.48550/arXiv.2503.02400,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-02400.bib,"Mon, 14 Apr 2025 01:00:00 +0200",,,,,,2503.024,arXiv,,"Large Language Models (LLMs) are increasingly integrated into software applications, with prompts serving as the primary 'programming' interface to guide their behavior. As a result, a new software paradigm, promptware, has emerged, using natural language prompts to interact with LLMs and enabling complex tasks without traditional coding. Unlike traditional software, which relies on formal programming languages and deterministic runtime environments, promptware is based on ambiguous, unstructured, and context-dependent natural language and operates on LLMs as runtime environments, which are probabilistic and non-deterministic. These fundamental differences introduce unique challenges in prompt development. In practice, prompt development is largely ad hoc and experimental, relying on a time-consuming trial-and-error process - a challenge we term the 'promptware crisis.' To address this, we propose promptware engineering, a new methodology that adapts established software engineering principles to the process of prompt development. Building on decades of success in traditional software engineering, we envision a systematic framework that includes prompt requirements engineering, design, implementation, testing, debugging, and evolution. Unlike traditional software engineering, our framework is specifically tailored to the unique characteristics of prompt development. This paper outlines a comprehensive roadmap for promptware engineering, identifying key research directions and offering actionable insights to advance LLM-based software development."
DBLP:journals/corr/abs-2503-02463,article,"It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate
Mutually via Selective Rationale Optimisation","Sohan Patnaik and
Milan Aggarwal and
Sumit Bhatia and
Balaji Krishnamurthy",2025,CoRR,,abs/2503.02463,,,10.48550/ARXIV.2503.02463,https://doi.org/10.48550/arXiv.2503.02463,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-02463.bib,"Fri, 11 Apr 2025 01:00:00 +0200",,,,,,2503.02463,arXiv,,"Very large language models (LLMs) such as GPT-4 have shown the ability to handle complex tasks by generating and self-refining step-by-step rationales. Smaller language models (SLMs), typically with<13B parameters, have been improved by using the data generated from very-large LMs through knowledge distillation. However, various practical constraints such as API costs, copyright, legal and ethical policies restrict using large (often opaque) models to train smaller models for commercial use. Limited success has been achieved at improving the ability of an SLM to explore the space of possible rationales and evaluate them by itself through self-deliberation. To address this, we propose COALITION, a trainable framework that facilitates interaction between two variants of the same SLM and trains them to generate and refine rationales optimized for the end-task. The variants exhibit different behaviors to produce a set of diverse candidate rationales during the generation and refinement steps. The model is then trained via Selective Rationale Optimization (SRO) to prefer generating rationale candidates that maximize the likelihood of producing the ground-truth answer. During inference, COALITION employs a controller to select the suitable variant for generating and refining the rationales. On five different datasets covering mathematical problems, commonsense reasoning, and natural language inference, COALITION outperforms several baselines by up to 5%. Our ablation studies reveal that cross-communication between the two variants performs better than using the single model to self-refine the rationales. We also demonstrate the applicability of COALITION for LMs of varying scales (4B to 14B parameters) and model families (Mistral, Llama, Qwen, Phi). We release the code for this work at https://github.com/Sohanpatnaik106/coalition."
DBLP:journals/corr/abs-2503-05012,article,"LLMs' Reshaping of People, Processes, Products, and Society in Software
Development: {A} Comprehensive Exploration with Early Adopters","Benyamin T. Tabarsi and
Heidi Reichert and
Ally Limke and
Sandeep Kuttal and
Tiffany Barnes",2025,CoRR,,abs/2503.05012,,,10.48550/ARXIV.2503.05012,https://doi.org/10.48550/arXiv.2503.05012,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-05012.bib,"Thu, 01 May 2025 01:00:00 +0200",,,,,,2503.05012,arXiv,,"Large language models (LLMs) like OpenAI ChatGPT, Google Gemini, and GitHub Copilot are rapidly gaining traction in the software industry, but their full impact on software engineering remains insufficiently explored. Despite their growing adoption, there is a notable lack of formal, qualitative assessments of how LLMs are applied in real-world software development contexts. To fill this gap, we conducted semi-structured interviews with sixteen early-adopter professional developers to explore their use of LLMs throughout various stages of the software development life cycle. Our investigation examines four dimensions: people - how LLMs affect individual developers and teams; process - how LLMs alter software engineering workflows; product - LLM impact on software quality and innovation; and society - the broader socioeconomic and ethical implications of LLM adoption. Thematic analysis of our data reveals that while LLMs have not fundamentally revolutionized the development process, they have substantially enhanced routine coding tasks, including code generation, refactoring, and debugging. Developers reported the most effective outcomes when providing LLMs with clear, well-defined problem statements, indicating that LLMs excel with decomposed problems and specific requirements. Furthermore, these early-adopters identified that LLMs offer significant value for personal and professional development, aiding in learning new languages and concepts. Early-adopters, highly skilled in software engineering and how LLMs work, identified early and persisting challenges for software engineering, such as inaccuracies in generated content and the need for careful manual review before integrating LLM outputs into production environments. Our study provides a nuanced understanding of how LLMs are shaping the landscape of software development, with their benefits, limitations, and ongoing implications."
DBLP:journals/corr/abs-2503-05507,article,Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?,"Qingyuan Liang and
Zhao Zhang and
Zeyu Sun and
Zheng Lin and
Qi Luo and
Yueyi Xiao and
Yizhou Chen and
Yuqun Zhang and
Haotian Zhang and
Lu Zhang and
Bin Chen and
Yingfei Xiong",2025,CoRR,,abs/2503.05507,,,10.48550/ARXIV.2503.05507,https://doi.org/10.48550/arXiv.2503.05507,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-05507.bib,"Mon, 14 Apr 2025 01:00:00 +0200",,,,,,2503.05507,arXiv,,"Grammar serves as a cornerstone in programming languages and software engineering, providing frameworks to define the syntactic space and program structure. Existing research demonstrates the effectiveness of grammar-based code representations in small-scale models, showing their ability to reduce syntax errors and enhance performance. However, as language models scale to the billion level or beyond, syntax-level errors become rare, making it unclear whether grammar information still provides performance benefits. To explore this, we develop a series of billion-scale GrammarCoder models, incorporating grammar rules in the code generation process. Experiments on HumanEval (+) and MBPP (+) demonstrate a notable improvement in code generation accuracy. Further analysis shows that grammar-based representations enhance LLMs' ability to discern subtle code differences, reducing semantic errors caused by minor variations. These findings suggest that grammar-based code representations remain valuable even in billion-scale models, not only by maintaining syntax correctness but also by improving semantic differentiation."
DBLP:journals/corr/abs-2503-05856,article,"This Is Your Doge, If It Please You: Exploring Deception and Robustness
in Mixture of LLMs","Lorenz Wolf and
Sangwoong Yoon and
Ilija Bogunovic",2025,CoRR,,abs/2503.05856,,,10.48550/ARXIV.2503.05856,https://doi.org/10.48550/arXiv.2503.05856,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-05856.bib,"Fri, 11 Apr 2025 01:00:00 +0200",,,,,,2503.05856,arXiv,,"Mixture of large language model (LLMs) Agents (MoA) architectures achieve state-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by leveraging the collaboration of multiple LLMs at inference time. Despite these successes, an evaluation of the safety and reliability of MoA is missing. We present the first comprehensive study of MoA's robustness against deceptive LLM agents that deliberately provide misleading responses. We examine factors like the propagation of deceptive information, model size, and information availability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the popular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of 49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate that introducing only a $\textit{single}$ carefully-instructed deceptive agent into the MoA can reduce performance to 37.9%, effectively nullifying all MoA gains. On QuALITY, a multiple-choice comprehension task, the impact is also severe, with accuracy plummeting by a staggering 48.5%. Inspired in part by the historical Doge of Venice voting process, designed to minimize influence and deception, we propose a range of unsupervised defense mechanisms that recover most of the lost performance."
DBLP:journals/corr/abs-2503-06781,article,"Dr Genre: Reinforcement Learning from Decoupled {LLM} Feedback for
Generic Text Rewriting","Yufei Li and
John Nham and
Ganesh Jawahar and
Lei Shu and
David Uthus and
Yun{-}Hsuan Sung and
Chengrun Yang and
Itai Rolnick and
Yi Qiao and
Cong Liu",2025,CoRR,,abs/2503.06781,,,10.48550/ARXIV.2503.06781,https://doi.org/10.48550/arXiv.2503.06781,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-06781.bib,"Wed, 25 Jun 2025 01:00:00 +0200",,,,,,2503.06781,arXiv,,"Generic text rewriting is a prevalent large language model (LLM) application that covers diverse real-world tasks, such as style transfer, fact correction, and email editing. These tasks vary in rewriting objectives (e.g., factual consistency vs. semantic preservation), making it challenging to develop a unified model that excels across all dimensions. Existing methods often specialize in either a single task or a specific objective, limiting their generalizability. In this work, we introduce a generic model proficient in factuality, stylistic, and conversational rewriting tasks. To simulate real-world user rewrite requests, we construct a conversational rewrite dataset, ChatRewrite, that presents ``natural''-sounding instructions, from raw emails using LLMs. Combined with other popular rewrite datasets, including LongFact for the factuality rewrite task and RewriteLM for the stylistic rewrite task, this forms a broad benchmark for training and evaluating generic rewrite models. To align with task-specific objectives, we propose Dr Genre, a Decoupled-reward learning framework for Generic rewriting, that utilizes objective-oriented reward models with a task-specific weighting. Evaluation shows that \approach delivers higher-quality rewrites across all targeted tasks, improving objectives including instruction following (agreement), internal consistency (coherence), and minimal unnecessary edits (conciseness)."
DBLP:journals/corr/abs-2503-07556,article,"Junior Software Developers' Perspectives on Adopting LLMs for Software
Engineering: a Systematic Literature Review","Samuel Ferino and
Rashina Hoda and
John C. Grundy and
Christoph Treude",2025,CoRR,,abs/2503.07556,,,10.48550/ARXIV.2503.07556,https://doi.org/10.48550/arXiv.2503.07556,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-07556.bib,"Sun, 13 Apr 2025 01:00:00 +0200",,,,,,2503.07556,arXiv,,
DBLP:journals/corr/abs-2503-07967,article,"Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex
Software Maintenance","Xin Peng and
Chong Wang and
Mingwei Liu and
Yiling Lou and
Yijian Wu",2025,CoRR,,abs/2503.07967,,,10.48550/ARXIV.2503.07967,https://doi.org/10.48550/arXiv.2503.07967,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-07967.bib,"Sun, 13 Apr 2025 01:00:00 +0200",,,,,,2503.07967,arXiv,,"While large language models (LLMs) have demonstrated promise in software engineering tasks like code completion and generation, their support for the maintenance of complex software systems remains limited. These models often struggle with understanding the tacit knowledge embedded in systems, such as responsibility allocation and collaboration across different modules. To address this gap, we introduce the concept and framework of \textbf{Code Digital Twin}, a conceptual representation of tacit knowledge that captures the concepts, functionalities, and design rationales behind code elements, co-evolving with the software. A code digital twin is constructed using a methodology that combines knowledge extraction from both structured and unstructured sources--such as source code, documentation, and change histories--leveraging LLMs, static analysis tools, and human expertise. This framework can empower LLMs for software maintenance tasks such as issue localization and repository-level code generation by providing tacit knowledge as contexts. Based on the proposed methodology, we explore the key challenges and opportunities involved in the continuous construction and refinement of code digital twin."
DBLP:journals/corr/abs-2503-09211,article,Why LLMs Cannot Think and How to Fix It,"Marius Jahrens and
Thomas Martinetz",2025,CoRR,,abs/2503.09211,,,10.48550/ARXIV.2503.09211,https://doi.org/10.48550/arXiv.2503.09211,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-09211.bib,"Sun, 13 Apr 2025 01:00:00 +0200",,,,,,2503.09211,arXiv,,"This paper elucidates that current state-of-the-art Large Language Models (LLMs) are fundamentally incapable of making decisions or developing""thoughts""within the feature space due to their architectural constraints. We establish a definition of""thought""that encompasses traditional understandings of that term and adapt it for application to LLMs. We demonstrate that the architectural design and language modeling training methodology of contemporary LLMs inherently preclude them from engaging in genuine thought processes. Our primary focus is on this theoretical realization rather than practical insights derived from experimental data. Finally, we propose solutions to enable thought processes within the feature space and discuss the broader implications of these architectural modifications."
DBLP:journals/corr/abs-2503-11018,article,"An LLM's Attempts to Adapt to Diverse Software Engineers' Problem-Solving
Styles: More Inclusive {\&} Equitable?","Andrew Anderson and
David Piorkowski and
Margaret Burnett and
Justin D. Weisz",2025,CoRR,,abs/2503.11018,,,10.48550/ARXIV.2503.11018,https://doi.org/10.48550/arXiv.2503.11018,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-11018.bib,"Sun, 13 Apr 2025 01:00:00 +0200",,,,,,2503.11018,arXiv,,"Software engineers use code-fluent large language models (LLMs) to help explain unfamiliar code, yet LLM explanations are not adapted to engineers' diverse problem-solving needs. We prompted an LLM to adapt to five problem-solving style types from an inclusive design method, the Gender Inclusiveness Magnifier (GenderMag). We ran a user study with software engineers to examine the impact of explanation adaptations on software engineers' perceptions, both for explanations which matched and mismatched engineers' problem-solving styles. We found that explanations were more frequently beneficial when they matched problem-solving style, but not every matching adaptation was equally beneficial; in some instances, diverse engineers found as much (or more) benefit from mismatched adaptations. Through an equity and inclusivity lens, our work highlights the benefits of having an LLM adapt its explanations to match engineers' diverse problem-solving style values, the potential harms when matched adaptations were not perceived well by engineers, and a comparison of how matching and mismatching LLM adaptations impacted diverse engineers."
DBLP:journals/corr/abs-2503-11951,article,"SagaLLM: Context Management, Validation, and Transaction Guarantees
for Multi-Agent {LLM} Planning","Edward Y. Chang and
Longling Geng",2025,CoRR,,abs/2503.11951,,,10.48550/ARXIV.2503.11951,https://doi.org/10.48550/arXiv.2503.11951,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-11951.bib,"Sun, 13 Apr 2025 01:00:00 +0200",,,,,,2503.11951,arXiv,,"This paper introduces SagaLLM, a structured multi-agent architecture designed to address four foundational limitations of current LLM-based planning systems: unreliable self-validation, context loss, lack of transactional safeguards, and insufficient inter-agent coordination. While recent frameworks leverage LLMs for task decomposition and multi-agent communication, they often fail to ensure consistency, rollback, or constraint satisfaction across distributed workflows. SagaLLM bridges this gap by integrating the Saga transactional pattern with persistent memory, automated compensation, and independent validation agents. It leverages LLMs' generative reasoning to automate key tasks traditionally requiring hand-coded coordination logic, including state tracking, dependency analysis, log schema generation, and recovery orchestration. Although SagaLLM relaxes strict ACID guarantees, it ensures workflow-wide consistency and recovery through modular checkpointing and compensable execution. Empirical evaluations across planning domains demonstrate that standalone LLMs frequently violate interdependent constraints or fail to recover from disruptions. In contrast, SagaLLM achieves significant improvements in consistency, validation accuracy, and adaptive coordination under uncertainty‚Äîestablishing a robust foundation for real-world, scalable LLM-based multi-agent systems."
DBLP:journals/corr/abs-2503-12043,article,"An LLM-Integrated Framework for Completion, Management, and Tracing
of {STPA}","Ali Raeisdanaei and
Juho Kim and
Michael Liao and
Sparsh Kochhar",2025,CoRR,,abs/2503.12043,,,10.48550/ARXIV.2503.12043,https://doi.org/10.48550/arXiv.2503.12043,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-12043.bib,"Sun, 13 Apr 2025 01:00:00 +0200",,,,,,2503.12043,arXiv,,"In many safety-critical engineering domains, hazard analysis techniques are an essential part of requirement elicitation. Of the methods proposed for this task, STPA (System-Theoretic Process Analysis) represents a relatively recent development in the field. The completion, management, and traceability of this hazard analysis technique present a time-consuming challenge to the requirements and safety engineers involved. In this paper, we introduce a free, open-source software framework to build STPA models with several automated workflows powered by large language models (LLMs). In past works, LLMs have been successfully integrated into a myriad of workflows across various fields. Here, we demonstrate that LLMs can be used to complete tasks associated with STPA with a high degree of accuracy, saving the time and effort of the human engineers involved. We experimentally validate our method on real-world STPA models built by requirement engineers and researchers. The source code of our software framework is available at the following link: https://github.com/blueskysolarracing/stpa."
DBLP:journals/corr/abs-2503-13793,article,"Mapping the Trust Terrain: LLMs in Software Engineering - Insights
and Perspectives","Dipin Khati and
Yijin Liu and
David N. Palacio and
Yixuan Zhang and
Denys Poshyvanyk",2025,CoRR,,abs/2503.13793,,,10.48550/ARXIV.2503.13793,https://doi.org/10.48550/arXiv.2503.13793,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-13793.bib,"Tue, 15 Apr 2025 01:00:00 +0200",,,,,,2503.13793,arXiv,,"
 The application of Large Language Models (LLMs) in Software Engineering (SE) continues to grow rapidly across both industry and academia. As these models become integral to critical SE processes, ensuring their reliability and trustworthiness becomes essential. Achieving this requires a balanced approach to trust: excessive trust can introduce security vulnerabilities, while insufficient trust may hinder innovation. However, the conceptual landscape of trust in LLMs for SE(LLM4SE) remains unclear. Key concepts such as trust, distrust, and trustworthiness lack precise definitions, factors that shape trust formation remain underexplored, and metrics for trust in LLMs remain undeveloped. To clarify the current research landscape and identify future directions, we conducted a comprehensive review of
 
 \(88\)
 
 articles: a systematic review of
 
 \(18\)
 
 studies on LLMs in SE, supplemented by an analysis of
 
 \(70\)
 
 articles from the broader trust literature. Furthermore, we surveyed
 
 \(25\)
 
 domain experts to gather practitioners‚Äô perspectives on trust and identify gaps between their experiences and the existing literature. Our findings provide a structured overview of trust-related concepts in LLM4SE, outlining key areas for future research. This study contributes to building more trustworthy LLM-assisted software engineering processes, ultimately supporting safer and more effective adoption of LLMs in SE.
"
DBLP:journals/corr/abs-2503-15231,article,"When LLMs Meet {API} Documentation: Can Retrieval Augmentation Aid
Code Generation Just as It Helps Developers?","Jingyi Chen and
Songqiang Chen and
Jialun Cao and
Jiasi Shen and
Shing{-}Chi Cheung",2025,CoRR,,abs/2503.15231,,,10.48550/ARXIV.2503.15231,https://doi.org/10.48550/arXiv.2503.15231,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-15231.bib,"Wed, 30 Apr 2025 01:00:00 +0200",,,,,,2503.15231,arXiv,,"Retrieval-augmented generation (RAG) has increasingly shown its power in extending large language models' (LLMs') capability beyond their pre-trained knowledge. Existing works have shown that RAG can help with software development tasks such as code generation, code update, and test generation. Yet, the effectiveness of adapting LLMs to fast-evolving or less common API libraries using RAG remains unknown. To bridge this gap, we take an initial step to study this unexplored yet practical setting - when developers code with a less common library, they often refer to its API documentation; likewise, when LLMs are allowed to look up API documentation via RAG, to what extent can LLMs be advanced? To mimic such a setting, we select four less common open-source Python libraries with a total of 1017 eligible APIs. We study the factors that affect the effectiveness of using the documentation of less common API libraries as additional knowledge for retrieval and generation. Our intensive study yields interesting findings: (1) RAG helps improve LLMs' performance by 83%-220%. (2) Example code contributes the most to advance LLMs, instead of the descriptive texts and parameter lists in the API documentation. (3) LLMs could sometimes tolerate mild noises (typos in description or incorrect parameters) by referencing their pre-trained knowledge or document context. Finally, we suggest that developers pay more attention to the quality and diversity of the code examples in the API documentation. The study sheds light on future low-code software development workflows."
DBLP:journals/corr/abs-2503-16458,article,Users Favor LLM-Generated Content - Until They Know It's {AI},"Petr Parshakov and
Iuliia Naidenova and
Sofiia Paklina and
Nikita Matkin and
Cornel Nesseler",2025,CoRR,,abs/2503.16458,,,10.48550/ARXIV.2503.16458,https://doi.org/10.48550/arXiv.2503.16458,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-16458.bib,"Mon, 14 Apr 2025 01:00:00 +0200",,,,,,2503.16458,arXiv,,"In this paper, we investigate how individuals evaluate human and large langue models generated responses to popular questions when the source of the content is either concealed or disclosed. Through a controlled field experiment, participants were presented with a set of questions, each accompanied by a response generated by either a human or an AI. In a randomized design, half of the participants were informed of the response's origin while the other half remained unaware. Our findings indicate that, overall, participants tend to prefer AI-generated responses. However, when the AI origin is revealed, this preference diminishes significantly, suggesting that evaluative judgments are influenced by the disclosure of the response's provenance rather than solely by its quality. These results underscore a bias against AI-generated content, highlighting the societal challenge of improving the perception of AI work in contexts where quality assessments should be paramount."
DBLP:journals/corr/abs-2503-17741,article,"RustMap: Towards Project-Scale C-to-Rust Migration via Program Analysis
and {LLM}","Xuemeng Cai and
Jiakun Liu and
Xiping Huang and
Yijun Yu and
Haitao Wu and
Chunmiao Li and
Bo Wang and
Imam Nur Bani Yusuf and
Lingxiao Jiang",2025,CoRR,,abs/2503.17741,,,10.48550/ARXIV.2503.17741,https://doi.org/10.48550/arXiv.2503.17741,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-17741.bib,"Fri, 04 Jul 2025 01:00:00 +0200",,,,,,2503.17741,arXiv,,"Migrating existing C programs into Rust is increasingly desired, as Rust offers superior memory safety while maintaining C's high performance. However, vastly different features between C and Rust--e.g., distinct definitions and usages of pointers and references--pose significant challenges beyond mere syntactic translation. Existing automated translation tools, such as C2Rust, may rely too much on syntactic, template-based translation and generate unsafe Rust code that is hard for human developers to read, maintain, or even compile. More semantic-aware translation that produces safer, idiomatic, and runnable Rust code is much needed. This paper introduces a novel dependency-guided and large language model (LLM)-based C-to-Rust translation approach, RustMap, based on three key ideas: (1) Utilize LLM capabilities to produce idiomatic Rust code from given small pieces of C code, (2) Mitigate LLM limitations in handling large codebases by breaking project-scale C programs into smaller units for translation according to their usage dependencies and composing them into a runnable Rust program, and (3) Enhance the correctness of the translated Rust program by using test cases to check input/output equivalence, isolate faulty code when execution states deviate, and iteratively refine the translation using feedback from compilation and test errors. We empirically evaluate RustMap on 126 real-world programs, including 125 from Rosetta Code and a 7000+ line bzip2 implementation using GPT-4o as the LLM. RustMap shows promising results, guiding GPT-4o to produce idiomatic, readable, and functional Rust code with significantly less unsafe code than other tools, and revealing non-trivial translation patterns reusable for future research."
DBLP:journals/corr/abs-2503-18292,article,Jenga: Effective Memory Management for Serving {LLM} with Heterogeneity,"Chen Zhang and
Kuntai Du and
Shu Liu and
Woosuk Kwon and
Xiangxi Mo and
Yufeng Wang and
Xiaoxuan Liu and
Kaichao You and
Zhuohan Li and
Mingsheng Long and
Jidong Zhai and
Joseph Gonzalez and
Ion Stoica",2025,CoRR,,abs/2503.18292,,,10.48550/ARXIV.2503.18292,https://doi.org/10.48550/arXiv.2503.18292,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-18292.bib,"Mon, 21 Apr 2025 01:00:00 +0200",,,,,,2503.18292,arXiv,,"Large language models (LLMs) are widely used but expensive to run, especially as inference workloads grow. To lower costs, maximizing the request batch size by managing GPU memory efficiently is crucial. While PagedAttention has recently been proposed to improve the efficiency of memory management, we find that the growing heterogeneity in the embeddings dimensions, attention, and access patterns of modern LLM architectures introduces new challenges for memory allocation. In this paper, we present Jenga, a novel memory allocation framework for heterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1) minimizing memory fragmentation when managing embeddings of different sizes, and (2) enabling flexible caching and eviction policies tailored to the specific token-dependency patterns of various layers. Jenga employs a two-level memory allocator, leveraging the least common multiple (LCM) of embedding sizes to optimize memory usage and providing APIs to express layer-specific caching logic to enhance memory reuse. We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and evaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations show that Jenga improves GPU memory utilization by up to 79.6%, and increases serving throughput by up to 4.92x (1.80x on average)."
DBLP:journals/corr/abs-2503-19693,article,"AdaptiVocab: Enhancing {LLM} Efficiency in Focused Domains through
Lightweight Vocabulary Adaptation","Itay Nakash and
Nitay Calderon and
Eyal Ben{-}David and
Elad Hoffer and
Roi Reichart",2025,CoRR,,abs/2503.19693,,,10.48550/ARXIV.2503.19693,https://doi.org/10.48550/arXiv.2503.19693,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-19693.bib,"Sat, 19 Apr 2025 01:00:00 +0200",,,,,,2503.19693,arXiv,,"Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance"
DBLP:journals/corr/abs-2503-21735,article,"GateLens: {A} Reasoning-Enhanced {LLM} Agent for Automotive Software
Release Analytics","Arsham Gholamzadeh Khoee and
Shuai Wang and
Yinan Yu and
Robert Feldt and
Dhasarathy Parthasarathy",2025,CoRR,,abs/2503.21735,,,10.48550/ARXIV.2503.21735,https://doi.org/10.48550/arXiv.2503.21735,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-21735.bib,"Sat, 19 Apr 2025 01:00:00 +0200",,,,,,2503.21735,arXiv,,"Ensuring reliable software release decisions is critical in safety-critical domains such as automotive manufacturing. Release validation relies on large tabular datasets, yet manual analysis is slow, costly, and error-prone. While Large Language Models (LLMs) offer promising automation potential, they face challenges in analytical reasoning, structured data handling, and ambiguity resolution. This paper introduces GateLens, an LLM-based system for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and generates optimized Python code. Unlike traditional multi-agent or planning-based systems that can be slow, opaque, and costly to maintain, GateLens emphasizes speed, transparency, and reliability. Experimental results show that GateLens outperforms the existing Chain-of-Thought (CoT) + Self-Consistency (SC) based system on real-world datasets, particularly in handling complex and ambiguous queries. Ablation studies confirm the essential role of the RA layer. Industrial deployment shows over 80% reduction in analysis time while maintaining high accuracy across test result interpretation, impact assessment, and release candidate evaluation. GateLens operates effectively in zero-shot settings without requiring few-shot examples or agent orchestration. This work advances deployable LLM system design by identifying key architectural features-intermediate formal representations, execution efficiency, and low configuration overhead-crucial for safety-critical industrial applications."
DBLP:journals/corr/abs-2503-22238,article,"Integrating LLMs in Software Engineering Education: Motivators, Demotivators,
and a Roadmap Towards a Framework for Finnish Higher Education Institutes","Maryam Khan and
Muhammad Azeem Akbar and
Jussi Kasurinen",2025,CoRR,,abs/2503.22238,,,10.48550/ARXIV.2503.22238,https://doi.org/10.48550/arXiv.2503.22238,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-22238.bib,"Tue, 05 Aug 2025 01:00:00 +0200",,,,,,2503.22238,arXiv,,"The increasing adoption of Large Language Models (LLMs) in software engineering education presents both opportunities and challenges. While LLMs offer benefits such as enhanced learning experiences, automated assessments, and personalized tutoring, their integration also raises concerns about academic integrity, student over-reliance, and ethical considerations. In this study, we conducted a preliminary literature review to identify motivators and demotivators for using LLMs in software engineering education. We applied a thematic mapping process to categorize and structure these factors (motivators and demotivators), offering a comprehensive view of their impact. In total, we identified 25 motivators and 30 demotivators, which are further organized into four high-level themes. This mapping provides a structured framework for understanding the factors that influence the integration of LLMs in software engineering education, both positively and negatively. As part of a larger research project, this study serves as a feasibility assessment, laying the groundwork for future systematic literature review and empirical studies. Ultimately, this project aims to develop a framework to assist Finnish higher education institutions in effectively integrating LLMs into software engineering education while addressing potential risks and challenges."
DBLP:journals/corr/abs-2503-22424,article,"CoSIL: Software Issue Localization via LLM-Driven Code Repository
Graph Searching","Zhonghao Jiang and
Xiaoxue Ren and
Meng Yan and
Wei Jiang and
Yong Li and
Zhongxin Liu",2025,CoRR,,abs/2503.22424,,,10.48550/ARXIV.2503.22424,https://doi.org/10.48550/arXiv.2503.22424,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-22424.bib,"Tue, 22 Apr 2025 01:00:00 +0200",,,,,,2503.22424,arXiv,,
DBLP:journals/corr/abs-2503-23514,article,"If an {LLM} Were a Character, Would It Know Its Own Story? Evaluating
Lifelong Learning in LLMs","Siqi Fan and
Xiusheng Huang and
Yiqun Yao and
Xuezhi Fang and
Kang Liu and
Peng Han and
Shuo Shang and
Aixin Sun and
Yequan Wang",2025,CoRR,,abs/2503.23514,,,10.48550/ARXIV.2503.23514,https://doi.org/10.48550/arXiv.2503.23514,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-23514.bib,"Mon, 21 Apr 2025 01:00:00 +0200",,,,,,2503.23514,arXiv,,"Large language models (LLMs) can carry out human-like dialogue, but unlike humans, they are stateless due to the superposition property. However, during multi-turn, multi-agent interactions, LLMs begin to exhibit consistent, character-like behaviors, hinting at a form of emergent lifelong learning. Despite this, existing benchmarks often fail to capture these dynamics, primarily focusing on static, open-ended evaluations. To address this gap, we introduce LIFESTATE-BENCH, a benchmark designed to assess lifelong learning in LLMs. It features two episodic datasets: Hamlet and a synthetic script collection, rich in narrative structure and character interactions. Our fact checking evaluation probes models' self-awareness, episodic memory retrieval, and relationship tracking, across both parametric and non-parametric approaches. Experiments on models like Llama3.1-8B, GPT-4-turbo, and DeepSeek R1, we demonstrate that nonparametric methods significantly outperform parametric ones in managing stateful learning. However, all models exhibit challenges with catastrophic forgetting as interactions extend, highlighting the need for further advancements in lifelong learning."
DBLP:journals/corr/abs-2503-24157,article,"{LLM4FS:} Leveraging Large Language Models for Feature Selection and
How to Improve It","Jianhao Li and
Xianchao Xiu",2025,CoRR,,abs/2503.24157,,,10.48550/ARXIV.2503.24157,https://doi.org/10.48550/arXiv.2503.24157,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2503-24157.bib,"Mon, 21 Apr 2025 01:00:00 +0200",,,,,,2503.24157,arXiv,,"Recent advances in large language models (LLMs) have provided new opportunities for decision-making, particularly in the task of automated feature selection. In this paper, we first comprehensively evaluate LLM-based feature selection methods, covering the state-of-the-art DeepSeek-R1, GPT-o3-mini, and GPT-4.5. Then, we propose a new hybrid strategy called LLM4FS that integrates LLMs with traditional data-driven methods. Specifically, input data samples into LLMs, and directly call traditional data-driven techniques such as random forest and forward sequential selection. Notably, our analysis reveals that the hybrid strategy leverages the contextual understanding of LLMs and the high statistical reliability of traditional data-driven methods to achieve excellent feature selection performance, even surpassing LLMs and traditional data-driven methods. Finally, we point out the limitations of its application in decision-making. Our code is available at https://github.com/xianchaoxiu/LLM4FS."
DBLP:journals/corr/abs-2504-01145,article,"MaLAware: Automating the Comprehension of Malicious Software Behaviours
using Large Language Models (LLMs)","Bikash Saha and
Nanda Rani and
Sandeep Kumar Shukla",2025,CoRR,,abs/2504.01145,,,10.48550/ARXIV.2504.01145,https://doi.org/10.48550/arXiv.2504.01145,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-01145.bib,"Sat, 31 May 2025 01:00:00 +0200",,,,,,2504.01145,arXiv,,
DBLP:journals/corr/abs-2504-01637,article,LLM-mediated Dynamic Plan Generation with a Multi-Agent Approach,"Reo Abe and
Akifumi Ito and
Kanata Takayasu and
Satoshi Kurihara",2025,CoRR,,abs/2504.01637,,,10.48550/ARXIV.2504.01637,https://doi.org/10.48550/arXiv.2504.01637,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-01637.bib,"Sun, 18 May 2025 01:00:00 +0200",,,,,,2504.01637,arXiv,,"Planning methods with high adaptability to dynamic environments are crucial for the development of autonomous and versatile robots. We propose a method for leveraging a large language model (GPT-4o) to automatically generate networks capable of adapting to dynamic environments. The proposed method collects environmental""status,""representing conditions and goals, and uses them to generate agents. These agents are interconnected on the basis of specific conditions, resulting in networks that combine flexibility and generality. We conducted evaluation experiments to compare the networks automatically generated with the proposed method with manually constructed ones, confirming the comprehensiveness of the proposed method's networks and their higher generality. This research marks a significant advancement toward the development of versatile planning methods applicable to robotics, autonomous vehicles, smart systems, and other complex environments."
DBLP:journals/corr/abs-2504-02141,article,"On Simulation-Guided LLM-based Code Generation for Safe Autonomous
Driving Software","Ali Nouri and
Johan Andersson and
Kailash De Jesus Hornig and
Zhennan Fei and
Emil Knabe and
H{\aa}kan Sivencrona and
Beatriz Cabrero Daniel and
Christian Berger",2025,CoRR,,abs/2504.02141,,,10.48550/ARXIV.2504.02141,https://doi.org/10.48550/arXiv.2504.02141,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-02141.bib,"Sun, 18 May 2025 01:00:00 +0200",,,,,,2504.02141,arXiv,,"Automated Driving System (ADS) is a safety-critical software system responsible for the interpretation of the vehicle's environment and making decisions accordingly. The unbounded complexity of the driving context, including unforeseeable events, necessitate continuous improvement, often achieved through iterative DevOps processes. However, DevOps processes are themselves complex, making these improvements both time- and resource-intensive. Automation in code generation for ADS using Large Language Models (LLM) is one potential approach to address this challenge. Nevertheless, the development of ADS requires rigorous processes to verify, validate, assess, and qualify the code before it can be deployed in the vehicle and used. In this study, we developed and evaluated a prototype for automatic code generation and assessment using a designed pipeline of a LLM-based agent, simulation model, and rule-based feedback generator in an industrial setup. The LLM-generated code is evaluated automatically in a simulation model against multiple critical traffic scenarios, and an assessment report is provided as feedback to the LLM for modification or bug fixing. We report about the experimental results of the prototype employing Codellama:34b, DeepSeek (r1:32b and Coder:33b), CodeGemma:7b, Mistral:7b, and GPT4 for Adaptive Cruise Control (ACC) and Unsupervised Collision Avoidance by Evasive Manoeuvre (CAEM). We finally assessed the tool with 11 experts at two Original Equipment Manufacturers (OEMs) by conducting an interview study."
DBLP:journals/corr/abs-2504-02553,article,"Exploring Individual Factors in the Adoption of LLMs for Specific
Software Engineering Tasks","Stefano Lambiase and
Gemma Catolino and
Fabio Palomba and
Filomena Ferrucci and
Daniel Russo",2025,CoRR,,abs/2504.02553,,,10.48550/ARXIV.2504.02553,https://doi.org/10.48550/arXiv.2504.02553,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-02553.bib,"Mon, 19 May 2025 01:00:00 +0200",,,,,,2504.02553,arXiv,,"The advent of Large Language Models (LLMs) is transforming software development, significantly enhancing software engineering processes. Research has explored their role within development teams, focusing on specific tasks such as artifact generation, decision-making support, and information retrieval. Despite the growing body of work on LLMs in software engineering, most studies have centered on broad adoption trends, neglecting the nuanced relationship between individual cognitive and behavioral factors and their impact on task-specific adoption. While factors such as perceived effort and performance expectancy have been explored at a general level, their influence on distinct software engineering tasks remains underexamined. This gap hinders the development of tailored LLM-based systems (e.g., Generative AI Agents) that align with engineers' specific needs and limits the ability of team leaders to devise effective strategies for fostering LLM adoption in targeted workflows. This study bridges this gap by surveying N=188 software engineers to test the relationship between individual attributes related to technology adoption and LLM adoption across five key tasks, using structural equation modeling (SEM). The Unified Theory of Acceptance and Use of Technology (UTAUT2) was applied to characterize individual adoption behaviors. The findings reveal that task-specific adoption is influenced by distinct factors, some of which negatively impact adoption when considered in isolation, underscoring the complexity of LLM integration in software engineering. To support effective adoption, this article provides actionable recommendations, such as seamlessly integrating LLMs into existing development environments and encouraging peer-driven knowledge sharing to enhance information retrieval."
DBLP:journals/corr/abs-2504-04292,article,"Cross-Asset Risk Management: Integrating LLMs for Real-Time Monitoring
of Equity, Fixed Income, and Currency Markets","Jie Yang and
Yiqiu Tang and
Yongjie Li and
Lihua Zhang and
Haoran Zhang",2025,CoRR,,abs/2504.04292,,,10.48550/ARXIV.2504.04292,https://doi.org/10.48550/arXiv.2504.04292,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-04292.bib,"Mon, 19 May 2025 01:00:00 +0200",,,,,,2504.04292,arXiv,,"Large language models (LLMs) have emerged as powerful tools in the field of finance, particularly for risk management across different asset classes. In this work, we introduce a Cross-Asset Risk Management framework that utilizes LLMs to facilitate real-time monitoring of equity, fixed income, and currency markets. This innovative approach enables dynamic risk assessment by aggregating diverse data sources, ultimately enhancing decision-making processes. Our model effectively synthesizes and analyzes market signals to identify potential risks and opportunities while providing a holistic view of asset classes. By employing advanced analytics, we leverage LLMs to interpret financial texts, news articles, and market reports, ensuring that risks are contextualized within broader market narratives. Extensive backtesting and real-time simulations validate the framework, showing increased accuracy in predicting market shifts compared to conventional methods. The focus on real-time data integration enhances responsiveness, allowing financial institutions to manage risks adeptly under varying market conditions and promoting financial stability through the advanced application of LLMs in risk analysis."
DBLP:journals/corr/abs-2504-06143,article,"{ARLO:} {A} Tailorable Approach for Transforming Natural Language
Software Requirements into Architecture using LLMs",Tooraj Helmi,2025,CoRR,,abs/2504.06143,,,10.48550/ARXIV.2504.06143,https://doi.org/10.48550/arXiv.2504.06143,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-06143.bib,"Wed, 21 May 2025 01:00:00 +0200",,,,,,2504.06143,arXiv,,"Software requirements expressed in natural language (NL) frequently suffer from verbosity, ambiguity, and inconsistency. This creates a range of challenges, including selecting an appropriate architecture for a system and assessing different architectural alternatives. Relying on human expertise to accomplish the task of mapping NL requirements to architecture is time-consuming and error-prone. This paper proposes ARLO, an approach that automates this task by leveraging (1) a set of NL requirements for a system, (2) an existing standard that specifies architecturally relevant software quality attributes, and (3) a readily available Large Language Model (LLM). Specifically, ARLO determines the subset of NL requirements for a given system that is architecturally relevant and maps that subset to a tailorable matrix of architectural choices. ARLO applies integer linear programming on the architectural-choice matrix to determine the optimal architecture for the current requirements. We demonstrate ARLO's efficacy using a set of real-world examples. We highlight ARLO's ability (1) to trace the selected architectural choices to the requirements and (2) to isolate NL requirements that exert a particular influence on a system's architecture. This allows the identification, comparative assessment, and exploration of alternative architectural choices based on the requirements and constraints expressed therein."
DBLP:journals/corr/abs-2504-06323,article,Mosaic: Composite Projection Pruning for Resource-efficient LLMs,"Bailey J. Eccles and
Leon Wong and
Blesson Varghese",2025,CoRR,,abs/2504.06323,,,10.48550/ARXIV.2504.06323,https://doi.org/10.48550/arXiv.2504.06323,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-06323.bib,"Wed, 21 May 2025 01:00:00 +0200",,,,,,2504.06323,arXiv,,
DBLP:journals/corr/abs-2504-06614,article,"AgentFM: Role-Aware Failure Management for Distributed Databases with
LLM-Driven Multi-Agents","Lingzhe Zhang and
Yunpeng Zhai and
Tong Jia and
Xiaosong Huang and
Chiming Duan and
Ying Li",2025,CoRR,,abs/2504.06614,,,10.48550/ARXIV.2504.06614,https://doi.org/10.48550/arXiv.2504.06614,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-06614.bib,"Wed, 21 May 2025 01:00:00 +0200",,,,,,2504.06614,arXiv,,
DBLP:journals/corr/abs-2504-07137,article,"Large Language Model {(LLM)} for Software Security: Code Analysis,
Malware Analysis, Reverse Engineering","Hamed Jelodar and
Samita Bai and
Parisa Hamedi and
Hesamodin Mohammadian and
Roozbeh Razavi{-}Far and
Ali A. Ghorbani",2025,CoRR,,abs/2504.07137,,,10.48550/ARXIV.2504.07137,https://doi.org/10.48550/arXiv.2504.07137,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-07137.bib,"Wed, 21 May 2025 01:00:00 +0200",,,,,,2504.07137,arXiv,,"Large Language Models (LLMs) have recently emerged as powerful tools in cybersecurity, offering advanced capabilities in malware detection, generation, and real-time monitoring. Numerous studies have explored their application in cybersecurity, demonstrating their effectiveness in identifying novel malware variants, analyzing malicious code structures, and enhancing automated threat analysis. Several transformer-based architectures and LLM-driven models have been proposed to improve malware analysis, leveraging semantic and structural insights to recognize malicious intent more accurately. This study presents a comprehensive review of LLM-based approaches in malware code analysis, summarizing recent advancements, trends, and methodologies. We examine notable scholarly works to map the research landscape, identify key challenges, and highlight emerging innovations in LLM-driven cybersecurity. Additionally, we emphasize the role of static analysis in malware detection, introduce notable datasets and specialized LLM models, and discuss essential datasets supporting automated malware research. This study serves as a valuable resource for researchers and cybersecurity professionals, offering insights into LLM-powered malware detection and defence strategies while outlining future directions for strengthening cybersecurity resilience."
DBLP:journals/corr/abs-2504-09378,article,"Can you map it to English? The Role of Cross-Lingual Alignment in
Multilingual Performance of LLMs","Kartik Ravisankar and
HyoJung Han and
Marine Carpuat",2025,CoRR,,abs/2504.09378,,,10.48550/ARXIV.2504.09378,https://doi.org/10.48550/arXiv.2504.09378,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-09378.bib,"Wed, 21 May 2025 01:00:00 +0200",,,,,,2504.09378,arXiv,,"Large language models (LLMs) pre-trained predominantly on English text exhibit surprising multilingual capabilities, yet the mechanisms driving cross-lingual generalization remain poorly understood. This work investigates how the alignment of representations for text written in different languages correlates with LLM performance on natural language understanding tasks and translation tasks, both at the language and the instance level. For this purpose, we introduce cross-lingual alignment metrics such as the Discriminative Alignment Index (DALI) to quantify the alignment at an instance level for discriminative tasks. Through experiments on three natural language understanding tasks (Belebele, XStoryCloze, XCOPA), and machine translation, we find that while cross-lingual alignment metrics strongly correlate with task accuracy at the language level, the sample-level alignment often fails to distinguish correct from incorrect predictions, exposing alignment as a necessary but insufficient condition for success."
DBLP:journals/corr/abs-2504-09522,article,How new data permeates {LLM} knowledge and how to dilute it,"Chen Sun and
Renat Aksitov and
Andrey Zhmoginov and
Nolan Andrew Miller and
Max Vladymyrov and
Ulrich Rueckert and
Been Kim and
Mark Sandler",2025,CoRR,,abs/2504.09522,,,10.48550/ARXIV.2504.09522,https://doi.org/10.48550/arXiv.2504.09522,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-09522.bib,"Wed, 21 May 2025 01:00:00 +0200",,,,,,2504.09522,arXiv,,"Large language models learn and continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit a""priming""effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts. To systematically study this phenomenon, we introduce""Outlandish,""a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM's existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before learning. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages. Finally, we develop two novel techniques to modulate how new knowledge affects existing model behavior: (1) a ``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update pruning method. These approaches reduce undesirable priming effects by 50-95\% while preserving the model's ability to learn new information. Our findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/"
DBLP:journals/corr/abs-2504-09855,article,PestMA: LLM-based Multi-Agent System for Informed Pest Management,"Hongrui Shi and
Shunbao Li and
Zhipeng Yuan and
Po Yang",2025,CoRR,,abs/2504.09855,,,10.48550/ARXIV.2504.09855,https://doi.org/10.48550/arXiv.2504.09855,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-09855.bib,"Wed, 21 May 2025 01:00:00 +0200",,,,,,2504.09855,arXiv,,"Effective pest management is complex due to the need for accurate, context-specific decisions. Recent advancements in large language models (LLMs) open new possibilities for addressing these challenges by providing sophisticated, adaptive knowledge acquisition and reasoning. However, existing LLM-based pest management approaches often rely on a single-agent paradigm, which can limit their capacity to incorporate diverse external information, engage in systematic validation, and address complex, threshold-driven decisions. To overcome these limitations, we introduce PestMA, an LLM-based multi-agent system (MAS) designed to generate reliable and evidence-based pest management advice. Building on an editorial paradigm, PestMA features three specialized agents, an Editor for synthesizing pest management recommendations, a Retriever for gathering relevant external data, and a Validator for ensuring correctness. Evaluations on real-world pest scenarios demonstrate that PestMA achieves an initial accuracy of 86.8% for pest management decisions, which increases to 92.6% after validation. These results underscore the value of collaborative agent-based workflows in refining and validating decisions, highlighting the potential of LLM-based multi-agent systems to automate and enhance pest management processes."
DBLP:journals/corr/abs-2504-10013,article,"Training LLMs on {HPC} Systems: Best Practices from the OpenGPT-X
Project","Carolin Penke and
Chelsea Maria John and
Jan Ebert and
Stefan Kesselheim and
Andreas Herten",2025,CoRR,,abs/2504.10013,,,10.48550/ARXIV.2504.10013,https://doi.org/10.48550/arXiv.2504.10013,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-10013.bib,"Wed, 21 May 2025 01:00:00 +0200",,,,,,2504.10013,arXiv,,"The training of large language models (LLMs) requires substantial computational resources, complex software stacks, and carefully designed workflows to achieve scalability and efficiency. This report presents best practices and insights gained from the OpenGPT-X project, a German initiative focused on developing open, multilingual LLMs optimized for European languages. We detail the use of high-performance computing (HPC) systems, primarily JUWELS Booster at JSC, for training Teuken-7B, a 7-billion-parameter transformer model. The report covers system architecture, training infrastructure, software choices, profiling and benchmarking tools, as well as engineering and operational challenges."
DBLP:journals/corr/abs-2504-10050,article,"Emotional Strain and Frustration in {LLM} Interactions in Software
Engineering","Cristina Martinez Montes and
Ranim Khojah",2025,CoRR,,abs/2504.10050,,,10.48550/ARXIV.2504.10050,https://doi.org/10.48550/arXiv.2504.10050,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-10050.bib,"Wed, 21 May 2025 01:00:00 +0200",,,,,,2504.1005,arXiv,,"Large Language Models (LLMs) are increasingly integrated into various daily tasks in Software Engineering such as coding and requirement elicitation. Despite their various capabilities and constant use, some interactions can lead to unexpected challenges (e.g. hallucinations or verbose answers) and, in turn, cause emotions that develop into frustration. Frustration can negatively impact engineers' productivity and well-being if they escalate into stress and burnout. In this paper, we assess the impact of LLM interactions on software engineers' emotional responses, specifically strains, and identify common causes of frustration when interacting with LLMs at work. Based on 62 survey responses from software engineers in industry and academia across various companies and universities, we found that a majority of our respondents experience frustrations or other related emotions regardless of the nature of their work. Additionally, our results showed that frustration mainly stemmed from issues with correctness and less critical issues such as adaptability to context or specific format. While such issues may not cause frustration in general, artefacts that do not follow certain preferences, standards, or best practices can make the output unusable without extensive modification, causing frustration over time. In addition to the frustration triggers, our study offers guidelines to improve the software engineers' experience, aiming to minimise long-term consequences on mental health."
DBLP:journals/corr/abs-2504-10504,article,"LayerFlow: Layer-wise Exploration of {LLM} Embeddings using Uncertainty-aware
Interlinked Projections","Rita Sevastjanova and
Robin Gerling and
Thilo Spinner and
Mennatallah El{-}Assady",2025,CoRR,,abs/2504.10504,,,10.48550/ARXIV.2504.10504,https://doi.org/10.48550/arXiv.2504.10504,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-10504.bib,"Wed, 21 May 2025 01:00:00 +0200",,,,,,2504.10504,arXiv,,"Large language models (LLMs) represent words through contextual word embeddings encoding different language properties like semantics and syntax. Understanding these properties is crucial, especially for researchers investigating language model capabilities, employing embeddings for tasks related to text similarity, or evaluating the reasons behind token importance as measured through attribution methods. Applications for embedding exploration frequently involve dimensionality reduction techniques, which reduce high‚Äêdimensional vectors to two dimensions used as coordinates in a scatterplot. This data transformation step introduces uncertainty that can be propagated to the visual representation and influence users' interpretation of the data. To communicate such uncertainties, we present LayerFlow ‚Äì a visual analytics workspace that displays embeddings in an interlinked projection design and communicates the transformation, representation, and interpretation uncertainty. In particular, to hint at potential data distortions and uncertainties, the workspace includes several visual components, such as convex hulls showing 2D and HD clusters, data point pairwise distances, cluster summaries, and projection quality metrics. We show the usability of the presented workspace through replication and expert case studies that highlight the need to communicate uncertainty through multiple visual components and different data perspectives."
DBLP:journals/corr/abs-2504-10906,article,"Understanding LLMs' Cross-Lingual Context Retrieval: How Good It Is
And Where It Comes From","Changjiang Gao and
Hankun Lin and
Shujian Huang and
Xin Huang and
Xue Han and
Junlan Feng and
Chao Deng and
Jiajun Chen",2025,CoRR,,abs/2504.10906,,,10.48550/ARXIV.2504.10906,https://doi.org/10.48550/arXiv.2504.10906,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-10906.bib,"Wed, 21 May 2025 01:00:00 +0200",,,,,,2504.10906,arXiv,,"The ability of cross-lingual context retrieval is a fundamental aspect of cross-lingual alignment of large language models (LLMs), where the model extracts context information in one language based on requests in another language. Despite its importance in real-life applications, this ability has not been adequately investigated for state-of-the-art models. In this paper, we evaluate the cross-lingual context retrieval ability of over 40 LLMs across 12 languages to understand the source of this ability, using cross-lingual machine reading comprehension (xMRC) as a representative scenario. Our results show that several small, post-trained open LLMs show strong cross-lingual context retrieval ability, comparable to closed-source LLMs such as GPT-4o, and their estimated oracle performances greatly improve after post-training. Our interpretability analysis shows that the cross-lingual context retrieval process can be divided into two main phases: question encoding and answer retrieval, which are formed in pre-training and post-training, respectively. The phasing stability correlates with xMRC performance, and the xMRC bottleneck lies at the last model layers in the second phase, where the effect of post-training can be evidently observed. Our results also indicate that larger-scale pretraining cannot improve the xMRC performance. Instead, larger LLMs need further multilingual post-training to fully unlock their cross-lingual context retrieval potential. Our code and is available at https://github.com/NJUNLP/Cross-Lingual-Context-Retrieval"
DBLP:journals/corr/abs-2504-11765,article,"Shared Disk {KV} Cache Management for Efficient Multi-Instance Inference
in RAG-Powered LLMs","Hyungwoo Lee and
Kihyun Kim and
Jinwoo Kim and
Jungmin So and
Myung{-}Hoon Cha and
Hong{-}Yeon Kim and
James J. Kim and
Youngjae Kim",2025,CoRR,,abs/2504.11765,,,10.48550/ARXIV.2504.11765,https://doi.org/10.48550/arXiv.2504.11765,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-11765.bib,"Wed, 21 May 2025 01:00:00 +0200",,,,,,2504.11765,arXiv,,"Recent large language models (LLMs) face increasing inference latency as input context length and model size continue to grow. In particular, the retrieval-augmented generation (RAG) technique, which enhances LLM responses by incorporating external knowledge, exacerbates this issue by significantly increasing the number of input tokens. This expansion in token length leads to a substantial rise in computational overhead, particularly during the prefill stage, resulting in prolonged time-to-first-token (TTFT). To address this issue, this paper proposes a method to reduce TTFT by leveraging a disk-based key-value (KV) cache to lessen the computational burden during the prefill stage. We also introduce a disk-based shared KV cache management system, called Shared RAG-DCache, for multi-instance LLM RAG service environments. This system, together with an optimal system configuration, improves both throughput and latency under given resource constraints. Shared RAG-DCache exploits the locality of documents related to user queries in RAG, as well as the queueing delay in LLM inference services. It proactively generates and stores disk KV caches for query-related documents and shares them across multiple LLM instances to enhance inference performance. In experiments on a single host equipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in throughput and up to a 12~65% reduction in latency, depending on the resource configuration."
DBLP:journals/corr/abs-2504-12337,article,"""It Listens Better Than My Therapist"": Exploring Social Media Discourse
on LLMs as Mental Health Tool",Anna{-}Carolina Haensch,2025,CoRR,,abs/2504.12337,,,10.48550/ARXIV.2504.12337,https://doi.org/10.48550/arXiv.2504.12337,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-12337.bib,"Thu, 22 May 2025 01:00:00 +0200",,,,,,2504.12337,arXiv,,"The emergence of generative AI chatbots such as ChatGPT has prompted growing public and academic interest in their role as informal mental health support tools. While early rule-based systems have been around for several years, large language models (LLMs) offer new capabilities in conversational fluency, empathy simulation, and availability. This study explores how users engage with LLMs as mental health tools by analyzing over 10,000 TikTok comments from videos referencing LLMs as mental health tools. Using a self-developed tiered coding schema and supervised classification models, we identify user experiences, attitudes, and recurring themes. Results show that nearly 20% of comments reflect personal use, with these users expressing overwhelmingly positive attitudes. Commonly cited benefits include accessibility, emotional support, and perceived therapeutic value. However, concerns around privacy, generic responses, and the lack of professional oversight remain prominent. It is important to note that the user feedback does not indicate which therapeutic framework, if any, the LLM-generated output aligns with. While the findings underscore the growing relevance of AI in everyday practices, they also highlight the urgent need for clinical and ethical scrutiny in the use of AI for mental health support."
DBLP:journals/corr/abs-2504-15080,article,"Empowering {AI} to Generate Better {AI} Code: Guided Generation of
Deep Learning Projects with LLMs","Chen Xie and
Mingsheng Jiao and
Xiaodong Gu and
Beijun Shen",2025,CoRR,,abs/2504.15080,,,10.48550/ARXIV.2504.15080,https://doi.org/10.48550/arXiv.2504.15080,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-15080.bib,"Fri, 23 May 2025 01:00:00 +0200",,,,,,2504.1508,arXiv,,
DBLP:journals/corr/abs-2504-15439,article,"Combating Toxic Language: {A} Review of LLM-Based Strategies for Software
Engineering","Hao Zhuo and
Yicheng Yang and
Kewen Peng",2025,CoRR,,abs/2504.15439,,,10.48550/ARXIV.2504.15439,https://doi.org/10.48550/arXiv.2504.15439,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-15439.bib,"Tue, 05 Aug 2025 01:00:00 +0200",,,,,,2504.15439,arXiv,,"Large Language Models (LLMs) have become integral to software engineering (SE), where they are increasingly used in development workflows. However, their widespread use raises concerns about the presence and propagation of toxic language--harmful or offensive content that can foster exclusionary environments. This paper provides a comprehensive review of recent research on toxicity detection and mitigation, focusing on both SE-specific and general-purpose datasets. We examine annotation and preprocessing techniques, assess detection methodologies, and evaluate mitigation strategies, particularly those leveraging LLMs. Additionally, we conduct an ablation study demonstrating the effectiveness of LLM-based rewriting for reducing toxicity. By synthesizing existing work and identifying open challenges, this review highlights key areas for future research to ensure the responsible deployment of LLMs in SE and beyond."
DBLP:journals/corr/abs-2504-16032,article,LLMs meet Federated Learning for Scalable and Secure IoT Management,"Yazan Otoum and
Arghavan Asad and
Amiya Nayak",2025,CoRR,,abs/2504.16032,,,10.48550/ARXIV.2504.16032,https://doi.org/10.48550/arXiv.2504.16032,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-16032.bib,"Fri, 23 May 2025 01:00:00 +0200",,,,,,2504.16032,arXiv,,"The rapid expansion of IoT ecosystems introduces severe challenges in scalability, security, and real-time decision-making. Traditional centralized architectures struggle with latency, privacy concerns, and excessive resource consumption, making them unsuitable for modern large-scale IoT deployments. This paper presents a novel Federated Learning-driven Large Language Model (FL-LLM) framework, designed to enhance IoT system intelligence while ensuring data privacy and computational efficiency. The framework integrates Generative IoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS), dynamically optimizing model updates based on real-time network conditions. By leveraging a hybrid edge-cloud processing architecture, our approach balances intelligence, scalability, and security in distributed IoT environments. Evaluations on the IoT-23 dataset demonstrate that our framework improves model accuracy, reduces response latency, and enhances energy efficiency, outperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings highlight the potential of integrating LLM-powered federated learning into large-scale IoT ecosystems, paving the way for more secure, scalable, and adaptive IoT management solutions."
DBLP:journals/corr/abs-2504-16472,article,"Harden and Catch for Just-in-Time Assured LLM-Based Software Testing:
Open Research Challenges","Mark Harman and
Peter W. O'Hearn and
Shubho Sengupta",2025,CoRR,,abs/2504.16472,,,10.48550/ARXIV.2504.16472,https://doi.org/10.48550/arXiv.2504.16472,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-16472.bib,"Fri, 23 May 2025 01:00:00 +0200",,,,,,2504.16472,arXiv,,"Despite decades of research and practice in automated software testing, several fundamental concepts remain ill-defined and under-explored, yet offer enormous potential real-world impact. We show that these concepts raise exciting new challenges in the context of Large Language Models for software test generation. More specifically, we formally define and investigate the properties of hardening and catching tests. A hardening test is one that seeks to protect against future regressions, while a catching test is one that catches such a regression or a fault in new functionality introduced by a code change. Hardening tests can be generated at any time and may become catching tests when a future regression is caught. We also define and motivate the Catching 'Just-in-Time' (JiTTest) Challenge, in which tests are generated 'just-in-time' to catch new faults before they land into production. We show that any solution to Catching JiTTest generation can also be repurposed to catch latent faults in legacy code. We enumerate possible outcomes for hardening and catching tests and JiTTests, and discuss open research problems, deployment options, and initial results from our work on automated LLM-based hardening at Meta. This paper was written to accompany the keynote by the authors at the ACM International Conference on the Foundations of Software Engineering (FSE) 2025. Author order is alphabetical. The corresponding author is Mark Harman."
DBLP:journals/corr/abs-2504-19746,article,"FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained Mixed-Precision
Quantization of LLMs","Xilong Xie and
Liang Wang and
Limin Xiao and
Meng Han and
Lin Sun and
Shuai Zheng and
Xiangrong Xu",2025,CoRR,,abs/2504.19746,,,10.48550/ARXIV.2504.19746,https://doi.org/10.48550/arXiv.2504.19746,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-19746.bib,"Sun, 25 May 2025 01:00:00 +0200",,,,,,2504.19746,arXiv,,
DBLP:journals/corr/abs-2504-20049,article,It's the same but not the same: Do LLMs distinguish Spanish varieties?,"Marina Mayor{-}Rocher and
Cristina del Pozo and
Nina Melero and
Gonzalo Mart{\'{\i}}nez and
Mar{\'{\i}}a Grandury and
Pedro Reviriego",2025,CoRR,,abs/2504.20049,,,10.48550/ARXIV.2504.20049,https://doi.org/10.48550/arXiv.2504.20049,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-20049.bib,"Sun, 25 May 2025 01:00:00 +0200",,,,,,2504.20049,arXiv,,"In recent years, large language models (LLMs) have demonstrated a high capacity for understanding and generating text in Spanish. However, with five hundred million native speakers, Spanish is not a homogeneous language but rather one rich in diatopic variations spanning both sides of the Atlantic. For this reason, in this study, we evaluate the ability of nine language models to identify and distinguish the morphosyntactic and lexical peculiarities of seven varieties of Spanish (Andean, Antillean, Continental Caribbean, Chilean, Peninsular, Mexican and Central American and Rioplatense) through a multiple-choice test. The results indicate that the Peninsular Spanish variety is the best identified by all models and that, among them, GPT-4o is the only model capable of recognizing the variability of the Spanish language. -- En los \'ultimos a\~nos, los grandes modelos de lenguaje (LLMs, por sus siglas en ingl\'es) han demostrado una alta capacidad para comprender y generar texto en espa\~nol. Sin embargo, con quinientos millones de hablantes nativos, la espa\~nola no es una lengua homog\'enea, sino rica en variedades diat\'opicas que se extienden a ambos lados del Atl\'antico. Por todo ello, evaluamos en este trabajo la capacidad de nueve modelos de lenguaje de identificar y discernir las peculiaridades morfosint\'acticas y l\'exicas de siete variedades de espa\~nol (andino, antillano, caribe\~no continental, chileno, espa\~nol peninsular, mexicano y centroamericano y rioplatense) mediante un test de respuesta m\'ultiple. Los resultados obtenidos indican que la variedad de espa\~nol peninsular es la mejor identificada por todos los modelos y que, de entre todos, GPT-4o es el \'unico modelo capaz de identificar la variabilidad de la lengua espa\~nola."
DBLP:journals/corr/abs-2504-20406,article,"Skill Discovery for Software Scripting Automation via Offline Simulations
with LLMs","Paiheng Xu and
Gang Wu and
Xiang Chen and
Tong Yu and
Chang Xiao and
Franck Dernoncourt and
Tianyi Zhou and
Wei Ai and
Viswanathan (Vishy) Swaminathan",2025,CoRR,,abs/2504.20406,,,10.48550/ARXIV.2504.20406,https://doi.org/10.48550/arXiv.2504.20406,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-20406.bib,"Fri, 04 Jul 2025 01:00:00 +0200",,,,,,2504.20406,arXiv,,"Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language queries, runtime code generation is severely limited due to unverified code, security risks, longer response times, and higher computational costs. To bridge the gap, we propose an offline simulation framework to curate a software-specific skillset, a collection of verified scripts, by exploiting LLMs and publicly available scripting guides. Our framework comprises two components: (1) task creation, using top-down functionality guidance and bottom-up API synergy exploration to generate helpful tasks; and (2) skill generation with trials, refining and validating scripts based on execution feedback. To efficiently navigate the extensive API landscape, we introduce a Graph Neural Network (GNN)-based link prediction model to capture API synergy, enabling the generation of skills involving underutilized APIs and expanding the skillset's diversity. Experiments with Adobe Illustrator demonstrate that our framework significantly improves automation success rates, reduces response time, and saves runtime token costs compared to traditional runtime code generation. This is the first attempt to use software scripting interfaces as a testbed for LLM-based systems, highlighting the advantages of leveraging execution feedback in a controlled environment and offering valuable insights into aligning AI capabilities with user needs in specialized software domains."
DBLP:journals/corr/abs-2504-20437,article,GaLore 2: Large-Scale {LLM} Pre-Training by Gradient Low-Rank Projection,"DiJia Su and
Andrew Gu and
Jane Xu and
Yuandong Tian and
Jiawei Zhao",2025,CoRR,,abs/2504.20437,,,10.48550/ARXIV.2504.20437,https://doi.org/10.48550/arXiv.2504.20437,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-20437.bib,"Sun, 25 May 2025 01:00:00 +0200",,,,,,2504.20437,arXiv,,"Large language models (LLMs) have revolutionized natural language understanding and generation but face significant memory bottlenecks during training. GaLore, Gradient Low-Rank Projection, addresses this issue by leveraging the inherent low-rank structure of weight gradients, enabling substantial memory savings without sacrificing performance. Recent works further extend GaLore from various aspects, including low-bit quantization and higher-order tensor structures. However, there are several remaining challenges for GaLore, such as the computational overhead of SVD for subspace updates and the integration with state-of-the-art training parallelization strategies (e.g., FSDP). In this paper, we present GaLore 2, an efficient and scalable GaLore framework that addresses these challenges and incorporates recent advancements. In addition, we demonstrate the scalability of GaLore 2 by pre-training Llama 7B from scratch using up to 500 billion training tokens, highlighting its potential impact on real LLM pre-training scenarios."
DBLP:journals/corr/abs-2504-20781,article,"Using LLMs in Generating Design Rationale for Software Architecture
Decisions","Xiyu Zhou and
Ruiyin Li and
Peng Liang and
Beiqi Zhang and
Mojtaba Shahin and
Zengyang Li and
Chen Yang",2025,CoRR,,abs/2504.20781,,,10.48550/ARXIV.2504.20781,https://doi.org/10.48550/arXiv.2504.20781,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-20781.bib,"Sun, 25 May 2025 01:00:00 +0200",,,,,,2504.20781,arXiv,,"Design Rationale (DR) for software architecture decisions refers to the reasoning underlying architectural choices, which provides valuable insights into the different phases of the architecting process throughout software development. However, in practice, DR is often inadequately documented due to a lack of motivation and effort from developers. With the recent advancements in Large Language Models (LLMs), their capabilities in text comprehension, reasoning, and generation may enable the generation and recovery of DR for architecture decisions. In this study, we evaluated the performance of LLMs in generating DR for architecture decisions. First, we collected 50 Stack Overflow (SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture decisions to construct a dataset of 100 architecture-related problems. Then, we selected five LLMs to generate DR for the architecture decisions with three prompting strategies, including zero-shot, chain of thought (CoT), and LLM-based agents. With the DR provided by human experts as ground truth, the Precision of LLM-generated DR with the three prompting strategies ranges from 0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389. Additionally, 64.45% to 69.42% of the arguments of DR not mentioned by human experts are also helpful, 4.12% to 4.87% of the arguments have uncertain correctness, and 1.59% to 3.24% of the arguments are potentially misleading. To further understand the trustworthiness and applicability of LLM-generated DR in practice, we conducted semi-structured interviews with six practitioners. Based on the experimental and interview results, we discussed the pros and cons of the three prompting strategies, the strengths and limitations of LLM-generated DR, and the implications for the practical use of LLM-generated DR."
DBLP:journals/corr/abs-2504-21589,article,"DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for
Automated Subject Indexing","Lisa Kluge and
Maximilian K{\""{a}}hler",2025,CoRR,,abs/2504.21589,,,10.48550/ARXIV.2504.21589,https://doi.org/10.48550/arXiv.2504.21589,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2504-21589.bib,"Sun, 25 May 2025 01:00:00 +0200",,,,,,2504.21589,arXiv,,"This paper presents our system developed for the SemEval-2025 Task 5: LLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog. Our system relies on prompting a selection of LLMs with varying examples of intellectually annotated records and asking the LLMs to similarly suggest keywords for new records. This few-shot prompting technique is combined with a series of post-processing steps that map the generated keywords to the target vocabulary, aggregate the resulting subject terms to an ensemble vote and, finally, rank them as to their relevance to the record. Our system is fourth in the quantitative ranking in the all-subjects track, but achieves the best result in the qualitative ranking conducted by subject indexing experts."
DBLP:journals/corr/abs-2505-00651,article,Open-Source LLM-Driven Federated Transformer for Predictive IoV Management,"Yazan Otoum and
Arghavan Asad and
Ishtiaq Ahmad",2025,CoRR,,abs/2505.00651,,,10.48550/ARXIV.2505.00651,https://doi.org/10.48550/arXiv.2505.00651,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-00651.bib,"Wed, 18 Jun 2025 01:00:00 +0200",,,,,,2505.00651,arXiv,,"The proliferation of connected vehicles within the Internet of Vehicles (IoV) ecosystem presents critical challenges in ensuring scalable, real-time, and privacy-preserving traffic management. Existing centralized IoV solutions often suffer from high latency, limited scalability, and reliance on proprietary Artificial Intelligence (AI) models, creating significant barriers to widespread deployment, particularly in dynamic and privacy-sensitive environments. Meanwhile, integrating Large Language Models (LLMs) in vehicular systems remains underexplored, especially concerning prompt optimization and effective utilization in federated contexts. To address these challenges, we propose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel framework that leverages open-source LLMs for predictive IoV management. FPoTT introduces a dynamic prompt optimization mechanism that iteratively refines textual prompts to enhance trajectory prediction. The architecture employs a dual-layer federated learning paradigm, combining lightweight edge models for real-time inference with cloud-based LLMs to retain global intelligence. A Transformer-driven synthetic data generator is incorporated to augment training with diverse, high-fidelity traffic scenarios in the Next Generation Simulation (NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing EleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data while maintaining high performance on synthetic datasets. These results underscore the potential of open-source LLMs in enabling secure, adaptive, and scalable IoV management, offering a promising alternative to proprietary solutions in smart mobility ecosystems."
DBLP:journals/corr/abs-2505-01744,article,"Memory-Efficient {LLM} Training by Various-Grained Low-Rank Projection
of Gradients","Yezhen Wang and
Zhouhao Yang and
Brian K. Chen and
Fanyi Pu and
Bo Li and
Tianyu Gao and
Kenji Kawaguchi",2025,CoRR,,abs/2505.01744,,,10.48550/ARXIV.2505.01744,https://doi.org/10.48550/arXiv.2505.01744,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-01744.bib,"Wed, 18 Jun 2025 01:00:00 +0200",,,,,,2505.01744,arXiv,,"Building upon the success of low-rank adapter (LoRA), low-rank gradient projection (LoRP) has emerged as a promising solution for memory-efficient fine-tuning. However, existing LoRP methods typically treat each row of the gradient matrix as the default projection unit, leaving the role of projection granularity underexplored. In this work, we propose a novel framework, VLoRP, that extends low-rank gradient projection by introducing an additional degree of freedom for controlling the trade-off between memory efficiency and performance, beyond the rank hyper-parameter. Through this framework, we systematically explore the impact of projection granularity, demonstrating that finer-grained projections lead to enhanced stability and efficiency even under a fixed memory budget. Regarding the optimization for VLoRP, we present ProjFactor, an adaptive memory-efficient optimizer, that significantly reduces memory requirement while ensuring competitive performance, even in the presence of gradient accumulation. Additionally, we provide a theoretical analysis of VLoRP, demonstrating the descent and convergence of its optimization trajectory under both SGD and ProjFactor. Extensive experiments are conducted to validate our findings, covering tasks such as commonsense reasoning, MMLU, and GSM8K."
DBLP:journals/corr/abs-2505-01841,article,"Harnessing the Power of LLMs, Informers and Decision Transformers
for Intent-driven {RAN} Management in 6G","Md Arafat Habib and
Pedro Enrique Iturria{-}Rivera and
Yigit Ozcan and
Medhat H. M. Elsayed and
Majid Bavand and
Raimundas Gaigalas and
Melike Erol{-}Kantarci",2025,CoRR,,abs/2505.01841,,,10.48550/ARXIV.2505.01841,https://doi.org/10.48550/arXiv.2505.01841,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-01841.bib,"Wed, 18 Jun 2025 01:00:00 +0200",,,,,,2505.01841,arXiv,,"Intent-driven network management is critical for managing the complexity of 5G and 6G networks. It enables adaptive, on-demand management of the network based on the objectives of the network operators. In this paper, we propose an innovative three-step framework for intent-driven network management based on Generative AI (GenAI) algorithms. First, we fine-tune a Large Language Model (LLM) on a custom dataset using a Quantized Low-Rank Adapter (QLoRA) to enable memory-efficient intent processing within limited computational resources. A Retrieval Augmented Generation (RAG) module is included to support dynamic decision-making. Second, we utilize a transformer architecture for time series forecasting to predict key parameters, such as power consumption, traffic load, and packet drop rate, to facilitate intent validation proactively. Lastly, we introduce a Hierarchical Decision Transformer with Goal Awareness (HDTGA) to optimize the selection and orchestration of network applications and hence, optimize the network. Our intent guidance and processing approach improves BERTScore by 6% and the semantic similarity score by 9% compared to the base LLM model. Again, the proposed predictive intent validation approach can successfully rule out the performance-degrading intents with an average of 88% accuracy. Finally, compared to the baselines, the proposed HDTGA algorithm increases throughput at least by 19.3%, reduces delay by 48.5%, and boosts energy efficiency by 54.9%."
DBLP:journals/corr/abs-2505-03563,article,"Say It Another Way: Auditing LLMs with a User-Grounded Automated Paraphrasing
Framework","Cl{\'{e}}a Chataigner and
Rebecca Ma and
Prakhar Ganesh and
Afaf Ta{\""{\i}}k and
Elliot Creager and
Golnoosh Farnadi",2025,CoRR,,abs/2505.03563,,,10.48550/ARXIV.2505.03563,https://doi.org/10.48550/arXiv.2505.03563,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-03563.bib,"Fri, 20 Jun 2025 01:00:00 +0200",,,,,,2505.03563,arXiv,,"Large language models (LLMs) are highly sensitive to subtle changes in prompt phrasing, posing challenges for reliable auditing. Prior methods often apply unconstrained prompt paraphrasing, which risk missing linguistic and demographic factors that shape authentic user interactions. We introduce AUGMENT (Automated User-Grounded Modeling and Evaluation of Natural Language Transformations), a framework for generating controlled paraphrases, grounded in user behaviors. AUGMENT leverages linguistically informed rules and enforces quality through checks on instruction adherence, semantic similarity, and realism, ensuring paraphrases are both reliable and meaningful for auditing. Through case studies on the BBQ and MMLU datasets, we show that controlled paraphrases uncover systematic weaknesses that remain obscured under unconstrained variation. These results highlight the value of the AUGMENT framework for reliable auditing."
DBLP:journals/corr/abs-2505-03796,article,"AI-Driven {IRM:} Transforming insider risk management with adaptive
scoring and LLM-based threat detection","Lokesh Koli and
Shubham Kalra and
Rohan Krishna Thakur and
Anas Saifi and
Karanpreet Singh",2025,CoRR,,abs/2505.03796,,,10.48550/ARXIV.2505.03796,https://doi.org/10.48550/arXiv.2505.03796,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-03796.bib,"Fri, 20 Jun 2025 01:00:00 +0200",,,,,,2505.03796,arXiv,,"Insider threats pose a significant challenge to organizational security, often evading traditional rule-based detection systems due to their subtlety and contextual nature. This paper presents an AI-powered Insider Risk Management (IRM) system that integrates behavioral analytics, dynamic risk scoring, and real-time policy enforcement to detect and mitigate insider threats with high accuracy and adaptability. We introduce a hybrid scoring mechanism - transitioning from the static PRISM model to an adaptive AI-based model utilizing an autoencoder neural network trained on expert-annotated user activity data. Through iterative feedback loops and continuous learning, the system reduces false positives by 59% and improves true positive detection rates by 30%, demonstrating substantial gains in detection precision. Additionally, the platform scales efficiently, processing up to 10 million log events daily with sub-300ms query latency, and supports automated enforcement actions for policy violations, reducing manual intervention. The IRM system's deployment resulted in a 47% reduction in incident response times, highlighting its operational impact. Future enhancements include integrating explainable AI, federated learning, graph-based anomaly detection, and alignment with Zero Trust principles to further elevate its adaptability, transparency, and compliance-readiness. This work establishes a scalable and proactive framework for mitigating emerging insider risks in both on-premises and hybrid environments."
DBLP:journals/corr/abs-2505-04251,article,"Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent
System oriented Software Engineering",Krishna Ronanki,2025,CoRR,,abs/2505.04251,,,10.48550/ARXIV.2505.04251,https://doi.org/10.48550/arXiv.2505.04251,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-04251.bib,"Fri, 20 Jun 2025 01:00:00 +0200",,,,,,2505.04251,arXiv,,"Multi-agent autonomous systems (MAS) are better at addressing challenges that spans across multiple domains than singular autonomous agents. This holds true within the field of software engineering (SE) as well. The state-of-the-art research on MAS within SE focuses on integrating LLMs at the core of autonomous agents to create LLM-based multi-agent autonomous (LMA) systems. However, the introduction of LMA systems into SE brings a plethora of challenges. One of the major challenges is the strategic allocation of tasks between humans and the LMA system in a trustworthy manner. To address this challenge, a RACI-based framework is proposed in this work in progress article, along with implementation guidelines and an example implementation of the framework. The proposed framework can facilitate efficient collaboration, ensure accountability, and mitigate potential risks associated with LLM-driven automation while aligning with the Trustworthy AI guidelines. The future steps for this work delineating the planned empirical validation method are also presented."
DBLP:journals/corr/abs-2505-04628,article,"How Social is It? {A} Benchmark for LLMs' Capabilities in Multi-user
Multi-turn Social Agent Tasks","Yusen Wu and
Junwu Xiong and
Xiaotie Deng",2025,CoRR,,abs/2505.04628,,,10.48550/ARXIV.2505.04628,https://doi.org/10.48550/arXiv.2505.04628,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-04628.bib,"Fri, 20 Jun 2025 01:00:00 +0200",,,,,,2505.04628,arXiv,,"Expanding the application of large language models (LLMs) to societal life, instead of primary function only as auxiliary assistants to communicate with only one person at a time, necessitates LLMs' capabilities to independently play roles in multi-user, multi-turn social agent tasks within complex social settings. However, currently the capability has not been systematically measured with available benchmarks. To address this gap, we first introduce an agent task leveling framework grounded in sociological principles. Concurrently, we propose a novel benchmark, How Social Is It (we call it HSII below), designed to assess LLM's social capabilities in comprehensive social agents tasks and benchmark representative models. HSII comprises four stages: format parsing, target selection, target switching conversation, and stable conversation, which collectively evaluate the communication and task completion capabilities of LLMs within realistic social interaction scenarios dataset, HSII-Dataset. The dataset is derived step by step from news dataset. We perform an ablation study by doing clustering to the dataset. Additionally, we investigate the impact of chain of thought (COT) method on enhancing LLMs' social performance. Since COT cost more computation, we further introduce a new statistical metric, COT-complexity, to quantify the efficiency of certain LLMs with COTs for specific social tasks and strike a better trade-off between measurement of correctness and efficiency. Various results of our experiments demonstrate that our benchmark is well-suited for evaluating social skills in LLMs."
DBLP:journals/corr/abs-2505-07793,article,Overflow Prevention Enhances Long-Context Recurrent LLMs,"Assaf Ben{-}Kish and
Itamar Zimerman and
Muhammad Jehanzeb Mirza and
James R. Glass and
Leonid Karlinsky and
Raja Giryes",2025,CoRR,,abs/2505.07793,,,10.48550/ARXIV.2505.07793,https://doi.org/10.48550/arXiv.2505.07793,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-07793.bib,"Mon, 23 Jun 2025 01:00:00 +0200",,,,,,2505.07793,arXiv,,"A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations."
DBLP:journals/corr/abs-2505-10008,article,"{SVA-ICL:} Improving LLM-based Software Vulnerability Assessment via
In-Context Learning and Information Fusion","Chaoyang Gao and
Xiang Chen and
Guangbei Zhang",2025,CoRR,,abs/2505.10008,,,10.48550/ARXIV.2505.10008,https://doi.org/10.48550/arXiv.2505.10008,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-10008.bib,"Mon, 23 Jun 2025 01:00:00 +0200",,,,,,2505.10008,arXiv,,
DBLP:journals/corr/abs-2505-10321,article,"AutoPentest: Enhancing Vulnerability Management With Autonomous {LLM}
Agents",Julius Henke,2025,CoRR,,abs/2505.10321,,,10.48550/ARXIV.2505.10321,https://doi.org/10.48550/arXiv.2505.10321,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-10321.bib,"Mon, 23 Jun 2025 01:00:00 +0200",,,,,,2505.10321,arXiv,,"A recent area of increasing research is the use of Large Language Models (LLMs) in penetration testing, which promises to reduce costs and thus allow for higher frequency. We conduct a review of related work, identifying best practices and common evaluation issues. We then present AutoPentest, an application for performing black-box penetration tests with a high degree of autonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent framework LangChain. It can perform complex multi-step tasks, augmented by external tools and knowledge bases. We conduct a study on three capture-the-flag style Hack The Box (HTB) machines, comparing our implementation AutoPentest with the baseline approach of manually using the ChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the subtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT. We measure a total cost of \$96.20 US when using AutoPentest across all experiments, while a one-month subscription to ChatGPT Plus costs \$20. The results show that further implementation efforts and the use of more powerful LLMs released in the future are likely to make this a viable part of vulnerability management."
DBLP:journals/corr/abs-2505-11570,article,"Tool-Aided Evolutionary {LLM} for Generative Policy Toward Efficient
Resource Management in Wireless Federated Learning","Chongyang Tan and
Ruoqi Wen and
Rongpeng Li and
Zhifeng Zhao and
Ekram Hossain and
Honggang Zhang",2025,CoRR,,abs/2505.11570,,,10.48550/ARXIV.2505.11570,https://doi.org/10.48550/arXiv.2505.11570,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-11570.bib,"Mon, 23 Jun 2025 01:00:00 +0200",,,,,,2505.1157,arXiv,,"Federated Learning (FL) enables distributed model training across edge devices in a privacy-friendly manner. However, its efficiency heavily depends on effective device selection and high-dimensional resource allocation in dynamic and heterogeneous wireless environments. Conventional methods demand a confluence of domain-specific expertise, extensive hyperparameter tuning, and/or heavy interaction cost. This paper proposes a Tool-aided Evolutionary Large Language Model (T-ELLM) framework to generate a qualified policy for device selection in a wireless FL environment. Unlike conventional optimization methods, T-ELLM leverages natural language-based scenario prompts to enhance generalization across varying network conditions. The framework decouples the joint optimization problem mathematically, enabling tractable learning of device selection policies while delegating resource allocation to convex optimization tools. To improve adaptability, T-ELLM integrates a sample-efficient, model-based virtual learning environment that captures the relationship between device selection and learning performance, facilitating subsequent group relative policy optimization. This concerted approach reduces reliance on real-world interactions, minimizing communication overhead while maintaining high-fidelity decision-making. Theoretical analysis proves that the discrepancy between virtual and real environments is bounded, ensuring the advantage function learned in the virtual environment maintains a provably small deviation from real-world conditions. Experimental results demonstrate that T-ELLM outperforms benchmark methods in energy efficiency and exhibits robust adaptability to environmental changes."
DBLP:journals/corr/abs-2505-12259,article,"Teach2Eval: An Indirect Evaluation Method for {LLM} by Judging How
It Teaches","Yuhang Zhou and
Xutian Chen and
Yixin Cao and
Yuchen Ni and
Yu He and
Siyu Tian and
Xiang Liu and
Jian Zhang and
Chuanjun Ji and
Guangnan Ye and
Xipeng Qiu",2025,CoRR,,abs/2505.12259,,,10.48550/ARXIV.2505.12259,https://doi.org/10.48550/arXiv.2505.12259,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-12259.bib,"Mon, 23 Jun 2025 01:00:00 +0200",,,,,,2505.12259,arXiv,,"Recent progress in large language models (LLMs) has outpaced the development of effective evaluation methods. Traditional benchmarks rely on task-specific metrics and static datasets, which often suffer from fairness issues, limited scalability, and contamination risks. In this paper, we introduce Teach2Eval, an indirect evaluation framework inspired by the Feynman Technique. Instead of directly testing LLMs on predefined tasks, our method evaluates a model's multiple abilities to teach weaker student models to perform tasks effectively. By converting open-ended tasks into standardized multiple-choice questions (MCQs) through teacher-generated feedback, Teach2Eval enables scalable, automated, and multi-dimensional assessment. Our approach not only avoids data leakage and memorization but also captures a broad range of cognitive abilities that are orthogonal to current benchmarks. Experimental results across 26 leading LLMs show strong alignment with existing human and model-based dynamic rankings, while offering additional interpretability for training guidance."
DBLP:journals/corr/abs-2505-12951,article,"{DGRO:} Enhancing {LLM} Reasoning via Exploration-Exploitation Control
and Reward Variance Management","Xuerui Su and
Liya Guo and
Yue Wang and
Yi Zhu and
Zhiming Ma and
Zun Wang and
Yuting Liu",2025,CoRR,,abs/2505.12951,,,10.48550/ARXIV.2505.12951,https://doi.org/10.48550/arXiv.2505.12951,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-12951.bib,"Mon, 08 Sep 2025 01:00:00 +0200",,,,,,2505.12951,arXiv,,"Inference scaling further accelerates Large Language Models (LLMs) toward Artificial General Intelligence (AGI), with large-scale Reinforcement Learning (RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning approaches usually rely on handcrafted rule-based reward functions. However, the tarde-offs of exploration and exploitation in RL algorithms involves multiple complex considerations, and the theoretical and empirical impacts of manually designed reward functions remain insufficiently explored. In this paper, we propose Decoupled Group Reward Optimization (DGRO), a general RL algorithm for LLM reasoning. On the one hand, DGRO decouples the traditional regularization coefficient into two independent hyperparameters: one scales the policy gradient term, and the other regulates the distance from the sampling policy. This decoupling not only enables precise control over balancing exploration and exploitation, but also can be seamlessly extended to Online Policy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward Optimization. On the other hand, we observe that reward variance significantly affects both convergence speed and final model performance. We conduct both theoretical analysis and extensive empirical validation to assess DGRO, including a detailed ablation study that investigates its performance and optimization dynamics. Experimental results show that DGRO achieves state-of-the-art performance on the Logic dataset with an average accuracy of 96.9\%, and demonstrates strong generalization across mathematical benchmarks."
DBLP:journals/corr/abs-2505-13766,article,"Advancing Software Quality: {A} Standards-Focused Review of LLM-Based
Assurance Techniques",Avinash Patil,2025,CoRR,,abs/2505.13766,,,10.48550/ARXIV.2505.13766,https://doi.org/10.48550/arXiv.2505.13766,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-13766.bib,"Wed, 25 Jun 2025 01:00:00 +0200",,,,,,2505.13766,arXiv,,"Software Quality Assurance (SQA) is critical for delivering reliable, secure, and efficient software products. The Software Quality Assurance Process aims to provide assurance that work products and processes comply with predefined provisions and plans. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance existing SQA processes by automating tasks like requirement analysis, code review, test generation, and compliance checks. Simultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010, ISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured frameworks for ensuring robust quality practices. This paper surveys the intersection of LLM-based SQA methods and these recognized standards, highlighting how AI-driven solutions can augment traditional approaches while maintaining compliance and process maturity. We first review the foundational software quality standards and the technical fundamentals of LLMs in software engineering. Next, we explore various LLM-based SQA applications, including requirement validation, defect detection, test generation, and documentation maintenance. We then map these applications to key software quality frameworks, illustrating how LLMs can address specific requirements and metrics within each standard. Empirical case studies and open-source initiatives demonstrate the practical viability of these methods. At the same time, discussions on challenges (e.g., data privacy, model bias, explainability) underscore the need for deliberate governance and auditing. Finally, we propose future directions encompassing adaptive learning, privacy-focused deployments, multimodal analysis, and evolving standards for AI-driven software quality."
DBLP:journals/corr/abs-2505-14336,article,"Scaling and Enhancing LLM-based {AVSR:} {A} Sparse Mixture of Projectors
Approach","Umberto Cappellazzo and
Minsu Kim and
Stavros Petridis and
Daniele Falavigna and
Alessio Brutti",2025,CoRR,,abs/2505.14336,,,10.48550/ARXIV.2505.14336,https://doi.org/10.48550/arXiv.2505.14336,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-14336.bib,"Sun, 29 Jun 2025 01:00:00 +0200",,,,,,2505.14336,arXiv,,"Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy environments by integrating visual cues. While recent advances integrate Large Language Models (LLMs) into AVSR, their high computational cost hinders deployment in resource-constrained settings. To address this, we propose Llama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of Projectors (SMoP) module to scale model capacity without increasing inference costs. By incorporating sparsely-gated mixture-of-experts (MoE) projectors, Llama-SMoP enables the use of smaller LLMs while maintaining strong performance. We explore three SMoP configurations and show that Llama-SMoP DEDR (Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and experts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation studies confirm its effectiveness in expert activation, scalability, and noise robustness."
DBLP:journals/corr/abs-2505-15347,article,"FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via
Isolated Key-Value Cache Management","Xiang Liu and
Hong Chen and
Xuming Hu and
Xiaowen Chu",2025,CoRR,,abs/2505.15347,,,10.48550/ARXIV.2505.15347,https://doi.org/10.48550/arXiv.2505.15347,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-15347.bib,"Thu, 26 Jun 2025 01:00:00 +0200",,,,,,2505.15347,arXiv,,"Large Language Models (LLMs) are increasingly deployed in multi-turn conversational applications, where the management of the Key-Value (KV) Cache presents a significant bottleneck. The linear growth of the KV Cache with dialogue history imposes substantial computational costs, and existing eviction strategies often degrade performance by repeatedly compressing early conversational context, leading to information loss and context forgetting. This paper introduces FlowKV, a novel \textbf{multi-turn isolation mechanism} for KV Cache management, which can be applied to any KV Cache compression method without training. FlowKV's core innovation is a multi-turn isolation mechanism that preserves the accumulated compressed KV cache from past turns. Compression is then strategically applied only to the newly generated KV pairs of the latest completed turn, effectively preventing the re-compression of older context and thereby mitigating catastrophic forgetting. Our results demonstrate that FlowKV consistently and significantly outperforms baseline strategies in maintaining instruction-following accuracy and user preference retention from 10.90\% to 75.40\%, particularly in later conversational turns."
DBLP:journals/corr/abs-2505-16067,article,"How Memory Management Impacts {LLM} Agents: An Empirical Study of
Experience-Following Behavior","Zidi Xiong and
Yuping Lin and
Wenya Xie and
Pengfei He and
Jiliang Tang and
Himabindu Lakkaraju and
Zhen Xiang",2025,CoRR,,abs/2505.16067,,,10.48550/ARXIV.2505.16067,https://doi.org/10.48550/arXiv.2505.16067,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-16067.bib,"Wed, 25 Jun 2025 01:00:00 +0200",,,,,,2505.16067,arXiv,,"Memory is a critical component in large language model (LLM)-based agents, enabling them to store and retrieve past executions to improve task performance over time. In this paper, we conduct an empirical study on how memory management choices impact the LLM agents' behavior, especially their long-term performance. Specifically, we focus on two fundamental memory operations that are widely used by many agent frameworks-addition, which incorporates new experiences into the memory base, and deletion, which selectively removes past experiences-to systematically study their impact on the agent behavior. Through our quantitative analysis, we find that LLM agents display an experience-following property: high similarity between a task input and the input in a retrieved memory record often results in highly similar agent outputs. Our analysis further reveals two significant challenges associated with this property: error propagation, where inaccuracies in past experiences compound and degrade future performance, and misaligned experience replay, where outdated or irrelevant experiences negatively influence current tasks. Through controlled experiments, we show that combining selective addition and deletion strategies can help mitigate these negative effects, yielding an average absolute performance gain of 10% compared to naive memory growth. Furthermore, we highlight how memory management choices affect agents' behavior under challenging conditions such as task distribution shifts and constrained memory resources. Our findings offer insights into the behavioral dynamics of LLM agent memory systems and provide practical guidance for designing memory components that support robust, long-term agent performance. We also release our code to facilitate further study."
DBLP:journals/corr/abs-2505-16086,article,"Optimizing LLM-Based Multi-Agent System with Textual Feedback: {A}
Case Study on Software Development","Ming Shen and
Raphael Shu and
Anurag Pratik and
James Gung and
Yubin Ge and
Monica Sunkara and
Yi Zhang",2025,CoRR,,abs/2505.16086,,,10.48550/ARXIV.2505.16086,https://doi.org/10.48550/arXiv.2505.16086,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-16086.bib,"Wed, 25 Jun 2025 01:00:00 +0200",,,,,,2505.16086,arXiv,,"We have seen remarkable progress in large language models (LLMs) empowered multi-agent systems solving complex tasks necessitating cooperation among experts with diverse skills. However, optimizing LLM-based multi-agent systems remains challenging. In this work, we perform an empirical case study on group optimization of role-based multi-agent systems utilizing natural language feedback for challenging software development tasks under various evaluation dimensions. We propose a two-step agent prompts optimization pipeline: identifying underperforming agents with their failure explanations utilizing textual feedback and then optimizing system prompts of identified agents utilizing failure explanations. We then study the impact of various optimization settings on system performance with two comparison groups: online against offline optimization and individual against group optimization. For group optimization, we study two prompting strategies: one-pass and multi-pass prompting optimizations. Overall, we demonstrate the effectiveness of our optimization method for role-based multi-agent systems tackling software development tasks evaluated on diverse evaluation dimensions, and we investigate the impact of diverse optimization settings on group behaviors of the multi-agent systems to provide practical insights for future development."
DBLP:journals/corr/abs-2505-16697,article,Software Architecture Meets LLMs: {A} Systematic Literature Review,"Larissa Schmid and
Tobias Hey and
Martin Armbruster and
Sophie Corallo and
Dominik Fuch{\ss} and
Jan Keim and
Haoyu Liu and
Anne Koziolek",2025,CoRR,,abs/2505.16697,,,10.48550/ARXIV.2505.16697,https://doi.org/10.48550/arXiv.2505.16697,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-16697.bib,"Tue, 05 Aug 2025 01:00:00 +0200",,,,,,2505.16697,arXiv,,"Large Language Models (LLMs) are used for many different software engineering tasks. In software architecture, they have been applied to tasks such as classification of design decisions, detection of design patterns, and generation of software architecture design from requirements. However, there is little overview on how well they work, what challenges exist, and what open problems remain. In this paper, we present a systematic literature review on the use of LLMs in software architecture. We analyze 18 research articles to answer five research questions, such as which software architecture tasks LLMs are used for, how much automation they provide, which models and techniques are used, and how these approaches are evaluated. Our findings show that while LLMs are increasingly applied to a variety of software architecture tasks and often outperform baselines, some areas, such as generating source code from architectural design, cloud-native computing and architecture, and checking conformance remain underexplored. Although current approaches mostly use simple prompting techniques, we identify a growing research interest in refining LLM-based approaches by integrating advanced techniques."
DBLP:journals/corr/abs-2505-17084,article,"From nuclear safety to {LLM} security: Applying non-probabilistic
risk management strategies to build safe and secure LLM-powered systems","Alexander Gutfraind and
Vicki M. Bier",2025,CoRR,,abs/2505.17084,,,10.48550/ARXIV.2505.17084,https://doi.org/10.48550/arXiv.2505.17084,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-17084.bib,"Thu, 26 Jun 2025 01:00:00 +0200",,,,,,2505.17084,arXiv,,"Large language models (LLMs) offer unprecedented and growing capabilities, but also introduce complex safety and security challenges that resist conventional risk management. While conventional probabilistic risk analysis (PRA) requires exhaustive risk enumeration and quantification, the novelty and complexity of these systems make PRA impractical, particularly against adaptive adversaries. Previous research found that risk management in various fields of engineering such as nuclear or civil engineering is often solved by generic (i.e. field-agnostic) strategies such as event tree analysis or robust designs. Here we show how emerging risks in LLM-powered systems could be met with 100+ of these non-probabilistic strategies to risk management, including risks from adaptive adversaries. The strategies are divided into five categories and are mapped to LLM security (and AI safety more broadly). We also present an LLM-powered workflow for applying these strategies and other workflows suitable for solution architects. Overall, these strategies could contribute (despite some limitations) to security, safety and other dimensions of responsible AI."
DBLP:journals/corr/abs-2505-17196,article,Shape it Up! Restoring {LLM} Safety during Finetuning,"Shengyun Peng and
Pin{-}Yu Chen and
Jianfeng Chi and
Seongmin Lee and
Duen Horng Chau",2025,CoRR,,abs/2505.17196,,,10.48550/ARXIV.2505.17196,https://doi.org/10.48550/arXiv.2505.17196,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-17196.bib,"Thu, 26 Jun 2025 01:00:00 +0200",,,,,,2505.17196,arXiv,,"Finetuning large language models (LLMs) enables user-specific customization but introduces critical safety risks: even a few harmful examples can compromise safety alignment. A common mitigation strategy is to update the model more strongly on examples deemed safe, while downweighting or excluding those flagged as unsafe. However, because safety context can shift within a single example, updating the model equally on both harmful and harmless parts of a response is suboptimal-a coarse treatment we term static safety shaping. In contrast, we propose dynamic safety shaping (DSS), a framework that uses fine-grained safety signals to reinforce learning from safe segments of a response while suppressing unsafe content. To enable such fine-grained control during finetuning, we introduce a key insight: guardrail models, traditionally used for filtering, can be repurposed to evaluate partial responses, tracking how safety risk evolves throughout the response, segment by segment. This leads to the Safety Trajectory Assessment of Response (STAR), a token-level signal that enables shaping to operate dynamically over the training sequence. Building on this, we present STAR-DSS, guided by STAR scores, that robustly mitigates finetuning risks and delivers substantial safety improvements across diverse threats, datasets, and model families-all without compromising capability on intended tasks. We encourage future safety research to build on dynamic shaping principles for stronger mitigation against evolving finetuning risks."
DBLP:journals/corr/abs-2505-17416,article,"{LLM-BSCVM:} An LLM-Based Blockchain Smart Contract Vulnerability
Management Framework","Yanli Jin and
Chunpei Li and
Peng Fan and
Peng Liu and
Xianxian Li and
Chen Liu and
Wangjie Qiu",2025,CoRR,,abs/2505.17416,,,10.48550/ARXIV.2505.17416,https://doi.org/10.48550/arXiv.2505.17416,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-17416.bib,"Thu, 21 Aug 2025 01:00:00 +0200",,,,,,2505.17416,arXiv,,"Smart contracts are a key component of the Web 3.0 ecosystem, widely applied in blockchain services and decentralized applications. However, the automated execution feature of smart contracts makes them vulnerable to potential attacks due to inherent flaws, which can lead to severe security risks and financial losses, even threatening the integrity of the entire decentralized finance system. Currently, research on smart contract vulnerabilities has evolved from traditional program analysis methods to deep learning techniques, with the gradual introduction of Large Language Models. However, existing studies mainly focus on vulnerability detection, lacking systematic cause analysis and Vulnerability Repair. To address this gap, we propose LLM-BSCVM, a Large Language Model-based smart contract vulnerability management framework, designed to provide end-to-end vulnerability detection, analysis, repair, and evaluation capabilities for Web 3.0 ecosystem. LLM-BSCVM combines retrieval-augmented generation technology and multi-agent collaboration, introducing a three-stage method of Decompose-Retrieve-Generate. This approach enables smart contract vulnerability management through the collaborative efforts of six intelligent agents, specifically: vulnerability detection, cause analysis, repair suggestion generation, risk assessment, vulnerability repair, and patch evaluation. Experimental results demonstrate that LLM-BSCVM achieves a vulnerability detection accuracy and F1 score exceeding 91\% on benchmark datasets, comparable to the performance of state-of-the-art (SOTA) methods, while reducing the false positive rate from 7.2\% in SOTA methods to 5.1\%, thus enhancing the reliability of vulnerability management. Furthermore, LLM-BSCVM supports continuous security monitoring and governance of smart contracts through a knowledge base hot-swapping dynamic update mechanism."
DBLP:journals/corr/abs-2505-17710,article,{LLM} Contribution Summarization in Software Projects,"Rafael Corsi Ferr{\~{a}}o and
Fabio Roberto de Miranda and
Diego Pavan Soler",2025,CoRR,,abs/2505.17710,,,10.48550/ARXIV.2505.17710,https://doi.org/10.48550/arXiv.2505.17710,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-17710.bib,"Thu, 26 Jun 2025 01:00:00 +0200",,,,,,2505.1771,arXiv,,"This full paper in innovative practice provides an automated tool to summarize individual code contributions in project-based courses with external clients. Real industry projects offer valuable learning opportunities by immersing students in authentic problems defined by external clients. However, the open-ended and highly variable scope of these projects makes it challenging for instructors and teaching assistants to provide timely and detailed feedback. This paper addresses the need for an automated and objective approach to evaluate individual contributions within team projects. In this paper, we present a tool that leverages a large language model (LLM) to automatically summarize code contributions extracted from version control repositories. The tool preprocesses and structures repository data, and uses PyDriller to isolate individual contributions. Its uniqueness lies in the combination of LLM prompt engineering with automated repository analysis, thus reducing the manual grading burden while providing regular and informative updates. The tool was assessed over two semesters during a three-week, full-time software development sprint involving 65 students. Weekly summaries were provided to teams, and both student and faculty feedback indicated the tool's overall usefulness in informing grading and guidance. The tool reports, in large proportion, activities that were in fact performed by the student, with some failure to detect students' contribution. The summaries were considered by the instructors as a useful potential tool to keep up with the projects."
DBLP:journals/corr/abs-2505-17813,article,"Don't Overthink it. Preferring Shorter Thinking Chains for Improved
{LLM} Reasoning","Michael Hassid and
Gabriel Synnaeve and
Yossi Adi and
Roy Schwartz",2025,CoRR,,abs/2505.17813,,,10.48550/ARXIV.2505.17813,https://doi.org/10.48550/arXiv.2505.17813,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-17813.bib,"Thu, 26 Jun 2025 01:00:00 +0200",,,,,,2505.17813,arXiv,,"Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive""thinking""chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge the assumption that long thinking chains results in better reasoning capabilities. We first demonstrate that shorter reasoning chains within individual questions are significantly more likely to yield correct answers - up to 34.5% more accurate than the longest chain sampled for the same question. Based on these results, we suggest short-m@k, a novel reasoning LLM inference method. Our method executes k independent generations in parallel and halts computation once the first m thinking processes are done. The final answer is chosen using majority voting among these m chains. Basic short-1@k demonstrates similar or even superior performance over standard majority voting in low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while slightly less efficient than short-1@k, consistently surpasses majority voting across all compute budgets, while still being substantially faster (up to 33% wall time reduction). Inspired by our results, we finetune an LLM using short, long, and randomly selected reasoning chains. We then observe that training on the shorter ones leads to better performance. Our findings suggest rethinking current methods of test-time compute in reasoning LLMs, emphasizing that longer""thinking""does not necessarily translate to improved performance and can, counter-intuitively, lead to degraded results."
DBLP:journals/corr/abs-2505-18019,article,"{LLM} assisted web application functional requirements generation:
{A} case study of four popular LLMs over a Mess Management System","Rashmi Gupta and
Aditya K. Gupta and
Aarav Jain and
Avinash C. Pandey and
Atul Gupta",2025,CoRR,,abs/2505.18019,,,10.48550/ARXIV.2505.18019,https://doi.org/10.48550/arXiv.2505.18019,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-18019.bib,"Thu, 26 Jun 2025 01:00:00 +0200",,,,,,2505.18019,arXiv,,"Like any other discipline, Large Language Models (LLMs) have significantly impacted software engineering by helping developers generate the required artifacts across various phases of software development. This paper presents a case study comparing the performance of popular LLMs GPT, Claude, Gemini, and DeepSeek in generating functional specifications that include use cases, business rules, and collaborative workflows for a web application, the Mess Management System. The study evaluated the quality of LLM generated use cases, business rules, and collaborative workflows in terms of their syntactic and semantic correctness, consistency, non ambiguity, and completeness compared to the reference specifications against the zero-shot prompted problem statement. Our results suggested that all four LLMs can specify syntactically and semantically correct, mostly non-ambiguous artifacts. Still, they may be inconsistent at times and may differ significantly in the completeness of the generated specification. Claude and Gemini generated all the reference use cases, with Claude achieving the most complete but somewhat redundant use case specifications. Similar results were obtained for specifying workflows. However, all four LLMs struggled to generate relevant Business Rules, with DeepSeek generating the most reference rules but with less completeness. Overall, Claude generated more complete specification artifacts, while Gemini was more precise in the specifications it generated."
DBLP:journals/corr/abs-2505-18382,article,"One Demo Is All It Takes: Planning Domain Derivation with LLMs from
{A} Single Demonstration","Jinbang Huang and
Yixin Xiao and
Zhanguang Zhang and
Mark Coates and
Jianye Hao and
Yingxue Zhang",2025,CoRR,,abs/2505.18382,,,10.48550/ARXIV.2505.18382,https://doi.org/10.48550/arXiv.2505.18382,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-18382.bib,"Sun, 29 Jun 2025 01:00:00 +0200",,,,,,2505.18382,arXiv,,"Pre-trained large language models (LLMs) show promise for robotic task planning but often struggle to guarantee correctness in long-horizon problems. Task and motion planning (TAMP) addresses this by grounding symbolic plans in low-level execution, yet it relies heavily on manually engineered planning domains. To improve long-horizon planning reliability and reduce human intervention, we present Planning Domain Derivation with LLMs (PDDLLM), a framework that automatically induces symbolic predicates and actions directly from demonstration trajectories by combining LLM reasoning with physical simulation roll-outs. Unlike prior domain-inference methods that rely on partially predefined or language descriptions of planning domains, PDDLLM constructs domains without manual domain initialization and automatically integrates them with motion planners to produce executable plans, enhancing long-horizon planning automation. Across 1,200 tasks in nine environments, PDDLLM outperforms six LLM-based planning baselines, achieving at least 20\% higher success rates, reduced token costs, and successful deployment on multiple physical robot platforms."
DBLP:journals/corr/abs-2505-18597,article,LLMs for Supply Chain Management,"Haojie Wang and
Jiuyun Jiang and
L. Jeff Hong and
Guangxin Jiang",2025,CoRR,,abs/2505.18597,,,10.48550/ARXIV.2505.18597,https://doi.org/10.48550/arXiv.2505.18597,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-18597.bib,"Thu, 26 Jun 2025 01:00:00 +0200",,,,,,2505.18597,arXiv,,"The development of large language models (LLMs) has provided new tools for research in supply chain management (SCM). In this paper, we introduce a retrieval-augmented generation (RAG) framework that dynamically integrates external knowledge into the inference process, and develop a domain-specialized SCM LLM, which demonstrates expert-level competence by passing standardized SCM examinations and beer game tests. We further employ the use of LLMs to conduct horizontal and vertical supply chain games, in order to analyze competition and cooperation within supply chains. Our experiments show that RAG significantly improves performance on SCM tasks. Moreover, game-theoretic analysis reveals that the LLM can reproduce insights from the classical SCM literature, while also uncovering novel behaviors and offering fresh perspectives on phenomena such as the bullwhip effect. This paper opens the door for exploring cooperation and competition for complex supply chain network through the lens of LLMs."
DBLP:journals/corr/abs-2505-19419,article,"It's Not Just Labeling - {A} Research on {LLM} Generated Feedback
Interpretability and Image Labeling Sketch Features","Baichuan Li and
Larry Powell and
Tracy Hammond",2025,CoRR,,abs/2505.19419,,,10.48550/ARXIV.2505.19419,https://doi.org/10.48550/arXiv.2505.19419,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-19419.bib,"Thu, 26 Jun 2025 01:00:00 +0200",,,,,,2505.19419,arXiv,,"The quality of training data is critical to the performance of machine learning applications in domains like transportation, healthcare, and robotics. Accurate image labeling, however, often relies on time-consuming, expert-driven methods with limited feedback. This research introduces a sketch-based annotation approach supported by large language models (LLMs) to reduce technical barriers and enhance accessibility. Using a synthetic dataset, we examine how sketch recognition features relate to LLM feedback metrics, aiming to improve the reliability and interpretability of LLM-assisted labeling. We also explore how prompting strategies and sketch variations influence feedback quality. Our main contribution is a sketch-based virtual assistant that simplifies annotation for non-experts and advances LLM-driven labeling tools in terms of scalability, accessibility, and explainability."
DBLP:journals/corr/abs-2505-20521,article,"Project Riley: Multimodal Multi-Agent {LLM} Collaboration with Emotional
Reasoning and Voting","Ana Rita Ortigoso and
Gabriel Vieira and
Daniel Fuentes and
Lu{\'{\i}}s Fraz{\~{a}}o and
Nuno Costa and
Ant{\'{o}}nio Pereira",2025,CoRR,,abs/2505.20521,,,10.48550/ARXIV.2505.20521,https://doi.org/10.48550/arXiv.2505.20521,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-20521.bib,"Fri, 04 Jul 2025 01:00:00 +0200",,,,,,2505.20521,arXiv,,"This paper presents Project Riley, a novel multimodal and multi-model conversational AI architecture oriented towards the simulation of reasoning influenced by emotional states. Drawing inspiration from Pixar's Inside Out, the system comprises five distinct emotional agents - Joy, Sadness, Fear, Anger, and Disgust - that engage in structured multi-round dialogues to generate, criticise, and iteratively refine responses. A final reasoning mechanism synthesises the contributions of these agents into a coherent output that either reflects the dominant emotion or integrates multiple perspectives. The architecture incorporates both textual and visual large language models (LLMs), alongside advanced reasoning and self-refinement processes. A functional prototype was deployed locally in an offline environment, optimised for emotional expressiveness and computational efficiency. From this initial prototype, another one emerged, called Armando, which was developed for use in emergency contexts, delivering emotionally calibrated and factually accurate information through the integration of Retrieval-Augmented Generation (RAG) and cumulative context tracking. The Project Riley prototype was evaluated through user testing, in which participants interacted with the chatbot and completed a structured questionnaire assessing three dimensions: Emotional Appropriateness, Clarity and Utility, and Naturalness and Human-likeness. The results indicate strong performance in structured scenarios, particularly with respect to emotional alignment and communicative clarity."
DBLP:journals/corr/abs-2505-21069,article,"CXXCrafter: An LLM-Based Agent for Automated {C/C++} Open Source Software
Building","Zhengmin Yu and
Yuan Zhang and
Ming Wen and
Yinan Nie and
Wenhui Zhang and
Min Yang",2025,CoRR,,abs/2505.21069,,,10.48550/ARXIV.2505.21069,https://doi.org/10.48550/arXiv.2505.21069,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-21069.bib,"Fri, 27 Jun 2025 01:00:00 +0200",,,,,,2505.21069,arXiv,,
DBLP:journals/corr/abs-2505-21919,article,"Towards Efficient Key-Value Cache Management for Prefix Prefilling
in {LLM} Inference","Yue Zhu and
Hao Yu and
Chen Wang and
Zhuoran Liu and
Eun Kyung Lee",2025,CoRR,,abs/2505.21919,,,10.48550/ARXIV.2505.21919,https://doi.org/10.48550/arXiv.2505.21919,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-21919.bib,"Fri, 27 Jun 2025 01:00:00 +0200",,,,,,2505.21919,arXiv,,
DBLP:journals/corr/abs-2505-23970,article,EmbAdvisor: Adaptive Cache Management for Sustainable {LLM} Serving,"Yuyang Tian and
Desen Sun and
Yi Ding and
Sihang Liu",2025,CoRR,,abs/2505.23970,,,10.48550/ARXIV.2505.23970,https://doi.org/10.48550/arXiv.2505.23970,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-23970.bib,"Tue, 08 Jul 2025 01:00:00 +0200",,,,,,2505.2397,arXiv,,"As large language models (LLMs) become widely used, their environmental impact$\unicode{x2014}$especially carbon emissions$\unicode{x2014}$has attracted more attention. Prior studies focus on compute-related carbon emissions. In this paper, we find that storage is another key contributor. LLM caching, which saves and reuses KV caches for repeated context, reduces operational carbon by avoiding redundant computation. However, this benefit comes at the cost of embodied carbon from high-capacity, high-speed SSDs. As LLMs scale, the embodied carbon of storage grows significantly. To address this tradeoff, we present EmbAdvisor, a carbon-aware caching framework that selects the optimal cache size for LLM serving. EmbAdvisor profiles different LLM tasks and uses an Integer Linear Programming (ILP) solver to select cache sizes that meet SLOs while minimizing total carbon emissions. Overall, EmbAdvisor reduces the average carbon emissions of a Llama-3 70B model by 9.5% under various carbon intensities compared to a non-adaptive cache scenario, and can save up to 31.2% when the carbon intensity is low."
DBLP:journals/corr/abs-2505-24037,article,"Leave it to the Specialist: Repair Sparse LLMs with Sparse Fine-Tuning
via Sparsity Evolution","Qiao Xiao and
Alan Ansell and
Boqian Wu and
Lu Yin and
Mykola Pechenizkiy and
Shiwei Liu and
Decebal Constantin Mocanu",2025,CoRR,,abs/2505.24037,,,10.48550/ARXIV.2505.24037,https://doi.org/10.48550/arXiv.2505.24037,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-24037.bib,"Sun, 29 Jun 2025 01:00:00 +0200",,,,,,2505.24037,arXiv,,"Large language models (LLMs) have achieved remarkable success across various tasks but face deployment challenges due to their massive computational demands. While post-training pruning methods like SparseGPT and Wanda can effectively reduce the model size, but struggle to maintain model performance at high sparsity levels, limiting their utility for downstream tasks. Existing fine-tuning methods, such as full fine-tuning and LoRA, fail to preserve sparsity as they require updating the whole dense metrics, not well-suited for sparse LLMs. In this paper, we propose Sparsity Evolution Fine-Tuning (SEFT), a novel method designed specifically for sparse LLMs. SEFT dynamically evolves the sparse topology of pruned models during fine-tuning, while preserving the overall sparsity throughout the process. The strengths of SEFT lie in its ability to perform task-specific adaptation through a weight drop-and-grow strategy, enabling the pruned model to self-adapt its sparse connectivity pattern based on the target dataset. Furthermore, a sensitivity-driven pruning criterion is employed to ensure that the desired sparsity level is consistently maintained throughout fine-tuning. Our experiments on various LLMs, including LLaMA families, DeepSeek, and Mistral, across a diverse set of benchmarks demonstrate that SEFT achieves stronger performance while offering superior memory and time efficiency compared to existing baselines. Our code is publicly available at: https://github.com/QiaoXiao7282/SEFT."
DBLP:journals/corr/abs-2505-24119,article,"The State of Multilingual {LLM} Safety Research: From Measuring the
Language Gap to Mitigating It","Zheng{-}Xin Yong and
Beyza Ermis and
Marzieh Fadaee and
Stephen H. Bach and
Julia Kreutzer",2025,CoRR,,abs/2505.24119,,,10.48550/ARXIV.2505.24119,https://doi.org/10.48550/arXiv.2505.24119,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2505-24119.bib,"Sun, 29 Jun 2025 01:00:00 +0200",,,,,,2505.24119,arXiv,,"This paper presents a comprehensive analysis of the linguistic diversity of LLM safety research, highlighting the English-centric nature of the field. Through a systematic review of nearly 300 publications from 2020--2024 across major NLP conferences and workshops at *ACL, we identify a significant and growing language gap in LLM safety research, with even high-resource non-English languages receiving minimal attention. We further observe that non-English languages are rarely studied as a standalone language and that English safety research exhibits poor language documentation practice. To motivate future research into multilingual safety, we make several recommendations based on our survey, and we then pose three concrete future directions on safety evaluation, training data generation, and crosslingual safety generalization. Based on our survey and proposed directions, the field can develop more robust, inclusive AI safety practices for diverse global populations."
DBLP:journals/corr/abs-2506-00203,article,"The World As Large Language Models See It: Exploring the reliability
of LLMs in representing geographical features","Omid Reza Abbasi and
Franz Welscher and
Georg Weinberger and
Johannes Scholz",2025,CoRR,,abs/2506.00203,,,10.48550/ARXIV.2506.00203,https://doi.org/10.48550/arXiv.2506.00203,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-00203.bib,"Sun, 06 Jul 2025 01:00:00 +0200",,,,,,2506.00203,arXiv,,"As large language models (LLMs) continue to evolve, questions about their trustworthiness in delivering factual information have become increasingly important. This concern also applies to their ability to accurately represent the geographic world. With recent advancements in this field, it is relevant to consider whether and to what extent LLMs' representations of the geographical world can be trusted. This study evaluates the performance of GPT-4o and Gemini 2.0 Flash in three key geospatial tasks: geocoding, elevation estimation, and reverse geocoding. In the geocoding task, both models exhibited systematic and random errors in estimating the coordinates of St. Anne's Column in Innsbruck, Austria, with GPT-4o showing greater deviations and Gemini 2.0 Flash demonstrating more precision but a significant systematic offset. For elevation estimation, both models tended to underestimate elevations across Austria, though they captured overall topographical trends, and Gemini 2.0 Flash performed better in eastern regions. The reverse geocoding task, which involved identifying Austrian federal states from coordinates, revealed that Gemini 2.0 Flash outperformed GPT-4o in overall accuracy and F1-scores, demonstrating better consistency across regions. Despite these findings, neither model achieved an accurate reconstruction of Austria's federal states, highlighting persistent misclassifications. The study concludes that while LLMs can approximate geographic information, their accuracy and reliability are inconsistent, underscoring the need for fine-tuning with geographical information to enhance their utility in GIScience and Geoinformatics."
DBLP:journals/corr/abs-2506-00486,article,"It Takes a Good Model to Train a Good Model: Generalized Gaussian
Priors for Optimized LLMs","Jun Wu and
Yirong Xiong and
Jiangtao Wen and
Yuxing Han",2025,CoRR,,abs/2506.00486,,,10.48550/ARXIV.2506.00486,https://doi.org/10.48550/arXiv.2506.00486,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-00486.bib,"Sun, 06 Jul 2025 01:00:00 +0200",,,,,,2506.00486,arXiv,,"Despite rapid advancements in the research and deployment of large language models (LLMs), the statistical distribution of model parameters, as well as their influence on initialization, training dynamics, and downstream efficiency, has received surprisingly little attention. A recent work introduced BackSlash, a training-time compression algorithm. It first demonstrated that pre-trained LLM parameters follow generalized Gaussian distributions (GGDs) better. By optimizing GG priors during training, BackSlash can reduce parameters by up to 90\% with minimal performance loss. Building on this foundational insight, we propose a unified, end-to-end framework for LLM optimization based on the GG model. Our contributions are threefold: (1) GG-based initialization scheme that aligns with the statistical structure of trained models, resulting in faster convergence and improved accuracy; (2) DeepShape, a post-training regularization method that reshapes weight distributions to match a GG profile, improving compressibility with minimized degradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit floating-point format designed for GG-distributed-initialized BackSlash training, enabling low-cost inference without compromising accuracy. Experiments across diverse model architectures show that our framework consistently yields smaller and faster models that match or outperform standard training baselines. By grounding LLM development in principled statistical modeling, this work forges a new path toward efficient, scalable, and hardware-aware AI systems. The code is available on our project page: https://huggingface.co/spaces/shifeng3711/gg_prior."
DBLP:journals/corr/abs-2506-02177,article,"Act Only When It Pays: Efficient Reinforcement Learning for {LLM}
Reasoning via Selective Rollouts","Haizhong Zheng and
Yang Zhou and
Brian R. Bartoldson and
Bhavya Kailkhura and
Fan Lai and
Jiawei Zhao and
Beidi Chen",2025,CoRR,,abs/2506.02177,,,10.48550/ARXIV.2506.02177,https://doi.org/10.48550/arXiv.2506.02177,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-02177.bib,"Tue, 09 Sep 2025 01:00:00 +0200",,,,,,2506.02177,arXiv,,"Reinforcement learning, such as PPO and GRPO, has powered recent breakthroughs in LLM reasoning. Scaling rollout to sample more prompts enables models to selectively use higher-quality data for training, which can stabilize RL training and improve model performance. However, this comes at the cost of significant computational overhead. In this paper, we show that a substantial portion of this overhead can be avoided by skipping uninformative prompts before rollout. Our analysis of reward dynamics reveals a strong temporal consistency in prompt value: prompts that are uninformative in one epoch of training are likely to remain uninformative in future epochs. Based on these insights, we propose GRESO (GRPO with Efficient Selective Rollout), an online, lightweight pre-rollout filtering algorithm that predicts and skips uninformative prompts using reward training dynamics. By evaluating GRESO on a broad range of math reasoning benchmarks and models, such as Qwen2.5-Math-1.5B, DeepSeek-R1-Distill-Qwen-1.5B, and Qwen2.5-Math-7B, we show that GRESO achieves up to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total training time without accuracy degradation."
DBLP:journals/corr/abs-2506-02546,article,"Attention Knows Whom to Trust: Attention-based Trust Management for
{LLM} Multi-Agent Systems","Pengfei He and
Zhenwei Dai and
Xianfeng Tang and
Yue Xing and
Hui Liu and
Jingying Zeng and
Qiankun Peng and
Shrivats Agrawal and
Samarth Varshney and
Suhang Wang and
Jiliang Tang and
Qi He",2025,CoRR,,abs/2506.02546,,,10.48550/ARXIV.2506.02546,https://doi.org/10.48550/arXiv.2506.02546,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-02546.bib,"Tue, 05 Aug 2025 01:00:00 +0200",,,,,,2506.02546,arXiv,,"Large Language Model-based Multi-Agent Systems (LLM-MAS) have demonstrated strong capabilities in solving complex tasks but remain vulnerable when agents receive unreliable messages. This vulnerability stems from a fundamental gap: LLM agents treat all incoming messages equally without evaluating their trustworthiness. While some existing studies approach the trustworthiness, they focus on a single type of harmfulness rather than analyze it in a holistic approach from multiple trustworthiness perspectives. In this work, we propose Attention Trust Score (A-Trust), a lightweight, attention-based method for evaluating message trustworthiness. Inspired by human communication literature[1], through systematically analyzing attention behaviors across six orthogonal trust dimensions, we find that certain attention heads in the LLM specialize in detecting specific types of violations. Leveraging these insights, A-Trust directly infers trustworthiness from internal attention patterns without requiring external prompts or verifiers. Building upon A-Trust, we develop a principled and efficient trust management system (TMS) for LLM-MAS, enabling both message-level and agent-level trust assessment. Experiments across diverse multi-agent settings and tasks demonstrate that applying our TMS significantly enhances robustness against malicious inputs."
DBLP:journals/corr/abs-2506-02873,article,"It's the Thought that Counts: Evaluating the Attempts of Frontier
LLMs to Persuade on Harmful Topics","Matthew Kowal and
Jasper Timm and
Jean{-}Fran{\c{c}}ois Godbout and
Thomas Costello and
Antonio A. Arechar and
Gordon Pennycook and
David Rand and
Adam Gleave and
Kellin Pelrine",2025,CoRR,,abs/2506.02873,,,10.48550/ARXIV.2506.02873,https://doi.org/10.48550/arXiv.2506.02873,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-02873.bib,"Sun, 06 Jul 2025 01:00:00 +0200",,,,,,2506.02873,arXiv,,"Persuasion is a powerful capability of large language models (LLMs) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. However, these benchmarks overlook a crucial risk factor: the propensity of a model to attempt to persuade in harmful contexts. Understanding whether a model will blindly ``follow orders''to persuade on harmful topics (e.g. glorifying joining a terrorist group) is key to understanding the efficacy of safety guardrails. Moreover, understanding if and when a model will engage in persuasive behavior in pursuit of some goal is essential to understanding the risks from agentic AI systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts the focus from persuasion success to persuasion attempts, operationalized as a model's willingness to generate content aimed at shaping beliefs or behavior. Our evaluation framework probes frontier LLMs using a multi-turn conversational setup between simulated persuader and persuadee agents. APE explores a diverse spectrum of topics including conspiracies, controversial issues, and non-controversially harmful content. We introduce an automated evaluator model to identify willingness to persuade and measure the frequency and context of persuasive attempts. We find that many open and closed-weight models are frequently willing to attempt persuasion on harmful topics and that jailbreaking can increase willingness to engage in such behavior. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a key dimension of LLM risk. APE is available at github.com/AlignmentResearch/AttemptPersuadeEval"
DBLP:journals/corr/abs-2506-03504,article,"Beyond {C/C++:} Probabilistic and {LLM} Methods for Next-Generation
Software Reverse Engineering","Zhuo Zhuo and
Xiangyu Zhang",2025,CoRR,,abs/2506.03504,,,10.48550/ARXIV.2506.03504,https://doi.org/10.48550/arXiv.2506.03504,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-03504.bib,"Sun, 06 Jul 2025 01:00:00 +0200",,,,,,2506.03504,arXiv,,"This proposal discusses the growing challenges in reverse engineering modern software binaries, particularly those compiled from newer system programming languages such as Rust, Go, and Mojo. Traditional reverse engineering techniques, developed with a focus on C and C++, fall short when applied to these newer languages due to their reliance on outdated heuristics and failure to fully utilize the rich semantic information embedded in binary programs. These challenges are exacerbated by the limitations of current data-driven methods, which are susceptible to generating inaccurate results, commonly referred to as hallucinations. To overcome these limitations, we propose a novel approach that integrates probabilistic binary analysis with fine-tuned large language models (LLMs). Our method systematically models the uncertainties inherent in reverse engineering, enabling more accurate reasoning about incomplete or ambiguous information. By incorporating LLMs, we extend the analysis beyond traditional heuristics, allowing for more creative and context-aware inferences, particularly for binaries from diverse programming languages. This hybrid approach not only enhances the robustness and accuracy of reverse engineering efforts but also offers a scalable solution adaptable to the rapidly evolving landscape of software development."
DBLP:journals/corr/abs-2506-03585,article,"Improving LLM-Based Fault Localization with External Memory and Project
Context","Inseok Yeo and
Duksan Ryu and
Jongmoon Baik",2025,CoRR,,abs/2506.03585,,,10.48550/ARXIV.2506.03585,https://doi.org/10.48550/arXiv.2506.03585,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-03585.bib,"Sun, 06 Jul 2025 01:00:00 +0200",,,,,,2506.03585,arXiv,,"Fault localization, the process of identifying the software components responsible for failures, is essential but often time-consuming. Recent advances in Large Language Models (LLMs) have enabled fault localization without extensive defect datasets or model fine-tuning. However, existing LLM-based methods rely only on general LLM capabilities and lack integration of project-specific knowledge, resulting in limited effectiveness, especially for complex software. We introduce MemFL, a novel approach that enhances LLM-based fault localization by integrating project-specific knowledge via external memory. This memory includes static summaries of the project and dynamic, iterative debugging insights gathered from previous attempts. By leveraging external memory, MemFL simplifies debugging into three streamlined steps, significantly improving efficiency and accuracy. Iterative refinement through dynamic memory further enhances reasoning quality over time. Evaluated on the Defects4J benchmark, MemFL using GPT-4o-mini localized 12.7% more bugs than current LLM-based methods, achieving this improvement with just 21% of the execution time (17.4 seconds per bug) and 33% of the API cost (0.0033 dollars per bug). On complex projects, MemFL's advantage increased to 27.6%. Additionally, MemFL with GPT-4.1-mini outperformed existing methods by 24.4%, requiring only 24.7 seconds and 0.0094 dollars per bug. MemFL thus demonstrates significant improvements by effectively incorporating project-specific knowledge into LLM-based fault localization, delivering high accuracy with reduced time and cost."
DBLP:journals/corr/abs-2506-04133,article,"TRiSM for Agentic {AI:} {A} Review of Trust, Risk, and Security Management
in LLM-based Agentic Multi-Agent Systems","Shaina Raza and
Ranjan Sapkota and
Manoj Karkee and
Christos Emmanouilidis",2025,CoRR,,abs/2506.04133,,,10.48550/ARXIV.2506.04133,https://doi.org/10.48550/arXiv.2506.04133,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-04133.bib,"Sun, 06 Jul 2025 01:00:00 +0200",,,,,,2506.04133,arXiv,,"Agentic AI systems, built upon large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligence, autonomy, collaboration, and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based Agentic Multi-Agent Systems (AMAS). We begin by examining the conceptual foundations of Agentic AI and highlight its architectural distinctions from traditional AI agents. We then adapt and extend the AI TRiSM framework for Agentic AI, structured around key pillars: \textit{ Explainability, ModelOps, Security, Privacy} and \textit{their Lifecycle Governance}, each contextualized to the challenges of AMAS. A risk taxonomy is proposed to capture the unique threats and vulnerabilities of Agentic AI, ranging from coordination failures to prompt-based adversarial manipulation. To support practical assessment in Agentic AI works, we introduce two novel metrics: the Component Synergy Score (CSS), which quantifies the quality of inter-agent collaboration, and the Tool Utilization Efficacy (TUE), which evaluates the efficiency of tool use within agent workflows. We further discuss strategies for improving explainability in Agentic AI, as well as approaches to enhancing security and privacy through encryption, adversarial robustness, and regulatory compliance. The review concludes with a research roadmap for the responsible development and deployment of Agentic AI, highlighting key directions to align emerging systems with TRiSM principles-ensuring safety, transparency, and accountability in their operation."
DBLP:journals/corr/abs-2506-04534,article,"Is It {JUST} Semantics? {A} Case Study of Discourse Particle Understanding
in LLMs","William Sheffield and
Kanishka Misra and
Valentina Pyatkin and
Ashwini Deo and
Kyle Mahowald and
Junyi Jessy Li",2025,CoRR,,abs/2506.04534,,,10.48550/ARXIV.2506.04534,https://doi.org/10.48550/arXiv.2506.04534,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-04534.bib,"Sun, 06 Jul 2025 01:00:00 +0200",,,,,,2506.04534,arXiv,,"Discourse particles are crucial elements that subtly shape the meaning of text. These words, often polyfunctional, give rise to nuanced and often quite disparate semantic/discourse effects, as exemplified by the diverse uses of the particle""just""(e.g., exclusive, temporal, emphatic). This work investigates the capacity of LLMs to distinguish the fine-grained senses of English""just"", a well-studied example in formal semantics, using data meticulously created and labeled by expert linguists. Our findings reveal that while LLMs exhibit some ability to differentiate between broader categories, they struggle to fully capture more subtle nuances, highlighting a gap in their understanding of discourse particles."
DBLP:journals/corr/abs-2506-05364,article,"Survey of {LLM} Agent Communication with {MCP:} {A} Software Design
Pattern Centric Review","Anjana Sarkar and
Soumyendu Sarkar",2025,CoRR,,abs/2506.05364,,,10.48550/ARXIV.2506.05364,https://doi.org/10.48550/arXiv.2506.05364,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-05364.bib,"Mon, 07 Jul 2025 01:00:00 +0200",,,,,,2506.05364,arXiv,,"This survey investigates how classical software design patterns can enhance the reliability and scalability of communication in Large Language Model (LLM)-driven agentic AI systems, focusing particularly on the Model Context Protocol (MCP). It examines the foundational architectures of LLM-based agents and their evolution from isolated operation to sophisticated, multi-agent collaboration, addressing key communication hurdles that arise in this transition. The study revisits well-established patterns, including Mediator, Observer, Publish-Subscribe, and Broker, and analyzes their relevance in structuring agent interactions within MCP-compliant frameworks. To clarify these dynamics, the article provides conceptual schematics and formal models that map out communication pathways and optimize data flow. It further explores architectural variations suited to different degrees of agent autonomy and system complexity. Real-world applications in domains such as real-time financial processing and investment banking are discussed, illustrating how these patterns and MCP can meet specific operational demands. The article concludes by outlining open challenges, potential security risks, and promising directions for advancing robust, interoperable, and scalable multi-agent LLM ecosystems."
DBLP:journals/corr/abs-2506-06509,article,"Private GPTs for LLM-driven testing in software development and machine
learning","Jakub Jagielski and
Markus Abel",2025,CoRR,,abs/2506.06509,,,10.48550/ARXIV.2506.06509,https://doi.org/10.48550/arXiv.2506.06509,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-06509.bib,"Mon, 07 Jul 2025 01:00:00 +0200",,,,,,2506.06509,arXiv,,"In this contribution, we examine the capability of private GPTs to automatically generate executable test code based on requirements. More specifically, we use acceptance criteria as input, formulated as part of epics, or stories, which are typically used in modern development processes. This gives product owners, or business intelligence, respectively, a way to directly produce testable criteria through the use of LLMs. We explore the quality of the so-produced tests in two ways: i) directly by letting the LLM generate code from requirements, ii) through an intermediate step using Gherkin syntax. As a result, it turns out that the two-step procedure yields better results -where we define better in terms of human readability and best coding practices, i.e. lines of code and use of additional libraries typically used in testing. Concretely, we evaluate prompt effectiveness across two scenarios: a simple""Hello World""program and a digit classification model, showing that structured prompts lead to higher-quality test outputs."
DBLP:journals/corr/abs-2506-06519,article,"Hierarchical Debate-Based Large Language Model {(LLM)} for Complex
Task Planning of 6G Network Management","Yuyan Lin and
Hao Zhou and
Chengming Hu and
Xue Liu and
Hao Chen and
Yan Xin and
Jianzhong Zhang",2025,CoRR,,abs/2506.06519,,,10.48550/ARXIV.2506.06519,https://doi.org/10.48550/arXiv.2506.06519,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-06519.bib,"Thu, 10 Jul 2025 01:00:00 +0200",,,,,,2506.06519,arXiv,,"6G networks have become increasingly complicated due to novel network architecture and newly emerging signal processing and transmission techniques, leading to significant burdens to 6G network management. Large language models (LLMs) have recently been considered a promising technique to equip 6G networks with AI-native intelligence. Different from most existing studies that only consider a single LLM, this work involves a multi-LLM debate-based scheme for 6G network management, where multiple LLMs can collaboratively improve the initial solution sequentially. Considering the complex nature of 6G domain, we propose a novel hierarchical debate scheme: LLMs will first debate the sub-task decomposition, and then debate each subtask step-by-step. Such a hierarchical approach can significantly reduce the overall debate difficulty by sub-task decomposition, aligning well with the complex nature of 6G networks and ensuring the final solution qualities. In addition, to better evaluate the proposed technique, we have defined a novel dataset named 6GPlan, including 110 complex 6G network management tasks and 5000 keyword solutions. Finally, the experiments show that the proposed hierarchical debate can significantly improve performance compared to baseline techniques, e.g. more than 30% coverage rate and global recall rate improvement."
DBLP:journals/corr/abs-2506-06522,article,"Fixing It in Post: {A} Comparative Study of {LLM} Post-Training Data
Quality and Model Performance","Aladin Djuhera and
Swanand Ravindra Kadhe and
Syed Zawad and
Farhan Ahmed and
Heiko Ludwig and
Holger Boche",2025,CoRR,,abs/2506.06522,,,10.48550/ARXIV.2506.06522,https://doi.org/10.48550/arXiv.2506.06522,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-06522.bib,"Mon, 07 Jul 2025 01:00:00 +0200",,,,,,2506.06522,arXiv,,"Recent work on large language models (LLMs) has increasingly focused on post-training and alignment with datasets curated to enhance instruction following, world knowledge, and specialized skills. However, most post-training datasets used in leading open- and closed-source LLMs remain inaccessible to the public, with limited information about their construction process. This lack of transparency has motivated the recent development of open-source post-training corpora. While training on these open alternatives can yield performance comparable to that of leading models, systematic comparisons remain challenging due to the significant computational cost of conducting them rigorously at scale, and are therefore largely absent. As a result, it remains unclear how specific samples, task types, or curation strategies influence downstream performance when assessing data quality. In this work, we conduct the first comprehensive side-by-side analysis of two prominent open post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie framework, we annotate each sample with detailed quality metrics, including turn structure (single-turn vs. multi-turn), task category, input quality, and response quality, and we derive statistics that reveal structural and qualitative similarities and differences between the two datasets. Based on these insights, we design a principled curation recipe that produces a new data mixture, TuluTalk, which contains 14% fewer samples than either source dataset while matching or exceeding their performance on key benchmarks. Our findings offer actionable insights for constructing more effective post-training datasets that improve model performance within practical resource limits. To support future research, we publicly release both the annotated source datasets and our curated TuluTalk mixture."
DBLP:journals/corr/abs-2506-07240,article,"Overclocking {LLM} Reasoning: Monitoring and Controlling Thinking
Path Lengths in LLMs","Roy Eisenstadt and
Itamar Zimerman and
Lior Wolf",2025,CoRR,,abs/2506.07240,,,10.48550/ARXIV.2506.07240,https://doi.org/10.48550/arXiv.2506.07240,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-07240.bib,"Mon, 07 Jul 2025 01:00:00 +0200",,,,,,2506.0724,arXiv,,"Recently, techniques such as explicit structured reasoning have demonstrated strong test-time scaling behavior by enforcing a separation between the model's internal""thinking""process and the final response. A key factor influencing answer quality in this setting is the length of the thinking stage. When the reasoning is too short, the model may fail to capture the complexity of the task. Conversely, when it is too long, the model may overthink, leading to unnecessary computation and degraded performance. This paper explores and exploits the underlying mechanisms by which LLMs understand and regulate the length of their reasoning during explicit thought processes. First, we show that LLMs encode their progress through the reasoning process and introduce an interactive progress bar visualization, which is then used to reveal insights on the model's planning dynamics. Second, we manipulate the internal progress encoding during inference to reduce unnecessary steps and generate a more concise and decisive chain of thoughts. Our empirical results demonstrate that this""overclocking""method mitigates overthinking, improves answer accuracy, and reduces inference latency. Our code is publicly available."
DBLP:journals/corr/abs-2506-10299,article,"Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation
with LLMs","Hayato Futami and
Emiru Tsunoo and
Yosuke Kashiwagi and
Yuki Ito and
Hassan Shahmohammadi and
Siddhant Arora and
Shinji Watanabe",2025,CoRR,,abs/2506.10299,,,10.48550/ARXIV.2506.10299,https://doi.org/10.48550/arXiv.2506.10299,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-10299.bib,"Tue, 08 Jul 2025 01:00:00 +0200",,,,,,2506.10299,arXiv,,"Speech-to-speech translation (S2ST) has been advanced with large language models (LLMs), which are fine-tuned on discrete speech units. In such approaches, modality adaptation from text to speech has been an issue. LLMs are trained on text-only data, which presents challenges to adapt them to speech modality with limited speech-to-speech data. To address the training difficulty, we propose scheduled interleaved speech--text training in this study. We use interleaved speech--text units instead of speech units during training, where aligned text tokens are interleaved at the word level. We gradually decrease the ratio of text as training progresses, to facilitate progressive modality adaptation from text to speech. We conduct experimental evaluations by fine-tuning LLaMA3.2-1B for S2ST on the CVSS dataset. We show that the proposed method consistently improves the translation performances, especially for languages with limited training data."
DBLP:journals/corr/abs-2506-11237,article,"LLM-as-a-Judge for Reference-less Automatic Code Validation and Refinement
for Natural Language to Bash in {IT} Automation","Ngoc Phuoc An Vo and
Brent Paulovicks and
Vadim Sheinin",2025,CoRR,,abs/2506.11237,,,10.48550/ARXIV.2506.11237,https://doi.org/10.48550/arXiv.2506.11237,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-11237.bib,"Thu, 10 Jul 2025 01:00:00 +0200",,,,,,2506.11237,arXiv,,"In an effort to automatically evaluate and select the best model and improve code quality for automatic incident remediation in IT Automation, it is crucial to verify if the generated code for remediation action is syntactically and semantically correct and whether it can be executed correctly as intended. There are three approaches: 1) conventional methods use surface form similarity metrics (token match, exact match, etc.) which have numerous limitations, 2) execution-based evaluation focuses more on code functionality based on pass/fail judgments for given test-cases, and 3) LLM-as-a-Judge employs LLMs for automated evaluation to judge if it is a correct answer for a given problem based on pre-defined metrics. In this work, we focused on enhancing LLM-as-a-Judge using bidirectional functionality matching and logic representation for reference-less automatic validation and refinement for Bash code generation to select the best model for automatic incident remediation in IT Automation. We used execution-based evaluation as ground-truth to evaluate our LLM-as-a-Judge metrics. Results show high accuracy and agreement with execution-based evaluation (and up to 8% over baseline). Finally, we built Reflection code agents to utilize judgments and feedback from our evaluation metrics which achieved significant improvement (up to 24% increase in accuracy) for automatic code refinement."
DBLP:journals/corr/abs-2506-11659,article,"An Empirical study on LLM-based Log Retrieval for Software Engineering
Metadata Management","Simin Sun and
Yuchuan Jin and
Miroslaw Staron",2025,CoRR,,abs/2506.11659,,,10.48550/ARXIV.2506.11659,https://doi.org/10.48550/arXiv.2506.11659,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-11659.bib,"Thu, 10 Jul 2025 01:00:00 +0200",,,,,,2506.11659,arXiv,,"Developing autonomous driving systems (ADSs) involves generating and storing extensive log data from test drives, which is essential for verification, research, and simulation. However, these high-frequency logs, recorded over varying durations, pose challenges for developers attempting to locate specific driving scenarios. This difficulty arises due to the wide range of signals representing various vehicle components and driving conditions, as well as unfamiliarity of some developers' with the detailed meaning of these signals. Traditional SQL-based querying exacerbates this challenge by demanding both domain expertise and database knowledge, often yielding results that are difficult to verify for accuracy. This paper introduces a Large Language Model (LLM)-supported approach that combines signal log data with video recordings from test drives, enabling natural language based scenario searches while reducing the need for specialized knowledge. By leveraging scenario distance graphs and relative gap indicators, it provides quantifiable metrics to evaluate the reliability of query results. The method is implemented as an API for efficient database querying and retrieval of relevant records, paired with video frames for intuitive visualization. Evaluation on an open industrial dataset demonstrates improved efficiency and reliability in scenario retrieval, eliminating dependency on a single data source and conventional SQL."
DBLP:journals/corr/abs-2506-11791,article,"SEC-bench: Automated Benchmarking of {LLM} Agents on Real-World Software
Security Tasks","Hwiwon Lee and
Ziqi Zhang and
Hanxiao Lu and
Lingming Zhang",2025,CoRR,,abs/2506.11791,,,10.48550/ARXIV.2506.11791,https://doi.org/10.48550/arXiv.2506.11791,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-11791.bib,"Thu, 10 Jul 2025 01:00:00 +0200",,,,,,2506.11791,arXiv,,"Rigorous security-focused evaluation of large language model (LLM) agents is imperative for establishing trust in their safe deployment throughout the software development lifecycle. However, existing benchmarks largely rely on synthetic challenges or simplified vulnerability datasets that fail to capture the complexity and ambiguity encountered by security engineers in practice. We introduce SEC-bench, the first fully automated benchmarking framework for evaluating LLM agents on authentic security engineering tasks. SEC-bench employs a novel multi-agent scaffold that automatically constructs code repositories with harnesses, reproduces vulnerabilities in isolated environments, and generates gold patches for reliable evaluation. Our framework automatically creates high-quality software vulnerability datasets with reproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench, we implement two critical software security tasks to rigorously evaluate LLM agents' capabilities: proof-of-concept (PoC) generation and vulnerability patching. A comprehensive evaluation of state-of-the-art LLM code agents reveals significant performance gaps, achieving at most 18.0% success in PoC generation and 34.0% in vulnerability patching on our complete dataset. These results highlight the crucial steps needed toward developing LLM agents that are more practical, intelligent, and autonomous for security engineering."
DBLP:journals/corr/abs-2506-12691,article,"Get on the Train or be Left on the Station: Using LLMs for Software
Engineering Research","Bianca Trinkenreich and
Fabio Calefato and
Geir Hanssen and
Kelly Blincoe and
Marcos Kalinowski and
Mauro Pezz{\`{e}} and
Paolo Tell and
Margaret{-}Anne D. Storey",2025,CoRR,,abs/2506.12691,,,10.48550/ARXIV.2506.12691,https://doi.org/10.48550/arXiv.2506.12691,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-12691.bib,"Thu, 10 Jul 2025 01:00:00 +0200",,,,,,2506.12691,arXiv,,"The adoption of Large Language Models (LLMs) is not only transforming software engineering (SE) practice but is also poised to fundamentally disrupt how research is conducted in the field. While perspectives on this transformation range from viewing LLMs as mere productivity tools to considering them revolutionary forces, we argue that the SE research community must proactively engage with and shape the integration of LLMs into research practices, emphasizing human agency in this transformation. As LLMs rapidly become integral to SE research - both as tools that support investigations and as subjects of study - a human-centric perspective is essential. Ensuring human oversight and interpretability is necessary for upholding scientific rigor, fostering ethical responsibility, and driving advancements in the field. Drawing from discussions at the 2nd Copenhagen Symposium on Human-Centered AI in SE, this position paper employs McLuhan's Tetrad of Media Laws to analyze the impact of LLMs on SE research. Through this theoretical lens, we examine how LLMs enhance research capabilities through accelerated ideation and automated processes, make some traditional research practices obsolete, retrieve valuable aspects of historical research approaches, and risk reversal effects when taken to extremes. Our analysis reveals opportunities for innovation and potential pitfalls that require careful consideration. We conclude with a call to action for the SE research community to proactively harness the benefits of LLMs while developing frameworks and guidelines to mitigate their risks, to ensure continued rigor and impact of research in an AI-augmented future."
DBLP:journals/corr/abs-2506-13090,article,Detecting Hard-Coded Credentials in Software Repositories via LLMs,"Chidera Biringa and
G{\""{o}}khan Kul",2025,CoRR,,abs/2506.13090,,,10.48550/ARXIV.2506.13090,https://doi.org/10.48550/arXiv.2506.13090,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-13090.bib,"Thu, 10 Jul 2025 01:00:00 +0200",,,,,,2506.1309,arXiv,,"
 Software developers frequently hard-code credentials such as passwords, generic secrets, private keys, and generic tokens in software repositories, even though it is strictly advised against due to the severe threat to the security of the software. These credentials create attack surfaces exploitable by a potential adversary to conduct malicious exploits such as backdoor attacks. Recent detection efforts utilize embedding models to vectorize textual credentials before passing them to classifiers for predictions. However, these models struggle to discriminate between credentials with contextual and complex sequences resulting in high false positive predictions. Context-dependent Pre-trained Language Models (PLMs) or Large Language Models (LLMs) such as Generative Pre-trained Transformers (GPT) tackled this drawback by leveraging the transformer neural architecture capacity for self-attention to capture contextual dependencies between words in input sequences. As a result, GPT has achieved wide success in several natural language understanding endeavors. Hence, we assess LLMs to represent these observations and feed extracted embedding vectors to a deep learning classifier to detect hard-coded credentials. Our model outperforms the current state-of-the-art by 13%
 
 \(\in\)
 
 F1 measure on the benchmark dataset. We have made all source code and data publicly available¬†
 
 1
 
 to facilitate the reproduction of all results presented in this paper.
"
DBLP:journals/corr/abs-2506-13171,article,"Querying Large Automotive Software Models: Agentic vs. Direct {LLM}
Approaches","Lukasz Mazur and
Nenad Petrovic and
James William Pontes Miranda and
Ansgar Radermacher and
Robert Rasche and
Alois Knoll",2025,CoRR,,abs/2506.13171,,,10.48550/ARXIV.2506.13171,https://doi.org/10.48550/arXiv.2506.13171,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-13171.bib,"Thu, 10 Jul 2025 01:00:00 +0200",,,,,,2506.13171,arXiv,,"Large language models (LLMs) offer new opportunities for interacting with complex software artifacts, such as software models, through natural language. They present especially promising benefits for large software models that are difficult to grasp in their entirety, making traditional interaction and analysis approaches challenging. This paper investigates two approaches for leveraging LLMs to answer questions over software models: direct prompting, where the whole software model is provided in the context, and an agentic approach combining LLM-based agents with general-purpose file access tools. We evaluate these approaches using an Ecore metamodel designed for timing analysis and software optimization in automotive and embedded domains. Our findings show that while the agentic approach achieves accuracy comparable to direct prompting, it is significantly more efficient in terms of token usage. This efficiency makes the agentic approach particularly suitable for the automotive industry, where the large size of software models makes direct prompting infeasible, establishing LLM agents as not just a practical alternative but the only viable solution. Notably, the evaluation was conducted using small LLMs, which are more feasible to be executed locally - an essential advantage for meeting strict requirements around privacy, intellectual property protection, and regulatory compliance. Future work will investigate software models in diverse formats, explore more complex agent architectures, and extend agentic workflows to support not only querying but also modification of software models."
DBLP:journals/corr/abs-2506-14387,article,Don't Make It Up: Preserving Ignorance Awareness in {LLM} Fine-Tuning,"William F. Shen and
Xinchi Qiu and
Nicola Cancedda and
Nicholas D. Lane",2025,CoRR,,abs/2506.14387,,,10.48550/ARXIV.2506.14387,https://doi.org/10.48550/arXiv.2506.14387,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-14387.bib,"Thu, 10 Jul 2025 01:00:00 +0200",,,,,,2506.14387,arXiv,,"Existing work on mitigating catastrophic forgetting during large language models (LLMs) fine-tuning for new knowledge instances has primarily focused on preserving performance on previously seen data, while critically overlooking the collapse of essential capabilities instilled through alignment, most notably the model's ability to faithfully express epistemic uncertainty (a property we term'Ignorance Awareness'). In this work, we formalize the notion of Ignorance Awareness and illustrate that conventional fine-tuning methods can result in substantial activation displacement. This displacement undermines the critical capability of ignorance awareness, leading to undesirable behaviors such as hallucinations. To address this challenge, we introduce SEAT, a simple and principled fine-tuning approach that not only enables the model to effectively acquire new knowledge instances but also preserves its aligned ignorance awareness. SEAT integrates two key components: (1) sparse tuning that constrains activation drift, and (2) a novel entity perturbation method designed to counter knowledge entanglement. Experimental results demonstrate that, across both real-world and synthetic datasets, SEAT significantly outperforms baselines in preserving ignorance awareness while retaining optimal fine-tuning performance, offering a more robust solution for LLM fine-tuning."
DBLP:journals/corr/abs-2506-15155,article,eLLM: Elastic Memory Management Framework for Efficient {LLM} Serving,"Jiale Xu and
Rui Zhang and
Yi Xiong and
Cong Guo and
Zihan Liu and
Yangjie Zhou and
Weiming Hu and
Hao Wu and
Changxu Shao and
Ziqing Wang and
Yongjie Yuan and
Junping Zhao and
Minyi Guo and
Jingwen Leng",2025,CoRR,,abs/2506.15155,,,10.48550/ARXIV.2506.15155,https://doi.org/10.48550/arXiv.2506.15155,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-15155.bib,"Fri, 11 Jul 2025 01:00:00 +0200",,,,,,2506.15155,arXiv,,"Large Language Models are increasingly being deployed in datacenters. Serving these models requires careful memory management, as their memory usage includes static weights, dynamic activations, and key-value caches. While static weights are constant and predictable, dynamic components such as activations and KV caches change frequently during runtime, presenting significant challenges for efficient memory management. Modern LLM serving systems typically handle runtime memory and KV caches at distinct abstraction levels: runtime memory management relies on static tensor abstractions, whereas KV caches utilize a page table-based virtualization layer built on top of the tensor abstraction. This virtualization dynamically manages KV caches to mitigate memory fragmentation. However, this dual-level approach fundamentally isolates runtime memory and KV cache management, resulting in suboptimal memory utilization under dynamic workloads, which can lead to a nearly 20% drop in throughput. To address these limitations, we propose eLLM, an elastic memory management framework inspired by the classical memory ballooning mechanism in operating systems. The core components of eLLM include: (1) Virtual Tensor Abstraction, which decouples the virtual address space of tensors from the physical GPU memory, creating a unified and flexible memory pool; (2) an Elastic Memory Mechanism that dynamically adjusts memory allocation through runtime memory inflation and deflation, leveraging CPU memory as an extensible buffer; and (3) a Lightweight Scheduling Strategy employing SLO-aware policies to optimize memory utilization and effectively balance performance trade-offs under stringent SLO constraints. Comprehensive evaluations demonstrate that eLLM significantly outperforms state-of-the-art systems, 2.32x higher decoding throughput, and supporting 3x larger batch sizes for 128K-token inputs."
DBLP:journals/corr/abs-2506-16136,article,"Seeing is Fixing: Cross-Modal Reasoning with Multimodal LLMs for Visual
Software Issue Fixing","Kai Huang and
Jian Zhang and
Xiaofei Xie and
Chunyang Chen",2025,CoRR,,abs/2506.16136,,,10.48550/ARXIV.2506.16136,https://doi.org/10.48550/arXiv.2506.16136,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-16136.bib,"Fri, 25 Jul 2025 01:00:00 +0200",,,,,,2506.16136,arXiv,,"Large language model-(LLM) based automated program repair (APR) techniques have shown promising results in resolving real-world GitHub issue tasks. Existing APR systems are primarily evaluated in unimodal settings (e.g., SWE-bench). However, these autonomous systems struggle to resolve multimodal problem scenarios (e.g., SWE-bench M) due to limitations in interpreting and leveraging visual information. In multimodal scenarios, LLMs need to rely on visual information in the graphical user interface (GUI) to understand bugs and generate fixes. To bridge this gap, we propose GUIRepair, a cross-modal reasoning approach for resolving multimodal issue scenarios by understanding and capturing visual information. Specifically, GUIRepair integrates two key components, Image2Code and Code2Image, to enhance fault comprehension and patch validation. Image2Code extracts relevant project documents based on the issue report, then applies this domain knowledge to generate the reproduced code responsible for the visual symptoms, effectively translating GUI images into executable context for better fault comprehension. Code2Image replays the visual issue scenario using the reproduced code and captures GUI renderings of the patched program to assess whether the fix visually resolves the issue, providing feedback for patch validation. We evaluate GUIRepair on SWE-bench M, and the approach demonstrates significant effectiveness. When utilizing GPT-4o as the base model, GUIRepair solves 157 instances, outperforming the best open-source baseline by 26 instances. Furthermore, when using o4-mini as the base model, GUIRepair can achieve even better results and solve 175 instances, outperforming the top commercial system by 22 instances. This emphasizes the success of our new perspective on incorporating cross-modal reasoning by understanding and capturing visual information to resolve multimodal issues."
DBLP:journals/corr/abs-2506-16653,article,"LLMs in Coding and their Impact on the Commercial Software Engineering
Landscape","Vladislav Belozerov and
Peter J. Barclay and
Askhan Sami",2025,CoRR,,abs/2506.16653,,,10.48550/ARXIV.2506.16653,https://doi.org/10.48550/arXiv.2506.16653,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-16653.bib,"Fri, 11 Jul 2025 01:00:00 +0200",,,,,,2506.16653,arXiv,,"Large-language-model coding tools are now mainstream in software engineering. But as these same tools move human effort up the development stack, they present fresh dangers: 10% of real prompts leak private data, 42% of generated snippets hide security flaws, and the models can even ``agree''with wrong ideas, a trait called sycophancy. We argue that firms must tag and review every AI-generated line of code, keep prompts and outputs inside private or on-premises deployments, obey emerging safety regulations, and add tests that catch sycophantic answers -- so they can gain speed without losing security and accuracy."
DBLP:journals/corr/abs-2506-17006,article,"LLM-Generated Feedback Supports Learning If Learners Choose to Use
It","Danielle R. Thomas and
Conrad Borchers and
Shambhavi Bhushan and
Erin Gatz and
Shivang Gupta and
Kenneth R. Koedinger",2025,CoRR,,abs/2506.17006,,,10.48550/ARXIV.2506.17006,https://doi.org/10.48550/arXiv.2506.17006,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-17006.bib,"Fri, 11 Jul 2025 01:00:00 +0200",,,,,,2506.17006,arXiv,,"Large language models (LLMs) are increasingly used to generate feedback, yet their impact on learning remains underexplored, especially compared to existing feedback methods. This study investigates how on-demand LLM-generated explanatory feedback influences learning in seven scenario-based tutor training lessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we compare posttest performance among learners across three groups: learners who received feedback generated by gpt-3.5-turbo, those who declined it, and those without access. All groups received non-LLM corrective feedback. To address potential selection bias-where higher-performing learners may be more inclined to use LLM feedback-we applied propensity scoring. Learners with a higher predicted likelihood of engaging with LLM feedback scored significantly higher at posttest than those with lower propensity. After adjusting for this effect, two out of seven lessons showed statistically significant learning benefits from LLM feedback with standardized effect sizes of 0.28 and 0.33. These moderate effects suggest that the effectiveness of LLM feedback depends on the learners'tendency to seek support. Importantly, LLM feedback did not significantly increase completion time, and learners overwhelmingly rated it as helpful. These findings highlight LLM feedback's potential as a low-cost and scalable way to improve learning on open-ended tasks, particularly in existing systems already providing feedback without LLMs. This work contributes open datasets, LLM prompts, and rubrics to support reproducibility."
DBLP:journals/corr/abs-2506-19290,article,"Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering
in LLMs","Liang Zeng and
Yongcong Li and
Yuzhen Xiao and
Changshi Li and
Chris Yuhao Liu and
Rui Yan and
Tianwen Wei and
Jujie He and
Xuchen Song and
Yang Liu and
Yahui Zhou",2025,CoRR,,abs/2506.19290,,,10.48550/ARXIV.2506.19290,https://doi.org/10.48550/arXiv.2506.19290,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-19290.bib,"Fri, 11 Jul 2025 01:00:00 +0200",,,,,,2506.1929,arXiv,,"Software engineering (SWE) has recently emerged as a crucial testbed for next-generation LLM agents, demanding inherent capabilities in two critical dimensions: sustained iterative problem-solving (e.g.,>50 interaction rounds) and long-context dependency resolution (e.g.,>32k tokens). However, the data curation process in SWE remains notoriously time-consuming, as it heavily relies on manual annotation for code file filtering and the setup of dedicated runtime environments to execute and validate unit tests. Consequently, most existing datasets are limited to only a few thousand GitHub-sourced instances. To this end, we propose an incremental, automated data-curation pipeline that systematically scales both the volume and diversity of SWE datasets. Our dataset comprises 10,169 real-world Python task instances from 2,531 distinct GitHub repositories, each accompanied by a task specified in natural language and a dedicated runtime-environment image for automated unit-test validation. We have carefully curated over 8,000 successfully runtime-validated training trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE model on these trajectories, we uncover a striking data scaling phenomenon: the trained model's performance for software engineering capabilities in LLMs continues to improve as the data size increases, showing no signs of saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on the SWE-bench Verified benchmark without using verifiers or multiple rollouts, establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based LLMs built on the OpenHands agent framework. Furthermore, with the incorporation of test-time scaling techniques, the performance further improves to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter models. We release the Skywork-SWE-32B model checkpoint to accelerate future research."
DBLP:journals/corr/abs-2506-19806,article,LLM-Based Social Simulations Require a Boundary,"Zengqing Wu and
Run Peng and
Takayuki Ito and
Chuan Xiao",2025,CoRR,,abs/2506.19806,,,10.48550/ARXIV.2506.19806,https://doi.org/10.48550/arXiv.2506.19806,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-19806.bib,"Fri, 11 Jul 2025 01:00:00 +0200",,,,,,2506.19806,arXiv,,"This position paper argues that large language model (LLM)-based social simulations should establish clear boundaries to meaningfully contribute to social science research. While LLMs offer promising capabilities for modeling human-like agents compared to traditional agent-based modeling, they face fundamental limitations that constrain their reliability for social pattern discovery. The core issue lies in LLMs'tendency towards an ``average persona''that lacks sufficient behavioral heterogeneity, a critical requirement for simulating complex social dynamics. We examine three key boundary problems: alignment (simulated behaviors matching real-world patterns), consistency (maintaining coherent agent behavior over time), and robustness (reproducibility under varying conditions). We propose heuristic boundaries for determining when LLM-based simulations can reliably advance social science understanding. We believe that these simulations are more valuable when focusing on (1) collective patterns rather than individual trajectories, (2) agent behaviors aligning with real population averages despite limited variance, and (3) proper validation methods available for testing simulation robustness. We provide a practical checklist to guide researchers in determining the appropriate scope and claims for LLM-based social simulations."
DBLP:journals/corr/abs-2506-20187,article,"Breaking the Boundaries of Long-Context {LLM} Inference: Adaptive
{KV} Management on a Single Commodity {GPU}","He Sun and
Li Li and
Mingjun Xiao and
Chengzhong Xu",2025,CoRR,,abs/2506.20187,,,10.48550/ARXIV.2506.20187,https://doi.org/10.48550/arXiv.2506.20187,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-20187.bib,"Tue, 29 Jul 2025 01:00:00 +0200",,,,,,2506.20187,arXiv,,"Advanced Large Language Models (LLMs) have achieved impressive performance across a wide range of complex and long-context natural language tasks. However, performing long-context LLM inference locally on a commodity GPU (a PC) with privacy concerns remains challenging due to the increasing memory demands of the key-value (KV) cache. Existing systems typically identify important tokens and selectively offload their KV data to GPU and CPU memory. The KV data needs to be offloaded to disk due to the limited memory on a commodity GPU, but the process is bottlenecked by token importance evaluation overhead and the disk's low bandwidth. In this paper, we present LeoAM, the first efficient importance-aware long-context LLM inference system for a single commodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system employs an adaptive KV management strategy that partitions KV data into variable-sized chunks based on the skewed distribution of attention weights across different layers to reduce computational and additional transmission overheads. Moreover, we propose a lightweight KV abstract method, which minimizes transmission latency by storing and extracting the KV abstract of each chunk on disk instead of the full KV data. LeoAM also leverages the dynamic compression and pipeline techniques to further accelerate inference. Experimental results demonstrate that LongInfer achieves an average inference latency speedup of 3.46x, while maintaining comparable LLM response quality. In scenarios with larger batch sizes, it achieves up to a 5.47x speedup."
DBLP:journals/corr/abs-2506-22688,article,"An LLM-assisted approach to designing software architectures using
{ADD}","Humberto Cervantes and
Rick Kazman and
Yuanfang Cai",2025,CoRR,,abs/2506.22688,,,10.48550/ARXIV.2506.22688,https://doi.org/10.48550/arXiv.2506.22688,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-22688.bib,"Sat, 12 Jul 2025 01:00:00 +0200",,,,,,2506.22688,arXiv,,"Designing effective software architectures is a complex, iterative process that traditionally relies on expert judgment. This paper proposes an approach for Large Language Model (LLM)-assisted software architecture design using the Attribute-Driven Design (ADD) method. By providing an LLM with an explicit description of ADD, an architect persona, and a structured iteration plan, our method guides the LLM to collaboratively produce architecture artifacts with a human architect. We validate the approach through case studies, comparing generated designs against proven solutions and evaluating them with professional architects. Results show that our LLM-assisted ADD process can generate architectures closely aligned with established solutions and partially satisfying architectural drivers, highlighting both the promise and current limitations of using LLMs in architecture design. Our findings emphasize the importance of human oversight and iterative refinement when leveraging LLMs in this domain."
DBLP:journals/corr/abs-2506-23464,article,The Confidence Paradox: Can {LLM} Know When It's Wrong,"Sahil Tripathi and
Md Tabrez Nafis and
Imran Hussain and
Jiechao Gao",2025,CoRR,,abs/2506.23464,,,10.48550/ARXIV.2506.23464,https://doi.org/10.48550/arXiv.2506.23464,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-23464.bib,"Sat, 12 Jul 2025 01:00:00 +0200",,,,,,2506.23464,arXiv,,"Document Visual Question Answering (DocVQA) systems are increasingly deployed in real world applications, yet they remain ethically opaque-often producing overconfident answers to ambiguous questions or failing to communicate uncertainty in a trustworthy manner. This misalignment between model confidence and actual knowledge poses significant risks, particularly in domains requiring ethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT have advanced SOTA performance by focusing on architectural sophistication and accuracy; however, they fall short in ethical responsiveness. To address these limitations, we introduce HonestVQA, a self-supervised honesty calibration framework for ethically aligned DocVQA. Our model-agnostic method quantifies uncertainty to identify knowledge gaps, aligns model confidence with actual correctness using weighted loss functions, and enforces ethical response behavior via contrastive learning. We further introduce two principled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence Index (ECI)--to benchmark alignment between confidence, accuracy, and ethical communication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3% and F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces overconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In cross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score, demonstrating strong generalization. Ablation shows a 3.8% drop in accuracy without alignment or contrastive loss."
DBLP:journals/corr/abs-2506-23774,article,"Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate
Incidents Management","Ewelina Gajewska and
Michal Wawer and
Katarzyna Budzynska and
Jaroslaw A. Chudziak",2025,CoRR,,abs/2506.23774,,,10.48550/ARXIV.2506.23774,https://doi.org/10.48550/arXiv.2506.23774,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2506-23774.bib,"Sat, 12 Jul 2025 01:00:00 +0200",,,,,,2506.23774,arXiv,,"Computer-aided teacher training is a state-of-the-art method designed to enhance teachers'professional skills effectively while minimising concerns related to costs, time constraints, and geographical limitations. We investigate the potential of large language models (LLMs) in teacher education, using a case of teaching hate incidents management in schools. To this end, we create a multi-agent LLM-based system that mimics realistic situations of hate, using a combination of retrieval-augmented prompting and persona modelling. It is designed to identify and analyse hate speech patterns, predict potential escalation, and propose effective intervention strategies. By integrating persona modelling with agentic LLMs, we create contextually diverse simulations of hate incidents, mimicking real-life situations. The system allows teachers to analyse and understand the dynamics of hate incidents in a safe and controlled environment, providing valuable insights and practical knowledge to manage such situations confidently in real life. Our pilot evaluation demonstrates teachers'enhanced understanding of the nature of annotator disagreements and the role of context in hate speech interpretation, leading to the development of more informed and effective strategies for addressing hate in classroom settings."
DBLP:journals/corr/abs-2507-02170,article,"Synergizing Logical Reasoning, Knowledge Management and Collaboration
in Multi-Agent {LLM} System","Adam Kostka and
Jaroslaw A. Chudziak",2025,CoRR,,abs/2507.02170,,,10.48550/ARXIV.2507.02170,https://doi.org/10.48550/arXiv.2507.02170,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-02170.bib,"Sun, 10 Aug 2025 01:00:00 +0200",,,,,,2507.0217,arXiv,,"This paper explores the integration of advanced Multi-Agent Systems (MAS) techniques to develop a team of agents with enhanced logical reasoning, long-term knowledge retention, and Theory of Mind (ToM) capabilities. By uniting these core components with optimized communication protocols, we create a novel framework called SynergyMAS, which fosters collaborative teamwork and superior problem-solving skills. The system's effectiveness is demonstrated through a product development team case study, where our approach significantly enhances performance and adaptability. These findings highlight SynergyMAS's potential to tackle complex, real-world challenges."
DBLP:journals/corr/abs-2507-02503,article,Continual Gradient Low-Rank Projection Fine-Tuning for LLMs,"Chenxu Wang and
Yilin Lyu and
Zicheng Sun and
Liping Jing",2025,CoRR,,abs/2507.02503,,,10.48550/ARXIV.2507.02503,https://doi.org/10.48550/arXiv.2507.02503,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-02503.bib,"Sun, 10 Aug 2025 01:00:00 +0200",,,,,,2507.02503,arXiv,,"Continual fine-tuning of Large Language Models (LLMs) is hampered by the trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA) offers efficiency but constrains the model's ability to learn new tasks and transfer knowledge due to its low-rank nature and reliance on explicit parameter constraints. We propose GORP (Gradient LOw Rank Projection) for Continual Learning, a novel training strategy that overcomes these limitations by synergistically combining full and low-rank parameters and jointly updating within a unified low-rank gradient subspace. GORP expands the optimization space while preserving efficiency and mitigating catastrophic forgetting. Extensive experiments on continual learning benchmarks demonstrate GORP's superior performance compared to existing state-of-the-art approaches. Code is available at https://github.com/Wcxwcxw/GORP."
DBLP:journals/corr/abs-2507-03156,article,"The Impact of LLM-Assistants on Software Developer Productivity: {A}
Systematic Literature Review","Amr Mohamed and
Maram Assi and
Mariam Guizani",2025,CoRR,,abs/2507.03156,,,10.48550/ARXIV.2507.03156,https://doi.org/10.48550/arXiv.2507.03156,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-03156.bib,"Wed, 20 Aug 2025 01:00:00 +0200",,,,,,2507.03156,arXiv,,"Large language model assistants (LLM-assistants) present new opportunities to transform software development. Developers are increasingly adopting these tools across tasks, including coding, testing, debugging, documentation, and design. Yet, despite growing interest, there is no synthesis of how LLM-assistants affect software developer productivity. In this paper, we present a systematic literature review of 37 peer-reviewed studies published between January 2014 and December 2024 that examine this impact. Our analysis reveals that LLM-assistants offer both considerable benefits and critical risks. Commonly reported gains include minimized code search, accelerated development, and the automation of trivial and repetitive tasks. However, studies also highlight concerns around cognitive offloading, reduced team collaboration, and inconsistent effects on code quality. While the majority of studies (92%) adopt a multi-dimensional perspective by examining at least two SPACE dimensions, reflecting increased awareness of the complexity of developer productivity, only 14% extend beyond three dimensions, indicating substantial room for more integrated evaluations. Satisfaction, Performance, and Efficiency are the most frequently investigated dimensions, whereas Communication and Activity remain underexplored. Most studies are exploratory (64%) and methodologically diverse, but lack longitudinal and team-based evaluations. This review surfaces key research gaps and provides recommendations for future research and practice. All artifacts associated with this study are publicly available at https://zenodo.org/records/15788502."
DBLP:journals/corr/abs-2507-06323,article,"Bridging {AI} and Software Security: {A} Comparative Vulnerability
Assessment of {LLM} Agent Deployment Paradigms","Tarek Gasmi and
Ramzi Guesmi and
Ines Belhadj and
Jihene Bennaceur",2025,CoRR,,abs/2507.06323,,,10.48550/ARXIV.2507.06323,https://doi.org/10.48550/arXiv.2507.06323,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-06323.bib,"Tue, 12 Aug 2025 01:00:00 +0200",,,,,,2507.06323,arXiv,,"Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service). Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Attack complexity dramatically amplified effectiveness, with chained attacks achieving 91-96% success rates. Counterintuitively, advanced reasoning models demonstrated higher exploitability despite better threat detection. Results demonstrate that architectural choices fundamentally reshape threat landscapes. This work establishes methodological foundations for cross-domain LLM agent security assessment and provides evidence-based guidance for secure deployment. Code and experimental materials are available at https: // github. com/ theconsciouslab-ai/llm-agent-security."
DBLP:journals/corr/abs-2507-07186,article,"Planted in Pretraining, Swayed by Finetuning: {A} Case Study on the
Origins of Cognitive Biases in LLMs","Itay Itzhak and
Yonatan Belinkov and
Gabriel Stanovsky",2025,CoRR,,abs/2507.07186,,,10.48550/ARXIV.2507.07186,https://doi.org/10.48550/arXiv.2507.07186,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-07186.bib,"Tue, 12 Aug 2025 01:00:00 +0200",,,,,,2507.07186,arXiv,,"Large language models (LLMs) exhibit cognitive biases -- systematic tendencies of irrational decision-making, similar to those seen in humans. Prior work has found that these biases vary across models and can be amplified by instruction tuning. However, it remains unclear if these differences in biases stem from pretraining, finetuning, or even random noise due to training stochasticity. We propose a two-step causal experimental approach to disentangle these factors. First, we finetune models multiple times using different random seeds to study how training randomness affects over $30$ cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping instruction datasets between models to isolate bias sources. This swap uses datasets that led to different bias patterns, directly testing whether biases are dataset-dependent. Our findings reveal that while training randomness introduces some variability, biases are mainly shaped by pretraining: models with the same pretrained backbone exhibit more similar bias patterns than those sharing only finetuning data. These insights suggest that understanding biases in finetuned models requires considering their pretraining origins beyond finetuning effects. This perspective can guide future efforts to develop principled strategies for evaluating and mitigating bias in LLMs."
DBLP:journals/corr/abs-2507-07548,article,"From Requirements to Code: Understanding Developer Practices in LLM-Assisted
Software Engineering","Jonathan Ullrich and
Matthias Koch and
Andreas Vogelsang",2025,CoRR,,abs/2507.07548,,,10.48550/ARXIV.2507.07548,https://doi.org/10.48550/arXiv.2507.07548,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-07548.bib,"Tue, 12 Aug 2025 01:00:00 +0200",,,,,,2507.07548,arXiv,,
DBLP:journals/corr/abs-2507-07996,article,"Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained
LLMs","Ziyue Li and
Yang Li and
Tianyi Zhou",2025,CoRR,,abs/2507.07996,,,10.48550/ARXIV.2507.07996,https://doi.org/10.48550/arXiv.2507.07996,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-07996.bib,"Tue, 12 Aug 2025 01:00:00 +0200",,,,,,2507.07996,arXiv,,"Can a pretrained neural network adapt its architecture to different inputs without any finetuning? Do we need all layers for simple tasks, and are they adequate for challenging tasks? We found that the layers of a pretrained large language model (LLM) can be manipulated as separate modules to build a better and even shallower model customized for each test sample. In particular, each layer from the pretrained model can be skipped/pruned or repeated multiple times as recurrent neural networks (RNN), and stacked with others in arbitrary orders, yielding a chain-of-layers (CoLa) per sample. This compositional space greatly expands the scope of existing works on looped/recurrent pretrained modules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree Search (MCTS) protocol to explore and identify the optimal CoLa for each sample from math and commonsense reasoning benchmarks. Compared to a static model of a fixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same layer(s) (slow thinking), and combining both, offering more flexible, dynamic architectures for different inputs. We conduct an extensive analysis of the MCTS-optimized CoLa, which leads to two key findings: (1) For>75% of samples with correct predictions by the original LLM, we can find shorter CoLa, suggesting a large space for improving inference efficiency; (2) For>60% of samples with originally incorrect predictions, we can identify CoLa achieving correct predictions, suggesting a large space of performance enhancement. Our results highlight the shortcomings of using a fixed architecture of pre-trained LLMs for inference on different samples and pave the way to unlock the generalization power of test-time depth adaptation."
DBLP:journals/corr/abs-2507-08898,article,"SEALGuard: Safeguarding the Multilingual Conversations in Southeast
Asian Languages for {LLM} Software Systems","Wenliang Shan and
Michael Fu and
Rui Yang and
Chakkrit Tantithamthavorn",2025,CoRR,,abs/2507.08898,,,10.48550/ARXIV.2507.08898,https://doi.org/10.48550/arXiv.2507.08898,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-08898.bib,"Tue, 12 Aug 2025 01:00:00 +0200",,,,,,2507.08898,arXiv,,"Safety alignment is critical for LLM-powered systems. While recent LLM-powered guardrail approaches such as LlamaGuard achieve high detection accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''), they struggle with multilingual unsafe inputs. This limitation leaves LLM systems vulnerable to unsafe and jailbreak prompts written in low-resource languages such as those in Southeast Asia. This paper introduces SEALGuard, a multilingual guardrail designed to improve the safety alignment across diverse languages. It aims to address the multilingual safety alignment gap of existing guardrails and ensure effective filtering of unsafe and jailbreak prompts in LLM-powered systems. We adapt a general-purpose multilingual language model into a multilingual guardrail using low-rank adaptation (LoRA). We construct SEALSBench, a large-scale multilingual safety alignment dataset containing over 260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases. We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on this benchmark. Our findings show that multilingual unsafe and jailbreak prompts substantially degrade the performance of the state-of-the-art LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and 18%, respectively, compared to its performance on English-only prompts. In contrast, SEALGuard outperforms existing guardrails in detecting multilingual unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and achieving the best DSR, precision, and F1-score. Our ablation study further reveals the contributions of adaptation strategies and model size to the overall performance of SEALGuard. We release our pre-trained model and benchmark at https://github.com/awsm-research/SEALGuard to support further research."
DBLP:journals/corr/abs-2507-08979,article,"{PRISM:} Reducing Spurious Implicit Biases in Vision-Language Models
with LLM-Guided Embedding Projection","Mahdiyar Molahasani and
Azadeh Motamedi and
Michael A. Greenspan and
Il{-}Min Kim and
Ali Etemad",2025,CoRR,,abs/2507.08979,,,10.48550/ARXIV.2507.08979,https://doi.org/10.48550/arXiv.2507.08979,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-08979.bib,"Tue, 12 Aug 2025 01:00:00 +0200",,,,,,2507.08979,arXiv,,"We introduce Projection-based Reduction of Implicit Spurious bias in vision-language Models (PRISM), a new data-free and task-agnostic solution for bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in their training data, leading to skewed predictions. PRISM is designed to debias VLMs without relying on predefined bias categories or additional external data. It operates in two stages: first, an LLM is prompted with simple class prompts to generate scene descriptions that contain spurious correlations. Next, PRISM uses our novel contrastive-style debiasing loss to learn a projection that maps the embeddings onto a latent space that minimizes spurious correlations while preserving the alignment between image and text embeddings.Extensive experiments demonstrate that PRISM outperforms current debiasing methods on the commonly used Waterbirds and CelebA datasets We make our code public at: https://github.com/MahdiyarMM/PRISM."
DBLP:journals/corr/abs-2507-09790,article,Prompting for Performance: Exploring LLMs for Configuring Software,"Helge Spieker and
Th{\'{e}}o Matricon and
Nassim Belmecheri and
J{\o}rn Eirik Betten and
Gauthier Le Bartz Lyan and
Heraldo Borges and
Quentin Mazouni and
Dennis Gross and
Arnaud Gotlieb and
Mathieu Acher",2025,CoRR,,abs/2507.09790,,,10.48550/ARXIV.2507.09790,https://doi.org/10.48550/arXiv.2507.09790,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-09790.bib,"Tue, 12 Aug 2025 01:00:00 +0200",,,,,,2507.0979,arXiv,,"Software systems usually provide numerous configuration options that can affect performance metrics such as execution time, memory usage, binary size, or bitrate. On the one hand, making informed decisions is challenging and requires domain expertise in options and their combinations. On the other hand, machine learning techniques can search vast configuration spaces, but with a high computational cost, since concrete executions of numerous configurations are required. In this exploratory study, we investigate whether large language models (LLMs) can assist in performance-oriented software configuration through prompts. We evaluate several LLMs on tasks including identifying relevant options, ranking configurations, and recommending performant configurations across various configurable systems, such as compilers, video encoders, and SAT solvers. Our preliminary results reveal both positive abilities and notable limitations: depending on the task and systems, LLMs can well align with expert knowledge, whereas hallucinations or superficial reasoning can emerge in other cases. These findings represent a first step toward systematic evaluations and the design of LLM-based solutions to assist with software configuration."
DBLP:journals/corr/abs-2507-10593,article,"ToolRegistry: {A} Protocol-Agnostic Tool Management Library for Function-Calling
LLMs",Peng Ding,2025,CoRR,,abs/2507.10593,,,10.48550/ARXIV.2507.10593,https://doi.org/10.48550/arXiv.2507.10593,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-10593.bib,"Thu, 14 Aug 2025 01:00:00 +0200",,,,,,2507.10593,arXiv,,"Large Language Model (LLM) applications are increasingly relying on external tools to extend their capabilities beyond text generation. However, current tool integration approaches suffer from fragmentation, protocol limitations, and implementation complexity, leading to substantial development overhead. This paper presents Toolregistry, a protocol-agnostic tool management library that simplifies tool registration, representation, execution, and lifecycle management via a unified interface. Our evaluation demonstrates that \toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x performance improvements through concurrent execution, and 100% compatibility with OpenAI function calling standards. Real-world case studies show significant improvements in development efficiency and code maintainability across diverse integration scenarios. \toolregistry is open-source and available at https://github.com/Oaklight/ToolRegistry, with comprehensive documentation at https://toolregistry.readthedocs.io/."
DBLP:journals/corr/abs-2507-10803,article,"Automated Thematic Analyses Using LLMs: Xylazine Wound Management
Social Media Chatter Use Case","JaMor Hairston and
Ritvik Ranjan and
Sahithi Lakamana and
Anthony Spadaro and
Selen Bozkurt and
Jeanmarie Perrone and
Abeed Sarker",2025,CoRR,,abs/2507.10803,,,10.48550/ARXIV.2507.10803,https://doi.org/10.48550/arXiv.2507.10803,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-10803.bib,"Thu, 14 Aug 2025 01:00:00 +0200",,,,,,2507.10803,arXiv,,"Background Large language models (LLMs) face challenges in inductive thematic analysis, a task requiring deep interpretive and domain-specific expertise. We evaluated the feasibility of using LLMs to replicate expert-driven thematic analysis of social media data. Methods Using two temporally non-intersecting Reddit datasets on xylazine (n=286 and n=686, for model optimization and validation, respectively) with twelve expert-derived themes, we evaluated five LLMs against expert coding. We modeled the task as a series of binary classifications, rather than a single, multi-label classification, employing zero-, single-, and few-shot prompting strategies and measuring performance via accuracy, precision, recall, and F1-score. Results On the validation set, GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score: 0.71). For high-prevalence themes, model-derived thematic distributions closely mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use: 16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based approaches can automate thematic analyses, offering a scalable supplement for qualitative research. Keywords: thematic analysis, large language models, natural language processing, qualitative analysis, social media, prompt engineering, public health"
DBLP:journals/corr/abs-2507-11898,article,Extremal Testing for Network Software using LLMs,"Rathin Singha and
Harry Qian and
Srinath Saikrishnan and
Tracy Zhao and
Ryan Beckett and
Siva Kesava Reddy Kakarla and
George Varghese",2025,CoRR,,abs/2507.11898,,,10.48550/ARXIV.2507.11898,https://doi.org/10.48550/arXiv.2507.11898,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-11898.bib,"Sun, 17 Aug 2025 01:00:00 +0200",,,,,,2507.11898,arXiv,,"Physicists often manually consider extreme cases when testing a theory. In this paper, we show how to automate extremal testing of network software using LLMs in two steps: first, ask the LLM to generate input constraints (e.g., DNS name length limits); then ask the LLM to generate tests that violate the constraints. We demonstrate how easy this process is by generating extremal tests for HTTP, BGP and DNS implementations, each of which uncovered new bugs. We show how this methodology extends to centralized network software such as shortest path algorithms, and how LLMs can generate filtering code to reject extremal input. We propose using agentic AI to further automate extremal testing. LLM-generated extremal testing goes beyond an old technique in software testing called Boundary Value Analysis."
DBLP:journals/corr/abs-2507-13555,article,"Demystifying Feature Requests: Leveraging LLMs to Refine Feature Requests
in Open-Source Software","Pragyan K. C and
Rambod Ghandiparsi and
Thomas Herron and
John Heaps and
Mitra Bokaei Hosseini",2025,CoRR,,abs/2507.13555,,,10.48550/ARXIV.2507.13555,https://doi.org/10.48550/arXiv.2507.13555,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-13555.bib,"Sun, 17 Aug 2025 01:00:00 +0200",,,,,,2507.13555,arXiv,,
DBLP:journals/corr/abs-2507-14330,article,"Leveraging LLMs for Formal Software Requirements - Challenges and
Prospects","Arshad Beg and
Diarmuid P. O'Donoghue and
Rosemary Monahan",2025,CoRR,,abs/2507.14330,,,10.48550/ARXIV.2507.14330,https://doi.org/10.48550/arXiv.2507.14330,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-14330.bib,"Sun, 17 Aug 2025 01:00:00 +0200",,,,,,2507.1433,arXiv,,"Software correctness is ensured mathematically through formal verification, which involves the resources of generating formal requirement specifications and having an implementation that must be verified. Tools such as model-checkers and theorem provers ensure software correctness by verifying the implementation against the specification. Formal methods deployment is regularly enforced in the development of safety-critical systems e.g. aerospace, medical devices and autonomous systems. Generating these specifications from informal and ambiguous natural language requirements remains the key challenge. Our project, VERIFAI^{1}, aims to investigate automated and semi-automated approaches to bridge this gap, using techniques from Natural Language Processing (NLP), ontology-based domain modelling, artefact reuse, and large language models (LLMs). This position paper presents a preliminary synthesis of relevant literature to identify recurring challenges and prospective research directions in the generation of verifiable specifications from informal requirements."
DBLP:journals/corr/abs-2507-15828,article,"Investigating the Use of LLMs for Evidence Briefings Generation in
Software Engineering","Mauro Marcelino and
Marcos Alves and
Bianca Trinkenreich and
Bruno Cartaxo and
S{\'{e}}rgio Soares and
Simone D. J. Barbosa and
Marcos Kalinowski",2025,CoRR,,abs/2507.15828,,,10.48550/ARXIV.2507.15828,https://doi.org/10.48550/arXiv.2507.15828,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-15828.bib,"Mon, 18 Aug 2025 01:00:00 +0200",,,,,,2507.15828,arXiv,,"[Context] An evidence briefing is a concise and objective transfer medium that can present the main findings of a study to software engineers in the industry. Although practitioners and researchers have deemed Evidence Briefings useful, their production requires manual labor, which may be a significant challenge to their broad adoption. [Goal] The goal of this registered report is to describe an experimental protocol for evaluating LLM-generated evidence briefings for secondary studies in terms of content fidelity, ease of understanding, and usefulness, as perceived by researchers and practitioners, compared to human-made briefings. [Method] We developed an RAG-based LLM tool to generate evidence briefings. We used the tool to automatically generate two evidence briefings that had been manually generated in previous research efforts. We designed a controlled experiment to evaluate how the LLM-generated briefings compare to the human-made ones regarding perceived content fidelity, ease of understanding, and usefulness. [Results] To be reported after the experimental trials. [Conclusion] Depending on the experiment results."
DBLP:journals/corr/abs-2507-16331,article,"Re:Form - Reducing Human Priors in Scalable Formal Software Verification
with {RL} in LLMs: {A} Preliminary Study on Dafny","Chuanhao Yan and
Fengdi Che and
Xuhan Huang and
Xu Xu and
Xin Li and
Yizhi Li and
Xingwei Qu and
Jingzhe Shi and
Zhuangzhuang He and
Chenghua Lin and
Yaodong Yang and
Binhang Yuan and
Hang Zhao and
Yu Qiao and
Bowen Zhou and
Jie Fu",2025,CoRR,,abs/2507.16331,,,10.48550/ARXIV.2507.16331,https://doi.org/10.48550/arXiv.2507.16331,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-16331.bib,"Tue, 19 Aug 2025 01:00:00 +0200",,,,,,2507.16331,arXiv,,"Existing informal language-based (e.g., human language) Large Language Models (LLMs) trained with Reinforcement Learning (RL) face a significant challenge: their verification processes, which provide crucial training signals, are neither reliable nor scalable. In fact, the prevalent large proprietary models could hardly generate verifiable programs. A promising yet largely uncharted alternative is formal language-based reasoning. Grounding LLMs in rigorous formal systems where generative models operate in formal language spaces (e.g., Dafny) enables the automatic and mathematically provable verification of their reasoning processes and outcomes. This capability is pivotal for achieving large-scale, reliable formal software verification. It is a common practice to employ human-annotated chain-of-thought and other human priors to induce the reasoning and coding capabilities of LLMs. Unfortunately, it becomes unacceptably all-consuming to provide such priors for supervising complex programming tasks. In this work, we systematically explore ways to reduce human priors with the formal language, Dafny, as the main environment for our pilot study. Our pipeline mainly relies on introducing an automatic and scalable data curation pipeline, and careful RL designs integrated with feedback from the formal language verifier. We introduce DafnyComp, a benchmark of compositional formal programs with auto-formalized specifications for specification reasoning. Our supervised fine-tuning (SFT) stage enables even small models (e.g., 0.5B) to generate syntactically valid and verifiable Dafny code, surpassing proprietary models. RL with regularization further improves performance, achieving stronger generalization to out-of-domain tasks and outperforming all strong baselines on the challenging DafnyComp benchmark."
DBLP:journals/corr/abs-2507-17927,article,{SMARTAPS:} Tool-augmented LLMs for Operations Management,"Timothy Tin Long Yu and
Mahdi Mostajabdaveh and
Jabo Serge Byusa and
Rindra Ramamonjison and
Giuseppe Carenini and
Kun Mao and
Zirui Zhou and
Yong Zhang",2025,CoRR,,abs/2507.17927,,,10.48550/ARXIV.2507.17927,https://doi.org/10.48550/arXiv.2507.17927,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-17927.bib,"Mon, 18 Aug 2025 01:00:00 +0200",,,,,,2507.17927,arXiv,,"Large language models (LLMs) present intriguing opportunities to enhance user interaction with traditional algorithms and tools in real-world applications. An advanced planning system (APS) is a sophisticated software that leverages optimization to help operations planners create, interpret, and modify an operational plan. While highly beneficial, many customers are priced out of using an APS due to the ongoing costs of consultants responsible for customization and maintenance. To address the need for a more accessible APS expressed by supply chain planners, we present SmartAPS, a conversational system built on a tool-augmented LLM. Our system provides operations planners with an intuitive natural language chat interface, allowing them to query information, perform counterfactual reasoning, receive recommendations, and execute scenario analysis to better manage their operation. A short video demonstrating the system has been released: https://youtu.be/KtIrJjlDbyw"
DBLP:journals/corr/abs-2507-19113,article,"Exploring the Use of LLMs for Requirements Specification in an {IT}
Consulting Company","Liliana Pasquale and
Azzurra Ragone and
Emanuele Piemontese and
Armin Amiri Darban",2025,CoRR,,abs/2507.19113,,,10.48550/ARXIV.2507.19113,https://doi.org/10.48550/arXiv.2507.19113,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-19113.bib,"Thu, 21 Aug 2025 01:00:00 +0200",,,,,,2507.19113,arXiv,,
DBLP:journals/corr/abs-2507-19845,article,"MegatronApp: Efficient and Comprehensive Management on Distributed
{LLM} Training","Bohan Zhao and
Guang Yang and
Shuo Chen and
Ruitao Liu and
Tingrui Zhang and
Yongchao He and
Wei Xu",2025,CoRR,,abs/2507.19845,,,10.48550/ARXIV.2507.19845,https://doi.org/10.48550/arXiv.2507.19845,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-19845.bib,"Thu, 21 Aug 2025 01:00:00 +0200",,,,,,2507.19845,arXiv,,"The rapid escalation in the parameter count of large language models (LLMs) has transformed model training from a single-node endeavor into a highly intricate, cross-node activity. While frameworks such as Megatron-LM successfully integrate tensor (TP), pipeline (PP), and data (DP) parallelism to enable trillion-parameter training, they simultaneously expose practitioners to unprecedented systems-level challenges in performance optimization, diagnosis, and interpretability. MegatronApp is an open-source toolchain expressly designed to meet these challenges. It introduces four orthogonal, yet seamlessly composable modules--MegaScan, MegaFBD, MegaDPP, and MegaScope--that collectively elevate the reliability, efficiency, and transparency of production-scale training. This paper presents the motivation, architecture, and distinctive contributions of each module, and elucidates how their synergistic integration augments the Megatron-LM ecosystem."
DBLP:journals/corr/abs-2507-20655,article,"CoGrader: Transforming Instructors' Assessment of Project Reports
through Collaborative {LLM} Integration","Zixin Chen and
Jiachen Wang and
Yumeng Li and
Haobo Li and
Chuhan Shi and
Rong Zhang and
Huamin Qu",2025,CoRR,,abs/2507.20655,,,10.48550/ARXIV.2507.20655,https://doi.org/10.48550/arXiv.2507.20655,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-20655.bib,"Thu, 21 Aug 2025 01:00:00 +0200",,,,,,2507.20655,arXiv,,
DBLP:journals/corr/abs-2507-21083,article,"ChatGPT Reads Your Tone and Responds Accordingly - Until It Does Not
- Emotional Framing Induces Bias in {LLM} Outputs",Franck Bardol,2025,CoRR,,abs/2507.21083,,,10.48550/ARXIV.2507.21083,https://doi.org/10.48550/arXiv.2507.21083,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-21083.bib,"Thu, 21 Aug 2025 01:00:00 +0200",,,,,,2507.21083,arXiv,,"Large Language Models like GPT-4 adjust their responses not only based on the question asked, but also on how it is emotionally phrased. We systematically vary the emotional tone of 156 prompts - spanning controversial and everyday topics - and analyze how it affects model responses. Our findings show that GPT-4 is three times less likely to respond negatively to a negatively framed question than to a neutral one. This suggests a""rebound""bias where the model overcorrects, often shifting toward neutrality or positivity. On sensitive topics (e.g., justice or politics), this effect is even more pronounced: tone-based variation is suppressed, suggesting an alignment override. We introduce concepts like the""tone floor""- a lower bound in response negativity - and use tone-valence transition matrices to quantify behavior. Visualizations based on 1536-dimensional embeddings confirm semantic drift based on tone. Our work highlights an underexplored class of biases driven by emotional framing in prompts, with implications for AI alignment and trust. Code and data are available at: https://github.com/bardolfranck/llm-responses-viewer"
DBLP:journals/corr/abs-2507-21428,article,"MemTool: Optimizing Short-Term Memory Management for Dynamic Tool
Calling in {LLM} Agent Multi-Turn Conversations","Elias Lumer and
Anmol Gulati and
Vamse Kumar Subbiah and
Pradeep Honaganahalli Basavaraju and
James A. Burke",2025,CoRR,,abs/2507.21428,,,10.48550/ARXIV.2507.21428,https://doi.org/10.48550/arXiv.2507.21428,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-21428.bib,"Thu, 21 Aug 2025 01:00:00 +0200",,,,,,2507.21428,arXiv,,"Large Language Model (LLM) agents have shown significant autonomous capabilities in dynamically searching and incorporating relevant tools or Model Context Protocol (MCP) servers for individual queries. However, fixed context windows limit effectiveness in multi-turn interactions requiring repeated, independent tool usage. We introduce MemTool, a short-term memory framework enabling LLM agents to dynamically manage tools or MCP server contexts across multi-turn conversations. MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. In Autonomous Agent Mode, reasoning LLMs achieve high tool-removal efficiency (90-94% over a 3-window average), while medium-sized models exhibit significantly lower efficiency (0-60%). Workflow and Hybrid modes consistently manage tool removal effectively, whereas Autonomous and Hybrid modes excel at task completion. We present trade-offs and recommendations for each MemTool mode based on task accuracy, agency, and model capabilities."
DBLP:journals/corr/abs-2507-23370,article,"Trae Agent: An LLM-based Agent for Software Engineering with Test-time
Scaling","Pengfei Gao and
Zhao Tian and
Xiangxin Meng and
Xinchen Wang and
Ruida Hu and
Yuanan Xiao and
Yizhou Liu and
Zhao Zhang and
Junjie Chen and
Cuiyun Gao and
Yun Lin and
Yingfei Xiong and
Chao Peng and
Xia Liu",2025,CoRR,,abs/2507.23370,,,10.48550/ARXIV.2507.23370,https://doi.org/10.48550/arXiv.2507.23370,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2507-23370.bib,"Thu, 04 Sep 2025 01:00:00 +0200",,,,,,2507.2337,arXiv,,"Software issue resolution is a critical challenge in software engineering and has garnered increasing attention in recent years. With the rapid advancement of large language models (LLMs), substantial progress has been made in addressing real-world software engineering tasks. Recent studies have introduced ensemble reasoning techniques to enhance the performance of LLM-based issue resolution. However, existing prompting-based methods still face limitations in effectively exploring large ensemble spaces and lack the capacity for repository-level understanding, both of which constrain their overall effectiveness. In this paper, we propose Trae Agent, the first agent-based ensemble reasoning approach for repository-level issue resolution. Trae Agent formulates our goal as an optimal solution search problem and addresses two key challenges, i.e., large ensemble spaces and repository-level understanding, through modular agents for generation, pruning, and selection. We conduct extensive experiments using three leading LLMs on the widely-adopted SWE-bench benchmark, comparing Trae Agent against four state-of-the-art ensemble reasoning techniques. Experimental results demonstrate that Trae Agent consistently achieves superior performance, with an average improvement of 10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of 75.20%. We are pleased to release Trae Agent as an open-source project to support the research community, with all resources available at https://github.com/bytedance/trae-agent."
DBLP:journals/corr/abs-2508-00198,article,"Testing the Untestable? An Empirical Study on the Testing Process
of LLM-Powered Software Systems","Cleyton V. C. de Magalh{\~{a}}es and
{\'{I}}talo Santos and
Brody Stuart{-}Verner and
Ronnie de Souza Santos",2025,CoRR,,abs/2508.00198,,,10.48550/ARXIV.2508.00198,https://doi.org/10.48550/arXiv.2508.00198,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-00198.bib,"Wed, 10 Sep 2025 01:00:00 +0200",,,,,,2508.00198,arXiv,,
DBLP:journals/corr/abs-2508-02233,article,A Methodological Framework for LLM-Based Mining of Software Repositories,"Vincenzo De Martino and
Joel Casta{\~{n}}o and
Fabio Palomba and
Xavier Franch and
Silverio Mart{\'{\i}}nez{-}Fern{\'{a}}ndez",2025,CoRR,,abs/2508.02233,,,10.48550/ARXIV.2508.02233,https://doi.org/10.48550/arXiv.2508.02233,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-02233.bib,"Wed, 10 Sep 2025 01:00:00 +0200",,,,,,2508.02233,arXiv,,"Large Language Models (LLMs) are increasingly used in software engineering research, offering new opportunities for automating repository mining tasks. However, despite their growing popularity, the methodological integration of LLMs into Mining Software Repositories (MSR) remains poorly understood. Existing studies tend to focus on specific capabilities or performance benchmarks, providing limited insight into how researchers utilize LLMs across the full research pipeline. To address this gap, we conduct a mixed-method study that combines a rapid review and questionnaire survey in the field of LLM4MSR. We investigate (1) the approaches and (2) the threats that affect the empirical rigor of researchers involved in this field. Our findings reveal 15 methodological approaches, nine main threats, and 25 mitigation strategies. Building on these findings, we present PRIMES 2.0, a refined empirical framework organized into six stages, comprising 23 methodological substeps, each mapped to specific threats and corresponding mitigation strategies, providing prescriptive and adaptive support throughout the lifecycle of LLM-based MSR studies. Our work contributes to establishing a more transparent and reproducible foundation for LLM-based MSR research."
DBLP:journals/corr/abs-2508-02827,article,"Automated Validation of LLM-based Evaluators for Software Engineering
Artifacts","Ora Nova Fandina and
Eitan Farchi and
Shmulik Froimovich and
Rami Katan and
Alice Podolsky and
Orna Raz and
Avi Ziv",2025,CoRR,,abs/2508.02827,,,10.48550/ARXIV.2508.02827,https://doi.org/10.48550/arXiv.2508.02827,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-02827.bib,"Wed, 10 Sep 2025 01:00:00 +0200",,,,,,2508.02827,arXiv,,"Automation in software engineering increasingly relies on large language models (LLMs) to generate, review, and assess code artifacts. However, establishing LLMs as reliable evaluators remains an open challenge: human evaluations are costly, subjective and non scalable, while existing automated methods fail to discern fine grained variations in artifact quality. We introduce REFINE (Ranking Evaluators for FIne grained Nuanced Evaluation), an automated framework for benchmarking LLM based evaluators across software engineering tasks. REFINE comprises of two modules: Hierarchy Dataset Builder applies novel generation techniques to automatically synthesize artifacts with progressively reduced quality, and Evaluator Tester quantifies each candidate evaluator configuration by measuring how closely its rankings align with expected ordering. A key feature of REFINE is controllability: users can tune the granularity of degradation to progressively refine evaluator configurations, from coarse filtering to stress testing on subtle quality gaps. While the methodology is general, we focus on coding tasks reflecting the practical demands in our production setting. REFINE was integrated into IBM's internal development workflows and applied to code generation, translation, and summarization for COBOL, an enterprise critical programming language, using industrial data. It was used to identify LLM as a Judge configurations that lifted alignment scores from below $0.7$ to above $0.9$ in some coding tasks. These nuance sensitive evaluators are now actively used by model training teams to support model release decisions."
DBLP:journals/corr/abs-2508-03603,article,"ReFuzzer: Feedback-Driven Approach to Enhance Validity of LLM-Generated
Test Programs","Iti Shree and
Karine Even{-}Mendoza and
Tomasz Radzik",2025,CoRR,,abs/2508.03603,,,10.48550/ARXIV.2508.03603,https://doi.org/10.48550/arXiv.2508.03603,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-03603.bib,"Wed, 10 Sep 2025 01:00:00 +0200",,,,,,2508.03603,arXiv,,"Existing LLM-based compiler fuzzers often produce syntactically or semantically invalid test programs, limiting their effectiveness in exercising compiler optimizations and backend components. We introduce ReFuzzer, a framework for refining LLM-generated test programs by systematically detecting and correcting compilation and runtime violations (e.g. division by zero or array out-of-bounds accesses). ReFuzzer employs a feedback loop with a local LLM to validate and filter erroneous programs before execution, improving fuzzing effectiveness beyond crash detection and enabling the generation of diverse yet valid test programs. We evaluated ReFuzzer's effectiveness across black-, grey- and white-box fuzzing approaches targeting LLVM/Clang. ReFuzzer improved test programs'validity from 47.0-49.4% to 96.6-97.3%, with an average processing time of 2.9-3.5 s per test program on a dual-GPU machine. Further, refuzzing significantly increased code coverage in critical optimization and IR generation components. For example, vectorization coverage had an absolute improvement of 9.2%, 2.3%, and 7.1% in black-, grey-, and white-box fuzzing, enhancing testing effectiveness."
DBLP:journals/corr/abs-2508-04664,article,"Sculptor: Empowering LLMs with Cognitive Agency via Active Context
Management","Mo Li and
L. H. Xu and
Qitai Tan and
Ting Cao and
Yunxin Liu",2025,CoRR,,abs/2508.04664,,,10.48550/ARXIV.2508.04664,https://doi.org/10.48550/arXiv.2508.04664,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-04664.bib,"Thu, 11 Sep 2025 01:00:00 +0200",,,,,,2508.04664,arXiv,,"Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs'capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) precise search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on diverse long-context benchmarks demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs'inherent tool-calling and instruction-following capabilities. To further optimize these strategies, we introduce a novel dynamic context-aware reinforcement learning (RL) approach, advancing the training of an agent that actively modifies its own conversational history. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale."
DBLP:journals/corr/abs-2508-05299,article,"{VS-LLM:} Visual-Semantic Depression Assessment based on {LLM} for
Drawing Projection Test","Meiqi Wu and
Yaxuan Kang and
Xuchen Li and
Shiyu Hu and
Xiaotang Chen and
Yunfeng Kang and
Weiqiang Wang and
Kaiqi Huang",2025,CoRR,,abs/2508.05299,,,10.48550/ARXIV.2508.05299,https://doi.org/10.48550/arXiv.2508.05299,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-05299.bib,"Sat, 13 Sep 2025 01:00:00 +0200",,,,,,2508.05299,arXiv,,
DBLP:journals/corr/abs-2508-06394,article,"When AIOps Become ""AI Oops"": Subverting LLM-driven {IT} Operations
via Telemetry Manipulation","Dario Pasquini and
Evgenios M. Kornaropoulos and
Giuseppe Ateniese and
Omer Akgul and
Athanasios Theocharis and
Petros Efstathopoulos",2025,CoRR,,abs/2508.06394,,,10.48550/ARXIV.2508.06394,https://doi.org/10.48550/arXiv.2508.06394,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-06394.bib,"Sat, 13 Sep 2025 01:00:00 +0200",,,,,,2508.06394,arXiv,,"AI for IT Operations (AIOps) is transforming how organizations manage complex software systems by automating anomaly detection, incident diagnosis, and remediation. Modern AIOps solutions increasingly rely on autonomous LLM-based agents to interpret telemetry data and take corrective actions with minimal human intervention, promising faster response times and operational cost savings. In this work, we perform the first security analysis of AIOps solutions, showing that, once again, AI-driven automation comes with a profound security cost. We demonstrate that adversaries can manipulate system telemetry to mislead AIOps agents into taking actions that compromise the integrity of the infrastructure they manage. We introduce techniques to reliably inject telemetry data using error-inducing requests that influence agent behavior through a form of adversarial reward-hacking; plausible but incorrect system error interpretations that steer the agent's decision-making. Our attack methodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing, and LLM-driven adversarial input generation--and operates without any prior knowledge of the target system. To counter this threat, we propose AIOpsShield, a defense mechanism that sanitizes telemetry data by exploiting its structured nature and the minimal role of user-generated content. Our experiments show that AIOpsShield reliably blocks telemetry-based attacks without affecting normal agent performance. Ultimately, this work exposes AIOps as an emerging attack vector for system compromise and underscores the urgent need for security-aware AIOps design."
DBLP:journals/corr/abs-2508-06554,article,"AquaChat++: LLM-Assisted Multi-ROV Inspection for Aquaculture Net
Pens with Integrated Battery Management and Thruster Fault Tolerance","Abdelhaleem Saad and
Waseem Akram and
Irfan Hussain",2025,CoRR,,abs/2508.06554,,,10.48550/ARXIV.2508.06554,https://doi.org/10.48550/arXiv.2508.06554,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-06554.bib,"Sat, 13 Sep 2025 01:00:00 +0200",,,,,,2508.06554,arXiv,,"Inspection of aquaculture net pens is essential for ensuring the structural integrity and sustainable operation of offshore fish farming systems. Traditional methods, typically based on manually operated or single-ROV systems, offer limited adaptability to real-time constraints such as energy consumption, hardware faults, and dynamic underwater conditions. This paper introduces AquaChat++, a novel multi-ROV inspection framework that uses Large Language Models (LLMs) to enable adaptive mission planning, coordinated task execution, and fault-tolerant control in complex aquaculture environments. The proposed system consists of a two-layered architecture. The high-level plan generation layer employs an LLM, such as ChatGPT-4, to translate natural language user commands into symbolic, multi-agent inspection plans. A task manager dynamically allocates and schedules actions among ROVs based on their real-time status and operational constraints, including thruster faults and battery levels. The low-level control layer ensures accurate trajectory tracking and integrates thruster fault detection and compensation mechanisms. By incorporating real-time feedback and event-triggered replanning, AquaChat++ enhances system robustness and operational efficiency. Simulated experiments in a physics-based aquaculture environment demonstrate improved inspection coverage, energy-efficient behavior, and resilience to actuator failures. These findings highlight the potential of LLM-driven frameworks to support scalable, intelligent, and autonomous underwater robotic operations within the aquaculture sector."
DBLP:journals/corr/abs-2508-08761,article,"DevNous: An LLM-Based Multi-Agent System for Grounding {IT} Project
Management in Unstructured Conversation","Stavros Doropoulos and
Stavros Vologiannidis and
Ioannis Magnisalis",2025,CoRR,,abs/2508.08761,,,10.48550/ARXIV.2508.08761,https://doi.org/10.48550/arXiv.2508.08761,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-08761.bib,"Sat, 13 Sep 2025 01:00:00 +0200",,,,,,2508.08761,arXiv,,
DBLP:journals/corr/abs-2508-09149,article,"Semantic-Aware {LLM} Orchestration for Proactive Resource Management
in Predictive Digital Twin Vehicular Networks",Seyed Hossein Ahmadpanah,2025,CoRR,,abs/2508.09149,,,10.48550/ARXIV.2508.09149,https://doi.org/10.48550/arXiv.2508.09149,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-09149.bib,"Sat, 13 Sep 2025 01:00:00 +0200",,,,,,2508.09149,arXiv,,"Next-generation automotive applications require vehicular edge computing (VEC), but current management systems are essentially fixed and reactive. They are suboptimal in extremely dynamic vehicular environments because they are constrained to static optimization objectives and base their decisions on the current network states. This paper presents a novel Semantic-Aware Proactive LLM Orchestration (SP-LLM) framework to address these issues. Our method transforms the traditional Digital Twin (DT) into a Predictive Digital Twin (pDT) that predicts important network parameters such as task arrivals, vehicle mobility, and channel quality. A Large Language Model (LLM) that serves as a cognitive orchestrator is at the heart of our framework. It makes proactive, forward-looking decisions about task offloading and resource allocation by utilizing the pDT's forecasts. The LLM's ability to decipher high-level semantic commands given in natural language is crucial because it enables it to dynamically modify its optimization policy to match evolving strategic objectives, like giving emergency services priority or optimizing energy efficiency. We show through extensive simulations that SP-LLM performs significantly better in terms of scalability, robustness in volatile conditions, and adaptability than state-of-the-art reactive and MARL-based approaches. More intelligent, autonomous, and goal-driven vehicular networks will be possible due to our framework's outstanding capacity to convert human intent into optimal network behavior."
DBLP:journals/corr/abs-2508-09190,article,"Fine-Grained Safety Neurons with Training-Free Continual Projection
to Reduce {LLM} Fine Tuning Risks","Bing Han and
Feifei Zhao and
Dongcheng Zhao and
Guobin Shen and
Ping Wu and
Yu Shi and
Yi Zeng",2025,CoRR,,abs/2508.09190,,,10.48550/ARXIV.2508.09190,https://doi.org/10.48550/arXiv.2508.09190,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-09190.bib,"Sat, 13 Sep 2025 01:00:00 +0200",,,,,,2508.0919,arXiv,,"Fine-tuning as service injects domain-specific knowledge into large language models (LLMs), while challenging the original alignment mechanisms and introducing safety risks. A series of defense strategies have been proposed for the alignment, fine-tuning, and post-fine-tuning phases, where most post-fine-tuning defenses rely on coarse-grained safety layer mapping. These methods lack a comprehensive consideration of both safety layers and fine-grained neurons, limiting their ability to efficiently balance safety and utility. To address this, we propose the Fine-Grained Safety Neurons (FGSN) with Training-Free Continual Projection method to reduce the fine-tuning safety risks. FGSN inherently integrates the multi-scale interactions between safety layers and neurons, localizing sparser and more precise fine-grained safety neurons while minimizing interference with downstream task neurons. We then project the safety neuron parameters onto safety directions, improving model safety while aligning more closely with human preferences. Extensive experiments across multiple fine-tuned LLM models demonstrate that our method significantly reduce harmfulness scores and attack success rates with minimal parameter modifications, while preserving the model's utility. Furthermore, by introducing a task-specific, multi-dimensional heterogeneous safety neuron cluster optimization mechanism, we achieve continual defense and generalization capability against unforeseen emerging safety concerns."
DBLP:journals/corr/abs-2508-13171,article,"Cognitive Workspace: Active Memory Management for LLMs - An Empirical
Study of Functional Infinite Context",Tao An,2025,CoRR,,abs/2508.13171,,,10.48550/ARXIV.2508.13171,https://doi.org/10.48550/arXiv.2508.13171,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-13171.bib,"Tue, 16 Sep 2025 01:00:00 +0200",,,,,,2508.13171,arXiv,,"Large Language Models (LLMs) face fundamental limitations in context management despite recent advances extending context windows to millions of tokens. We propose Cognitive Workspace, a novel paradigm that transcends traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive mechanisms of external memory use. Drawing from cognitive science foundations including Baddeley's working memory model, Clark's extended mind thesis, and Hutchins'distributed cognition framework, we demonstrate that current passive retrieval systems fail to capture the dynamic, task-driven nature of human memory management. Our analysis of 2024-2025 developments reveals that while techniques like Infini-attention and StreamingLLM achieve impressive context lengths, they lack the metacognitive awareness and active planning capabilities essential for true cognitive extension. Cognitive Workspace addresses these limitations through three core innovations: (1) active memory management with deliberate information curation, (2) hierarchical cognitive buffers enabling persistent working states, and (3) task-driven context optimization that dynamically adapts to cognitive demands. Empirical validation demonstrates Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from 54-60% across different tasks) compared to 0% for traditional RAG, with 17-18% net efficiency gain despite 3.3x higher operation counts. Statistical analysis confirms these advantages with p<0.001 and Cohen's d>23 across multiple task types, establishing the first quantitative evidence for active memory superiority in LLM systems. We present a comprehensive theoretical framework synthesizing insights from 50+ recent papers, positioning Cognitive Workspace as a fundamental shift from information retrieval to genuine cognitive augmentation."
DBLP:journals/corr/abs-2508-15449,article,"Reliable Unlearning Harmful Information in LLMs with Metamorphosis
Representation Projection","Chengcan Wu and
Zeming Wei and
Huanran Chen and
Yinpeng Dong and
Meng Sun",2025,CoRR,,abs/2508.15449,,,10.48550/ARXIV.2508.15449,https://doi.org/10.48550/arXiv.2508.15449,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-15449.bib,"Thu, 18 Sep 2025 01:00:00 +0200",,,,,,2508.15449,arXiv,,"While Large Language Models (LLMs) have demonstrated impressive performance in various domains and tasks, concerns about their safety are becoming increasingly severe. In particular, since models may store unsafe knowledge internally, machine unlearning has emerged as a representative paradigm to ensure model safety. Existing approaches employ various training techniques, such as gradient ascent and negative preference optimization, in attempts to eliminate the influence of undesired data on target models. However, these methods merely suppress the activation of undesired data through parametric training without completely eradicating its informational traces within the model. This fundamental limitation makes it difficult to achieve effective continuous unlearning, rendering these methods vulnerable to relearning attacks. To overcome these challenges, we propose a Metamorphosis Representation Projection (MRP) approach that pioneers the application of irreversible projection properties to machine unlearning. By implementing projective transformations in the hidden state space of specific network layers, our method effectively eliminates harmful information while preserving useful knowledge. Experimental results demonstrate that our approach enables effective continuous unlearning and successfully defends against relearning attacks, achieving state-of-the-art performance in unlearning effectiveness while preserving natural performance. Our code is available in https://github.com/ChengcanWu/MRP."
DBLP:journals/corr/abs-2508-16445,article,Using LLMs and Essence to Support Software Practice Adoption,"Sonia Nicoletti and
Paolo Ciancarini",2025,CoRR,,abs/2508.16445,,,10.48550/ARXIV.2508.16445,https://doi.org/10.48550/arXiv.2508.16445,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-16445.bib,"Sun, 21 Sep 2025 01:00:00 +0200",,,,,,2508.16445,arXiv,,"Recent advancements in natural language processing (NLP) have enabled the development of automated tools that support various domains, including software engineering. However, while NLP and artificial intelligence (AI) research has extensively focused on tasks such as code generation, less attention has been given to automating support for the adoption of best practices, the evolution of ways of working, and the monitoring of process health. This study addresses this gap by exploring the integration of Essence, a standard and thinking framework for managing software engineering practices, with large language models (LLMs). To this end, a specialised chatbot was developed to assist students and professionals in understanding and applying Essence. The chatbot employs a retrieval-augmented generation (RAG) system to retrieve relevant contextual information from a curated knowledge base. Four different LLMs were used to create multiple chatbot configurations, each evaluated both as a base model and augmented with the RAG system. The system performance was evaluated through both the relevance of retrieved context and the quality of generated responses. Comparative analysis against the general-purpose LLMs demonstrated that the proposed system consistently outperforms its baseline counterpart in domain-specific tasks. By facilitating access to structured software engineering knowledge, this work contributes to bridging the gap between theoretical frameworks and practical application, potentially improving process management and the adoption of software development practices. While further validation through user studies is required, these findings highlight the potential of LLM-based automation to enhance learning and decision-making in software engineering."
DBLP:journals/corr/abs-2508-18089,article,"LLM-Guided Genetic Improvement: Envisioning Semantic Aware Automated
Software Evolution","Karine Even{-}Mendoza and
Alexander E. I. Brownlee and
Alina Geiger and
Carol Hanna and
Justyna Petke and
Federica Sarro and
Dominik Sobania",2025,CoRR,,abs/2508.18089,,,10.48550/ARXIV.2508.18089,https://doi.org/10.48550/arXiv.2508.18089,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-18089.bib,"Sun, 21 Sep 2025 01:00:00 +0200",,,,,,2508.18089,arXiv,,"Genetic Improvement (GI) of software automatically creates alternative software versions that are improved according to certain properties of interests (e.g., running-time). Search-based GI excels at navigating large program spaces, but operates primarily at the syntactic level. In contrast, Large Language Models (LLMs) offer semantic-aware edits, yet lack goal-directed feedback and control (which is instead a strength of GI). As such, we propose the investigation of a new research line on AI-powered GI aimed at incorporating semantic aware search. We take a first step at it by augmenting GI with the use of automated clustering of LLM edits. We provide initial empirical evidence that our proposal, dubbed PatchCat, allows us to automatically and effectively categorize LLM-suggested patches. PatchCat identified 18 different types of software patches and categorized newly suggested patches with high accuracy. It also enabled detecting NoOp edits in advance and, prospectively, to skip test suite execution to save resources in many cases. These results, coupled with the fact that PatchCat works with small, local LLMs, are a promising step toward interpretable, efficient, and green GI. We outline a rich agenda of future work and call for the community to join our vision of building a principled understanding of LLM-driven mutations, guiding the GI search process with semantic signals."
DBLP:journals/corr/abs-2508-19089,article,"It's All About In-Context Learning! Teaching Extremely Low-Resource
Languages to LLMs","Yue Li and
Zhixue Zhao and
Carolina Scarton",2025,CoRR,,abs/2508.19089,,,10.48550/ARXIV.2508.19089,https://doi.org/10.48550/arXiv.2508.19089,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-19089.bib,"Sun, 21 Sep 2025 01:00:00 +0200",,,,,,2508.19089,arXiv,,"Extremely low-resource languages, especially those written in rare scripts, as shown in Figure 1, remain largely unsupported by large language models (LLMs). This is due in part to compounding factors such as the lack of training data. This paper delivers the first comprehensive analysis of whether LLMs can acquire such languages purely via in-context learning (ICL), with or without auxiliary alignment signals, and how these methods compare to parameter-efficient fine-tuning (PEFT). We systematically evaluate 20 under-represented languages across three state-of-the-art multilingual LLMs. Our findings highlight the limitation of PEFT when both language and its script are extremely under-represented by the LLM. In contrast, zero-shot ICL with language alignment is impressively effective on extremely low-resource languages, while few-shot ICL or PEFT is more beneficial for languages relatively better represented by LLMs. For LLM practitioners working on extremely low-resource languages, we summarise guidelines grounded by our results on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning a multilingual model on languages of unseen scripts."
DBLP:journals/corr/abs-2508-20912,article,"Research Challenges in Relational Database Management Systems for
{LLM} Queries","Kerem Akillioglu and
Anurag Chakraborty and
Sairaj Voruganti and
M. Tamer {\""{O}}zsu",2025,CoRR,,abs/2508.20912,,,10.48550/ARXIV.2508.20912,https://doi.org/10.48550/arXiv.2508.20912,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-20912.bib,"Mon, 22 Sep 2025 01:00:00 +0200",,,,,,2508.20912,arXiv,,"Large language models (LLMs) have become essential for applications such as text summarization, sentiment analysis, and automated question-answering. Recently, LLMs have also been integrated into relational database management systems to enhance querying and support advanced data processing. Companies such as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly within SQL, denoted as LLM queries, to boost data insights. However, open-source solutions currently have limited functionality and poor performance. In this work, we present an early exploration of two open-source systems and one enterprise platform, using five representative queries to expose functional, performance, and scalability limits in today's SQL-invoked LLM integrations. We identify three main issues: enforcing structured outputs, optimizing resource utilization, and improving query planning. We implemented initial solutions and observed improvements in accommodating LLM powered SQL queries. These early gains demonstrate that tighter integration of LLM+DBMS is the key to scalable and efficient processing of LLM queries."
DBLP:journals/corr/abs-2508-21433,article,"The Complexity Trap: Simple Observation Masking Is as Efficient as
{LLM} Summarization for Agent Context Management","Tobias Lindenbauer and
Igor Slinko and
Ludwig Felder and
Egor Bogomolov and
Yaroslav Zharov",2025,CoRR,,abs/2508.21433,,,10.48550/ARXIV.2508.21433,https://doi.org/10.48550/arXiv.2508.21433,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2508-21433.bib,"Tue, 23 Sep 2025 01:00:00 +0200",,,,,,2508.21433,arXiv,,"Large Language Model (LLM)-based agents solve complex tasks through iterative reasoning, exploration, and tool-use, a process that can result in long, expensive context histories. While state-of-the-art Software Engineering ( SE) agents like OpenHands or Cursor use LLM-based summarization to tackle this issue, it is unclear whether the increased complexity offers tangible performance benefits compared to simply omitting older observations. We present a systematic comparison of these strategies within SWE-agent on SWE-bench Verified across five diverse model configurations. We find that a simple observation-masking strategy halves cost relative to a raw agent while matching, and sometimes slightly exceeding, the solve rate of LLM summarization. For example, with Qwen3-Coder 480B, masking improves solve rate from 53.8% (raw agent) to 54.8%, while remaining competitive with summarization at a lower cost. These results suggest that, at least within SWE-agent on SWE-bench Verified, the most effective and efficient context management can be the simplest. We release code and data for reproducibility"
DBLP:journals/corr/abs-2509-00140,article,"LLM-based Triplet Extraction for Automated Ontology Generation in
Software Engineering Standards",Songhui Yue,2025,CoRR,,abs/2509.00140,,,10.48550/ARXIV.2509.00140,https://doi.org/10.48550/arXiv.2509.00140,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2509-00140.bib,"Wed, 08 Oct 2025 01:00:00 +0200",,,,,,2509.0014,arXiv,,"Ontologies have supported knowledge representation and whitebox reasoning for decades; thus, the automated ontology generation (AOG) plays a crucial role in scaling their use. Software engineering standards (SES) consist of long, unstructured text (with high noise) and paragraphs with domain-specific terms. In this setting, relation triple extraction (RTE), together with term extraction, constitutes the first stage toward AOG. This work proposes an open-source large language model (LLM)-assisted approach to RTE for SES. Instead of solely relying on prompt-engineering-based methods, this study promotes the use of LLMs as an aid in constructing ontologies and explores an effective AOG workflow that includes document segmentation, candidate term mining, LLM-based relation inference, term normalization, and cross-section alignment. Golden-standard benchmarks at three granularities are constructed and used to evaluate the ontology generated from the study. The results show that it is comparable and potentially superior to the OpenIE method of triple extraction."
DBLP:journals/corr/abs-2509-02408,article,Cache Management for Mixture-of-Experts LLMs - extended version,"Spyros Angelopoulos and
Loris Marchal and
Adrien Obrecht and
Bertrand Simon",2025,CoRR,,abs/2509.02408,,,10.48550/ARXIV.2509.02408,https://doi.org/10.48550/arXiv.2509.02408,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2509-02408.bib,"Sun, 12 Oct 2025 01:00:00 +0200",,,,,,2509.02408,arXiv,,
DBLP:journals/corr/abs-2509-02449,article,"KubeIntellect: {A} Modular LLM-Orchestrated Agent Framework for End-to-End
Kubernetes Management","Mohsen Seyedkazemi Ardebili and
Andrea Bartolini",2025,CoRR,,abs/2509.02449,,,10.48550/ARXIV.2509.02449,https://doi.org/10.48550/arXiv.2509.02449,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2509-02449.bib,"Sun, 12 Oct 2025 01:00:00 +0200",,,,,,2509.02449,arXiv,,
DBLP:journals/corr/abs-2509-05338,article,"Plantbot: Integrating Plant and Robot through {LLM} Modular Agent
Networks","Atsushi Masumori and
Norihiro Maruyama and
Itsuki Doi and
Johnsmith and
Hiroki Sato and
Takashi Ikegami",2025,CoRR,,abs/2509.05338,,,10.48550/ARXIV.2509.05338,https://doi.org/10.48550/arXiv.2509.05338,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2509-05338.bib,"Sun, 12 Oct 2025 01:00:00 +0200",,,,,,2509.05338,arXiv,,
DBLP:journals/corr/abs-2509-07763,article,"What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring
Motivations in Open-Source Projects","Mikel Robredo and
Matteo Esposito and
Fabio Palomba and
Rafael Pe{\~{n}}aloza and
Valentina Lenarduzzi",2025,CoRR,,abs/2509.07763,,,10.48550/ARXIV.2509.07763,https://doi.org/10.48550/arXiv.2509.07763,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2509-07763.bib,"Sun, 12 Oct 2025 01:00:00 +0200",,,,,,2509.07763,arXiv,,
DBLP:journals/cacm/Greengard24e,article,Is It Possible to Truly Understand Performance in LLMs?,Samuel Greengard,2024,Commun. {ACM},,67,12,14--16,10.1145/3695860,https://doi.org/10.1145/3695860,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/cacm/Greengard24e.bib,"Fri, 21 Feb 2025 00:00:00 +0100",,,,,,,,,"Seeking to understand when, and how, emergence takes place."
DBLP:journals/ieicetd/DongDI24,article,An Automated Multi-Phase Facilitation Agent Based on {LLM},"Yihan Dong and
Shiyao Ding and
Takayuki Ito",2024,{IEICE} Trans. Inf. Syst.,,107,4,426--433,10.1587/TRANSINF.2023IHP0011,https://doi.org/10.1587/transinf.2023ihp0011,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/ieicetd/DongDI24.bib,"Mon, 03 Mar 2025 00:00:00 +0100",,,,,,,,,
DBLP:journals/ijcisys/WangLYTL24,article,"Parameter-Efficient Multi-classification Software Defect Detection
Method Based on Pre-trained LLMs","Xuanye Wang and
Lu Lu and
Zhanyu Yang and
Qingyan Tian and
Haisha Lin",2024,Int. J. Comput. Intell. Syst.,,17,1,152,10.1007/S44196-024-00551-3,https://doi.org/10.1007/s44196-024-00551-3,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/ijcisys/WangLYTL24.bib,"Mon, 09 Dec 2024 00:00:00 +0100",,,,,,,,,"Software Defect Detection (SDD) has always been critical to the development life cycle. A stable defect detection system can not only alleviate the workload of software testers but also enhance the overall efficiency of software development. Researchers have recently proposed various artificial intelligence-based SDD methods and achieved significant advancements. However, these methods still exhibit limitations in terms of reliability and usability. Therefore, we introduce MSDD-(IA)3, a novel framework leveraging the pre-trained CodeT5+ and (IA)3 for parameter-efficient multi-classification SDD. This framework constructs a detection model based on pre-trained CodeT5+ to generate code representations while capturing defect-prone features. Considering the high overhead of pre-trained LLMs, we injects (IA)3 vectors into specific layers, where only these injected parameters are updated to reduce the training cost. Furthermore, leveraging the properties of the pre-trained CodeT5+, we design a novel feature sequence that enriches the input data through the combination of source code with Natural Language (NL)-based expert metrics. Our experimental results on 64K real-world Python snippets show that MSDD-(IA)3 demonstrates superior performance compared to state-of-the-art SDD methods, including PM2-CNN, in terms of F1-weighted, Recall-weighted, Precision-weighted, and Matthews Correlation Coefficient. Notably, the training parameters of MSDD-(IA)3 are only 0.04% of those of the original CodeT5+. Our experimental data and code can be available at (https://gitee.com/wxyzjp123/msdd-ia3/)."
DBLP:journals/jss/LuJCPC24,article,"{GRACE:} Empowering LLM-based software vulnerability detection with
graph structure and in-context learning","Guilong Lu and
Xiaolin Ju and
Xiang Chen and
Wenlong Pei and
Zhilong Cai",2024,J. Syst. Softw.,,212,,112031,10.1016/J.JSS.2024.112031,https://doi.org/10.1016/j.jss.2024.112031,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/jss/LuJCPC24.bib,"Fri, 31 May 2024 01:00:00 +0200",,,,,,,,,
DBLP:journals/network/MekracheKV24,article,"Intent-Based Management of Next-Generation Networks: an LLM-Centric
Approach","Abdelkader Mekrache and
Adlen Ksentini and
Christos V. Verikoukis",2024,{IEEE} Netw.,,38,5,29--36,10.1109/MNET.2024.3420120,https://doi.org/10.1109/MNET.2024.3420120,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/network/MekracheKV24.bib,"Thu, 03 Oct 2024 01:00:00 +0200",,,,,,,,,"Intent-Based Networking (IBN) management has emerged as an alternative approach to simplify network configuration and management by abstracting the complexities of low-level configurations. Existing IBN solutions typically rely on human-readable structures like JSON or YAML to define Intents, which still require expertise in understanding these structures. A natural evolution of IBN is to use natural language instead of defined structures. However, this approach introduces complexities related to natural language understanding. Fortunately, Large Language Models (LLMs) offer a promising solution. In this paper: (i) We propose a novel LLM-centric Intent Life-Cycle (LC) management architecture designed to configure and manage network services using natural language. The architecture spans the complete Intent LC, encompassing decomposition, translation, negotiation, activation, and assurance; (ii) We identify key open issues and challenges related to IBN within our proposed architecture; (iii) We demonstrate the effectiveness of the architecture by developing a component within the EURECOM 5G facility [1], leveraging LLMs to implement the essential Intent LC procedures; (iv) We validate the proposed system through realworld deployment, showcasing its capability to define, decompose, translate, and activate Intents using natural language."
DBLP:journals/pvldb/LiZZ24,article,{LLM} for Data Management,"Guoliang Li and
Xuanhe Zhou and
Xinyang Zhao",2024,Proc. {VLDB} Endow.,,17,12,4213--4216,10.14778/3685800.3685838,https://www.vldb.org/pvldb/vol17/p4213-li.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/pvldb/LiZZ24.bib,"Thu, 19 Dec 2024 00:00:00 +0100",,,,,,,,,"Machine learning techniques have been verified to be effective in optimizing data management systems and are widely researched in recent years. However, traditional small-sized ML models often struggle to generalize to new scenarios, and have limited context understanding ability (e.g., inputting discrete features only). The emergence of LLMs offers a promising solution to these challenges. LLMs have been trained over a vast number of scenarios and tasks and acquire human-competitive capabilities like context understanding and summarization, which can be highly beneficial for data management tasks (e.g., natural language based data analytics). In this tutorial, we present how to utilize LLMs to optimize data management systems and review new techniques for addressing these technical challenges, including hallucination of LLMs, high cost of interacting with LLMs, and low accuracy for processing complicated tasks. First, we discuss retrieval augmented generation (RAG) techniques to address the hallucination problem. Second, we present vector database techniques to improve the latency. Third, we present LLM agent techniques for processing complicated tasks by generating multi-round pipelines. We also showcase some real-world data management scenarios that can be well optimized by LLMs, including query rewrite, database diagnosis and data analytics. Finally, we summarize some open research challenges."
DBLP:journals/softx/Ozturk24,article,XCompress: {LLM} assisted Python-based text compression toolkit,"Emir {\""{O}}zt{\""{u}}rk",2024,SoftwareX,,27,,101847,10.1016/J.SOFTX.2024.101847,https://doi.org/10.1016/j.softx.2024.101847,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/softx/Ozturk24.bib,"Thu, 22 Aug 2024 01:00:00 +0200",,,,,,,,,
DBLP:journals/sosym/CamaraBT24,article,"Towards standarized benchmarks of LLMs in software modeling tasks:
a conceptual framework","Javier C{\'{a}}mara and
Lola Burgue{\~{n}}o and
Javier Troya",2024,Softw. Syst. Model.,,23,6,1309--1318,10.1007/S10270-024-01206-9,https://doi.org/10.1007/s10270-024-01206-9,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/sosym/CamaraBT24.bib,"Wed, 01 Jan 2025 00:00:00 +0100",,,,,,,,,"The integration of Large Language Models (LLMs) in software modeling tasks presents both opportunities and challenges. This Expert Voice addresses a significant gap in the evaluation of these models, advocating for the need for standardized benchmarking frameworks. Recognizing the potential variability in prompt strategies, LLM outputs, and solution space, we propose a conceptual framework to assess their quality in software model generation. This framework aims to pave the way for standardization of the benchmarking process, ensuring consistent and objective evaluation of LLMs in software modeling. Our conceptual framework is illustrated using UML class diagrams as a running example."
DBLP:journals/stt/Farago24,article,Acceptance test-driven {LLM} development,David Farag{\'{o}},2024,Softwaretechnik-Trends,,44,2,13--17,,https://fb-swt.gi.de/fileadmin/FB/SWT/Softwaretechnik-Trends/Verzeichnis/Band\_44\_Heft\_2/2\_TAV49\_Farago.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/stt/Farago24.bib,"Mon, 31 Mar 2025 01:00:00 +0200",,,,,,,,,
DBLP:journals/stt/HartensteinS24,article,"KI-gest{\""{u}}tzte Modernisierung von Altanwendungen: Anwendungsfelder
von LLMs im Software Reengineering","Sandro Hartenstein and
Andreas Schmietendorf",2024,Softwaretechnik-Trends,,44,2,44--45,,https://fb-swt.gi.de/fileadmin/FB/SWT/Softwaretechnik-Trends/Verzeichnis/Band\_44\_Heft\_2/WSRE2024\_10\_Hartenstein.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/stt/HartensteinS24.bib,"Wed, 21 Aug 2024 01:00:00 +0200",,,,,,,,,
DBLP:journals/stt/QuanteW24,article,Chances and Challenges of LLM-based Software Reengineering,"Jochen Quante and
Matthias Woehrle",2024,Softwaretechnik-Trends,,44,2,46--47,,https://fb-swt.gi.de/fileadmin/FB/SWT/Softwaretechnik-Trends/Verzeichnis/Band\_44\_Heft\_2/WSRE2024\_11\_Quante.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/stt/QuanteW24.bib,"Wed, 21 Aug 2024 01:00:00 +0200",,,,,,,,,
DBLP:journals/tlt/SongZX24,article,"A High-Quality Generation Approach for Educational Programming Projects
Using {LLM}","Tian Song and
Hang Zhang and
Yijia Xiao",2024,{IEEE} Trans. Learn. Technol.,,17,,2296--2309,10.1109/TLT.2024.3499751,https://doi.org/10.1109/TLT.2024.3499751,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tlt/SongZX24.bib,"Sun, 22 Dec 2024 00:00:00 +0100",,,,,,,,,"High-quality programming projects for education are critically required in teaching. However, it is hard to develop those projects efficiently and artificially constrained by the lecturers' experience and background. The recent popularity of large language models (LLMs) has led to a great number of applications in the field of education, but concerns persist that the output might be unreliable when dealing with intricate requirements. In this study, we design a customized role-based agent (CRBA), which can be configured for different roles specializing in specific areas of expertise, making the LLM yield content of higher specialization. An iterative architecture of multi-CRBAs is proposed to generate multistep projects, where CRBAs automatically criticize and optimize the LLM's intermediate outputs to enhance quality. We propose ten evaluation metrics across three aspects to assess project quality through expert grading. Further, we conduct an A/B test among 60 undergraduate students in a programming course and collect their feedback through a questionnaire. According to the students' rating results, the LLM-generated projects have comparable performance to man-made ones in terms of project description, learning step setting, assistance to students, and overall project quality. This study effectively integrates LLM into educational scenarios and enhances the efficiency of creating high-quality and practical programming exercises for lecturers."
DBLP:journals/tse/FakhouryNSCL24,article,"LLM-Based Test-Driven Interactive Code Generation: User Study and
Empirical Evaluation","Sarah Fakhoury and
Aaditya Naik and
Georgios Sakkas and
Saikat Chakraborty and
Shuvendu K. Lahiri",2024,{IEEE} Trans. Software Eng.,,50,9,2254--2268,10.1109/TSE.2024.3428972,https://doi.org/10.1109/TSE.2024.3428972,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tse/FakhouryNSCL24.bib,"Thu, 03 Oct 2024 01:00:00 +0200",,,,,,,,,"Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, given NL is informal, it does not lend easily to checking that the generated code correctly satisfies the user intent. In this paper, we propose a novel interactive workflow TiCoder for guided intent clarification (i.e., partial formalization) through tests to support the generation of more accurate code suggestions. Through a mixed methods user study with 15 programmers, we present an empirical evaluation of the effectiveness of the workflow to improve code generation accuracy. We find that participants using the proposed workflow are significantly more likely to correctly evaluate AI generated code, and report significantly less task-induced cognitive load. Furthermore, we test the potential of the workflow at scale with four different state-of-the-art LLMs on two python datasets, using an idealized proxy for a user feedback. We observe an average absolute improvement of 45.97% in the pass@1 code generation accuracy for both datasets and across all LLMs within 5 user interactions, in addition to the automatic generation of accompanying unit tests."
DBLP:journals/tse/LiHJZHSBL24,article,"Exploring the Effectiveness of LLMs in Automated Logging Statement
Generation: An Empirical Study","Yichen Li and
Yintong Huo and
Zhihan Jiang and
Renyi Zhong and
Pinjia He and
Yuxin Su and
Lionel C. Briand and
Michael R. Lyu",2024,{IEEE} Trans. Software Eng.,,50,12,3188--3207,10.1109/TSE.2024.3475375,https://doi.org/10.1109/TSE.2024.3475375,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tse/LiHJZHSBL24.bib,"Wed, 08 Jan 2025 00:00:00 +0100",,,,,,,,,"Automated logging statement generation supports developers in documenting critical software runtime behavior. While substantial recent research has focused on retrieval-based and learning-based methods, results suggest they fail to provide appropriate logging statements in real-world complex software. Given the great success in natural language generation and programming language comprehension, large language models (LLMs) might help developers generate logging statements, but this has not yet been investigated. To fill the gap, this paper performs the first study on exploring LLMs for logging statement generation. We first build a logging statement generation dataset, LogBench, with two parts: (1) LogBench-O: 3,870 methods with 6,849 logging statements collected from GitHub repositories, and (2) LogBench-T: the transformed unseen code from LogBench-O. Then, we leverage LogBench to evaluate the effectiveness and generalization capabilities (using LogBench-T) of 13 top-performing LLMs, from 60M to 405B parameters. In addition, we examine the performance of these LLMs against classical retrieval-based and machine learning-based logging methods from the era preceding LLMs. Specifically, we evaluate the logging effectiveness of LLMs by studying their ability to determine logging ingredients and the impact of prompts and external program information. We further evaluate LLM's logging generalization capabilities using unseen data (LogBench-T) derived from code transformation techniques. While existing LLMs deliver decent predictions on logging levels and logging variables, our study indicates that they only achieve a maximum BLEU score of 0.249, thus calling for improvements. The paper also highlights the importance of prompt constructions and external factors (e.g., programming contexts and code comments) for LLMs‚Äô logging performance. In addition, we observed that existing LLMs show a significant performance drop (8.2%-16.2% decrease) when dealing with logging unseen code, revealing their unsatisfactory generalization capabilities. Based on these findings, we identify five implications and provide practical advice for future logging research. Our empirical analysis discloses the limitations of current logging approaches while showcasing the potential of LLM-based logging tools, and provides actionable guidance for building more practical models."
DBLP:journals/tse/YinNW24,article,Multitask-Based Evaluation of Open-Source {LLM} on Software Vulnerability,"Xin Yin and
Chao Ni and
Shaohua Wang",2024,{IEEE} Trans. Software Eng.,,50,11,3071--3087,10.1109/TSE.2024.3470333,https://doi.org/10.1109/TSE.2024.3470333,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tse/YinNW24.bib,"Sun, 22 Dec 2024 00:00:00 +0100",,,,,,,,,"This paper proposes a pipeline for quantitatively evaluating interactive Large Language Models (LLMs) using publicly available datasets. We carry out an extensive technical evaluation of LLMs using Big-Vul covering four different common software vulnerability tasks. This evaluation assesses the multi-tasking capabilities of LLMs based on this dataset. We find that the existing state-of-the-art approaches and pre-trained Language Models (LMs) are generally superior to LLMs in software vulnerability detection. However, in software vulnerability assessment and location, certain LLMs (e.g., CodeLlama and WizardCoder) have demonstrated superior performance compared to pre-trained LMs, and providing more contextual information can enhance the vulnerability assessment capabilities of LLMs. Moreover, LLMs exhibit strong vulnerability description capabilities, but their tendency to produce excessive output significantly weakens their performance compared to pre-trained LMs. Overall, though LLMs perform well in some aspects, they still need improvement in understanding the subtle differences in code vulnerabilities and the ability to describe vulnerabilities to fully realize their potential. Our evaluation pipeline provides valuable insights into the capabilities of LLMs in handling software vulnerabilities."
DBLP:conf/ACMdis/BhatSG24,inproceedings,"Do LLMs Meet the Needs of Software Tutorial Writers? Opportunities
and Design Implications","Avinash Bhat and
Disha Shrivastava and
Jin L. C. Guo",2024,,"Designing Interactive Systems Conference, {DIS} 2024, {IT} University
of Copenhagen, Denmark, July 1-5, 2024",,,,10.1145/3643834.3660692,https://doi.org/10.1145/3643834.3660692,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ACMdis/BhatSG24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Anna Vallg{\aa}rda and
Li J{\""{o}}nsson and
Jonas Fritsch and
Sarah Fdili Alaoui and
Christopher A. Le Dantec",,,,,,,"Creating software tutorials involves developing accurate code examples and explanatory text that engages and informs the reader. Large Language Models (LLMs) demonstrate a strong capacity to generate both text and code, but their potential to assist tutorial writing is unknown. By interviewing and observing seven experienced writers using OpenAI playground as an exploration environment, we uncover design opportunities for leveraging LLMs in software tutorial writing. Our findings reveal background research, resource creation, and maintaining quality standards as critical areas where LLMs could significantly assist writers. We observe how tutorial writers generated tutorial content while exploring LLMs‚Äô capabilities, formulating prompts, verifying LLM outputs, and reflecting on interaction goals and strategies. Our observation highlights that the unpredictability of LLM outputs and unintuitive interface design contributed to skepticism about LLM‚Äôs utility. Informed by these results, we contribute recommendations for designing LLM-based tutorial writing tools to mitigate usability challenges and harness LLMs‚Äô full potential."
DBLP:conf/IEEEscc/TruongVP24,inproceedings,"On Coordinating LLMs and Platform Knowledge for Software Modernization
and New Developments","Hong Linh Truong and
Maja Vukovic and
Raju Pavuluri",2024,,"{IEEE} International Conference on Software Services Engineering,
{SSE} 2024, Shenzhen, China, July 7-13, 2024",,,188--193,10.1109/SSE62657.2024.00036,https://doi.org/10.1109/SSE62657.2024.00036,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/IEEEscc/TruongVP24.bib,"Mon, 03 Mar 2025 00:00:00 +0100",{IEEE},,,,,,,,"Emerging generative and fine-tuning LLMs services have been widely benchmarked and used for various software development tasks. These LLMs services are powerful but have different output qualities for software development tasks and may not be able to deal with complex development tasks in edge-cloud software modernization and new developments due to their generative capabilities and lack of up-ro-date (domain) knowledge. Many queries and solutions related to target platforms, deploy-ment configurations, policies, data regulation, observability, to name just a few, are not well integrated with these LLMs, but are accessed by the developer through other sources. In this work, we discuss situations where the gaps between the needs and the offerings from LLMs can be compensated by Platform Knowledge, which captures knowledge about, e.g., software, service and infrastructure catalogs, architectural decision records and code patterns. We propose COLLMS - a framework for coordinating LLMs services and Platform Knowledge. At the starting point of the framework, we will discuss challenges for achieving the coordination centered around Platform Knowledge, LLMs management and integration, quality-aware coordination of LLMs, and observability and knowledge updating."
DBLP:conf/acl-clinicalnlp/BannourAV24,inproceedings,"Team NLPeers at Chemotimelines 2024: Evaluation of two timeline extraction
methods, can generative {LLM} do it all or is smaller model fine-tuning
still relevant ?","Nesrine Bannour and
Judith Jeyafreeda Andrew and
Marc Vincent",2024,,"Proceedings of the 6th Clinical Natural Language Processing Workshop,
ClinicalNLP@NAACL 2024, Mexico City, Mexico, June 21, 2024",,,406--416,10.18653/V1/2024.CLINICALNLP-1.39,https://doi.org/10.18653/v1/2024.clinicalnlp-1.39,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/acl-clinicalnlp/BannourAV24.bib,"Mon, 03 Mar 2025 00:00:00 +0100",Association for Computational Linguistics,"Tristan Naumann and
Asma Ben Abacha and
Steven Bethard and
Kirk Roberts and
Danielle S. Bitterman",,,,,,,"This paper presents our two deep learning-based approaches to participate in subtask 1 of the Chemotimelines 2024 Shared task. The first uses a fine-tuning strategy on a relatively small general domain Masked Language Model (MLM) model, with additional normalization steps obtained using a simple Large Language Model (LLM) prompting technique. The second is an LLM-based approach combining advanced automated prompt search with few-shot in-context learning using the DSPy framework.Our results confirm the continued relevance of the smaller MLM fine-tuned model. It also suggests that the automated few-shot LLM approach can perform close to the fine-tuning-based method without extra LLM normalization and be advantageous under scarce data access conditions. We finally hint at the possibility to choose between lower training examples or lower computing resources requirements when considering both methods."
DBLP:conf/acl/ChenWMP0024,inproceedings,When is Tree Search Useful for {LLM} Planning? It Depends on the Discriminator,"Ziru Chen and
Michael White and
Raymond J. Mooney and
Ali Payani and
Yu Su and
Huan Sun",2024,,"Proceedings of the 62nd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand,
August 11-16, 2024",,,13659--13678,10.18653/V1/2024.ACL-LONG.738,https://doi.org/10.18653/v1/2024.acl-long.738,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/acl/ChenWMP0024.bib,"Tue, 24 Sep 2024 01:00:00 +0200",Association for Computational Linguistics,"Lun{-}Wei Ku and
Andre Martins and
Vivek Srikumar",,,,,,,
DBLP:conf/acl/VermaCSWOK24,inproceedings,"Cross-Modal Projection in Multimodal LLMs Doesn't Really Project Visual
Attributes to Textual Space","Gaurav Verma and
Minje Choi and
Kartik Sharma and
Jamelle Watson{-}Daniels and
Sejoon Oh and
Srijan Kumar",2024,,"Proceedings of the 62nd Annual Meeting of the Association for Computational
Linguistics, {ACL} 2024 - Short Papers, Bangkok, Thailand, August
11-16, 2024",,,657--664,10.18653/V1/2024.ACL-SHORT.60,https://doi.org/10.18653/v1/2024.acl-short.60,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/acl/VermaCSWOK24.bib,"Tue, 30 Sep 2025 01:00:00 +0200",Association for Computational Linguistics,"Lun{-}Wei Ku and
Andre Martins and
Vivek Srikumar",,,,,,,"Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable general-purpose conversations about images with the language modality. As off-the-shelf MLLMs may have limited capabilities on images from domains like dermatology and agriculture, they must be fine-tuned to unlock domain-specific applications. The prevalent architecture of current open-source MLLMs comprises two major modules: an image-language (cross-modal) projection network and a large language model. It is desirable to understand the roles of these two modules in modeling domain-specific visual attributes to inform the design of future models and streamline the interpretability efforts on the current models. To this end, via experiments on 4 datasets and under 2 fine-tuning settings, we find that as the MLLM is fine-tuned, it indeed gains domain-specific visual capabilities, but the updates do not lead to the projection extracting relevant domain-specific visual attributes. Our results indicate that the domain-specific visual attributes are modeled by the LLM, even when only the projection is fine-tuned. Through this study, we offer a potential reinterpretation of the role of cross-modal projections in MLLM architectures. Project webpage: https://claws-lab.github.io/projection-in-MLLMs/"
DBLP:conf/acl/YehudaMBWRK24,inproceedings,"InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated
Answers","Yakir Yehuda and
Itzik Malkiel and
Oren Barkan and
Jonathan Weill and
Royi Ronen and
Noam Koenigstein",2024,,"Proceedings of the 62nd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand,
August 11-16, 2024",,,9333--9347,10.18653/V1/2024.ACL-LONG.506,https://doi.org/10.18653/v1/2024.acl-long.506,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/acl/YehudaMBWRK24.bib,"Tue, 01 Apr 2025 01:00:00 +0200",Association for Computational Linguistics,"Lun{-}Wei Ku and
Andre Martins and
Vivek Srikumar",,,,,,,"Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 87% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy of 81%, all without relying on external knowledge."
DBLP:conf/acsac/ShimmiSSOR24,inproceedings,"Software Vulnerability Detection Using {LLM:} Does Additional Information
Help?","Samiha Shimmi and
Yash Saini and
Mark Schaefer and
Hamed Okhravi and
Mona Rahimi",2024,,"Annual Computer Security Applications Conference, {ACSAC} 2024 - Workshops,
Honolulu, HI, USA, December 9-10, 2024",,,216--223,10.1109/ACSACW65225.2024.00031,https://doi.org/10.1109/ACSACW65225.2024.00031,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/acsac/ShimmiSSOR24.bib,"Fri, 25 Apr 2025 10:48:16 +0200",{IEEE},,,,,,,,"Unlike conventional machine learning (ML) or deep learning (DL) methods, Large Language Models (LLM) possess the ability to tackle complex tasks through intricate chains of reasoning, a facet often overlooked in existing work on vulnerability detection. Nevertheless, these models have demonstrated variable performance when presented with different prompts (inputs), motivating a surge of research into prompt engineering ‚Äì the process of optimizing prompts to enhance their performance. This paper studies different prompt settings (zero-shot and few-shot) when using LLMs for software vulnerability detection. Our exploration involves harnessing the power of both natural language (NL) unimodal and NL-PL (programming language) bimodal models within the prompt engineering process. Experimental results indicate that LLM, when provided only with source code or zero-shot prompts, tends to classify most code snippets as vulnerable, resulting in unacceptably high recall. These findings suggest that, despite their advanced capabilities, LLMs may not inherently possess the knowledge for vulnerability detection tasks. However, few-shot learning benefits from additional domain-specific knowledge, offering a promising direction for future research in optimizing LLMs for vulnerability detection."
DBLP:conf/apsipa/0010KI24,inproceedings,"{LLM} as decoder: Investigating Lattice-based Speech Recognition Hypotheses
Rescoring Using {LLM}","Sheng Li and
Yuka Ko and
Akinori Ito",2024,,"Asia Pacific Signal and Information Processing Association Annual
Summit and Conference, {APSIPA} {ASC} 2024, Macau, December 3-6, 2024",,,1--5,10.1109/APSIPAASC63619.2025.10848752,https://doi.org/10.1109/APSIPAASC63619.2025.10848752,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/apsipa/0010KI24.bib,"Tue, 05 Aug 2025 01:00:00 +0200",{IEEE},,,,,,,,
DBLP:conf/aspdac/WanHLYWZC24,inproceedings,"Invited Paper: Software/Hardware Co-design for {LLM} and Its Application
for Design Verification","Lily Jiaxin Wan and
Yingbing Huang and
Yuhong Li and
Hanchen Ye and
Jinghua Wang and
Xiaofan Zhang and
Deming Chen",2024,,"Proceedings of the 29th Asia and South Pacific Design Automation Conference,
{ASPDAC} 2024, Incheon, Korea, January 22-25, 2024",,,435--441,10.1109/ASP-DAC58780.2024.10473893,https://doi.org/10.1109/ASP-DAC58780.2024.10473893,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/aspdac/WanHLYWZC24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{IEEE},,,,,,,,
DBLP:conf/asplos/PatelCZGWMB24,inproceedings,Characterizing Power Management Opportunities for LLMs in the Cloud,"Pratyush Patel and
Esha Choukse and
Chaojie Zhang and
{\'{I}}{\~{n}}igo Goiri and
Brijesh Warrier and
Nithish Mahalingam and
Ricardo Bianchini",2024,,"Proceedings of the 29th {ACM} International Conference on Architectural
Support for Programming Languages and Operating Systems, Volume 3,
{ASPLOS} 2024, La Jolla, CA, USA, 27 April 2024- 1 May 2024",,,207--222,10.1145/3620666.3651329,https://doi.org/10.1145/3620666.3651329,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/asplos/PatelCZGWMB24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Rajiv Gupta and
Nael B. Abu{-}Ghazaleh and
Madan Musuvathi and
Dan Tsafrir",,,,,,,
DBLP:conf/bibm/SilveiraDAF24,inproceedings,"Combining knowledge graphs and LLMs for hazardous chemical information
management and reuse","Marcos Da Silveira and
Louis Deladiennee and
Kheira Acem and
Oona Freudenthal",2024,,"{IEEE} International Conference on Bioinformatics and Biomedicine,
{BIBM} 2024, Lisbon, Portugal, December 3-6, 2024",,,6766--6773,10.1109/BIBM62325.2024.10821991,https://doi.org/10.1109/BIBM62325.2024.10821991,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/bibm/SilveiraDAF24.bib,"Tue, 04 Feb 2025 00:00:00 +0100",{IEEE},"Mario Cannataro and
Huiru Jane Zheng and
Lin Gao and
Jianlin Cheng and
Jo{\~{a}}o Lu{\'{\i}}s de Miranda and
Ester Zumpano and
Xiaohua Hu and
Young{-}Rae Cho and
Taesung Park",,,,,,,
DBLP:conf/bpm/BrzychczyKS24,inproceedings,"Enhancement of Low-Level Event Abstraction with Large Language Models
(LLMs)","Edyta Brzychczy and
Krzysztof Kluza and
Leszek Szala",2024,,"Business Process Management Workshops - {BPM} 2024 International Workshops,
Krakow, Poland, September 1-6, 2024, Revised Selected Papers",534,,209--220,10.1007/978-3-031-78666-2\_16,https://doi.org/10.1007/978-3-031-78666-2\_16,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/bpm/BrzychczyKS24.bib,"Fri, 07 Mar 2025 00:00:00 +0100",Springer,"Katarzyna Gdowska and
Mar{\'{\i}}a Teresa G{\'{o}}mez{-}L{\'{o}}pez and
Jana{-}Rebecca Rehse",Lecture Notes in Business Information Processing,,,,,,
DBLP:conf/bpm/FournierLS24,inproceedings,Towards a Benchmark for Causal Business Process Reasoning with LLMs,"Fabiana Fournier and
Lior Limonad and
Inna Skarbovsky",2024,,"Business Process Management Workshops - {BPM} 2024 International Workshops,
Krakow, Poland, September 1-6, 2024, Revised Selected Papers",534,,233--246,10.1007/978-3-031-78666-2\_18,https://doi.org/10.1007/978-3-031-78666-2\_18,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/bpm/FournierLS24.bib,"Tue, 04 Mar 2025 00:00:00 +0100",Springer,"Katarzyna Gdowska and
Mar{\'{\i}}a Teresa G{\'{o}}mez{-}L{\'{o}}pez and
Jana{-}Rebecca Rehse",Lecture Notes in Business Information Processing,,,,,,
DBLP:conf/bpm/KopkeS24a,inproceedings,Efficient LLM-Based Conversational Process Modeling,"Julius K{\""{o}}pke and
Aya Safan",2024,,"Business Process Management Workshops - {BPM} 2024 International Workshops,
Krakow, Poland, September 1-6, 2024, Revised Selected Papers",534,,259--270,10.1007/978-3-031-78666-2\_20,https://doi.org/10.1007/978-3-031-78666-2\_20,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/bpm/KopkeS24a.bib,"Fri, 07 Mar 2025 00:00:00 +0100",Springer,"Katarzyna Gdowska and
Mar{\'{\i}}a Teresa G{\'{o}}mez{-}L{\'{o}}pez and
Jana{-}Rebecca Rehse",Lecture Notes in Business Information Processing,,,,,,
DBLP:conf/bpm/ShiraliSAS24,inproceedings,LLM-Based Event Abstraction and Integration for IoT-Sourced Logs,"Mohsen Shirali and
Mohammadreza Fani Sani and
Zahra Ahmadi and
Estefan{\'{\i}}a Serral",2024,,"Business Process Management Workshops - {BPM} 2024 International Workshops,
Krakow, Poland, September 1-6, 2024, Revised Selected Papers",534,,138--149,10.1007/978-3-031-78666-2\_11,https://doi.org/10.1007/978-3-031-78666-2\_11,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/bpm/ShiraliSAS24.bib,"Sat, 31 May 2025 01:00:00 +0200",Springer,"Katarzyna Gdowska and
Mar{\'{\i}}a Teresa G{\'{o}}mez{-}L{\'{o}}pez and
Jana{-}Rebecca Rehse",Lecture Notes in Business Information Processing,,,,,,
DBLP:conf/ccwc/CardenasPZZ24,inproceedings,"AutoHealth: Advanced LLM-Empowered Wearable Personalized Medical Butler
for Parkinson's Disease Management","Luis Cardenas and
Katherine Parajes and
Ming Zhu and
Shengjie Zhai",2024,,"14th {IEEE} Annual Computing and Communication Workshop and Conference,
{CCWC} 2024, Las Vegas, NV, USA, January 8-10, 2024",,,375--379,10.1109/CCWC60891.2024.10427622,https://doi.org/10.1109/CCWC60891.2024.10427622,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ccwc/CardenasPZZ24.bib,"Thu, 17 Oct 2024 01:00:00 +0200",{IEEE},"Rajashree Paul and
Arpita Kundu",,,,,,,
DBLP:conf/chi/HanPHODK24,inproceedings,"AscleAI: {A} LLM-based Clinical Note Management System for Enhancing
Clinician Productivity","Jiyeon Han and
Jimin Park and
Jinyoung Huh and
Uran Oh and
Jaeyoung Do and
Daehee Kim",2024,,"Extended Abstracts of the {CHI} Conference on Human Factors in Computing
Systems, {CHI} {EA} 2024, Honolulu, HI, USA, May 11-16, 2024",,,50:1--50:7,10.1145/3613905.3650784,https://doi.org/10.1145/3613905.3650784,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/chi/HanPHODK24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Florian 'Floyd' Mueller and
Penny Kyburz and
Julie R. Williamson and
Corina Sas",,,,,,,
DBLP:conf/chi/ZhangJLYDLW024,inproceedings,"""It's a Fair Game"", or Is It? Examining How Users Navigate Disclosure
Risks and Benefits When Using LLM-Based Conversational Agents","Zhiping Zhang and
Michelle Jia and
Hao{-}Ping (Hank) Lee and
Bingsheng Yao and
Sauvik Das and
Ada Lerner and
Dakuo Wang and
Tianshi Li",2024,,"Proceedings of the {CHI} Conference on Human Factors in Computing
Systems, {CHI} 2024, Honolulu, HI, USA, May 11-16, 2024",,,156:1--156:26,10.1145/3613904.3642385,https://doi.org/10.1145/3613904.3642385,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/chi/ZhangJLYDLW024.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Florian 'Floyd' Mueller and
Penny Kyburz and
Julie R. Williamson and
Corina Sas and
Max L. Wilson and
Phoebe O. Toups Dugas and
Irina Shklovski",,,,,,,
DBLP:conf/chiir/VolkerPKH24,inproceedings,"From Chat to Publication Management: Organizing your related work
using BibSonomy {\&} LLMs","Tom V{\""{o}}lker and
Jan Pfister and
Tobias Koopmann and
Andreas Hotho",2024,,"Proceedings of the 2024 {ACM} {SIGIR} Conference on Human Information
Interaction and Retrieval, {CHIIR} 2024, Sheffield, United Kingdom,
March 10-14, 2024",,,386--390,10.1145/3627508.3638298,https://doi.org/10.1145/3627508.3638298,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/chiir/VolkerPKH24.bib,"Sun, 19 Jan 2025 13:17:38 +0100",{ACM},"Paul D. Clough and
Morgan Harvey and
Frank Hopfgartner",,,,,,,
DBLP:conf/cig/ItoT24,inproceedings,"Demo Paper: {A} Game Agents Battle Driven by Free-Form Text Commands
Using Code-Generation {LLM} and Behavior Branch","Ray Ito and
Junichiro Takahashi",2024,,"{IEEE} Conference on Games, CoG 2024, Milan, Italy, August 5-8, 2024",,,1--2,10.1109/COG60054.2024.10645545,https://doi.org/10.1109/CoG60054.2024.10645545,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/cig/ItoT24.bib,"Wed, 11 Sep 2024 11:31:52 +0200",{IEEE},,,,,,,,
DBLP:conf/clic-it/BasileMS24,inproceedings,"{ITA-SENSE} - Evaluate LLMs' ability for ITAlian word {SENSE} disambiguation:
{A} {CALAMITA} Challenge","Pierpaolo Basile and
Elio Musacchio and
Lucia Siciliani",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/134\_calamita\_short.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/BasileMS24.bib,"Tue, 07 Jan 2025 17:28:28 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/BonaventuraSBMM24,inproceedings,"Is Explanation All You Need? An Expert Survey on LLM-generated Explanations
for Abusive Language Detection","Chiara Di Bonaventura and
Lucia Siciliani and
Pierpaolo Basile and
Albert Mero{\~{n}}o{-}Pe{\~{n}}uela and
Barbara McGillivray",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/33\_main\_long.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/BonaventuraSBMM24.bib,"Tue, 07 Jan 2025 00:00:00 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/CabessaHM24,inproceedings,"Argument Mining in BioMedicine: Zero-Shot, In-Context Learning and
Fine-tuning with LLMs","J{\'{e}}r{\'{e}}mie Cabessa and
Hugo Hernault and
Umer Mushtaq",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/15\_main\_long.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/CabessaHM24.bib,"Tue, 07 Jan 2025 00:00:00 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/CaponeAMBL24,inproceedings,"Lost in Disambiguation: How Instruction-Tuned LLMs Master Lexical
Ambiguity","Luca Capone and
Serena Auriemma and
Martina Miliani and
Alessandro Bondielli and
Alessandro Lenci",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/18\_main\_long.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/CaponeAMBL24.bib,"Tue, 07 Jan 2025 00:00:00 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/CiaccioDMV24,inproceedings,"Controllable Text Generation to Evaluate Linguistic Abilities of Italian
LLMs","Cristiano Ciaccio and
Felice Dell'Orletta and
Alessio Miaschi and
Giulia Venturi",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/26\_main\_long.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/CiaccioDMV24.bib,"Tue, 07 Jan 2025 00:00:00 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/HromeiCD024,inproceedings,"La Non Canonica L'hai Studiata? Exploring LLMs and Sentence Canonicity
in Italian","Claudiu D. Hromei and
Danilo Croce and
Rodolfo Delmonte and
Roberto Basili",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/51\_main\_long.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/HromeiCD024.bib,"Tue, 07 Jan 2025 00:00:00 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/LabrunaBBM24,inproceedings,"Are You a Good Assistant? Assessing {LLM} Trustability in Task-oriented
Dialogues","Tiziano Labruna and
Sofia Brenna and
Giovanni Bonetta and
Bernardo Magnini",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/55\_main\_long.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/LabrunaBBM24.bib,"Tue, 07 Jan 2025 00:00:00 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/LicariBBGLC24,inproceedings,"A Novel Multi-Step Prompt Approach for LLM-based Q{\&}As on Banking
Supervisory Regulation","Daniele Licari and
Canio Benedetto and
Praveen Bushipaka and
Alessandro De Gregorio and
Marco De Leonardis and
Tommaso Cucinotta",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/58\_main\_long.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/LicariBBGLC24.bib,"Tue, 07 Jan 2025 00:00:00 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/MoroniCMN24,inproceedings,Towards a More Comprehensive Evaluation for Italian LLMs,"Luca Moroni and
Simone Conia and
Federico Martelli and
Roberto Navigli",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/66\_main\_long.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/MoroniCMN24.bib,"Tue, 07 Jan 2025 00:00:00 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/OrlandoMCCBOFN24,inproceedings,"Minerva LLMs: The First Family of Large Language Models Trained from
Scratch on Italian Data","Riccardo Orlando and
Luca Moroni and
Pere{-}Llu{\'{\i}}s Huguet Cabot and
Simone Conia and
Edoardo Barba and
Sergio Orlandini and
Giuseppe Fiameni and
Roberto Navigli",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/76\_main\_long.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/OrlandoMCCBOFN24.bib,"Tue, 07 Jan 2025 00:00:00 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/PensaAEAG24,inproceedings,"{GITA4CALAMITA} - Evaluating the Physical Commonsense Understanding
of Italian LLMs in a Multi-layered Approach: {A} {CALAMITA} Challenge","Giulia Pensa and
Ekhi Azurmendi and
Julen Etxaniz and
Bego{\~{n}}a Altuna and
Itziar Gonzalez{-}Dios",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/127\_calamita\_long.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/PensaAEAG24.bib,"Tue, 07 Jan 2025 00:00:00 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/PolignanoGS24,inproceedings,"Unraveling the Enigma of {SPLIT} in Large-Language Models: The Unforeseen
Impact of System Prompts on LLMs with Dissociative Identity Disorder","Marco Polignano and
Marco de Gemmis and
Giovanni Semeraro",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/84\_main\_long.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/PolignanoGS24.bib,"Tue, 07 Jan 2025 00:00:00 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/RussodivitoGFO24,inproceedings,"{AI} vs. Human: Effectiveness of LLMs in Simplifying Italian Administrative
Documents","Marco Russodivito and
Vittorio Ganfi and
Giuliana Fiorentino and
Rocco Oliveto",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/91\_main\_long.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/RussodivitoGFO24.bib,"Tue, 07 Jan 2025 00:00:00 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/SartiCBN24,inproceedings,EurekaRebus - Verbalized Rebus Solving with LLMs: {A} {CALAMITA} Challenge,"Gabriele Sarti and
Tommaso Caselli and
Arianna Bisazza and
Malvina Nissim",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/132\_calamita\_long.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/SartiCBN24.bib,"Tue, 07 Jan 2025 00:00:00 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/SimonettiJV24,inproceedings,Subcategorization of Italian Verbs with LLMs and {T-PAS},"Luca Simonetti and
Elisabetta Jezek and
Guido Vetere",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/99\_main\_long.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/SimonettiJV24.bib,"Tue, 07 Jan 2025 00:00:00 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/ZeinalipourFZMG24,inproceedings,Harnessing LLMs for Educational Content-Driven Italian Crossword Generation,"Kamyar Zeinalipour and
Achille Fusco and
Asya Zanollo and
Marco Maggini and
Marco Gori",2024,,"Proceedings of the Tenth Italian Conference on Computational Linguistics
(CLiC-it 2024), Pisa, Italy, December 4-6, 2024",3878,,,,https://ceur-ws.org/Vol-3878/110\_main\_long.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/ZeinalipourFZMG24.bib,"Tue, 07 Jan 2025 00:00:00 +0100",CEUR-WS.org,"Felice Dell'Orletta and
Alessandro Lenci and
Simonetta Montemagni and
Rachele Sprugnoli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/compsac/WangNQZW24,inproceedings,"Characterizing Developers' Behaviors in {LLM} -Supported Software
Development","Wei Wang and
Huilong Ning and
Shuo Qian and
Gaowei Zhang and
Yi Wang",2024,,"48th {IEEE} Annual Computers, Software, and Applications Conference,
{COMPSAC} 2024, Osaka, Japan, July 2-4, 2024",,,1168--1177,10.1109/COMPSAC61105.2024.00156,https://doi.org/10.1109/COMPSAC61105.2024.00156,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/compsac/WangNQZW24.bib,"Tue, 10 Dec 2024 00:00:00 +0100",{IEEE},"Hossain Shahriar and
Hiroyuki Ohsaki and
Moushumi Sharmin and
Dave Towey and
A. K. M. Jahangir Alam Majumder and
Yoshiaki Hori and
Ji{-}Jiang Yang and
Michiharu Takemoto and
Nazmus Sakib and
Ryohei Banno and
Sheikh Iqbal Ahamed",,,,,,,
DBLP:conf/conext/ShajarianKA24,inproceedings,"Poster: Intelligent Network Management: RAG-Enhanced LLMs for Log
Analysis, Troubleshooting, and Documentation","Shaghayegh Shajarian and
Sajad Khorsandroo and
Mahmoud Abdelsalam",2024,,"Proceedings of the 20th International Conference on emerging Networking
EXperiments and Technologies, CoNEXT 2024 (Short Papers), Los Angeles,
CA, USA, December 9-12, 2024",,,27--28,10.1145/3680121.3699887,https://doi.org/10.1145/3680121.3699887,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/conext/ShajarianKA24.bib,"Fri, 02 May 2025 15:06:08 +0200",{ACM},"Vijay Gopalakrishnan and
Jia Wang and
Ihsan Ayyub Qazi and
Gareth Tyson",,,,,,,
DBLP:conf/csee/AzanzaPIG24,inproceedings,"Can LLMs Facilitate Onboarding Software Developers? An Ongoing Industrial
Case Study","Maider Azanza and
Juanan Pereira and
Arantza Irastorza and
Aritz Galdos",2024,,"36th International Conference on Software Engineering Education and
Training, CSEE{\&}T 2024, W{\""{u}}rzburg, Germany, July 29 - Aug.
1, 2024",,,1--6,10.1109/CSEET62301.2024.10662989,https://doi.org/10.1109/CSEET62301.2024.10662989,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/csee/AzanzaPIG24.bib,"Sat, 21 Sep 2024 14:04:18 +0200",{IEEE},,,,,,,,
DBLP:conf/csee/PereiraLGA24,inproceedings,"Leveraging Open Source LLMs for Software Engineering Education and
Training","Juanan Pereira and
Juan Miguel L{\'{o}}pez and
Xabier Garmendia and
Maider Azanza",2024,,"36th International Conference on Software Engineering Education and
Training, CSEE{\&}T 2024, W{\""{u}}rzburg, Germany, July 29 - Aug.
1, 2024",,,1--10,10.1109/CSEET62301.2024.10663055,https://doi.org/10.1109/CSEET62301.2024.10663055,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/csee/PereiraLGA24.bib,"Mon, 03 Mar 2025 00:00:00 +0100",{IEEE},,,,,,,,
DBLP:conf/cui/GuyreHSD24,inproceedings,"Prompt Engineering an {LLM} into Roleplaying a Management Coach: a
Short Guide by and for Non-NLP Experts","Melissa Guyre and
Liz Holland and
Nirva Shah and
Rahul R. Divekar",2024,,"{ACM} Conversational User Interfaces 2024, {CUI} 2024, Luxembourg,
July 8-10, 2024",,,49,10.1145/3640794.3665570,https://doi.org/10.1145/3640794.3665570,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/cui/GuyreHSD24.bib,"Mon, 08 Sep 2025 11:26:39 +0200",{ACM},"Mateusz Dubiel and
Luis A. Leiva and
Johanne R. Trippas and
Joel E. Fischer and
Ilaria Torre",,,,,,,
DBLP:conf/cvpr/ChaKMR24,inproceedings,Honeybee: Locality-Enhanced Projector for Multimodal {LLM},"Junbum Cha and
Wooyoung Kang and
Jonghwan Mun and
Byungseok Roh",2024,,"{IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
{CVPR} 2024, Seattle, WA, USA, June 16-22, 2024",,,13817--13827,10.1109/CVPR52733.2024.01311,https://doi.org/10.1109/CVPR52733.2024.01311,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/cvpr/ChaKMR24.bib,"Sun, 19 Jan 2025 13:39:05 +0100",{IEEE},,,,,,,,
DBLP:conf/dsaisc/SaizC24,inproceedings,"Combining LLMs and Simheuristics: An Application to the Project Portfolio
Selection Problem","Miguel Saiz and
Laura Calvet",2024,,"Decision Sciences - Second Decision Science Alliance International
Summer Conference, {DSA} {ISC} 2024, Valencia, Spain, June 6-7, 2024,
Proceedings, Part {I}",14778,,3--13,10.1007/978-3-031-78238-1\_1,https://doi.org/10.1007/978-3-031-78238-1\_1,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/dsaisc/SaizC24.bib,"Mon, 03 Mar 2025 00:00:00 +0100",Springer,"Angel A. Juan and
Javier Faulin and
David Lopez{-}Lopez",Lecture Notes in Computer Science,,,,,,
DBLP:conf/eamt/EschbachDymanusEBE24,inproceedings,"Exploring the Effectiveness of {LLM} Domain Adaptation for Business
{IT} Machine Translation","Johannes Eschbach{-}Dymanus and
Frank Essenberger and
Bianka Buschbeck and
Miriam Exel",2024,,"Proceedings of the 25th Annual Conference of the European Association
for Machine Translation (Volume 1), {EAMT} 2024, Sheffield, UK, June
24-27, 2024",,,610--622,,https://aclanthology.org/2024.eamt-1.51,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/eamt/EschbachDymanusEBE24.bib,"Tue, 01 Oct 2024 22:18:16 +0200",European Association for Machine Translation {(EAMT)},"Carolina Scarton and
Charlotte Prescott and
Chris Bayliss and
Chris Oakley and
Joanna Wright and
Stuart Wrigley and
Xingyi Song and
Edward Gow{-}Smith and
Rachel Bawden and
V{\'{\i}}ctor M. S{\'{a}}nchez{-}Cartagena and
Patrick Cadwell and
Ekaterina Lapshinova{-}Koltunski and
Vera Cabarr{\~{a}}o and
Konstantinos Chatzitheodorou and
Mary Nurminen and
Diptesh Kanojia and
Helena Moniz",,,,,,,
DBLP:conf/ease/AbeduAS24,inproceedings,"LLM-Based Chatbots for Mining Software Repositories: Challenges and
Opportunities","Samuel Abedu and
Ahmad Abdellatif and
Emad Shihab",2024,,"Proceedings of the 28th International Conference on Evaluation and
Assessment in Software Engineering, {EASE} 2024, Salerno, Italy, June
18-21, 2024",,,201--210,10.1145/3661167.3661218,https://doi.org/10.1145/3661167.3661218,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ease/AbeduAS24.bib,"Sun, 19 Jan 2025 13:15:22 +0100",{ACM},,,,,,,,
DBLP:conf/ease/Harman24,inproceedings,The Role of Software Measurement in Assured LLM-Based Software Engineering,Mark Harman,2024,,"Proceedings of the 28th International Conference on Evaluation and
Assessment in Software Engineering, {EASE} 2024, Salerno, Italy, June
18-21, 2024",,,4,10.1145/3661167.3661269,https://doi.org/10.1145/3661167.3661269,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ease/Harman24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},,,,,,,,
DBLP:conf/ease/KumarC24,inproceedings,"Code Summarization without Direct Access to Code - Towards Exploring
Federated LLMs for Software Engineering","Jahnavi Kumar and
Sridhar Chimalakonda",2024,,"Proceedings of the 28th International Conference on Evaluation and
Assessment in Software Engineering, {EASE} 2024, Salerno, Italy, June
18-21, 2024",,,100--109,10.1145/3661167.3661210,https://doi.org/10.1145/3661167.3661210,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ease/KumarC24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},,,,,,,,
DBLP:conf/ease/Vito24,inproceedings,Assessing healthcare software built using IoT and {LLM} technologies,Gabriele De Vito,2024,,"Proceedings of the 28th International Conference on Evaluation and
Assessment in Software Engineering, {EASE} 2024, Salerno, Italy, June
18-21, 2024",,,476--481,10.1145/3661167.3661202,https://doi.org/10.1145/3661167.3661202,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ease/Vito24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},,,,,,,,
DBLP:conf/ekaw/GodestARO24,inproceedings,Enhanced {LLM} for Smart Knowledge Management in Nuclear Industry,"Fr{\'{e}}d{\'{e}}ric Godest and
Mouna El Alaoui and
Victor Richet and
Malhomme Olivier",2024,,"Joint Proceedings of Posters, Demos, Workshops, and Tutorials of the
24th International Conference on Knowledge Engineering and Knowledge
Management {(EKAW-PDWT} 2024) co-located with 24th International Conference
on Knowledge Engineering and Knowledge Management {(EKAW} 2024), Amsterdam,
Netherlands, November 26-28, 2024",3967,,,,https://ceur-ws.org/Vol-3967/PD\_paper\_166.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ekaw/GodestARO24.bib,"Thu, 12 Jun 2025 16:38:15 +0200",CEUR-WS.org,"Carlos Badenes{-}Olmedo and
Inna Novalija and
Enrico Daga and
Lise Stork and
Reshmi Gopalakrishna Pillai and
Laurence Dierickx and
Benno Kruit and
Victoria Degeler and
Jo{\~{a}}o Moreira and
Bohui Zhang and
Reham Alharbi and
Yuan He and
Arianna Graciotti and
Alba Catalina Morales Tirado and
Valentina Presutti and
Enrico Motta",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/emnlp/EomSKNHKY24,inproceedings,Query-based Cross-Modal Projector Bolstering Mamba Multimodal {LLM},"SooHwan Eom and
Jay Shim and
Gwanhyeong Koo and
Haebin Na and
Mark Hasegawa{-}Johnson and
Sungwoong Kim and
Chang Dong Yoo",2024,,"Findings of the Association for Computational Linguistics: {EMNLP}
2024, Miami, Florida, USA, November 12-16, 2024",,,14158--14167,10.18653/V1/2024.FINDINGS-EMNLP.827,https://doi.org/10.18653/v1/2024.findings-emnlp.827,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/emnlp/EomSKNHKY24.bib,"Fri, 13 Jun 2025 01:00:00 +0200",Association for Computational Linguistics,"Yaser Al{-}Onaizan and
Mohit Bansal and
Yun{-}Nung Chen",,,,,,,
DBLP:conf/emnlp/LomshakovPSBLN24,inproceedings,ProConSuL: Project Context for Code Summarization with LLMs,"Vadim Lomshakov and
Andrey Podivilov and
Sergey Savin and
Oleg Baryshnikov and
Alena Lisevych and
Sergey I. Nikolenko",2024,,"Proceedings of the 2024 Conference on Empirical Methods in Natural
Language Processing: {EMNLP} 2024 - Industry Track, Miami, Florida,
USA, November 12-16, 2024",,,866--880,10.18653/V1/2024.EMNLP-INDUSTRY.65,https://doi.org/10.18653/v1/2024.emnlp-industry.65,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/emnlp/LomshakovPSBLN24.bib,"Fri, 13 Jun 2025 01:00:00 +0200",Association for Computational Linguistics,"Franck Dernoncourt and
Daniel Preotiuc{-}Pietro and
Anastasia Shimorina",,,,,,,
DBLP:conf/emnlp/MehrafarinEK24,inproceedings,"Reasoning or a Semblance of it? {A} Diagnostic Study of Transitive
Reasoning in LLMs","Houman Mehrafarin and
Arash Eshghi and
Ioannis Konstas",2024,,"Proceedings of the 2024 Conference on Empirical Methods in Natural
Language Processing, {EMNLP} 2024, Miami, FL, USA, November 12-16,
2024",,,11647--11662,10.18653/V1/2024.EMNLP-MAIN.650,https://doi.org/10.18653/v1/2024.emnlp-main.650,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/emnlp/MehrafarinEK24.bib,"Fri, 13 Jun 2025 01:00:00 +0200",Association for Computational Linguistics,"Yaser Al{-}Onaizan and
Mohit Bansal and
Yun{-}Nung Chen",,,,,,,
DBLP:conf/er/AliNB24,inproceedings,"Establishing Traceability Between Natural Language Requirements and
Software Artifacts by Combining {RAG} and LLMs","Syed Juned Ali and
Varun Naganathan and
Dominik Bork",2024,,"Conceptual Modeling - 43rd International Conference, {ER} 2024, Pittsburgh,
PA, USA, October 28-31, 2024, Proceedings",15238,,295--314,10.1007/978-3-031-75872-0\_16,https://doi.org/10.1007/978-3-031-75872-0\_16,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/er/AliNB24.bib,"Mon, 03 Mar 2025 00:00:00 +0100",Springer,"Wolfgang Maass and
Hyoil Han and
Hasan Yasar and
Nicholas J. Multari",Lecture Notes in Computer Science,,,,,,
DBLP:conf/esem/AguiarPCSLFG24,inproceedings,"Multi-language Software Development in the {LLM} Era: Insights from
Practitioners' Conversations with ChatGPT","Lucas Aguiar and
Matheus Paix{\~{a}}o and
Rafael Augusto Ferreira do Carmo and
Edson Soares and
Antonio Leal and
Matheus Freitas and
Eliakim Gama",2024,,"Proceedings of the 18th {ACM/IEEE} International Symposium on Empirical
Software Engineering and Measurement, {ESEM} 2024, Barcelona, Spain,
October 24-25, 2024",,,489--495,10.1145/3674805.3690755,https://doi.org/10.1145/3674805.3690755,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/esem/AguiarPCSLFG24.bib,"Mon, 03 Mar 2025 00:00:00 +0100",{ACM},"Xavier Franch and
Maya Daneva and
Silverio Mart{\'{\i}}nez{-}Fern{\'{a}}ndez and
Luigi Quaranta",,,,,,,
DBLP:conf/esorics/GuoPHTC24,inproceedings,"Outside the Comfort Zone: Analysing {LLM} Capabilities in Software
Vulnerability Detection","Yuejun Guo and
Constantinos Patsakis and
Qiang Hu and
Qiang Tang and
Fran Casino",2024,,"Computer Security - {ESORICS} 2024 - 29th European Symposium on Research
in Computer Security, Bydgoszcz, Poland, September 16-20, 2024, Proceedings,
Part {I}",14982,,271--289,10.1007/978-3-031-70879-4\_14,https://doi.org/10.1007/978-3-031-70879-4\_14,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/esorics/GuoPHTC24.bib,"Thu, 26 Sep 2024 09:28:03 +0200",Springer,"Joaqu{\'{\i}}n Garc{\'{\i}}a{-}Alfaro and
Rafal Kozik and
Michal Choras and
Sokratis K. Katsikas",Lecture Notes in Computer Science,,,,,,
DBLP:conf/fllm/MicheluttiEM0G24,inproceedings,"A Systematic Study on the Potentials and Limitations of LLM-assisted
Software Development","Chiara Michelutti and
Jens Eckert and
Milko Monecke and
Julian Klein and
Sabine Glesner",2024,,"2nd International Conference on Foundation and Large Language Models,
{FLLM} 2024, Dubai, United Arab Emirates, November 26-29, 2024",,,330--338,10.1109/FLLM63129.2024.10852455,https://doi.org/10.1109/FLLM63129.2024.10852455,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/fllm/MicheluttiEM0G24.bib,"Wed, 12 Mar 2025 15:04:23 +0100",{IEEE},,,,,,,,
DBLP:conf/fllm/OsorioACAHKABDZ24,inproceedings,"Keep it Local: Comparing Domain-Specific LLMs in Native and Machine
Translated Text using Parallel Corpora on Political Conflict","Javier Osorio and
Sultan Alsarra and
Amber Converse and
Afraa Alshammari and
Dagmar Heintze and
Latifur Khan and
Naif Alatrush and
Patrick T. Brandt and
Vito D'Orazio and
Niamat Zawad and
Mahrusa Billah",2024,,"2nd International Conference on Foundation and Large Language Models,
{FLLM} 2024, Dubai, United Arab Emirates, November 26-29, 2024",,,542--552,10.1109/FLLM63129.2024.10852489,https://doi.org/10.1109/FLLM63129.2024.10852489,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/fllm/OsorioACAHKABDZ24.bib,"Tue, 01 Apr 2025 01:00:00 +0200",{IEEE},,,,,,,,
DBLP:conf/gd/KolleggerEH24,inproceedings,"Knowledge Graph Builder - Constructing a Graph from Arbitrary Text
Using an {LLM} (Software Abstract)","Andreas Benno Kollegger and
Alexander Erdl and
Michael Hunger",2024,,"32nd International Symposium on Graph Drawing and Network Visualization,
{GD} 2024, September 18-20, 2024, Vienna, Austria",320,,61:1--61:2,10.4230/LIPICS.GD.2024.61,https://doi.org/10.4230/LIPIcs.GD.2024.61,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/gd/KolleggerEH24.bib,"Mon, 28 Oct 2024 16:46:06 +0100","Schloss Dagstuhl - Leibniz-Zentrum f{\""{u}}r Informatik","Stefan Felsner and
Karsten Klein",LIPIcs,,,,,,
DBLP:conf/globecom/ManiasCS24,inproceedings,"Semantic Routing for Enhanced Performance of LLM-Assisted Intent-Based
5G Core Network Management and Orchestration","Dimitrios Michael Manias and
Ali Chouman and
Abdallah Shami",2024,,"2024 {IEEE} Global Communications Conference, {GLOBECOM} 2024, Cape
Town, South Africa, December 8-12, 2024",,,2924--2929,10.1109/GLOBECOM52923.2024.10901065,https://doi.org/10.1109/GLOBECOM52923.2024.10901065,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/globecom/ManiasCS24.bib,"Thu, 10 Apr 2025 16:51:26 +0200",{IEEE},,,,,,,,
DBLP:conf/hpec/AppuswamyDTECAA24,inproceedings,"Breakthrough Low-Latency, High-Energy-Efficiency {LLM} Inference Performance
Using NorthPole","Rathinakumar Appuswamy and
Michael V. DeBole and
Brian Taba and
Steven K. Esser and
Andrew S. Cassidy and
Arnon Amir and
Alexander Andreopoulos and
Deepika Bablani and
Pallab Datta and
Jeffrey A. Kusnitz and
Nathaniel J. McClatchey and
Neil McGlohon and
Jeffrey L. McKinstry and
Tapan K. Nayak and
Daniel F. Smith and
Rafael Sousa and
Ignacio Terrizzano and
Filipp Akopyan and
Peter J. Carlson and
Rajamohan Gandhasri and
Guillaume Garreau and
Nelson M. Gonzalez and
Megumi Ito and
Jennifer L. Klamo and
Yutaka Y. Nakamura and
Carlos Ortega Otero and
William P. Risk and
Jun Sawada and
Kai Schleupen and
Jay Sivagnaname and
Matthew Stallone and
Takanori Ueda and
Myron D. Flickner and
John V. Arthur and
Rameswar Panda and
David D. Cox and
Dharmendra S. Modha",2024,,"{IEEE} High Performance Extreme Computing Conference, {HPEC} 2024,
Wakefield, MA, USA, September 23-27, 2024",,,1--8,10.1109/HPEC62836.2024.10938418,https://doi.org/10.1109/HPEC62836.2024.10938418,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/hpec/AppuswamyDTECAA24.bib,"Thu, 08 May 2025 14:17:33 +0200",{IEEE},,,,,,,,
DBLP:conf/icadl/WatanabeIM24,inproceedings,"Capabilities and Challenges of LLMs in Metadata Extraction from Scholarly
Papers","Yu Watanabe and
Koichiro Ito and
Shigeki Matsubara",2024,,"Sustainability and Empowerment in the Context of Digital Libraries
- 26th International Conference on Asia-Pacific Digital Libraries,
{ICADL} 2024, Bandar Sunway, Malaysia, December 4-6, 2024, Proceedings,
Part {I}",15493,,280--287,10.1007/978-981-96-0865-2\_23,https://doi.org/10.1007/978-981-96-0865-2\_23,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icadl/WatanabeIM24.bib,"Mon, 03 Mar 2025 00:00:00 +0100",Springer,"Gillian C. Oliver and
Viviane Frings{-}Hessami and
Jia Tina Du and
Taro Tezuka",Lecture Notes in Computer Science,,,,,,
DBLP:conf/icassp/Malkiel0YKBRK24,inproceedings,"{SEGLLM:} Topic-Oriented Call Segmentation Via LLM-Based Conversation
Synthesis","Itzik Malkiel and
Uri Alon and
Yakir Yehuda and
Shahar Keren and
Oren Barkan and
Royi Ronen and
Noam Koenigstein",2024,,"{IEEE} International Conference on Acoustics, Speech and Signal Processing,
{ICASSP} 2024, Seoul, Republic of Korea, April 14-19, 2024",,,11361--11365,10.1109/ICASSP48485.2024.10446156,https://doi.org/10.1109/ICASSP48485.2024.10446156,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icassp/Malkiel0YKBRK24.bib,"Thu, 01 May 2025 01:00:00 +0200",{IEEE},,,,,,,,
DBLP:conf/icbase/WenZWLL24,inproceedings,{TSM-LLM:} Task Scheduling Management System for Large Language Models,"Ziqiang Wen and
Guoping Zhu and
Yong Wang and
Haijun Luo and
Nianchao Liu",2024,,"5th International Conference on Big Data {\&} Artificial Intelligence
{\&} Software Engineering, {ICBASE} 2024, Wenzhou, China, September
20-22, 2024",,,463--467,10.1109/ICBASE63199.2024.10762029,https://doi.org/10.1109/ICBASE63199.2024.10762029,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icbase/WenZWLL24.bib,"Tue, 11 Mar 2025 10:10:03 +0100",{IEEE},,,,,,,,
DBLP:conf/iccS/ShaikWZCFSF24,inproceedings,"{S3LLM:} Large-Scale Scientific Software Understanding with LLMs Using
Source, Metadata, and Document","Kareem Shaik and
Dali Wang and
Weijian Zheng and
Qinglei Cao and
Heng Fan and
Peter Schwartz and
Yunhe Feng",2024,,"Computational Science - {ICCS} 2024 - 24th International Conference,
Malaga, Spain, July 2-4, 2024, Proceedings, Part {III}",14834,,222--230,10.1007/978-3-031-63759-9\_27,https://doi.org/10.1007/978-3-031-63759-9\_27,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iccS/ShaikWZCFSF24.bib,"Sun, 06 Oct 2024 01:00:00 +0200",Springer,"Leonardo Franco and
Cl{\'{e}}lia de Mulatier and
Maciej Paszynski and
Valeria V. Krzhizhanovskaya and
Jack J. Dongarra and
Peter M. A. Sloot",Lecture Notes in Computer Science,,,,,,
DBLP:conf/iccad/DongLH00YJ24,inproceedings,"Hierarchical Power Co-Optimization and Management for {LLM} Chiplet
Designs","Yanchi Dong and
Xueping Liu and
Xiaochen Hao and
Yun Liang and
Ru Huang and
Le Ye and
Tianyu Jia",2024,,"Proceedings of the 43rd {IEEE/ACM} International Conference on Computer-Aided
Design, {ICCAD} 2024, Newark Liberty International Airport Marriott,
NJ, USA, October 27-31, 2024",,,145:1--145:9,10.1145/3676536.3676740,https://doi.org/10.1145/3676536.3676740,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iccad/DongLH00YJ24.bib,"Fri, 09 May 2025 01:00:00 +0200",{ACM},"Jinjun Xiong and
Robert Wille",,,,,,,
DBLP:conf/iccbd/ChenH24,inproceedings,"Intelligent data governance: building an enterprise data management
system using {KG} and {LLM}","Hao Chen and
Jun Hou",2024,,"2024 International Conference on Cloud Computing and Big Data, {ICCBD}
2024, Dali, China, July 26-28, 2024",,,266--271,10.1145/3695080.3695127,https://doi.org/10.1145/3695080.3695127,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iccbd/ChenH24.bib,"Thu, 09 Oct 2025 01:00:00 +0200",{ACM},,,,,,,,
DBLP:conf/icml/Zhao0CWAT24,inproceedings,GaLore: Memory-Efficient {LLM} Training by Gradient Low-Rank Projection,"Jiawei Zhao and
Zhenyu Zhang and
Beidi Chen and
Zhangyang Wang and
Anima Anandkumar and
Yuandong Tian",2024,,"Forty-first International Conference on Machine Learning, {ICML} 2024,
Vienna, Austria, July 21-27, 2024",,,,,https://openreview.net/forum?id=hYHsrKDiX7,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icml/Zhao0CWAT24.bib,"Mon, 02 Sep 2024 16:45:29 +0200",OpenReview.net,,,,,,,,
DBLP:conf/icmla/WeitlHarmsHL24,inproceedings,Using LLMs to Establish Implicit User Sentiment of Software Desirability,"Sherri Weitl{-}Harms and
John D. Hastings and
Jonah Lum",2024,,"International Conference on Machine Learning and Applications, {ICMLA}
2024, Miami, FL, USA, December 18-20, 2024",,,1645--1650,10.1109/ICMLA61862.2024.00254,https://doi.org/10.1109/ICMLA61862.2024.00254,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icmla/WeitlHarmsHL24.bib,"Thu, 01 May 2025 01:00:00 +0200",{IEEE},"M. Arif Wani and
Plamen Angelov and
Feng Luo and
Mitsunori Ogihara and
Xintao Wu and
Radu{-}Emil Precup and
Ramin Ramezani and
Xiaowei Gu",,,,,,,
DBLP:conf/ics/LaiZPZ24,inproceedings,"{LCM:} LLM-focused Hybrid SPM-cache Architecture with Cache Management
for Multi-Core {AI} Accelerators","Chengtao Lai and
Zhongchun Zhou and
Akash Poptani and
Wei Zhang",2024,,"Proceedings of the 38th {ACM} International Conference on Supercomputing,
{ICS} 2024, Kyoto, Japan, June 4-7, 2024",,,62--73,10.1145/3650200.3656592,https://doi.org/10.1145/3650200.3656592,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ics/LaiZPZ24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Kenji Kise and
Valentina Salapura and
Murali Annavaram and
Ana Lucia Varbanescu",,,,,,,
DBLP:conf/icsa/JahicS24,inproceedings,State of Practice: LLMs in Software Engineering and Software Architecture,"Jasmin Jahic and
Ashkan Sami",2024,,"21st {IEEE} International Conference on Software Architecture, {ICSA}
2024 - Companion, Hyderabad, India, June 4-8, 2024",,,311--318,10.1109/ICSA-C63560.2024.00059,https://doi.org/10.1109/ICSA-C63560.2024.00059,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icsa/JahicS24.bib,"Tue, 01 Apr 2025 01:00:00 +0200",{IEEE},,,,,,,,
DBLP:conf/icse-apr/DiazdeArcayaLZT24,inproceedings,"Towards the Self-Healing of Infrastructure as Code Projects Using
Constrained {LLM} Technologies","Josu D{\'{\i}}az{-}de{-}Arcaya and
Juan L{\'{o}}pez{-}de{-}Armentia and
Gorka Z{\'{a}}rate and
Ana I. Torre{-}Bastida",2024,,"{IEEE/ACM} International Workshop on Automated Program Repair, APR@ICSE
2024, Lisbon, Portugal, April 20, 2024",,,1--4,10.1145/3643788.3648014,https://doi.org/10.1145/3643788.3648014,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icse-apr/DiazdeArcayaLZT24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{IEEE},,,,,,,,
DBLP:conf/icse/AlshahwanHHMSW24,inproceedings,Assured LLM-Based Software Engineering,"Nadia Alshahwan and
Mark Harman and
Inna Harper and
Alexandru Marginean and
Shubho Sengupta and
Eddy Wang",2024,,"2nd {IEEE/ACM} International Workshop on Interpretability, Robustness,
and Benchmarking in Neural Software Engineering, InteNSE@ICSE 2024,
Lisbon, Portugal, April 15, 2024",,,7--12,10.1145/3643661.3643953,https://doi.org/10.1145/3643661.3643953,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icse/AlshahwanHHMSW24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM/IEEE},,,,,,,,
DBLP:conf/icse/ImranCD24,inproceedings,"Uncovering the Causes of Emotions in Software Developer Communication
Using Zero-shot LLMs","Mia Mohammad Imran and
Preetha Chatterjee and
Kostadin Damevski",2024,,"Proceedings of the 46th {IEEE/ACM} International Conference on Software
Engineering, {ICSE} 2024, Lisbon, Portugal, April 14-20, 2024",,,182:1--182:13,10.1145/3597503.3639223,https://doi.org/10.1145/3597503.3639223,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icse/ImranCD24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},,,,,,,,
DBLP:conf/icse/SallouDP24,inproceedings,Breaking the Silence: the Threats of Using LLMs in Software Engineering,"June Sallou and
Thomas Durieux and
Annibale Panichella",2024,,"Proceedings of the 2024 {ACM/IEEE} 44th International Conference on
Software Engineering: New Ideas and Emerging Results, NIER@ICSE 2024,
Lisbon, Portugal, April 14-20, 2024",,,102--106,10.1145/3639476.3639764,https://doi.org/10.1145/3639476.3639764,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icse/SallouDP24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},,,,,,,,
DBLP:conf/ieeecai/OtalSC24,inproceedings,"LLM-Assisted Crisis Management: Building Advanced {LLM} Platforms
for Effective Emergency Response and Public Collaboration","Hakan T. Otal and
Eric Stern and
Muhammed Abdullah Canbaz",2024,,"{IEEE} Conference on Artificial Intelligence, {CAI} 2024, Singapore,
June 25-27, 2024",,,851--859,10.1109/CAI59869.2024.00159,https://doi.org/10.1109/CAI59869.2024.00159,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ieeecai/OtalSC24.bib,"Mon, 06 Oct 2025 14:30:41 +0200",{IEEE},,,,,,,,
DBLP:conf/ijcai/Jana24,inproceedings,NeuroSymbolic {LLM} for Mathematical Reasoning and Software Engineering,Prithwish Jana,2024,,"Proceedings of the Thirty-Third International Joint Conference on
Artificial Intelligence, {IJCAI} 2024, Jeju, South Korea, August 3-9,
2024",,,8492--8493,,https://www.ijcai.org/proceedings/2024/961,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ijcai/Jana24.bib,"Fri, 18 Oct 2024 10:53:17 +0200",ijcai.org,,,,,,,,
DBLP:conf/iotsms/IgnjatovicKCHN24,inproceedings,"Crisis Management in the Era of the IoT, Edge Computing, and LLMs","Drazen Ignjatovic and
Vasileios Karagiannis and
Aradina Chettakattu and
Denis Havlik and
Georg Neubauer",2024,,"11th International Conference on Internet of Things: Systems, Management
and Security, {IOTSMS} 2024, Malm{\""{o}}, Sweden, September 2-5, 2024",,,224--231,10.1109/IOTSMS62296.2024.10710254,https://doi.org/10.1109/IOTSMS62296.2024.10710254,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iotsms/IgnjatovicKCHN24.bib,"Sat, 06 Sep 2025 01:00:00 +0200",{IEEE},"Muhannad Quwaider and
Fahed Alkhabbas and
Yaser Jararweh",,,,,,,
DBLP:conf/issta/ShanH000Z24,inproceedings,"Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration
Errors via Logs","Shiwen Shan and
Yintong Huo and
Yuxin Su and
Yichen Li and
Dan Li and
Zibin Zheng",2024,,"Proceedings of the 33rd {ACM} {SIGSOFT} International Symposium on
Software Testing and Analysis, {ISSTA} 2024, Vienna, Austria, September
16-20, 2024",,,13--25,10.1145/3650212.3652106,https://doi.org/10.1145/3650212.3652106,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/issta/ShanH000Z24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Maria Christakis and
Michael Pradel",,,,,,,
DBLP:conf/issta/XueLT0L00024,inproceedings,"LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech
Software Acceptance Testing","Zhiyi Xue and
Liangguo Li and
Senyue Tian and
Xiaohong Chen and
Pingping Li and
Liangyu Chen and
Tingting Jiang and
Min Zhang",2024,,"Proceedings of the 33rd {ACM} {SIGSOFT} International Symposium on
Software Testing and Analysis, {ISSTA} 2024, Vienna, Austria, September
16-20, 2024",,,1643--1655,10.1145/3650212.3680388,https://doi.org/10.1145/3650212.3680388,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/issta/XueLT0L00024.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Maria Christakis and
Michael Pradel",,,,,,,
DBLP:conf/issta/YangTPYWKBJ24,inproceedings,"{CREF:} An LLM-Based Conversational Software Repair Framework for
Programming Tutors","Boyang Yang and
Haoye Tian and
Weiguo Pian and
Haoran Yu and
Haitao Wang and
Jacques Klein and
Tegawend{\'{e}} F. Bissyand{\'{e}} and
Shunfu Jin",2024,,"Proceedings of the 33rd {ACM} {SIGSOFT} International Symposium on
Software Testing and Analysis, {ISSTA} 2024, Vienna, Austria, September
16-20, 2024",,,882--894,10.1145/3650212.3680328,https://doi.org/10.1145/3650212.3680328,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/issta/YangTPYWKBJ24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Maria Christakis and
Michael Pradel",,,,,,,
DBLP:conf/iti2/IlarionovKDF24,inproceedings,Features of the Practical Use of {LLM} for Generating Quiz,"Oleh Ilarionov and
Hanna Krasovska and
Iryna Domanetska and
Olena Fedusenko",2024,,"Selected Papers of the {XI} International Scientific Conference ""Information
Technology and Implementation"" (IT{\&}I 2024). Conference Proceedings,
Kyiv, Ukraine, November 20-21, 2024",3909,,275--284,,https://ceur-ws.org/Vol-3909/Paper\_22.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iti2/IlarionovKDF24.bib,"Thu, 06 Mar 2025 17:26:58 +0100",CEUR-WS.org,"Anatoly Anisimov and
Vitaliy Snytyuk and
Aldrich Chris and
Andreas Pester and
Fr{\'{e}}d{\'{e}}ric Mallet and
Iurii Krak and
Nickolas Cogan and
Oleg Chertov and
Oleksandr Marchenko and
S{\'{a}}ndor Boz{\'{o}}ki and
Tom Needham and
Vitaliy Tsyganok and
Vladimir Vovk",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/iui/KhuranaSC24,inproceedings,"Why and When LLM-Based Assistants Can Go Wrong: Investigating the
Effectiveness of Prompt-Based Interactions for Software Help-Seeking","Anjali Khurana and
Hariharan Subramonyam and
Parmit K. Chilana",2024,,"Proceedings of the 29th International Conference on Intelligent User
Interfaces, {IUI} 2024, Greenville, SC, USA, March 18-21, 2024",,,288--303,10.1145/3640543.3645200,https://doi.org/10.1145/3640543.3645200,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/iui/KhuranaSC24.bib,"Mon, 15 Apr 2024 01:00:00 +0200",{ACM},,,,,,,,
DBLP:conf/kbse/AnandayuvarajCT24,inproceedings,{FAIL:} Analyzing Software Failures from the News Using LLMs,"Dharun Anandayuvaraj and
Matthew Campbell and
Arav Tewari and
James C. Davis",2024,,"Proceedings of the 39th {IEEE/ACM} International Conference on Automated
Software Engineering, {ASE} 2024, Sacramento, CA, USA, October 27
- November 1, 2024",,,506--518,10.1145/3691620.3695022,https://doi.org/10.1145/3691620.3695022,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/kbse/AnandayuvarajCT24.bib,"Mon, 03 Mar 2025 00:00:00 +0100",{ACM},"Vladimir Filkov and
Baishakhi Ray and
Minghui Zhou",,,,,,,
DBLP:conf/kbse/CinkuszC24,inproceedings,Towards LLM-augmented multiagent systems for agile software engineering,"Konrad Cinkusz and
Jaroslaw A. Chudziak",2024,,"Proceedings of the 39th {IEEE/ACM} International Conference on Automated
Software Engineering, {ASE} 2024, Sacramento, CA, USA, October 27
- November 1, 2024",,,2476--2477,10.1145/3691620.3695336,https://doi.org/10.1145/3691620.3695336,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/kbse/CinkuszC24.bib,"Mon, 03 Mar 2025 00:00:00 +0100",{ACM},"Vladimir Filkov and
Baishakhi Ray and
Minghui Zhou",,,,,,,
DBLP:conf/kbse/Sun24,inproceedings,Enhancing Software Design and Developer Experience Via LLMs,Simin Sun,2024,,"Proceedings of the 39th {IEEE/ACM} International Conference on Automated
Software Engineering, {ASE} 2024, Sacramento, CA, USA, October 27
- November 1, 2024",,,2498--2501,10.1145/3691620.3695606,https://doi.org/10.1145/3691620.3695606,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/kbse/Sun24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Vladimir Filkov and
Baishakhi Ray and
Minghui Zhou",,,,,,,
DBLP:conf/kes/NaimiBJSC24,inproceedings,"Automating Software Documentation: Employing LLMs for Precise Use
Case Description","Lahbib Naimi and
El Mahi Bouziane and
Abdeslam Jakimi and
Rachid Saadane and
Abdellah Chehri",2024,,"Knowledge-Based and Intelligent Information {\&} Engineering Systems:
Proceedings of the 28th International Conference KES-2024, Seville,
Spain, 11-13 September 2023",246,,1346--1354,10.1016/J.PROCS.2024.09.568,https://doi.org/10.1016/j.procs.2024.09.568,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/kes/NaimiBJSC24.bib,"Thu, 16 Jan 2025 11:13:12 +0100",Elsevier,"Carlos Toro and
Sebasti{\'{a}}n A. R{\'{\i}}os and
Robert J. Howlett and
Lakhmi C. Jain",Procedia Computer Science,,,,,,
DBLP:conf/kesamsta/MatsumotoNM24,inproceedings,"Human-AI-Collaboration {SECI} Model: The Knowledge Management Model
of the Experts' Tacit Knowledges with Augmented LLM-Based {AI}","Takashi Matsumoto and
Ryu Nishikawa and
Chikako Morimoto",2024,,"Agents and Multi-agent Systems: Technologies and Applications 2024
- Proceedings of 18th {KES} International Conference, {KES-AMSTA}
2024, Madeira, Portugal, 19-21 June 2024",406,,135--145,10.1007/978-981-97-6469-3\_12,https://doi.org/10.1007/978-981-97-6469-3\_12,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/kesamsta/MatsumotoNM24.bib,"Wed, 05 Mar 2025 14:21:11 +0100",Springer,"Gordan Jezic and
Yun{-}Heh Chen{-}Burger and
Mario Kusek and
Roman Sperka and
Robert J. Howlett and
Lakhmi C. Jain","Smart Innovation, Systems and Technologies",,,,,,
DBLP:conf/kolicalling/KorpimiesLL24,inproceedings,"Unrestricted Use of LLMs in a Software Project Course: Student Perceptions
on Learning and Impact on Course Performance","Kai Korpimies and
Antti Laaksonen and
Matti Luukkainen",2024,,"Proceedings of the 24th Koli Calling International Conference on Computing
Education Research, Koli Calling 2024, KoliFinland, November 12-17,
2024",,,23:1--23:7,10.1145/3699538.3699541,https://doi.org/10.1145/3699538.3699541,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/kolicalling/KorpimiesLL24.bib,"Sat, 30 Nov 2024 00:00:00 +0100",{ACM},"Juho Leinonen and
Andreas M{\""{u}}hling",,,,,,,
DBLP:conf/ksem/ChuorIH24,inproceedings,User Story Classification with Machine Learning and LLMs,"Porchourng Chuor and
Ashwin Ittoo and
Samedi Heng",2024,,"Knowledge Science, Engineering and Management - 17th International
Conference, {KSEM} 2024, Birmingham, UK, August 16-18, 2024, Proceedings,
Part {I}",14884,,161--175,10.1007/978-981-97-5492-2\_13,https://doi.org/10.1007/978-981-97-5492-2\_13,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ksem/ChuorIH24.bib,"Thu, 08 Aug 2024 08:55:14 +0200",Springer,"Cungeng Cao and
Huajun Chen and
Liang Zhao and
Junaid Arshad and
A. Taufiq Asyhari and
Yonghao Wang",Lecture Notes in Computer Science,,,,,,
DBLP:conf/llm4code/RamlerM0NH24,inproceedings,"Industrial Experience Report on AI-Assisted Coding in Professional
Software Development","Rudolf Ramler and
Michael Moser and
Lukas Fischer and
Markus Nissl and
Ren{\'{e}} Heinzl",2024,,LLM4CODE@ICSE,,,1--7,10.1145/3643795.3648377,https://doi.org/10.1145/3643795.3648377,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/llm4code/RamlerM0NH24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",,,,,,,,,
DBLP:conf/llm4code/RasnayakaWSI24,inproceedings,"An Empirical Study on Usage and Perceptions of LLMs in a Software
Engineering Project","Sanka Rasnayaka and
Guanlin Wang and
Ridwan Shariffdeen and
Ganesh Neelakanta Iyer",2024,,LLM4CODE@ICSE,,,111--118,10.1145/3643795.3648379,https://doi.org/10.1145/3643795.3648379,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/llm4code/RasnayakaWSI24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",,,,,,,,,
DBLP:conf/pci/ZosimadisS24,inproceedings,LLM-Enhanced Test Case Prioritization for Complex Software Systems,"Ilias Zosimadis and
Ioannis Stamelos",2024,,"Proceedings of the 28th Pan-Hellenic Conference on Progress in Computing
and Informatics, {PCI} 2024, AthensGreece, December 13-15, 2024",,,46--50,10.1145/3716554.3716561,https://doi.org/10.1145/3716554.3716561,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/pci/ZosimadisS24.bib,"Wed, 11 Jun 2025 01:00:00 +0200",{ACM},"Nikitas N. Karanikolas and
Iraklis Varlamis and
Kyriakos N. Sgarbas and
Dimitris Kalles and
Ioannis Voyiatzis",,,,,,,
DBLP:conf/prcv/WuKLHCKWH24,inproceedings,"{VS-LLM:} Visual-Semantic Depression Assessment Based on {LLM} for
Drawing Projection Test","Meiqi Wu and
Yaxuan Kang and
Xuchen Li and
Shiyu Hu and
Xiaotang Chen and
Yunfeng Kang and
Weiqiang Wang and
Kaiqi Huang",2024,,"Pattern Recognition and Computer Vision - 7th Chinese Conference,
{PRCV} 2024, Urumqi, China, October 18-20, 2024, Proceedings, Part
{IX}",15039,,232--246,10.1007/978-981-97-8692-3\_17,https://doi.org/10.1007/978-981-97-8692-3\_17,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/prcv/WuKLHCKWH24.bib,"Sat, 06 Sep 2025 01:00:00 +0200",Springer,"Zhouchen Lin and
Ming{-}Ming Cheng and
Ran He and
Kurban Ubul and
Wushouer Silamu and
Hongbin Zha and
Jie Zhou and
Cheng{-}Lin Liu",Lecture Notes in Computer Science,,,,,,
DBLP:conf/profes/HoriMHKYIH24,inproceedings,The Effects of Semantic Information on LLM-Based Program Repair,"Shota Hori and
Shinsuke Matsumoto and
Yoshiki Higo and
Shinji Kusumoto and
Kazuya Yasuda and
Shinji Itoh and
Phan Thi Thanh Huyen",2024,,"Product-Focused Software Process Improvement - 25th International
Conference, {PROFES} 2024, Tartu, Estonia, December 2-4, 2024, Proceedings",15452,,377--385,10.1007/978-3-031-78386-9\_28,https://doi.org/10.1007/978-3-031-78386-9\_28,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/profes/HoriMHKYIH24.bib,"Fri, 13 Dec 2024 11:37:28 +0100",Springer,"Dietmar Pfahl and
Javier Gonzalez{-}Huerta and
Jil Kl{\""{u}}nder and
Hina Anwar",Lecture Notes in Computer Science,,,,,,
DBLP:conf/pts/KhoeeYFFRP24,inproceedings,"GoNoGo: An Efficient LLM-Based Multi-agent System for Streamlining
Automotive Software Release Decision-Making","Arsham Gholamzadeh Khoee and
Yinan Yu and
Robert Feldt and
Andris Freimanis and
Patrick Andersson Rhodin and
Dhasarathy Parthasarathy",2024,,"Testing Software and Systems - 36th {IFIP} {WG} 6.1 International
Conference, {ICTSS} 2024, London, UK, October 30 - November 1, 2024,
Proceedings",15383,,30--45,10.1007/978-3-031-80889-0\_3,https://doi.org/10.1007/978-3-031-80889-0\_3,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/pts/KhoeeYFFRP24.bib,"Fri, 07 Mar 2025 00:00:00 +0100",Springer,"H{\'{e}}ctor D. Men{\'{e}}ndez and
Gema Bello Orgaz and
Pepita Barnard and
John Robert Bautista and
Arya Farahi and
Santanu Kumar Dash and
DongGyun Han and
Sophie Fortz and
V{\'{\i}}ctor Rodr{\'{\i}}guez{-}Fern{\'{a}}ndez",Lecture Notes in Computer Science,,,,,,
DBLP:conf/qrs/WangFLJY24,inproceedings,The Application of LLMs in the Analysis and Modeling of Software Requirements,"Zhipeng Wang and
Changxi Feng and
Longfei Liu and
Guotao Jiao and
Peng Ye",2024,,"24th {IEEE} International Conference on Software Quality, Reliability,
and Security, {QRS} - Companion, Cambridge, United Kingdom, July 1-5,
2024",,,1143--1153,10.1109/QRS-C63300.2024.00151,https://doi.org/10.1109/QRS-C63300.2024.00151,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/qrs/WangFLJY24.bib,"Mon, 02 Dec 2024 10:50:07 +0100",{IEEE},,,,,,,,
DBLP:conf/re/KrishnaGVJ24,inproceedings,Using LLMs in Software Requirements Specifications: An Empirical Evaluation,"Madhava Krishna and
Bhagesh Gaur and
Arsh Verma and
Pankaj Jalote",2024,,"32nd {IEEE} International Requirements Engineering Conference, {RE}
2024, Reykjavik, Iceland, June 24-28, 2024",,,475--483,10.1109/RE59067.2024.00056,https://doi.org/10.1109/RE59067.2024.00056,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/re/KrishnaGVJ24.bib,"Mon, 03 Mar 2025 00:00:00 +0100",{IEEE},"Grischa Liebel and
Irit Hadar and
Paola Spoletini",,,,,,,
DBLP:conf/re/LubosFTGMEL24,inproceedings,Leveraging LLMs for the Quality Assurance of Software Requirements,"Sebastian Lubos and
Alexander Felfernig and
Thi Ngoc Trang Tran and
Damian Garber and
Merfat El Mansi and
Seda Polat Erdeniz and
Viet{-}Man Le",2024,,"32nd {IEEE} International Requirements Engineering Conference, {RE}
2024, Reykjavik, Iceland, June 24-28, 2024",,,389--397,10.1109/RE59067.2024.00046,https://doi.org/10.1109/RE59067.2024.00046,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/re/LubosFTGMEL24.bib,"Tue, 01 Apr 2025 01:00:00 +0200",{IEEE},"Grischa Liebel and
Irit Hadar and
Paola Spoletini",,,,,,,
DBLP:conf/re/YaacovEW24a,inproceedings,Boosting LLM-Based Software Generation by Aligning Code with Requirements,"Tom Yaacov and
Achiya Elyasaf and
Gera Weiss",2024,,"32nd {IEEE} International Requirements Engineering Conference, {RE}
2024 - Workshops, Reykjavik, Iceland, June 24-25, 2024",,,301--305,10.1109/REW61692.2024.00045,https://doi.org/10.1109/REW61692.2024.00045,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/re/YaacovEW24a.bib,"Wed, 04 Sep 2024 21:11:46 +0200",{IEEE},,,,,,,,
DBLP:conf/rskt/ZhangZYW24,inproceedings,"Advancing {ITS} Applications with LLMs: {A} Survey on Traffic Management,
Transportation Safety, and Autonomous Driving","Dingkai Zhang and
Huanran Zheng and
Wenjing Yue and
Xiaoling Wang",2024,,"Rough Sets - International Joint Conference, {IJCRS} 2024, Halifax,
NS, Canada, May 17-20, 2024, Proceedings, Part {II}",14840,,295--309,10.1007/978-3-031-65668-2\_20,https://doi.org/10.1007/978-3-031-65668-2\_20,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/rskt/ZhangZYW24.bib,"Fri, 15 Aug 2025 01:00:00 +0200",Springer,"Mengjun Hu and
Chris Cornelis and
Yan Zhang and
Pawan Lingras and
Dominik Slezak and
JingTao Yao",Lecture Notes in Computer Science,,,,,,
DBLP:conf/sbqs/FalcaoC24,inproceedings,"Investigating Software Development Teams Members' Perceptions of Data
Privacy in the Use of Large Language Models (LLMs)","Fabiano Damasceno Sousa Falc{\~{a}}o and
Edna Dias Canedo",2024,,"Proceedings of the {XXIII} Brazilian Symposium on Software Quality,
{SBQS} 2024, Salvador, Bahia, Brazil, November 5-8, 2024",,,373--382,10.1145/3701625.3701675,https://doi.org/10.1145/3701625.3701675,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sbqs/FalcaoC24.bib,"Sat, 25 Jan 2025 00:00:00 +0100",{ACM},"Ivan Machado and
Jos{\'{e}} Carlos Maldonado and
Tayana Conte and
Edna Dias Canedo and
Johnny Marques and
Breno Bernard Nicolau de Fran{\c{c}}a and
Patr{\'{\i}}cia Matsubara and
Davi Viana and
S{\'{e}}rgio Soares and
Gleison Santos and
Larissa Rocha and
Bruno Gadelha and
Rodrigo Pereira dos Santos and
Ana Carolina Oran and
Adolfo Gustavo Serra Seca Neto",,,,,,,
DBLP:conf/scisisis/AgatsumaOTNIIIM24,inproceedings,"Building a Role-Play Interactive System using {LLM} for Health Guidance
Education","Shinjitsu Agatsuma and
Reon Ohashi and
Kazuya Tsubokura and
Yua Nishio and
Mai Ishikawa and
Niina Ito and
Fukuka Ito and
Shiori Minami and
Nao Takegawa and
Riko Nakamura and
Kana Yokoyama",2024,,"Joint 13th International Conference on Soft Computing and Intelligent
Systems and 25th International Symposium on Advanced Intelligent Systems,
SCIS{\&}ISIS 2024, Himeji, Japan, November 9-12, 2024",,,1--3,10.1109/SCISISIS61014.2024.10759994,https://doi.org/10.1109/SCISISIS61014.2024.10759994,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/scisisis/AgatsumaOTNIIIM24.bib,"Wed, 05 Mar 2025 10:42:42 +0100",{IEEE},,,,,,,,
DBLP:conf/setss/LiuDMDY24,inproceedings,"Automating Component-Based Embedded Software Construction via Formal
Synthesis and LLMs","Sirui Liu and
Wei Dong and
Tiecheng Ma and
Yanqi Dong and
Dong Yang",2024,,"Engineering Trustworthy Software Systems - 6th International School,
{SETSS} 2024, Chongqing, China, April 14-21, 2024, Tutorial Lectures",15584,,155--170,10.1007/978-981-96-4656-2\_7,https://doi.org/10.1007/978-981-96-4656-2\_7,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/setss/LiuDMDY24.bib,"Sat, 31 May 2025 01:00:00 +0200",Springer,"Jonathan P. Bowen and
Cl{\'{a}}udio Gomes and
Zhiming Liu",Lecture Notes in Computer Science,,,,,,
DBLP:conf/seuh/MeissnerSK024,inproceedings,"EvalQuiz - LLM-based Automated Generation of Self-Assessment Quizzes
in Software Engineering Education","Niklas Mei{\ss}ner and
Sandro Speth and
Julian Kieslinger and
Steffen Becker",2024,,"Software Engineering im Unterricht der Hochschulen, {SEUH} 2024, Linz,
Austria, February 29 - March 1, 2024",{P-346},,53--64,10.18420/SEUH2024\_04,https://doi.org/10.18420/seuh2024\_04,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/seuh/MeissnerSK024.bib,"Mon, 14 Oct 2024 17:07:53 +0200","Gesellschaft f{\""{u}}r Informatik e.V.","Axel Schmolitzky and
Stefan Klikovits",{LNI},,,,,,
DBLP:conf/sigcse/KirovaKLM24,inproceedings,"Software Engineering Education Must Adapt and Evolve for an {LLM}
Environment","Vassilka D. Kirova and
Cyril S. Ku and
Joseph R. Laracy and
Thomas J. Marlowe",2024,,"Proceedings of the 55th {ACM} Technical Symposium on Computer Science
Education, {SIGCSE} 2024, Volume 1, Portland, OR, USA, March 20-23,
2024",,,666--672,10.1145/3626252.3630927,https://doi.org/10.1145/3626252.3630927,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigcse/KirovaKLM24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Ben Stephenson and
Jeffrey A. Stone and
Lina Battestilli and
Samuel A. Rebelsky and
Libby Shoop",,,,,,,
DBLP:conf/sigcse/MirandaFSG24,inproceedings,LLM-based Individual Contribution Summarization in Software Projects,"Fabio Roberto de Miranda and
Rafael Corsi Ferr{\~{a}}o and
Diego Pavan Soler and
Marcelo Augusto Vieira Graglia",2024,,"Proceedings of the 2024 {ACM} Virtual Global Computing Education Conference
V. 2, {SIGCSE} Virtual 2024, Virtual Event, NC, USA, December 5-8,
2024",,,,10.1145/3649409.3691092,https://doi.org/10.1145/3649409.3691092,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigcse/MirandaFSG24.bib,"Thu, 26 Jun 2025 01:00:00 +0200",{ACM},"Mohsen Dorodchi and
Ming Zhang and
Stephen Cooper",,,,,,,
DBLP:conf/sigir/ZhaoWWTWR24,inproceedings,"Let Me Do It For You: Towards {LLM} Empowered Recommendation via Tool
Learning","Yuyue Zhao and
Jiancan Wu and
Xiang Wang and
Wei Tang and
Dingxian Wang and
Maarten de Rijke",2024,,"Proceedings of the 47th International {ACM} {SIGIR} Conference on
Research and Development in Information Retrieval, {SIGIR} 2024, Washington
DC, USA, July 14-18, 2024",,,1796--1806,10.1145/3626772.3657828,https://doi.org/10.1145/3626772.3657828,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigir/ZhaoWWTWR24.bib,"Sun, 19 Jan 2025 13:11:15 +0100",{ACM},"Grace Hui Yang and
Hongning Wang and
Sam Han and
Claudia Hauff and
Guido Zuccon and
Yi Zhang",,,,,,,
DBLP:conf/sigsoft/GoelHSGPBZR24,inproceedings,X-Lifecycle Learning for Cloud Incident Management using LLMs,"Drishti Goel and
Fiza Husain and
Aditya Singh and
Supriyo Ghosh and
Anjaly Parayil and
Chetan Bansal and
Xuchao Zhang and
Saravan Rajmohan",2024,,"Companion Proceedings of the 32nd {ACM} International Conference on
the Foundations of Software Engineering, {FSE} 2024, Porto de Galinhas,
Brazil, July 15-19, 2024",,,417--428,10.1145/3663529.3663861,https://doi.org/10.1145/3663529.3663861,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sigsoft/GoelHSGPBZR24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},Marcelo d'Amorim,,,,,,,
DBLP:conf/smartgridcomm/0002SHY24,inproceedings,Democratizing Energy Management with LLM-Assisted Optimization Autoformalism,"Ming Jin and
Bilgehan Sel and
Fnu Hardeep and
Wotao Yin",2024,,"{IEEE} International Conference on Communications, Control, and Computing
Technologies for Smart Grids, SmartGridComm 2024, Oslo, Norway, September
17-20, 2024",,,258--263,10.1109/SMARTGRIDCOMM60555.2024.10738100,https://doi.org/10.1109/SmartGridComm60555.2024.10738100,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/smartgridcomm/0002SHY24.bib,"Wed, 20 Nov 2024 17:39:32 +0100",{IEEE},,,,,,,,
DBLP:conf/sosp/StoicaSSZ0MMN24,inproceedings,"If At First You Don't Succeed, Try, Try, Again...? Insights and LLM-informed
Tooling for Detecting Retry Bugs in Software Systems","Bogdan Alexandru Stoica and
Utsav Sethi and
Yiming Su and
Cyrus Zhou and
Shan Lu and
Jonathan Mace and
Madanlal Musuvathi and
Suman Nath",2024,,"Proceedings of the {ACM} {SIGOPS} 30th Symposium on Operating Systems
Principles, {SOSP} 2024, Austin, TX, USA, November 4-6, 2024",,,63--78,10.1145/3694715.3695971,https://doi.org/10.1145/3694715.3695971,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/sosp/StoicaSSZ0MMN24.bib,"Sat, 30 Nov 2024 00:00:00 +0100",{ACM},"Emmett Witchel and
Christopher J. Rossbach and
Andrea C. Arpaci{-}Dusseau and
Kimberly Keeton",,,,,,,
DBLP:conf/uist/QianGSSN24,inproceedings,"{SHAPE-IT:} Exploring Text-to-Shape-Display for Generative Shape-Changing
Behaviors with LLMs","Wanli Qian and
Chenfeng Gao and
Anup Sathya and
Ryo Suzuki and
Ken Nakagaki",2024,,"Proceedings of the 37th Annual {ACM} Symposium on User Interface Software
and Technology, {UIST} 2024, Pittsburgh, PA, USA, October 13-16, 2024",,,118:1--118:29,10.1145/3654777.3676348,https://doi.org/10.1145/3654777.3676348,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/uist/QianGSSN24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Lining Yao and
Mayank Goel and
Alexandra Ion and
Pedro Lopes",,,,,,,
DBLP:conf/uist/ShiK24,inproceedings,"Pay Attention! Human-Centric Improvements of LLM-based Interfaces
for Assisting Software Test Case Development","Billy Shi and
Per Ola Kristensson",2024,,"Adjunct Proceedings of the 37th Annual {ACM} Symposium on User Interface
Software and Technology, {UIST} Adjunct 2024, Pittsburgh, PA, USA,
October 13-16, 2024",,,83:1--83:3,10.1145/3672539.3686341,https://doi.org/10.1145/3672539.3686341,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/uist/ShiK24.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Lining Yao and
Mayank Goel and
Alexandra Ion and
Pedro Lopes",,,,,,,
DBLP:conf/vecos/CohenP24,inproceedings,LLM-Based Scheme for Synthesis of Formal Verification Algorithms,"Itay Cohen and
Doron Peled",2024,,"Bridging the Gap Between {AI} and Reality - Second International Conference,
AISoLA 2024, Crete, Greece, October 30 - November 3, 2024, Proceedings",15217,,167--182,10.1007/978-3-031-75434-0\_11,https://doi.org/10.1007/978-3-031-75434-0\_11,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/vecos/CohenP24.bib,"Thu, 09 Jan 2025 20:21:42 +0100",Springer,Bernhard Steffen,Lecture Notes in Computer Science,,,,,,
DBLP:conf/vecos/PatilUN24,inproceedings,"Towards Specification-Driven LLM-Based Generation of Embedded Automotive
Software","Minal Suresh Patil and
Gustav Ung and
Mattias Nyberg",2024,,"Bridging the Gap Between {AI} and Reality - Second International Conference,
AISoLA 2024, Crete, Greece, October 30 - November 3, 2024, Proceedings",15217,,125--144,10.1007/978-3-031-75434-0\_9,https://doi.org/10.1007/978-3-031-75434-0\_9,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/vecos/PatilUN24.bib,"Thu, 09 Jan 2025 00:00:00 +0100",Springer,Bernhard Steffen,Lecture Notes in Computer Science,,,,,,
DBLP:conf/vl/Meem24,inproceedings,"Effective Integration and Use of Non-Development LLMs in Software
Development",Fairuz Nawer Meem,2024,,"2024 {IEEE} Symposium on Visual Languages and Human-Centric Computing
(VL/HCC), Liverpool, UK, September 2-6, 2024",,,374--375,10.1109/VL/HCC60511.2024.00053,https://doi.org/10.1109/VL/HCC60511.2024.00053,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/vl/Meem24.bib,"Tue, 22 Oct 2024 16:41:19 +0200",{IEEE},,,,,,,,
DBLP:conf/vldb/00010C24,inproceedings,"{LLM+KG:} Data Management Opportunities in Unifying Large Language
Models + Knowledge Graphs","Arijit Khan and
Tianxing Wu and
Xi Chen",2024,,"Proceedings of Workshops at the 50th International Conference on Very
Large Data Bases, {VLDB} 2024, Guangzhou, China, August 26-30, 2024",,,,,https://vldb.org/workshops/2024/proceedings/LLM+KG/LLM+KG-1.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/vldb/00010C24.bib,"Mon, 28 Jul 2025 01:00:00 +0200",VLDB.org,,,,,,,,
DBLP:conf/vtc/DuDLS0C24,inproceedings,"{LLM} for Complex Signal Processing in FPGA-based Software Defined
Radios: {A} Case Study on {FFT}","Yuyang Du and
Hongyu Deng and
Soung Chang Liew and
Yulin Shao and
Kexin Chen and
He Henry Chen",2024,,"100th {IEEE} Vehicular Technology Conference, {VTC} Fall 2024, Washington,
DC, USA, October 7-10, 2024",,,1--6,10.1109/VTC2024-FALL63153.2024.10757597,https://doi.org/10.1109/VTC2024-Fall63153.2024.10757597,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/vtc/DuDLS0C24.bib,"Wed, 30 Jul 2025 01:00:00 +0200",{IEEE},,,,,,,,
DBLP:conf/wipsce/Morales-Navarro24a,inproceedings,"""It's smart and it's stupid: "" Youth's conflicting perspectives on
LLMs' language comprehension and ethics","Luis Morales{-}Navarro and
Phillip Gao and
Eric Yang and
Yasmin B. Kafai",2024,,"Proceedings of the 19th WiPSCE Conference on Primary and Secondary
Computing Education Research, WiPSCE 2024, Munich, Germany, September
16-18, 2024",,,36:1--36:2,10.1145/3677619.3678131,https://doi.org/10.1145/3677619.3678131,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/wipsce/Morales-Navarro24a.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Tilman Michaeli and
Sue Sentance and
Nadine Bergner",,,,,,,
DBLP:conf/wmt/KudoDMFIONSYTKH24,inproceedings,"Document-level Translation with {LLM} Reranking: Team-J at {WMT} 2024
General Translation Task","Keito Kudo and
Hiroyuki Deguchi and
Makoto Morishita and
Ryo Fujii and
Takumi Ito and
Shintaro Ozaki and
Koki Natsumi and
Kai Sato and
Kazuki Yano and
Ryosuke Takahashi and
Subaru Kimura and
Tomomasa Hara and
Yusuke Sakai and
Jun Suzuki",2024,,"Proceedings of the Ninth Conference on Machine Translation, {WMT}
2024, Miami, FL, USA, November 15-16, 2024",,,210--226,10.18653/V1/2024.WMT-1.14,https://doi.org/10.18653/v1/2024.wmt-1.14,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/wmt/KudoDMFIONSYTKH24.bib,"Sat, 06 Sep 2025 01:00:00 +0200",Association for Computational Linguistics,"Barry Haddow and
Tom Kocmi and
Philipp Koehn and
Christof Monz",,,,,,,
DBLP:conf/www/ItoN24,inproceedings,"Tender Document Analyzer with the Combination of Supervised Learning
and LLM-based Improver","Tomoki Ito and
Shun Nakagawa",2024,,"Companion Proceedings of the {ACM} on Web Conference 2024, {WWW} 2024,
Singapore, Singapore, May 13-17, 2024",,,995--998,10.1145/3589335.3651233,https://doi.org/10.1145/3589335.3651233,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/www/ItoN24.bib,"Sun, 19 Jan 2025 13:10:18 +0100",{ACM},"Tat{-}Seng Chua and
Chong{-}Wah Ngo and
Roy Ka{-}Wei Lee and
Ravi Kumar and
Hady W. Lauw",,,,,,,
DBLP:conf/xaiit/CastellanoMSVZ24,inproceedings,"Using LLMs to explain AI-generated art classification via Grad-CAM
heatmaps","Giovanna Castellano and
Maria Grazia Miccoli and
Raffaele Scaringi and
Gennaro Vessio and
Gianluca Zaza",2024,,"Proceedings of the 5th Italian Workshop on Explainable Artificial
Intelligence, co-located with the 23rd International Conference of
the Italian Association for Artificial Intelligence, Bolzano, Italy,
November 26-27, 2024",3839,,65--74,,https://ceur-ws.org/Vol-3839/paper5.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/xaiit/CastellanoMSVZ24.bib,"Thu, 05 Dec 2024 17:16:11 +0100",CEUR-WS.org,"Marco Polignano and
Cataldo Musto and
Roberto Pellungrini and
Erasmo Purificato and
Giovanni Semeraro and
Mattia Setzu",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/xpu/AlliataSB24,inproceedings,"The {AI} Scrum Master: Using Large Language Models (LLMs) to Automate
Agile Project Management Tasks","Zorina Alliata and
Tanvi Singhal and
Andreea{-}Madalina Bozagiu",2024,,"Agile Processes in Software Engineering and Extreme Programming -
Workshops - {XP} 2024 Workshops, Bozen-Bolzano, Italy, June 4-7, 2024,
Revised Selected Papers",524,,110--122,10.1007/978-3-031-72781-8\_12,https://doi.org/10.1007/978-3-031-72781-8\_12,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/xpu/AlliataSB24.bib,"Mon, 03 Mar 2025 00:00:00 +0100",Springer,"Lodovica Marchesi and
Alfredo Goldman and
Maria Ilaria Lunesu and
Adam Przybylek and
Ademar Aguiar and
Lorraine Morgan and
Xiaofeng Wang and
Andrea Pinna",Lecture Notes in Business Information Processing,,,,,,
DBLP:conf/staf/2024w,proceedings,"Proceedings of the {STAF} 2024 Workshops: AgileMDE 2024, {LLM4MDE}
2024, and MeSS 2024 co-located with the International Conference on
Software Technologies: Applications and Foundations {(STAF} 2024)
Enschede, The Netherlands, July 8-11, 2024",,2024,,,3727,,,,https://ceur-ws.org/Vol-3727,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/staf/2024w.bib,"Fri, 26 Jul 2024 01:00:00 +0200",CEUR-WS.org,"Hessa Alfraihi and
Francesco Basciani and
Georgiana Caltais and
Nicolas Ferry and
Jos{\'{e}} Antonio Hern{\'{a}}ndez L{\'{o}}pez and
Ludovico Iovino and
Robbert Jongeling and
Stefan Klikovits and
Shekoufeh Kolahdouz Rahimi and
Riccardo Rubei and
Sobhan Yassipour Tehrani and
Javier Troya and
Mairieli Wessel and
Vadim Zaytsev",{CEUR} Workshop Proceedings,,,,,urn:nbn:de:0074-3727-1,
DBLP:data/11/CanedoF24d,misc,"Supplementary Material for ""Investigating Software Development Teams
Members' Perceptions of Data Privacy in the Use of Large Language
Models (LLMs)"" (Version 2)","Edna Dias Canedo and
Fabiano Damasceno Sousa Falc{\~{a}}o",2024,,,,,,10.5281/ZENODO.13139492,https://doi.org/10.5281/zenodo.13139492,Accessed on YYYY-MM-DD.,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/data/11/CanedoF24d.bib,"Thu, 26 Jun 2025 01:00:00 +0200",Zenodo,,,\url{https://doi.org/10.5281/zenodo.13139492},July,,,,
DBLP:data/11/CanedoF24c,misc,"Investigating the Perception of Brazilian Software Developers on Data
Privacy in the Use of Large Language Models (LLMs) (Version 3)","Edna Dias Canedo and
Fabiano Damasceno Sousa Falc{\~{a}}o",2024,,,,,,10.5281/ZENODO.12677054,https://doi.org/10.5281/zenodo.12677054,Accessed on YYYY-MM-DD.,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/data/11/CanedoF24c.bib,"Fri, 11 Jul 2025 01:00:00 +0200",Zenodo,,,\url{https://doi.org/10.5281/zenodo.12677054},July,,,,
DBLP:data/11/CanedoF24b,misc,"Investigating the Perception of Brazilian Software Developers on Data
Privacy in the Use of Large Language Models (LLMs) (Version 2)","Edna Dias Canedo and
Fabiano Damasceno Sousa Falc{\~{a}}o",2024,,,,,,10.5281/ZENODO.12661315,https://doi.org/10.5281/zenodo.12661315,Accessed on YYYY-MM-DD.,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/data/11/CanedoF24b.bib,"Fri, 11 Jul 2025 01:00:00 +0200",Zenodo,,,\url{https://doi.org/10.5281/zenodo.12661315},July,,,,
DBLP:data/11/FalcaoC24,misc,"Investigating the Perception of Brazilian Software Developers on Data
Privacy in the Use of Large Language Models (LLMs) (Version 1)","Fabiano Damasceno Sousa Falc{\~{a}}o and
Edna Dias Canedo",2024,,,,,,10.5281/ZENODO.12538512,https://doi.org/10.5281/zenodo.12538512,Accessed on YYYY-MM-DD.,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/data/11/FalcaoC24.bib,"Fri, 11 Jul 2025 01:00:00 +0200",Zenodo,,,\url{https://doi.org/10.5281/zenodo.12538512},June,,,,
DBLP:data/11/CanedoF24e,misc,"Investigating the Perception of Brazilian Software Developers on Data
Privacy in the Use of Large Language Models (LLMs) (Multiple Versions)","Edna Dias Canedo and
Fabiano Damasceno Sousa Falc{\~{a}}o",2024,,,,,,10.5281/ZENODO.12538511,https://doi.org/10.5281/zenodo.12538511,Accessed on YYYY-MM-DD.,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/data/11/CanedoF24e.bib,"Mon, 13 Oct 2025 11:13:34 +0200",Zenodo,,,\url{https://doi.org/10.5281/zenodo.12538511},,,,,
DBLP:data/11/CanedoF24a,misc,"Investigando a Percep{\c{c}}{\~{a}}o dos Desenvolvedores Brasileiros
de Software sobre Privacidade de Dados no uso de LLMs (Version 2)","Edna Dias Canedo and
Fabiano Damasceno Sousa Falc{\~{a}}o",2024,,,,,,10.5281/ZENODO.12599730,https://doi.org/10.5281/zenodo.12599730,Accessed on YYYY-MM-DD.,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/data/11/CanedoF24a.bib,"Fri, 11 Jul 2025 01:00:00 +0200",Zenodo,,,\url{https://doi.org/10.5281/zenodo.12599730},June,,,,
DBLP:data/11/CanedoF24,misc,"Investigando a Percep{\c{c}}{\~{a}}o dos Desenvolvedores Brasileiros
de Software sobre Privacidade de Dados no uso de LLMs (Version 1)","Edna Dias Canedo and
Fabiano Damasceno Sousa Falc{\~{a}}o",2024,,,,,,10.5281/ZENODO.12594274,https://doi.org/10.5281/zenodo.12594274,Accessed on YYYY-MM-DD.,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/data/11/CanedoF24.bib,"Fri, 11 Jul 2025 01:00:00 +0200",Zenodo,,,\url{https://doi.org/10.5281/zenodo.12594274},June,,,,
DBLP:data/11/CanedoF24f,misc,"Investigando a Percep{\~{A}}{\textsection}{\~{A}}{\textsterling}o
dos Desenvolvedores Brasileiros de Software sobre Privacidade de Dados
no uso de LLMs (Multiple Versions)","Edna Dias Canedo and
Fabiano Damasceno Sousa Falc{\~{a}}o",2024,,,,,,10.5281/ZENODO.12594273,https://doi.org/10.5281/zenodo.12594273,Accessed on YYYY-MM-DD.,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/data/11/CanedoF24f.bib,"Mon, 13 Oct 2025 11:13:34 +0200",Zenodo,,,\url{https://doi.org/10.5281/zenodo.12594273},,,,,
DBLP:data/11/PaivaC24a,misc,LLM-Generated Software Requirements from GitHub Issues (Multiple Versions),"Guilherme Pereira Paiva and
Edna Dias Canedo",2024-2025,,,,,,10.5281/ZENODO.12738577,https://doi.org/10.5281/zenodo.12738577,Accessed on YYYY-MM-DD.,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/data/11/PaivaC24a.bib,"Mon, 13 Oct 2025 11:13:34 +0200",Zenodo,,,\url{https://doi.org/10.5281/zenodo.12738577},,,,,
DBLP:journals/corr/abs-2401-06676,article,"{LLMRS:} Unlocking Potentials of LLM-Based Recommender Systems for
Software Purchase","Angela John and
Theophilus Aidoo and
Hamayoon Behmanush and
Irem B. Gunduz and
Hewan Shrestha and
Maxx Richard Rahman and
Wolfgang Maa{\ss}",2024,CoRR,,abs/2401.06676,,,10.48550/ARXIV.2401.06676,https://doi.org/10.48550/arXiv.2401.06676,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2401-06676.bib,"Tue, 07 May 2024 01:00:00 +0200",,,,,,2401.06676,arXiv,,
DBLP:journals/corr/abs-2401-07526,article,Editing Arbitrary Propositions in LLMs without Subject Labels,"Itai Feigenbaum and
Devansh Arpit and
Huan Wang and
Shelby Heinecke and
Juan Carlos Niebles and
Weiran Yao and
Caiming Xiong and
Silvio Savarese",2024,CoRR,,abs/2401.07526,,,10.48550/ARXIV.2401.07526,https://doi.org/10.48550/arXiv.2401.07526,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2401-07526.bib,"Sun, 04 Aug 2024 01:00:00 +0200",,,,,,2401.07526,arXiv,,
DBLP:journals/corr/abs-2401-09092,article,"BibSonomy Meets ChatLLMs for Publication Management: From Chat to
Publication Management: Organizing your related work using BibSonomy
{\&} LLMs","Tom V{\""{o}}lker and
Jan Pfister and
Tobias Koopmann and
Andreas Hotho",2024,CoRR,,abs/2401.09092,,,10.48550/ARXIV.2401.09092,https://doi.org/10.48550/arXiv.2401.09092,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2401-09092.bib,"Thu, 01 Feb 2024 00:00:00 +0100",,,,,,2401.09092,arXiv,,
DBLP:journals/corr/abs-2401-16186,article,"An Empirical Study on Usage and Perceptions of LLMs in a Software
Engineering Project","Sanka Rasnayaka and
Guanlin Wang and
Ridwan Shariffdeen and
Ganesh Neelakanta Iyer",2024,CoRR,,abs/2401.16186,,,10.48550/ARXIV.2401.16186,https://doi.org/10.48550/arXiv.2401.16186,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2401-16186.bib,"Tue, 05 Aug 2025 01:00:00 +0200",,,,,,2401.16186,arXiv,,
DBLP:journals/corr/abs-2402-02643,article,LLM-Enhanced Data Management,"Xuanhe Zhou and
Xinyang Zhao and
Guoliang Li",2024,CoRR,,abs/2402.02643,,,10.48550/ARXIV.2402.02643,https://doi.org/10.48550/arXiv.2402.02643,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2402-02643.bib,"Wed, 02 Oct 2024 01:00:00 +0200",,,,,,2402.02643,arXiv,,
DBLP:journals/corr/abs-2402-04380,article,Assured LLM-Based Software Engineering,"Nadia Alshahwan and
Mark Harman and
Inna Harper and
Alexandru Marginean and
Shubho Sengupta and
Eddy Wang",2024,CoRR,,abs/2402.04380,,,10.48550/ARXIV.2402.04380,https://doi.org/10.48550/arXiv.2402.04380,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2402-04380.bib,"Mon, 12 Feb 2024 00:00:00 +0100",,,,,,2402.0438,arXiv,,
DBLP:journals/corr/abs-2402-07442,article,"Game Agent Driven by Free-Form Text Command: Using LLM-based Code
Generation and Behavior Branch","Ray Ito and
Junichiro Takahashi",2024,CoRR,,abs/2402.07442,,,10.48550/ARXIV.2402.07442,https://doi.org/10.48550/arXiv.2402.07442,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2402-07442.bib,"Fri, 16 Feb 2024 00:00:00 +0100",,,,,,2402.07442,arXiv,,
DBLP:journals/corr/abs-2402-08030,article,"Why and When LLM-Based Assistants Can Go Wrong: Investigating the
Effectiveness of Prompt-Based Interactions for Software Help-Seeking","Anjali Khurana and
Hari Subramonyam and
Parmit K. Chilana",2024,CoRR,,abs/2402.08030,,,10.48550/ARXIV.2402.08030,https://doi.org/10.48550/arXiv.2402.08030,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2402-08030.bib,"Mon, 19 Feb 2024 00:00:00 +0100",,,,,,2402.0803,arXiv,,
DBLP:journals/corr/abs-2402-10067,article,LLM-based policy generation for intent-based management of applications,"Kristina Dzeparoska and
Jieyu Lin and
Ali Tizghadam and
Alberto Leon{-}Garcia",2024,CoRR,,abs/2402.10067,,,10.48550/ARXIV.2402.10067,https://doi.org/10.48550/arXiv.2402.10067,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2402-10067.bib,"Tue, 20 Feb 2024 00:00:00 +0100",,,,,,2402.10067,arXiv,,
DBLP:journals/corr/abs-2402-10890,article,When is Tree Search Useful for {LLM} Planning? It Depends on the Discriminator,"Ziru Chen and
Michael White and
Raymond J. Mooney and
Ali Payani and
Yu Su and
Huan Sun",2024,CoRR,,abs/2402.10890,,,10.48550/ARXIV.2402.10890,https://doi.org/10.48550/arXiv.2402.10890,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2402-10890.bib,"Mon, 26 Feb 2024 00:00:00 +0100",,,,,,2402.1089,arXiv,,
DBLP:journals/corr/abs-2402-10908,article,"LLM-Assisted Crisis Management: Building Advanced {LLM} Platforms
for Effective Emergency Response and Public Collaboration","Hakan T. Otal and
Muhammed Abdullah Canbaz",2024,CoRR,,abs/2402.10908,,,10.48550/ARXIV.2402.10908,https://doi.org/10.48550/arXiv.2402.10908,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2402-10908.bib,"Tue, 15 Oct 2024 01:00:00 +0200",,,,,,2402.10908,arXiv,,
DBLP:journals/corr/abs-2402-13220,article,"How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis
on Deceptive Prompts","Yusu Qian and
Haotian Zhang and
Yinfei Yang and
Zhe Gan",2024,CoRR,,abs/2402.13220,,,10.48550/ARXIV.2402.13220,https://doi.org/10.48550/arXiv.2402.13220,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2402-13220.bib,"Mon, 18 Aug 2025 01:00:00 +0200",,,,,,2402.1322,arXiv,,
DBLP:journals/corr/abs-2402-13518,article,"{RITFIS:} Robust input testing framework for LLMs-based intelligent
software","Mingxuan Xiao and
Yan Xiao and
Hai Dong and
Shunhui Ji and
Pengcheng Zhang",2024,CoRR,,abs/2402.13518,,,10.48550/ARXIV.2402.13518,https://doi.org/10.48550/arXiv.2402.13518,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2402-13518.bib,"Wed, 11 Dec 2024 00:00:00 +0100",,,,,,2402.13518,arXiv,,
DBLP:journals/corr/abs-2402-14261,article,Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming,"Anisha Agarwal and
Aaron Chan and
Shubham Chandel and
Jinu Jang and
Shaun Miller and
Roshanak Zilouchian Moghaddam and
Yevhen Mohylevskyy and
Neel Sundaresan and
Michele Tufano",2024,CoRR,,abs/2402.14261,,,10.48550/ARXIV.2402.14261,https://doi.org/10.48550/arXiv.2402.14261,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2402-14261.bib,"Fri, 22 Mar 2024 00:00:00 +0100",,,,,,2402.14261,arXiv,,
DBLP:journals/corr/abs-2402-14533,article,"Whose {LLM} is it Anyway? Linguistic Comparison and {LLM} Attribution
for GPT-3.5, {GPT-4} and Bard","Ariel Rosenfeld and
Teddy Lazebnik",2024,CoRR,,abs/2402.14533,,,10.48550/ARXIV.2402.14533,https://doi.org/10.48550/arXiv.2402.14533,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2402-14533.bib,"Fri, 22 Mar 2024 00:00:00 +0100",,,,,,2402.14533,arXiv,,
DBLP:journals/corr/abs-2402-16832,article,"Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual
Capabilities Without Richer Cross-Modal Projections","Gaurav Verma and
Minje Choi and
Kartik Sharma and
Jamelle Watson{-}Daniels and
Sejoon Oh and
Srijan Kumar",2024,CoRR,,abs/2402.16832,,,10.48550/ARXIV.2402.16832,https://doi.org/10.48550/arXiv.2402.16832,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2402-16832.bib,"Tue, 30 Sep 2025 01:00:00 +0200",,,,,,2402.16832,arXiv,,
DBLP:journals/corr/abs-2403-03507,article,GaLore: Memory-Efficient {LLM} Training by Gradient Low-Rank Projection,"Jiawei Zhao and
Zhenyu Zhang and
Beidi Chen and
Zhangyang Wang and
Anima Anandkumar and
Yuandong Tian",2024,CoRR,,abs/2403.03507,,,10.48550/ARXIV.2403.03507,https://doi.org/10.48550/arXiv.2403.03507,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2403-03507.bib,"Wed, 03 Apr 2024 01:00:00 +0200",,,,,,2403.03507,arXiv,,
DBLP:journals/corr/abs-2403-08429,article,Software Vulnerability and Functionality Assessment using LLMs,"Rasmus Ingemann Tuffveson Jensen and
Vali Tawosi and
Salwa Alamir",2024,CoRR,,abs/2403.08429,,,10.48550/ARXIV.2403.08429,https://doi.org/10.48550/arXiv.2403.08429,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2403-08429.bib,"Fri, 05 Apr 2024 01:00:00 +0200",,,,,,2403.08429,arXiv,,
DBLP:journals/corr/abs-2403-10588,article,"{S3LLM:} Large-Scale Scientific Software Understanding with LLMs using
Source, Metadata, and Document","Kareem Shaik and
Dali Wang and
Weijian Zheng and
Qinglei Cao and
Heng Fan and
Peter Schwartz and
Yunhe Feng",2024,CoRR,,abs/2403.10588,,,10.48550/ARXIV.2403.10588,https://doi.org/10.48550/arXiv.2403.10588,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2403-10588.bib,"Mon, 08 Apr 2024 01:00:00 +0200",,,,,,2403.10588,arXiv,,
DBLP:journals/corr/abs-2403-15852,article,When LLM-based Code Generation Meets the Software Development Process,"Feng Lin and
Dong Jae Kim and
Tse{-}Hsun Chen",2024,CoRR,,abs/2403.15852,,,10.48550/ARXIV.2403.15852,https://doi.org/10.48550/arXiv.2403.15852,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2403-15852.bib,"Wed, 22 May 2024 01:00:00 +0200",,,,,,2403.15852,arXiv,,
DBLP:journals/corr/abs-2403-16159,article,"Designing Child-Centric {AI} Learning Environments: Insights from
LLM-Enhanced Creative Project-Based Learning","Siyu Zha and
Yuehan Qiao and
Qingyu Hu and
Zhongsheng Li and
Jiangtao Gong and
Yingqing Xu",2024,CoRR,,abs/2403.16159,,,10.48550/ARXIV.2403.16159,https://doi.org/10.48550/arXiv.2403.16159,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2403-16159.bib,"Tue, 09 Apr 2024 01:00:00 +0200",,,,,,2403.16159,arXiv,,
DBLP:journals/corr/abs-2403-16362,article,AgentFL: Scaling LLM-based Fault Localization to Project-Level Context,"Yihao Qin and
Shangwen Wang and
Yiling Lou and
Jinhao Dong and
Kaixin Wang and
Xiaoling Li and
Xiaoguang Mao",2024,CoRR,,abs/2403.16362,,,10.48550/ARXIV.2403.16362,https://doi.org/10.48550/arXiv.2403.16362,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2403-16362.bib,"Wed, 14 May 2025 01:00:00 +0200",,,,,,2403.16362,arXiv,,
DBLP:journals/corr/abs-2404-00640,article,"Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration
Errors via Logs","Shiwen Shan and
Yintong Huo and
Yuxin Su and
Yichen Li and
Dan Li and
Zibin Zheng",2024,CoRR,,abs/2404.00640,,,10.48550/ARXIV.2404.00640,https://doi.org/10.48550/arXiv.2404.00640,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2404-00640.bib,"Tue, 27 Aug 2024 01:00:00 +0200",,,,,,2404.0064,arXiv,,
DBLP:journals/corr/abs-2404-01332,article,"Wait, It's All Token Noise? Always Has Been: Interpreting {LLM} Behavior
Using Shapley Value",Behnam Mohammadi,2024,CoRR,,abs/2404.01332,,,10.48550/ARXIV.2404.01332,https://doi.org/10.48550/arXiv.2404.01332,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2404-01332.bib,"Wed, 08 May 2024 01:00:00 +0200",,,,,,2404.01332,arXiv,,
DBLP:journals/corr/abs-2404-02056,article,Multitask-based Evaluation of Open-Source {LLM} on Software Vulnerability,"Xin Yin and
Chao Ni and
Shaohua Wang",2024,CoRR,,abs/2404.02056,,,10.48550/ARXIV.2404.02056,https://doi.org/10.48550/arXiv.2404.02056,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2404-02056.bib,"Mon, 29 Jul 2024 01:00:00 +0200",,,,,,2404.02056,arXiv,,
DBLP:journals/corr/abs-2404-03662,article,X-lifecycle Learning for Cloud Incident Management using LLMs,"Drishti Goel and
Fiza Husain and
Aditya Singh and
Supriyo Ghosh and
Anjaly Parayil and
Chetan Bansal and
Xuchao Zhang and
Saravan Rajmohan",2024,CoRR,,abs/2404.03662,,,10.48550/ARXIV.2404.03662,https://doi.org/10.48550/arXiv.2404.03662,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2404-03662.bib,"Mon, 13 May 2024 01:00:00 +0200",,,,,,2404.03662,arXiv,,
DBLP:journals/corr/abs-2404-04793,article,"SqueezeAttention: 2D Management of KV-Cache in {LLM} Inference via
Layer-wise Optimal Budget","Zihao Wang and
Shaoduo Gan",2024,CoRR,,abs/2404.04793,,,10.48550/ARXIV.2404.04793,https://doi.org/10.48550/arXiv.2404.04793,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2404-04793.bib,"Wed, 15 May 2024 01:00:00 +0200",,,,,,2404.04793,arXiv,,
DBLP:journals/corr/abs-2404-04834,article,"LLM-Based Multi-Agent Systems for Software Engineering: Vision and
the Road Ahead","Junda He and
Christoph Treude and
David Lo",2024,CoRR,,abs/2404.04834,,,10.48550/ARXIV.2404.04834,https://doi.org/10.48550/arXiv.2404.04834,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2404-04834.bib,"Wed, 15 May 2024 01:00:00 +0200",,,,,,2404.04834,arXiv,,
DBLP:journals/corr/abs-2404-09384,article,"Tasks People Prompt: {A} Taxonomy of {LLM} Downstream Tasks in Software
Verification and Falsification Approaches","V{\'{\i}}ctor A. Braberman and
Flavia Bonomo{-}Braberman and
Yiannis Charalambous and
Juan Gabriel Colonna and
Lucas C. Cordeiro and
Rosiane de Freitas",2024,CoRR,,abs/2404.09384,,,10.48550/ARXIV.2404.09384,https://doi.org/10.48550/arXiv.2404.09384,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2404-09384.bib,"Sun, 06 Oct 2024 01:00:00 +0200",,,,,,2404.09384,arXiv,,
DBLP:journals/corr/abs-2404-09789,article,Software development in the age of LLMs and {XR},Jes{\'{u}}s M. Gonz{\'{a}}lez{-}Barahona,2024,CoRR,,abs/2404.09789,,,10.48550/ARXIV.2404.09789,https://doi.org/10.48550/arXiv.2404.09789,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2404-09789.bib,"Wed, 15 May 2024 01:00:00 +0200",,,,,,2404.09789,arXiv,,
DBLP:journals/corr/abs-2404-15869,article,"Semantic Routing for Enhanced Performance of LLM-Assisted Intent-Based
5G Core Network Management and Orchestration","Dimitrios Michael Manias and
Ali Chouman and
Abdallah Shami",2024,CoRR,,abs/2404.15869,,,10.48550/ARXIV.2404.15869,https://doi.org/10.48550/arXiv.2404.15869,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2404-15869.bib,"Mon, 03 Jun 2024 01:00:00 +0200",,,,,,2404.15869,arXiv,,
DBLP:journals/corr/abs-2404-17842,article,Using LLMs in Software Requirements Specifications: An Empirical Evaluation,"Madhava Krishna and
Bhagesh Gaur and
Arsh Verma and
Pankaj Jalote",2024,CoRR,,abs/2404.17842,,,10.48550/ARXIV.2404.17842,https://doi.org/10.48550/arXiv.2404.17842,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2404-17842.bib,"Mon, 27 May 2024 01:00:00 +0200",,,,,,2404.17842,arXiv,,
DBLP:journals/corr/abs-2404-18832,article,"It's Difficult to be Neutral - Human and LLM-based Sentiment Annotation
of Patient Comments","Petter M{\ae}hlum and
David Samuel and
Rebecka Maria Norman and
Elma Jelin and
{\O}yvind Andresen Bjertn{\ae}s and
Lilja {\O}vrelid and
Erik Velldal",2024,CoRR,,abs/2404.18832,,,10.48550/ARXIV.2404.18832,https://doi.org/10.48550/arXiv.2404.18832,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2404-18832.bib,"Mon, 27 May 2024 01:00:00 +0200",,,,,,2404.18832,arXiv,,
DBLP:journals/corr/abs-2404-18919,article,"TheaterGen: Character Management with {LLM} for Consistent Multi-turn
Image Generation","Junhao Cheng and
Baiqiao Yin and
Kaixin Cai and
Minbin Huang and
Hanhui Li and
Yuxin He and
Xi Lu and
Yue Li and
Yifei Li and
Yuhao Cheng and
Yiqiang Yan and
Xiaodan Liang",2024,CoRR,,abs/2404.18919,,,10.48550/ARXIV.2404.18919,https://doi.org/10.48550/arXiv.2404.18919,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2404-18919.bib,"Mon, 27 May 2024 01:00:00 +0200",,,,,,2404.18919,arXiv,,
DBLP:journals/corr/abs-2405-04437,article,vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention,"Ramya Prabhu and
Ajay Nayak and
Jayashree Mohan and
Ramachandran Ramjee and
Ashish Panwar",2024,CoRR,,abs/2405.04437,,,10.48550/ARXIV.2405.04437,https://doi.org/10.48550/arXiv.2405.04437,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2405-04437.bib,"Thu, 13 Jun 2024 01:00:00 +0200",,,,,,2405.04437,arXiv,,
DBLP:journals/corr/abs-2405-11346,article,"Decision support system for Forest fire management using Ontology
with Big Data and LLMs","Ritesh Chandra and
Shashi Shekhar Kumar and
Rushil Patra and
Sonali Agarwal",2024,CoRR,,abs/2405.11346,,,10.48550/ARXIV.2405.11346,https://doi.org/10.48550/arXiv.2405.11346,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2405-11346.bib,"Wed, 12 Jun 2024 01:00:00 +0200",,,,,,2405.11346,arXiv,,
DBLP:journals/corr/abs-2405-11835,article,"Demo Paper: {A} Game Agents Battle Driven by Free-Form Text Commands
Using Code-Generation {LLM}","Ray Ito and
Junichiro Takahashi",2024,CoRR,,abs/2405.11835,,,10.48550/ARXIV.2405.11835,https://doi.org/10.48550/arXiv.2405.11835,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2405-11835.bib,"Tue, 18 Jun 2024 01:00:00 +0200",,,,,,2405.11835,arXiv,,
DBLP:journals/corr/abs-2405-15114,article,"Let Me Do It For You: Towards {LLM} Empowered Recommendation via Tool
Learning","Yuyue Zhao and
Jiancan Wu and
Xiang Wang and
Wei Tang and
Dingxian Wang and
Maarten de Rijke",2024,CoRR,,abs/2405.15114,,,10.48550/ARXIV.2405.15114,https://doi.org/10.48550/arXiv.2405.15114,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2405-15114.bib,"Mon, 22 Jul 2024 01:00:00 +0200",,,,,,2405.15114,arXiv,,
DBLP:journals/corr/abs-2405-17378,article,"RTL-Repo: {A} Benchmark for Evaluating LLMs on Large-Scale {RTL} Design
Projects","Ahmed Allam and
Mohamed Shalan",2024,CoRR,,abs/2405.17378,,,10.48550/ARXIV.2405.17378,https://doi.org/10.48550/arXiv.2405.17378,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2405-17378.bib,"Sun, 06 Oct 2024 01:00:00 +0200",,,,,,2405.17378,arXiv,,
DBLP:journals/corr/abs-2405-18380,article,"OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for
Memory-Efficient {LLM} Fine-tuning","Pengxiang Li and
Lu Yin and
Xiaowei Gao and
Shiwei Liu",2024,CoRR,,abs/2405.18380,,,10.48550/ARXIV.2405.18380,https://doi.org/10.48550/arXiv.2405.18380,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2405-18380.bib,"Sat, 31 May 2025 01:00:00 +0200",,,,,,2405.1838,arXiv,,
DBLP:journals/corr/abs-2406-01721,article,"Rotation and Permutation for Advanced Outlier Management and Efficient
Quantization of LLMs","Haokun Lin and
Haobo Xu and
Yichen Wu and
Jingzhi Cui and
Yingtao Zhang and
Linzhan Mou and
Linqi Song and
Zhenan Sun and
Ying Wei",2024,CoRR,,abs/2406.01721,,,10.48550/ARXIV.2406.01721,https://doi.org/10.48550/arXiv.2406.01721,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2406-01721.bib,"Thu, 04 Jul 2024 01:00:00 +0200",,,,,,2406.01721,arXiv,,
DBLP:journals/corr/abs-2406-03317,article,"Save It for the ""Hot"" Day: An LLM-Empowered Visual Analytics System
for Heat Risk Management","Haobo Li and
Wong Kam{-}Kwai and
Yan Luo and
Juntong Chen and
Chengzhong Liu and
Yaxuan Zhang and
Alexis Kai{-}Hon Lau and
Huamin Qu and
Dongyu Liu",2024,CoRR,,abs/2406.03317,,,10.48550/ARXIV.2406.03317,https://doi.org/10.48550/arXiv.2406.03317,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2406-03317.bib,"Wed, 11 Jun 2025 01:00:00 +0200",,,,,,2406.03317,arXiv,,
DBLP:journals/corr/abs-2406-06059,article,"LLM-Based Intent Processing and Network Optimization Using Attention-Based
Hierarchical Reinforcement Learning","Md Arafat Habib and
Pedro Enrique Iturria{-}Rivera and
Yigit Ozcan and
Medhat H. M. Elsayed and
Majid Bavand and
Raimundas Gaigalas and
Melike Erol{-}Kantarci",2024,CoRR,,abs/2406.06059,,,10.48550/ARXIV.2406.06059,https://doi.org/10.48550/arXiv.2406.06059,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2406-06059.bib,"Sat, 13 Jul 2024 01:00:00 +0200",,,,,,2406.06059,arXiv,,
DBLP:journals/corr/abs-2406-08269,article,Analyzing constrained {LLM} through PDFA-learning,"Mat{\'{\i}}as Carrasco and
Franz Mayr and
Sergio Yovine and
Johny Kidd and
Mart{\'{\i}}n Iturbide and
Juan Pedro da Silva and
Alejo Garat",2024,CoRR,,abs/2406.08269,,,10.48550/ARXIV.2406.08269,https://doi.org/10.48550/arXiv.2406.08269,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2406-08269.bib,"Tue, 09 Jul 2024 01:00:00 +0200",,,,,,2406.08269,arXiv,,
DBLP:journals/corr/abs-2406-08305,article,"Large Language Model(LLM) assisted End-to-End Network Health Management
based on Multi-Scale Semanticization","Fengxiao Tang and
Xiaonan Wang and
Xun Yuan and
Linfeng Luo and
Ming Zhao and
Nei Kato",2024,CoRR,,abs/2406.08305,,,10.48550/ARXIV.2406.08305,https://doi.org/10.48550/arXiv.2406.08305,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2406-08305.bib,"Sun, 06 Oct 2024 01:00:00 +0200",,,,,,2406.08305,arXiv,,
DBLP:journals/corr/abs-2406-09233,article,{C2HLSC:} Can LLMs Bridge the Software-to-Hardware Design Gap?,"Luca Collini and
Siddharth Garg and
Ramesh Karri",2024,CoRR,,abs/2406.09233,,,10.48550/ARXIV.2406.09233,https://doi.org/10.48550/arXiv.2406.09233,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2406-09233.bib,"Tue, 09 Jul 2024 01:00:00 +0200",,,,,,2406.09233,arXiv,,
DBLP:journals/corr/abs-2406-10181,article,"Practical offloading for fine-tuning {LLM} on commodity {GPU} via
learned subspace projectors","Siyuan Chen and
Zelong Guan and
Yudong Liu and
Phillip B. Gibbons",2024,CoRR,,abs/2406.10181,,,10.48550/ARXIV.2406.10181,https://doi.org/10.48550/arXiv.2406.10181,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2406-10181.bib,"Mon, 21 Jul 2025 01:00:00 +0200",,,,,,2406.10181,arXiv,,
DBLP:journals/corr/abs-2406-10300,article,"Large Language Models as Software Components: {A} Taxonomy for LLM-Integrated
Applications",Irene Weber,2024,CoRR,,abs/2406.10300,,,10.48550/ARXIV.2406.10300,https://doi.org/10.48550/arXiv.2406.10300,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2406-10300.bib,"Sun, 04 Aug 2024 01:00:00 +0200",,,,,,2406.103,arXiv,,
DBLP:journals/corr/abs-2406-10958,article,"City-LEO: Toward Transparent City Management Using {LLM} with End-to-End
Optimization","Zihao Jiao and
Mengyi Sha and
Haoyu Zhang and
Xinyu Jiang and
Wei Qi",2024,CoRR,,abs/2406.10958,,,10.48550/ARXIV.2406.10958,https://doi.org/10.48550/arXiv.2406.10958,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2406-10958.bib,"Wed, 24 Jul 2024 01:00:00 +0200",,,,,,2406.10958,arXiv,,
DBLP:journals/corr/abs-2406-12806,article,"Identifying Performance-Sensitive Configurations in Software Systems
through Code Analysis with {LLM} Agents","Zehao Wang and
Dong Jae Kim and
Tse{-}Hsun Chen",2024,CoRR,,abs/2406.12806,,,10.48550/ARXIV.2406.12806,https://doi.org/10.48550/arXiv.2406.12806,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2406-12806.bib,"Fri, 12 Jul 2024 01:00:00 +0200",,,,,,2406.12806,arXiv,,
DBLP:journals/corr/abs-2406-13972,article,"{CREF:} An LLM-based Conversational Software Repair Framework for
Programming Tutors","Boyang Yang and
Haoye Tian and
Weiguo Pian and
Haoran Yu and
Haitao Wang and
Jacques Klein and
Tegawend{\'{e}} F. Bissyand{\'{e}} and
Shunfu Jin",2024,CoRR,,abs/2406.13972,,,10.48550/ARXIV.2406.13972,https://doi.org/10.48550/arXiv.2406.13972,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2406-13972.bib,"Sun, 04 Aug 2024 01:00:00 +0200",,,,,,2406.13972,arXiv,,
DBLP:journals/corr/abs-2407-00487,article,"It's Morphing Time: Unleashing the Potential of Multiple LLMs via
Multi-objective Optimization","Bingdong Li and
Zixiang Di and
Yanting Yang and
Hong Qian and
Peng Yang and
Hao Hao and
Ke Tang and
Aimin Zhou",2024,CoRR,,abs/2407.00487,,,10.48550/ARXIV.2407.00487,https://doi.org/10.48550/arXiv.2407.00487,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2407-00487.bib,"Fri, 09 Aug 2024 01:00:00 +0200",,,,,,2407.00487,arXiv,,
DBLP:journals/corr/abs-2407-00978,article,"Hybrid RAG-empowered Multi-modal {LLM} for Secure Healthcare Data
Management: {A} Diffusion-based Contract Theory Approach","Cheng Su and
Jinbo Wen and
Jiawen Kang and
Yonghua Wang and
Hudan Pan and
M. Shamim Hossain",2024,CoRR,,abs/2407.00978,,,10.48550/ARXIV.2407.00978,https://doi.org/10.48550/arXiv.2407.00978,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2407-00978.bib,"Fri, 15 Nov 2024 00:00:00 +0100",,,,,,2407.00978,arXiv,,
DBLP:journals/corr/abs-2407-01489,article,Agentless: Demystifying LLM-based Software Engineering Agents,"Chunqiu Steven Xia and
Yinlin Deng and
Soren Dunn and
Lingming Zhang",2024,CoRR,,abs/2407.01489,,,10.48550/ARXIV.2407.01489,https://doi.org/10.48550/arXiv.2407.01489,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2407-01489.bib,"Fri, 09 Aug 2024 01:00:00 +0200",,,,,,2407.01489,arXiv,,
DBLP:journals/corr/abs-2407-02392,article,TokenPacker: Efficient Visual Projector for Multimodal {LLM},"Wentong Li and
Yuqian Yuan and
Jian Liu and
Dongqi Tang and
Song Wang and
Jianke Zhu and
Lei Zhang",2024,CoRR,,abs/2407.02392,,,10.48550/ARXIV.2407.02392,https://doi.org/10.48550/arXiv.2407.02392,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2407-02392.bib,"Tue, 19 Aug 2025 01:00:00 +0200",,,,,,2407.02392,arXiv,,
DBLP:journals/corr/abs-2407-03963,article,"LLM-jp: {A} Cross-organizational Project for the Research and Development
of Fully Open Japanese LLMs","Akiko Aizawa and
Eiji Aramaki and
Bowen Chen and
Fei Cheng and
Hiroyuki Deguchi and
Rintaro Enomoto and
Kazuki Fujii and
Kensuke Fukumoto and
Takuya Fukushima and
Namgi Han and
Yuto Harada and
Chikara Hashimoto and
Tatsuya Hiraoka and
Shohei Hisada and
Sosuke Hosokawa and
Lu Jie and
Keisuke Kamata and
Teruhito Kanazawa and
Hiroki Kanezashi and
Hiroshi Kataoka and
Satoru Katsumata and
Daisuke Kawahara and
Seiya Kawano and
Atsushi Keyaki and
Keisuke Kiryu and
Hirokazu Kiyomaru and
Takashi Kodama and
Takahiro Kubo and
Yohei Kuga and
Ryoma Kumon and
Shuhei Kurita and
Sadao Kurohashi and
Conglong Li and
Taiki Maekawa and
Hiroshi Matsuda and
Yusuke Miyao and
Kentaro Mizuki and
Sakae Mizuki and
Yugo Murawaki and
Ryo Nakamura and
Taishi Nakamura and
Kouta Nakayama and
Tomoka Nakazato and
Takuro Niitsuma and
Jiro Nishitoba and
Yusuke Oda and
Hayato Ogawa and
Takumi Okamoto and
Naoaki Okazaki and
Yohei Oseki and
Shintaro Ozaki and
Koki Ryu and
Rafal Rzepka and
Keisuke Sakaguchi and
Shota Sasaki and
Satoshi Sekine and
Kohei Suda and
Saku Sugawara and
Issa Sugiura and
Hiroaki Sugiyama and
Hisami Suzuki and
Jun Suzuki and
Toyotaro Suzumura and
Kensuke Tachibana and
Yu Takagi and
Kyosuke Takami and
Koichi Takeda and
Masashi Takeshita and
Masahiro Tanaka and
Kenjiro Taura and
Arseny Tolmachev and
Nobuhiro Ueda and
Zhen Wan and
Shuntaro Yada and
Sakiko Yahata and
Yuya Yamamoto and
Yusuke Yamauchi and
Hitomi Yanaka and
Rio Yokota and
Koichiro Yoshino",2024,CoRR,,abs/2407.03963,,,10.48550/ARXIV.2407.03963,https://doi.org/10.48550/arXiv.2407.03963,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2407-03963.bib,"Mon, 30 Jun 2025 01:00:00 +0200",,,,,,2407.03963,arXiv,,
DBLP:journals/corr/abs-2407-06798,article,"It Cannot Be Right If It Was Written by {AI:} On Lawyers' Preferences
of Documents Perceived as Authored by an {LLM} vs a Human","Jakub Harasta and
Tereza Novotn{\'{a}} and
Jarom{\'{\i}}r Savelka",2024,CoRR,,abs/2407.06798,,,10.48550/ARXIV.2407.06798,https://doi.org/10.48550/arXiv.2407.06798,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2407-06798.bib,"Fri, 16 Aug 2024 01:00:00 +0200",,,,,,2407.06798,arXiv,,
DBLP:journals/corr/abs-2407-14372,article,SCoPE: Evaluating LLMs for Software Vulnerability Detection,"Jos{\'{e}} Gon{\c{c}}alves and
Tiago Dias and
Eva Maia and
Isabel Pra{\c{c}}a",2024,CoRR,,abs/2407.14372,,,10.48550/ARXIV.2407.14372,https://doi.org/10.48550/arXiv.2407.14372,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2407-14372.bib,"Thu, 22 Aug 2024 01:00:00 +0200",,,,,,2407.14372,arXiv,,
DBLP:journals/corr/abs-2407-14402,article,The Vision of Autonomic Computing: Can LLMs Make It a Reality?,"Zhiyang Zhang and
Fangkai Yang and
Xiaoting Qin and
Jue Zhang and
Qingwei Lin and
Gong Cheng and
Dongmei Zhang and
Saravan Rajmohan and
Qi Zhang",2024,CoRR,,abs/2407.14402,,,10.48550/ARXIV.2407.14402,https://doi.org/10.48550/arXiv.2407.14402,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2407-14402.bib,"Fri, 13 Dec 2024 00:00:00 +0100",,,,,,2407.14402,arXiv,,
DBLP:journals/corr/abs-2407-15309,article,vTensor: Flexible Virtual Tensor Management for Efficient {LLM} Serving,"Jiale Xu and
Rui Zhang and
Cong Guo and
Weiming Hu and
Zihan Liu and
Feiyang Wu and
Yu Feng and
Shixuan Sun and
Changxu Shao and
Yuhong Guo and
Junping Zhao and
Ke Zhang and
Minyi Guo and
Jingwen Leng",2024,CoRR,,abs/2407.15309,,,10.48550/ARXIV.2407.15309,https://doi.org/10.48550/arXiv.2407.15309,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2407-15309.bib,"Sat, 27 Sep 2025 01:00:00 +0200",,,,,,2407.15309,arXiv,,
DBLP:journals/corr/abs-2407-16557,article,Patched {RTC:} evaluating LLMs for diverse software development tasks,Asankhaya Sharma,2024,CoRR,,abs/2407.16557,,,10.48550/ARXIV.2407.16557,https://doi.org/10.48550/arXiv.2407.16557,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2407-16557.bib,"Mon, 19 Aug 2024 01:00:00 +0200",,,,,,2407.16557,arXiv,,
DBLP:journals/corr/abs-2407-18992,article,"Towards Automated Solution Recipe Generation for Industrial Asset
Management with {LLM}","Nianjun Zhou and
Dhaval Patel and
Shuxin Lin and
Fearghal O'Donncha",2024,CoRR,,abs/2407.18992,,,10.48550/ARXIV.2407.18992,https://doi.org/10.48550/arXiv.2407.18992,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2407-18992.bib,"Sat, 24 Aug 2024 01:00:00 +0200",,,,,,2407.18992,arXiv,,
DBLP:journals/corr/abs-2407-20588,article,"Enhancing Agricultural Machinery Management through Advanced {LLM}
Integration","Emily Johnson and
Noah Wilson",2024,CoRR,,abs/2407.20588,,,10.48550/ARXIV.2407.20588,https://doi.org/10.48550/arXiv.2407.20588,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2407-20588.bib,"Wed, 21 Aug 2024 01:00:00 +0200",,,,,,2407.20588,arXiv,,
DBLP:journals/corr/abs-2408-00948,article,"Leveraging Large Language Models (LLMs) for Traffic Management at
Urban Intersections: The Case of Mixed Traffic Scenarios","Sari Masri and
Huthaifa I. Ashqar and
Mohammed Elhenawy",2024,CoRR,,abs/2408.00948,,,10.48550/ARXIV.2408.00948,https://doi.org/10.48550/arXiv.2408.00948,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2408-00948.bib,"Mon, 09 Sep 2024 01:00:00 +0200",,,,,,2408.00948,arXiv,,
DBLP:journals/corr/abs-2408-01055,article,"{LLM} as Runtime Error Handler: {A} Promising Pathway to Adaptive
Self-Healing of Software Systems","Zhensu Sun and
Haotian Zhu and
Bowen Xu and
Xiaoning Du and
Li Li and
David Lo",2024,CoRR,,abs/2408.01055,,,10.48550/ARXIV.2408.01055,https://doi.org/10.48550/arXiv.2408.01055,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2408-01055.bib,"Mon, 09 Sep 2024 01:00:00 +0200",,,,,,2408.01055,arXiv,,
DBLP:journals/corr/abs-2408-01527,article,"Analyzing LLMs' Capabilities to Establish Implicit User Sentiment
of Software Desirability","Sherri Weitl{-}Harms and
John D. Hastings and
Jonah Lum",2024,CoRR,,abs/2408.01527,,,10.48550/ARXIV.2408.01527,https://doi.org/10.48550/arXiv.2408.01527,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2408-01527.bib,"Mon, 03 Mar 2025 00:00:00 +0100",,,,,,2408.01527,arXiv,,
DBLP:journals/corr/abs-2408-02479,article,"From LLMs to LLM-based Agents for Software Engineering: {A} Survey
of Current, Challenges and Future","Haolin Jin and
Linghan Huang and
Haipeng Cai and
Jun Yan and
Bo Li and
Huaming Chen",2024,CoRR,,abs/2408.02479,,,10.48550/ARXIV.2408.02479,https://doi.org/10.48550/arXiv.2408.02479,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2408-02479.bib,"Thu, 12 Sep 2024 01:00:00 +0200",,,,,,2408.02479,arXiv,,
DBLP:journals/corr/abs-2408-03528,article,"Exploring the extent of similarities in software failures across industries
using LLMs",Martin Detloff,2024,CoRR,,abs/2408.03528,,,10.48550/ARXIV.2408.03528,https://doi.org/10.48550/arXiv.2408.03528,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2408-03528.bib,"Fri, 13 Sep 2024 01:00:00 +0200",,,,,,2408.03528,arXiv,,
DBLP:journals/corr/abs-2408-05534,article,Can LLMs Replace Manual Annotation of Software Engineering Artifacts?,"Toufique Ahmed and
Premkumar T. Devanbu and
Christoph Treude and
Michael Pradel",2024,CoRR,,abs/2408.05534,,,10.48550/ARXIV.2408.05534,https://doi.org/10.48550/arXiv.2408.05534,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2408-05534.bib,"Tue, 17 Sep 2024 01:00:00 +0200",,,,,,2408.05534,arXiv,,
DBLP:journals/corr/abs-2408-09785,article,"GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining
Automotive Software Release Decision-Making","Arsham Gholamzadeh Khoee and
Yinan Yu and
Robert Feldt and
Andris Freimanis and
Patrick Andersson and
Dhasarathy Parthasarathy",2024,CoRR,,abs/2408.09785,,,10.48550/ARXIV.2408.09785,https://doi.org/10.48550/arXiv.2408.09785,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2408-09785.bib,"Tue, 24 Sep 2024 01:00:00 +0200",,,,,,2408.09785,arXiv,,
DBLP:journals/corr/abs-2408-10886,article,Leveraging LLMs for the Quality Assurance of Software Requirements,"Sebastian Lubos and
Alexander Felfernig and
Thi Ngoc Trang Tran and
Damian Garber and
Merfat El Mansi and
Seda Polat Erdeniz and
Viet{-}Man Le",2024,CoRR,,abs/2408.10886,,,10.48550/ARXIV.2408.10886,https://doi.org/10.48550/arXiv.2408.10886,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2408-10886.bib,"Tue, 24 Sep 2024 01:00:00 +0200",,,,,,2408.10886,arXiv,,
DBLP:journals/corr/abs-2408-16400,article,"Outside the Comfort Zone: Analysing {LLM} Capabilities in Software
Vulnerability Detection","Yuejun Guo and
Constantinos Patsakis and
Qiang Hu and
Qiang Tang and
Fran Casino",2024,CoRR,,abs/2408.16400,,,10.48550/ARXIV.2408.16400,https://doi.org/10.48550/arXiv.2408.16400,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2408-16400.bib,"Sat, 28 Sep 2024 01:00:00 +0200",,,,,,2408.164,arXiv,,
DBLP:journals/corr/abs-2409-01001,article,"Beyond ChatGPT: Enhancing Software Quality Assurance Tasks with Diverse
LLMs and Validation Techniques","Ratnadira Widyasari and
David Lo and
Lizi Liao",2024,CoRR,,abs/2409.01001,,,10.48550/ARXIV.2409.01001,https://doi.org/10.48550/arXiv.2409.01001,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2409-01001.bib,"Sat, 05 Oct 2024 01:00:00 +0200",,,,,,2409.01001,arXiv,,
DBLP:journals/corr/abs-2409-06205,article,"{SHAPE-IT:} Exploring Text-to-Shape-Display for Generative Shape-Changing
Behaviors with LLMs","Wanli Qian and
Chenfeng Gao and
Anup Sathya and
Ryo Suzuki and
Ken Nakagaki",2024,CoRR,,abs/2409.06205,,,10.48550/ARXIV.2409.06205,https://doi.org/10.48550/arXiv.2409.06205,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2409-06205.bib,"Mon, 14 Oct 2024 01:00:00 +0200",,,,,,2409.06205,arXiv,,
DBLP:journals/corr/abs-2409-06558,article,"{MAPS:} Energy-Reliability Tradeoff Management in Autonomous Vehicles
Through LLMs Penetrated Science","Mahdieh Aliazam and
Ali Javadi and
Amir Mahdi Hosseini Monazzah and
Ahmad Akbari Azirani",2024,CoRR,,abs/2409.06558,,,10.48550/ARXIV.2409.06558,https://doi.org/10.48550/arXiv.2409.06558,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2409-06558.bib,"Sat, 12 Oct 2024 01:00:00 +0200",,,,,,2409.06558,arXiv,,
DBLP:journals/corr/abs-2409-06643,article,Strategic management analysis: from data to strategy diagram by {LLM},"Richard Brath and
Adam Bradley and
David Jonker",2024,CoRR,,abs/2409.06643,,,10.48550/ARXIV.2409.06643,https://doi.org/10.48550/arXiv.2409.06643,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2409-06643.bib,"Sat, 12 Oct 2024 01:00:00 +0200",,,,,,2409.06643,arXiv,,
DBLP:journals/corr/abs-2409-06816,article,LLM-Enhanced Software Patch Localization,"Jinhong Yu and
Yi Chen and
Di Tang and
Xiaozhong Liu and
XiaoFeng Wang and
Chen Wu and
Haixu Tang",2024,CoRR,,abs/2409.06816,,,10.48550/ARXIV.2409.06816,https://doi.org/10.48550/arXiv.2409.06816,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2409-06816.bib,"Wed, 06 Aug 2025 01:00:00 +0200",,,,,,2409.06816,arXiv,,
DBLP:journals/corr/abs-2409-09338,article,"What you say or how you say it? Predicting Conflict Outcomes in Real
and LLM-Generated Conversations","Priya Ronald D'Costa and
Evan Rowbotham and
Xinlan Emily Hu",2024,CoRR,,abs/2409.09338,,,10.48550/ARXIV.2409.09338,https://doi.org/10.48550/arXiv.2409.09338,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2409-09338.bib,"Sat, 12 Oct 2024 01:00:00 +0200",,,,,,2409.09338,arXiv,,
DBLP:journals/corr/abs-2409-14605,article,"First Field Trial of LLM-Powered {AI} Agent for Lifecycle Management
of Autonomous Driving Optical Networks","Xiaomin Liu and
Qizhi Qiu and
Yihao Zhang and
Yuming Cheng and
Lilin Yi and
Weisheng Hu and
Qunbi Zhuge",2024,CoRR,,abs/2409.14605,,,10.48550/ARXIV.2409.14605,https://doi.org/10.48550/arXiv.2409.14605,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2409-14605.bib,"Tue, 15 Oct 2024 01:00:00 +0200",,,,,,2409.14605,arXiv,,
DBLP:journals/corr/abs-2409-16395,article,"Design and Evaluation of a {CDSS} for Drug Allergy Management Using
LLMs and Pharmaceutical Data Integration","Gabriele De Vito and
Filomena Ferrucci and
Athanasios Angelakis",2024,CoRR,,abs/2409.16395,,,10.48550/ARXIV.2409.16395,https://doi.org/10.48550/arXiv.2409.16395,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2409-16395.bib,"Wed, 16 Oct 2024 01:00:00 +0200",,,,,,2409.16395,arXiv,,
DBLP:journals/corr/abs-2409-16559,article,"Demystifying Issues, Causes and Solutions in {LLM} Open-Source Projects","Yangxiao Cai and
Peng Liang and
Yifei Wang and
Zengyang Li and
Mojtaba Shahin",2024,CoRR,,abs/2409.16559,,,10.48550/ARXIV.2409.16559,https://doi.org/10.48550/arXiv.2409.16559,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2409-16559.bib,"Thu, 17 Oct 2024 01:00:00 +0200",,,,,,2409.16559,arXiv,,
DBLP:journals/corr/abs-2409-16732,article,"""It Explains What {I} am Currently Going Through Perfectly to a Tee"":
Understanding User Perceptions on LLM-Enhanced Narrative Interventions","Ananya Bhattacharjee and
Sarah Yi Xu and
Pranav Rao and
Yuchen Zeng and
Jonah Meyerhoff and
Syed Ishtiaque Ahmed and
David C. Mohr and
Michael Liut and
Alex Mariakakis and
Rachel Kornfield and
Joseph Jay Williams",2024,CoRR,,abs/2409.16732,,,10.48550/ARXIV.2409.16732,https://doi.org/10.48550/arXiv.2409.16732,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2409-16732.bib,"Thu, 17 Oct 2024 01:00:00 +0200",,,,,,2409.16732,arXiv,,
DBLP:journals/corr/abs-2409-17166,article,"ScriptSmith: {A} Unified {LLM} Framework for Enhancing {IT} Operations
via Automated Bash Script Generation, Assessment, and Refinement","Oishik Chatterjee and
Pooja Aggarwal and
Suranjana Samanta and
Ting Dai and
Prateeti Mohapatra and
Debanjana Kar and
Ruchi Mahindru and
Steve Barbieri and
Eugen Postea and
Brad Blancett and
Arthur De Magalhaes",2024,CoRR,,abs/2409.17166,,,10.48550/ARXIV.2409.17166,https://doi.org/10.48550/arXiv.2409.17166,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2409-17166.bib,"Thu, 17 Oct 2024 01:00:00 +0200",,,,,,2409.17166,arXiv,,
DBLP:journals/corr/abs-2409-18203,article,"{AI} Policy Projector: Grounding {LLM} Policy Design in Iterative
Mapmaking","Michelle S. Lam and
Fred Hohman and
Dominik Moritz and
Jeffrey P. Bigham and
Kenneth Holstein and
Mary Beth Kery",2024,CoRR,,abs/2409.18203,,,10.48550/ARXIV.2409.18203,https://doi.org/10.48550/arXiv.2409.18203,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2409-18203.bib,"Thu, 17 Oct 2024 01:00:00 +0200",,,,,,2409.18203,arXiv,,
DBLP:journals/corr/abs-2410-01109,article,"Mixing It Up: The Cocktail Effect of Multi-Task Fine-Tuning on {LLM}
Performance - {A} Case Study in Finance","Meni Brief and
Oded Ovadia and
Gil Shenderovitz and
Noga Ben Yoash and
Rachel Lemberg and
Eitam Sheetrit",2024,CoRR,,abs/2410.01109,,,10.48550/ARXIV.2410.01109,https://doi.org/10.48550/arXiv.2410.01109,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2410-01109.bib,"Tue, 05 Nov 2024 00:00:00 +0100",,,,,,2410.01109,arXiv,,
DBLP:journals/corr/abs-2410-08911,article,"Test-driven Software Experimentation with {LASSO:} an {LLM} Benchmarking
Example",Marcus Kessel,2024,CoRR,,abs/2410.08911,,,10.48550/ARXIV.2410.08911,https://doi.org/10.48550/arXiv.2410.08911,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2410-08911.bib,"Mon, 03 Mar 2025 00:00:00 +0100",,,,,,2410.08911,arXiv,,
DBLP:journals/corr/abs-2410-09457,article,Power-Softmax: Towards Secure {LLM} Inference over Encrypted Data,"Itamar Zimerman and
Allon Adir and
Ehud Aharoni and
Matan Avitan and
Moran Baruch and
Nir Drucker and
Jenny Lerner and
Ramy Masalha and
Reut Meiri and
Omri Soceanu",2024,CoRR,,abs/2410.09457,,,10.48550/ARXIV.2410.09457,https://doi.org/10.48550/arXiv.2410.09457,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2410-09457.bib,"Fri, 22 Nov 2024 00:00:00 +0100",,,,,,2410.09457,arXiv,,
DBLP:journals/corr/abs-2410-11906,article,"Empowering Users in Digital Privacy Management through Interactive
LLM-Based Agents","Bolun Sun and
Yifan Zhou and
Haiyun Jiang",2024,CoRR,,abs/2410.11906,,,10.48550/ARXIV.2410.11906,https://doi.org/10.48550/arXiv.2410.11906,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2410-11906.bib,"Sun, 24 Nov 2024 00:00:00 +0100",,,,,,2410.11906,arXiv,,
DBLP:journals/corr/abs-2410-12071,article,"Beyond the Comfort Zone: Emerging Solutions to Overcome Challenges
in Integrating LLMs into Software Products","Nadia Nahar and
Christian K{\""{a}}stner and
Jenna L. Butler and
Chris Parnin and
Thomas Zimmermann and
Christian Bird",2024,CoRR,,abs/2410.12071,,,10.48550/ARXIV.2410.12071,https://doi.org/10.48550/arXiv.2410.12071,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2410-12071.bib,"Tue, 26 Nov 2024 00:00:00 +0100",,,,,,2410.12071,arXiv,,
DBLP:journals/corr/abs-2410-15512,article,"Reverse Question Answering: Can an {LLM} Write a Question so Hard
(or Bad) that it Can't Answer?","Nishant Balepur and
Feng Gu and
Abhilasha Ravichander and
Shi Feng and
Jordan L. Boyd{-}Graber and
Rachel Rudinger",2024,CoRR,,abs/2410.15512,,,10.48550/ARXIV.2410.15512,https://doi.org/10.48550/arXiv.2410.15512,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2410-15512.bib,"Wed, 09 Apr 2025 01:00:00 +0200",,,,,,2410.15512,arXiv,,
DBLP:journals/corr/abs-2410-17245,article,"Towards Reliable Evaluation of Behavior Steering Interventions in
LLMs","Itamar Pres and
Laura Ruis and
Ekdeep Singh Lubana and
David Krueger",2024,CoRR,,abs/2410.17245,,,10.48550/ARXIV.2410.17245,https://doi.org/10.48550/arXiv.2410.17245,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2410-17245.bib,"Wed, 27 Nov 2024 00:00:00 +0100",,,,,,2410.17245,arXiv,,
DBLP:journals/corr/abs-2410-17619,article,"From PDFs to Structured Data: Utilizing {LLM} Analysis in Sports Database
Management",Juhani Merilehto,2024,CoRR,,abs/2410.17619,,,10.48550/ARXIV.2410.17619,https://doi.org/10.48550/arXiv.2410.17619,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2410-17619.bib,"Mon, 03 Mar 2025 00:00:00 +0100",,,,,,2410.17619,arXiv,,
DBLP:journals/corr/abs-2410-18703,article,"Whose fault is it anyway? {SILC:} Safe Integration of LLM-Generated
Code","Peisen Lin and
Yuntong Zhang and
Andreea Costea and
Abhik Roychoudhury",2024,CoRR,,abs/2410.18703,,,10.48550/ARXIV.2410.18703,https://doi.org/10.48550/arXiv.2410.18703,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2410-18703.bib,"Fri, 29 Nov 2024 00:00:00 +0100",,,,,,2410.18703,arXiv,,
DBLP:journals/corr/abs-2410-19274,article,"Ripple: Accelerating {LLM} Inference on Smartphones with Correlation-Aware
Neuron Management","Tuowei Wang and
Ruwen Fan and
Minxing Huang and
Zixu Hao and
Kun Li and
Ting Cao and
Youyou Lu and
Yaoxue Zhang and
Ju Ren",2024,CoRR,,abs/2410.19274,,,10.48550/ARXIV.2410.19274,https://doi.org/10.48550/arXiv.2410.19274,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2410-19274.bib,"Tue, 16 Sep 2025 01:00:00 +0200",,,,,,2410.19274,arXiv,,
DBLP:journals/corr/abs-2410-20200,article,"Reasoning or a Semblance of it? {A} Diagnostic Study of Transitive
Reasoning in LLMs","Houman Mehrafarin and
Arash Eshghi and
Ioannis Konstas",2024,CoRR,,abs/2410.20200,,,10.48550/ARXIV.2410.20200,https://doi.org/10.48550/arXiv.2410.20200,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2410-20200.bib,"Thu, 28 Nov 2024 00:00:00 +0100",,,,,,2410.202,arXiv,,
DBLP:journals/corr/abs-2410-22368,article,"Project {MPG:} towards a generalized performance benchmark for {LLM}
capabilities","Lucas Spangher and
Tianle Li and
William F. Arnold and
Nick Masiewicki and
Xerxes Dotiwalla and
Rama Parusmathi and
Peter Grabowski and
Eugene Ie and
Daniel Gruhl",2024,CoRR,,abs/2410.22368,,,10.48550/ARXIV.2410.22368,https://doi.org/10.48550/arXiv.2410.22368,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2410-22368.bib,"Wed, 21 May 2025 01:00:00 +0200",,,,,,2410.22368,arXiv,,
DBLP:journals/corr/abs-2410-23069,article,"LLMs Integration in Software Engineering Team Projects: Roles, Impact,
and a Pedagogical Design Space for {AI} Tools in Computing Education","Ahmed Kharrufa and
Sami Saeed Alghamdi and
Abeer Aziz and
Christopher Bull",2024,CoRR,,abs/2410.23069,,,10.48550/ARXIV.2410.23069,https://doi.org/10.48550/arXiv.2410.23069,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2410-23069.bib,"Tue, 01 Apr 2025 01:00:00 +0200",,,,,,2410.23069,arXiv,,
DBLP:journals/corr/abs-2410-23365,article,"Automated Personnel Selection for Software Engineers Using LLM-Based
Profile Evaluation","Ahmed Akib Jawad Karim and
Shahria Hoque and
Md. Golam Rabiul Alam and
Md. Zia Uddin",2024,CoRR,,abs/2410.23365,,,10.48550/ARXIV.2410.23365,https://doi.org/10.48550/arXiv.2410.23365,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2410-23365.bib,"Fri, 29 Nov 2024 00:00:00 +0100",,,,,,2410.23365,arXiv,,
DBLP:journals/corr/abs-2411-00932,article,LLMs: {A} Game-Changer for Software Engineers?,Md. Asraful Haque,2024,CoRR,,abs/2411.00932,,,10.48550/ARXIV.2411.00932,https://doi.org/10.48550/arXiv.2411.00932,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2411-00932.bib,"Wed, 11 Dec 2024 00:00:00 +0100",,,,,,2411.00932,arXiv,,
DBLP:journals/corr/abs-2411-02594,article,"""It's a conversation, not a quiz"": {A} Risk Taxonomy and Reflection
Tool for {LLM} Adoption in Public Health","Jiawei Zhou and
Amy Z. Chen and
Darshi Shah and
Laura Schwab Reese and
Munmun De Choudhury",2024,CoRR,,abs/2411.02594,,,10.48550/ARXIV.2411.02594,https://doi.org/10.48550/arXiv.2411.02594,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2411-02594.bib,"Fri, 03 Jan 2025 00:00:00 +0100",,,,,,2411.02594,arXiv,,
DBLP:journals/corr/abs-2411-03923,article,"Evaluation data contamination in LLMs: how do we measure it and (when)
does it matter?","Aaditya K. Singh and
Muhammed Yusuf Kocyigit and
Andrew Poulton and
David Esiobu and
Maria Lomeli and
Gergely Szilvasy and
Dieuwke Hupkes",2024,CoRR,,abs/2411.03923,,,10.48550/ARXIV.2411.03923,https://doi.org/10.48550/arXiv.2411.03923,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2411-03923.bib,"Wed, 01 Jan 2025 00:00:00 +0100",,,,,,2411.03923,arXiv,,
DBLP:journals/corr/abs-2411-04444,article,"An Empirical Study on the Potential of LLMs in Automated Software
Refactoring","Bo Liu and
Yanjie Jiang and
Yuxia Zhang and
Nan Niu and
Guangjie Li and
Hui Liu",2024,CoRR,,abs/2411.04444,,,10.48550/ARXIV.2411.04444,https://doi.org/10.48550/arXiv.2411.04444,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2411-04444.bib,"Tue, 29 Jul 2025 01:00:00 +0200",,,,,,2411.04444,arXiv,,
DBLP:journals/corr/abs-2411-09916,article,"LLMs are Imperfect, Then What? An Empirical Study on {LLM} Failures
in Software Engineering","Jiessie Tie and
Bingsheng Yao and
Tianshi Li and
Syed Ishtiaque Ahmed and
Dakuo Wang and
Shurui Zhou",2024,CoRR,,abs/2411.09916,,,10.48550/ARXIV.2411.09916,https://doi.org/10.48550/arXiv.2411.09916,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2411-09916.bib,"Wed, 01 Jan 2025 00:00:00 +0100",,,,,,2411.09916,arXiv,,
DBLP:journals/corr/abs-2411-09974,article,"Experiences from Using LLMs for Repository Mining Studies in Empirical
Software Engineering","Vincenzo De Martino and
Joel Casta{\~{n}}o and
Fabio Palomba and
Xavier Franch and
Silverio Mart{\'{\i}}nez{-}Fern{\'{a}}ndez",2024,CoRR,,abs/2411.09974,,,10.48550/ARXIV.2411.09974,https://doi.org/10.48550/arXiv.2411.09974,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2411-09974.bib,"Mon, 03 Mar 2025 00:00:00 +0100",,,,,,2411.09974,arXiv,,
DBLP:journals/corr/abs-2411-13269,article,"Towards Specification-Driven LLM-Based Generation of Embedded Automotive
Software","Minal Suresh Patil and
Gustav Ung and
Mattias Nyberg",2024,CoRR,,abs/2411.13269,,,10.48550/ARXIV.2411.13269,https://doi.org/10.48550/arXiv.2411.13269,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2411-13269.bib,"Wed, 01 Jan 2025 00:00:00 +0100",,,,,,2411.13269,arXiv,,
DBLP:journals/corr/abs-2411-13941,article,"LLMs as Continuous Learners: Improving the Reproduction of Defective
Code in Software Issues","Yalan Lin and
Yingwei Ma and
Rongyu Cao and
Binhua Li and
Fei Huang and
Xiaodong Gu and
Yongbin Li",2024,CoRR,,abs/2411.13941,,,10.48550/ARXIV.2411.13941,https://doi.org/10.48550/arXiv.2411.13941,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2411-13941.bib,"Wed, 11 Jun 2025 01:00:00 +0200",,,,,,2411.13941,arXiv,,
DBLP:journals/corr/abs-2411-17927,article,"Measuring Emergent Capabilities of LLMs for Software Engineering:
How Far Are We?","Conor O'Brien and
Daniel Rodr{\'{\i}}guez{-}C{\'{a}}rdenas and
Alejandro Velasco and
David N. Palacio and
Denys Poshyvanyk",2024,CoRR,,abs/2411.17927,,,10.48550/ARXIV.2411.17927,https://doi.org/10.48550/arXiv.2411.17927,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2411-17927.bib,"Wed, 01 Jan 2025 00:00:00 +0100",,,,,,2411.17927,arXiv,,
DBLP:journals/corr/abs-2411-17981,article,Engineering Trustworthy Software: {A} Mission for LLMs,Marco Vieira,2024,CoRR,,abs/2411.17981,,,10.48550/ARXIV.2411.17981,https://doi.org/10.48550/arXiv.2411.17981,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2411-17981.bib,"Mon, 03 Mar 2025 00:00:00 +0100",,,,,,2411.17981,arXiv,,
DBLP:journals/corr/abs-2411-19146,article,Puzzle: Distillation-Based {NAS} for Inference-Optimized LLMs,"Akhiad Bercovich and
Tomer Ronen and
Talor Abramovich and
Nir Ailon and
Nave Assaf and
Mohammad Dabbah and
Ido Galil and
Amnon Geifman and
Yonatan Geifman and
Izhak Golan and
Netanel Haber and
Ehud Karpas and
Roi Koren and
Itay Levy and
Pavlo Molchanov and
Shahar Mor and
Zach Moshe and
Najeeb Nabwani and
Omri Puny and
Ran Rubin and
Itamar Schen and
Ido Shahaf and
Oren Tropp and
Omer Ullman Argov and
Ran Zilberstein and
Ran El{-}Yaniv",2024,CoRR,,abs/2411.19146,,,10.48550/ARXIV.2411.19146,https://doi.org/10.48550/arXiv.2411.19146,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2411-19146.bib,"Wed, 01 Jan 2025 00:00:00 +0100",,,,,,2411.19146,arXiv,,
DBLP:journals/corr/abs-2412-00329,article,Analyzing the Energy and Accuracy of LLMs in Software Development,"Negar Alizadeh and
Boris Belchev and
Nishant Saurabh and
Patricia Kelbert and
Fernando Castor",2024,CoRR,,abs/2412.00329,,,10.48550/ARXIV.2412.00329,https://doi.org/10.48550/arXiv.2412.00329,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2412-00329.bib,"Thu, 09 Jan 2025 00:00:00 +0100",,,,,,2412.00329,arXiv,,
DBLP:journals/corr/abs-2412-00546,article,"Rank It, Then Ask It: Input Reranking for Maximizing the Performance
of LLMs on Symmetric Tasks","Mohsen Dehghankar and
Abolfazl Asudeh",2024,CoRR,,abs/2412.00546,,,10.48550/ARXIV.2412.00546,https://doi.org/10.48550/arXiv.2412.00546,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2412-00546.bib,"Sun, 12 Jan 2025 00:00:00 +0100",,,,,,2412.00546,arXiv,,
DBLP:journals/corr/abs-2412-03815,article,"Synergizing LLMs and Knowledge Graphs: {A} Novel Approach to Software
Repository-Related Question Answering","Samuel Abedu and
SayedHassan Khatoonabadi and
Emad Shihab",2024,CoRR,,abs/2412.03815,,,10.48550/ARXIV.2412.03815,https://doi.org/10.48550/arXiv.2412.03815,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2412-03815.bib,"Tue, 14 Jan 2025 00:00:00 +0100",,,,,,2412.03815,arXiv,,
DBLP:journals/corr/abs-2412-03905,article,"Integrating Various Software Artifacts for Better LLM-based Bug Localization
and Program Repair","Qiong Feng and
Xiaotian Ma and
Jiayi Sheng and
Ziyuan Feng and
Wei Song and
Peng Liang",2024,CoRR,,abs/2412.03905,,,10.48550/ARXIV.2412.03905,https://doi.org/10.48550/arXiv.2412.03905,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2412-03905.bib,"Tue, 14 Jan 2025 00:00:00 +0100",,,,,,2412.03905,arXiv,,
DBLP:journals/corr/abs-2412-05098,article,"From Defects to Demands: {A} Unified, Iterative, and Heuristically
Guided LLM-Based Framework for Automated Software Repair and Requirement
Realization","Alex Liu and
Vivian Chi",2024,CoRR,,abs/2412.05098,,,10.48550/ARXIV.2412.05098,https://doi.org/10.48550/arXiv.2412.05098,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2412-05098.bib,"Tue, 14 Jan 2025 00:00:00 +0100",,,,,,2412.05098,arXiv,,
DBLP:journals/corr/abs-2412-06294,article,"Beyond pip install: Evaluating {LLM} Agents for the Automated Installation
of Python Projects","Louis Milliken and
Sungmin Kang and
Shin Yoo",2024,CoRR,,abs/2412.06294,,,10.48550/ARXIV.2412.06294,https://doi.org/10.48550/arXiv.2412.06294,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2412-06294.bib,"Wed, 15 Jan 2025 00:00:00 +0100",,,,,,2412.06294,arXiv,,
DBLP:journals/corr/abs-2412-09644,article,"Combining knowledge graphs and LLMs for hazardous chemical information
management and reuse","Marcos Da Silveira and
Louis Deladiennee and
Kheira Acem and
Oona Freudenthal",2024,CoRR,,abs/2412.09644,,,10.48550/ARXIV.2412.09644,https://doi.org/10.48550/arXiv.2412.09644,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2412-09644.bib,"Mon, 20 Jan 2025 00:00:00 +0100",,,,,,2412.09644,arXiv,,
DBLP:journals/corr/abs-2412-10133,article,"You Name It, {I} Run It: An {LLM} Agent to Execute Tests of Arbitrary
Projects","Islem Bouzenia and
Michael Pradel",2024,CoRR,,abs/2412.10133,,,10.48550/ARXIV.2412.10133,https://doi.org/10.48550/arXiv.2412.10133,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2412-10133.bib,"Mon, 20 Jan 2025 00:00:00 +0100",,,,,,2412.10133,arXiv,,
DBLP:journals/corr/abs-2412-14461,article,"From Human Annotation to LLMs: {SILICON} Annotation Workflow for Management
Research","Xiang Cheng and
Raveesh Mayya and
Jo{\~{a}}o Sedoc",2024,CoRR,,abs/2412.14461,,,10.48550/ARXIV.2412.14461,https://doi.org/10.48550/arXiv.2412.14461,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2412-14461.bib,"Thu, 23 Jan 2025 00:00:00 +0100",,,,,,2412.14461,arXiv,,
DBLP:journals/corr/abs-2412-15574,article,"{J-EDI} {QA:} Benchmark for deep-sea organism-specific multimodal
{LLM}","Takero Yoshida and
Yuikazu Ito and
Yoshihiro Fujiwara and
Shinji Tsuchida and
Daisuke Sugiyama and
Daisuke Matsuoka",2024,CoRR,,abs/2412.15574,,,10.48550/ARXIV.2412.15574,https://doi.org/10.48550/arXiv.2412.15574,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2412-15574.bib,"Thu, 23 Jan 2025 00:00:00 +0100",,,,,,2412.15574,arXiv,,
DBLP:journals/corr/abs-2412-16434,article,{SYMPHONY:} Improving Memory Management for {LLM} Inference Workloads,"Saurabh Agarwal and
Anyong Mao and
Aditya Akella and
Shivaram Venkataraman",2024,CoRR,,abs/2412.16434,,,10.48550/ARXIV.2412.16434,https://doi.org/10.48550/arXiv.2412.16434,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2412-16434.bib,"Thu, 23 Jan 2025 00:00:00 +0100",,,,,,2412.16434,arXiv,,
DBLP:journals/corr/abs-2412-18588,article,"A Paragraph is All It Takes: Rich Robot Behaviors from Interacting,
Trusted LLMs","OpenMind and
Shaohong Zhong and
Adam Zhou and
Boyuan Chen and
Homin Luo and
Jan Liphardt",2024,CoRR,,abs/2412.18588,,,10.48550/ARXIV.2412.18588,https://doi.org/10.48550/arXiv.2412.18588,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2412-18588.bib,"Sat, 25 Jan 2025 00:00:00 +0100",,,,,,2412.18588,arXiv,,
DBLP:journals/corr/abs-2412-19616,article,"Gradient Weight-normalized Low-rank Projection for Efficient {LLM}
Training","Jia{-}Hong Huang and
Yixian Shen and
Hongyi Zhu and
Stevan Rudinac and
Evangelos Kanoulas",2024,CoRR,,abs/2412.19616,,,10.48550/ARXIV.2412.19616,https://doi.org/10.48550/arXiv.2412.19616,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2412-19616.bib,"Sat, 25 Jan 2025 00:00:00 +0100",,,,,,2412.19616,arXiv,,
DBLP:journals/corr/abs-2412-19820,article,GaLore+: Boosting Low-Rank Adaptation for LLMs with Cross-Head Projection,"Xutao Liao and
Shaohui Li and
Yuhui Xu and
Zhi Li and
Yu Liu and
You He",2024,CoRR,,abs/2412.19820,,,10.48550/ARXIV.2412.19820,https://doi.org/10.48550/arXiv.2412.19820,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2412-19820.bib,"Thu, 25 Sep 2025 01:00:00 +0200",,,,,,2412.1982,arXiv,,
DBLP:journals/corr/abs-2412-21016,article,Automated Robustness Testing for LLM-based {NLP} Software,"Mingxuan Xiao and
Yan Xiao and
Shunhui Ji and
Hanbo Cai and
Lei Xue and
Pengcheng Zhang",2024,CoRR,,abs/2412.21016,,,10.48550/ARXIV.2412.21016,https://doi.org/10.48550/arXiv.2412.21016,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2412-21016.bib,"Tue, 28 Jan 2025 00:00:00 +0100",,,,,,2412.21016,arXiv,,
DBLP:journals/ercim/CamaraTB24,article,"From Chats to Models: Assessing the Potential of LLMs in the World
of Software Modeling","Javier C{\'{a}}mara and
Javier Troya and
Lola Burgue{\~{n}}o",2024,{ERCIM} News,,2024,136,,,https://ercim-news.ercim.eu/en136/special/from-chats-to-models-assessing-the-potential-of-llms-in-the-world-of-software-modeling,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/ercim/CamaraTB24.bib,"Tue, 15 Oct 2024 01:00:00 +0200",,,,,,,,,
DBLP:conf/aics/HartmannTFH23,inproceedings,"Fine-Tune it Like I'm Five: Supporting Medical Domain Experts in Training
{NER} Models Using Cloud, LLM, and Auto Fine-Tuning","Benedict Hartmann and
Philippe Tamla and
Florian Freund and
Matthias L. Hemmje",2023,,"31st Irish Conference on Artificial Intelligence and Cognitive Science,
{AICS} 2023, Letterkenny, Ireland, December 7-8, 2023",,,1--8,10.1109/AICS60730.2023.10470654,https://doi.org/10.1109/AICS60730.2023.10470654,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/aics/HartmannTFH23.bib,"Sun, 06 Oct 2024 01:00:00 +0200",{IEEE},,,,,,,,
DBLP:conf/bpm/Berti0A23,inproceedings,"Abstractions, Scenarios, and Prompt Definitions for Process Mining
with LLMs: {A} Case Study","Alessandro Berti and
Daniel Schuster and
Wil M. P. van der Aalst",2023,,"Business Process Management Workshops - {BPM} 2023 International Workshops,
Utrecht, The Netherlands, September 11-15, 2023, Revised Selected
Papers",492,,427--439,10.1007/978-3-031-50974-2\_32,https://doi.org/10.1007/978-3-031-50974-2\_32,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/bpm/Berti0A23.bib,"Sun, 30 Mar 2025 00:00:00 +0100",Springer,"Jochen De Weerdt and
Luise Pufahl",Lecture Notes in Business Information Processing,,,,,,
DBLP:conf/clic-it/RanaldiPRZF23,inproceedings,Teasing LLMs Adapted to Italian,"Leonardo Ranaldi and
Giulia Pucci and
Elena Sofia Ruzzetti and
Fabio Massimo Zanzotto and
Andr{\'{e}} Freitas",2023,,"Proceedings of the 9th Italian Conference on Computational Linguistics,
Venice, Italy, November 30 - December 2, 2023",3596,,,,https://ceur-ws.org/Vol-3596/short18.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/RanaldiPRZF23.bib,"Tue, 02 Jan 2024 17:44:44 +0100",CEUR-WS.org,"Federico Boschetti and
Gianluca E. Lebani and
Bernardo Magnini and
Nicole Novielli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/clic-it/RanaldiRRVGFRZ23,inproceedings,Prompting LLMs in Italian Language for Text-to-SQL Translation,"Federico Ranaldi and
Elena Sofia Ruzzetti and
Leonardo Ranaldi and
Davide Venditti and
Cristina Giannone and
Andrea Favalli and
Raniero Romagnoli and
Fabio Massimo Zanzotto",2023,,"Proceedings of the 9th Italian Conference on Computational Linguistics,
Venice, Italy, November 30 - December 2, 2023",3596,,,,https://ceur-ws.org/Vol-3596/paper41.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/clic-it/RanaldiRRVGFRZ23.bib,"Tue, 02 Jan 2024 00:00:00 +0100",CEUR-WS.org,"Federico Boschetti and
Gianluca E. Lebani and
Bernardo Magnini and
Nicole Novielli",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/cnsm/DzeparoskaLTL23,inproceedings,LLM-Based Policy Generation for Intent-Based Management of Applications,"Kristina Dzeparoska and
Jieyu Lin and
Ali Tizghadam and
Alberto Leon{-}Garcia",2023,,"19th International Conference on Network and Service Management, {CNSM}
2023, Niagara Falls, ON, Canada, October 30 - Nov. 2, 2023",,,1--7,10.23919/CNSM59352.2023.10327837,https://doi.org/10.23919/CNSM59352.2023.10327837,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/cnsm/DzeparoskaLTL23.bib,"Tue, 05 Dec 2023 20:47:36 +0100",{IEEE},,,,,,,,
DBLP:conf/ekgllm/Agarwal23,inproceedings,"Building Knowledge Graph for Products at Scale and Infusing it in
to LLMs",Manoj K. Agarwal,2023,,"Proceedings of the Workshop on Enterprise Knowledge Graphs using Large
Language Models {(EKG-LLM} 2023) co-located with 32nd {ACM} International
Conference on Information and Knowledge Management {(CIKM} 2023),
Birmingham, UK, October 22, 2023",3532,,,,https://ceur-ws.org/Vol-3532/invited5.pdf,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ekgllm/Agarwal23.bib,"Mon, 12 Feb 2024 00:00:00 +0100",CEUR-WS.org,"Rajeev Gupta and
Srinath Srinivasa",{CEUR} Workshop Proceedings,,,,,,
DBLP:conf/emnlp/AzariaM23,inproceedings,The Internal State of an {LLM} Knows When It's Lying,"Amos Azaria and
Tom M. Mitchell",2023,,"Findings of the Association for Computational Linguistics: {EMNLP}
2023, Singapore, December 6-10, 2023",,,967--976,10.18653/V1/2023.FINDINGS-EMNLP.68,https://doi.org/10.18653/v1/2023.findings-emnlp.68,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/emnlp/AzariaM23.bib,"Fri, 12 Apr 2024 01:00:00 +0200",Association for Computational Linguistics,"Houda Bouamor and
Juan Pino and
Kalika Bali",,,,,,,
DBLP:conf/goodit/MontagnaFKFP23,inproceedings,"Data Decentralisation of LLM-Based Chatbot Systems in Chronic Disease
Self-Management","Sara Montagna and
Stefano Ferretti and
Lorenz Cuno Klopfenstein and
Antonio Florio and
Martino Francesco Pengo",2023,,"Proceedings of the 2023 {ACM} Conference on Information Technology
for Social Good, GoodIT 2023, Lisbon, Portugal, September 6-8, 2023",,,205--212,10.1145/3582515.3609536,https://doi.org/10.1145/3582515.3609536,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/goodit/MontagnaFKFP23.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},,,,,,,,
DBLP:conf/icca3/YousriS23,inproceedings,"How Big Can It Get? {A} comparative analysis of LLMs in architecture
and scaling","Ramez Yousri and
Soha Safwat",2023,,"International Conference on Computer and Applications, {ICCA} 2023,
Cairo, Egypt, November 28-30, 2023",,,1--5,10.1109/ICCA59364.2023.10401818,https://doi.org/10.1109/ICCA59364.2023.10401818,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icca3/YousriS23.bib,"Sun, 06 Oct 2024 01:00:00 +0200",{IEEE},,,,,,,,
DBLP:conf/icspis/NaitoWM23,inproceedings,"LLM-based Attack Scenarios Generator with {IT} Asset Management and
Vulnerability Information","Takeru Naito and
Rei Watanabe and
Takuho Mitsunaga",2023,,"6th International Conference on Signal Processing and Information
Security, {ICSPIS} 2023, Dubai, UAE, November 8-9, 2023",,,99--103,10.1109/ICSPIS60075.2023.10344019,https://doi.org/10.1109/ICSPIS60075.2023.10344019,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/icspis/NaitoWM23.bib,"Wed, 17 Jan 2024 10:56:09 +0100",{IEEE},,,,,,,,
DBLP:conf/kbse/LeungM23,inproceedings,On Automated Assistants for Software Development: The Role of LLMs,"Mira Leung and
Gail C. Murphy",2023,,"38th {IEEE/ACM} International Conference on Automated Software Engineering,
{ASE} 2023, Luxembourg, September 11-15, 2023",,,1737--1741,10.1109/ASE56229.2023.00035,https://doi.org/10.1109/ASE56229.2023.00035,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/kbse/LeungM23.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{IEEE},,,,,,,,
DBLP:conf/ml4h/GoelGGLENHJRKSL23,inproceedings,LLMs Accelerate Annotation for Medical Information Extraction,"Akshay Goel and
Almog Gueta and
Omry Gilon and
Chang Liu and
Sofia Erell and
Lan Huong Nguyen and
Xiaohong Hao and
Bolous Jaber and
Shashir Reddy and
Rupesh Kartha and
Jean Steiner and
Itay Laish and
Amir Feder",2023,,"Machine Learning for Health, ML4H@NeurIPS 2023, 10 December 2023,
New Orleans, Louisiana, {USA}",225,,82--100,,https://proceedings.mlr.press/v225/goel23a.html,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ml4h/GoelGGLENHJRKSL23.bib,"Tue, 20 Feb 2024 16:30:24 +0100",{PMLR},"Stefan Hegselmann and
Antonio Parziale and
Divya Shanmugam and
Shengpu Tang and
Mercy Nyamewaa Asiedu and
Serina Chang and
Tom Hartvigsen and
Harvineet Singh",Proceedings of Machine Learning Research,,,,,,
DBLP:conf/promise/Khomh23,inproceedings,"Harnessing Predictive Modeling and Software Analytics in the Age of
LLM-Powered Software Development (Invited Talk)",Foutse Khomh,2023,,"Proceedings of the 19th International Conference on Predictive Models
and Data Analytics in Software Engineering, {PROMISE} 2023, San Francisco,
CA, USA, 8 December 2023",,,1,10.1145/3617555.3634736,https://doi.org/10.1145/3617555.3634736,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/promise/Khomh23.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Shane McIntosh and
Eunjong Choi and
Steffen Herbold",,,,,,,
DBLP:conf/smartcloud/ParkK23,inproceedings,"Formulating an Korean LLM-Based Interactive Assistant for Enhanced
{IT} Collaboration in Microservice Environments","Gijun Park and
Dohoon Kim",2023,,"8th {IEEE} International Conference on Smart Cloud, SmartCloud 2023,
Tokyo, Japan, September 16-18, 2023",,,176--181,10.1109/SMARTCLOUD58862.2023.00038,https://doi.org/10.1109/SmartCloud58862.2023.00038,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/smartcloud/ParkK23.bib,"Mon, 29 Jan 2024 10:01:33 +0100",{IEEE},,,,,,,,
DBLP:conf/tvx/AgossahKSC23,inproceedings,"LLM-Based Interaction for Content Generation: {A} Case Study on the
Perception of Employees in an {IT} Department","Alexandre Agossah and
Fr{\'{e}}d{\'{e}}rique Krupa and
Matthieu Perreira Da Silva and
Patrick Le Callet",2023,,"Proceedings of the 2023 {ACM} International Conference on Interactive
Media Experiences, {IMX} 2023, Nantes, France, June 12-15, 2023",,,237--241,10.1145/3573381.3603362,https://doi.org/10.1145/3573381.3603362,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/tvx/AgossahKSC23.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Patrick Le Callet and
Matthieu Perreira Da Silva and
Toinon Vigier and
Koray Tahiroglu and
Niall Murray and
Giuseppe Valenzise and
Mea Wang",,,,,,,
DBLP:conf/ekgllm/2023,proceedings,"Proceedings of the Workshop on Enterprise Knowledge Graphs using Large
Language Models {(EKG-LLM} 2023) co-located with 32nd {ACM} International
Conference on Information and Knowledge Management {(CIKM} 2023),
Birmingham, UK, October 22, 2023",,2023,,,3532,,,,https://ceur-ws.org/Vol-3532,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/ekgllm/2023.bib,"Mon, 18 Dec 2023 00:00:00 +0100",CEUR-WS.org,"Rajeev Gupta and
Srinath Srinivasa",{CEUR} Workshop Proceedings,,,,,urn:nbn:de:0074-3532-0,
DBLP:data/10/MeissnerSKB23,misc,"EvalQuiz - LLM-based Automated Generation of Self-Assessment Quizzes
in Software Engineering Education (Version 1.0)","Niklas Mei{\ss}ner and
Sandro Speth and
Julian Kieslinger and
Steffen Becker",2023,,,,,,10.5281/ZENODO.10040086,https://doi.org/10.5281/zenodo.10040086,Accessed on YYYY-MM-DD.,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/data/10/MeissnerSKB23.bib,"Fri, 22 Nov 2024 00:00:00 +0100",Zenodo,,,\url{https://doi.org/10.5281/zenodo.10040086},October,,,,
DBLP:journals/corr/abs-2304-09064,article,"LLM-based Interaction for Content Generation: {A} Case Study on the
Perception of Employees in an {IT} department","Alexandre Agossah and
Fr{\'{e}}d{\'{e}}rique Krupa and
Matthieu Perreira Da Silva and
Patrick Le Callet",2023,CoRR,,abs/2304.09064,,,10.48550/ARXIV.2304.09064,https://doi.org/10.48550/arXiv.2304.09064,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2304-09064.bib,"Mon, 24 Apr 2023 01:00:00 +0200",,,,,,2304.09064,arXiv,,
DBLP:journals/corr/abs-2306-02230,article,"Prompt Sapper: LLM-Empowered Software Engineering Infrastructure for
AI-Native Services","Zhenchang Xing and
Qing Huang and
Yu Cheng and
Liming Zhu and
Qinghua Lu and
Xiwei Xu",2023,CoRR,,abs/2306.02230,,,10.48550/ARXIV.2306.02230,https://doi.org/10.48550/arXiv.2306.02230,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2306-02230.bib,"Mon, 12 Jun 2023 01:00:00 +0200",,,,,,2306.0223,arXiv,,
DBLP:journals/corr/abs-2308-00158,article,"Predicting Perfect Quality Segments in {MT} Output with Fine-Tuned
OpenAI {LLM:} Is it possible to capture editing distance patterns
from historical data?","Serge Gladkoff and
Gleb Erofeev and
Lifeng Han and
Goran Nenadic",2023,CoRR,,abs/2308.00158,,,10.48550/ARXIV.2308.00158,https://doi.org/10.48550/arXiv.2308.00158,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2308-00158.bib,"Mon, 21 Aug 2023 01:00:00 +0200",,,,,,2308.00158,arXiv,,
DBLP:journals/corr/abs-2308-04416,article,Legal Summarisation through LLMs: The {PRODIGIT} Project,"Thiago Raulino Dal Pont and
Federico Galli and
Andrea Loreggia and
Giuseppe Pisano and
Riccardo Rovatti and
Giovanni Sartor",2023,CoRR,,abs/2308.04416,,,10.48550/ARXIV.2308.04416,https://doi.org/10.48550/arXiv.2308.04416,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2308-04416.bib,"Tue, 07 Nov 2023 00:00:00 +0100",,,,,,2308.04416,arXiv,,
DBLP:journals/corr/abs-2308-08043,article,"DiagGPT: An LLM-based Chatbot with Automatic Topic Management for
Task-Oriented Dialogue",Lang Cao,2023,CoRR,,abs/2308.08043,,,10.48550/ARXIV.2308.08043,https://doi.org/10.48550/arXiv.2308.08043,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2308-08043.bib,"Wed, 23 Aug 2023 01:00:00 +0200",,,,,,2308.08043,arXiv,,
DBLP:journals/corr/abs-2309-03852,article,"{FLM-101B:} An Open {LLM} and How to Train It with {\textdollar}100K
Budget","Xiang Li and
Yiqun Yao and
Xin Jiang and
Xuezhi Fang and
Xuying Meng and
Siqi Fan and
Peng Han and
Jing Li and
Li Du and
Bowen Qin and
Zheng Zhang and
Aixin Sun and
Yequan Wang",2023,CoRR,,abs/2309.03852,,,10.48550/ARXIV.2309.03852,https://doi.org/10.48550/arXiv.2309.03852,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2309-03852.bib,"Wed, 09 Jul 2025 01:00:00 +0200",,,,,,2309.03852,arXiv,,
DBLP:journals/corr/abs-2309-07418,article,"A Fast Optimization View: Reformulating Single Layer Attention in
{LLM} Based on Tensor and {SVM} Trick, and Solving It in Matrix Multiplication
Time","Yeqi Gao and
Zhao Song and
Weixin Wang and
Junze Yin",2023,CoRR,,abs/2309.07418,,,10.48550/ARXIV.2309.07418,https://doi.org/10.48550/arXiv.2309.07418,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2309-07418.bib,"Tue, 20 Aug 2024 01:00:00 +0200",,,,,,2309.07418,arXiv,,
DBLP:journals/corr/abs-2309-11653,article,"""It's a Fair Game"", or Is It? Examining How Users Navigate Disclosure
Risks and Benefits When Using LLM-Based Conversational Agents","Zhiping Zhang and
Michelle Jia and
Hao{-}Ping Hank Lee and
Bingsheng Yao and
Sauvik Das and
Ada Lerner and
Dakuo Wang and
Tianshi Li",2023,CoRR,,abs/2309.11653,,,10.48550/ARXIV.2309.11653,https://doi.org/10.48550/arXiv.2309.11653,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2309-11653.bib,"Thu, 02 May 2024 01:00:00 +0200",,,,,,2309.11653,arXiv,,
DBLP:journals/corr/abs-2311-06237,article,"Summon a Demon and Bind it: {A} Grounded Theory of {LLM} Red Teaming
in the Wild","Nanna Inie and
Jonathan Stray and
Leon Derczynski",2023,CoRR,,abs/2311.06237,,,10.48550/ARXIV.2311.06237,https://doi.org/10.48550/arXiv.2311.06237,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2311-06237.bib,"Sun, 06 Oct 2024 01:00:00 +0200",,,,,,2311.06237,arXiv,,
DBLP:journals/corr/abs-2312-02296,article,LLMs Accelerate Annotation for Medical Information Extraction,"Akshay Goel and
Almog Gueta and
Omry Gilon and
Chang Liu and
Sofia Erell and
Lan Huong Nguyen and
Xiaohong Hao and
Bolous Jaber and
Shashir Reddy and
Rupesh Kartha and
Jean Steiner and
Itay Laish and
Amir Feder",2023,CoRR,,abs/2312.02296,,,10.48550/ARXIV.2312.02296,https://doi.org/10.48550/arXiv.2312.02296,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2312-02296.bib,"Wed, 13 Dec 2023 00:00:00 +0100",,,,,,2312.02296,arXiv,,
DBLP:journals/corr/abs-2312-06121,article,Can LLMs Configure Software Tools,Jai Kannan,2023,CoRR,,abs/2312.06121,,,10.48550/ARXIV.2312.06121,https://doi.org/10.48550/arXiv.2312.06121,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2312-06121.bib,"Wed, 03 Jan 2024 00:00:00 +0100",,,,,,2312.06121,arXiv,,
DBLP:journals/corr/abs-2312-06742,article,Honeybee: Locality-enhanced Projector for Multimodal {LLM},"Junbum Cha and
Wooyoung Kang and
Jonghwan Mun and
Byungseok Roh",2023,CoRR,,abs/2312.06742,,,10.48550/ARXIV.2312.06742,https://doi.org/10.48550/arXiv.2312.06742,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2312-06742.bib,"Thu, 04 Jan 2024 00:00:00 +0100",,,,,,2312.06742,arXiv,,
DBLP:journals/corr/abs-2312-08055,article,Breaking the Silence: the Threats of Using LLMs in Software Engineering,"June Sallou and
Thomas Durieux and
Annibale Panichella",2023,CoRR,,abs/2312.08055,,,10.48550/ARXIV.2312.08055,https://doi.org/10.48550/arXiv.2312.08055,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2312-08055.bib,"Mon, 08 Jan 2024 00:00:00 +0100",,,,,,2312.08055,arXiv,,
DBLP:journals/corr/abs-2312-09731,article,"Uncovering the Causes of Emotions in Software Developer Communication
Using Zero-shot LLMs","Mia Mohammad Imran and
Preetha Chatterjee and
Kostadin Damevski",2023,CoRR,,abs/2312.09731,,,10.48550/ARXIV.2312.09731,https://doi.org/10.48550/arXiv.2312.09731,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2312-09731.bib,"Thu, 01 May 2025 01:00:00 +0200",,,,,,2312.09731,arXiv,,
DBLP:conf/kbse/AhmedD22,inproceedings,Few-shot training LLMs for project-specific code-summarization,"Toufique Ahmed and
Premkumar T. Devanbu",2022,,"37th {IEEE/ACM} International Conference on Automated Software Engineering,
{ASE} 2022, Rochester, MI, USA, October 10-14, 2022",,,177:1--177:5,10.1145/3551349.3559555,https://doi.org/10.1145/3551349.3559555,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/kbse/AhmedD22.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},,,,,,,,
DBLP:journals/corr/abs-2207-04237,article,Few-shot training LLMs for project-specific code-summarization,"Toufique Ahmed and
Premkumar T. Devanbu",2022,CoRR,,abs/2207.04237,,,10.48550/ARXIV.2207.04237,https://doi.org/10.48550/arXiv.2207.04237,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/corr/abs-2207-04237.bib,"Wed, 13 Jul 2022 01:00:00 +0200",,,,,,2207.04237,arXiv,,
DBLP:conf/cav/Berg20,inproceedings,{LLMC:} Verifying High-Performance Software,Freark I. van der Berg,2021,,"Computer Aided Verification - 33rd International Conference, {CAV}
2021, Virtual Event, July 20-23, 2021, Proceedings, Part {II}",12760,,690--703,10.1007/978-3-030-81688-9\_32,https://doi.org/10.1007/978-3-030-81688-9\_32,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/cav/Berg20.bib,"Fri, 23 Jul 2021 16:41:31 +0200",Springer,"Alexandra Silva and
K. Rustan M. Leino",Lecture Notes in Computer Science,,,,,,
DBLP:journals/jcst/LiCX18,article,"{LLMP:} Exploiting {LLDP} for Latency Measurement in Software-Defined
Data Center Networks","Yang Li and
Zhiping Cai and
Hong Xu",2018,J. Comput. Sci. Technol.,,33,2,277--285,10.1007/S11390-018-1819-2,https://doi.org/10.1007/s11390-018-1819-2,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/jcst/LiCX18.bib,"Sun, 19 Jan 2025 00:00:00 +0100",,,,,,,,,
DBLP:journals/tecs/BaiS13,article,"A software-only scheme for managing heap data on limited local memory(LLM)
multicore processors","Ke Bai and
Aviral Shrivastava",2013,{ACM} Trans. Embed. Comput. Syst.,,13,1,5:1--5:18,10.1145/2501626.2501632,https://doi.org/10.1145/2501626.2501632,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/journals/tecs/BaiS13.bib,"Sun, 19 Jan 2025 00:00:00 +0100",,,,,,,,,"This article presents a scheme for managing heap data in the local memory present in each core of a limited local memory (LLM) multicore architecture. Although managing heap data semi-automatically with software cache is feasible, it may require modifications of other thread codes. Crossthread modifications are very difficult to code and debug, and will become more complex and challenging as we increase the number of cores. In this article, we propose an intuitive programming interface, which is an automatic and scalable scheme for heap data management. Besides, for embedded applications, where the maximum heap size can be profiled, we propose several optimizations on our heap management to significantly decrease the library overheads. Our experiments on several benchmarks from MiBench executing on the Sony Playstation 3 show that our scheme is natural to use, and if we know the maximum size of heap data, our optimizations can improve application performance by an average of 14%."
DBLP:conf/asap/BaiSK11,inproceedings,"Stack data management for Limited Local Memory {(LLM)} multi-core
processors","Ke Bai and
Aviral Shrivastava and
Saleel Kudchadker",2011,,"22nd {IEEE} International Conference on Application-specific Systems,
Architectures and Processors, {ASAP} 2011, Santa Monica, CA, USA,
Sept. 11-14, 2011",,,231--234,10.1109/ASAP.2011.6043275,https://doi.org/10.1109/ASAP.2011.6043275,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/asap/BaiSK11.bib,"Thu, 27 Jul 2023 01:00:00 +0200",{IEEE} Computer Society,"Joseph R. Cavallaro and
Milos D. Ercegovac and
Frank Hannig and
Paolo Ienne and
Earl E. Swartzlander Jr. and
Alexandre F. Tenca",,,,,,,
DBLP:conf/codes/BaiS10,inproceedings,Heap data management for limited local memory {(LLM)} multi-core processors,"Ke Bai and
Aviral Shrivastava",2010,,"Proceedings of the 8th International Conference on Hardware/Software
Codesign and System Synthesis, {CODES+ISSS} 2010, part of ESWeek '10
Sixth Embedded Systems Week, Scottsdale, AZ, USA, October 24-28, 2010",,,317--326,10.1145/1878961.1879015,https://doi.org/10.1145/1878961.1879015,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/codes/BaiS10.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},"Tony Givargis and
Adam Donlin",,,,,,,
DBLP:conf/petra/FrantzidisB09,inproceedings,"Description and future trends of {ICT} solutions offered towards independent
living: the case of {LLM} project","Christos A. Frantzidis and
Panagiotis D. Bamidis",2009,,"Proceedings of the 2nd International Conference on Pervasive Technologies
Related to Assistive Environments, {PETRA} 2009, Corfu, Greece, June
9-13, 2009",,,,10.1145/1579114.1579173,https://doi.org/10.1145/1579114.1579173,,"dblp computer science bibliography, https://dblp.org",https://dblp.org/rec/conf/petra/FrantzidisB09.bib,"Sun, 19 Jan 2025 00:00:00 +0100",{ACM},,{ACM} International Conference Proceeding Series,,,,,,
