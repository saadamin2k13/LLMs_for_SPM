title,authors,doi,year,abstract
Multi-task Learning based Pre-trained Language Model for Code Completion,"Liu, F; Li, G; Zhao, YF; Jin, Z",10.1145/3324884.3416591,2020,"Code completion is one of the most useful features in the Integrated Development Environments (IDEs), which can accelerate software development by suggesting the next probable token based on the contextual code in real-time. Recent studies have shown that statistical language modeling techniques can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from two major drawbacks: a) Existing research uses static embeddings, which map a word to the same vector regardless of its context. The differences in the meaning of a token in varying contexts are lost when each token is associated with a single representation; b) Existing language model based code completion models perform poor on completing identifiers, and the type information of the identifiers is ignored in most of these models. To address these challenges, in this paper, we develop a multi-task learning based pre-trained language model for code understanding and code generation with a Transformer-based neural architecture. We pre-train it with hybrid objective functions that incorporate both code understanding and code generation tasks. Then we fine-tune the pre-trained model on code completion. During the completion, our model does not directly predict the next token. Instead, we adopt multi-task learning to predict the token and its type jointly and utilize the predicted type to assist the token prediction. Experiments results on two real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods."
The Programmer's Assistant: Conversational Interaction with a Large Language Model for Software Development,"Ross, SI; Martinez, F; Houde, S; Muller, M; Weisz, JD",10.1145/3581641.3584037,2023,"Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model's responses. We developed a prototype system - the Programmer's Assistant - in order to explore the utility of conversational interactions grounded in code, as well as software engineers' receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant's capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development."
LLM4SecHW: Leveraging Domain-Specific Large Language Model for Hardware Debugging,"Fu, WM; Yang, KC; Dutta, RG; Guo, XL; Qu, G",10.1109/AsianHOST59942.2023.10409307,2023,"This paper presents LLM4SECHW, a novel framework for hardware debugging that leverages domain-specific Large Language Model (LLM). Despite the success of LLMs in automating various software development tasks, their application in the hardware security domain has been limited due to the constraints of commercial LLMs and the scarcity of domain-specific data. To address these challenges, we propose a unique approach to compile a dataset of open-source hardware design defects and their remediation steps, utilizing version control data. This dataset provides a substantial foundation for training machine learning models for hardware. LLM4SECHW employs fine-tuning of medium-sized LLMs based on this dataset, enabling the identification a nd r ectification of bugs in hardware designs. This pioneering approach offers a reference workflow for the application of fine-tuning domain-specific LLMs in ot her research areas. We evaluate the performance of our proposed system on various open-source hardware designs, demonstrating its efficacy i n accurately identifying and correcting defects. Our work brings a new perspective on automating the quality control process in hardware design."
LARCH: Large Language Model-based Automatic Readme Creation with Heuristics,"Koreeda, Y; Morishita, T; Imaichi, O; Sogawa, Y",10.1145/3583780.3614744,2023,"Writing a readme is a crucial aspect of software development as it plays a vital role in managing and reusing program code. Though it is a pain point for many developers, automatically creating one remains a challenge even with the recent advancements in large language models (LLMs), because it requires generating an abstract description from thousands of lines of code. In this demo paper, we show that LLMs are capable of generating a coherent and factually correct readmes if we can identify a code fragment that is representative of the repository. Building upon this finding, we developed LARCH (LLM-based Automatic Readme Creation with Heuristics) which leverages representative code identification with heuristics and weak supervision. Through human and automated evaluations, we illustrate that LARCH can generate coherent and factually correct readmes in the majority of cases, outperforming a baseline that does not rely on representative code identification. We have made LARCH open-source and provided a cross-platform Visual Studio Code interface and command-line interface, accessible at https://github.com/hitachi-nlp/larch. A demo video showcasing LARCH's capabilities is available at https://youtu.be/ZUKkh5ED-O4."
Zero-shot Prompting for Code Complexity Prediction Using GitHub Copilot,"Siddiq, ML; Samee, A; Azgor, SR; Haider, MA; Sawraz, SI; Santos, JCS",10.1109/NLBSE59153.2023.00018,2023,"Code generation models are gaining popularity because they can produce correct code from a prompt, speeding up the software development process. GitHub Copilot is currently one of the most commonly used tools for code generation. This tool is based on GPT3, a Large Language Model (LLM), and can perform zero-shot prompting tasks i.e., tasks for which the model is not specifically trained. In this paper, we describe a preliminary study that investigates whether GitHub Copilot can predict the runtime complexity of a given program using zeroshot prompting. In our study, we found that GitHub Copilot can correctly predict the runtime complexity 45.44% times in the first suggestion and 56.38% times considering all suggestions. We also compared Copilot to other machine learning, neural network, and transformer-based approaches for code complexity prediction. We observed that Copilot outperformed other approaches for predicting code with linear complexity O(n)."
ReqGen: Keywords-Driven Software Requirements Generation,"Zhao, ZY; Zhang, L; Lian, XL; Gao, XY; Lv, HY; Shi, L",10.3390/math11020332,2023,"Software requirements specification is undoubtedly critical for the whole software life-cycle. Currently, writing software requirements specifications primarily depends on human work. Although massive studies have been proposed to speed up the process via proposing advanced elicitation and analysis techniques, it is still a time-consuming and error-prone task, which needs to take domain knowledge and business information into consideration. In this paper, we propose an approach, named ReqGen, which can provide further assistance by automatically generating natural language requirements specifications based on certain given keywords. Specifically, ReqGen consists of three critical steps. First, keywords-oriented knowledge is selected from the domain ontology and is injected into the basic Unified pre-trained Language Model (UniLM) for domain fine-tuning. Second, a copy mechanism is integrated to ensure the occurrence of keywords in the generated statements. Finally, a requirements-syntax-constrained decoding is designed to close the semantic and syntax distance between the candidate and reference specifications. Experiments on two public datasets from different groups and domains show that ReqGen outperforms six popular natural language generation approaches with respect to the hard constraint of keywords' (phrases') inclusion, BLEU, ROUGE, and syntax compliance. We believe that ReqGen can promote the efficiency and intelligence of specifying software requirements."
Towards a question answering assistant for software development using a transformer-based language model,"Vale, LD; Maia, MD",10.1109/BotSE52550.2021.00016,2021,"Question answering platforms, such as Stack Overflow, have impacted substantially how developers search for solutions for their programming problems. The crowd knowledge content available from such platforms has also been used to leverage software development tools. The recent advances on Natural Language Processing, specifically on more powerful language models, have demonstrated ability to enhance text understanding and generation. In this context, we aim at investigating the factors that can influence on the application of such models for understanding source code related data and produce more interactive and intelligent assistants for software development. In this preliminary study, we particularly investigate if a how-to question filter and the level of context in the question may impact the results of a question answering transformer-based model. We suggest that fine-tuning models with corpus based on how-to questions can impact positively in the model and more contextualized questions also induce more objective answers."
CAT-LM Training Language Models on Aligned Code And Tests,"Rao, N; Jain, K; Alon, U; Le Goues, C; Hellendoorn, VJ",10.1109/ASE56229.2023.00193,2023,"Testing is an integral but often neglected part of the software development process. Classical test generation tools such as EvoSuite generate behavioral test suites by optimizing for coverage, but tend to produce tests that are hard to understand. Language models trained on code can generate code that is highly similar to that written by humans, but current models are trained to generate each file separately, as is standard practice in natural language processing, and thus fail to consider the code-under-test context when producing a test file. In this work, we propose the Aligned Code And Tests Language Model (CAT-LM), a GPT-style language model with 2.7 Billion parameters, trained on a corpus of Python and Java projects. We utilize a novel pretraining signal that explicitly considers the mapping between code and test files when available. We also drastically increase the maximum sequence length of inputs to 8,192 tokens, 4x more than typical code generation models, to ensure that the code context is available to the model when generating test code. We analyze its usefulness for realistic applications, showing that sampling with filtering (e.g., by compilability, coverage) allows it to efficiently produce tests that achieve coverage similar to ones written by developers while resembling their writing style. By utilizing the code context, CAT-LM generates more valid tests than even much larger language models trained with more data (CodeGen 16B and StarCoder) and substantially outperforms a recent test-specific model (TeCo) at test completion. Overall, our work highlights the importance of incorporating software-specific insights when training language models for code and paves the way to more powerful automated test generation."
Is ChatGPT Capable of Crafting Gamification Strategies for Software Engineering Tasks?,"Fukith, T; Torchiano, M",10.1145/3617553.3617887,2023,"Gamification has gained significant attention in the last decade for its potential to enhance engagement and motivation in various domains. During the last year ChatGPT, a state-of-the-art large language model has received even more attention both in the field of scientific research and in common use by individuals or companies. In this study, we investigate the possibility of adopting ChatGPT as a tool for designing gamification platforms in the Software Engineering domain. Leveraging the capabilities of ChatGPT, we assess how good is it at generating effective suggestions and ideas for designers or developers. To evaluate ChatGPT's potential as a gamification platform creator we narrowed the context to one particular Software Engineering activity, asking for possible aspects of the activity to be gamified. Each proposed aspect was subsequently unraveled by ChatGPT both asking in a shared and separate context, first following the conversational nature of the model, then applying a validated design framework. The study assesses ChatGPT's ability to select and integrate game elements to build a thriving gamification environment by framing the design of the platform to a state-of-the-art conceptual framework. To evaluate the goodness of the design choices made we relied both on the Octalysis framework and on personal experience. The findings of the papers show that ChatGPT can only create simple playful experiences not very effective. Although, by instructing the model with more specific desired mechanics and dynamics, it is possible to guide it toward the application of the ideas suggested. We argue that ChatGPT is not capable of building a gamified environment on its own, but it could still be used to build the foundation of a gamification platform as long as the designers refine and rough out the advice gained from a user-centered solution."
You Augment Me: Exploring ChatGPT-based Data Augmentation for Semantic Code Search,"Wang, YL; Guo, LH; Shi, ES; Chen, WQ; Chen, JC; Zhong, WJ; Wang, MH; Li, H; Zhang, HY; Lyu, ZY; Zheng, ZB",10.1109/ICSME58846.2023.00014,2023,"Code search plays a crucial role in software development, enabling developers to retrieve and reuse code using natural language queries. While the performance of code search models improves with an increase in high-quality data, obtaining such data can be challenging and expensive. Recently, large language models (LLMs) such as ChatGPT have made remarkable progress in both natural and programming language understanding and generation, offering user-friendly interaction via simple prompts. Inspired by these advancements, we propose a novel approach ChatDANCE, which utilizes high-quality and diverse augmented data generated by a large language model and leverages a filtering mechanism to eliminate low-quality augmentations. Specifically, we first propose a set of ChatGPT prompting rules that are specifically designed for source code and queries. Then, we leverage ChatGPT to rewrite code and queries based on the according prompts and then propose a filtering mechanism which trains a cross-encoder from the backbone model UniXcoder to filter out code and query pairs with low matching scores. Finally, we re-train the backbone model using the obtained high-quality augmented data. Experimental results show that ChatDANCE achieves state-of-the-art performance, improving the best baseline by 13.2% (R@1) and 7% (MRR). Surprisingly, we find that this augment-filter-retrain strategy enables the backbone model (UniXcoder) to self-grow. Moreover, extensive experiments show the effectiveness of each component and ChatDANCE has stable performance under different hyperparameter settings. In addition, we conduct qualitative and quantitative analyses to investigate why ChatDANCE works well and find that it learns a more uniform distribution of representations and effectively aligns the code and query spaces. We have made the code and data anonymously available at https://anonymous.4open.science/r/ChatDANCE."
A Unified Knowledge Graph Augmentation Service for Boosting Domain-specific NLP Tasks,"Ding, RQ; Han, X; Wang, LY",,2023,"By focusing the pre-training process on domain-specific corpora, some domain-specific pre-trained language models (PLMs) have achieved state-of-the-art results. However, it is under-investigated to design a unified paradigm to inject domain knowledge in the PLM fine-tuning stage. We propose KnowledgeDA, a unified domain language model development service to enhance the task-specific training procedure with domain knowledge graphs. Given domain-specific task texts input, KnowledgeDA can automatically generate a domain-specific language model following three steps: (i) localize domain knowledge entities in texts via an embedding-similarity approach; (ii) generate augmented samples by retrieving replaceable domain entity pairs from two views of both knowledge graph and training data; (iii) select high-quality augmented samples for fine-tuning via confidence-based assessment. We implement a prototype of KnowledgeDA to learn language models for two domains, healthcare and software development. Experiments on domain-specific text classification and QA tasks verify the effectiveness and generalizability of KnowledgeDA."
SPM: Structured Pretraining and Matching Architectures for Relevance Modeling in Meituan Search,"Zan, W; Han, YP; Jiang, XT; Xiao, Y; Yang, Y; Chen, DY; Chen, S",10.1145/3583780.3615500,2023,"In e-commerce search, relevance between query and documents is an essential requirement for satisfying user experience. Different from traditional e-commerce platforms that offer products, users search on life service platforms such as Meituan mainly for product providers, which usually have abundant structured information, e.g. name, address, category, thousands of products. Modeling search relevance with these rich structured contents is challenging due to the following issues: (1) there is language distribution discrepancy among different fields of structured document, making it difficult to directly adopt off-the-shelf pretrained language model based methods like BERT. (2) different fields usually have different importance and their length vary greatly, making it difficult to extract document information helpful for relevance matching. To tackle these issues, in this paper we propose a novel two-stage pretraining and matching architecture for relevance matching with rich structured documents. At pretraining stage, we propose an effective pretraining method that employs both query and multiple fields of document as inputs, including an effective information compression method for lengthy fields. At relevance matching stage, a novel matching method is proposed by leveraging domain knowledge in search query to generate more effective document representations for relevance scoring. Extensive offline experiments and online A/B tests on millions of users verify that the proposed architectures effectively improve the performance of relevance modeling. The model has already been deployed online, serving the search traffic of Meituan for over a year."
"Flakify: A Black-Box, Language Model-Based Predictor for Flaky Tests","Fatima, S; Ghaleb, TA; Briand, L",10.1109/TSE.2022.3201209,2023,"Software testing assures that code changes do not adversely affect existing functionality. However, a test case can be flaky, i.e., passing and failing across executions, even for the same version of the source code. Flaky test cases introduce overhead to software development as they can lead to unnecessary attempts to debug production or testing code. Besides rerunning test cases multiple times, which is time-consuming and computationally expensive, flaky test cases can be predicted using machine learning (ML) models, thus reducing the wasted cost of re-running and debugging these test cases. However, the state-of-the-art ML-based flaky test case predictors rely on pre-defined sets of features that are either project-specific, i.e., inapplicable to other projects, or require access to production code, which is not always available to software test engineers. Moreover, given the non-deterministic behavior of flaky test cases, it can be challenging to determine a complete set of features that could potentially be associated with test flakiness. Therefore, in this article, we propose Flakify, a black-box, language model-based predictor for flaky test cases. Flakify relies exclusively on the source code of test cases, thus not requiring to (a) access to production code (black-box), (b) rerun test cases, (c) pre-define features. To this end, we employed CodeBERT, a pre-trained language model, and fine-tuned it to predict flaky test cases using the source code of test cases. We evaluated Flakify on two publicly available datasets (FlakeFlagger and IDoFT) for flaky test cases and compared our technique with the FlakeFlagger approach, the best state-of-the-art ML-based, white-box predictor for flaky test cases, using two different evaluation procedures: (1) cross-validation and (2) per-project validation, i.e., prediction on new projects. Flakify achieved F1-scores of 79% and 73% on the FlakeFlagger dataset using cross-validation and per-project validation, respectively. Similarly, Flakify achieved F1-scores of 98% and 89% on the IDoFT dataset using the two validation procedures, respectively. Further, Flakify surpassed FlakeFlagger by 10 and 18 percentage points (pp) in terms of precision and recall, respectively, when evaluated on the FlakeFlagger dataset, thus reducing the cost bound to be wasted on unnecessarily debugging test cases and production code by the same percentages (corresponding to reduction rates of 25% and 64%). Flakify also achieved significantly higher prediction results when used to predict test cases on new projects, suggesting better generalizability over FlakeFlagger. Our results further show that a black-box version of FlakeFlagger is not a viable option for predicting flaky test cases."
Source Code Assessment and Classification Based on Estimated Error Probability Using Attentive LSTM Language Model and Its Application in Programming Education,"Rahman, MM; Watanobe, Y; Nakamura, K",10.3390/app10082973,2020,"The rate of software development has increased dramatically. Conventional compilers cannot assess and detect all source code errors. Software may thus contain errors, negatively affecting end-users. It is also difficult to assess and detect source code logic errors using traditional compilers, resulting in software that contains errors. A method that utilizes artificial intelligence for assessing and detecting errors and classifying source code as correct (error-free) or incorrect is thus required. Here, we propose a sequential language model that uses an attention-mechanism-based long short-term memory (LSTM) neural network to assess and classify source code based on the estimated error probability. The attentive mechanism enhances the accuracy of the proposed language model for error assessment and classification. We trained the proposed model using correct source code and then evaluated its performance. The experimental results show that the proposed model has logic and syntax error detection accuracies of 92.2% and 94.8%, respectively, outperforming state-of-the-art models. We also applied the proposed model to the classification of source code with logic and syntax errors. The average precision, recall, and F-measure values for such classification are much better than those of benchmark models. To strengthen the proposed model, we combined the attention mechanism with LSTM to enhance the results of error assessment and detection as well as source code classification. Finally, our proposed model can be effective in programming education and software engineering by improving code writing, debugging, error-correction, and reasoning."
A Technique to Pre-trained Neural Network Language Model Customization to Software Development Domain,"Dudarin, PV; Tronin, VG; Svyatov, K",10.1007/978-3-030-30763-9_14,2019,"According to the CHAOS report from Standish Group during 1992-2017, the degree of success of projects in the development of software intensive systems (Software Intensive Systems, SIS) has changed insignificantly, remaining at the level of 50% inconsistency with the initial requirements (finance, time and functionality) for medium-sized projects. The annual financial losses in the world due to the total failures are of the order of hundreds of billion dollars. The majority of information about software projects has textual representation. Analysis of this information is vital for project status understanding, revealing problems on the early stage. Nowadays the majority of tasks in NLP field are solved by means of neural network language models. These models already have shown state-of-the-art results in classification, translation, named entity recognition, and so on. Pre-trained models are accessible in the internet, but the real life problem domain could differ from the origin domain where the network was learned. In this paper an approach to vocabulary expansion for neural network language model by means of hierarchical clustering is presented. This technique allows one to adopt pre-trained language model to a different domain."
Automated Extraction of Requirement Entities by Leveraging LSTM-CRF and Transfer Learning,"Li, MY; Yang, Y; Shi, L; Wang, Q; Hu, J; Peng, XH; Liao, WM; Pi, GZ",10.1109/ICSME46990.2020.00029,2020,"Requirement entities, explicit specification of concepts that define the primary function objects, play an important role in requirement analysis for software development and maintenance. It is a labor-intensive activity to extract requirement entities from textual requirements, which is typically done manually. A few existing studies propose automated methods to support key requirement concept extraction. However, they face two main challenges: lack of domain-specific natural language processing techniques and expensive labeling effort. To address the challenges, this study presents a novel approach named RENE, which employs LSTM-CRF model for requirement entity extraction and introduces the general knowledge to reduce the demands for labeled data. It consists of four phases: 1) Model construction, where RENE builds LSTM-CRF model and an isomorphic LSTM language model for transfer learning; 2) LSTM language model training, where RENE captures general knowledge and adapt to requirement context; 3) LSTM-CRF training, where RENE trains the LSTM-CRF model with the transferred layers; 4) Requirement entity extraction, where RENE applies the trained LSTM-CRF model to a new-coming requirement, and automatically extracts its requirement entities. RENE is evaluated using two methods: evaluation on historical dataset and user study. The evaluation on the historical dataset shows that RENE could achieve 79% precision, 81% recall, and 80% F1. The evaluation results from the user study also suggest that RENE could produce more accurate and comprehensive requirement entities, compared with those produced by engineers."
Choose Your Programming Copilot A Comparison of the Program Synthesis Performance of GitHub Copilot and Genetic Programming,"Sobania, D; Briesch, M; Rothlauf, F",10.1145/3512290.3528700,2022,"GitHub Copilot, an extension for the Visual Studio Code development environment powered by the large-scale language model Codex, makes automatic program synthesis available for software developers. This model has been extensively studied in the field of deep learning, however, a comparison to genetic programming, which is also known for its performance in automatic program synthesis, has not yet been carried out. In this paper, we evaluate GitHub Copilot on standard program synthesis benchmark problems and compare the achieved results with those from the genetic programming literature. In addition, we discuss the performance of both approaches. We find that the performance of the two approaches on the benchmark problems is quite similar, however, in comparison to GitHub Copilot, the program synthesis approaches based on genetic programming are not yet mature enough to support programmers in practical software development. Genetic programming usually needs a huge amount of expensive hand-labeled training cases and takes too much time to generate solutions. Furthermore, source code generated by genetic programming approaches is often bloated and difficult to understand. For future work on program synthesis with genetic programming, we suggest researchers to focus on improving the execution time, readability, and usability."
Transfer Learning Code Vectorizer based Machine Learning Models for Software Defect Prediction,"Singh, R; Singh, J; Gill, MS; Malhotra, R; Garima",,2020,"Software development life cycle comprises of planning, design, implementation, testing and eventually, deployment. Software defect prediction can be used in the initial stages of the development life cycle for identifying defective modules. Researchers have devised various methods that can be used for effective software defect prediction. The prediction of the presence of defects or bugs in a software module can facilitate the testing process as it would enable developers and testers to allocate their time and resources on modules that are prone to defects. Transfer learning can be used for transferring knowledge obtained from one domain into the other. In this paper, we propose Transfer Learning Code Vectorizer, a novel method that derives features from the text of the software source code itself and uses those features for defect prediction. We focus on the software code and convert it into vectors using a pre-trained deep learning language model. These code vectors are subsequently passed through machine and deep learning models. Further, we compare the results of using deep learning on the text of the software code versus the usage of software metrics for prediction of defects. In terms of weighted F1 scores, the experiments show that applying the proposed TLCV method outperforms the other machine learning techniques by 9.052%."
Supporting Product Management Lifecycle with Common Best Practices,"Walter, B; Jolevski, IA; Garnizov, I; Arsovic, A",10.1007/978-3-031-42310-9_15,2023,"Product Lifecycle Management is a process that helps projects to pass through various phases of software development and maintenance. Since phases are usually associated with entry- and exit-criteria that could be considered onerous or excessively effort-prone, implementation of PLM poses various risks. In this paper we show how Common Best Practices could support the software teams in meeting PLM requirements and facilitate smoother transition between phases."
Context-Sensitive Spelling Checker for Assamese Language,"Choudhury, R; Deb, N; Kashyap, K",10.1007/978-981-13-1280-9_18,2019,"The task of finding real-word error in sentences is a complex problem in the field of natural language processing. In this paper, we are presenting a new method for context-sensitive spell checking for Assamese language, a technique to tackle the issue of real-word error detection and correction. The emphasis is concentrated on n-gram language model for Assamese language to develop the spelling checker. The purpose is to detect spelling mistake according to the context of the sentence and suggest the most probable word out of a confusion set generated for each of the misspelled words in the sentence. The system was tested for spell checking and word suggestion on the basis of precision and recall of correct and incorrect words. An overall performance of 76% was attained. It can be inferred that the system has the potentiality for further research and other software development for Assamese language."
Consumer Engagement in the Design of PLM Systems: A Review of Best Practices,"Nwogu, U; Evans, R",10.1007/978-3-031-43662-8_27,2023,"Digitization has disrupted all aspects of Product Development (PD) from idea conceptualization through to the retirement or recycling of a product. With advancements in collaborative technologies, product development teams must seek competitive advantage from their use but, at the same time, improve their understanding about how they can best be integrated into current workflows and processes. Product Lifecycle Management (PLM) systems are designed to assist firms in the management of the entire PD lifecycle. They use advanced technologies to bring together all stakeholders involved in the PD lifecycle with the aim of satisfying the complex needs and requirements of consumers. Using a mix of literature review and content analysis tools, this paper presents a critical review of best practices for engaging consumers in the design of PLM systems. Specifically, we examine current levels of consumer engagement, benefits of consumer engagement, the effective integration of consumers, and best practices for consumer engagement in the design of PLM systems."
Harnessing Predictive Modeling and Software Analytics in the Age of LLM-Powered Software Development,"Khomh, F",10.1145/3617555.3634736,2023,"In the rapidly evolving landscape of software development, Large Language Models (LLM) have emerged as powerful tools that can significantly impact the way software code is written, reviewed, and optimized, making them invaluable resources for programmers. They offer developers the ability to leverage pre-trained knowledge and tap into vast code repositories, enabling faster development cycles and reducing the time spent on repetitive or mundane coding tasks. However, while these models offer substantial benefits, their adoption also presents multiple challenges. For example, they might generate code snippets that are syntactically correct but functionally flawed, requiring human review and validation. Moreover, the ethical considerations surrounding these models, such as biases in the training data, should be carefully addressed to ensure fair and inclusive software development practices. This talk will provide an overview and reflection on some of these challenges, present some preliminary solutions, and discuss opportunities for predictive models and data analytics."
PLM/ALM Integration With The Asset Administration Shell,"Deuter, A; Imort, S",10.1016/j.promfg.2020.11.040,2020,"Modern product development utilizes both Product Lifecycle Management (PLM) and Application Lifecycle Management (ALM). PLM addresses the hardware lifecycle of a product, whereas ALM addresses the software lifecycle. In recent years, industry and academia have developed several PLM/ALM integration concepts to realize efficient management of the product lifecycle across all domains. However, the solutions available in practice are typically vendor-driven. Therefore, they are not generally applicable even if standards such as OSLC (Open Services for Lifecycle Cooperation) are applied. The consortium Plattform Industrie 4.0 has recently introduced a standardized digital representation of an asset (e.g., a smart product): the Asset Administration Shell (AAS). The AAS has the potential to integrate PLM/ALM data sets in a single product model and hence to provide a generally applicable interface for PLM/ALM integration. However, until now there has not been a concept to prove this potential. The aim of this work is to develop such new strategies (named Plm4AAS) using AAS submodels. This article explains the semi-automatic generation of PLM/ALM submodels and how to link elements between these submodels. The AASX Package Explorer, an AAS management software tool, is used to demonstrate the results. The article finishes with a discussion about the potential of the AAS as a standardized concept for PLM/ALM integration. (C) 2020 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the scientific committee of the 5th International Conference on System-Integrated Intelligence."
Software Vulnerability Detection using Large Language Models,"Das Purba, M; Ghosh, A; Radford, BJ; Chu, B",10.1109/ISSREW60843.2023.00058,2023,"Software development is among the first demonstrations of using Large Language Models (LLMs) to enhance human productivity. Such a co-pilot paradigm envisions LLM working side-by-side with human developers to assist in programming tasks. Ensuring the security of software products is a critical factor for the success of such a paradigm. There have been various anecdotal reports on the success of using LLMs to detect vulnerabilities in programs. This paper reports a set of experiments applying four well-known LLMs to two widely referenced public datasets to evaluate the performance of LLMs in detecting software vulnerabilities. Our results show a significant performance gap between these LLMs and those from popular static analysis tools, primarily due to their high false positive rates. However, LLMs show great promise in identifying subtle patterns commonly associated with software vulnerabilities. This observation suggests a possible path forward by combining LLMs and other program analysis techniques to achieve better software vulnerability detection."
CCTEST: Testing and Repairing Code Completion Systems,"Li, ZJ; Wang, CZ; Liu, ZB; Wang, HX; Chen, D; Wang, S; Gao, CY",10.1109/ICSE48619.2023.00110,2023,"Code completion, a highly valuable topic in the software development domain, has been increasingly promoted for use by recent advances in large language models (LLMs). To date, visible LLM-based code completion frameworks such as GitHub Copilot and GPT are trained using deep learning over vast quantities of unstructured text and open source code. As the paramount component and the cornerstone in daily programming tasks, code completion has largely boosted professionals' efficiency in building real-world software systems. In contrast to this flourishing market, we find that code completion systems often output suspicious results, and to date, an automated testing and enhancement framework for code completion systems is not available. This research proposes CCTEST, a framework to test and repair code completion systems in black-box settings. CCTEST features a set of novel mutation strategies, namely program structure-consistent (PSC) mutations, to generate mutated code completion inputs. Then, it detects inconsistent outputs, representing possibly erroneous cases, from all the completed code cases. Moreover, CCTEST repairs the code completion outputs by selecting the output that mostly reflects the average appearance of all output cases, as the final output of the code completion systems. With around 18K test inputs, we detected 33,540 inputs that can trigger erroneous cases (with a true positive rate of 86%) from eight popular LLM-based code completion systems. With repairing, we show that the accuracy of code completion systems is notably increased by 40% and 67% with respect to BLEU score and Levenshtein edit similarity."
An initial investigation of ChatGPT unit test generation capability,"Guilherme, VH; Vincenzi, AMR",10.1145/3624032.3624035,2023,"Context: Software testing ensures software quality, but developers often disregard it. The use of automated testing generation is pursued to reduce the consequences of overlooked test cases in a software project. Problem: In the context of Java programs, several tools can completely automate generating unit test sets. Additionally, studies are conducted to offer evidence regarding the quality of the generated test sets. However, it is worth noting that these tools rely on machine learning and other AI algorithms rather than incorporating the latest advancements in Large Language Models (LLMs). Solution: This work aims to evaluate the quality of Java unit tests generated by an OpenAI LLM algorithm, using metrics like code coverage and mutation test score. Method: For this study, 33 programs used by other researchers in the field of automated test generation were selected. This approach was employed to establish a baseline for comparison purposes. For each program, 33 unit test sets were generated automatically, without human interference, by changing Open AI API parameters. After executing each test set, metrics such as code line coverage, mutation score, and success rate of test execution were collected to evaluate the efficiency and effectiveness of each set. Summary of Results: Our findings revealed that the OpenAI LLM test set demonstrated similar performance across all evaluated aspects compared to traditional automated Java test generation tools used in the previous research. These results are particularly remarkable considering the simplicity of the experiment and the fact that the generated test code did not undergo human analysis."
Extracting Domain Models from Textual Requirements in the Era of Large Language Models,"Arulmohan, S; Meurs, MJ; Mosser, S",10.1109/MODELS-C59198.2023.00096,2023,"Requirements Engineering is a critical part of the software lifecycle, describing what a given piece of software will do (functional) and how it will do it (non-functional). Requirements documents are often textual, and it is up to software engineers to extract the relevant domain models from the text, which is an error-prone and time-consuming task. Considering the recent attention gained by Large Language Models (LLMs), we explored how they could support this task. This paper investigates how such models can be used to extract domain models from agile product backlogs and compare them to (i) a state-of-practice tool as well as (ii) a dedicated Natural Language Processing (NLP) approach, on top of a reference dataset of 22 products and 1, 679 user stories. Based on these results, this paper is a first step towards using LLMs and/or tailored NLP to support automated requirements engineering thanks to model extraction using artificial intelligence."
Cannabidiol acts as molecular switch in innate immune cells to promote the biosynthesis of inflammation-resolving lipid mediators,"Peltner, LK; Gluthmann, L; Borner, F; Pace, S; Hoffstetter, RK; Kretzer, C; Bilancia, R; Pollastro, F; Koeberle, A; Appendino, G; Rossi, A; Newcomer, ME; Gilbert, NC; Werz, O; Jordan, PM",10.1016/j.chembiol.2023.08.001,2023,"Cannabinoids are phytochemicals from cannabis with anti-inflammatory actions in immune cells. Lipid mediators (LM), produced from polyunsaturated fatty acids (PUFA), are potent regulators of the immune response and impact all stages of inflammation. How cannabinoids influence LM biosynthetic networks is unknown. Here, we reveal cannabidiol (CBD) as a potent LM class-switching agent that stimulates the production of specialized pro-resolving mediators (SPMs) but suppresses pro-inflammatory eicosanoid biosynthesis. Detailed metabololipidomics analysis in human monocyte-derived macrophages showed that CBD (i) upregulates exotoxin-stimulated generation of SPMs, (ii) suppresses 5-lipoxygenase (LOX) mediated leukotriene production, and (iii) strongly induces SPM and 12/15-LOX product formation in resting cells by stimulation of phospholipase A2-dependent PUFA release and through Ca2+-independent, allosteric 15-LOX-1 activation. Finally, in zymosan-induced murine peritonitis, CBD increased SPM and 12/15-LOX products and suppressed pro-inflammatory eicosanoid levels in vivo. Switching eicosanoid to SPM production is a plausible mode of action of CBD and a promising inflammation-resolving strategy."
Differential impact of 5-lipoxygenase-activating protein antagonists on the biosynthesis of leukotrienes and of specialized pro-resolving mediators,"Dahlke, P; Peltner, LK; Jordan, PM; Werz, O",10.3389/fphar.2023.1219160,2023,"Lipoxygenases (LOX) transform arachidonic acid (AA, C20:4) and docosahexaenoic acid (DHA, C22:6) into bioactive lipid mediators (LMs) that comprise not only pro-inflammatory leukotrienes (LTs) but also the specialized pro-resolving mediators (SPMs) that promote inflammation resolution and tissue regeneration. The 5-LOX-activating protein (FLAP) is known to provide AA as a substrate to 5-LOX for generating LTs, such as LTB4, a potent chemoattractant and activator of phagocytes. Notably, 5-LOX is also involved in the biosynthesis of certain SPMs, namely, lipoxins and D-resolvins, implying a role of FLAP in SPM formation. FLAP antagonists have been intensively developed as LT biosynthesis inhibitors, but how they impact SPM formation is a matter of debate. Here, we show that FLAP antagonism suppresses the conversion of AA by 5-LOX to LT and lipoxins, while the conversion of DHA to SPM is unaffected. Screening of multiple prominent FLAP antagonists for their effects on LM formation in human M1- and M2-monocyte-derived macrophages by comprehensive LM profiling showed that all nine compounds reduced the production of 5-LOX-derived LTs but increased the formation of SPMs from DHA, e.g., resolvin D5. Some FLAP antagonists, especially those that contain an indole or benzimidazole moiety, even elicited SPM formation in resting M2-monocyte-derived macrophages. Intriguingly, in coincubations of human neutrophils and platelets that produce substantial AA-derived lipoxin and DHA-derived RvD5, FLAP antagonism abolished lipoxin formation, but resolvin D5 levels remained unaffected. Conclusively, antagonism of FLAP suppresses the conversion of AA by 5-LOX to LTs and lipoxins but not the conversion of DHA by 5-LOX to SPM, which should be taken into account for the development of such compounds as anti-inflammatory drugs."
Developing the Requirements of a PLM/ALM Integration: An Industrial Case Study,"Deuter, A; Otte, A; Ebert, M; Possel-DÃ¶lken, F",10.1007/978-3-030-16134-7_11,2019,"The digitization of the industry, the drive towards smart factories as well as the Internet of Production (IoP) require rising smartness of products and services. Smart physical products are often mechatronic products that include increasing amounts of software. The development of software, however, comes along with new challenges for companies specialized in developing mechanical, electrical or electronic products. Some of these challenges address the product lifecycle management (PLM)-related business and work processes. The management of software lifecycles requires a much more rigorous requirements management. Furthermore, special solutions for management of source code in distributed development teams are needed. The build-process and testing activities need to be conducted in a systematic manner. The generation and provision of different licensing models need to be mastered and finally the issue of security needs to be addressed for any product that can be networked-which by the way is a strategic target of nearly any product developing company. Application Lifecycle Management (ALM) covers many of the above-mentioned issues. IT solutions for ALM are comparable to traditional PLM solutions, but focus particularly on software as a product. Thus, these systems have become widely used by software companies in the same manner as PLM solutions belong to the standard enterprise IT environment of companies developing physical products. With software penetrating traditional physical products, product managers, product developers, manufacturing staff etc. need to work with both, PLM and ALM, since neither solution is able to cover both domains sufficiently. However, ALM and PLM solutions feature redundant functionality. Thus, best practices for the systematic integration of ALM and PLM are required."
Putrescine Improves the Resistance of Peach Fruit to Cold Stress via Elevating the Polyamines Conjugated to Plasma Membranes and Maintaining Membrane Conformation,"Liu, HP; Liu, DX",10.1134/S1021443723601726,2023,"Exogenous polyamines can enhance postharvest fruit cold resistance and alleviate cold injury. However, the mechanism underlying polyamine-mediated cold resistance remains to be eluciated. Therefore, in this research, polyamines conjugated to plasma membrane (PLM) and membrane conformation were detected in the putrescine pretreated peach fruits under cold condition. The results showed that cold stress resulted in the changes in the PLM conformation and cause the cold injury as judged by the increased plasma membrane permeability and the decreased sulphydryl content. Pretreatment with putrescine mitigated the cold injury of the fruits and resulted in the stabilization of PLM, coupled with the increased contents of non-covalently (NC)-conjugated spermidine (Spd) and spermine (Spm), and covalently-conjugated (C) putrescine (Put) and Spd in PLM. From the results, it could be inferred that polyamines conjugated to PLM and membrane conformation stabilization might be involved in the putrescine-mediated resistance of the harvested peach fruits to cold stress. This inference was confirmed by the complementary experiments with methylglyoxal-bis(guanylhydrazone) (MGBG) and phenanthroline, which inhibited S-adenosylmethionine decarboxylase and transglutaminase activities, respectively. MGBG restrained the putrescine-induced increases in NC-conjugated Spd and Spm and phenanthroline declined the increases in C-conjugated putrescine in PLM induced by exogenous putrescine. Furthermore, both MGBG and phenanthroline reversed the mitigation effects of putrescine on fruit cold injury, in parallel with the decrease of membrane conformation stabilization. Collectively, we could conclude that putrescine-mediated fruit cold resistance involved in the increases in the polyamines conjugated to PLM and the maintenance of membrane conformation. The relationship between the conjugated polyamines and membrane conformation stabilization was discussed in the research."
Developing the Requirements of a PLM/ALM Integration: An Industrial Case Study,"Deuter, A; Otte, A; Ebert, M; Possel-DÃ¶lken, F",10.1016/j.promfg.2018.06.020,2018,"The digitization of the industry requires smart products and services. Smart products are mechatronic products with an increasing amount of software. To get high quality smart products to the market quickly, manufacturers need to reshape their product lifecycle processes. They need to apply system engineering-based methods to enable smooth cross-domain developments with a special focus on the software domain. One significant challenge faced by manufacturers is the harmonization of product lifecycle management (PLM), which addresses the hardware lifecycle, with application lifecycle management (ALM), which addresses the software lifecycle. To support manufacturers in this challenging activity, this paper demonstrates a proven process for developing use cases and requirements associated with a PLM/ALM integration. This process has been elicited during an industrial case study in a manufacturing company. This paper explains this process in detail. A generally applicable approach for developing the requirements of a PLM/ALM integration is extracted by removing the company-specific factors. (C) 2018 The Authors. Published by Elsevier B.V."
Shifting the Biosynthesis of Leukotrienes Toward Specialized Pro-Resolving Mediators by the 5-Lipoxygenase-Activating Protein (FLAP) Antagonist BRP-201,"Kretzer, C; Jordan, PM; Bilancia, R; Rossi, A; Maz, TG; Banoglu, E; Schubert, US; Werz, O",10.2147/JIR.S345510,2022,"Background and Purpose: Lipid mediators (LM) play crucial roles in the complex inflammation process with respect to initiation, maintenance, and resolution. Proinflammatory leukotrienes (LTs), generated by 5-lipoxygenase (LOX) and the 5-LOX-activating protein (FLAP), initiate and maintain inflammation while specialized pro-resolving mediators (SPMs) formed by various LOXs as key enzymes promote inflammation resolution and the return to homeostasis. Since 5-LOX also contributes to SPM biosynthesis, smart pharmacological manipulation of the 5-LOX pathway and accompanied activation of 12-/15-LOXs may accomplish suppression of LT formation but maintain or even elevate SPM formation. Here, we demonstrated that the FLAP antagonist BRP-201 possesses such pharmacological profile and causes a switch from LT toward SPM formation. Methods and Results: Comprehensive LM metabololipidomics with activated human monocyte-derived macrophages (MDM) of M1 or M2 phenotype showed that BRP-201 strongly inhibits LT formation induced by bacterial exotoxins. In parallel, SPM levels and 12/15-LOX-derived products were markedly elevated, in particular in M2-MDM. Intriguingly, in unstimulated MDM, BRP-201 induced formation of 12/15-LOX products including SPM and caused 15-LOX-1 subcellular redistribution without affecting 5-LOX. Experiments with HEK293 cells stably expressing either 5-LOX with or without FLAP, 15-LOX-1 or 15-LOX-2 confirmed suppression of 5-LOX product formation due to FLAP antagonism by BRP-201 but activated 15-LOX-1 in the absence of FLAP. Finally, in zymosan-induced murine peritonitis, BRP-201 (2 mg/kg, ip) lowered LT levels but elevated 12/15-LOX products including SPMs. Conclusion: BRP-201 acts as FLAP antagonist but also as 12/15-LOX activator switching formation of pro-inflammatory LTs toward inflammation-resolving SPM, which reflects a beneficial pharmacological profile for intervention in inflammation."
Deep-Learning-Driven Techniques for Real-Time Multimodal Health and Physical Data Synthesis,"Haleem, MS; Ekuban, A; Antonini, A; Pagliara, S; Pecchia, L; Allocca, C",10.3390/electronics12091989,2023,"With the advent of Artificial Intelligence for healthcare, data synthesis methods present crucial benefits in facilitating the fast development of AI models while protecting data subjects and bypassing the need to engage with the complexity of data sharing and processing agreements. Existing technologies focus on synthesising real-time physiological and physical records based on regular time intervals. Real health data are, however, characterised by irregularities and multimodal variables that are still hard to reproduce, preserving the correlation across time and different dimensions. This paper presents two novel techniques for synthetic data generation of real-time multimodal electronic health and physical records, (a) the Temporally Correlated Multimodal Generative Adversarial Network and (b) the Document Sequence Generator. The paper illustrates the need and use of these techniques through a real use case, the H2020 GATEKEEPER project of AI for healthcare. Furthermore, the paper presents the evaluation for both individual cases and a discussion about the comparability between techniques and their potential applications of synthetic data at the different stages of the software development life-cycle."
Rule-Based Architectural Design Pattern Recognition with GPT Models,"JÃ¡nki, ZR; Bilicki, V",10.3390/electronics12153364,2023,"Architectural design patterns are essential in software development because they offer proven solutions to large-scale structural problems in software systems and enable developers to create software that is more maintainable, scalable, and comprehensible. Model-View-Whatever (MVW) design patterns are prevalent in many areas of software development, but their use in Web development is on the rise. There are numerous subtypes of MVW design patterns applicable to Web systems, but there is no exhaustive listing of them. Additionally, it is unclear how these subtypes can be utilized in contemporary Web development, as their usage is typically unconscious. Here, we discuss and define the most prevalent MVW design patterns used in Web development, as well as provide Angular framework examples and guidance on when to employ a particular design pattern. On the premise of the primary characteristics of design patterns, we created a rule system that large language models (LLMs) can comprehend without doubt. Here, we demonstrate how effectively Generative Pre-trained Transformer (GPT) models can identify various design patterns based on our principles and verify the quality of our recommendations. Together, our solution and GPT models constitute an effective natural language processing (NLP) solution capable of detecting MVW design patterns in Angular projects with an average accuracy of 90%."
The Impact of the Web Data Access Object (WebDAO) Design Pattern on Productivity,"Janki, ZR; Bilicki, V",10.3390/computers12080149,2023,"In contemporary software development, it is crucial to adhere to design patterns because well-organized and readily maintainable source code facilitates bug fixes and the development of new features. A carefully selected set of design patterns can have a significant impact on the productivity of software development. Data Access Object (DAO) is a frequently used design pattern that provides an abstraction layer between the application and the database and is present in the back-end. As serverless development arises, more and more applications are using the DAO design pattern, but it has been moved to the front-end. We refer to this pattern as WebDAO. It is evident that the DAO pattern improves development productivity, but it has never been demonstrated for WebDAO. Here, we evaluated the open source Angular projects to determine whether they use WebDAO. For automatic evaluation, we trained a Natural Language Processing (NLP) model that can recognize the WebDAO design pattern with 92% accuracy. On the basis of the results, we analyzed the entire history of the projects and presented how the WebDAO design pattern impacts productivity, taking into account the number of commits, changes, and issues."
Targeting biosynthetic networks of the proinflammatory and proresolving lipid metabolome,"Werner, M; Jordan, PM; Romp, E; Czapka, A; Rao, ZG; Kretzer, C; Koeberle, A; Garscha, U; Pace, S; Claesson, HE; Serhan, CN; Werz, O; Gerstmeier, J",10.1096/fj.201802509R,2019,"Nonsteroidal anti-inflammatory drugs interfere with the metabolism of arachidonic acid to proinflammatory prostaglandins and leukotrienes by targeting cyclooxygenases (COXs), 5-lipoxygenase (LOX), or the 5-LOX-activating protein (FLAP). These and related enzymes act in conjunction with marked crosstalk within a complex lipid mediator (LM) network where also specialized proresolving LMs (SPMs) are formed. Here, we present how prominent LM pathways can be differentially modulated in human proinflammatory M1 and proresolving M2 macrophage phenotypes that, upon exposure to Escherichia coli, produce either abundant prostaglandins and leukotrienes (M1) or SPMs (M2). Targeted liquid chromatography-tandem mass spectrometry-based metabololipidomics was applied to analyze and quantify the specific LM profiles. Besides expected on-target actions, we found that: 1) COX or 15-LOX-1 inhibitors elevate inflammatory leukotriene levels, 2) FLAP and 5-LOX inhibitors reduce leukotrienes in M1 but less so in M2 macrophages, 3) zileuton blocks resolution-initiating SPM biosynthesis, whereas FLAP inhibition increases SPM levels, and 4) that the 15-LOX-1 inhibitor 3887 suppresses SPM formation in M2 macrophages. Conclusively, interference with discrete LM biosynthetic enzymes in different macrophage phenotypes considerably affects the LM metabolomes with potential consequences for inflammation-resolution pharmacotherapy. Our data may allow better appraisal of the therapeutic potential of these drugs to intervene with inflammatory disorders.Werner, M., Jordan, P. M., Romp, E., Czapka, A., Rao, Z., Kretzer, C., Koeberle, A., Garscha, U., Pace, S., Claesson, H.-E., Serhan, C. N., Werz, O., Gerstmeier, J. Targeting biosynthetic networks of the proinflammatory and proresolving lipid metabolome."
Transformable Spinose Nanodrums with Self-Supplied H2O2 for Photothermal and Cascade Catalytic Therapy of Tumor,"Liu, MD; Guo, DK; Zeng, RY; Guo, WH; Ding, XL; Li, CX; Chen, Y; Sun, YX; Zhang, XZ",10.1002/smtd.202100361,2021,"Advances in enzymes involve an efficient biocatalytic process, which has demonstrated great potential in biomedical applications. However, designing a functional carrier for enzymes equipped with satisfactory degradability and loading efficiency, remains a challenge. Here, based on transformable liquid metal (LM), a spinose nanodrum is designed as protein carrier to deliver enzyme for tumor treatment. With the assistance of spines and a special drum-like shape, it is found that the spiny LM can carry much more enzymes than spherical LM under the same condition. Benefiting from the satisfactory enzyme loading efficiency of spiny LM, a plasma amine oxidase immobilized spinose LM nanosystem enveloped with epigallocatechin gallate (EGCG)-Fe3+ (LMPE) is fabricated for photothermal and cascade catalytic tumor therapy. Activated by the acidic condition in the tumor microenvironment, the LMPE can oxidize spermine (Spm) and spermidine (Spd) to generate hydrogen peroxide (H2O2) for Fenton catalytic reaction to produce the lethal hydroxyl radical (center dot OH) for tumor cell killing. Combined with remarkable photothermal performance of LM, LMPE exhibits significant inhibition of tumor in vivo."
Anti-inflammatory celastrol promotes a switch from leukotriene biosynthesis to formation of specialized pro-resolving lipid mediators,"Pace, S; Zhang, KH; Jordan, PM; Bilancia, R; Wang, WF; BÃ¶rner, F; Hofstetter, RK; Potenza, M; Kretzer, C; Gerstmeier, J; Fischer, D; Lorkowski, S; Gilbert, NC; Newcomer, ME; Rossi, A; Chen, XC; Werz, O",10.1016/j.phrs.2021.105556,2021,"The pentacyclic triterpenoid quinone methide celastrol (CS) from Tripterygium wilfordii Hook. F. effectively ameliorates inflammation with potential as therapeutics for inflammatory diseases. However, the molecular mechanisms underlying the anti-inflammatory and inflammation-resolving features of CS are incompletely understood. Here we demonstrate that CS potently inhibits the activity of human 5-lipoxygenase (5-LOX), the key enzyme in pro-inflammatory leukotriene (LT) formation, in cell-free assays with IC50 = 0.19-0.49 mu M. Employing metabololipidomics using ultra-performance liquid chromatography coupled to tandem mass spectrometry in activated human polymorphonuclear leukocytes or M1 macrophages we found that CS (1 mu M) potently suppresses 5-LOX-derived products without impairing the formation of lipid mediators (LM) formed by 12-/15-LOXs as well as fatty acid substrate release. Intriguingly, CS induced the generation of 12-/15-LOX-derived LM including the specialized pro-resolving mediator (SPM) resolvin D5 in human M2 macrophages. Finally, intraperitoneal pre-treatment of mice with 10 mg/kg CS strongly impaired zymosan-induced LT formation and simultaneously elevated the levels of SPM and related 12-/15-LOX-derived LM in peritoneal exudates, spleen and plasma in vivo. Conclusively, CS promotes a switch from LT biosynthesis to formation of SPM which may underlie the anti-inflammatory and inflammation-resolving effects of CS, representing an interesting pharmacological strategy for intervention with inflammatory disorders."
Development of an application for providing corneal topography reports based on artificial intelligence,"Lucena, AR; de AraÃºjo, MO; Carneiro, RFL; Cavalcante, TD; Ribeiro, ABN; Anselmo, FJM",10.5935/0004-2749.2022-0051,2022,"Purpose: To develop an application (TopEye) in the iOS platform for mobile devices to allow the capture and interpretation of color maps generated by corneal topographers using artificial intelligence. Methods: In the execution, follow- up, and assessment of the project, we used the Scrum methodology and interactive and incremental development process for the project management and agile software development. The ge-nerated diagnostic pattern bank consists of 1,172 examples of corneal topography, divided into 275 spherical, 302 symmetrical, 295 asymmetrical, and 300 irregular patterns (keratoconus). For the development of the artificial intelligence of the application, network training was established with 240 images of each pattern type, with a total of 960 patterns (81.91%). The remaining 212 images (18.09%) were used to test the application and will be used for the results. The process is semi-automatic, so the topographic image is captured with a smartphone, the examiner performs the contour of the corneal relief manually, and then the neural network performs the diagnosis. Results: The application diagnosed 201 cases (94.81%) correctly. In 212 images, the algorithm missed the classification of 11 cases (5.19%). The major error that occurred was in distinguishing between symmetrical and asymmetrical classes. In keratoconus screening, the application reached 95.00% sensitivity and 98.68% specificity. Conclusion: The work resulted in obtaining an efficient application to capture topographic images using a smartphone camera and their interpretations through applied artificial intelligence."
Process Improvement in Software Requirements Engineering: A Systematic Mapping Study,"Almeyda, S; DÃ¡vila, A",10.1134/S0361768822080084,2022,"Software analysis is the process carried out to obtain requirements that reflects the needs of a client's stakeholders and that allows the construction of a software product that meets their expectations. However, it is also known as a process where many defects are injected. In this context, although process improvement has contributed to the software industry, in the case of software requirements it needs to be studied to determine the improvements obtained and established models. In the literature reviewed, a similar mapping study with 4 research question was identified and used as a reference. The objective of this work is to structure the available literature on process improvement in the software requirements engineering (SRE) domain to identify the improvement phases, paradigms, principles, and established models. For this purpose, a systematic mapping study (SMS) was carried out in the most recognized digital databases. The mapping carried out recovered a total of 1,495 studies, and after the process, 86 primary studies were obtained. In this SMS had established and answered 13 research questions. The different models that are applied throughout the software requirements engineering process were identified, and accepted studies were classified and findings on SRE process improvement were collected. The most used models are CMMI, Requirements Engineering Good Practice Guide (REGPG), and ISO/IEC 15504. Also, 62% of accepted studies are of the proposal and evaluation type; that is, they propose a framework and study the implementation of a proposal in one or more case studies respectively. On the other hand, it was found that most of the studies focused on the process improvement analysis phase. Likewise, in contrast with a previous study, proposal and validation type of studies increased in 9 papers each one from 2014 to date. This shows the interest of the scientific community in this domain."
Beneficial Modulation of Lipid Mediator Biosynthesis in Innate Immune Cells by Antirheumatic Tripterygium wilfordii Glycosides,"Zhang, KH; Pace, S; Jordan, PM; Peltner, LK; Weber, A; Fischer, D; Hofstetter, RK; Chen, XC; Werz, O",10.3390/biom11050746,2021,"Tripterygium wilfordii glycosides (TWG) is a traditional Chinese medicine with effectiveness against rheumatoid arthritis (RA), supported by numerous clinical trials. Lipid mediators (LM) are biomolecules produced from polyunsaturated fatty acids mainly by cyclooxygenases (COX) and lipoxygenases (LOX) in complex networks which regulate inflammation and immune responses and are strongly linked to RA. The mechanism by which TWG affects LM networks in RA treatment remains elusive. Employing LM metabololipidomics using ultra-performance liquid chromatography-tandem mass spectrometry revealed striking modulation of LM pathways by TWG in human monocyte-derived macrophage (MDM) phenotypes. In inflammatory M1-MDM, TWG (30 mu g/mL) potently suppressed agonist-induced formation of 5-LOX products which was confirmed in human PMNL and traced back to direct inhibition of 5-LOX (IC50 = 2.9 mu g/mL). TWG also efficiently blocked thromboxane formation in M1-MDM without inhibiting other prostanoids and COX enzymes. Importantly, in anti-inflammatory M2-MDM, TWG (30 mu g/mL) induced pronounced formation of specialized pro-resolving mediators (SPM) and related 12/15-LOX-derived SPM precursors, without COX and 5-LOX activation. During MDM polarization, TWG (1 mu g/mL) decreased the capacity to generate pro-inflammatory 5-LOX and COX products, cytokines and markers for M1 phenotypes. Together, suppression of pro-inflammatory LM but SPM induction may contribute to the antirheumatic properties of TWG."
Pretecto- and ponto-cerebellar pathways to the pigeon oculomotor cerebellum follow a zonal organization,"GutiÃ©rrez-IbÃ¡Ã±ez, C; Pilon, MC; Wylie, DR",10.1002/cne.25247,2022,"Both birds and mammals have relatively large forebrains and cerebella. In mammals, there are extensive sensory-motor projections to the cerebellum through the pontine nuclei originating from several parts of the cerebral cortex. Similar forebrain-to-cerebellum pathways exist in birds, but the organization of this circuitry has not been studied extensively. Birds have two nuclei at the base of the brainstem that are thought to be homologous to the pontine nuclei of mammals, the medial and lateral pontine nuclei (PM, PL). Additionally, birds are unique in that they have a pretectal nucleus called the medial spiriform nucleus (SpM) that, like the pontine nuclei, also receives projections from the forebrain and projects to the oculomotor cerebellum (OCb; folia VI to VIII). The OCb also receives input from the pretectal nucleus lentiformis mesencephali (LM), which analyzes visual optic flow information resulting from self-movement. In this study, we used single or double injections of fluorescent tracers to study the organization of these inputs from PM, PL, SpM and LM to the OCb in pigeons. We found that these inputs follow a zonal organization. The most medial zone in the OCb, zone A1, receives bilateral inputs from the lateral SpM, PL and LM. Zones A2 and C receive a bilateral projection from the medial SpM, and a mostly contralateral projection from PM and LM. We discuss how the pathway to zone A1 processes mainly visuo-motor information to spinal premotor areas, whereas the pathways to zone A2/C processes somato-motor and visuo-motor information and may have a feedback/modulatory role."
"Ethical Tools, Methods and Principles in Software Engineering and Development: Case Ethical User Stories","Halme, E",10.1007/978-3-031-21388-5_48,2022,"The great leap with the development of Artificial Intelligence (AI) and Machine Learning (ML) technology has increased the range of different requirements for software quality, especially in terms of ethics. To implement high-level requirements, like ethical principles, into the workflow of software engineering, new requirements engineer tools are to be developed. Ethical User Stories (EUS) offers a simple way of implementing ethics in software development. This research has investigated the idea of using familiar requirements engineering artifacts, User Stories, to implement ethical principles, into the workflow of software engineering and operationalizing the studied phenomena of EUS. The preliminary results, found through two ongoing empirical studies with a data collection of 600+ EUS, show that EUS is a pressure-free, human-centric and accessible approach to Ethically Aligned Design (EAD) that intertwines with quality characteristics and relieves the developer from the heavy burden of ethical consideration to a smooth workflow of software engineering. An effective EUS is consistent throughout the user story and shares the idea that user-driven ethical motivation generates system functionality or benefits non-functional software design for quality assurance."
Novel benzoxanthene lignans that favorably modulate lipid mediator biosynthesis: A promising pharmacological strategy for anti-inflammatory therapy,"Gerstmeier, J; Kretzer, C; Di Micco, S; Miek, L; Butschek, H; Cantone, V; Bilancia, R; Rizza, R; Troisi, F; Cardullo, N; Tringali, C; Ialenti, A; Rossi, A; Bifulco, G; Werz, O; Pace, S",10.1016/j.bcp.2019.03.003,2019,"Lipid mediators (LM) encompass pro-inflammatory prostaglandins (PG) and leukotrienes (LT) but also specialized pro-resolving mediators (SPM) which display pivotal bioactivities in health and disease. Pharmacological intervention with inflammatory disorders such as osteoarthritis and rheumatoid arthritis commonly employs anti-inflammatory drugs that can suppress PG and LT formation, which however, possess limited effectiveness and side effects. Here, we report on the discovery and characterization of the two novel benzoxanthene lignans 1 and 2 that modulate select LM biosynthetic enzymes enabling the switch from pro-inflammatory LT to SPM biosynthesis as potential pharmacological strategy to intervene with inflammation. In cell-free assays, compound 1 and 2 inhibit microsomal prostaglandin E-2 synthase-1 and leukotriene C-4 synthase (IC50 similar to 0.6-3.4 mu M) and potently interfere with 5-lipoxygenase (5-LOX), the key enzyme in LT biosynthesis (IC50 = 0.04 and 0.09 mu M). In human neutrophils, monocytes and M1 and M2 macrophages, compound 1 and 2 efficiently suppress LT biosynthesis (IC50 < 1 mu M), accompanied by elevation of 15-LOX-derived LM including SPM. In zymosan-induced murine peritonitis, compound 1 and 2 ameliorated self-limited inflammation along with suppression of early LT formation and elevation of subsequent SPM biosynthesis in vivo. Together, these novel benzoxanthene lignans promote the LM class switch from pro-inflammatory towards pro-resolving LM to terminate inflammation, suggesting their suitability as novel leads for pharmacotherapy of arthritis and related inflammatory disorders."
How to Write Ethical User Stories? Impacts of the ECCOLA Method,"Halme, E; Vakkuri, V; Kultanen, J; Jantunen, M; Kemell, KK; Rousi, R; Abrahamsson, P",10.1007/978-3-030-78098-2_3,2021,"Artificial Intelligence (AI) systems are increasing in significance within software services. Unfortunately, these systems are not flawless. Their faults, failures and other systemic issues have emphasized the urgency for consideration of ethical standards and practices in AI engineering. Despite the growing number of studies in AI ethics, comparatively little attention has been placed on how ethical issues can be mitigated in software engineering (SE) practice. Currently understanding is lacking regarding the provision of useful tools that can help companies transform high-level ethical guidelines for AI ethics into the actual workflow of developers. In this paper, we explore the idea of using user stories to transform abstract ethical requirements into tangible outcomes in Agile software development. We tested this idea by studying master's level student projects (15 teams) developing web applications for a real industrial client over the course of five iterations. These projects resulted in 250+ user stories that were analyzed for the purposes of this paper. The teams were divided into two groups: half of the teams worked using the ECCOLA method for AI ethics in SE, while the other half, a control group, was used to compare the effectiveness of ECCOLA. Both teams were tasked with writing user stories to formulate customer needs into system requirements. Based on the data, we discuss the effectiveness of ECCOLA, and Primary Empirical Contributions (PECs) from formulating ethical user stories in Agile development."
Estimates of suspended solid transport in the Para River Estuary,"Carneiro, AG; Prestes, YO; Rollnic, M",10.1590/S2675-28242020068281,2020,"The aim of the present study was to quantify the suspended solid concentrations (C-SSL) and suspended solid transport (T-SSL) in the mixing zone of the Para River Estuary. This estuary is located at the southeastern extreme of the estuarine complex of the Amazon River, and receives input from Tocantins and Para rivers and other local tributaries, with a total discharge of approximately 10(4) m(3) s(-1) Two field campaigns were conducted to collect samples of Suspended Particulate Matter (SPM) and measure turbidity. Bottom and surface samples were collected hourly throughout a semidiurnal tidal cycle at two points in the estuary (Left Margin [LM]: -0.7287 degrees -48.2408 degrees and Right Margin [RM]: -0.6051 degrees -48.4048 degrees) in both the dry and rainy seasons. The TSSL was determined using a linear correlation between SPM and turbidity. The results indicate that both the RM and LM points were export routes of suspended solids to the adjacent ocean. However, both lateral and seasonal differences were observed in the PRE samples, with the RM point exporting more suspended solids to the adjacent ocean than the LM point.TheTSSL was higher in near-bottom layers on the RM, and the lutocline was broader in comparison with the LM. The correlation between the CSSL at the bottom and in the water column was higher on the LM, and indicated that approximately 80% of the CSSL on this margin derives from resuspension.The sum of the evidence indicates that both points export suspended solids to the ocean."
Governance in Ethical and Trustworthy AI Systems: Extension of the ECCOLA Method for AI Ethics Governance Using GARP,"Agbese, M; Alanen, HK; Antikainen, J; Erika, H; Isomaki, H; Jantunen, M; Kemell, KK; Rousi, R; Vainio-Pekka, H; Vakkuri, V",10.37190/e-Inf230101,2023,"Background: The continuous development of artificial intelligence (AI) and increasing rate of adoption by software startups calls for governance measures to be implemented at the design and development stages to help mitigate AI governance concerns. Most AI ethical design and development tools mainly rely on AI ethics principles as the primary governance and regulatory instrument for developing ethical AI that inform AI governance. However, AI ethics principles have been identified as insufficient for AI governance due to lack of information robustness, requiring the need for additional governance measures. Adaptive governance has been proposed to combine established governance practices with AI ethics principles for improved information and subsequent AI governance. Our study explores adaptive governance as a means to improve information robustness of AI ethical design and development tools. We combine information governance practices with AI ethics principles using ECCOLA, a tool for ethical AI software development at the early developmental stages.Aim: How can ECCOLA improve its robustness by adapting it with GARP (R) IG practices?Methods: We use ECCOLA as a case study and critically analyze its AI ethics principles with information governance practices of the Generally Accepted Recordkeeping principles (GARP (R)).Results: We found that ECCOLA's robustness can be improved by adapting it with Information governance practices of retention and disposal.Conclusions: We propose an extension of ECCOLA by a new governance theme and card, # 21."
AI-Powered Chatbots and the Transformation of Work: Findings from a Case Study in Software Development and Software Engineering,"SÃ¼sse, T; Kobert, M; Grapenthin, S; Voigt, BF",10.1007/978-3-031-42622-3_49,2023,"The recent technological enhancements in the field of large language models and their integration into collaborative processes, for example, as chatbots, are perceived as key drivers for further transformations of work. However, the transformative effects of these technological enhancements have to be more thoroughly investigated in specific work contexts to benefit from the great potential of improvement. This research article provides findings of a case study research on how employees in software engineering perceive the collaboration with AI-powered chatbots, such as chatGPT. We investigate patterns employees develop to cope with the novel demands arising during the collaboration with these technologies and discuss our empirical findings regarding a conceptual framework of AI-related competences and another case study from a different industry. The findings contribute to a better understanding of human actors' AI-related coping patterns as key prerequisites for a more responsible and sustainable usage of this technology in professional work contexts."
Automotive SPICE Draft PAM V4.0 in Action: BETA Assessment,"Moselhy, N; Adel, A; Seddik, A",10.1007/978-3-031-42310-9_7,2023,"After the revolution of new constraints like Cybersecurity in modern industries and high-tech fields, and the innovation of Artificial Intelligence and machine learning in the fields of Software Development in general, or the vast application of ChatGPT [1] in the automotive industry in specific, there was a urge towards the simplification of process models to cope with the change in projects nature and serve all purposes. Accordingly, VDA-QMC [2] has released a new simplified draft version of the Automotive SPICE PAM (version 4.0) [3] that encompasses many of these ideas, which is currently under review. In this paper, we take the opportunity to demonstrate the results of a pilot assessment of this new version on a few mockup project samples, focusing on areas for improvements in hopes to enhance the final version expected June, 2023 into a more practical approach. The paper also urges theVDAto officially consider the results of this case study into the expected new version release of Automotive SPICE to ensure a more reliable and complete version."
BIMBOT-(ARTIFICIAL INTELLIGENCE APPLIED TO BIM DESIGN),"FrÃ­as, C; PeÃ±a, JM; SÃ¡nchez, E; Almeida, L",10.4995/ege.2020.13942,2020,"BIMBOT is an intelligent design assistant for AEC industry. Its toolset runs on a BIM modelling software and produces a series of design solutions through optimised BIM models. It works with the use of advanced artificial intelligence (AI) methods (soft computing optimisation and machine learning) and supported by NoSQL databases. BIMBOT works in several stages: First, the definition of constraints/priorities established by the user runs a generative design process boosted by several AI methods. It creates different solutions on BIM models stored and refined from a catalogue of intelligent objects. So, an interactive process begins in which the users may import BIM models with proposed designs, create or edit them on-the-fly and get assisted by a series of configurable metrics that drive the quality of the design according to the initial preferences. So, we get a complete BIM project as a result of the iterative process. Finally, the continuous training of the algorithms will improve the efficiency in future designs. BIMBOT is conceived to extend the skills designers through software development BIM allowing them to be more productive in complex tasks in their design process. BIMBOT is funded by the European Eureka/Eurostars program (E!12863)."
An Intelligent Journey to Machine Learning Applications in Component-Based Software Engineering,"Wangoo, DP",10.1007/978-981-15-0222-4_16,2020,"The automation of software development process is a leading edge for the Software 2.0 trend. Machine learning with software engineering has been used in a variety of domains and in all the phases of the software development life cycle process. The journey of machine learning in software engineering lays down the time lines and milestones to be achieved in the intelligent automation process of software development. From designing to testing and security, machine learning has automated almost all the phases of software development life cycle with supervised and unsupervised learning and the future holds the panoramic view of automated intelligence where the machine learns by itself without any explicit programming. The goal of the paper is to provide useful insight into the significant arena of software intelligence andlay down the potential ground for various research analysis in the software engineering processes."
Effort Estimation in Agile Software Development: An Updated Review,"Dantas, E; Perkusich, M; Dilorenzo, E; Santos, DFS; Almeida, H; Perkusich, A",10.1142/S0218194018400302,2018,"One of the main issues of an agile software project is how to accurately estimate development effort. In 2014, a Systematic Literature Review (SLR) regarding this subject was published. The authors concluded that there were several gaps in the literature, such as the low level of accuracy of the techniques and little consensus on appropriate cost drivers. The goal of our work is to provide an updated review of the state of the art based on this reference SLR work. We applied a Forward Snowballing approach, in which our seed set included the former SLR and its selected papers. We identified a strong indication of solutions based on Artificial Intelligence and Machine Learning methods for effort estimation in Agile Software Development (ASD). We also identified that there is a gap in terms of agreement on suitable cost drivers. Thus, we applied Thematic Analysis in the selected papers and identified a representative set of 10 cost drivers for effort estimation. This updated review of the state of the art resulted in 24 new relevant papers selected."
Applying virtual reality to teach the software development process to novice software engineers,"Gulec, U; Yilmaz, M; Isler, V; Clarke, PM",10.1049/sfw2.12047,2021,"Software development is a complicated process that requires experienced human resources to produce successful software products. Although this process needs experience from the individuals, it is hard to provide this experience without encountering real incidents during the software development process. To fill this gap, this study proposes a Virtual Reality Based Software Development Framework (VR-SODEF), which provides an interactive virtual reality experience for individuals learning about the tasks of software development starting from requirement analysis through software testing. In the VR-SODEF, the participant takes on the role of a novice software developer being recruited into a virtual software development organisation who should work alongside five virtual characters, played by artificial intelligence. This exclusive viewpoint draws participants from the 2D separation of the classical experience and virtually into the world of the software development itself. Participants experience the intense dramatic elements created for simulation and confront the challenges of virtual software practitioners in a somewhat uncompromising virtual simulation environment. To examine the efficiency of the VR-SODEF, it was tested on 32 computing students, with results indicating that virtual reality can be an effective educational medium, especially for skills that might traditionally be acquired through experience rather than traditional classroom-based teaching."
Ethical Perspectives in AI: A Two-folded Exploratory Study From Literature and Active Development Projects,"de Cerqueira, JAS; Althoff, LD; de Almeida, PS; Canedo, ED",,2021,"Background: Interest in Artificial Intelligence (AI) based systems has been gaining traction at a fast pace, both for software development teams and for society as a whole. This increased interest has lead to the employment of AI techniques such as Machine Learning and Deep Learning for diverse purposes, like medicine and surveillance systems, and such uses have raised the awareness about the ethical implications of the usage of AI systems. Aims: With this work we aim to obtain an overview of the current state of the literature and software projects on tools, methods and techniques used in practical AI ethics. Method: We have conducted an exploratory study in both a scientific database and a software projects repository in order to understand their current state on techniques, methods and tools used for implementing AI ethics. Results: A total of 182 abstracts were retrieved and five classes were devised from the analysis in Scopus, 1) AI in Agile and Business for Requirement Engineering (RE) (22.8%), 2) RE in Theoretical Context (14.8%), 3) Quality Requirements (22.6%), 4) Proceedings and Conferences (22%), 5) AI in Requirements Engineering (17.8%). Furthermore, out of 589 projects from GitHub, we found 21 tools for implementing AI ethics. Highlighted publicly available tools found to assist the implementation of AI ethics are InterpretML, Deon and TransparentAI. Conclusions: The combined energy of both explored sources fosters an enhanced debate and stimulates progress towards AI ethics in practice."
A Deployment Model to Extend Ethically Aligned AI Implementation Method ECCOLA,"Antikainen, J; Agbese, M; Alanen, HK; Halme, E; IsomÃ¤ki, H; Jantunen, M; Kemell, KK; Rousi, R; Vainio-Pekka, H; Vakkuri, V",10.1109/REW53955.2021.00043,2021,"There is a struggle in Artificial intelligence (AI) ethics to gain ground in actionable methods and models to be utilized by practitioners while developing and implementing ethically sound AI systems. AI ethics is a vague concept without a consensus of definition or theoretical grounding and bearing little connection to practice. Practice involving primarily technical tasks like software development is not aptly equipped to process and decide upon ethical considerations. Efforts to create tools and guidelines to help people working with AI development have been concentrating almost solely on the technical aspects of AI. A few exceptions do apply, such as the ECCOIA method for creating ethically aligned AI -systems. ECCOIA has proven results in terms of increased ethical considerations in AI systems development. Yet, it is a novel innovation, and room for development still exists. This study aims to extend ECCOIA with a deployment model to drive the adoption of ECCOIA, as any method - no matter how good -is of no value without adoption and use. The model includes simple metrics to facilitate the communication of ethical gaps or outcomes of ethical AI development. It offers the opportunity to assess any AI system at any given life-cycle phase, e.g., opening possibilities like analyzing the ethicality of an AI system under acquisition"
Digital Transformation of Software Development: Implications for the Future of Work,"Laato, S; MÃ¤ntymÃ¤ki, M; Birkstedt, T; Islam, AKMN; Hyrynsalmi, S",10.1007/978-3-030-85447-8_50,2021,"In this work we explore digital transformation in software development. A set of interviews were conducted among industry experts to identify and elucidate the drivers and trajectories of digital transformation within the software industry. Using the Gioia method for qualitative analysis and synthesis, two major trajectories were found: (1) automation increasingly impacts several key activities related to software development; and (2) the importance of software and digital products is increasing in sectors where the core product or service has not traditionally been software-intensive. The findings have implications for the future of work in the context of software business. First, software developers and operators are increasingly needed, and more heavily involved across industry sectors. Second, as the level of automation becomes higher, the roles of automated testing and governance are highlighted, meaning a significant portion of development time will be spent in creating and validating automated tests. Third and finally, the importance of digital skills will increase also in non-IT roles as digital elements infuse into traditionally physical goods and services."
Automatic Component Prediction for Issue Reports Using Fine-Tuned Pretrained Language Models,"Wang, DS; Lee, CG",10.1109/ACCESS.2022.3229426,2022,"Various issues or bugs are reported during the software development. It takes considerable effort, time, and cost for the software developers to triage these issues manually. Many previous studies have proposed various method to automate the triage process by predicting component using word-based language models. However, these methods still suffer from unsatisfactory performance due to their structural limitations and ignorance of the word context. In this paper, we propose a novel technique based on pretrained language models and it aims to predict a component of an issue report. Our approach fine-tunes the pretrained language models to conduct multilabel classifications. The proposed approach outperforms the previous state-of-the-art method by more than 30% with respect to the recall at ${k}$ on all the datasets considered in our experiment. This improvement suggests that fine-tuned pretrained language models can help us to predict issue components effectively."
InferFix: End-to-End Program Repair with LLMs,"Jin, M; Shahriar, S; Tufano, M; Shi, X; Lu, S; Sundaresan, N; Svyatkovskiy, A",10.1145/3611643.3613892,2023,"Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose InferFix: a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs. InferFix combines a Retriever - transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator - an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated InferredBugs, a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that InferFix outperforms strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java. We discuss the deployment of InferFix alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow."
Towards Greener Yet Powerful Code Generation via Quantization: An Empirical Study,"Wei, XK; Gonugondla, SK; Wang, SQ; Ahmad, W; Ray, B; Qian, HF; Li, XP; Kumar, V; Wang, ZJ; Tian, YC; Sun, Q; Athiwaratkun, B; Shang, MY; Ramanathan, MK; Bhatia, P; Xiang, B",10.1145/3611643.3616302,2023,"ML-powered code generation aims to assist developers to write code in a more productive manner by intelligently generating code blocks based on natural language prompts. Recently, large pretrained deep learning models have pushed the boundary of code generation and achieved impressive performance.] However, the huge number of model parameters poses a significant challenge to their adoption in a typical software development environment, where a developer might use a standard laptop or mid-size server to develop code. Such large models cost significant resources in terms of memory, latency, dollars, as well as carbon footprint. Model compression is a promising approach to address these challenges. We have identified quantization as one of the most promising compression techniques for code-generation as it avoids expensive retraining costs. As quantization represents model parameters with lower-bit integer (e.g., int8), the model size and runtime latency would both benefit.] We empirically evaluate quantized models on code generation tasks across different dimensions: (i) resource usage and carbon footprint, (ii) accuracy, and (iii) robustness. Through systematic experiments we find a code-aware quantization recipe that could run even a 6-billion-parameter model in a regular laptop without significant accuracy or robustness degradation. We find that the recipe is readily applicable to code summarization task as well."
Privacy-preserving multi-source semi-supervised domain adaptation for seizure prediction,"Liang, D; Liu, AP; Wu, L; Li, C; Qian, RB; Chen, X",10.1007/s11571-023-10026-4,2024,"Domain adaptation (DA) has been frequently used to solve the inter-patient variability problem in EEG-based seizure prediction. However, existing DA methods require access to the existing patients' data when adapting the model, which leads to privacy concerns. Besides, most of them treat the whole existing patients' data as one single source and attempt to minimize the discrepancy with the target patient. This manner ignores the inter-patient variability among source patients, making the adaptation more difficult. Considering theses issues simultaneously, we present a novel multi-source-free semi-supervised domain adaptive seizure prediction model (MSF-SSDA-SPM). MSF-SSDA-SPM considers each source patient as one single source and generates a pretrained model from each source. Without requiring access to the source data, MSF-SSDA-SPM performs adaptation just using these pretrained source models and limited labeled target data. Specifically, we freeze the classifiers of all the source models and optimize the source feature extractors in a joint manner. Then we design a knowledge distillation strategy to integrate the knowledge of these well-adapted source models into one single target model. On the CHB-MIT dataset, MSF-SSDA-SPM attains a sensitivity of 88.6%, a FPR of 0.182/h and an AUC of 0.856; on the Kaggle dataset, it achieves 78.6%, 0.178/h and 0.784, respectively. Experimental results demonstrate that MSF-SSDA-SPM achieves both high privacy-protection and promising prediction performance."
Assemble Foundation Models for Automatic Code Summarization,"Gu, J; Salza, P; Gall, HC",10.1109/SANER53432.2022.00112,2022,"Automatic code summarization is beneficial to software development and maintenance since it reduces the burden of manual tasks. Currently, artificial intelligence is undergoing a paradigm shift. The foundation models pretrained on massive data and finetuned to downstream tasks surpass specially customized models. This trend inspired us to consider reusing foundation models instead of learning from scratch. Based on this, we propose a flexible and robust approach for automatic code summarization based on neural networks. We assemble available foundation models, such as CodeBERT and GPT-2, into a single model named AdaMo. Moreover, we utilize Gaussian noise as the simulation of contextual information to optimize the latent representation. Furthermore, we introduce two adaptive schemes from the perspective of knowledge transfer, namely continuous pretraining and intermediate finetuning, and design intermediate stage tasks for general sequence-to-sequence learning. Finally, we evaluate AdaMo against a benchmark dataset for code summarization, by comparing it with state-of-the-art models."
Generative AI for Software Practitioners,"Ebert, C; Louridas, P",10.1109/MS.2023.3265877,2023,"Generative artificial intelligence (AI) tools, such as Bard, ChatGPT, and CoPilot, have rapidly gained widespread usage. They also have the potential to boost software engineering productivity. In this article, we elaborate technologies and usage of generative AI in the software industry. We address questions, such as: How does generative AI improve software productivity? How to connect generative AI to software development, and what are the risks? Which technologies have what sorts of benefits? Practitioner guidance and case studies are shared from our industry context. I look forward to hearing from you about this column and the technologies that matter most for your work.-Christof Ebert"
Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book,"MacNeil, S; Tran, A; Hellas, A; Kim, J; Sarsa, S; Denny, P; Bernstein, S; Leinonen, J",10.1145/3545945.3569785,2023,"Advances in natural language processing have resulted in large language models (LLMs) that can generate code and code explanations. In this paper, we report on our experiences generating multiple code explanation types using LLMs and integrating them into an interactive e-book on web software development. Three different types of explanations - a line-by-line explanation, a list of important concepts, and a high-level summary of the code - were created. Students could view explanations by clicking a button next to code snippets, which showed the explanation and asked about its utility. Our results show that all explanation types were viewed by students and that the majority of students perceived the code explanations as helpful to them. However, student engagement varied by code snippet complexity, explanation type, and code snippet length. Drawing on our experiences, we discuss future directions for integrating explanations generated by LLMs into CS classrooms."
Towards Human-Bot Collaborative Software Architecting with ChatGPT,"Ahmad, A; Waseem, M; Liang, P; Fahmideh, M; Aktar, MS; Mikkonen, T",10.1145/3593434.3593468,2023,"Architecting software-intensive systems can be a complex process. It deals with the daunting tasks of unifying stakeholders' perspectives, designers' intellect, tool-based automation, pattern-driven reuse, and so on, to sketch a blueprint that guides software implementation and evaluation. Despite its benefits, architecture-centric software engineering (ACSE) suffers from a multitude of challenges. ACSE challenges could stem from a lack of standardized processes, socio-technical limitations, and scarcity of human expertise etc. that can impede the development of existing and emergent classes of software. Software Development Bots (DevBots) trained on large language models can help synergise architects' knowledge with artificially intelligent decision support to enable rapid architecting in a human-bot collaborative ACSE. An emerging solution to enable this collaboration is ChatGPT, a disruptive technology not primarily introduced for software engineering, but is capable of articulating and refining architectural artifacts based on natural language processing. We detail a case study that involves collaboration between a novice software architect and ChatGPT to architect a service-based software. Future research focuses on harnessing empirical evidence about architects' productivity and explores socio-technical aspects of architecting with ChatGPT to tackle challenges of ACSE."
The pipeline for the continuous development of artificial intelligence models-Current state of research and practice,"Steidl, M; Felderer, M; Ramler, R",10.1016/j.jss.2023.111615,2023,"Companies struggle to continuously develop and deploy Artificial Intelligence (AI) models to complex production systems due to AI characteristics while assuring quality. To ease the development process, continuous pipelines for AI have become an active research area where consolidated and in-depth analysis regarding the terminology, triggers, tasks, and challenges is required.This paper includes a Multivocal Literature Review (MLR) where we consolidated 151 relevant formal and informal sources. In addition, nine-semi structured interviews with participants from academia and industry verified and extended the obtained information. Based on these sources, this paper provides and compares terminologies for Development and Operations (DevOps) and Continuous Integration (CI)/Continuous Delivery (CD) for AI, Machine Learning Operations (MLOps), (end-to-end) lifecycle management, and Continuous Delivery for Machine Learning (CD4ML). Furthermore, the paper provides an aggregated list of potential triggers for reiterating the pipeline, such as alert systems or schedules. In addition, this work uses a taxonomy creation strategy to present a consolidated pipeline comprising tasks regarding the continuous development of AI. This pipeline consists of four stages: Data Handling, Model Learning, Software Development and System Operations. Moreover, we map challenges regarding pipeline implementation, adaption, and usage for the continuous development of AI to these four stages.(c) 2023 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/)."
Going green with artificial intelligence: The path of technological change towards the renewable energy transition,"Yin, HT; Wen, J; Chang, CP",10.24136/oc.2023.032,2023,"Research background: The twin pressures of economic downturn and climate change faced by countries around the world have become more pronounced over the past decade. A re-newable energy transition is believed to play a central role in mitigating the economic-climate paradox. While the architectural and computational power of artificial intelligence is particu- larly well suited to address the challenges of massive data processing and demand forecasting during a renewable energy transition, there is very scant empirical assessment that takes a social science perspective and explores the effects of AI development on the energy transi-tion.Purpose of the article: This paper aims to answer two key questions: One is, how does AI software development promote or inhibit the shift of energy consumption towards renewa-bles? The other is, under what policy interventions does AI software development have a more positive effect on promoting renewable energy consumption?Methods: We employ a dataset of 62 economies covering the period 2011-2020 to analyze the impact of AI software development on the energy transition, where possible confounders, including political and economic characteristics and time-invariant elements, are controlled using fixed-effects estimation along with specified covariates.Findings & value added: AI software development can promote the energy transition to-wards renewables. There is suggestive evidence that the core mechanism linking such a posi-tive relationship tends to lie in improving innovation performance in environmental monitor-ing rather than in green computing. Government support for R&D in renewable energy tech-nologies is found to be significantly beneficial for harnessing the positive impact of AI soft-ware development on the energy transition. Compared to non-market-based environmental policies, market-based environmental policies have a more significant positive moderating effect on the relationship between AI software development and energy transition."
Software effort estimation modeling and fully connected artificial neural network optimization using soft computing techniques,"Kassaymeh, S; Alweshah, M; Al-Betar, MA; Hammouri, AI; Al-Ma'aitah, MA",10.1007/s10586-023-03979-y,2024,"In software engineering, the planning and budgeting stages of a software project are of great importance to all stakeholders, including project managers as well as clients. The estimated costs and scheduling time needed to develop any software project before and/or during startup form the basis of a project's success. The main objective of soft- ware estimation techniques is to determine the actual effort and/or time required for project development. The use of machine learning methods to address the estimation problem has, in general, proven remarkably successful for many engineering problems. In this study, a fully connected neural network (FCNN) model and a metaheuristic, gray wolf optimizer (GWO), called GWO-FC, is proposed to tackle the software development effort estimation (SEE) problem. The GWO is integrated with FCNN to optimize the FCNN parameters in order to enhance the accuracy of the obtained results by improving the FCNN's ability to explore the parameter search field and avoid falling into local optima. The proposed technique was evaluated utilizing various benchmark SEE datasets. Furthermore, various recent algorithms from the literature were employed to verify the GWO-FC performance. In terms of accuracy, comparative outcomes reveal that the GWO-FC performs better than other methods in most datasets and evaluation criteria. Experimental outcomes reveal the strong potential of the GWO-FC method to achieve reliable estimation results."
Artificial Intelligence and User Experience in reciprocity: Contributions and state of the art,"Virvou, M",10.3233/IDT-230092,2023,"Among the primary aims of Artificial Intelligence (AI) is the enhancement of User Experience (UX) by providing deep understanding, profound empathy, tailored assistance, useful recommendations, and natural communication with human interactants while they are achieving their goals through computer use. To this end, AI is used in varying techniques to automate sophisticated functions in UX and thereby changing what UX is apprehended by the users. This is achieved through the development of intelligent interactive systems such as virtual assistants, recommender systems, and intelligent tutoring systems. The changes are well received, as technological achievements but create new challenges of trust, explainability and usability to humans, which in turn need to be amended by further advancements of AI in reciprocity. AI can be utilised to enhance the UX of a system while the quality of the UX can influence the effectiveness of AI. The state of the art in AI for UX is constantly evolving, with a growing focus on designing transparent, explainable, and fair AI systems that prioritise user control and autonomy, protect user data privacy and security, and promote diversity and inclusivity in the design process. Staying up to date with the latest advancements and best practices in this field is crucial. This paper conducts a critical analysis of published academic works and research studies related to AI and UX, exploring their interrelationship and the cause-effect cycle between the two. Ultimately, best practices for achieving a successful interrelationship of AI in UX are identified and listed based on established methods or techniques that have been proven to be effective in previous research reviewed."
GAMMA: Revisiting Template-based Automated Program Repair via Mask Prediction,"Zhang, QJ; Fang, CR; Zhang, TK; Yu, BW; Sun, WS; Chen, ZY",10.1109/ASE56229.2023.00063,2023,"Automated program repair (APR) aims to fix software bugs without manual debugging efforts and plays a crucial role in software development and maintenance. Template-based APR has been widely investigated and shown promising results. However, it is challenging for template-based APR to select the appropriate donor code, which is an important repair ingredient for generating candidate patches. Inappropriate donor code may cause plausible but incorrect patch generation even with correct fix patterns, limiting the repair performance. In this paper, we aim to revisit template-based APR, and propose GAMMA, to directly leverage large pre-trained language models for donor code generation. Our main insight is that instead of retrieving donor code in the local buggy file, we can directly predict the correct code tokens based on the context code snippets and repair patterns by a cloze task. Specifically, (1) GAMMA revises a variety of fix templates from state-of-the-art template-based APR techniques (i.e., TBar) and transforms them into mask patterns. (2) GAMMA adopts a pre-trained language model to predict the correct code for masked code as a fill-in-the-blank task. Although our idea is general and can be built on various existing pre-trained language models, we have implemented GAMMA as a practical APR tool based on the recent UniXcoder model. The experimental results demonstrate that GAMMA correctly repairs 82 bugs on Defects4J-v1.2, which achieves 20.59% (14 bugs) and 26.15% (17 bugs) improvement over the previous state-of-the-art template-based approach TBar and learning-based one Recoder. Furthermore, GAMMA repairs 45 bugs and 22 bugs from the additional Defects4J-v2.0 and QuixBugs, indicating the generalizability of GAMMA in addressing the dataset overfitting issue. We also prove that adopting other pre-trained language models can provide substantial advancement, e.g., CodeBERT-based and ChatGPT-based GAMMA is able to fix 80 and 67 bugs on Defects4J-v1.2, indicating the scalability of GAMMA. Overall, our study highlights the promising future of adopting pre-trained models to generate correct patches on top of fix patterns in practice."
NLP-Based Automated Compliance Checking of Data Processing Agreements Against GDPR,"Cejas, OA; Azeem, MI; Abualhaija, S; Briand, LC",10.1109/TSE.2023.3288901,2023,"When the entity processing personal data (the processor) differs from the one collecting personal data (the controller), processing personal data is regulated in Europe by the General Data Protection Regulation (GDPR) through data processing agreements (DPAs). Checking the compliance of DPAs contributes to the compliance verification of software systems as DPAs are an important source of requirements for software development involving the processing of personal data. However, manually checking whether a given DPA complies with GDPR is challenging as it requires significant time and effort for understanding and identifying DPA-relevant compliance requirements in GDPR and then verifying these requirements in the DPA. Legal texts introduce additional complexity due to convoluted language and inherent ambiguity leading to potential misunderstandings. In this paper, we propose an automated solution to check the compliance of a given DPA against GDPR. In close interaction with legal experts, we first built two artifacts: (i) the shall requirements extracted from the GDPR provisions relevant to DPA compliance and (ii) a glossary table defining the legal concepts in the requirements. Then, we developed an automated solution that leverages natural language processing (NLP) technologies to check the compliance of a given DPA against these shall requirements. Specifically, our approach automatically generates phrasal-level representations for the textual content of the DPA and compares them against predefined representations of the shall requirements. By comparing these two representations, the approach not only assesses whether the DPA is GDPR compliant but it further provides recommendations about missing information in the DPA. Over a dataset of 30 actual DPAs, the approach correctly finds 618 out of 750 genuine violations while raising 76 false violations, and further correctly identifies 524 satisfied requirements. The approach has thus an average precision of 89.1%, a recall of 82.4%, and an accuracy of 84.6%. Compared to a baseline that relies on off-the-shelf NLP tools, our approach provides an average accuracy gain of approximate to 20 percentage points. The accuracy of our approach can be improved to approximate to 94% with limited manual verification effort."
CODEEDITOR: Learning to Edit Source Code with Pre-trained Models,"Li, J; Li, G; Li, Z; Jin, Z; Hu, X; Zhang, KC; Fu, ZY",10.1145/3597207,2023,"Developers often perform repetitive code editing activities (up to 70%) for various reasons (e.g., code refactoring) during software development. Many deep learning (DL) models have been proposed to automate code editing by learning from the code editing history. Among DL-based models, pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for automatic code editing. In this article, we propose a novel pre-training task specialized in code editing and present an effective pre-trained code editing model named CODEEDITOR. Compared to previous code infilling tasks, our pre-training task further improves the performance and generalization ability of code editing models. Specifically, we collect lots of real-world code snippets as the ground truth and use a powerful generator to rewrite them into mutated versions. Then, we pre-train our CODEEDITOR to edit mutated versions into the corresponding ground truth, to learn edit patterns. We conduct experiments on four code editing datasets and evaluate the pre-trained CODEEDITOR in three settings (i.e., fine-tuning, few-shot, and zero-shot). (1) In the fine-tuning setting, we train the pre-trained CODEEDITOR with four datasets and evaluate it on the test data. CODEEDITOR outperforms the SOTA baselines by 15%, 25.5%, 9.4%, and 26.6% on four datasets. (2) In the few-shot setting, we train the pre-trained CODEEDITOR with limited data and evaluate it on the test data. CODEEDITOR substantially performs better than all baselines, even outperforming baselines that are fine-tuned with all data. (3) In the zero-shot setting, we evaluate the pre-trained CODEEDITOR on the test data without training. CODEEDITOR correctly edits 1,113 programs, while the SOTA baselines cannot work. The results show that the superiority of our pre-training task and the pre-trained CODEEDITOR is more effective in automatic code editing."
Effort and Cost Estimation Using Decision Tree Techniques and Story Points in Agile Software Development,"SÃ¡nchez, ER; Santacruz, EFV; Maceda, HC",10.3390/math11061477,2023,"Early effort estimation is important for efficiently planning the use of resources in an Information Technology (IT) project. However, limited research has been conducted on the topic of effort estimation in agile software development using artificial intelligence. This research project contributes to strengthening the use of hybrid models composed of algorithmic models and learning oriented techniques as a project-level effort estimation method in agile frameworks. Effort estimation in agile methods such as Scrum uses a story point approach that measures, using an arithmetic scale, the effort required to complete a release of the system. This project relied on labeled historical data to estimate the completion time measured in days and the total cost of a project set in Pakistani rupees (PKR). using a decision tree, random forest and AdaBoost to improve the accuracy of predictions. Models were trained using 10-fold cross-validation and the relative error was used as a comparison with literature results. The bootstrap aggregation (bagging) ensemble made of the three techniques provides the highest accuracy, and project classification also improves the estimates."
MORGAN: a modeling recommender system based on graph kernel,"Di Sipio, C; Di Rocco, J; Di Ruscio, D; Nguyen, PT",10.1007/s10270-023-01102-8,2023,"Model-driven engineering (MDE) is an effective means of synchronizing among stakeholders, thereby being a crucial part of the software development life cycle. In recent years, MDE has been on the rise, triggering the need for automatic modeling assistants to support metamodelers during their daily activities. Among others, it is crucial to enable model designers to choose suitable components while working on new (meta)models. In our previous work, we proposed MORGAN, a graph kernel-based recommender system to assist developers in completing models and metamodels. To provide input for the recommendation engine, we convert training data into a graph-based format, making use of various natural language processing (NLP) techniques. The extracted graphs are then fed as input for a recommendation engine based on graph kernel similarity, which performs predictions to provide modelers with relevant recommendations to complete the partially specified (meta)models. In this paper, we extend the proposed tool in different dimensions, resulting in a more advanced recommender system. Firstly, we equip it with the ability to support recommendations for JSON schema that provides a model representation of data handling operations. Secondly, we introduce additional preprocessing steps and a kernel similarity function based on item frequency, aiming to enhance the capabilities, providing more precise recommendations. Thirdly, we study the proposed enhancements, conducting a well-structured evaluation by considering three real-world datasets. Although the increasing size of the training data negatively affects the computation time, the experimental results demonstrate that the newly introduced mechanisms allow MORGAN to improve its recommendations compared to its preceding version."
Applications of natural language processing in software traceability: A systematic mapping study?,"Pauzi, Z; Capiluppi, A",10.1016/j.jss.2023.111616,2023,"A key part of software evolution and maintenance is the continuous integration from collaborative efforts, often resulting in complex traceability challenges between software artifacts: features and modules remain scattered in the source code, and traceability links become harder to recover. In this paper, we perform a systematic mapping study dealing with recent research recovering these links through information retrieval, with a particular focus on natural language processing (NLP). Our search strategy gathered a total of 96 papers in focus of our study, covering a period from 2013 to 2021. We conducted trend analysis on NLP techniques and tools involved, and traceability efforts (applying NLP) across the software development life cycle (SDLC). Based on our study, we have identified the following key issues, barriers, and setbacks: syntax convention, configuration, translation, explainability, properties representation, tacit knowledge dependency, scalability, and data availability. Based on these, we consolidated the following open challenges: representation similarity across artifacts, the effectiveness of NLP for traceability, and achieving scalable, adaptive, and explainable models. To address these challenges, we recommend a holistic framework for NLP solutions to achieve effective traceability and efforts in achieving interoperability and explainability in NLP models for traceability. (c) 2023 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/)."
AI Unreliable Answers: A Case Study on ChatGPT,"Amaro, I; Della Greca, A; Francese, R; Tortora, G; Tucci, C",10.1007/978-3-031-35894-4_2,2023,"ChatGPT is a general domain chatbot which is object of great attention stimulating all the world discussions on the power and the consequences of the Artificial Intelligence diffusion in all the field, ranging from education, research, music to software development, health care, cultural heritage, and entertainment. In this paper, we try to investigate whether and when the answers provided by ChatGPT are unreliable and how this is perceived by expert users, such as Computer Science students. To this aim, we first analyze the reliability of the answers provided by ChatGPT by experimenting its narrative, problem solving, searching, and logic capabilities and report examples of answers. Then, we conducted a user study in which 15 participants that already knew the chatbot proposed a set of predetermined queries generating both correct and incorrect answers and then we collected their satisfaction. Results revealed that even if the present version of ChatGPT sometimes is unreliable, people still plan to use it. Thus, it is recommended to use the present version of ChatGPT always with the support of human verification and interpretation."
Software fault prediction using deep learning techniques,"Batool, I; Khan, TA",10.1007/s11219-023-09642-4,2023,"Software fault prediction (SFP) techniques identify faults at the early stages of the software development life cycle (SDLC). We find machine learning techniques commonly used for SFP compared to deep learning methods, which can produce more accurate results. Deep learning offers exceptional results in various domains, such as computer vision, natural language processing, and speech recognition. In this study, we use three deep learning methods, namely, long short-term memory (LSTM), bidirectional LSTM (BILSTM), and radial basis function network (RBFN) to predict software faults and compare our results with existing models to show how our results are more accurate. Our study uses Chidamber and Kemerer (CK) metrics-based datasets to conduct experiments and test our proposed algorithm. We conclude that LSTM and BILSTM perform better, whereas RBFN is faster in producing the required results. We use k-fold cross-validation to do the model evaluation. Our proposed models provide software developers with a more accurate and efficient SFP mechanism."
"Gait analysis comparison between manual marking, 2D pose estimation algorithms, and 3D marker-based system","Menychtas, D; Petrou, N; Kansizoglou, I; Giannakou, E; Grekidis, A; Gasteratos, A; Gourgoulis, V; Douda, E; Smilios, I; Michalopoulou, M; Sirakoulis, GC; Aggelousis, N",10.3389/fresc.2023.1238134,2023,"IntroductionRecent advances in Artificial Intelligence (AI) and Computer Vision (CV) have led to automated pose estimation algorithms using simple 2D videos. This has created the potential to perform kinematic measurements without the need for specialized, and often expensive, equipment. Even though there's a growing body of literature on the development and validation of such algorithms for practical use, they haven't been adopted by health professionals. As a result, manual video annotation tools remain pretty common. Part of the reason is that the pose estimation modules can be erratic, producing errors that are difficult to rectify. Because of that, health professionals prefer the use of tried and true methods despite the time and cost savings pose estimation can offer.MethodsIn this work, the gait cycle of a sample of the elderly population on a split-belt treadmill is examined. The Openpose (OP) and Mediapipe (MP) AI pose estimation algorithms are compared to joint kinematics from a marker-based 3D motion capture system (Vicon), as well as from a video annotation tool designed for biomechanics (Kinovea). Bland-Altman (B-A) graphs and Statistical Parametric Mapping (SPM) are used to identify regions of statistically significant difference.ResultsResults showed that pose estimation can achieve motion tracking comparable to marker-based systems but struggle to identify joints that exhibit small, but crucial motion.DiscussionJoints such as the ankle, can suffer from misidentification of their anatomical landmarks. Manual tools don't have that problem, but the user will introduce a static offset across the measurements. It is proposed that an AI-powered video annotation tool that allows the user to correct errors would bring the benefits of pose estimation to professionals at a low cost."
A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI,"Santa Barletta, V; Caivano, D; Gigante, D; Ragone, A",10.1145/3593434.3593478,2023,"In the last years, the raise of Artificial Intelligence (AI), and its pervasiveness in our lives, has sparked a flourishing debate about the ethical principles that should lead its implementation and use in society. Driven by these concerns, we conduct a rapid review of several frameworks providing principles, guidelines, and/or tools to help practitioners in the development and deployment of Responsible AI (RAI) applications. We map each framework w.r.t. the different Software Development Life Cycle (SDLC) phases discovering that most of these frameworks fall just in the Requirements Elicitation phase, leaving the other phases uncovered. Very few of these frameworks offer supporting tools for practitioners, and they are mainly provided by private companies. Our results reveal that there is not a catching-all framework supporting both technical and non-technical stakeholders in the implementation of real-world projects. Our findings highlight the lack of a comprehensive framework encompassing all RAI principles and all (SDLC) phases that could be navigated by users with different skill sets and with different goals."
An NLP-based quality attributes extraction and prioritization framework in Agile-driven software development,"Ahmed, M; Khan, SUR; Alam, KA",10.1007/s10515-022-00371-9,2023,"Software quality plays a significant role in ensuring the customer demands and expectations. Generally speaking, Quality of the software is a functional behaviour that heavily depends on the non-functional requirements. However, generally software engineer's pay relatively lesser attention to the non-functional requirements. Moreover, it is of vital importance to have a clear view of software's quality as early as possible, because it can affect the different artefacts of the software at later development stages including implementation, testing, and maintenance. The early-stage conformance of software quality is more important in agile-based software development where the requirements are more volatile than any other development environments. The early knowledge about the software quality can positively impact on the design decisions in agile-based software development context. Motivated by this, we propose a conceptual framework for automatic extraction and prioritization of quality attributes from the user stories in an agile-based development context. The proposed framework contains two main components including QAExtractor and QAPrioritiser. The core of this framework (QAExtractor) is based on natural language processing, which generalise the user stories for a specific quality attribute. In contrast, QAPrioritiser ranks the extracted quality attributes grounded on the frequency, roles impact, and criticality factor value. We validate the effectiveness of the proposed framework using two case studies. The results revealed that the proposed framework outperforms the existing technique in terms of precision, recall, and F measure."
Does Industry 5.0 Reproduce Gender (In)equalities at Organisations? Understanding the Interaction of Human Resources and Software Development Teams in Supplying Human Capitals,"Aydin, E; Rahman, M; Ozeren, E",10.1007/s10796-023-10450-1,2023,"The aim of this study is to illustrate the significance of human resources and software development teams in the process of value co-creation, specifically in the provision of human capital within the framework of Industry 5.0. This investigation takes into account ethical considerations, machine ethics, and gender inequalities. In order to achieve this aim, we conduct semi-structured in-depth qualitative interviews with 12 Human Resources Specialists and 12 Computer Engineers in large scale organisations in Turkey. As a theoretical lens, we adopt modified grounded theory to explore the interaction of teams for demonstrating how they design and manage the digital process by considering the human-machine collaboration aspect of Industry 5.0. Based on the interviews, there are three main themes in the present research: digitalisation in tracking personnel data, ensuring ethical actions in digitalisation of organisational process, and reflections of digitalisation to gender inequality. Since studies on diversity and industry 5.0 are scarce, this research demonstrates the ethical and adverse aspects of industry 5.0, and how it reflects to gender inequality in organisations."
Automating Code Review Activities by Large-Scale Pre-training,"Li, ZY; Lu, S; Guo, D; Duan, N; Jannu, S; Jenks, G; Majumder, D; Green, J; Svyatkovskiy, A; Fu, SY; Sundaresan, N",10.1145/3540250.3549081,2022,"Code review is an essential part to software development lifecycle since it aims at guaranteeing the quality of codes. Modern code review activities necessitate developers viewing, understanding and even running the programs to assess logic, functionality, latency, style and other factors. It turns out that developers have to spend far too much time reviewing the code of their peers. Accordingly, it is in significant demand to automate the code review process. In this research, we focus on utilizing pre-training techniques for the tasks in the code review scenario. We collect a large-scale dataset of real-world code changes and code reviews from open-source projects in nine of the most popular programming languages. To better understand code diffs and reviews, we propose CodeReviewer, a pre-trained model that utilizes four pre-training tasks tailored specifically for the code review scenario. To evaluate our model, we focus on three key tasks related to code review activities, including code change quality estimation, review comment generation and code refinement. Furthermore, we establish a high-quality benchmark dataset based on our collected data for these three tasks and conduct comprehensive experiments on it. The experimental results demonstrate that our model outperforms the previous state-of-the-art pre-training approaches in all tasks. Further analysis show that our proposed pre-training tasks and the multilingual pre-training dataset benefit the model on the understanding of code changes and reviews."
The Impact of ChatGPT on Streaming Media: A Crowdsourced and Data-Driven Analysis using Twitter and Reddit,"Feng, YH; Poralla, P; Dash, S; Li, KC; Desai, V; Qiu, MK",10.1109/BigDataSecurity-HPSC-IDS58521.2023.00046,2023,"ChatGPT, a general-purpose text generation Al model, is reshaping various domains ranging from education and software development to legal defense and novel writing. Despite its potential impact, there is a lack of research on how ChaIGPT might influence streaming media, which is an essential part of everyday entertainment. As a result, it remains unclear how ChatGPT is changing the future of streaming media. To bridge such a research gap, in this paper, we propose a crowdsourced, data-driven framework that leverages two social media platforms, Twitter and Reddit, to explore the impact of ChatGPT on streaming media. Through extensive analysis of social media data collected from Twitter and Reddit, we reveal how ChatGPT is transforming streaming media from diverse perspectives. Our data analytics demonstrates that ChatGPT is sparking both fear and excitement in the context of the streaming media and enhancing the downstream visual generative models, such as DALLE-2 and Stable Diffusion Videos. To the best of our knowledge, this study is the first large-scale and systematical investigation into the effects of ChatGPT on streaming media. Hope our findings will inspire further research and discussions on this topic across academia and industry."
Agile Effort Estimation: Have We Solved the Problem Yet? Insights From a Replication Study,"Tawosi, V; Moussa, R; Sarro, F",10.1109/TSE.2022.3228739,2023,"In the last decade, several studies have explored automated techniques to estimate the effort of agile software development. We perform a close replication and extension of a seminal work proposing the use of Deep Learning for Agile Effort Estimation (namely Deep-SE), which has set the state-of-the-art since. Specifically, we replicate three of the original research questions aiming at investigating the effectiveness of Deep-SE for both within-project and cross-project effort estimation. We benchmark Deep-SE against three baselines (i.e., Random, Mean and Median effort estimators) and a previously proposed method to estimate agile software project development effort (dubbed TF/IDF-SVM), as done in the original study. To this end, we use the data from the original study and an additional dataset of 31,960 issues mined from TAWOS, as using more data allows us to strengthen the confidence in the results, and to further mitigate external validity threats. The results of our replication show that Deep-SE outperforms the Median baseline estimator and TF/IDF-SVM in only very few cases with statistical significance (8/42 and 9/32 cases, respectively), thus confounding previous findings on the efficacy of Deep-SE. The two additional RQs revealed that neither augmenting the training set nor pre-training Deep-SE play lead to an improvement of its accuracy and convergence speed. These results suggest that using semantic similarity is not enough to differentiate user stories with respect to their story points; thus, future work has yet to explore and find new techniques and features that obtain accurate agile software development estimates."
A Mixed Reality Approach for Innovative Pair Programming Education with a Conversational AI Virtual Avatar,"Manfredi, G; Erra, U; Gilio, G",10.1145/3593434.3593952,2023,"yPair Programming (PP) is an Agile software development methodology that involves two developers working together on a single computer. However, the physical presence of two developers has become a challenge in recent years due to the pandemic, necessitating remote collaboration methods such as Distributed Pair Programming (DPP). DPP has been found to have similar benefits to in-person PP, but the issue of team compatibility remains unresolved. These are more evident in the educational field of Agile methodologies. To address these challenges, we developed a novel approach by creating a Mixed Reality (MR) application that enables users to learn PP with the assistance of a conversational intelligent virtual avatar. The application uses the HoloLens MR device and a Conversational Agent (CA) extension integrated into Visual Studio Code to provide suggestions for improving the code written by the user. The virtual avatar animates these suggestions, making it appear to speak and interact with the user in real time. This system aims to overcome the limitations of common DPP methods, allowing a single developer to learn and apply the PP methodology even when a human partner is unavailable."
Speeding Up the Engineering of Interactive Systems with Generative AI,"Schmidt, A",10.1145/3596454.3597176,2023,"This keynote discusses the opportunities and challenges of using Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) as tools for developing interactive systems. We will look at different stages in the development lifecycle of interactive systems and assess the value of AI support. We explore how GenAI and LLMs can potentially speed-up the ideation, requirements elicitation, architecture development, prototyping, implementation, and testing of interactive systems. The talk will outline emerging practices, such as the use of prompts for code and system generation, to facilitate prototyping and accelerate implementation. We will outline fundamental challenges and suggest emerging research directions, and pose research questions. What will software development tools look like in the future? How can we efficiently use AI to develop interactive systems without compromising quality? We also speculate about the implications of these developments for researchers, practitioners, and society. We believe that it will massively accelerate the digital transformation. Interactive AI-based tools for systems and software development will become a major research direction."
Exploring Early Adopters' Perceptions of ChatGPT as a Code Generation Tool,"Scoccia, GL",10.1109/ASEW60602.2023.00016,2023,"ChatGPT is an artificial intelligence chatbot developed by OpenAI, able of interacting in a conversational way by taking into account successive input prompts. Among many possible uses, ChatGPT has been found to possess code generation capabilities, being able to generate code snippets and assist developers in their programming tasks. This paper performs a qualitative exploration of perceptions of early adopters regarding the use of ChatGPT for code generation, acknowledging the substantial impact this tool can have in the software development landscape. We collected a diverse set of discussions from early adopters of ChatGPT code generation capabilities and, leveraging an open card sorting methodology categorized it into relevant topics with the goal of extracting insights into the experiences, opinions, and challenges they faced. We found that early adopters (i) report their own mixed usage experiences, (ii) share suggestions for prompt engineering, (iii) debate the extent to which they can trust generated code, and (iv) discuss the impact that ChatGPT can have on the software development process. We discuss the implications of the insights we extracted from early adopters' perspectives and provide recommendations for future research."
Along the Margins: Marginalized Communities' Ethical Concerns about Social Platforms,"Olson, L; Guzman, E; Kunneman, F",10.1109/ICSE-SEIS58686.2023.00013,2023,"In this paper, we identified marginalized communities' ethical concerns about social platforms. We performed this identification because recent platform malfeasance indicates that software teams prioritize shareholder concerns over user concerns. Additionally, these platform shortcomings often have devastating effects on marginalized populations. We first scraped 586 marginalized communities' subreddits, aggregated a dataset of their social platform mentions and manually annotated mentions of ethical concerns in these data. We subsequently analyzed trends in the manually annotated data and tested the extent to which ethical concerns can be automatically classified by means of natural language processing (NLP). We found that marginalized communities' ethical concerns predominantly revolve around discrimination and misrepresentation, and reveal deficiencies in current software development practices. As such, researchers and developers could use our work to further investigate these concerns and rectify current software flaws. General Abstract- In this paper, we identified marginalized communities' ethical concerns about social platforms. We did this because recent platform wrongdoing indicates that software teams prioritize profit over user concerns. Additionally, these platform shortcomings often have devastating effects on marginalized populations. To accomplish this, we collected Reddit posts from marginalized communities' subreddits where users mention social media platforms. Then, we labeled whether posts contained mentions of ethical concerns, like privacy or misinformation. Finally, we established trends within the resulting data and used artificial intelligence (AI) to find these ethical concerns automatically. We discovered that marginalized communities' ethical concerns revolve around discrimination and misrepresentation, among other problems, and reveal deficiencies in current social platforms. As such, researchers and software engineers could use our work to further investigate these concerns and rectify present software flaws."
BERT-Based Approach for Greening Software Requirements Engineering Through Non-Functional Requirements,"Subahi, AF",10.1109/ACCESS.2023.3317798,2023,"The incorporation of sustainability principles during the requirements engineering phase of the development life cycle constitutes greening software requirements. This incorporation can have a variety of effects on the software design employed in modern and cutting-edge information technology (IT) systems. When sustainability principles are incorporated into requirements engineering, software design priorities can change and address current design issues such as energy and resource consumption, modularity, maintainability, and adaptability. In contrast to other green approaches that consider sustainable development, there is a further need to investigate the relationship between software development and the relevant green principles of sustainability during the requirements engineering phase. We present a new mechanism for mapping software nonfunctional requirements (NFRs) to defined dimensions of green software sustainability, consisting of two mapping steps: 1) between NFRs and sustainability dimensions; and 2) between sustainability dimensions and two clusters of green IT aspects defined in this work. The overall architecture of the promising approach is based on the use of the Bidirectional Encoder Representations from Transformers (BERT) language model with an expanded dataset. We consider transfer learning and domain-specific fine-tuning capabilities for constructing and evaluating a model specifically tailored for developing a proof of concept of the greening software requirements engineering task, as language models have recently emerged as a potent technique in the field of software engineering, with numerous applications in code analysis, automated documentation, and code generation. In addition, we test the model's performance using an extended version of the PROMISE_exp dataset after adding a new binary classification column for categorizing sustainability dimensions into two defined clusters: Eco-technical and Socioeconomic, and having a selected domain expert label the raw data. The model's efficiency is evaluated using four matrices-1) accuracy; 2) precision; 3) recall; and 4) F1 score-across a variety of epoch and batch sizes. Our numerical results demonstrate the viability of the approach in text classification tasks via performing well in mapping NFRs to software sustainability dimensions. This acts as a proof of concept for automating the sustainability measurement of software awareness at the early development stage. In addition, the results emphasize the importance of domain-specific fine-tuning and transfer learning for obtaining high performance in classification tasks in requirements engineering."
Cross-project clone consistent-defect prediction via transfer-learning method,"Jiang, WC; Qiu, SJ; Liang, TC; Zhang, FL",10.1016/j.ins.2023.03.118,2023,"Code clones are comparable code snippets that are introduced into software by developers in order to increase software development productivity. A change to code clone may result in a consistent-defect if the developers forget to verify the consistency of the code after the change. To reduce such change-related maintenance costs, researchers have proposed a number of methods for predicting clone consistency in advance. Unfortunately, the effectiveness of these cross-project models is unsatisfactory, and performing such predictions with insufficient data remains a challenge. Meanwhile, cross-project defect prediction via transfer learning method is prevalent in the software engineering community. Consequently, we first construct an empirical study to explore whether transfer-learning techniques could well be utilized for clone cross-project consistent-defect prediction in the initial stages of software development. In this paper, we employ transfer-learning techniques to predict clone consistency at both the time of clone creating and clone changing in order to avoid clone consistent-defects and maintenance. We conduct an experiment on open-source projects to evaluate the effectiveness of various transfer-learning methods. Our investigation demonstrates that transfer-learning techniques have a beneficial impact on predicting cross-project clone consistent-defect, and that the size of the dataset also has a positive effect on prediction. In order to promote software safety and security, we recommend that developers leverage transfer-learning to enhance the capability for clone cross -project consistent-defect prediction early in the software development phase."
AI and Blockchain-based source code vulnerability detection and prevention system for multiparty software development,"Nath, P; Mushahary, JR; Roy, U; Brahma, M; Singh, PK",10.1016/j.compeleceng.2023.108607,2023,"With the growing demand for application software, there is a race among industries to develop software as quickly as possible. However, maintaining pace and ensuring bug-free software has become increasingly challenging in a work-from-home arrangement as software developers are not under constant supervision. It increases the possibility of buggy products, and traditional testing techniques fail to provide optimal performance. We propose an Artificial Intelligence (AI) and blockchain-based novel decentralized software testing system. The proposed system aims to detect and prevent vulnerable code by synergizing deep learning capabilities and smart-contractpowered blockchain. The vulnerability detection is performed automatically without relying on manually written rules. We propose a non-vulnerability score range map to classify the source code. Furthermore, we integrate an InterPlanetary File System (IPFS) to ensure efficient storage over the blockchain. We conduct a testbed-based experiment to demonstrate the effectiveness of AI and blockchain integration for secure code development and testing."
Automatic Code Documentation Generation Using GPT-3,"Khan, JY; Uddin, G",10.1145/3551349.3559548,2022,"Source code documentation is an important artifact for efficient software development. Code documentation could greatly benefit from automation since manual documentation is often labouring, resource and time-intensive. In this paper, we employed Codex for automatic code documentation creation. Codex is a GPT-3 based model pre-trained on both natural and programming languages. We find that Codex outperforms existing techniques even with basic settings like one-shot learning (i.e., providing only one example for training). Codex achieves an overall BLEU score of 20.6 for six different programming languages (11.2% improvement over earlier state-of-the-art techniques). Thus, Codex shows promise and warrants in-depth future studies for automatic code documentation generation to support diverse development tasks."
Using Deep Learning to Generate Complete Log Statements,"Mastropaolo, A; Pascarella, L; Bavota, G",10.1145/3510003.3511561,2022,"Logging is a practice widely adopted in several phases of the software lifecycle. For example, during software development log statements allow engineers to verify and debug the system by exposing fine-grained information of the running software. While the benefits of logging are undisputed, taking proper decisions about where to inject log statements, what information to log, and at which log level (e.g., error, warning) is crucial for the logging effectiveness. In this paper, we present LANCE (Log stAtemeNt reCommEnder), the first approach supporting developers in all these decisions. LANCE features a Text-To-Text-Transfer-Transformer (T5) model that has been trained on 6,894,456 Java methods. LANCE takes as input a Java method and injects in it a full log statement, including a human-comprehensible logging message and properly choosing the needed log level and the statement location. Our results show that LANCE is able to (i) properly identify the location in the code where to inject the statement in 65.9% of Java methods requiring it; (ii) selecting the proper log level in 66.2% of cases; and (iii) generate a completely correct log statement including a meaningful logging message in 15.2% of cases."
Adaptive Testing and Debugging of NLP Models,"Ribeiro, MT; Lundberg, S",,2022,"Current approaches to testing and debugging NLP models rely on highly variable human creativity and extensive labor, or only work for a very restrictive class of bugs. We present AdaTest, a process which uses large scale language models (LMs) in partnership with human feedback to automatically write unit tests highlighting bugs in a target model. Such bugs are then addressed through an iterative text-fix-retest loop, inspired by traditional software development. In experiments with expert and non-expert users and commercial / research models for 8 different tasks, AdaTest makes users 5-10x more effective at finding bugs than current approaches, and helps users effectively fix bugs without adding new bugs."
Evaluation of an artificial intelligence project in the software industry based on fuzzy analytic hierarchy process and complex adaptive systems,"Chang, TS",10.1108/JEIM-02-2022-0056,2023,"PurposeArtificial intelligence (AI) is the most progressive commodity among current information system applications. In-house development and sales of beneficial products are difficult for many software development and service companies (SDSCs). SDSCs have some implicit concerns about implementing AI software development due to the complexity of AI technology; they require an evaluation framework to avoid development failure. To fill the void, this study identified the factors influencing SDSCs when developing AI software development.Design/methodology/approachBased on complex adaptive systems theory, three aspects were developed as the main factors of hierarchy, namely, employees' capabilities, environmental resources and team capabilities. Fuzzy analytic hierarchy process (FAHP) was used to assess the SDSCs' attitude. Based on SDSCs, attitudes toward implementing AI software projects were collected to calculate the hierarchy of factors.FindingsThe outcome of FAHP is used as understanding the key factors of SDSCs for selecting an AI software project, toward the improvement of overall project planning. Employees' stress resistance was considered as a priority for the project, although professional AI skills and resources were also important.Originality/valueThis study suggested three variables developed using complex adaptive systems. This study contributes to a better understanding of the critical aspects of developing AI software projects in SDSCs. The study's findings have practical and academic implications for SDSCs and subsequent academic development, broadening the scope of AI software development research."
Combining low-code development with ChatGPT to novel no-code approaches: A focus-group study,"Martins, J; Branco, F; Mamede, H",10.1016/j.iswa.2023.200289,2023,"Low-code tools are a trend in software development for business solutions due to their agility and ease of use. There are a certain number of vendors with such solutions. Still, in most Western countries, there is a clear need for the existence of greater quantities of certified and experienced professionals to work with those tools. This means that companies with more resources can attract and maintain those professionals, whilst other smaller organizations must rely on an endless search for this scarce resource. We will present and validate a model designed to transform ChatGPT into a low-code developer, addressing the demand for a more skilled human resource solution. This innovative tool underwent rigorous validation via a focus group study, engaging a panel of highly experienced experts. Their invaluable insights and feedback on the proposed model were systematically gathered and meticulously analysed."
Non Functional Requirements Identification and Classification Using Transfer Learning Model,"Khan, MA; Khan, MS; Khan, I; Ahmad, S; Huda, S",10.1109/ACCESS.2023.3295238,2023,"In this research study, we address the critical task of identifying and classifying non-functional requirements (NFRs) in software development. NFRs, described in the software requirements specification (SRS) document, offer a comprehensive system view and are closely aligned with software design and architecture. However, they are often overlooked compared to functional requirements, leading to potential issues such as rework, increased maintenance efforts, and inefficient resource utilization, impacting project cost and budget. To streamline software development, we propose a novel approach based on transfer learning methods to automate NFR identification and classification, aiming to reduce development time and resource consumption, ultimately leading to improved efficiency. We evaluate multiple state-of-the-art transfer learning models, including XLNet, BERT, Distil BERT, Distil Roberta, Electra-base, and Electra-small, for this purpose. Among them, XLNet demonstrates exceptional performance, achieving an impressive value of 0.91489 for Accuracy, Precision, Recall, and F1 Score. This research highlights the importance of considering non-functional requirements (NFRs) in software development and the negative consequences of neglecting them. It also emphasizes the benefits of using the XLNet tool to automate the identification and classification of NFRs. By using XLNet, we aim to make software development easier, optimize resource usage, and improve the overall quality of software systems."
The development of a competence framework for artificial intelligence professionals using probabilistic topic modelling,"Brauner, S; Murawski, M; Bick, M",10.1108/JEIM-09-2022-0341,2025,"PurposeThe current gap between the required and available artificial intelligence (AI) professionals poses significant challenges for organisations and academia. Organisations are challenged to identify and secure the appropriate AI competencies. Simultaneously, academia is challenged to design, offer and quickly scale academic programmes in line with industry needs and train new generations of AI professionals. Therefore, identifying and structuring AI competencies is necessary to effectively overcome the AI competence shortage.Design/methodology/approachA probabilistic topic model was applied to explore the AI competence categories empirically. The authors analysed 1159 AI-related online job ads published on LinkedIn.FindingsThe authors identified five predominant competence categories: (1) Data Science, (2) AI Software Development, (3) AI Product Development and Management, (4) AI Client Servicing, and (5) AI Research. These five competence categories were summarised under the developed AI competence framework.Originality/valueThe AI competence framework contributes to clarifying and structuring the diverse AI landscape. These findings have the potential to aid various stakeholders involved in the process of training, recruiting and selecting AI professionals. They may guide organisations in constructing a complementary portfolio of AI competencies by helping users match the right competence requirements with an organisation's needs and business objectives. Similarly, they can support academia in designing academic programmes aligned with industry needs. Furthermore, while focusing on AI, this study contributes to the research stream of information technology (IT) competencies."
Validity of Machine Learning in Assessing Large Texts Through Sustainability Indicators,"GarcÃ­a-Esparza, JA; Pardo, J; Altaba, P; Alberich, M",10.1007/s11205-023-03075-z,2023,"As machine learning becomes more widely used in policy and environmental impact settings, concerns about accuracy and fairness arise. These concerns have piqued the interest of researchers, who have advanced new approaches and theoretical insights to enhance data gathering, treatment and models' training. Nonetheless, few works have looked at the trade-offs between appropriateness and accuracy in indicator evaluation to comprehend how these constraints and approaches may better redound into policymaking and have a more significant impact across culture and sustainability matters for urban governance. This empirical study fulfils this void by researching indicators' accuracy and utilizing algorithmic models to test the benefits of large text-based analysis. Here we describe applied work in which we find affinity and occurrence in indicators trade-offs that result be significant in practice to evaluate large texts. In the study, objectivity and fairness are kept substantially without sacrificing accuracy, explicitly focusing on improving the processing of indicators to be truthfully assessed. This observation is robust when cross-referring indicators and unique words. The empirical results advance a novel form of large text analysis through machine intelligence and refute a widely held belief that artificial intelligence text processing necessitates either accepting a significant reduction in accuracy or fairness."
Studying the characteristics of AIOps projects on GitHub,"Aghili, R; Li, H; Khomh, F",10.1007/s10664-023-10382-z,2023,"Artificial Intelligence for IT Operations (AIOps) leverages AI approaches to handle the massive amount of data generated during the operations of software systems. Prior works have proposed various AIOps solutions to support different tasks in system operations and maintenance, such as anomaly detection. In this study, we conduct an in-depth analysis of open-source AIOps projects to understand the characteristics of AIOps in practice. We first carefully identify a set of AIOps projects from GitHub and analyze their repository metrics (e.g., the used programming languages). Then, we qualitatively examine the projects to understand their input data, analysis techniques, and goals. Finally, we assess the quality of these projects using different quality metrics, such as the number of bugs. To provide context, we also sample two sets of baseline projects from GitHub: a random sample of machine learning projects and a random sample of general-purposed projects. By comparing different metrics between our identified AIOps projects and these baselines, we derive meaningful insights. Our results reveal a recent and growing interest in AIOps solutions. However, the quality metrics indicate that AIOps projects suffer from more issues than our baseline projects. We also pinpoint the most common issues in AIOps approaches and discuss potential solutions to address these challenges. Our findings offer valuable guidance to researchers and practitioners, enabling them to comprehend the current state of AIOps practices and shed light on different ways of improving AIOps' weaker aspects. To the best of our knowledge, this work marks the first attempt to characterize open-source AIOps projects."
Exploring the Implications of OpenAI Codex on Education for Industry 4.0,"Brennan, RW; Lesage, J",10.1007/978-3-031-24291-5_20,2023,"In this paper we explore the potential benefits and drawbacks of the OpenAI Codex code completion model on teaching and learning in Industry 4.0 oriented undergraduate engineering programs. Two sets of test are performed with the model: the first investigates the Codex model's ability to generate code in Python's main programming paradigms, and the second focuses on a programming exercise typical for an undergraduate course in automation and controls. Our results show that, although Codex is very capable of assisting students with simple code completions, students will still need to have a strong intuition for software development based on Industry 4.0 standards to use this technology properly."
Validation of Artificial Intelligence Containing Products Across the Regulated Healthcare Industries,"Higgins, DC; Johner, C",10.1007/s43441-023-00530-4,2023,"PurposeThe introduction of artificial intelligence / machine learning (AI/ML) products to the regulated fields of pharmaceutical research and development (R&D) and drug manufacture, and medical devices (MD) and in vitro diagnostics (IVD), poses new regulatory problems: a lack of a common terminology and understanding leads to confusion, delays and product failures. Validation as a key step in product development, common to each of these sectors including computerized systems and AI/ML development, offers an opportune point of comparison for aligning people and processes for cross-sectoral product development.MethodsA comparative approach, built upon workshops and a subsequent written sequence of exchanges, is summarized in a look-up table suitable for mixed-teams work.Results1. A bottom-up, definitions led, approach which leads to a distinction between broad vs narrow validation, and their relationship to regulatory regimes. 2. Common basis introduction to the primary methodologies for software validation, including AI-containing software validation. 3. Pharmaceutical drug development and MD/IVD-specific perspectives on compliant AI software development, as a basis for collaboration.ConclusionsAlignment of the terms and methodologies used in validation of software products containing artificial intelligence/machine learning (AI/ML) components across the regulated industries of human health is a vital first step in streamlining processes and improving workflows."
Generating multiple conceptual models from behavior-driven development scenarios,"Gupta, A; Poels, G; Bera, P",10.1016/j.datak.2023.102141,2023,"Researchers have proposed that generating conceptual models automatically from user stories might be useful for agile software development. It is, however, unclear from the state-of-the-art what a consistent and complementary set of models to generate is, how these models can be generated such that relationships and dependencies in a set of related user stories are unveiled, and why these models are useful in agile software development projects. In this paper, we address these questions through a Design Science research study. First, we define four stylized versions of Unified Modeling Language (UML) diagrams (i.e., use case diagram, class diagram, activity diagram, state machine diagram) that will be the target of the model generation. Although these stylized UML diagrams have a reduced abstract syntax, they offer different perspectives on the software system in focus with potential usefulness for requirements and software engineering. Second, we develop an automated model generation approach based on different design artifacts including a Natural Language Processing (NLP) tool that implements our approach. Key to our solution is the use of the Behavior-Driven Development (BDD) scenario template to document user stories. Using an example set of BDD scenarios as source of the model generation, we demonstrate the feasibility of our approach via the NLP tool that implements our approach. Third, we conduct an empirical study with experts in agile software development involving the researcher-guided interactive use of our tool to explore the use of the generated models. This study shows the perceived usefulness of the models that our tool can generate and identifies different uses and benefits of the models for requirements analysis, system design, software implementation, and testing in projects that employ agile methods."
What Matters in Hiring Professionals for Global Software Development? A SLR and NLP Criteria Clustering,"dos Santos, EA; de Souza, DGB; da Silva, CES",10.1109/TEM.2023.3279769,2023,"Globalization stimulated a new era of Global Software Development (GSD), followed by the gig economy (GE) phenomenon, which jointly caused considerable transformations in software development markets, mainly after the recent supply chain disruptions. The cultural and geographic barriers have compelled numerous organizations to devise comprehensive digital technologies to overcome this situation. Likewise, the rising unemployment rates led the workforce into short-term contracts or to the on-demand market known as the gig economy. Together with the enhancement in global software development, the organizations found a direction to restore their activities. However, when organizations are immersed in fast-paced environments, selecting skilled professionals is difficult and risky, especially with a lack of qualified professionals. This article identifies the criteria for hiring professionals in the GSD or GE context and proposes a novel approach to clustering them. To do so, we collected the criteria from a broad subject through a systematic literature review, then applied natural language processing with the SBERT algorithm to get the sentence embeddings. Further, we cluster the criteria by applying the k-means algorithm. After that, we innovatively and responsively grouped the clusters formed by repeating the SBERT and k-means algorithms and created its mind map. Our findings disclosed 319 criteria and 6 cluster groups comprising a mind map hierarchical structure. Consequently, these outcomes have pedagogical implications to assist specialists from education institutions in designing new course domains. Such as, it can be helpful to practitioners to assist in hiring professional processes in the GSD or GE context."
A Multidimensional Model of the New Work Environment in the Digital Age to Increase a Company's Performance and Competitiveness,"Rozman, M; Oreski, D; Tominc, P",10.1109/ACCESS.2023.3257104,2023,"The purpose of the paper is to develop a multidimensional model of the new work environment in the digital age to increase a company's performance and competitiveness in VUCA (volatility, uncertainty, complexity, and ambiguity) business environment. The multidimensional model covers the implementation of an agile work environment through the prism of using artificial intelligence technology to increase company's performance and competitiveness. Researched determined multidimensional aspects for successful implementation of work environment in the digital age are, therefore 1) drivers for shifting towards agility, 2) implementation of agile leadership, 3) implementation of an agile work environment, 4) implementation of AI technology in work environment, 5) company's performance, 6) competitiveness. The main survey involved randomly selected 473 medium-sized and large companies in Slovenia. Structural equation modelling was used for statistical data analysis. The results show that drivers for shifting towards agility have a positive effect on implementation of agile leadership. Also, results show that implementation of agile leadership and implementation of AI technology in work environment have a positive effect on implementation of an agile work environment. Moreover, results show that implementation of an agile work environment has positive effect on company's performance and competitiveness. The paper highlights the important multidimensional aspects of the successful implementation of an agile work environment to increase the company's performance and competitiveness. Also, our results will contribute to the proper implementation of the work environment in the digital age and give owners or top managers a broad insight into the various aspects that must be considered in their business governance in today's rapidly changing business environment."
Nature-Based Prediction Model of Bug Reports Based on Ensemble Machine Learning Model,"Alsaedi, SA; Noaman, AY; Gad-Elrab, AAA; Eassa, FE",10.1109/ACCESS.2023.3288156,2023,"In software development systems, the maintenance process of software systems attracted the attention of researchers due to its importance in fixing the defects discovered in the software testing by using bug reports (BRs) which include detailed information like description, status, reporter, assignee, priority, and severity of the bug and other information. The main problem in this process is how to analyze these BRs to discover all defects in the system, which is a tedious and time-consuming task if done manually because the number of BRs increases dramatically. Thus, the automated solution is the best. Most of the current research focuses on automating this process from different aspects, such as detecting the severity or priority of the bug. However, they did not consider the nature of the bug, which is a multi-class classification problem. This paper solves this problem by proposing a new prediction model to analyze BRs and predict the nature of the bug. The proposed model constructs an ensemble machine learning algorithm using natural language processing (NLP) and machine learning techniques. We simulate the proposed model by using a publicly available dataset for two online software bug repositories (Mozilla and Eclipse), which includes six classes: Program Anomaly, GUI, Network or Security, Configuration, Performance, and Test-Code. The simulation results show that the proposed model can achieve better accuracy than most existing models, namely, 90.42% without text augmentation and 96.72% with text augmentation."
Developing an Advanced Software Requirements Classification Model Using BERT: An Empirical Evaluation Study on Newly Generated Turkish Data,"Yucalar, F",10.3390/app132011127,2023,"Requirements Engineering (RE) is an important step in the whole software development lifecycle. The problem in RE is to determine the class of the software requirements as functional (FR) and non-functional (NFR). Proper and early identification of these requirements is vital for the entire development cycle. On the other hand, manual identification of these classes is a timewaster, and it needs to be automated. Methodically, machine learning (ML) approaches are applied to address this problem. In this study, twenty ML algorithms, such as Naive Bayes, Rotation Forests, Convolutional Neural Networks, and transformers such as BERT, were used to predict FR and NFR. Any ML algorithm requires a dataset for training. For this goal, we generated a unique Turkish dataset having collected the requirements from real-world software projects with 4600 samples. The generated Turkish dataset was used to assess the performance of the three groups of ML algorithms in terms of F-score and related statistical metrics. In particular, out of 20 ML algorithms, BERTurk was found to be the most successful algorithm for discriminating FR and NFR in terms of a 95% F-score metric. From the FR and NFR identification problem point of view, transformer algorithms show significantly better performances."
Evaluating Privacy Questions From Stack Overflow: Can ChatGPT Compete?,"Defile, Z; Radel, S; Godinez, J; Engstrom, G; Brucker, T; Young, K; Ghanavati, S",10.1109/REW57809.2023.00048,2023,"Stack Overflow and other similar forums are commonly used by developers to seek answers for their software development as well as privacy-related concerns. Recently, ChatGPT has been used as an alternative to generate code or produce responses to developers' questions. In Ibis paper, we aim to understand developers' privacy challenges by evaluating the types of privacy-related questions asked on Stack Overflow. We then conduct a comparative analysis between the accepted responses given by Stack Overflow users and the responses produced by ChatGPT for those extracted questions to identify if ChatGPT could serve as a viable alternative. Our results show that most privacy-related questions are related to choice/consent, aggregation, and identification. Furthermore, our findings illustrate that ChatGPT generates similarly correct responses for about 56% of questions while for the rest of the responses, the answers from Stack Overflow are slightly more accurate than ChatGPT."
Classification of Bugs in Cloud Computing Applications Using Machine Learning Techniques,"Tabassum, N; Namoun, A; Alyas, T; Tufail, A; Taqi, M; Kim, KH",10.3390/app13052880,2023,"In software development, the main problem is recognizing the security-oriented issues within the reported bugs due to their unacceptable failure rate to provide satisfactory reliability on customer and software datasets. The misclassification of bug reports has a direct impact on the effectiveness of the bug prediction model. The misclassification issue surely compromises the accuracy of the system. Manually reviewing bug reports is necessary to solve this problem, but doing so takes a lot of time and is tiresome for developers and testers. This paper proposes a novel hybrid approach based on natural language processing (NLP) and machine learning. To address these issues, the intended outcomes are multi-class supervised classification and bug prioritization using supervised classifiers. After being collected, the dataset was prepared for vectorization, subjected to exploratory data analysis, and preprocessed. The feature extraction and selection methods used for a bag of words are TF-IDF and word2vec. Machine learning models are created after the dataset has undergone a full transformation. This study proposes, develops, and assesses four classifiers: multinomial Naive Bayes, decision tree, logistic regression, and random forest. The hyper-parameters of the models are tuned, and it is concluded that random forest outperformed with a 91.73% test and 100% training accuracy. The SMOTE technique was used to balance the highly imbalanced dataset, which was initially created for the justified classification. The comparison between balanced and imbalanced dataset models clearly showed the importance of the balanced dataset in classification as it outperformed in all experiments."
New Game Artificial Intelligence Tools for Virtual Mine on Unreal Engine,"Abu-Abed, F; Zhironkin, S",10.3390/app13106339,2023,"Currently, the gamification of virtual reality for training miners, especially for emergencies, and designing the extraction of minerals in difficult technological conditions has been embodied in the Virtual Mine software and hardware. From a software development point of view, Virtual Mine is indistinguishable from other virtual reality games, and this offers a chance to use the potential of rapidly developing game software in mining, including engines, 3D modeling tools, audio editors, etc., to solve a wide range of game development tasks. The chosen direction will optimize the work of developers by providing a tool for developing game artificial intelligence to solve problems that require implementing the behavior of game agents without using a rigidly defined choice of scenarios or chains of these scenarios. The aim of the work is to expand the possibilities of working with game artificial intelligence on the Unreal Engine game engine to make it more functional. As a result, a tool has been obtained that can be used to optimize the time and improve the quality of the development of game artificial intelligence for Virtual Mine using flexible development approaches. The asset editor was developed, application modes and their working tabs were defined, and a graphical node system for the behavioral graph editor was created. A system for executing a behavioral graph is given; algorithms for its operation and features for executing nodes of a behavioral graph are presented."
TABASCO: A transformer based contextualization toolkit,"Moharil, A; Sharma, A",10.1016/j.scico.2023.102994,2023,"Ambiguity means that a single reader can interpret the natural language (NL) software requirement in more than one way or that multiple readers come to different interpretations. Ambiguous NL software requirements may result in the production of poor quality software artifacts in later stages of software development life cycle. In the literature, several approaches have been proposed to identify multiple meanings of commonly used terms in different application domains, i.e., cross-domain ambiguities. Unfortunately, none of these approaches are able to identify different contexts in which a term has been used within the requirements document of a single application domain or in a multidisciplinary project document. We call this type of ambiguity as intra-domain ambiguity. We have designed and developed TABASCO, a tool for detecting and identifying such ambiguities present in the software requirements and other project-related documents. TABASCO focuses on detecting nouns which have been used in different contexts by computing contextual embeddings represented as real-valued vector for every instance of a candidate noun present in the document using the Bidirectional Encoder Representations from Transformers (BERT) as a language model. In the next step, K-means clustering algorithm with cosine similarity metric is applied which allows creating multiple clusters for a candidate noun such that each cluster contains all those instances which have been used in a similar context. TABASCO provides an intuitive graphical user interface which can be used to generate a summary report and a detailed report for each target noun providing the details of different contexts in which this noun has been used in the project. The detailed report also presents the most similar words for every occurrence of the target noun and some example sentences from the corpus to show the context-specific meaning. Our demonstrative experiments using two case studies show that TABASCO can be very useful for identifying intra-domain ambiguities in software requirements and other project-related documents written using NL text."
Deep-learning predicted PET can be subtracted from the true clinical fluorodeoxyglucose PET co-registered to MRI to identify the epileptogenic zone in focal epilepsy,"Flaus, A; Jung, JL; Ostrowky-Coste, K; Rheims, S; Guenot, M; Bouvard, S; Janier, M; Yaakub, SN; Lartizien, C; Costes, N; Hammers, A",10.1002/epi4.12820,2023,"Objective: Normal interictal [F-18]FDG-PET can be predicted from the corresponding T1w MRI with Generative Adversarial Networks (GANs). A technique we call SIPCOM (Subtraction Interictal PET Co-registered to MRI) can then be used to compare epilepsy patients ' predicted and clinical PET. We assessed the ability of SIPCOM to identify the Resection Zone (RZ) in patients with drug-resistant epilepsy (DRE) with reference to visual and statistical parametric mapping (SPM) analysis. Methods: Patients with complete presurgical work-up and subsequent SEEG and cortectomy were included. RZ localisation, the reference region, was assigned to one of eighteen anatomical brain regions. SIPCOM was implementedto MRI was visually assessed by two trained readers, and a standard SPM analysis was performed. Results: Twenty patients aged 17-50 (32 +/- 7.8) years were included, 14 (70%) with temporal lobe epilepsy (TLE). Eight (40%) were MRI-negative. After surgery, 14 patients (70%) had a good outcome (Engel I-II). RZ localisation rate was 60% with SIPCOM vs 35% using SPM (P = 0.015) and vs 85% using visual analysis (P = 0.54). Results were similar for Engel I-II patients, the RZ localisation rate was 64% with SIPCOM vs 36% with SPM. With SIPCOM localisation was correct in 67% in MRI-positive vs 50% in MRI-negative patients, and 64% in TLE vs 43% in extra-TLE. The average number of false-positive clusters was 2.2 +/- 1.3 using SIPCOM vs 2.3 +/- 3.1 using SPM. All RZs localized with SPM were correctly localized with SIPCOM. In one case, PET and MRI were visually reported as negative, but both SIPCOM and SPM localized the RZ. Significance: SIPCOM performed better than the reference computer-assisted method (SPM) for RZ detection in a group of operated DRE patients. SIPCOM ' s impact on epilepsy management needs to be prospectively validated."
Learning to Predict User-Defined Types,"Jesse, K; Devanbu, PT; Sawant, A",10.1109/TSE.2022.3178945,2023,"TypeScript is a widely adopted gradual typed language where developers can optionally type variables, functions, parameters and more. Probabilistic type inference approaches with ML (machine learning) work well especially for commonly occurring types such as boolean, number, and string. TypeScript permits a wide range of types including developer defined class names and type interfaces. These developer defined types, termed user-defined types, can be written within the realm of language naming conventions. The set of user-defined types is boundless and existing bounded type guessing approaches are an imperfect solution. Existing works either under perform in user-defined types or ignore user-defined types altogether. This work leverages a BERT-style pre-trained model, with multi-task learning objectives, to learn how to type user-defined classes and interfaces. Thus we present DiverseTyper, a solution that explores the diverse set of user-defined types by uniquely aligning classes and interfaces declarations to the places in which they are used. DiverseTyper surpasses all existing works including those that model user-defined types."
Pin or Fuse? Exploiting Scratchpad Memory to Reduce Off-Chip Data Transfer in DNN Accelerators,"Jeong, HJ; Yeo, J; Bahk, C; Park, J",10.1145/3579990.3580017,2023,"Growing interests in on-device AI have led to the proliferation of accelerators dedicated to neural network inference. Most ASIC accelerators are equipped with compiler-controlled scratchpad memory (SPM) used as a last-level cache to reduce the number of accesses to off-chip memory. A widely-used strategy for utilizing SPM is fused-layer execution, which divides a DNN model into groups of layers and forwards the intermediate results within each group without eviction to the off-chip memory. However, layer fusion has an inherent limitation that the fusion of consecutive layers increases the amount of computations, leading to sub-optimal performance. This paper introduces a new dimension to SPM usage, which temporarily pins a feature map on SPM. Pinning reduces off-chip transfer without computation increase, but it is not applicable to all feature maps due to limited SPM size. We find that superior performance can be achieved by combination of pinning and fusion in MobileNet. Based on this observation, we propose a model-level optimization method that jointly applies pinning and fusion to minimize inference latency under memory constraints. Scheduling and allocation schemes are presented for automatic generation of optimized codes. Evaluation on the commercial AI accelerator shows that the proposed method reduces off-chip transfer of feature maps by 50% and improves inference latency by 15% on average without additional hardware, compared to the state-of-the-art fusion approach."
An Empirical Study of License Conflict in Free and Open Source Software,"Cui, X; Wu, JZ; Wu, YJ; Wang, X; Luo, TY; Qu, S; Ling, X; Yang, MT",10.1109/ICSE-SEIP58684.2023.00050,2023,"Free and Open Source Software (FOSS) has become the fundamental infrastructure of mainstream software projects. FOSS is subject to various legal terms and restrictions, depending on the type of open source license in force. Hence it is important to remain compliant with the FOSS license terms. Identifying the licenses that provide FOSS and understanding the terms of those licenses is not easy, especially when dealing with a large amount of reuse that is common in modern software development. Since reused software is often large, automated license analysis is needed to address these issues and support users in license compliant reuse of FOSS. However, existing license assessment tools can only identify the name and quantity of licenses embedded in software and thus cannot identify whether the licenses are being used safely and correctly. Moreover, they cannot provide a comprehensive analysis of the compatibility and potential risk that come with the term conflicts. In this paper, we propose DIKE, an automated tool that can perform license detection and conflict analysis for FOSS. First, DIKE extracts 12 terms under 3,256 unique open source licenses by manual analysis and Natural Language Processing (NLP) and constructs a license knowledge base containing the responsibilities of the terms. Second, DIKE scans all licenses from the code snippet for the input software and outputs the scan results in a tree structure. Third, the scan results match the license knowledge base to detect license conflicts from terms and conditions. DIKE designs two solutions for software with license conflicts: license replacement and code replacement. To demonstrate the effectiveness of DIKE, we first evaluate with the term extraction and responsibility classification, and the results show that their F1-scores reach 0.816 and 0.948, respectively. In addition, we conduct a measurement study of 16,341 popular projects from GitHub based on our proposed DIKE to explore the conflict of license usage in FOSS. The results show that 1,787 open source licenses are used in the project, and 27.2% of licenses conflict. Our new findings suggest that conflicts are prevalent in FOSS, warning the open source community about intellectual property risks."
A step toward building a unified framework for managing AI bias,"Rana, SA; Azizul, ZH; Awan, AA",10.7717/peerj-cs.1630,2023,"Integrating artificial intelligence (AI) has transformed living standards. However, AI's efforts are being thwarted by concerns about the rise of biases and unfairness. The problem advocates strongly for a strategy for tackling potential biases. This article thoroughly evaluates existing knowledge to enhance fairness management, which will serve as a foundation for creating a unified framework to address any bias and its subsequent mitigation method throughout the AI development pipeline. We map the software development life cycle (SDLC), machine learning life cycle (MLLC) and cross industry standard process for data mining (CRISP-DM) together to have a general understanding of how phases in these development processes are related to each other. The map should benefit researchers from multiple technical backgrounds. Biases are categorised into three distinct classes; pre-existing, technical and emergent bias, and subsequently, three mitigation strategies; conceptual, empirical and technical, along with fairness management approaches; fairness sampling, learning and certification. The recommended practices for debias and overcoming challenges encountered further set directions for successfully establishing a unified framework."
Chat GPT-Based Design-Time DevSecOps,"Petrovic, N",10.1109/ICEST58410.2023.10187247,2023,"Adoption of DevOps-enabled software development has become one of constituent processes within the workflow behind competitive organizations in any area of industry. Its main purpose consists of automation when it comes to steps of development, testing and deployment, aiming to achieve continuous integration and delivery of products and services. On the other side, these highly automatized steps are prone to security flaws and various types of vulnerabilities, which could have fatal consequences, especially in critical domains of usage, such as sensitive usage scenarios related to public infrastructure and healthcare. For that reason, the so-called DevSecOps has emerged, whose main scope are security concerns in DevOps-based automated workflows. In this paper, Python API of novel ChatGPT conversational agent service is leveraged for static code analysis of Infrastructure as Code (IaC) scripts. Moreover, we perform aggregation and post-processing of results returned by ChatGPT, making them more useful when it comes to end-users, such as DevOps engineers and system administrators. When it comes to evaluation, we focus on Ansible and Terraform IaC script case studies."
Software Engineering Using Autonomous Agents: Are We There Yet?,"Suri, S; Das, SN; Singi, K; Dey, K; Sharma, VS; Kaulgud, V",10.1109/ASE56229.2023.00174,2023,"Autonomous agents equipped with Large Language Models (LLMs) are rapidly gaining prominence as a revolutionary technology within the realm of Software Engineering. These intelligent and autonomous systems demonstrate the capacity to perform tasks and make independent decisions, leveraging their intrinsic reasoning and decision-making abilities. This paper delves into the current state of autonomous agents, their capabilities, challenges, and opportunities in Software Engineering practices. By employing different prompts (with or without context), we conclude the advantages of context-rich prompts for autonomous agents. Prompts with context enhance user requirement understanding, avoiding irrelevant details that could hinder task comprehension and degrade model performance, particularly when dealing with complex frameworks such as Spring Boot, Django, Flask, etc. This exploration is conducted using Auto-GPT (v0.3.0), an open-source application powered by GPT-3.5 and GPT-4 which intelligently connects the thoughts of Large Language Models (LLMs) to independently accomplish the assigned goals or tasks."
An Extended Survey Concerning the Significance of Artificial Intelligence and Machine Learning Techniques for Bug Triage and Management,"Bocu, R; Baicoianu, A; Kerestely, A",10.1109/ACCESS.2023.3329732,2023,"Bug reports are generated in large numbers during the software development processes in the software industry. The manual processing of these issues is usually time consuming and prone to errors, consequently delaying the entire software development process. Thus, a properly designed bug triage and management process implies that essential operations, such as duplicate detection, bug assignments to proper developers, and determination of the importance level, are sustained by efficient algorithmic models and implementation approaches. Designing and implementing a proper bug triage and management process becomes an essential scientific research topic, as it may significantly optimize the software development and business process in the information technology industry. Consequently, this paper thoroughly surveys the most significant related scientific contributions analytically and constructively, distinguishing it from similar survey papers. The paper proposes optimal algorithmic and software solutions for particular real-world use cases that are analyzed. It concludes by presenting the most important open research questions and challenges. Additionally, the paper provides a valuable scientific literature survey for any researcher or practitioner in software bug triage and management systems based on artificial intelligence and machine learning techniques."
"A Software Requirements Ecosystem: Linking Forum, Issue Tracker, and FAQs for Requirements Management","Tizard, J; Devine, P; Wang, HC; Blincoe, K",10.1109/TSE.2022.3219458,2023,"User feedback is an important resource in modern software development, often containing requirements that help address user concerns and desires for a software product. The feedback in online channels is a recent focus for software engineering researchers, with multiple studies proposing automatic analysis tools. In this work, we investigate the product forums of two large open source software projects. Through a quantitative analysis, we show that forum feedback is often manually linked to related issue tracker entries and product documentation. By linking feedback to their existing documentation, development teams enhance their understanding of known issues, and direct their users to known solutions. We discuss how the links between forum, issue tracker, and product documentation form a requirements ecosystem that has not been identified in the previous literature. We apply state-of-the-art deep-learning to automatically match forum posts with related issue tracker entries. Our approach identifies requirement matches with a mean average precision of 58.9% and hit ratio of 82.2%. Additionally, we apply deep-learning using an innovative clustering technique, achieving promising performance when matching forum posts to related product documentation. We discuss the possible applications of these automated techniques to support the flow of requirements between forum, issue tracker, and product documentation."
Slice-Based Code Change Representation Learning,"Zhang, FY; Chen, BH; Zhao, YF; Peng, X",10.1109/SANER56733.2023.00038,2023,"Code changes are at the very core of software development and maintenance. Deep learning techniques have been used to build a model from a massive number of code changes to solve software engineering tasks, e.g., commit message generation and bug-fix commit identification. However, existing code change representation learning approaches represent code change as lexical tokens or syntactical AST (abstract syntax tree) paths, limiting the capability to learn semantics of code changes. Besides, they mostly do not consider noisy or tangled code change, hurting the accuracy of solved tasks. To address the above problems, we first propose a slice-based code change representation approach which considers data and control dependencies between changed code and unchanged code. Then, we propose a pre-trained sparse Transformer model, named CCS2VEC, to learn code change representations with three pre-training tasks. Our experiments by fine-tuning our pre-trained model on three downstream tasks have demonstrated the improvement of CCS2VEC over the state-of-the-art CC2VEC."
"A Qualitative Study on Artificial Intelligence and Its Impact on the Project Schedule, Cost and Risk Management Knowledge Areas as Presented in PMBOKÂ®","Fridgeirsson, TV; Ingason, HT; Jonasson, HI; Gunnarsdottir, H",10.3390/app131911081,2023,"The aim of this paper is to study the main areas in which artificial intelligence (AI) will impact the field of project management in relation to cost, risk and scheduling. The research model was based on a previous study of the ten project management knowledge areas presented in PMI's PMBOK 6th edition, where project schedule, cost and risk management knowledge areas were identified as being the ones most likely to be affected by the development of AI. A group of graduates from a Master of Project Management program were assessed in an online questionnaire, reflecting the PMBOK's elements of best practices and how AI will affect the project management profession in the future. Different elements of the three knowledge areas were considered to be affected more by AI than others. The schedule baseline is the element believed to be affected the most out of the project schedule management elements. For project cost management, the estimation of resource costs is believed to be affected the most. In the case of project risk management, the application of AI will have the strongest impact on the probability and impact formats."
A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT,"White, J; Fu, Q; Hays, S; Sandborn, M; Olea, C; Gilbert, H; Elnashar, A; Spencer-Smith, J; Schmidt, DC",,2023,"Prompt engineering is becoming a critical skill for software developers by facilitating enhanced interactions with conversational large language models (LLMs), such as ChatGPT, Claude, and Gemini. This emerging discipline focuses on crafting prompts, which are instructions that guide LLMs in generating precise outputs, automating tasks, and ensuring adherence to specific qualitative and quantitative standards. Prompts are also a form of natural language programming that tailor the dialogue between users and LLMs, optimizing input, output, and interaction dynamics for many computational tasks, such as developing software, analyzing documents, and/or addressing cyber vulnerabilities. This paper introduces a comprehensive catalog of prompt engineering techniques-structured as a collection of patterns-aimed at addressing common challenges encountered when integrating LLMs into the software development lifecycle. These prompt patterns serve as an effective means for knowledge transfer, similar to software patterns. In particular, they provide reusable solutions to common problems faced in particular contexts, such as output generation and interaction when conversing with LLMs in the domain of software-reliant systems. This paper provides three contributions to research on-and the practice of-prompt engineering for applying LLMs to aid users performing computational tasks. First, it establishes a framework for documenting and deploying prompt patterns across various domains, focusing on enhancing LLM utility in software development endeavors. Second, it curates a catalog of prompt patterns, validated through successful application in refining LLM interactions and outputs. Third, it explores the synergistic potential of creating more complex prompts by combining multiple prompt patterns."
An empirical study of automated privacy requirements classification in issue reports,"Sangaroonsilp, P; Choetkiertikul, M; Dam, HK; Ghose, A",10.1007/s10515-023-00387-9,2023,"The recent advent of data protection laws and regulations has emerged to protect privacy and personal information of individuals. As the cases of privacy breaches and vulnerabilities are rapidly increasing, people are aware and more concerned about their privacy. These bring a significant attention to software development teams to address privacy concerns in developing software applications. As today's software development adopts an agile, issue-driven approach, issues in an issue tracking system become a centralised pool that gathers new requirements, requests for modification and all the tasks of the software project. Hence, establishing an alignment between those issues and privacy requirements is an important step in developing privacy-aware software systems. This alignment also facilitates privacy compliance checking which may be required as an underlying part of regulations for organisations. However, manually establishing those alignments is labour intensive and time consuming. In this paper, we explore a wide range of machine learning and natural language processing techniques which can automatically classify privacy requirements in issue reports. We employ six popular techniques namely Bag-of-Words (BoW), N-gram Inverse Document Frequency (N-gram IDF), Term Frequency-Inverse Document Frequency (TF-IDF), Word2Vec, Convolutional Neural Network (CNN) and Bidirectional Encoder Representations from Transformers (BERT) to perform the classification on privacy-related issue reports in Google Chrome and Moodle projects. The evaluation showed that BoW, N-gram IDF, TF-IDF and Word2Vec techniques are suitable for classifying privacy requirements in those issue reports. In addition, N-gram IDF is the best performer in both projects."
AI And Energy Efficiency,"Omar, R",10.1109/ICSA-C57050.2023.00040,2023,"Remarkable progress has been reported in the deployment of artificial intelligence and machine learning applications in a broad range of capabilities such as healthcare, game playing, image recognition, and machine translation. At the same time, the growing usage of AI systems demand for more energy contributed to increasing CO2 emissions. Most of the studies are focusing on increased accuracy rather than energy efficiency of these models. In this research, we are investigating the energy cost associated with AI models, and techniques to help increase the energy efficiency of AI models, and conduct empirical experiments to validate the goodness of the techniques identified. The proposed techniques will help the AI developers, engineers, and community achieve increased energy efficiency of AI models. In addition, we also identify an approach that will allow the software development community to create an energy-efficient configuration of software through the use of machine learning"
Overview of transparency and inspectability mechanisms to achieve accountability of artificial intelligence systems,"Hauer, MP; Krafft, TD; Zweig, K",10.1017/dap.2023.30,2023,"Several governmental organizations all over the world aim for algorithmic accountability of artificial intelligence systems. However, there are few specific proposals on how exactly to achieve it. This article provides an extensive overview of possible transparency and inspectability mechanisms that contribute to accountability for the technical components of an algorithmic decision-making system. Following the different phases of a generic software development process, we identify and discuss several such mechanisms. For each of them, we give an estimate of the cost with respect to time and money that might be associated with that measure."
False Alarm Reduction Method for Weakness Static Analysis Using BERT Model,"Nguyen, DH; Seo, A; Nnamdi, NP; Son, Y",10.3390/app13063502,2023,"In the era of the fourth Industrial Revolution, software has recently been applied in many fields. As the size and complexity of software increase, security attack problems continue to arise owing to potential software defects, resulting in significant social losses. To reduce software defects, a secure software development life cycle (SDLC) should be systematically developed and managed. In particular, a software weakness analyzer that uses a static analysis tool to check software weaknesses at the time of development is a very effective tool for solving software weaknesses. However, because numerous false alarms can be reported even when they are not real weaknesses, programmers and reviewers must review them, resulting in a decrease in the productivity of development. In this study, we present a system that uses the BERT model to determine the reliability of the weakness analysis results generated by the static analysis tool and to reduce false alarms by reclassifying the derived results into a decision tree model. Thus, it is possible to maintain the advantages of static analysis tools and increase productivity by reducing the cost of program development and the review process."
AI-Assisted Security: A Step towards Reimagining Software Development for a Safer Future,"Shi, Y; Sakib, N; Shahriar, H; Lo, D; Chi, HM; Qian, K",10.1109/COMPSAC57700.2023.00142,2023,"The security threats to software are increasing dramatically, and software security is non-negotiable in this age of information technology. AI-based tools can make code suggestions, improve developer productivity, and block insecure coding patterns for secure software development. In this paper, we analyze how GitHub Copilot provides AI assistants to erase code vulnerabilities and study how we should use AI assistants for secure software development."
Defining Virtual Consumerism Through Content and Sentiment Analyses,"Tunca, S; Wilk, V; Sezen, B",10.1089/cyber.2022.0079,2023,"This study set out to better understand virtual consumerism (VC) by applying natural language processing (NLP) methods for sentiment and content analyses. A total of 318 articles related to VC were identified on theguardian.com Web site and analyzed by text mining methodology. A thematic, content analysis using the Leximancer program was performed to explore VC as a concept, and its related concepts and concept associations. For the purposes of deep-dive insights, further content and sentiment analyses were performed with MonkeyLearn and valence aware dictionary for sentiment reasoning. This triangulation in methodology enabled a comprehensive unstructured qualitative data analysis. The study identified key themes that characterize and define VC. It uncovered that, although there is predominantly positive sentiment toward VC reported in The Guardian online articles, negative sentiment also exists, presenting challenges for the industry to maneuver. The findings reveal that in the context of VC, a virtual experience is also a social experience in a virtual space, which is becoming and evolving. There are certain industries and sectors that are embracing VC, such as marketing, advertising and public relations, software development/IT, art/design, and entertainment, as well as science/technology. Some sectors and industries are experiencing challenges, such as security/law enforcement and medical, and hence display negative sentiment toward VC. Overall, this study presents a working definition of VC, a synopsis of the state of VC, and highlights areas for potential research to further our understanding of this phenomenon. It contributes to an improved understanding of VC for the industry and academia, and provides impetus for future studies focused on the emergent VC-relevant conceptual relationships."
Impact of code smells on software development environments: a study based on ENTROPY-CODAS method,"Anand, A; Gupta, P; Tamura, Y; Papic, L",10.1108/IJQRM-08-2022-0254,2024,"PurposeThe relationship between the various existing smell taxonomies and the smell impacting factors has been established. The ideology is to identify the most critical smell influencing factors in the vicinity of various software development environments.Design/methodology/approachTo fulfill the said task, the utilization of the amalgamation of two multicriteria decision-making techniques, namely, Entropy method and CODAS method, is presented.FindingsThrough this article, the most critical smell impacting criteria with respect to the smell taxonomies is identified. Furthermore, the behaviour of 4 software development principles was then analysed, and their working state has been successfully assessed.Originality/valueThe ideology to study design-related smells in the software system has been studied by a lot of researchers. Some of them have worked upon their detection and the corresponding refactoration process with the help of several algorithms like machine learning and artificial intelligence. But how and to what extent these design-related smells impact the software development environment has remained out of the limelight till now. Through this article, this research gap has been identified, and an attempt to fill it has been made."
"Automated, interactive, and traceable domain modelling empowered by artificial intelligence","Saini, R; Mussbacher, G; Guo, JLC; Kienzle, J",10.1007/s10270-021-00942-6,2022,"Model-Based Software Engineering provides various modelling formalisms for capturing the structural, behavioral, configuration, and intentional aspects of software systems. One of the most widely used kinds of models-domain models-are used during requirements analysis or the early stages of design to capture the domain concepts and relationships in the form of class diagrams. Modellers perform domain modelling to transform the problem descriptions that express informal requirements in natural language to domain models, which are more concise and analyzable. However, this manual practice of domain modelling is laborious and time-consuming. Existing approaches, which aim to assist modellers by automating or semi-automating the construction of domain models from problem descriptions, fail to address three non-trivial aspects of automated domain modelling. First, automatically extracted domain models from existing approaches are not accurate enough to be used directly or with minor modifications for software development or teaching purposes. Second, existing approaches do not support modeller-system interactions beyond providing recommendations. Finally, existing approaches do not facilitate the modellers to learn the rationale behind the modelling decisions taken by an extractor system. Therefore, in this paper, we extend our previous work to facilitate bot-modeller interactions. We propose an algorithm to discover alternative configurations during bot-modeller interactions. Our bot uses this algorithm to find alternative configurations and then present these configurations in the form of suggestions to modellers. Our bot then updates the domain model in response to the acceptance of these suggestions by a modeller. Furthermore, we evaluate the bot for its effectiveness and performance for the test problem descriptions. Our bot achieves median F1 scores of 86%, 91%, and 90% in the Found Configurations, Offered Suggestions, and Updated Domain Models categories, respectively. We also show that the median time taken by our bot to find alternative configurations is 55.5ms for the problem descriptions which are similar to the test problem descriptions in terms of model size and complexity. Finally, we conduct a pilot user study to assess the benefits and limitations of our bot and present the lessons learned from our study in preparation for a large-scale user study."
Compiling Requirements from Models for Early Phase Scope Estimation in Agile Software Development Projects,"Bisikirskiene, L; Ceponiene, L; Jurgelaitis, M; Ablonskis, L; Grigonyte, E",10.3390/app132212353,2023,"Inadequate early scope estimation is a common problem in software projects, leading to failures in meeting project requirements. Agile projects usually do not concentrate on a comprehensive requirements analysis and specification before the start of the project, making scope assessment difficult. This paper presents the methodology for facilitating a more accurate early estimation of project scope, based on requirements information gathered in various forms (requirements models and textual descriptions) during the requirements workshop. The requirements from different sources are compiled into one list and reconciled, since they are prepared by a number of participants in the requirements workshop using different notations (UML diagrams, SysML models, Story map) and may have differences in the vocabulary. Reconciliation encompasses the unification of vocabulary, as well as the identification and the removal of overlaps in requirements. The final list of requirements is used to estimate the scope of the project in story points. The estimate can be presented to the client and used as a basis for the project contract. A case study on the application of the proposed methodology is presented, using the animal shelter information system as a development project. It demonstrates that the methodology is viable and can facilitate the gathering of a more extensive set of requirements, thus ensuring a more detailed scope estimation."
Exploring the Use of Natural Language Processing Techniques for Enhancing Genetic Improvement,"Krauss, O",10.1109/GI59320.2023.00014,2023,"We explore the potential of using large-scale Natural Language Processing (NLP) models, such as GPT-3, for enhancing genetic improvement in software development. These models have previously been used to automatically find bugs, or improve software. We propose utilizing these models as a novel mutator, as well as for explaining the patches generated by genetic improvement algorithms. Our initial findings indicate promising results, but further research is needed to determine the scalability and applicability of this approach across different programming languages."
Can we Knapsack Software Defect Prediction? Nokia 5G Case,"Stradowski, S; Madeyski, L",10.1109/ICSE-Companion58688.2023.00104,2023,"As software products become larger and more complex, the test infrastructure needed for quality assurance grows similarly, causing a constant increase in operational and maintenance costs. Although rising in popularity, most Artificial Intelligence (AI) and Machine Learning (ML) Software Defect Prediction (SDP) solutions address singular test phases. In contrast, the need to address the whole Software Development Life Cycle (SDLC) is rarely explored. Therefore in this paper, we define the problem of extending the SDP concept to the entire SDLC, as this may be one of the significant next steps for the field. Furthermore, we explore the similarity between the defined challenge and the widely known Multidimensional Knapsack Problem (MKP). We use Nokia's 5G wireless technology test process to illustrate the proposed concept. Resulting comparison validates the applicability of MKP to optimize the overall test cycle, which can be similarly relevant to any large-scale industrial software development process."
Technical Debt Classification in Issue Trackers using Natural Language Processing based on Transformers,"Skryseth, D; Shivashankar, K; PilÃ¡n, I; Martini, A",10.1109/TechDebt59074.2023.00017,2023,"Background: Technical Debt (TD) needs to be controlled and tracked during software development. Support to automatically track TD in issue trackers is limited. Aim: We explore the usage of a large dataset of developer-labeled TD issues in combination with cutting-edge Natural Language Processing (NLP) approaches to automatically classify TD in issue trackers. Method: We mine and analyze more than 160GB of textual data from GitHub projects, collecting over 55,600 TD issues and consolidating them into a large dataset (GTD dataset). We use such datasets to train and test Transformer ML models. Then we test the model's generalization ability by testing them on six unseen projects. Finally, we re-train the models including part of the TD issues from the target project to test their adaptability. Results and conclusion: (i) We create and release the GTD dataset, a comprehensive dataset including TD issues from 6,401 public repositories with various contexts; (ii) By training Transformers using the GTD dataset, we achieve performance metrics that are promising; (iii) Our results are a significant step forward towards supporting the automatic classification of TD in issue trackers, especially when the models are adapted to the context of unseen projects after fine-tuning."
Self-Supervised Query Reformulation for Code Search,"Mao, YT; Wan, CC; Jiang, YZ; Gu, XD",10.1145/3611643.3616306,2023,"Automatic query reformulation is a widely utilized technology for enriching user requirements and enhancing the outcomes of code search. It can be conceptualized as a machine translation task, wherein the objective is to rephrase a given query into a more comprehensive alternative. While showing promising results, training such a model typically requires a large parallel corpus of query pairs (i.e., the original query and a reformulated query) that are confidential and unpublished by online code search engines. This restricts its practicality in software development processes. In this paper, we propose SSQR, a self-supervised query reformulation method that does not rely on any parallel query corpus. Inspired by pre-trained models, SSQR treats query reformulation as a masked language modeling task conducted on an extensive unannotated corpus of queries. SSQR extends T5 (a sequence-to-sequence model based on Transformer) with a new pre-training objective named corrupted query completion (CQC), which randomly masks words within a complete query and trains T5 to predict the masked content. Subsequently, for a given query to be reformulated, SSQR identifies potential locations for expansion and leverages the pre-trained T5 model to generate appropriate content to fill these gaps. The selection of expansions is then based on the information gain associated with each candidate. Evaluation results demonstrate that SSQR outperforms unsupervised baselines significantly and achieves competitive performance compared to supervised methods."
Driving the Technology Value Stream by Analyzing App Reviews,"Das, S; Deb, N; Chaki, N; Cortesi, A",10.1109/TSE.2023.3270708,2023,"An emerging feature of mobile application software is the need to quickly produce new versions to solve problems that emerged in previous versions. This helps adapt to changing user needs and preferences. In a continuous software development process, the user reviews collected by the apps themselves can play a crucial role to detect which components need to be reworked. This paper proposes a novel framework that enables software companies to drive their technology value stream based on the feedback (or reviews) provided by the end-users of an application. The proposed end-to-end framework exploits different Natural Language Processing (NLP) tasks to best understand the needs and goals of the end users. We also provide a thorough and in-depth analysis of the framework, the performance of each of the modules, and the overall contribution in driving the technology value stream. An analysis of reviews with sixteen popular Android Play Store applications from various genres over a long period of time provides encouraging evidence of the effectiveness of the proposed approach."
Towards Code Generation from BDD Test Case Specifications: A Vision,"Chemnitz, L; Reichenbach, D; Aldebes, H; Naveed, M; Narasimhan, K; Mezini, M",10.1109/CAIN58948.2023.00031,2023,"Automatic code generation has recently attracted large attention and is becoming more significant to the software development process. Solutions based on Machine Learning and Artificial Intelligence are being used to increase human and software efficiency in potent and innovative ways. In this paper, we aim to leverage these developments and introduce a novel approach to generating frontend component code for the popular Angular framework. We propose to do this using behavior-driven development test specifications as input to a transformer-based machine learning model; however, we do not provide any proofof-concept solution in this work. Our approach aims to drastically reduce the development time needed for web applications while potentially increasing software quality and introducing new research ideas toward automatic code generation."
Generative Artificial Intelligence and the Economics of Effective Prompting,"Kshetri, N",10.1109/MC.2023.3314322,2023,Skills and expertise in writing effective generative artificial intelligence (GAI) prompts are increasingly in demand. This article introduces the concepts of prompting and prompt engineering and lays out guidelines for crafting effective GAI prompts.
Guiding Feature Models Synthesis from User-Stories: An Exploratory Approach,"Georges, T; Rice, L; Huchard, M; KÃ¶nig, M; Nebut, C; Tibermacine, C",10.1145/3571788.3571797,2023,"User-stories are commonly used to define requirements in agile project management. In Software Product Lines (SPL), a user-story corresponds to a feature description (or part of it), that can be shared by several products. In practice, large SPL include a huge number of user-stories, making variability hard to grasp and handle. In this paper we present an exploratory approach that aims to guide the synthesis of Feature Models that capture and structure the commonalities and the variability expressed in these user-stories. The built Feature Models aim to help the project understanding, maintenance and evolution. Our approach first decomposes the user-stories to extract the roles and the features, using natural language processing techniques. In a second step, we group user-stories having the same topics thanks to a clustering method. This contributes to extract more general features. In a third step, we leverage the use of Formal Concept Analysis to extract logical constraints between the features that guide Feature Model synthesis. We illustrate our approach using a dataset from our industrial partner."
Exploring Distributional Shifts in Large Language Models for Code Analysis,"Arakelyan, S; Das, RJ; Mao, Y; Ren, X",,2023,"We systematically study how three large language models with code capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data. We consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. We establish that samples from each new domain present all the models with a significant challenge of distribution shift. We study how established methods adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. Moreover, this solution can outperform direct finetuning for very low-data scenarios. Finally, we consider variations of this approach to create a more broadly applicable method to adapt to multiple domains at once. We find that for code generation, a model adapted to multiple domains simultaneously performs on par with those adapted to a single domain(1)."
Evaluation of the Performance Impact of SPM Allocation on a Novel Scratchpad Memory,"Imhmed, E; Ceh-Varela, E; Cook, J; Parten, C",10.1109/COMPSAC57700.2023.00133,2023,"Local Memory Store (LMStore) is a novel scratchpad memory (SPM) design, with recent research evaluation showing its capability for improving program performance. However, the performance of LMStore depends on its memory layout decided by its allocation scheme. In this paper, we evaluate the impact of SPM allocation on LMStore performance. Our experimental results, using benchmarks from the Malardalen WCET benchmark suite executing on LMStore architecture modeled in the PyCacheSim simulator, demonstrate that LMStore with a stack distance-based SPM allocation scheme significantly improves data movement by an average of 44.46% compared to a Cache-only architecture, and by an average of 23.89% compared to LMStore with a frequency-based SPM allocation scheme."
The feasibility and inevitability of stealth attacks,"Tyukin, IY; Higham, DJ; Bastounis, A; Woldegeorgis, E; Gorban, AN",10.1093/imamat/hxad027,2023,"We develop and study new adversarial perturbations that enable an attacker to gain control over decisions in generic Artificial Intelligence (AI) systems including deep learning neural networks. In contrast to adversarial data modification, the attack mechanism we consider here involves alterations to the AI system itself. Such a stealth attack could be conducted by a mischievous, corrupt or disgruntled member of a software development team. It could also be made by those wishing to exploit a 'democratization of AI' agenda, where network architectures and trained parameter sets are shared publicly. We develop a range of new implementable attack strategies with accompanying analysis, showing that with high probability a stealth attack can be made transparent, in the sense that system performance is unchanged on a fixed validation set which is unknown to the attacker, while evoking any desired output on a trigger input of interest. The attacker only needs to have estimates of the size of the validation set and the spread of the AI's relevant latent space. In the case of deep learning neural networks, we show that a one-neuron attack is possible-a modification to the weights and bias associated with a single neuron-revealing a vulnerability arising from over-parameterization. We illustrate these concepts using state-of-the-art architectures on two standard image data sets. Guided by the theory and computational results, we also propose strategies to guard against stealth attacks."
Ontology-Based Automatic Reasoning and NLP for Tracing Software Requirements into Models with the OntoTrace Tool,"Mosquera, D; Ruiz, M; Pastor, O; Spielberger, J",10.1007/978-3-031-29786-1_10,2023,"Context and motivation. Traceability is an essential part of quality assurance tasks for software maintainability, validation, and verification. However, the effort required to create and maintain traces is still high compared to their benefits. Problem. Some authors have proposed traceability tools to address this challenge, yet some of those tools require historical traceability data to generate traces, representing an entry barrier to software development teams that do not do traceability. Another common requirement of existing traceability tools is the scope of artefacts to be traced, hindering the adaptability of traceability tools in practice. Principal ideas. Motivated by the mentioned challenges, in this paper we propose OntoTraceV2.0: a tool for supporting trace generation of arbitrary software artefacts without depending on historical traceability data. The architecture of OntoTraceV2.0 integrates ontology-based automatic reasoning to facilitate adaptability for tracing arbitrary artefacts and natural language processing for discovering traces based on text-based similarity between artefacts. We conducted a quasi-experiment with 36 subjects to validate OntoTraceV2.0 in terms of efficiency, effectiveness, and satisfaction. Contribution. We found that OntoTraceV2.0 positively affects the subjects' efficiency and satisfaction during trace generation compared to a manual approach. Although the subjects' average effectiveness is higher using OntoTraceV2.0, we observe no statistical difference with the manual trace generation approach. Even though such results are promising, further replications are needed to avoid certain threats to validity. We conclude the paper by analysing the experimental results and limitations we found, drawing on future challenges, and proposing the next research endeavours."
Autonomous Molecular Structure Imaging with High-Resolution Atomic Force Microscopy for Molecular Mixture Discovery,"Arias, S; Zhang, YL; Zahl, P; Hollen, S",10.1021/acs.jpca.3c01685,2023,"Dueto its single-molecule sensitivity, high-resolution atomicforce microscopy (HR-AFM) has proved to be a valuable and uniquelyadvantageous tool to study complex molecular mixtures, which holdpromise for developing clean energy and achieving environmental sustainability.However, significant challenges remain to achieve the full potentialof the sophisticated and time-consuming experiments. Automation combinedwith machine learning (ML) and artificial intelligence (AI) is keyto overcoming these challenges. Here we present Auto-HR-AFM, an AItool to automatically collect HR-AFM images of petroleum-based mixtures.We trained an instance segmentation model to teach Auto-HR-AFM howto recognize features in HR-AFM images. Auto-HR-AFM then uses thatinformation to optimize the imaging by adjusting the probe-moleculedistance for each molecule in the run. Auto-HR-AFM is the initialtool that will lead to fully automated scanning probe microscopy (SPM)experiments, from start to finish. This automation will allow SPMto become a mainstream characterization technique for complex mixtures,an otherwise unattainable target."
An Extensive Study on Model Architecture and Program Representation in the Domain of Learning-based Automated Program Repair,"HorvÃ¡th, D; Csuvik, V; GyimÃ³thy, T; VidÃ¡cs, L",10.1109/APR59189.2023.00013,2023,"Bug fixing is one of the most time-consuming and resource-intensive tasks in the software development life cycle. Automated Program Repair (APR) might be able to help in this process, but it still has to overcome many obstacles. Deep learning models have shown promise for automated program repair in recent years, but their effectiveness can depend on the representation of the source code used as input. In this paper, we conduct an experimental study to compare the performance of deep learning models on two popular programming languages, Java and JavaScript, using three different code representations: raw text, command sequences, and abstract syntax trees (ASTs). We also experiment with varying models, including T5, CodeT5, (for solving sequence-to-sequence tasks) RoBERTa, and GPTNeo (to encode/decode AST graph information). We evaluate the models on a set of real-world defects from open-source projects and compare the performance, and the repair patches generated by the models. Our results show that training on command sequence representation outperforms most other configurations. We achieve a best of 19.88% accuracy on the java-small dataset, and 11.87% on java-medium, using text representation. Using command sequence representation, we achieve 30.64% on javasmall and 18.53% on the medium dataset. However, when representing the source with ast+text information, our models significantly underperform compared to other representations, achieving results below one percent. Our findings contribute to a better understanding of the strengths and limitations of deep learning models for automated program repair and provide practical guidance for their use in practice."
BERT-Based GitHub Issue Report Classification,"Siddiq, ML; Santos, JCS",10.1145/3528588.3528660,2022,"Issue tracking is one of the integral parts of software development, especially for open source projects. GitHub, a commonly used software management tool, provides its own issue tracking system. Each issue can have various tags, which are manually assigned by the project's developers. However, manually labeling software reports is a time-consuming and error-prone task. In this paper, we describe a BERT-based classification technique to automatically label issues as questions, bugs, or enhancements. We evaluate our approach using a dataset containing over 800,000 labeled issues from real open source projects available on GitHub. Our approach classified reported issues with an average F1-score of 0.8571. Our technique outperforms a previous machine learning technique based on FastText."
Artificial-intelligence-driven scanning probe microscopy,"Krull, A; Hirsch, P; Rother, C; Schiffrin, A; Krull, C",10.1038/s42005-020-0317-3,2020,"Enabling atomic-precision mapping and manipulation of surfaces, scanning probe microscopy requires constant human supervision to assess image quality and probe conditions. Here, the authors demonstrate DeepSPM, a machine learning approach allowing to acquire and classify data autonomously in multi-day Scanning Tunnelling Microscopy experiments. Scanning probe microscopy (SPM) has revolutionized the fields of materials, nano-science, chemistry, and biology, by enabling mapping of surface properties and surface manipulation with atomic precision. However, these achievements require constant human supervision; fully automated SPM has not been accomplished yet. Here we demonstrate an artificial intelligence framework based on machine learning for autonomous SPM operation (DeepSPM). DeepSPM includes an algorithmic search of good sample regions, a convolutional neural network to assess the quality of acquired images, and a deep reinforcement learning agent to reliably condition the state of the probe. DeepSPM is able to acquire and classify data continuously in multi-day scanning tunneling microscopy experiments, managing the probe quality in response to varying experimental conditions. Our approach paves the way for advanced methods hardly feasible by human operation (e.g., large dataset acquisition and SPM-based nanolithography). DeepSPM can be generalized to most SPM techniques, with the source code publicly available."
Aspect-Based API Review Classification: How Far Can Pre-Trained Transformer Model Go?,"Yang, CR; Xu, BW; Khan, JY; Uddin, G; Han, DY; Yang, Z; Lo, D",10.1109/SANER53432.2022.00054,2022,"APIs (Application Programming Interfaces) are reusable software libraries and are building blocks for modern rapid software development. Previous research shows that programmers frequently share and search for reviews of APIs on the mainstream software question and answer (Q&A) platforms like Stack Overflow, which motivates researchers to design tasks and approaches related to process API reviews automatically. Among these tasks, classifying API reviews into different aspects (e.g., performance or security), which is called the aspect-based API review classification, is of great importance. The current state-of-the-art (SOTA) solution to this task is based on the traditional machine learning algorithm. Inspired by the great success achieved by pre-trained models on many software engineering tasks, this study fine-tunes six pre-trained models for the aspect-based API review classification task and compares them with the current SOTA solution on an API review benchmark collected by Uddin et al. The investigated models include four models (BERT, RoBERTa, ALBERT and XLNet) that are pre-trained on natural languages, BERTOverflow that is pre-trained on text corpus extracted from posts on Stack Overflow, and CosSensBERT that is designed for handling imbalanced data. The results show that all the six fine-tuned models outperform the traditional machine learning-based tool. More specifically, the improvement on the F1-score ranges from 21.0% to 30.2%. We also find that BERTOverflow, a model pre-trained on the corpus from Stack Overflow, does not show better performance than BERT. The result also suggests that CosSensBERT also does not exhibit better performance than BERT in terms of F1, but it is still worthy of being considered as it achieves better performance on MCC and AUC."
Sources of Risk of AI Systems,"Steimers, A; Schneider, M",10.3390/ijerph19063641,2022,"Artificial intelligence can be used to realise new types of protective devices and assistance systems, so their importance for occupational safety and health is continuously increasing. However, established risk mitigation measures in software development are only partially suitable for applications in AI systems, which only create new sources of risk. Risk management for systems that for systems using AI must therefore be adapted to the new problems. This work objects to contribute hereto by identifying relevant sources of risk for AI systems. For this purpose, the differences between AI systems, especially those based on modern machine learning methods, and classical software were analysed, and the current research fields of trustworthy AI were evaluated. On this basis, a taxonomy could be created that provides an overview of various AI-specific sources of risk. These new sources of risk should be taken into account in the overall risk assessment of a system based on AI technologies, examined for their criticality and managed accordingly at an early stage to prevent a later system failure."
Detecting privacy requirements from User Stories with NLP transfer learning models,"Casillo, F; Deufemia, V; Gravino, C",10.1016/j.infsof.2022.106853,2022,"Context: To provide privacy-aware software systems, it is crucial to consider privacy from the very beginning of the development. However, developers do not have the expertise and the knowledge required to embed the legal and social requirements for data protection into software systems.Objective: We present an approach to decrease privacy risks during agile software development by automatically detecting privacy-related information in the context of user story requirements, a prominent notation in agile Requirement Engineering (RE).Methods: The proposed approach combines Natural Language Processing (NLP) and linguistic resources with deep learning algorithms to identify privacy aspects into User Stories. NLP technologies are used to extract information regarding the semantic and syntactic structure of the text. This information is then processed by a pre-trained convolutional neural network, which paved the way for the implementation of a Transfer Learning technique. We evaluate the proposed approach by performing an empirical study with a dataset of 1680 user stories.Results: The experimental results show that deep learning algorithms allow to obtain better predictions than those achieved with conventional (shallow) machine learning methods. Moreover, the application of Transfer Learning allows to considerably improve the accuracy of the predictions, ca. 10%.Conclusions: Our study contributes to encourage software engineering researchers in considering the opportunities to automate privacy detection in the early phase of design, by also exploiting transfer learning models."
Systematic Mapping: Artificial Intelligence Techniques in Software Engineering,"Sofian, H; Yunus, NAM; Ahmad, R",10.1109/ACCESS.2022.3174115,2022,"Artificial Intelligence (AI) has become a core feature of today's real-world applications, making it a trending topic within the software engineering (SE) community. The rise in the availability of AI techniques encompasses the capability to make rapid, automated, impactful decisions and predictions, leading to the adoption of AI techniques in SE. With industry revolution 4.0, the role of software engineering has become critical for developing productive, efficient, and quality software. Thus, there is a major need for AI techniques to be applied to enhance and improve the critical activities within the software engineering phases. Software is developed through intelligent software engineering phases. This paper concerns a systematic mapping study that aimed to characterize the publication landscape of AI techniques in software engineering. Gaps are identified and discussed by mapping these AI techniques against the SE phases to which they contributed. Many systematic mapping review papers have been produced only for a specific AI technique or a specific SE phase or activity. Hence, to our best of knowledge within the last decade, there is no systematic mapping review that has fully explored the overall trends in AI techniques and their application to all SE phases."
Source Code Summarization with Structural Relative Position Guided Transformer,"Gong, Z; Gao, CY; Wang, YS; Gu, WC; Peng, Y; Xu, ZL",10.1109/SANER53432.2022.00013,2022,"Source code summarization aims at generating concise and clear natural language descriptions for programming languages. Well-written code summaries are beneficial for programmers to participate in the software development and maintenance process. To learn the semantic representations of source code, recent efforts focus on incorporating the syntax structure of code into neural networks such as Transformer. Such Transformer-based approaches can better capture the long-range dependencies than other neural networks including Recurrent Neural Networks (RNNs), however, most of them do not consider the structural relative correlations between tokens, e.g., relative positions in Abstract Syntax Trees (ASTs), which is beneficial for code semantics learning. To model the structural dependency, we propose a StruCtural RelatIve Position guided Transformer, named SCRIPT. SCRIPT first obtains the structural relative positions between tokens via parsing the ASTs of source code, and then passes them into two types of Transformer encoders. One Transformer directly adjusts the input according to the structural relative distance; and the other Transformer encodes the structural relative positions during computing the self-attention scores. Finally, we stack these two types of Transformer encoders to learn representations of source code. Experimental results show that the proposed SCRIPT outperforms the state-of-the-art methods by at least 1.6%, 1.4% and 2.8% with respect to BLEU, ROUGEL and METEOR on benchmark datasets, respectively. We further show that how the proposed SCRIPT captures the structural relative dependencies."
Towards Requirements Specification Collaboration Forum for Embedded Software Systems,"Fariha, A; Alwidian, S; Azim, A",10.1109/MODELS-C59198.2023.00061,2023,"Effective requirements specification for embedded software systems relies heavily on the collaboration between stakeholders to articulate accurate functional and non-functional requirements. At present, requirement engineers manually coordinate the sophisticated task of correct stakeholders selection for requirements and compilation of feedback in the embedded software domain. Because embedded software has an extensive number of stakeholders, the absence of an efficient collaboration platform causes a prolonged project completion time, elevated maintenance costs, or project failure. In this preliminary research paper, a stakeholder collaboration platform is proposed using an auto-encoder-based recommender system and SysML modeling language for embedded software systems. In the proposed framework, forums are the collaboration space for stakeholders to contribute to requirements analysis and specifications and are generated from the requirements diagram of the SysML modeling language. Owners of the requirements will be directly assigned to the forum from the SysML requirement profile information. To ensure adequate stakeholder engagement in the requirements specification process, the Collaborative Denoising Auto-Encoder (CDAE) recommender system is used for advanced auto-recommendations of requirement forums to stakeholders. This approach facilitates feedback collection and analysis to refine requirements specifications. The automatic forum creation and the advanced recommendation process of this framework will add no overhead to the requirements engineers or analysts; at the same time, a centralized collaboration platform for the stakeholders will save requirements analysis time and avoid future conflicts."
Five Futures with AI Coding Agents,"Tanimoto, SL",10.1145/3594671.3594685,2023,"Many computer programmers are beginning to use computational agents to help them develop software. This article raises questions about the nature of programmer-to-agent relationships. The author's intent is to foster thought that will help human programmers best prepare for such relationships and perhaps design the relationships, ultimately keeping their jobs and improving their programming experience."
"An industrial experience report on model-based, AI-enabled proposal development for an RFP/RFI","Nistala, P; Rajbhoj, A; Kulkarni, V; Noronha, S; Joshi, A",10.1016/j.scico.2023.103058,2024,"Large organizations respond to huge volumes of Request for Proposals (RFP)/ Request for Information (RFI) every year. The process of developing a proposal for an RFP/ RFI is completely manual and time-, effort-, and intellect-intensive. While Model Driven Engineering (MDE) approaches have been popular in downstream Software Development Lifecycle (SDLC) phases to transform the design models into code, there has been a gap in leveraging model-based techniques in document-centric phases of proposal development, requirements analysis, etc. This paper presents an automated proposal development approach for a client-supplied RFP/RFI using a combination of model-based and AI-enabled techniques and describes the case study of its successful deployment to hundreds of presales users across multiple geographies. We explain the Proposal system and report on the experience of deploying the system in the industry, bring out its efficacy, and user feedback, and discuss the lessons learnt."
Verification strategy for artificial intelligence components in nuclear plant instrumentation and control systems,"Park, J; Kim, T; Koo, S",10.1016/j.pnucene.2023.104842,2023,"The application of artificial intelligence (AI) systems in nuclear power plants has the potential to facilitate significant advancements in the industry. However, nuclear system engineers and regulators have expressed concerns regarding the reliability of AI components owing to a lack of documentation and appropriate verification methods that differ from those used for traditional software. Accordingly, this study proposes a systematic approach for the verification and validation of AI components based on nuclear software regulatory guidelines. The proposed strategy includes process verification and functional testing activities specifically tailored to AI components for instrumentation and control systems. This approach serves as a foundation for fostering understanding and agreement among system engineers, operating licensees, and regulatory bodies on the use of AI components in nuclear power plants."
A Comparative Study of Transformer-based Neural Text Representation Techniques on Bug Triaging,"Dipongkor, AK; Moran, K",10.1109/ASE56229.2023.00217,2023,"Bug report management has been shown to be an important and time consuming software maintenance task. Often, the first step in managing bug reports is related to triaging a bug to the appropriate developer who is best suited to understand, localize, and fix the target bug. Additionally, assigning a given bug to a particular part of a software project can help to expedite the fixing process. However, despite the importance of these activities, they are quite challenging, where days can be spent on the manual triaging process. Past studies have attempted to leverage the limited textual data of bug reports to train text classification models that automate this process - to varying degrees of success. However, the textual representations and machine learning models used in prior work are limited by their expressiveness, often failing to capture nuanced textual patterns that might otherwise aid in the triaging process. Recently, large, transformer-based, pre-tained neural text representation techniques (i.e., large language models or LLMs) such as BERT and CodeBERT have achieved greater performance with simplified training procedures in several natural language processing tasks, including text classification. However, the potential for using these techniques to improve upon prior approaches for automated bug triaging is not well studied or understood. Therefore, in this paper we offer one of the first investigations that fine-tunes transformer-based language models for the task of bug triaging on four open source datasets, spanning a collective 53 years of development history with over 400 developers and over 150 software project components. Our study includes both a quantitative and qualitative analysis of effectiveness. Our findings illustrate that DeBERTa is the most effective technique across the triaging tasks of developer and component assignment, and the measured performance delta is statistically significant compared to other techniques. However, through our qualitative analysis, we also observe that each technique possesses unique abilities best suited to certain types of bug reports."
Automatic software code repair using deep learning techniques,"Abdollahpour, MM; Ashtiani, M; Bakhshi, F",10.1007/s11219-023-09653-1,2024,"In the multi-hundred-billion-dollar industry of software development, the debugging process is an expensive task for developers. So, much effort has been put into debugging automation. In the last decade, researchers have repaired codes according to predefined rules which are only effective in limited types of bugs. Through a lot of experiments, researchers have found that deep learning models are useful in code repair automation similar to the revolutionary results these approaches have produced in various other fields. To solve such a difficult problem, recent works focus on debugging bugs that appear on one line of code. It has been reported that this type of bug occurs at an average rate of 1 out of every 1600 lines of code in a software project, which is significant. The current research follows these approaches and introduces a novel automatic code-repair system. We have employed the transfer learning technique to reuse a pre-trained model on the problem. The proposed system is designed based on the encoder-decoder architecture. In the encoder, a new pre-trained Bert model named JavaBert is used. Then, the model was fine tuned. The decoder is a transformer with an autoregressive structure. ManySStuBs4J [1] dataset is used for evaluation purposes. The results of the evaluations show that the proposed system has higher accuracy and BLEU criteria than CodeBert and the baseline model. Baseline is a simple model that acts as a reference in machine learning studies and CodeBert is one of the most similar models to the proposed model. The bilingual evaluation understudy score (BLEU) improvement is between 0.04 and 0.16%, the accuracy improvement is between 0.64 and 5.81%, the recall improvement is between 1.08 and 9.2%, and the F-score improvement is between 3.27 and 6.18%."
Teaching Simulations Supported by Artificial Intelligence in the Real World,"Chaturvedi, I; Cambria, E; Welsch, RE",10.3390/educsci13020187,2023,"Video conferencing has enabled synchronous communication in a classroom and created multi-sensory content to stimulate learners. Artificial intelligence involves complex equations that are better taught using a constructive pedagogy where students experiment with alternative ways of solving the same problem. Multiple-choice questions have high reliability and can easily reveal student skill levels in a quick way. The Australian Computer Society accreditation exercise ensures that the content for each subject serves as a flexible template for teaching. The geographical extent of the country requires the presence of multiple subordinate campuses affiliated to a main campus. Following the concept of strands, it was also necessary to show continuity in learning and assessments between the first- and second-year subjects. Student feedback for subjects with artificial intelligence-based simulations showed that several students found it difficult to understand lectures and assignments. Hence, to measure student learning, we introduced a Kahoot quiz during the recess of each lecture that students could join through their mobile phones from different campuses. Software project management is challenging for students with vision or attention-related disorders. We taught them how to use charts to visually observe variables and narrow down possible relationships before performing in-depth analysis. One of the main purposes of education is employability. Hence, greater context to real world industry examples was introduced into lectures."
Intelligent nuclear decommissioning solution: Code for site characterization and management of overall surveys,"Byun, H; Park, JD; An, SHY; Kim, J; Kim, J; Lee, DY; Lee, BS",10.1016/j.anucene.2023.110212,2024,"The site characterization should be conducted during the entire decommissioning period to verify the compliance with regulatory guidance and to demonstrate the safety for site release. A technical manual called Multi-Agency Radiation Survey and Site Investigation Manual (MARSSIM) has been widely used as a reference for the site characterization. Historically, it has required enormous resources because of extensive data handling. And there is a possibility of a human error because of the expert's judgement reliance. In this study, software called COde for Site characterization and Management of Overall Surveys (COSMOS) has been developed with Artificial Intelligence (AI) models. One of the AI model is possible to revolutionary reduce time for finding keyword related contamination, and the other manages database by digitalizing drawings. Also, other survey modules are possible to conduct related all of statistical tests. In the end, COSMOS would be utilized as a total management solution for site characterization."
Semi-automated Software Requirements Categorisation using Machine Learning Algorithms,"Talele, P; Apte, S; Phalnikar, R; Talele, H",,2023,"Requirement engineering is a mandatory phase of the Software development life cycle (SDLC) that includes defining and documenting system requirements in the Software Requirements Specification (SRS). As the complexity increases, it becomes difficult to categorise the requirements intofunctional andnon-functional requirements.Presently,the dearthofautomated techniques necessitates reliance on labour-intensive and time- consuming manual methods for this purpose. This research endeavours to address this gap by investigating and contrasting two prominent feature extraction techniques and their efficacy in automating the classification of requirements. Natural language processing methods are used in the text pre-processing phase, followed by the Term Frequency - Inverse Document Frequency (TF-IDF) and Word2Vec for feature extraction for further understanding. These features are used as input to the Machine Learning algorithms. This study compares existing machine learning algorithms and discusses their correctness in categorising the software requirements. In our study, we have assessed the algorithms Decision Tree (DT), Random Forest (RF), Logistic Regression (LR), Neural Network (NN), K-Nearest Neighbour (KNN) and Support Vector Machine (SVM) on the precision and accuracy parameters. The results obtained in this study showed that the TF-IDF feature selection algorithm performed better in categorising requirements than the Word2Vec algorithm, with an accuracy of 91.20% for the Support Vector Machine (SVM) and Random Forest algorithm as compared to 87.36% for the SVM algorithm. A 3.84% difference is seen between the two when applied to the publicly available PURE dataset. We believe these results will aid developers in building products that aid in requirement engineering."
Can ChatGPT correct code based on logical steps?,"Souma, N; Ito, W; Obara, M; Kawaguchi, T; Akinobu, Y; Kurabayashi, T; Tanno, H; Kuramitsu, K",10.1109/APSEC60848.2023.00094,2023,"ChatGPT presents emerging opportunities in software development, yet its capabilities for understanding code remain largely understudied. This study aims to focus on the logical aspect of code comprehension of ChatGPT by examining its performance in detecting and fixing bugs. Our preliminary results suggest that ChatGPT seems to correct code in a different way than human logical steps."
Exploring the inhibitors for competitive AI software development through cloud driven transformation,"Hasteer, N; Sindhwani, R; Behl, A; Varshney, A; Sharma, A",10.1007/s10479-023-05619-5,2024,"COVID-19 has compelled every sector to shift towards virtualizing, and to curb the spread of this deadly virus, technologies like cloud computing, artificial intelligence, and IoT have played a vital role. Cloud-based crowdsourcing is a sourcing method through which organizations may post application development tasks as a contest, and the crowd may participate to win prizes or earn monetary support through online means. Many IT firms hesitate to adopt this methodology for competitive software development despite its advantages. This study's primary purpose is to discover the inhibitors and criteria restricting IT firms from incorporating cloud-based crowdsourcing in their workflow. An extensive literature review has been conducted to identify the inhibitors and criteria. The authors applied a novel three-phased methodology to the identified inhibitors to analyze them with mixed method approach. Authors have used the m-TISM method to develop a hierarchal model and find the interrelation between the identified inhibitors, which were further analyzed and segmented with the help of MICMAC analysis. The PF-AHP method was used to find the weightage of the identified criteria, and the PF-CoCoSo method was applied to find the ranking of the inhibitors considering the weights of the criteria. The study reveals that Difficulty in Price Fixing, Labor Exploitation, and Risk of Dependency are the critical challenges to adoption. The study reflects how the Contingency Management Theory and the Resource Dependency Theory acts as frameworks for navigating complexity and reducing risks in the ever-changing world of software development. The study concludes with key inputs that may help IT firms adopt and promote cloud-based crowdsourcing methodology for software development. These inputs may help companies to attract experienced developers who prefer working from home and prove worthy even during disruptions caused due to unprecedented situations."
On Automated Assistants for Software Development: The Role of LLMs,"Leung, M; Murphy, G",10.1109/ASE56229.2023.00035,2023,"Software developers handle many complex tasks that include gathering and applying domain knowledge, coordinating subtasks, designing interfaces, turning ideas into elegant code, and more. They must switch contexts between these tasks, incurring more cognitive costs. Recent advances in large language models (LLMs) open up new possibilities for moving beyond the support provided by automated assistants (AAs) available today. In this paper, we explore if a human memory model can provide a framework for the systematic investigation of AAs for software development based on LLMs and other new technologies."
Authorship Identification of Binary and Disassembled Codes Using NLP Methods,"Romanov, A; Kurtukova, A; Fedotova, A; Shelupanov, A",10.3390/info14070361,2023,"This article is part of a series aimed at determining the authorship of source codes. Analyzing binary code is a crucial aspect of cybersecurity, software development, and computer forensics, particularly in identifying malware authors. Any program is machine code, which can be disassembled using specialized tools and analyzed for authorship identification, similar to natural language text using Natural Language Processing methods. We propose an ensemble of fastText, support vector machine (SVM), and the authors' hybrid neural network developed in previous works in this research. The improved methodology was evaluated using a dataset of source codes written in C and C++ languages collected from GitHub and Google Code Jam. The collected source codes were compiled into executable programs and then disassembled using reverse engineering tools. The average accuracy of author identification for disassembled codes using the improved methodology exceeds 0.90. Additionally, the methodology was tested on the source codes, achieving an average accuracy of 0.96 in simple cases and over 0.85 in complex cases. These results validate the effectiveness of the developed methodology and its applicability to solving cybersecurity challenges."
Rise of Distributed Deep Learning Training in the Big Model Era: From a Software Engineering Perspective,"Liu, XZ; Gu, DD; Chen, ZP; Wen, JF; Zhang, ZL; Ma, Y; Wang, HY; Jin, X",10.1145/3597204,2023,"Deep learning (DL) has become a key component of modern software. In the big model era, the rich features of DL-based software (i.e., DL software) substantially rely on powerful DL models, e.g., BERT, GPT-3, and the recently emerging GPT-4, which are trained on the powerful cloud with large datasets. Hence, training effective DL models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices (e.g., a cluster of GPUs) in the training process, which is known as distributed deep learning training, or distributed training for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current DL-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers' issues in distributed training. To this end, we focus on popular DL frameworks that support distributed training (including TensorFlow, PyTorch, Keras, and Horovod) and analyze 1,131 real-world developers' issues about using these frameworks reported on Stack Overflow and GitHub. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. We find that: (1) many distributed-specific faults and non-distributed-specific faults inherently share the same fault symptoms, making it challenging to debug; (2) most of the fault symptoms have frequent fix patterns; (3) about half of the faults are related to system-level configurations. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop DL-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless APIs for cloud platforms."
An Intelligent Tool for Classifying Issue Reports,"Laiq, M",10.1109/NLBSE59153.2023.00010,2023,"A considerable amount of issue reports are submitted daily in large-scale software development. Manually reviewing and classifying each issue report is challenging and error-prone. Thus, to assist practitioners, in this paper, we propose and evaluate an automatic supervised machine learning-based approach that can automatically predict the newly submitted issue report type (i.e., bug, feature, question, or documentation). We applied the supervised machine learning-based approach to over 1.4 million issue reports data from real open-source projects. We performed our experiments using Stochastic Gradient Descent (SGD)-based classifier and achieved an F1 micro average score of 0.8523."
Unveiling the potential of large language models in generating semantic and cross-language clones,"Roy, PR; Alam, AI; Al-omari, F; Roy, B; Roy, CK; Schneider, KA",10.1109/IWSC60764.2023.00011,2023,"Semantic and Cross-language code clone generation may be useful for code reuse, code comprehension, refactoring and benchmarking. OpenAI's GPT model has potential in such clone generation as GPT is used for text generation. When developers copy/paste codes from Stack Overflow (SO) or within a system, there might be inconsistent changes leading to unexpected behaviours. Similarly, if someone possesses a code snippet in a particular programming language but seeks equivalent functionality in a different language, a semantic cross-language code clone generation approach could provide valuable assistance. In this study, using SemanticCloneBench as a vehicle, we evaluated how well the GPT-3 model could help generate semantic and cross-language clone variants for a given fragment. We have comprised a diverse set of code fragments and assessed GPT-3's performance in generating code variants. Through extensive experimentation and analysis, where 9 judges spent 158 hours to validate, we investigate the model's ability to produce accurate and semantically correct variants. Our findings shed light on GPT-3's strengths in code generation, offering insights into the potential applications and challenges of using advanced language models in software development. Our quantitative analysis yields compelling results. In the realm of semantic clones, GPT-3 attains an impressive accuracy of 62.14% and 0.55 BLEU score, achieved through few-shot prompt engineering. Furthermore, the model shines in transcending linguistic confines, boasting an exceptional 91.25% accuracy in generating cross-language clones."
Optimizing throughput of Seq2Seq model training on the IPU platform for AI-accelerated CFD simulations,"Rosciszewski, P; Krzywaniak, A; Iserte, S; Rojek, K; Gepner, P",10.1016/j.future.2023.05.004,2023,"Intelligence Processing Units (IPU) have proven useful for many AI applications. In this paper, we evaluate them within the emerging field of AI for simulation, where traditional numerical simulations are supported by artificial intelligence approaches. We focus specifically on a program for training machine learning models supporting a computational fluid dynamics application. We use custom TensorFlow provided by the Poplar Software Development Kit to adapt the program for the IPU-POD16 platform and investigate its ease of use and performance scalability. Training a model on data from OpenFOAM simulations allows us to get accurate simulation state predictions in test time. We describe how to optimize multi-threading runtime options and utilize the popdist library to overcome a performance bottleneck in feeding training data to the IPU on the host side. Due to communication overheads, using data parallelism to utilize two IPUs instead of one does not improve the throughput. However, once the intra-IPU costs have been paid, the hardware capabilities for inter-IPU communication allow for good scalability. Increasing the number of IPUs from two to 16 improves the throughput from 560.8 to 2805.8 samples/s. Additionally, the experimental results show that reducing the precision of input data storage from FP32 to FP16 allows to improve training throughput by 12%, while tuning selected runtime variables, by up to 6.3%. (c) 2023 Published by Elsevier B.V."
Using artificial intelligence techniques for COVID-19 genome analysis,"Nawaz, MS; Fournier-Viger, P; Shojaee, A; Fujita, H",10.1007/s10489-021-02193-w,2021,"The genome of the novel coronavirus (COVID-19) disease was first sequenced in January 2020, approximately a month after its emergence in Wuhan, capital of Hubei province, China. COVID-19 genome sequencing is critical to understanding the virus behavior, its origin, how fast it mutates, and for the development of drugs/vaccines and effective preventive strategies. This paper investigates the use of artificial intelligence techniques to learn interesting information from COVID-19 genome sequences. Sequential pattern mining (SPM) is first applied on a computer-understandable corpus of COVID-19 genome sequences to see if interesting hidden patterns can be found, which reveal frequent patterns of nucleotide bases and their relationships with each other. Second, sequence prediction models are applied to the corpus to evaluate if nucleotide base(s) can be predicted from previous ones. Third, for mutation analysis in genome sequences, an algorithm is designed to find the locations in the genome sequences where the nucleotide bases are changed and to calculate the mutation rate. Obtained results suggest that SPM and mutation analysis techniques can reveal interesting information and patterns in COVID-19 genome sequences to examine the evolution and variations in COVID-19 strains respectively."
Learning Software Project Management From Analyzing Q&A's in the Stack Exchange,"Ahmadi, A; Delkhosh, F; Deshpande, G; Patterson, RA; Ruhe, G",10.1109/ACCESS.2023.3235953,2023,"Software Project Management (SPM) is considered the key driver for the success or failure of software projects. Project failure is caused by various factors, the most important of which is poor SPM. Thus, we investigated the needs of practitioners by focusing on Project Management Q & A communities. More precisely, we targeted Stack Exchange to identify the primary needs of software project managers. More than 5000 SPM questions were analyzed from the conceptual model given by the Project Management Body of Knowledge PMBOK. For pre-training of the Machine Learning classifiers, we implemented Bidirectional Encoder Representations from Transformers (BERT) and Doc2Vec text embedding and compared their performance. Our results showed that BERT outperforms Doc2Vec for pre-training in almost all scenarios. Schedule management, followed by resource management, are the main PMBOK knowledge areas of concern for project managers. Among the process groups, the emphasis of the questions is on planning. We compared the findings with the learning and training status quo in 11 top Canadian universities. We analyzed 46 SPM-related courses and found that the rank correlation of PMBOK knowledge areas is 0.23 between the key content of the analyzed courses and the focus of Q & A's knowledge areas analyzed from Stack Exchange."
Generalizability of NLP-based Models for Modern Software Development Cross-Domain Environments,"Krasniqi, R; Do, H",10.1109/NLBSE59153.2023.00009,2023,"Natural Language Processing (NLP) has shown to be effective for solving complex problems in the Software Engineering (SE) domain, such as building chatbots and its ability to translate multi-languages. Despite the advances allowed by NLP, there are technical loopholes that hinder its fullest potential within the SE domain. The open problem remains in their generalizability for modern software development tasks that typically operate in a dynamic environment, such as AWS and SaaS platforms. The problem with these setups is that they may not contain labeled data. This poses a challenge when applying most prominent data-centric NLP models such as BERT transformer models. This position paper highlights some of the most pressing challenges drawn between the intersection of NLP and SE domains. Our vision revolves around improving the NLP model generalizability for dynamic cross-domain environments that contain little or no labeled target-domain data. We discuss these challenges and propose a research roadmap to tackle this problem as a research community emanating from SE lenses."
A Prompt-Based Approach for Software Development,"Hamdi, M; Kim, LD",10.1109/CSCI62032.2023.00267,2023,"Generative Language-Based AI (Gen-LBAI) models have drawn significant interest in a wide range of fields including software engineering. There has been increasing interest in leveraging Gen-LBAI models in various areas of software engineering for efficient software development. Successful adoption of AI models can save time by generating various types of software artifacts, which lets the developer focus on more critical and creative tasks. In this paper, we present a prompt-based approach for software development using an AI model. The approach presents a set of guidelines for designing prompts and discusses where and how AI models can he used in software development for improved efficiency. The approach is evaluated in a case study using ChatGPT-4 and compared with manual development. The evaluation shows that the approach increases 41 % in efficiency compared to manual development."
Requests classification in the customer service area for software companies using machine learning and natural language processing,"Arias-Barahona, MX; Arteaga-Arteaga, HB; Orozco-Arias, S; FlÃ³rez-RuÃ­z, JC; Valencia-DÃ­az, MA; Tabares-Soto, R",10.7717/peerj-cs.1016,2023,"Artificial intelligence (AI) is one of the components recognized for its potential to transform the way we live today radically. It makes it possible for machines to learn from experience, adjust to new contributions and perform tasks like human beings. The business field is the focus of this research. This article proposes implementing an incident classification model using machine learning (ML) and natural language processing (NLP). The application is for the technical support area in a software development company that currently resolves customer requests manually. Through ML and NLP techniques applied to company data, it is possible to know the category of a request given by the client. It increases customer satisfaction by reviewing historical records to analyze their behavior and correctly provide the expected solution to the incidents presented. Also, this practice would reduce the cost and time spent on relationship management with the potential consumer. This work evaluates different Machine Learning models, such as support vector machine (SVM), Extra Trees, and Random Forest. The SVM algorithm demonstrates the highest accuracy of 98.97% with class balance, hyper-parameter optimization, and pre-processing techniques."
An Infinity of Pong: A Raspberry Pi Pico W handheld writes its own games,"Peiro, JAG",10.1109/MSPEC.2023.10040549,2023,"There is currently a lot of interest in AI tools designed to help programmers write software. GitHub's Copilot and Amazon's CodeWhisperer apply deep-learning techniques originally developed for generating natural-language text by adapting it to generate source code. The idea is that programmers can use these tools as a kind of auto-complete on steroids, using prompts to produce chunks of code that developers can integrate into their software."
TULAP - An Accessible and Sustainable Platform for Turkish Natural Language Processing Resources,"ÃskÃ¼darli, S; Sen, M; Akkurt, F; GÃ¼rbÃ¼z, M; GÃ¼ngÃ¶r, O; ÃzgÃ¼r, A; GÃ¼ngÃ¶r, T",,2023,"Access to natural language processing resources is essential for their continuous improvement. This can be especially challenging in educational institutions where the software development effort required to package and release research outcomes may be overwhelming and under-recognized. Access to well-prepared and reliable research outcomes is important both for their developers as well as the greater research community. This paper presents an approach to address this concern with two main goals: (1) to create an open-source easily deployable platform where resources can be easily shared and explored, and (2) to use this platform to publish open-source Turkish NLP resources (datasets and tools) created by a research lab. The Turkish Natural Language Processing (TULAP) was designed and developed as an easy-to-use platform to share dataset and tool resources which supports interactive tool demos. Numerous open access Turkish NLP resources have been shared on TULAP. All tools are containerized to support portability for custom use. This paper describes the design, implementation, and deployment of TULAP with use cases (available at https://tulap.cmpe.boun.edu.tr/). A short video demonstrating our system is available at https://figshare.com/articles/media/TULAP_Demo/22179047."
Detecting Bot on GitHub Leveraging Transformer-based Models: A Preliminary Study,"Zhang, J; Wu, XJ; Zhang, Y; Xu, SY",10.1109/APSEC60848.2023.00087,2023,"Bots are prevalent contributors in collaborative software development, necessitating accurate detection techniques. This preliminary study aims to leveraging public datasets and Transformer-based models (i.e., BERT, CodeBERT, RoBERTa, BART, and PLBART) for the bot detection task. Our experimental result reveals that CodeBERT achieves the highest performance, with an impressive accuracy score of 94.1%."
ReqGo: A Semi-Automated Requirements Management Tool,"Koh, SJ; Chua, FF",10.14716/ijtech.v14i4.5631,2023,"This study deals with issues of changes in requirements management by dealing with requirements ambiguity and prioritization. A hypothesis about the possibility of integrating machine learning techniques and requirements management processes has been proven. It highlights the efforts in automating requirements ambiguity identification, requirements classification, and prioritization considering multi-criteria in decision-making through the utilization of Natural Language Processing (NLP) techniques and Universal Sentence Encoder. Naive Bayes (NB) classifier has been applied with its remarkable performance on binarily classifying requirements. Although existing methods proved to improve one or two of the process significantly, it rarely integrates the whole requirements management activity. The proposed tool helps the development team to manage the requirements systematically. The prioritization algorithm is proved to work as expected by considering multiple constraints before calculating the priority value. Meanwhile, it identifies the ambiguity that exists in the requirement automatically. The ambiguity classifier successfully identifies 87.5% of requirements accurately. Possible future work could be done in improving the prioritization module by allowing automated estimation of priority value upon requirements change. Future work may extend the automation coverage by providing test case generation."
An Initial Exploration of New Approaches to Sight Singing and Ear Training Teaching in the Context of Artificial Intelligence,"Zhu, KY",10.47750/RAA/11.3.18,2023,"In recent years, artificial intelligence technology has ushered in profound changes in the field of education, presenting both opportunities and challenges. It can assist students in independent learning and critical thinking, guiding them to effectively utilize fragmented time and extending classroom teaching in terms of breadth and depth. Additionally, it comprehensively aids teachers in pre-class instructional material preparation, innovative classroom teaching methods, and precise teaching evaluation and feedback through comprehensive and objective data. This paper aims to explore a new paradigm for sight singing and ear training teaching under the backdrop of artificial intelligence by analyzing the numerous drawbacks in traditional pedagogy and the current status of AI based sight singing and ear training software development and utilization."
"IA, not only AI","Hegland, F",10.1145/3603163.3609036,2023,"This is a demo and overview of my work today, demonstrating my approach to what my mentor Doug Engelbart called 'Intelligence Augmentation'(IA), as opposed to only following the 'Artificial Intelligence' (AI) approach. The work has been implemented on the macOS platform, by my independent software development company 'The Augmented Text Company', in the form of the text tool 'Liquid', the word processor 'Author' and the PDF viewer 'Reader', using Visual- Meta to embed and read rich metadata in an open and robust manner."
Scaling Web API Integrations,"Chari, G; Sheffer, B; Branavan, SRK; D'ippolito, NPA",10.1109/ICSE-SEIP58684.2023.00007,2023,"In ASAPP, a company that offers AI solutions to enterprise customers, internal services consume data from our customers' web APIs. Implementing and maintaining integrations between our customers' APIs and internal services is a major effort for the company. In this paper, we present a scalable approach for integrating web APIs in enterprise software that is lightweight and semi-automatic. It leverages a combination of Ontology-Based Data Access architectures (OBDA), a Domain Specific Language (DSL) called IBL, Natural Language Processing (NLP) models, and Automated Planning techniques. The OBDA architecture decouples our platform from our customers' APIs via an ontology that acts as a single internal data access point. IBL is a functional and graphical DSL that enables domain experts to implement integrations, even if they don't have software development expertise. To reduce the effort of manually writing the IBL code, an NLP model suggests correspondences from each web API to the ontology. Given the API, ontology, and selected mappings for a set of desired fields from the ontology, we define an Automated Planning problem. The resulting policy is finally fed to a code synthesizer that generates the appropriate IBL method implementing the desired integration. This approach has been in production in ASAPP for 2 years with more than 300 integrations already implemented. Results indicate a approximate to 50% reduction in effort due to implementing integrations with IBL. Preliminary results on the IBL automatic code generation show an encouraging further approximate to 25% reduction so far."
E-commerce utilization analysis and growth strategy for smes using an artificial intelligence,"Zhong, YJ",10.3233/JIFS-232406,2023,"E-commerce is becoming a robust catalyst to enlarge the business actions and construct an active consumer based on emergence of a global economy. E-commerce is offering the opportunities for Small and Medium-sized Enterprises (SMEs) with limited resources to decrease the operating costs and improve the profitability by overcoming the operational problems. In addition, SMEs use e-commerce websitesas sales channels between the businesses, their competitor, and consumers. Between the success of e-commerce and manufacturing SMEs, however, the moderating influence of entrepreneurial competencies does not seem to be as significant. Hence, in this paper, Deep Convolutional Neural Network based onSales Prediction Model (DCNN-SPM) has been suggested for analyzing SME enterprises' e-commerce utilization and development. Consistent with the user decision-making requirements of online product sales, united with the impelling factors of online product sales in different SME industries and the benefits of Artificial Intelligence (AI), this study builds a sales prediction model appropriate for online products. Furthermore, it evaluates the model's adaptability to different types of online products. Our model can automatically extract the useful features from raw log data and predict the sales utilizing those extracted features by DCNN. The experimental outcomes show that our suggested DCNN-SPM has achieved a high customer satisfaction ratio of 98.7% and a customer is buying behaviour analysis of 97.6%."
Curriculum for a New Five-Year Academic Program in Intelligent Systems Engineering and Software Engineering,"Solo, AMG",10.1109/CSCI62032.2023.00280,2023,"This research paper proposes a curriculum for a five-year academic program with a bachelor's degree (honors) in intelligent systems engineering and software engineering and a master's degree in intelligent systems engineering and software engineering. This program includes courses in data structures, compiler design, operating system design, firmware design, database systems, computer graphics and virtual reality design, static and dynamic website design, development of chatbots and voice assistants, software engineering methodology, knowledge-based systems, fuzzy logic, neural networks, evolutionary computation, evolutionary multiojective optimization, machine learning, image processing, computer vision, pattern recognition, voice recognition, natural language processing, data science, control systems, intelligent control systems, robotics, digital signal processing, mathematics, engineering physics, biology, etc. These degrees will allow graduates to have a good understanding of all of the main branches of intelligent systems engineering and software engineering as well as other relevant subjects in electrical and computer engineering."
Induction of Co-existing Items Available in Distributed Version Control Systems for Software Development,"Ãzyer, S",10.34028/iajit/20/6/4,2023,"Software development in Open-Source Software systems (OSS) allow developers to share their code and modify other developers' code. That leads to collaboration in the development. They can either discuss on the items to be developed, including the errors and technical problems that were faced. One popular OSS platform is github which already has a large number of developers and projects. The data residing in the issues part of github is sufficiently large, complex and unstructured. It could be processed to find novel discoveries. This work concentrates on one selected project to be analyzed systematically. Routine Extract, Transform and Load (ETL) steps have been identified to clean the data before applying natural language processing for prioritizing and taking actions for the requirements. In a collaborative environment. Our work uses terms and guides developers for tracking the co-occurrence of the terms used together to help them focus on the important issues."
hsSpMV : A Heterogeneous and SPM-aggregated SpMV for SW26010-Pro many-core processor,"Pan, JS; Xiao, L; Tian, M; Wang, L; Yang, CC; Chen, RJ; Ren, ZH; Liu, AJ; Zhu, GH",10.1109/CCGRID57682.2023.00016,2023,"Sparse matrix vector multiplication (SpMV) is a critical performance bottleneck for numerical simulation and artificial intelligence training. The new generation of Sunway supercomputer is the advanced exascale supercomputer in China. The SW26010-Pro many-core processor renders itself as a competitive candidate for its attractive computational power in both numerical simulation and artificial intelligence training. In this paper, we propose a heterogeneous and SPM-aggregated SpMV kernel, specifically designed for the SW26010-Pro many-core processor. To fully exploit the computational power of the SW26010-Pro and balance the load of each core group(CG) during computation, we employ asynchronous computation workflow and propose the SPM-aggregated strategy and vector adaptive mapping algorithm. In addition, we propose the two-level data partition scheme to implement computational load balance. In order to improve memory access efficiency, we directly access memory via DMA controller to replace the discrete memory access. Using several optimizations, we achieve a 77.16x speedup compared to the original implementation. Our experimental results show that the hsSpMV yields up to 3.82x speedups on average compared to the SpMV kernel of the state-of-the-art Sunway math library xMath2.0."
A Viewpoint on Software Supply Chain Security: Are We Getting Lost in Translation?,"Melara, MS; Torres-Arias, S",10.1109/MSEC.2023.3316568,2023,Many of the adoption challenges in securing the software supply chain are largely caused by the language we use to describe risk and defenses and other sociocultural gaps. We shed light on the impacts of these gaps and opportunities to overcome them.
Automatic use case classification based on topic grouping for requirements engineering,"Vahabi, S; Hozhabri, A",10.1007/s11334-023-00535-0,2024,"Requirements Engineering (RE) is the fundamental concept in software engineering scope. Automatic analysis for identifying the requirements can decrease the time consumption, delay, and cost of software development as well. To achieve the best analysis of texts, Natural Language Processing helps to disambiguate the meanings to a great extent. Automatic RE can help much more because automatic RE has more accuracy as well as very low latency in the feature extraction, contrary to manual extraction. The current paper focuses on these issues and suggests a framework for the spontaneous detecting of requirements from customers' documents. The new framework applies to the first stage of requirements analysis. For this reason and according to the evaluation results, the study's new framework could detect topics, use cases, and topic groups by at least 97%, 92%, and 95%, respectively, on the customers' real requirement documents."
Conceptualising Software Development Lifecycle for Engineering AI Planning Systems,"Georgievski, I",10.1109/CAIN58948.2023.00019,2023,"Given the prominence of AI planning in research and industry, the development of AI planning software and its integration into production architectures are becoming important. However, building and managing planning software is a complex and expertise-dependent process without methodological support that would ensure AI planning applications have high quality and industrial strength. To that end, we propose a lifecycle for developing AI planning systems that consists of ten phases related to the design, development, and operation of planning systems."
Operating System Fingerprinting Tool Based on Classical Machine Learning Algorithms,"PÃ©rez-Jove, R; Munteanu, CR; Dorado, J; Pazos, A; VÃ¡zquez-Naya, J",10.23919/JNIC58574.2023.10205734,2023,"Operating System (OS) fingerprinting aims to identify the OS of a machine analysing its network traffic. Traditional OS fingerprinting tools use a rule-based approach to perform this task, analysing the traffic characteristics of the target machine and comparing these values against an OS signature database in order to obtain a result. The problem with this approach arises when there is no signature in the database that matches the values of the tests carried out. This situation can occur due to configuration changes, installation of hardening tools or the appearance of new OSs. In this sense, the application of Artificial Intelligence (AI) techniques to this task has shown to offer good results, while it can solve some of the problems exposed. In this context, this work proposes a new OS fingerprinting tool, modular and easily extensible, that bases its operation on the application of AI models. The tool is composed of different modules that implement the fingerprinting process: a capture module, capable of collecting and processing network traffic both actively and passively, an AI classifier module, which generates signatures from the traffic and identify their OS applying AI models, and one user interface, to interact with the user. The ML model which classifies the OSs was developed applying classical ML algorithms to the p0f OS signature database."
Design and evaluation of an intelligent physical examination system in improving the satisfaction of patients with chronic disease,"Chen, X; Duan, RX; Shen, Y; Jiang, H",10.1016/j.heliyon.2023.e23906,2024,"Background: and Purpose: Enhancing patient satisfaction remains crucial for healthcare quality. The utilization of artificial intelligence (AI) in the Internet of Health Things (loHT) can streamline the medical examination process. Most Traditional Chinese Medicine (TCM) examinations are non-invasive and contribute significantly to patient satisfaction. Our aim was to establish an intelligent physical examination system that amalgamates TCM and Western medicine and to conduct a preliminary investigation into its effectiveness in enhancing the satisfaction of patients with chronic diseases. Materials and methods: Experts from clinical departments, the equipment department, and the software development department were invited to participate in group discussions to determine the design principles and organizational structure of the intelligent physical examination system. This system integrates TCM and Western medicine. We compared the satisfaction levels of patients examined using the intelligent physical examination system with those examined using the traditional medical examination system. Results: An intelligent physical examination system, combining TCM and Western medicine, was developed. A total of 106 patients were finally enrolled (intelligent group vs. control group) to evaluate satisfaction. There were no statistically significant differences between the intelligent group and the control group in age, gender, education, or income level. We identified significant differences in five aspects of satisfaction: 1) the physical examination environment; 2) the attitude and responsiveness of doctors; 3) the attitude and responsiveness of nurses; 4) the effectiveness of obtaining results; and 5) the information regarding physical examination and medical advice (p < 0.05). Furthermore, these differences remained statistically significant even after adjusting for age, gender, education, and income level.Conclusions: The intelligent physical examination system effectively capitalized on the advantages of combining AI with the integration of TCM and Western medicine, substantially optimizing the medical examination process. In comparison to the traditional physical examination system, the intelligent system significantly enhanced patient satisfaction. Future improvements could involve integrating chronic disease follow-up technology into the system."
Improving Code Refinement for Code Review Via Input Reconstruction and Ensemble Learning,"Lu, JW; Tang, ZJ; Liu, ZX",10.1109/APSEC60848.2023.00026,2023,"Code review is crucial for ensuring the quality of source code in software development. Automating the code review process is essential to save time and reduce costs, as manually reviewing code can be time-consuming and challenging for developers. Code refinement, an important task for automating code review, aims to automatically modify the code under review to address reviewers' comments. Previous research has fine-tuned pre-trained models like CodeT5 and CodeReviewer for code refinement, showing promising results. However, fine-tuning these models can make them forget the knowledge learned during pre-training and lead to suboptimal performance. To overcome this challenge, we employ an information retrieval method to enable the model to recall its learned knowledge. Furthermore, we propose using prompt templates to reconstruct the input and align the formats of the input data used during fine-tuning and pre-training, thus alleviating knowledge forgetting. Multiple models are created using the retrieval reconstruction and prompt reconstruction methods mentioned above, which are highly complementary. An ensemble learning method is employed to identify the most promising output from the outputs of these models. Our ensemble model achieves an Exact Match (EM) score of 36.32, surpassing the state-of-the-art CodeReviewer model by 19.3% and the popular GPT-3.5-Turbo model by 49.6%."
GA-SCS: Graph-Augmented Source Code Summarization,"Zhang, ML; Zhou, G; Yu, WT; Huang, NB; Liu, WF",10.1145/3554820,2023,"Automatic source code summarization system aims to generate a valuable natural language description for a program, which can facilitate software development and maintenance, code categorization, and retrieval. However, previous sequence-based research did not consider the long-distance dependence and highly structured characteristics of source code simultaneously. In this article, we present a Transformer-based GraphAugmented Source Code Summarization (GA-SCS), which can effectively incorporate inherent structural and textual features of source code to generate an effective code description. Specifically, we develop a graphbased structure feature extraction scheme leveraging abstract syntax tree and graph attention networks to mine global syntactic information. And then, to take full advantage of the lexical and syntactic information of code snippets, we extend the original attention to a syntax-informed self-attention mechanism in our encoder. In the training process, we also adopt a reinforcement learning strategy to enhance the readability and informativity of generated code summaries. We utilize the Java dataset and Python dataset to evaluate the performance of different models. Experimental results demonstrate that our GA-SCS model outperforms all competitive methods on BLEU, METEOR, ROUGE, and human evaluations."
Aligning Documentation and Q&A Forum through Constrained Decoding with Weak Supervision,"Pudari, R; Zhou, SY; Ahmed, I; Dai, ZY; Zhou, SR",10.1109/ICSME58846.2023.00043,2023,"Stack Overflow (SO) is a widely used question-andanswer (Q&A) forum dedicated to software development. It plays a supplementary role to official documentation (DOC for short) by offering practical examples and resolving uncertainties. However, the process of simultaneously consulting both the documentation and SO posts can be challenging and time-consuming due to their disconnected nature. In this study, we propose DOSA, a novel approach to automatically align SO and DOC, which inject domain-specific knowledge about the DOC structure into large language models (LLMs) through weak supervision and constrained decoding, thereby enhancing knowledge retrieval and streamlining task completion during the software development procedure. Our preliminary experiments find that DOSA outperforms various widely-used baselines, showing the promise of using generative retrieval models to perform low-resource software engineering tasks."
Verification and Validation for a Project Information Model Based on a Blockchain,"Serrano, W; Barnett, J",10.1007/978-981-19-1610-6_19,2023,"Agile project management based on minimum viable products has some benefits against the traditional waterfall method. Agile supports an early return of investment that supports circular reinvesting and makes the product more adaptable to variable social-economical environments. However, agile also presents some intrinsic issues due to its iterative approach. Project information requires an efficient record of the requirements, aims, governance not only for the investors, owners or users but also to keep evidence in future health and safety and other statutory compliance-related issues. In order to address the agile project management issues and address new safety regulations, this paper proposes a project information model (PIM) based on a distributed ledger technology (DLT) with a ranked procedure for the verification and validation (V&V) of data. Each V&V phase inserts a process of authenticity, data abstraction and analytics that adds value to the information founded on artificial intelligence (AI) and natural language processing (NLP). The underlying DLT consists of smart contracts embedded on a private Ethereum blockchain. This approach supports a decentralised approach in which every project stakeholder owns, manages and stores the data. The presented model is validated in a real scenario: University College London-Real Estate-Pearl Project."
Modern Metrics (MM): Software size estimation using function points for artificial intelligence and data analytics applications and finding the effort modifiers of the functional units using indian software industry,"Dhas, JTM; Midhunchakravarthy, J",10.47974/JDMSC-1734,2023,"SPM, Software Project Management is an engineering process for scheduling, developing, testing, and maintaining a software system. The size of a software is one of the important attributes in determining all the activities of SPM. Price, time, and effort are the basic factors for developing software in a successful and profitable manner. The fast explosions in technological development of software systems attracted all the fields that existed in the universe. The modern software system uses many programming languages, domains, operating systems, technologies, development cycles, topologies, etc. So, the industry is expecting a new, versatile, and highly updated sizing technique for modern software. Modern Metrics (MM) is a new anticipated size estimation technique for present software systems with updated functional values and metrics using Function Point (FP). This article analyzed the functional unit metric values (effort modifiers) of the MM with real world applications. MM is a novel method for determining the solution to the SPM challenges."
Towards AI-Driven Software Development: Challenges and Lessons from the Field (Keynote),"Yahav, E",10.1145/3611643.3633451,2023,"AI is changing the way we develop software. AI is becoming powerful enough to change the nature of interaction between humans and machines and not only to raise the level of abstraction. AI-driven software development is poised to transform the entire software development lifecycle (SDLC). As we move towards AI-driven software development, we must revisit some fundamental assumptions and address the following challenges: center dot How does the SDLC change when autonomous agents can handle some tasks? What is the role of code and version control? center dot Interaction model: What is the right human-machine interaction? How do we best communicate intent to the AI? How to best consume results? center dot Contextual awareness: How do we make the AI contextually aware of our development environment? Can we make the AI hyper-local and tailored to our problem and solution domains? center dot Trust: How can we trust the suggested results? How can we trust results that are not provided as code? In this talk, we will start with practical AI-assisted software development, including lessons from the field, based on our experience serving millions of users with Tabnine. We will cover different tasks in the SDLC and various techniques for addressing them in the face of the challenges above."
Hybrid Approach on the Project Development Within Automotive Industry,"Popa, O; Fagarasan, C; Pisla, A; Mihele, C; Felician, R",10.1007/978-3-031-15602-1_20,2023,"The automotive industry is one of the most spread and synergic industries in dynamic markets, gathering around many different sectors from material extractions to robotics and cloud computing. The current research analyses the influences and particularities of project management and identifies the challenges and advantages of the two principal methodologies used in the software development projects within the automotive industry, the Agile and the Waterfall ones. Once with I4.0 and worldwide digitalization, the global software development industry becomes a very significant and dynamic pillar in the contemporary environment. It is more and more evident that almost all devices and machines in our lives become interconnected and more intelligent (software-driven). From ordinary smartphones to complex, innovative airplanes, the automotive industry is part of global digitalization. The software development industry evolution has two essential characteristics: agility and speed. These aspects have a real impact in all other industries, including software development, practically in any product or process development. In the last 30 years, automotive development started to focus significantly on vehicle's brain capabilities. In the automotive world, automation and controls represented the forerunner of the software development industry, but today, logic and the car's computers started to work with artificial intelligence. The complexity and the development dimensions are many times increased, and the industry took significant steps forward. The safety aspects of the vehicle are crucial for the entire product development. For this reason, the industry has many regulations and standards that ensure the product's quality and safety. Therefore, in automotive, the software development is not aligned with the general software development from the speed and agility point of view and the paper shows a possible harmonization of the methodology in automotive software Project development through a hybrid solution."
Impact of Machine Learning on Software Development Life Cycle,"Navaei, M; Tabrizi, N",10.5220/0011997200003464,2023,"This research concludes an overall summary of the publications so far on the applied Machine Learning (ML) techniques in different phases of Software Development Life Cycle (SDLC) that includes Requirement Analysis, Design, Implementation, Testing, and Maintenance. We have performed a systematic review of the research studies published from 2015-2023 and revealed that Software Requirements Analysis phase has the least number of papers published; in contrast, Software Testing is the phase with the greatest number of papers published."
AI techniques and tools in Agile Software Development: Preliminary research,"Peras, D; Stapic, Z; Matijevic, M",,2023,"In recent years, there has been significant growth in research related to Agile software development (ASD), accompanied by a notable rise in the adoption of artificial intelligence (AI) tools and techniques. AI is believed to have the potential to bring about a transformation in agile software development, leading to improved product quality, increased production efficiency, and higher project success rates. Consequently, there is a compelling need to incorporate AI methods and tools into the agile software development process. This study presents the results of a literature review and answers research questions on AI techniques and tools, their purposes, and their benefits when used in the context of ASD. In a multi-phased process, a total of 374 documents were gathered and examined. 28 papers satisfied the inclusion and quality assessment requirements. A total of 7 different AI techniques were identified, of which machine learning (ML) was the one predominantly used. The purposes and benefits of AI techniques were identified and discussed. Recommendations for future research were given to tackle detected research gaps."
User acceptance test for software development in the agricultural domain using natural language processing,"Antonelli, L; Camilleri, G; Torres, D; Zarate, P",10.1080/12460125.2023.2229579,2024,"Software test case design is one of the most challenging activities since many actors with different backgrounds must cover most of the user's needs and expectations. In the agricultural domain tasks can be done in very different ways since practices vary worldwide. Thus, in this context, it is very hard to design test cases to validate requested functionality that automatizes some farm tasks. This paper proposes an approach to make the testing step easier, designing User Acceptance Tests (UATs) from requirements captured through scenarios. The scenarios capture the knowledge of different stakeholders (farmers) and using natural language processing tools, the approach proposed to consolidate the set of scenarios in a consistent and coherent base of knowledge organised in a tree, from where the design of test cases is extracted using the Task/Method model, a tool from the Artificial Intelligence."
Using AI to Manage Project Deadlines-Case Study of a Global Human Capital Management (HCM) Software Company,"Sheoraj, Y; Sungkur, RK",10.1007/978-981-19-3590-9_35,2023,"Nowadays, organizations are constantly evolving in the way they manage their projects while innovating and automating certain tasks and processes for a smoother transition of projects. This is being achieved through the integration of artificial intelligence (AI) and machine learning. AI is continuously driving businesses to success and modernizing the world of projects. The research is based on a global human capital management company as a case study to investigate the issues that they face to meet their software project deadlines and then proposing a framework using AI to overcome their challenges. The mixed methods research with both qualitative and quantitative design was opted for this research. A systematic literature review (SLR) was carried out initially to identify the challenges involved in meeting project deadlines as a primary method for data collection. Focus group interviews were then carried out to collect data from team members and project managers working on projects and facing challenges to meet project deadlines. The proposed AI framework was then evaluated and 72.7% of participants rated the framework as effective and were keen to integrate AI at their workplace for smooth transition of tasks. A final framework was then proposed after the feedback received from participants."
From programming-to-modeling-to-prompts smart ubiquitous applications,"Khalfi, MF; Tabbiche, MN; Adjoudj, R",10.3233/AIS-220355,2023,"Since its introduction by Mark Weiser, ubiquitous computing has received increased interest in the dawn of technological advancement. Supported by wireless technology advancement, embedded systems, miniaturization, and the integration of various intelligent and communicative devise, context-aware ubiquitous applications actively and intelligently use rich contextual information to assist their users. However, their designs are subject to continuous changes imposed by external factors. Nowadays, software engineering, particularly in the fields of Model-Driven Engineering, displays a strong tendency towards developing applications for pervasive computing. This trend is also fueled by the rise of generative artificial intelligence, paving the way for a new generation of no-code development tools and models specifically trained on open-source code repositories to generate applications from their descriptions. The specificities of our approach lies in starting with a graphical model expressed using a domain-specific language (DSL) composed of symbols and formal notations. This allows for graphically instantiating and editing applications, guiding and assisting experts from various engineering fields in defining ubiquitous applications that are eventually transformed into peculiar models. We believe that creating intelligent models is the best way to promote software development efficiency. We have used and evaluated recurrent neural networks, leveraging the recurrence of processing the same contextual information collected within this model, and enabling iterative adaptation to future evolutions in ubiquitous systems. We propose a prototype instantiated by our meta-model which tracks the movements of individuals who were positive for COVID-19 and confirmed to be contagious. Different deep learning models and classical machine learning techniques are considered and compared for the task of detection/classification of COVID-19. Results obtained from all techniques were evaluated with confusion matrices, accuracy, precision, recall and F1-score. In summary, most of the results are very impressive. Our deep learning approach used a RNN architecture produced up to 92.1% accuracy. With the recent development of OpenAI Codex, optimized for programming languages, we provided the same requirements to the Codex model and asked it to generate the source code for the COVID-19 application, comparing it with the application generated by our workshop."
Trust evaluation in virtual software development teams using BERT-based language models Evaluaci?n de confianza en equipos virtuales de desarrollo de software usando modelos de lenguajes basados en BERT,"Zapata, S; Gallardo, F; Sevilla, G; Torres, E; Forradellas, R",10.24215/16666038.23.e04,2023,"Nowadays, people from different geographical areas can be closely related thanks to advances in information and communication technologies. This has a greater impact in software development organizations where their members form virtual work teams. In these new co-located work scenarios, the construction of interpersonal trust is more complex and its impact is very relevant in the performance of software development teams. This paper presents the results of the performance evaluation of four pre-trained language models based on BERT applied to trust analysis tasks. For this work, a small dataset of 1453 comments obtained from software projects stored on Github was created. The evaluated language models achieved moderately good values, in the order of 0.84 for the F1-score metric, which augurs that with further research they could be significantly improved."
Exploring the Impact of AI technology Adoption on Productivity of Agile Software Development Teams with High Attrition: A Pilot Study,"Sathe, CA; Panse, C",,2023,"Purpose: This study explores the effect of the adoption of AI tools and frameworks on the productivity of Agile software development teams with high attrition rates. Design/methodology/approach: For this study, uses a combined theory approach to build and test a model for the effects of productivity factors along with the impact of AI technology on to the Agile software development team's productivity having a high attrition rate. A web-based survey is used to collect the sample data, which was eventually tested with the help of Smart PLS using partial least square methods and Bootstrapping. Findings: This study shows that the adoption of AI technology has a partial mediation effect on the Agile software development teams having a high attrition rate but could help sustain it by automating mundane tasks for predictable outputs. Research limitations/implications: Limited availability of data regarding factors of productivity, inadvertent sampling bias, and selecting the appropriate size of a data sample Originality: The study findings shows that the AI technologies and tools, will help teams to sustain their productivity even though there are constraints like high attrition."
Sustainability for Artificial Intelligence Products and Services - Initial How-to for IT Practitioners,"Lammert, D; Abdullai, L; Betz, S; Porras, J",,2023,"Year after year, software engineers celebrate new achievements in the field of AI. At the same time, the question about the impacts of AI on society remains insufficiently answered in terms of a comprehensive technology assessment. This article aims to provide software practitioners with a theoretically grounded and practically tested approach that enables an initial understanding of the potential multidimensional impacts. Subsequently, the results form the basis for discussions on AI software requirements. The approach is based on the Sustainability Awareness Framework (SusAF) and Participatory Design. We conducted three workshops on different AI topics: 1. Autonomous Driving, 2. Music Composition, and 3. Memory Avatars. Based on the results of the workshops we conclude that a two-level approach should be adopted: First, a broad one that includes a diverse selection of stakeholders and overall impact analysis. Then, in a second step, specific approaches narrowing down the stakeholders and focusing on one or few impact areas."
Self-attention Bi-RNN for developer emotion recognition based on EEG,"Wang, YD; Zheng, YH; Cao, L; Zhang, ZL; Ruan, QS; Wu, QF",10.1049/sfw2.12080,2023,"Intelligent software development is a new issue in software engineering study and practice that has attracted a lot of interest. It is based on the synergy between artificial intelligence (AI) and software engineering, and it tries to improve both software productivity and quality. EEG-based emotion recognition is promising to provide real-time feedback for developers for happy coding. More and more researchers are focussing on EEG-based emotion recognition for long-time emotional monitoring. This paper proposes an emotional EEG classification method called Self-Attention Bidirectional Recurrent Neural Network (SABi-RNN). One second EEG data is treated as a sample, and each sample is regarded as a sentence, and the number of channels is regarded as the embedding dimension of the word. A novel embedding method is applied to optimise features, and an attention layer to visualise the embedding of raw data. The method proposed is tested with the DEAP dataset and DREAMER dataset, the dependent-subject emotion recognition rate can reach about 0.942 and 0.982, and the currency of cross-subject emotion recognition gains 0.58 and 0.76 with few target data. Upon the attention model, we visualise the weight of attention in the time dimension and the 32-channel dimension, which can provide more intuitive suggestions to reduce channels. The impact of different regularisation methods, and frequency bandwidth is also discussed. The regularisation method we proposed plays an important role in classification, which improves the accuracy by 20% compared to no regularisation. With the Beta frequency band, in the subject-dependent sentiment classification paradigm, the sentiment classification accuracy rate is 0.96. In the independent-subject sentiment classification paradigm, sentiment classification with transfer learning obtains 0.59."
GAP-Gen: Guided Automatic Python Code Generation,"Zhao, JC; Song, YR; Wang, JL; Harris, IG",,2023,"Automatic code generation from natural language descriptions can be highly beneficial during the process of software development. In this work, we propose GAP-Gen, a Guided Automatic Python Code Generation method based on Python syntactic constraints and semantic constraints. We first introduce Python syntactic constraints in the form of Syntax-Flow, which is a simplified version of Abstract Syntax Tree (AST) reducing the size and high complexity of real python AST but maintaining crucial syntactic information of Python code. In addition to Syntax-Flow, we introduce Variable-Flow which abstracts variable and function names consistently throughout the code. In our work, rather than pretraining, we focus on modifying the finetuning process which reduces computational requirements but retains high generation performance on automatic Python code generation task. GAP-Gen fine-tunes the transformer-based language models T5 and CodeT5 using the Code-to-Docstring datasets CodeSearchNet, CodeSearchNet AdvTest and Code-Docstring-Corpus from EdinburghNLP. Our experiments show that GAP-Gen achieves better results on automatic Python code generation task than previous works. Our implementation is available on the github(1)."
USQA: A User Story Quality Analyzer prototype for supporting software engineering students,"JimÃ©nez, S; Alanis, A; BeltrÃ¡n, C; JuÃ¡rez-RamÃ­rez, R; RamÃ­rez-Noriega, A; Tona, C",10.1002/cae.22620,2023,"The Standish Group Reports 83.9% of IT Projects fail, and one of the top factors in failed projects is the incomplete requirements or user stories. Therefore, it is essential to teach undergraduate students from computer science degree programs how to create complete user stories. Computer science programs include some subjects or topics involving requirements or user stories collection and writing, such as Requirements Engineering, Software Engineering, Project Management, or Quality Software Assurance. For that reason, we designed a web application called User Story Quality Analyzer (USQA) that uses Natural Language Processing modules to detect errors regarding aspects of usefulness, completeness, and polysemes in the user stories creation. The tool was proved from three perspectives: (1) a reliability test, where 35 user stories developed by experts were tested in the app to prove the prototype's reliability; (2) usability and utility analysis; 48 students interacted with the tool and responded a Satisfaction Usability Scale and an open-ended question, the students reported a high usability score; (3) finally, error classification, we gathered 159 user stories processed by the system, and we classified the students' common errors considering incompleteness and polysemes. After the evaluations, we concluded that USQA could evaluate the user stories as an expert, which could help the professors/teachers/instructors in their courses by providing feedback to the students when they are writing user stories."
A Unified Software and Hardware Platform for Machine Learning Aided Wireless Systems,"Ichwana Putra, D; Harry Bintang Pratama, M; Isshiki, R; Nagao, Y; Lanante, L Jr; Ochi, H",10.1587/transfun.2023SDP0006,2023,"This paper presents a unified software and hardware wireless AI platform (USHWAP) for developing and evaluating machine learning in wireless systems. The platform integrates multi-software development such as MATLAB and Python with hardware platforms like FPGA and SDR, allowing for flexible and scalable device and edge computing application development. The USHWAP is implemented and validated using FPGAs and SDRs. Wireless signal classification, wireless LAN sensing, and rate adaptation are used as examples to showcase the platform's capabilities. The platform enables versatile development, including software simulation and real-time hardware implementation, offering flexibility and scalability for multiple applications. It is intended to be used by wireless-AI researchers to develop and evaluate intelligent algorithms in a laboratory environment."
COMET: A ML-Based Tool for Evaluating the Effectiveness of Software Design Communication,"Lindgren, MH; Persson, J; Jolak, R; Dobslaw, F",10.1109/MODELS-C59198.2023.00119,2023,"Communication is one of the most demanding activities in software development. The effectiveness of communication can be measured by analyzing three interpersonal communication dimensions: Active Discussion, Creative Conflict, and Conversation Management. Previous work relied on manually labeling the communication dimensions to analyze the effectiveness of software design discussions, a process that is time-consuming and not applicable to real-time use. In this study, natural language processing and supervised machine learning are used to create COMET, a tool for automatic classification and assessment of the effectiveness of interpersonal communication during software design meetings. To determine the optimal classification approach, nine different classifiers are examined. The classifier model that performed the best is Random Forest which managed to achieve 93.66% accuracy, 93.76% precision, and 93.63% recall when trained and tested with a stratified 10-fold cross-validation technique."
Computer-Aided Evaluation for Argument-Based Certification,"Daw, Z; Wang, T; Oh, C; Low, M; Amundson, I; Wang, GQ; Melville, R; Nuzzo, P",10.1109/DASC58513.2023.10311322,2023,"Next-generation certification processes are expected to be influenced by two dominant trends: customizable certification, as advocated by Overarching Properties (OPs), and the continuous and rapid delivery approach from the coupling of software development and operations (DevOps). In light of these trends, certification processes must adapt to support the ongoing evaluation of assurance while accommodating the emergence of new evidence and evolving safety and security practices, particularly for systems enabled by artificial intelligence (AI). These requirements call for the development of computer-aided evaluation tools that automate repetitive tasks and offer visualization aids to assist evaluators and applicants in making informed decisions. This paper aims to address the challenges associated with assurance evaluation via a survey-based analysis and proposes a user interface that streamlines the evaluation of certification arguments."
On Retrofitting Provenance for Transparent and Fair Software - Drivers and Challenges,"Dietrich, J; Galster, M; Luczak-Roesch, M",10.1109/FairWare59297.2023.00007,2023,"There have been ongoing discussions about how to ensure transparency and fairness in software that utilise artificial intelligence (AI). However, transparency and fairness are not limited to AI. Modern (non-AI) software is often constructed in a black box fashion. This means, components and services provide some functionality, but details on how this is achieved are hidden. Common software development and design principles like encapsulation and information hiding promote this. Engineers often only look inside the black boxes when they need to fix problems, e.g., when tracing bugs or vulnerabilities. The demand for transparency has created a need to open those black boxes also to non-engineers. For instance, businesses need to demonstrate regulation compliance, and end users want to understand how systems make fair decisions that affect them. However, adding provenance (i.e., the ability to gather information about data and algorithms used in systems) to existing systems is invasive and costly, and current approaches to collect provenance data are not designed to expose data to end users. We argue that this requires provenance retrofitting, i.e., adding provenance capabilities to systems mechanically, and exposing provenance data through standard language and service application programming interfaces (APIs). This could facilitate an infrastructure that supports transparency, which then can in turn be used to create feedback mechanisms for users that in the long term can improve the fairness of software. In this paper we discuss drivers, objectives, key challenges and some possible approaches to provenance retrofitting."
User Stories and Natural Language Processing: A Systematic Literature Review,"Raharjana, IK; Siahaan, D; Fatichah, C",10.1109/ACCESS.2021.3070606,2021,"Context: User stories have been widely accepted as artifacts to capture the user requirements in agile software development. They are short pieces of texts in a semi-structured format that express requirements. Natural language processing (NLP) techniques offer a potential advantage in user story applications. Objective: Conduct a systematic literature review to capture the current state-of-the-art of NLP research on user stories. Method: The search strategy is used to obtain relevant papers from SCOPUS, ScienceDirect, IEEE Xplore, ACM Digital Library, SpringerLink, and Google Scholar. Inclusion and exclusion criteria are applied to filter the search results. We also use the forward and backward snowballing techniques to obtain more comprehensive results. Results: The search results identified 718 papers published between January 2009 to December 2020. After applying the inclusion/exclusion criteria and the snowballing technique, we identified 38 primary studies that discuss NLP techniques in user stories. Most studies used NLP techniques to extract aspects of who, what, and why from user stories. The purpose of NLP studies in user stories is broad, ranging from discovering defects, generating software artifacts, identifying the key abstraction of user stories, and tracing links between model and user stories. Conclusion: NLP can help system analysts manage user stories. Implementing NLP in user stories has many opportunities and challenges. Considering the exploration of NLP techniques and rigorous evaluation methods is required to obtain quality research. As with NLP research in general, the ability to understand a sentence's context continues to be a challenge."
Interrogating Algorithmic Bias: From Speculative Fiction to Liberatory Design,"Gaskins, N",10.1007/s11528-022-00783-0,2023,"This paper reviews algorithmic or artificial intelligence (AI) bias in education technology, especially through the lenses of speculative fiction, speculative and liberatory design. It discusses the causes of the bias and reviews literature on various ways that algorithmic/AI bias manifests in education and in communities that are underrepresented in EdTech software development. While other recent work has responded to mainstream or private sector technology development, this review looks elsewhere where practitioners, artists, and activists engage underrepresented communities in brainstorming processes to identify and solve tough challenges. Their creative work includes films, toolkits, applications, prototypes and other physical artifacts, and other future-facing ideas that can provide guideposts for private sector development. Acknowledging the gaps in what has been studied, this paper proposes a different approach that includes speculative and liberatory design thinking, which can help developers better understand the educational and personal contexts of underrepresented groups. Early efforts to advocate for fairness and equity in AI and EdTech by groups such as the Algorithmic Justice League, the EdTech Equity Project, and EdSAFE AI Alliance is also explored."
A model-driven approach to machine learning and software modeling for the IoT Generating full source code for smart Internet of Things (IoT) services and cyber-physical systems (CPS),"Moin, A; Challenger, M; Badii, A; GÃ¼nnemann, S",10.1007/s10270-021-00967-x,2022,"Models are used in both Software Engineering (SE) and Artificial Intelligence (AI). SE models may specify the architecture at different levels of abstraction and for addressing different concerns at various stages of the software development life-cycle, from early conceptualization and design, to verification, implementation, testing and evolution. However, AI models may provide smart capabilities, such as prediction and decision-making support. For instance, in Machine Learning (ML), which is currently the most popular sub-discipline of AI, mathematical models may learn useful patterns in the observed data and can become capable of making predictions. The goal of this work is to create synergy by bringing models in the said communities together and proposing a holistic approach to model-driven software development for intelligent systems that require ML. We illustrate how software models can become capable of creating and dealing with ML models in a seamless manner. The main focus is on the domain of the Internet of Things (IoT), where both ML and model-driven SE play a key role. In the context of the need to take a Cyber-Physical System-of-Systems perspective of the targeted architecture, an integrated design environment for both SE and ML sub-systems would best support the optimization and overall efficiency of the implementation of the resulting system. In particular, we implement the proposed approach, called ML-Quadrat, based on ThingML, and validate it using a case study from the IoT domain, as well as through an empirical user evaluation. It transpires that the proposed approach is not only feasible, but may also contribute to the performance leap of software development for smart Cyber-Physical Systems (CPS) which are connected to the IoT, as well as an enhanced user experience of the practitioners who use the proposed modeling solution."
An Authoritative Study on the Near Future Effect of Artificial Intelligence on Project Management Knowledge Areas,"Fridgeirsson, TV; Ingason, HT; Jonasson, HI; Jonsdottir, H",10.3390/su13042345,2021,"The purpose of this study is to explore how Artificial Intelligence (AI) might augment the project management profession in each of the 10 categories of project management knowledge areas, as defined in the Project Management Body of Knowledge (PMBOK) of the Project Management Institute (PMI). In a survey, a group of project management experts were asked to state their insights into AI's likely effect on project management in the next 10 years. Results clearly illustrated that AI will be an integrated part of future project management practice and will affect project management knowledge areas in the near future. According to these findings, the management of cost, schedule, and risk, in particular, will be highly affected by AI. The research indicates that AI is very useful for processes where historical data is available and can be used for estimation and planning. In addition, it is clear that AI can monitor schedules, adjust forecasts, and maintain baselines. According to the findings, AI will have less impact in knowledge areas and processes that require human leadership skills, such as developing and managing teams and the management of stakeholders. The results indicate proprietarily the project management knowledge areas as defined by PMI that AI is likely to augment and sustain."
An NLP-Based Architecture for the Autocompletion of Partial Domain Models,"BurgueÃ±o, L; ClarisÃ³, R; GÃ©rard, S; Li, S; Cabot, J",10.1007/978-3-030-79382-1_6,2021,"Domain models capture the key concepts and relationships of a business domain. Typically, domain models are manually defined by software designers in the initial phases of a software development cycle, based on their interactions with the client and their own domain expertise. Given the key role of domain models in the quality of the final system, it is important that they properly reflect the reality of the business. To facilitate the definition of domain models and improve their quality, we propose to move towards a more assisted domain modeling building process where an NLP-based assistant will provide autocomplete suggestions for the partial model under construction based on the automatic analysis of the textual information available for the project (contextual knowledge) and/or its related business domain (general knowledge). The process will also take into account the feedback collected from the designer's interaction with the assistant. We have developed a proof-of-concept tool and have performed a preliminary evaluation that shows promising results."
Project management: openings for disruption from AI and advanced analytics,"Niederman, F",10.1108/ITP-09-2020-0639,2021,"Purpose The purpose of this essay is to illustrate how project management pull and AI or analytics technology push are likely to result in incremental and disruptive evolution of project management capabilities and practices. Design/methodology/approach This paper is written as a critical essay reflecting the experience and reflections of the author with many ideas drawn from and extending selected items from project management, artificial intelligence (AI) and analytics literatures. Findings Neither AI nor sophisticated analytics is likely to elicit hands on attention from project managers, other than those producing AI or analytics-based artifacts or using these tools to create their products and services. However, through the conduit of packaged software support for project management, new tools and approaches can be expected to more effectively support current activities, to streamline or eliminate activities that can be automated, to extend current capabilities with the availability of increased data, computing capacity and mathematically based algorithms and to suggest ways to reconceive how projects are done and whether they are needed. Research limitations/implications This essay includes projections of possible, some likely and some unlikely, events and states that have not yet occurred. Although the hope and purpose are to alert readers to the possibilities of what may occur as logical extensions of current states, it is improbable that all such projections will come to pass at all or in the way described. Nonetheless, consideration of the future ranging from current trends, the interplay among intersecting trends and scenarios of future states can sharpen awareness of the effects of current choices regarding actions, decisions and plans improving the probability that the authors can move toward desired rather than undesired future states. Practical implications Project managers not involved personally with creating AI or analytics products can avoid mastering detailed skill sets in AI and analytics, but should scan for new software features and affordances that they can use enable new levels of productivity, net benefit creation and ability to sleep well at night. Originality/value This essay brings together AI, analytics and project management to imagine and anticipate possible directions for the evolution of the project management domain."
Collaborative APIs recommendation for Artificial Intelligence of Things with information fusion,"Xu, YS; Wu, YC; Gao, HH; Song, SL; Yin, YY; Xiao, XC",10.1016/j.future.2021.07.004,2021,"With the rapid development of Artificial Intelligence of Things (AIoT), many applications are developed and deployed, especially mobile applications and edge applications. Many softwares are developed to support such diverse applications. To facilitate the development of softwares for AIoT, developers and programmers usually rely on the employment of the mature application programming interfaces (APIs). However, it is difficult for developers to select the most suitable APIs to finish the development, decreasing the development efficiency and delaying the development schedule. To solve these problems, in this paper, we propose a collaborative framework for APIs recommendation for AIoT, and also propose a joint matrix factorization technique for information fusion to fully use different types of information in AIoT. The built framework uses the invocation records among users and APIs during the development. Using collaborative technologies, we yield the similarity relationships among users and among APIs and build three novel APIs recommendation models. We collected a real-world dataset and performed sufficient experiments. The experimental results demonstrate that our models produce superior recommendation accuracy. (C) 2021 Elsevier B.V. All rights reserved."
Systematic Literature Mapping of User Story Research,"Amna, AR; Poels, G",10.1109/ACCESS.2022.3173745,2022,"User stories are a widely used artifact in Agile software development. Currently, only a limited number of secondary studies have reviewed the research on the user story technique. These research reviews focused on specific research topics related to ambiguity of requirements, effort estimation, and the application of Natural Language Processing. To our knowledge, a systematic mapping of all user story research has not been performed. To this end, we study the academic literature to investigate what user stories research has been performed, what types of problems have been identified, what sort of solutions or other types of research outcomes have been achieved, how mature the research is, and what research gaps exist. We followed Systematic Mapping Study guidelines to synthesize the currently available academic research on user stories. In total, we found 186 unique peer-reviewed studies, published in the period 2001-2021. We observed that research on the user story technique and its use had grown exponentially over the last seven years. Further, using a five-dimensional classification framework- requirements engineering activity, problem class, outcome class, type of research, type of publication- we observed several patterns in the classification of these studies across the different framework dimensions, which provided insights into the state-of-the-art and maturity of the research. We also identified four research gaps: the paucity of focused literature reviews; a lack of research on the role that user stories play in human cognition and interaction; a lack of comprehensive and mature solutions for resolving ambiguity issues with user stories early in the project; and a lack of validation and evaluation of proposed solutions. Several research opportunities are suggested, making our paper a useful reference for future research on user stories allowing researchers to clearly position their contributions."
Efficient Edge-AI Application Deployment for FPGAs,"Kalapothas, S; Flamis, G; Kitsos, P",10.3390/info13060279,2022,"Field Programmable Gate Array (FPGA) accelerators have been widely adopted for artificial intelligence (AI) applications on edge devices (Edge-AI) utilizing Deep Neural Networks (DNN) architectures. FPGAs have gained their reputation due to the greater energy efficiency and high parallelism than microcontrollers (MCU) and graphical processing units (GPU), while they are easier to develop and more reconfigurable than the Application Specific Integrated Circuit (ASIC). The development and building of AI applications on resource constraint devices such as FPGAs remains a challenge, however, due to the co-design approach, which requires a valuable expertise in low-level hardware design and in software development. This paper explores the efficacy and the dynamic deployment of hardware accelerated applications on the Kria KV260 development platform based on the Xilinx Kria K26 system-on-module (SoM), which includes a Zynq multiprocessor system-on-chip (MPSoC). The platform supports the Python-based PYNQ framework and maintains a high level of versatility with the support of custom bitstreams (overlays). The demonstration proved the reconfigurabibilty and the overall ease of implementation with low-footprint machine learning (ML) algorithms."
Catlss: An Intelligent Tool for Categorizing Issues Reports using Transformers,"Izadi, M",10.1145/3528588.3528662,2022,"Users use Issue Tracking Systems to keep track and manage issue reports in their repositories. An issue is a rich source of software information that contains different reports including a problem, a request for new features, or merely a question about the software product. As the number of these issues increases, it becomes harder to manage them manually. Thus, automatic approaches are proposed to help facilitate the management of issue reports. This paper describes Catlss, an automatic Categorizer of Issue reports which is built upon the Transformer-based pre-trained RoBERTa model. Catlss classifies issue reports into three main categories of Bug report, Enhancement/feature request, and Question. First, the datasets provided for the NLBSE tool competition are cleaned and preprocessed. Then, the pre-trained RoBERTa model is fine-tuned on the preprocessed dataset. Evaluating Catlss on about 80 thousand issue reports from GitHub, indicates that it performs very well surpassing the competition baseline, TicketTagger, and achieving 87.2% Fl-score (micro average). Additionally, as Catlss is trained on a wide set of repositories, it is a generic prediction model, hence applicable for any unseen software project or projects with little historical data. Scripts for cleaning the datasets, training Catlss and evaluating the model are publicly available.(1)"
Artificial Intelligence in Software Requirements Engineering: State-of-the-Art,"Liu, KH; Reddivari, S; Reddivari, K",10.1109/IRI54793.2022.00034,2022,"Requirements Engineering (RE) is a very important activity in the software development life cycle. Poorly executed RE steps can result in poor quality software and expensive maintenance cost. Although researchers have previously related and applied artificial intelligence (AI) to RE, little is known about the specific role of AI in RE process. In particular, there are insufficient understandings about how AI should be incorporated in the RE process to produce high quality, clear and detailed requirements. In this paper, we present the current state-of-the-art of AI in RE. We reviewed the literature published between January 2015 to December 2021 in order to understand how the state of the art of AI branches such as machine learning, classification, and natural language processing (NLP) has advanced the field of RE. Each recent study is summarized and the advancement to the RE field is presented. There is an apparent direction of applying NLP techniques and supervised learning techniques such as classification to requirements documents. This study provides a summary and direction of the AI applications in the field of RE."
Exploring the factors of students' intention to participate in AI software development,"Chen, SY; Su, YS; Ku, YY; Lai, CF; Hsiao, KL",10.1108/LHT-12-2021-0480,2024,"Purpose Although many universities have begun to provide artificial intelligence (AI)-related courses for students, the influence of the course on students' intention to participate in the development of AI-related products/services needs to be verified. In order to explore the factors that influence students' participation in AI services and system development, this study uses self-efficacy, AI literacy, and the theory of planned behaviour (TPB) to investigate students' intention to engage in AI software development. Design/methodology/approach The questionnaire was distributed online to collect university students' responses in central Taiwan. The research model and eleven hypotheses are tested using 151 responses. The testing process adopted SmartPLS 3.3 and SPSS 26 software. Findings AI programming self-efficacy, AI literacy, and course satisfaction directly affected the intention to participate in AI software development. Moreover, course playfulness significantly affected course satisfaction and AI literacy. However, course usefulness positively affected course satisfaction but did not significantly affect AI literacy and AI programming self-efficacy. Originality/value The model improves our comprehension of the influence of AI literacy and AI programming self-efficacy on the intention. Moreover, the effects of AI course usefulness and playfulness on literacy and self-efficacy were verified. The findings and insights can help design the AI-related course and encourage university students to participate in AI software development. The study concludes with suggestions for course design for AI course instructors or related educators."
Guideline for software life cycle in health informatics,"Hauschild, AC; Martin, R; Holst, SC; Wienbeck, J; Heider, D",10.1016/j.isci.2022.105534,2022,"The long-lasting trend ofmedical informatics is to adapt novel technologies in the medical context. In particular, incorporating artificial intelligence to support clinical decision-making can significantly improve monitoring, diagnostics, and prognostics for the patient's and medic's sake. However, obstacles hinder a timely technology transfer from research to the clinic. Due to the pressure for novelty in the research context, projects rarely implement quality standards. Here, we propose a guideline for academic software life cycle processes tailored to the needs and capabilities of research organizations. While the complete implementation of a software life cycle according to commercial standards is not feasible in scientificwork, we propose a subset of elements thatwe are convinced will provide a significant benefit while keeping the effort within a feasible range. Ultimately, the emerging quality checks for academic software development can pave the way for an accelerated deployment of academic advances in clinical practice."
Work Systems in the Indian Information Technology (IT) Industry Delivering Artificial Intelligence (AI) Solutions and the Challenges of Work from Home,"Venumuddala, VR; Kamath, R",10.1007/s10796-022-10259-4,2023,"Our study is based on a workplace ethnography conducted between Jan-May 2020 in an AI research lab of an Indian service-based IT organization, whose operations shifted from co-located work to work from home (WFH) owing to the recent pandemic. The field notes of the ethnographer, working as a full-time intern in a running AI project within this lab, is the basis for the qualitative data for this study. We discuss the socio-technical aspects and the specific challenges of distributed team-working due to the WFH norms facing such emerging research units, which are rapidly diffusing across the IT industry in the offshoring context, particularly in India. We rely on work system theory as a map to bring out key findings from our ethnographic observations. The findings point to the importance of having workflows compatible with the specific work roles in such emerging work systems - particularly for the beginner roles in the AI space. Our study contributes to the IS literature by depicting the challenges of distributed teams in a relatively novel setting emerging in offshoring contexts like the Indian IT sector, and suggests implications for managers handling AI projects and tackling employee-focused Human Resource practices in such settings."
Fault-Aware Neural Code Rankers,"Inala, JP; Wang, CL; Yang, M; Codas, A; EncarnaciÃ³n, M; Lahiri, SK; Musuvathi, M; Gao, JF",,2022,"Large language models (LLMs) have demonstrated an impressive ability to generate code for various programming tasks. In many instances, LLMs can generate a correct program for a task when given numerous trials. Consequently, a recent trend is to do large scale sampling of programs using a model and then filtering/ranking the programs based on the program execution on a small number of known unit tests to select one candidate solution. However, these approaches assume that the unit tests are given and assume the ability to safely execute the generated programs (which can do arbitrary dangerous operations such as file manipulations). Both of the above assumptions are impractical in real-world software development. In this paper, we propose CODERANKER, a neural ranker that can predict the correctness of a sampled program without executing it. Our CODERANKER is fault-aware i.e., it is trained to predict different kinds of execution information such as predicting the exact compile/runtime error type (e.g., an IndexError or a TypeError). We show that CODERANKER can significantly increase the pass@1 accuracy of various code generation models (including Codex [11], GPT-Neo, GPT-J) on APPS [25], HumanEval [11] and MBPP [3] datasets."
Verification and Validation for data marketplaces via a blockchain and smart contracts,"Serrano, W",10.1016/j.bcra.2022.100100,2022,"Actual challenges with data in physical infrastructure include: 1) the adversity of its velocity based on access and retrieval, thus integration; 2) its value as its intrinsic quality; 3) its extensive volume with a limited variety in terms of systems; and finally, 4) its veracity, as data can be modified to obtain an economical advantage. Physical infrastructure design based on Agile project management and minimum viable products provides benefits against the traditional waterfall method. Agile supports an early return on investment that promotes circular reinvesting while making the product more adaptable to variable social-economical environments. However, Agile also presents inherent issues due to its iterative approach. Furthermore, project information requires an efficient re-cord of the aims, requirements, and governance not only for the investors, owners, or users but also to keep evidence in future health & safety and other statutory compliance. In order to address these issues, this article presents a Validation and Verification (V&V) model for data marketplaces with a hierarchical process; each data V&V stage provides a layer of data abstraction, value-added services, and authenticity based on Artificial Intel-ligence (AI). In addition, this proposed solution applies Distributed Ledger Technology (DLT) for a decentralised approach where each user keeps and maintains the data within a ledger. The presented model is validated in real data marketplace applications: 1) live data for the Newcastle Urban Observatory Smart City Project, where data are collected from sensors embedded within the smart city via APIs; 2) static data for University College London (UCL)-Real Estate-PEARL Project, where different project users and stakeholders introduce data into a Project Information Model (PIM)."
RETRACTED: Artificial Intelligence Technology Based on Deep Learning in Building Construction Management System Modeling (Retracted Article),"Wang, HB; Hu, Y",10.1155/2022/5602842,2022,"In order to explore the application of AI technology in construction management system modeling, the author proposed the application of a deep learning-based AI technology in construction management system modeling. The 3 D reconstruction deep learning model is first introduced, and then the model idea of the construction progress reliability control system is designed based on BIM (building information model). Second, the construction process of the 4dbim model is described, and the construction method is introduced. The construction of the model provides data information for the construction schedule reliability control system. Finally, the three functional modules of progress monitoring, progress reliability early warning, and progress prediction are realized by combining the S-curve comparison method, and the work of the system is described through case simulation. The early warning result is from June 7 to June 11, the progress deviation is between (-2%, 2%), and the progress is basically controlled. On June 13, the planned workload was 81.099%, and the actual cumulative workload was 7.099%, which was 4% less than the planned workload. The project progress was out of date, so it needs to be closely tracked. On June 15, the planned workload was 85.511%, and the actual cumulative workload was 80.899%, 4.5% less than the planned workload. The forecast result is the line forecast of the actual cumulative completion percentage on June 17. After calculation, the forecast result on June 17 is 84.311%. The progress deviation on June 17 was 5.21%. If no timely delay is taken on June 15, the delay will get worse. In addition, the system can predict the completion period of the project. When the actual percentage of cumulative completion is greater than or equal to 100%, it indicates that the project has been completed. Therefore, we can calculate the completion period of the three-storey project, and the construction period is about June 29 or 30. When the simulation can be carried out, the simulation number is set as 1000 times, the completion probability of the project is only 40%, and the completion probability is not too high. Artificial intelligence technology can realize progress monitoring, progress reliability early warning, and progress prediction. This system model prepares for software development and is conducive to improving the progress reliability control level of construction enterprises. Starting from the schedule planning subsystem and the schedule control subsystem, this paper studies the application of the artificial intelligence technology based on deep learning in the modeling of a building construction management system. The results show that this technology can effectively improve the efficiency of the building construction schedule management. Compared with the existing management methods, it shows great advantages in terms of operating costs and ease of use. It also promotes the application of artificial intelligence technology in the construction phase."
From Bit to Bedside: A Practical Framework for Artificial Intelligence Product Development in Healthcare,"Higgins, D; Madai, VI",10.1002/aisy.202000052,2020,"Artificial intelligence (AI) in healthcare holds great potential to expand access to high-quality medical care, while reducing systemic costs. Despite hitting headlines regularly and many publications of proofs-of-concept, certified products are failing to break through to the clinic. AI in healthcare is a multiparty process with deep knowledge required in multiple individual domains. A lack of understanding of the specific challenges in the domain is the major contributor to the failure to deliver on the big promises. Herein, a decision perspective framework for the development of AI-driven biomedical products from conception to market launch is presented. The framework highlights the risks, objectives, and key results which are typically required to navigate a three-phase process to market-launch of a validated medical AI product. Clinical validation, regulatory affairs, data strategy, and algorithmic development are addressed. The development process proposed for AI in healthcare software strongly diverges from modern consumer software development processes. Key time points to guide founders, investors, and key stakeholders throughout the process are highlighted. This framework should be seen as a template for innovation frameworks, which can be used to coordinate team communications and responsibilities toward a viable product development roadmap, thus unlocking the potential of AI in medicine."
A Hitchhiker's Guide to Model-Driven Engineering for Data-Centric Systems,"Combemale, B; Kienzle, J; Mussbacher, G; Ali, H; Amyot, D; Bagherzadeh, M; Batot, E; Bencomo, N; Benni, B; Bruel, JM; Cabot, J; Cheng, BHC; Collet, P; Engels, G; Heinrich, R; Jezequel, JM; Koziolek, A; Mosser, S; Reussner, R; Sahraoui, H; Saini, R; Sallou, J; Stinckwich, S; Syriani, E; Wimmer, M",10.1109/MS.2020.2995125,2021,The models and data framework demystifies the different roles that models and data play in software development and operation and clarifies where machine learning and artificial intelligence techniques could be used.
The State of the ML-universe: 10 Years of Artificial Intelligence & Machine Learning Software Development on GitHub,"Gonzalez, D; Zimmermann, T; Nagappan, N",10.1145/3379597.3387473,2020,"In the last few years, artificial intelligence (AI) and machine learning (ML) have become ubiquitous terms. These powerful techniques have escaped obscurity in academic communities with the recent onslaught of AI & ML tools, frameworks, and libraries that make these techniques accessible to a wider audience of developers. As a result, applying AI & ML to solve existing and emergent problems is an increasingly popular practice. However, little is known about this domain from the software engineering perspective. Many AI & ML tools and applications are open source, hosted on platforms such as GitHub that provide rich tools for large-scale distributed software development. Despite widespread use and popularity, these repositories have never been examined as a community to identify unique properties, development patterns, and trends. In this paper, we conducted a large-scale empirical study of AI & ML Tool (700) and Application (4,524) repositories hosted on GitHub to develop such a characterization. While not the only platform hosting AI & ML development, GitHub facilitates collecting a rich data set for each repository with high traceability between issues, commits, pull requests and users. To compare the AI & ML community to the wider population of repositories, we also analyzed a set of 4,101 unrelated repositories. We enhance this characterization with an elaborate study of developer workflow that measures collaboration and autonomy within a repository. We've captured key insights of this community's 10 year history such as it's primary language (Python) and most popular repositories (Tensorflow, Tesseract). Our findings show the AI & ML community has unique characteristics that should be accounted for in future research."
A Cloud-Based Computing Framework for Artificial Intelligence Innovation in Support of Multidomain Operations,"Robertson, J; Fossaceca, J; Bennett, K",10.1109/TEM.2021.3088382,2022,"The DoD's artificial intelligence (AI) strategy requires the delivery of transformative and disruptive capabilities that impact the character of the future battlefield and the pace of threats that US forces must be prepared to handle. Candidate frameworks must also address key mission areas while enabling partnerships with the private sector, academia, and global allies. To meet these challenges, a flexible, cost-effective, and scalable computing infrastructure that incorporates cutting edge technologies and complies with stringent information assurance requirements is necessary. The DoD AI strategy mandates the agile employment of innovative AI capabilities that rapidly and iteratively execute experimentation with new operating concepts, and leverage lessons learned in subsequent experiments. Using cloud computing, we present a flexible approach to solve complex systems problems. Promoting rapid experimentation and collaboration on problems such as recursive algorithm implementation, deep learning, and inference in neural networks has enabled inherent advantages over existing computing frameworks. Leveraging the cloud to implement shared responsibility security models, serverless architectures, and high-performance virtual machines, aspects of the AI lifecycle including build, deploy, and monitor have resulted in an adaptable and scalable computing framework that is not only disruptive to the current computing paradigm but also promotes enhanced and productive collaboration."
CaPBug-A Framework for Automatic Bug Categorization and Prioritization Using NLP and Machine Learning Algorithms,"Ahmed, HA; Bawany, NZ; Shamsi, JA",10.1109/ACCESS.2021.3069248,2021,"Bug reports facilitate software development teams in improving the quality of software. These reports include significant information related to problems encountered within a software, possible enhancement suggestions, and other potential issues. Bug reports are typically complex and are too detailed; hence a lot of resources are required to analyze and process them manually. Moreover, it leads to delays in the resolution of high priority bugs. Accurate and timely processing of bug reports based on their category and priority plays a significant role in improving the quality of software maintenance. Therefore, an automated process of categorization and prioritization of bug reports is needed to address the aforementioned issues. Automated categorization and prioritization of bug reports have been explored recently by many researchers; however, limited progress has been made in this regard. In this research, we present a novel framework, titled CaPBug, for automated categorization and prioritization of bug reports. The framework is implemented using Natural Language Processing (NLP) and supervised machine learning algorithms. A baseline corpus is built with six categories and five prioritization levels by analyzing more than 2000 bug reports of Mozilla and Eclipse repository. Four classification algorithms i.e., Naive Bayes, Random Forest, Decision Tree, and Logistic Regression have been used to categorize and prioritize bug reports. We demonstrate that the CaPBug framework achieved an accuracy of 88.78% by using a Random Forest classifier with a textual feature for predicting the category. Similarly, using the CaPBug framework, an accuracy of 90.43% was achieved in predicting the priority of bug reports. Synthetic Minority Over-Sampling Technique (SMOTE) has been applied to address the class imbalance issue in priority classes."
Quality-Aware DevOps Research: Where Do We Stand?,"Alnafessah, A; Ul Gias, A; Wang, RN; Zhu, LL; Casale, G; Filieri, A",10.1109/ACCESS.2021.3064867,2021,"DevOps is an emerging paradigm that reduces the barriers between developers and operations teams to offer continuous fast delivery and enable quick responses to changing requirements within the software life cycle. A significant volume of activity has been carried out in recent years with the aim of coupling DevOps stages with tools and methods to improve the quality of the produced software and the underpinning delivery methodology. While the research community has produced a sustained effort by conducting numerous studies and innovative development tools to support quality analyses within DevOps, there is still a limited cohesion between the research themes in this domain and a shortage of surveys that holistically examine quality engineering work within DevOps. In this paper, we address the gap by comprehensively surveying existing efforts in this area, categorizing them according to the stage of the DevOps lifecycle to which they primarily contribute. The survey holistically spans across all the DevOps stages, identify research efforts to improve architectural design, modeling and infrastructure-as-code, continuous-integration/continuous-delivery (CI/CD), testing and verification, and runtime management. Our analysis also outlines possible directions for future work in quality-aware DevOps, looking in particular at AI for DevOps and DevOps for AI software."
Smart Proxy Modeling of a Fractured Reservoir Model for Production Optimization: Implementation of Metaheuristic Algorithm and Probabilistic Application,"Ng, CSW; Ghahfarokhi, AJ; Amar, MN; TorsÃ¦ter, O",10.1007/s11053-021-09844-2,2021,"Numerical reservoir simulation has been recognized as one of the most frequently used aids in reservoir management. Despite having high calculability performance, it presents an acute shortcoming, namely the long computational time induced by the complexities of reservoir models. This situation applies aptly in the modeling of fractured reservoirs because these reservoirs are strongly heterogeneous. Therefore, the domains of artificial intelligence and machine learning (ML) were used to alleviate this computational challenge by creating a new class of reservoir modeling, namely smart proxy modeling (SPM). SPM is a ML approach that requires a spatio-temporal database extracted from the numerical simulation to be built. In this study, we demonstrate the procedures of SPM based on a synthetic fractured reservoir model, which is a representation of dual-porosity dual-permeability model. The applied ML technique for SPM is artificial neural network. We then present the application of the smart proxies in production optimization to illustrate its practicality. Apart from applying the backpropagation algorithms, we implemented particle swarm optimization (PSO), which is one of the metaheuristic algorithms, to build the SPM. We also propose an additional procedure in SPM by integrating the probabilistic application to examine the overall performance of the smart proxies. In this work, we inferred that the PSO had a higher chance to improve the reliability of smart proxies with excellent training results and predictive performance compared with the considered backpropagation approaches."
The Current State of Industrial Practice in Artificial Intelligence Ethics,"Vakkuri, V; Kemell, KK; Kultanen, J; Abrahamsson, P",10.1109/MS.2020.2985621,2020,High-level guidelines and tools for managing artificial intelligence (AI) ethics have been introduced to help industry organizations make more ethical AI systems. The results of a survey of 211 software companies provide insights into the current state of industrial practice.
A transformer-based IDE plugin for vulnerability detection,"Mamede, C; Pinconschi, E; Abreu, R",10.1145/3551349.3559534,2022,"Automatic vulnerability detection is of paramount importance to promote the security of an application and should be exercised at the earliest stages within the software development life cycle (SDLC) to reduce the risk of exposure. Despite the advancements with state-of-the-art deep learning techniques in software vulnerability detection, the development environments are not yet leveraging their performance. In this work, we integrate the Transformers architecture, one of the main highlights of advances in deep learning for Natural Language Processing, within a developer-friendly tool for code security. We introduce VDet for Java, a transformer-based VS Code extension that enables one to discover vulnerabilities in Java files. Our preliminary model evaluation presents an accuracy of 98.9% for multi-label classification and can detect up to 21 vulnerability types. The demonstration of our tool can be found at https://youtu.be/OjiUBQ6TdqE, and source code and datasets are available at https://github.com/TQRG/VDET-for-Java."
The Role of AI Chatbots in Mental Health Related Public Services in a (Post)Pandemic World: A Review and Future Research Agenda,"Damij, N; Bhattacharya, S",10.1109/TEMSCONEUROPE54743.2022.9801962,2022,"The purpose of this paper is to explore the advances in artificial intelligence (AI) chatbots as part of public services, mainly when applied to mental health in today's post-pandemic world. The adoption of AI chatbots to keep up with basic customer support business activities is based on extensive knowledge, both from the hard (software development) and the soft side (increasing the added value to the service/product). However, using chatbots as extenders of public services to support mental health in pandemic times is an emerging research topic. Hence, the paper identifies niche and under-explored research gaps in state-of-the-art literature, thus contributing to the body of academic knowledge. The paper adopts a design science approach to formulate the problem statement, articulate the objectives of target solutions, and propose a design and development framework for future mental health chatbots by employing an extensive literature review. Findings from this paper emphasize considerations of ethical issues and governance, purposeful and goal-oriented design, and AI-based technology as critical enablers for designing new mental health chatbots. The paper contributes to the knowledge by providing clear and structured future research priorities and offers a framework for designing more effective and intelligent mental health chatbots that public organizations and managers may find useful."
Information retrieval versus deep learning approaches for generating traceability links in bilingual projects,"Lin, JF; Liu, YL; Cleland-Huang, J",10.1007/s10664-021-10050-0,2022,"Software traceability links are established between diverse artifacts of the software development process in order to support tasks such as compliance analysis, safety assurance, and requirements validation. However, practice has shown that it is difficult and costly to create and maintain trace links in non-trivially sized projects. For this reason, many researchers have proposed and evaluated automated approaches based on information retrieval and deep-learning. Generating trace links automatically can also be challenging - especially in multi-national projects which include artifacts written in multiple languages. The intermingled language use can reduce the efficiency of automated tracing solutions. In this work, we analyze patterns of intermingled language that we observed in several different projects, and then comparatively evaluate different tracing algorithms. These include Information Retrieval techniques, such as the Vector Space Model (VSM), Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA), and various models that combine mono- and cross-lingual word embeddings with the Generative Vector Space Model (GVSM), and a deep-learning approach based on a BERT language model. Our experimental analysis of trace links generated for 14 Chinese-English projects indicates that our MultiLingual Trace-BERT approach performed best in large projects with close to 2-times the accuracy of the best IR approach, while the IR-based GVSM with neural machine translation and a monolingual word embedding performed best on small projects."
Automatic software vulnerability classification by extracting vulnerability triggers,"Sun, XB; Li, LL; Bo, LL; Wu, XX; Wei, Y; Li, B",10.1002/smr.2508,2024,"Vulnerability classification is a significant activity in software development and software maintenance. Natural Language Processing (NLP) techniques, which utilize the descriptions in public repositories, are widely used in automatic software vulnerability classification. However, vulnerability descriptions are ordinarily short and contain many technical terms, making them difficult for machines to automatically comprehend. In this paper, we present an approach based on vulnerability triggers to automatically classify vulnerabilities. First, we extract vulnerability triggers with Bert Question and Answer (Bert Q&A). Then, we use Recurrent Convolutional Neural Networks for Text classification (TextRCNN) to classify vulnerabilities based on Common Weakness Enumeration (CWE). We statistically perform an analysis of vulnerability triggers and comprehensively evaluate the classification performance of our approach on a set of 4769 prelabeled vulnerability entries, as well as compare it with state-of-the-art vulnerability classification approaches. Experiment results show that our approach can achieve a F1-measure of 95% on extraction and 80.8% on classification."
Using AI to develop a framework to prevent employees from missing project deadlines in software projects - case study of a global human capital management (HCM) software company,"Sheoraj, Y; Sungkur, RK",10.1016/j.advengsoft.2022.103143,2022,Nowadays Organizations are constantly evolving on the way they manage their projects while innovating and automating certain tasks and processes for a smoother transition of projects. This is being achieved through the integration of Artificial intelligence (AI) and Machine learning. AI is continuously driving businesses to success and modernizing the world of projects. The research is based on a global human capital management company as a case study to investigate on the issues that they face to meet their software project deadlines and then proposing a framework using AI to overcome their challenges. The mixed methods research with both qualitative and quantitative design was opted for this research. A systematic literature review (SLR) was carried out initially to identify the challenges involved in meeting project deadlines as a primary method for data collection. Focus group interviews were then carried out to collect data from team members and project managers working on projects and facing challenges to meet project deadlines. The proposed AI framework was then evaluated and 72.7% participants rated the framework as effective and was keen to integrate AI at their workplace for smooth transition of tasks. A final framework was then proposed after the feedback received from participants.
Towards Design and Feasibility Analysis of DePaaS: AI Based Global Unified Software Defect Prediction Framework,"Pandit, M; Gupta, D; Anand, D; Goyal, N; Aljahdali, HM; Mansilla, AO; Kadry, S; Kumar, A",10.3390/app12010493,2022,"Featured Application DePaaS has the potential to be used as a global, shared platform for availing software defects prediction services by choosing appropriate base project, defect prediction model and prediction granularity. Over time, DePaaS can potentially become a rich source of defects metadata and provide deep insights into developing efficient software defects prediction models. It can promote inter-agency collaboration, data sharing, continuous improvement, and further research into application of artificial intelligence, genetic programming, and other techniques for solving key problems of software engineering. Using artificial intelligence (AI) based software defect prediction (SDP) techniques in the software development process helps isolate defective software modules, count the number of software defects, and identify risky code changes. However, software development teams are unaware of SDP and do not have easy access to relevant models and techniques. The major reason for this problem seems to be the fragmentation of SDP research and SDP practice. To unify SDP research and practice this article introduces a cloud-based, global, unified AI framework for SDP called DePaaS-Defects Prediction as a Service. The article describes the usage context, use cases and detailed architecture of DePaaS and presents the first response of the industry practitioners to DePaaS. In a first of its kind survey, the article captures practitioner's belief into SDP and ability of DePaaS to solve some of the known challenges of the field of software defect prediction. This article also provides a novel process for SDP, detailed description of the structure and behaviour of DePaaS architecture components, six best SDP models offered by DePaaS, a description of algorithms that recommend SDP models, feature sets and tunable parameters, and a rich set of challenges to build, use and sustain DePaaS. With the contributions of this article, SDP research and practice could be unified enabling building and using more pragmatic defect prediction models leading to increase in the efficiency of software testing."
Supporting AI Engineering on the IoT Edge through Model-Driven TinyML,"Moin, A; Challenger, M; Badii, A; Gunnemann, S",10.1109/COMPSAC54236.2022.00140,2022,"Software engineering of network-centric Artificial Intelligence (AI) and Internet of Things (IoT) enabled Cyber-Physical Systems (CPS) and services, involves complex design and validation challenges. In this paper, we propose a novel approach, based on the model-driven software engineering paradigm, in particular the domain-specific modeling methodology. We focus on a sub-discipline of AI, namely Machine Learning (ML) and propose the delegation of data analytics and ML to the IoT edge. This way, we may increase the service quality of ML, for example, its availability and performance, regardless of the network conditions, as well as maintaining the privacy, security and sustainability. We let practitioners assign ML tasks to heterogeneous edge devices, including highly resource-constrained embedded microcontrollers with main memories in the order of Kilobytes, and energy consumption in the order of milliwatts. This is known as TinyML. Furthermore, we show how software models with different levels of abstraction, namely platform-independent and platform-specific models can be used in the software development process. Finally, we validate the proposed approach using a case study addressing the predictive maintenance of a hydraulics system with various networked sensors and actuators."
Software defect prediction via LSTM,"Deng, JH; Lu, L; Qiu, SJ",10.1049/iet-sen.2019.0149,2020,"Software quality plays an important role in the software lifecycle. Traditional software defect prediction approaches mainly focused on using hand-crafted features to detect defects. However, like human languages, programming languages contain rich semantic and structural information, and the cause of defective code is closely related to its context. Failing to catch this significant information, the performance of traditional approaches is far from satisfactory. In this study, the authors leveraged a long short-term memory (LSTM) network to automatically learn the semantic and contextual features from the source code. Specifically, they first extract the program's Abstract Syntax Trees (ASTs), which is made up of AST nodes, and then evaluate what and how much information they can preserve for several node types. They traverse the AST of each file and fed them into the LSTM network to automatically the semantic and contextual features of the program, which is then used to determine whether the file is defective. Experimental results on several opensource projects showed that the proposed LSTM method is superior to the state-of-the-art methods."
Req2Spec: Transforming Software Requirements into Formal Specifications Using Natural Language Processing,"Nayak, A; Timmapathini, HP; Murali, V; Ponnalagu, K; Venkoparao, VG; Post, A",10.1007/978-3-030-98464-9_8,2022,"Context and motivation] Requirement analysis and Test specification generation are critical activities in the Software Development Life Cycle (SDLC), which if not done correctly can lead to defects in the software system. Manually performing these tasks on Natural Language Requirements (NLR) is time consuming and error prone. [Question/problem] The problem is to facilitate the automation of these activities by transforming the NLR into Formal Specifications. [Principal ideas/results] In this paper we present Req2Spec, a Natural Language Processing (NLP) based pipeline that performs syntactic and semantic analysis on NLR to generate formal specifications that can be readily consumed by HANFOR, an industry scale Requirements analysis and Test specification generation tool. We considered 222 automotive domain software requirements at BOSCH, 71% of which were correctly formalized. [Contribution] Req2Spec will be an aid to stakeholders of the SDLC as it seamlessly integrates with HANFOR enabling automation."
Towards a Data Engineering Process in Data-Driven Systems Engineering,"Petersen, P; Stage, H; Langner, J; Ries, L; Rigoll, P; Hohl, CP; Sax, E",10.1109/ISSE54508.2022.10005441,2022,"Highly Automated Driving (HAD) has become one of the leading trends in the automotive industry. Mandatory tasks like environment perception and scene understanding challenge existing rule-based methods. Thus, data-driven technologies and Artificial Intelligence (AI) have been introduced to automotive software development. Utilizing data in the development process has become essential as these systems are no longer developed with classical systems engineering methods, but rather by deriving requirements from and training the algorithms with recorded real-world data. This entails the introduction of data-driven workflows and data-management as new aspects of Automotive Systems Engineering (ASE). Tasks related to the development of Artificial Intelligence (AI) software differ from their classical engineering and programming counterparts. Thus, engineers require new tools and methods for developing safe and accurate AI-based software and handling data efficiently during ASE. Another important aspect of data-driven development is ensuring data quality throughout the systems engineering process. Hence, this paper aims to take a step towards the introduction of a data engineering process in data-driven automotive systems engineering. Putting a spotlight on developing well-designed data sets as the central element for training and validating AI-based software. Besides determining the quality of data sets, we present steps towards improving data and data set quality."
An integration engineering framework for machine learning in healthcare,"Assadi, A; Laussen, PC; Goodwin, AJ; Goodfellow, S; Dixon, W; Greer, RW; Jegatheeswaran, A; Singh, D; McCradden, M; Gallant, SN; Goldenberg, A; Eytan, D; Mazwi, ML",10.3389/fdgth.2022.932411,2022,"Background and Objectives: Machine Learning offers opportunities to improve patient outcomes, team performance, and reduce healthcare costs. Yet only a small fraction of all Machine Learning models for health care have been successfully integrated into the clinical space. There are no current guidelines for clinical model integration, leading to waste, unnecessary costs, patient harm, and decreases in efficiency when improperly implemented. Systems engineering is widely used in industry to achieve an integrated system of systems through an interprofessional collaborative approach to system design, development, and integration. We propose a framework based on systems engineering to guide the development and integration of Machine Learning models in healthcare.Methods: Applied systems engineering, software engineering and health care Machine Learning software development practices were reviewed and critically appraised to establish an understanding of limitations and challenges within these domains. Principles of systems engineering were used to develop solutions to address the identified problems. The framework was then harmonized with the Machine Learning software development process to create a systems engineering-based Machine Learning software development approach in the healthcare domain.Results: We present an integration framework for healthcare Artificial Intelligence that considers the entirety of this system of systems. Our proposed framework utilizes a combined software and integration engineering approach and consists of four phases: (1) Inception, (2) Preparation, (3) Development, and (4) Integration. During each phase, we present specific elements for consideration in each of the three domains of integration: The Human, The Technical System, and The Environment. There are also elements that are considered in the interactions between these domains.Conclusion: Clinical models are technical systems that need to be integrated into the existing system of systems in health care. A systems engineering approach to integration ensures appropriate elements are considered at each stage of model design to facilitate model integration. Our proposed framework is based on principles of systems engineering and can serve as a guide for model development, increasing the likelihood of successful Machine Learning translation and integration."
Requirements Engineering Challenges in Building AI-Based Complex Systems,"Belani, H; Vukovic, M; Car, Z",10.1109/REW.2019.00051,2019,"This paper identifies and tackles the challenges of the requirements engineering discipline when applied to development of AI-based complex systems. Due to their complex behaviour, there is an immanent need for a tailored development process for such systems. However, there is still no widely used and specifically tailored process in place to effectively and efficiently deal with requirements suitable for specifying a software solution that uses machine learning. By analysing the related work from software engineering and artificial intelligence fields, potential contributions have been recognized from agent-based software engineering and goal-oriented requirements engineering research, as well as examples from large product development companies. The challenges have been discussed, with proposals given how and when to tackle them. RE4AI taxonomy has also been outlined, to inform the tailoring of development process."
General Roadmap and Core Steps for the Development of AI Tools in Digital Pathology,"Makhlouf, Y; Salto-Tellez, M; James, J; O'Reilly, P; Maxwell, P",10.3390/diagnostics12051272,2022,"Integrating artificial intelligence (AI) tools in the tissue diagnostic workflow will benefit the pathologist and, ultimately, the patient. The generation of such AI tools has two parallel and yet interconnected processes, namely the definition of the pathologist's task to be delivered in silico, and the software development requirements. In this review paper, we demystify this process, from a viewpoint that joins experienced pathologists and data scientists, by proposing a general pathway and describing the core steps to build an AI digital pathology tool. In doing so, we highlight the importance of the collaboration between AI scientists and pathologists, from the initial formulation of the hypothesis to the final, ready-to-use product."
MolSSI Education: Empowering the Next Generation of Computational Molecular Scientists,"Nash, JA; Mostafanejad, M; Crawford, TD; McDonald, AR",10.1109/MCSE.2022.3165607,2022,"The Molecular Sciences Software Institute (MolSSI) is a research and education center that supports software development in the computational molecular sciences (CMS). One of MolSSI's core objectives is to provide education and training for the next generation of computational researchers. MolSSI Education targets various career stages and skill levels through its live workshops, online resources, and software fellowship program. MolSSI Education focuses its efforts on four areas: programming and software development, high-performance computing, artificial intelligence, faculty and curriculum development, and the software fellowship program. This article delineates educational efforts at the MolSSI, overall goals, and resources that can be useful to researchers in the CMS."
ModelOps: Cloud-based Lifecycle Management for Reliable and Trusted AI,"Hummer, W; Muthusamy, V; Rausch, T; Dube, P; El Maghraoui, K; Murthi, A; Oum, P",10.1109/IC2E.2019.00025,2019,"This paper proposes a cloud-based framework and platform for end-to-end development and lifecycle management of artificial intelligence (AI) applications. We build on our previous work on platform-level support for cloud-managed deep learning services, and show how the principles of software lifecycle management can be leveraged and extended to enable automation, trust, reliability, traceability, quality control, and reproducibility of AI pipelines. Based on a discussion of use cases and current challenges, we describe a framework for managing AI application lifecycles and its key components. We also show concrete examples that illustrate how this framework enables managing and executing model training and continuous learning pipelines while infusing trusted AI principles."
Pseudocode Generation from Source Code Using the BART Model,"Alokla, A; Gad, W; Nazih, W; Aref, M; Salem, AB",10.3390/math10213967,2022,"In the software development process, more than one developer may work on developing the same program and bugs in the program may be fixed by a different developer; therefore, understanding the source code is an important issue. Pseudocode plays an important role in solving this problem, as it helps the developer to understand the source code. Recently, transformer-based pre-trained models achieved remarkable results in machine translation, which is similar to pseudocode generation. In this paper, we propose a novel automatic pseudocode generation from the source code based on a pre-trained Bidirectional and Auto-Regressive Transformer (BART) model. We fine-tuned two pre-trained BART models (i.e., large and base) using a dataset containing source code and its equivalent pseudocode. In addition, two benchmark datasets (i.e., Django and SPoC) were used to evaluate the proposed model. The proposed model based on the BART large model outperforms other state-of-the-art models in terms of BLEU measurement by 15% and 27% for Django and SPoC datasets, respectively."
"AI-powered model repair: an experience report-lessons learned, challenges, and opportunities","Barriga, A; Rutle, A; Heldal, R",10.1007/s10270-022-00983-5,2022,"Artificial intelligence has already proven to be a powerful tool to automate and improve how we deal with software development processes. The application of artificial intelligence to model-driven engineering projects is becoming more and more popular; however, within the model repair field, the use of this technique remains mostly an open challenge. In this paper, we explore some existing approaches in the field of AI-powered model repair. From the existing approaches in this field, we identify a series of challenges which the community needs to overcome. In addition, we present a number of research opportunities by taking inspiration from other fields which have successfully used artificial intelligence, such as code repair. Moreover, we discuss the connection between the existing approaches and the opportunities with the identified challenges. Finally, we present the outcomes of our experience of applying artificial intelligence to model repair."
From critical technical practice to reflexive data science,"Hirsbrunner, SD; Tebbe, M; MÃ¼ller-Birn, C",10.1177/13548565221132243,2024,"In this article, we reconsider elements of Agre's critical technical practice approach (Agre, 1997) for critical technical practice approach for reflexive artificial intelligence (AI) research and explore ways and expansions to make it productive for an operationalization in contemporary data science. Drawing on Jorg Niewohner's co-laboration approach, we show how frictions within interdisciplinary work can be made productive for reflection. We then show how software development environments can be repurposed to infrastructure reflexivities and to make co-laborative engagement with AI-related technology possible and productive. We document our own co-laborative engagement with machine learning and highlight three exemplary critical technical practices that emerged out of the co-laboration: negotiating comparabilities, shifting contextual attention and challenging similarity and difference. We finally wrap up the conceptual and empirical elements and propose Reflexive Data Science (RDS) as a methodology for co-laborative engagement and infrastructured reflexivities in contemporary AI-related research. We come back to Agre's ways of operationalizing reflexivity and introduce the building blocks of RDS: (1) organizing encounters of social contestation, (2) infrastructuring a network of anchoring devices enabling reflection, (3) negotiating timely matters of concern and (4) designing for reflection. With our research, we aim at contributing to the methodological underpinnings of epistemological and social reflection in contemporary AI research."
MZA: A Data Conversion Tool to Facilitate Software Development and Artificial Intelligence Research in Multidimensional Mass Spectrometry,"Bilbao, A; Ross, DH; Lee, JY; Donor, MT; Williams, SM; Zhu, Y; Ibrahim, YM; Smith, RD; Zheng, XY",10.1021/acs.jproteome.2c00313,2022,"Modern mass spectrometry-based workflows employing hybrid instrumentation and orthogonal separations collect multidimensional data, potentially allowing deeper understanding in omics studies through adoption of artificial intelligence methods. However, the large volume of these rich spectra challenges existing data storage and access technologies, therefore precluding informatics advancements. We present MZA (pronounced m-za), the mass-to-charge (m/z) generic data storage and access tool designed to facilitate software development and artificial intelligence research in multidimensional mass spectrometry measurements. Composed of a data conversion tool and a simple file structure based on the HDF5 format, MZA provides easy, cross-platform and cross-programming language access to raw MS-data, enabling fast development of new tools in data science programming languages such as Python and R. The software executable, example MS-data and example Python and R scripts are freely available at https://github.com/PNNL-m-q/mza."
Word Embeddings for the Software Engineering Domain,"Efstathiou, V; Chatzilenas, C; Spinellis, D",10.1145/3196398.3196448,2018,"The software development process produces vast amounts of textual data expressed in natural language. Outcomes from the natural language processing community have been adapted in software engineering research for leveraging this rich textual information; these include methods and readily available tools, often furnished with pre-trained models. State of the art pre-trained models however, capture general, common sense knowledge, with limited value when it comes to handling data specific to a specialized domain. There is currently a lack of domain-specific pre-trained models that would further enhance the processing of natural language artefacts related to software engineering. To this end, we release a word2vec model trained over 15GB of textual data from Stack Overflow posts. We illustrate how the model disambiguates polysemous words by interpreting them within their software engineering context. In addition, we present examples of fine-grained semantics captured by the model, that imply transferability of these results to diverse, targeted information retrieval tasks in software engineering and motivate for further reuse of the model."
Predicting Bug-Fixing Time: DistilBERT Versus Google BERT,"Ardimento, P",10.1007/978-3-031-21388-5_46,2022,"The problem of bug-fixing time can be treated as a supervised text categorization task in Natural Language Processing. In recent years, following the use of deep learning also in the field of Natural Language Processing, pre-trained contextualized representations of words have become widespread. One of the most used pre-trained language representations models is named Google BERT (hereinafter, for brevity, BERT). BERT uses a self-attention mechanism that allows learning the bidirectional context representation of a word in a sentence, which constitutes one of the main advantages over the previously proposed solutions. However, due to the large size of BERT, it is difficult for it to put it into production. To address this issue, a smaller, faster, cheaper and lighter version of BERT, named DistilBERT, has been introduced at the end of 2019. This paper compares the efficacy of BERT and DistilBERT, combined with the Logistic Regression, in predicting bug-fixing time from bug reports of a large-scale open-source software project, LiveCode. In the experimentation carried out, DistilBERT retains almost 100% of its language understanding capabilities and, in the best case, it is 63.28% faster than BERT. Moreover, with a not time-consuming tuning of the C parameter in Logistic Regression, the DistilBERT provides an accuracy value even better than BERT."
Efficient Extraction of Technical Requirements Applying Data Augmentation,"GrÃ¤ssler, I; Preuss, D; Brandt, L; Mohr, M",10.1109/ISSE54508.2022.10005452,2022,"Requirements for complex technical systems are documented in natural language sources. Manually extracting requirements from these documents - e.g., to transfer them to a requirements management tool - is time-consuming and error-prone. Today, machine learning approaches are used to classify natural language requirements and thus enable extraction of these requirements. However, in practice there is often not enough labeled domain-specific data available to train such models. For this reason, this work investigates the performance in artificially generating requirements through data augmentation. First, success criteria for a method for extracting and augmenting requirements are elicited in cooperation with industry experts. Second, the performance in the augmentation of requirements data is investigated. The results show that GPT-J is suitable for generating artificial requirements: weighted average F1-score: 62.74 %. Third, a method is developed to extract requirements from specifications, augment requirements data, and then classify the requirements. As a final step, the method is evaluated with requirements data from three industry case examples of the engineering service provider EDAG Engineering GmbH: assembly latch hood, adjustable stopper hood and trunk curtain roller blind. Evaluation shows that especially the transferability of models is improved when they are trained with augmented data. The developed method facilitates eliciting complete requirements sets. Performance of artificial intelligence models in requirements extraction is improved applying augmented data and therefore the method leads to efficient product development."
Artificial Intelligence (AI) for Sustainable Institutional Food Systems: Implementation of AI Tools for School Nutrition Program Management in the United States of America,"Camarena, S",10.3389/fsufs.2022.743810,2022,"School food programs can improve the nutritional status at community level, and can be a powerful tool to facilitate a transition to sustainable food systems, particularly through their purchasing methods and meal/waste management. Artificial Intelligence (AI) can enable or inhibit transitions to sustainability, due to its capacity to facilitate transformational change and disruption. Thus, AI can have major impacts on achieving the United Nations' Sustainable Development Goals (SDGs), including those related to food systems. This paper focuses on critically exploring different aspects during the implementation of a software project in US schools that used AI techniques to enable agility and provide healthy food options for schoolchildren. Participant observation, semi-structured interviews, and document analysis were used to inform the case study and uncover the new processes developed using the technology. As a key barrier to the effective management of school food programs and nutrition is the administrative load associated with statutory compliance, this case study demonstrates the difference that AI-powered tools can make in alleviating the weight of administrative processes. An Information and Communication Technology (ICT)-enabled boundary spanning framework is used to extend the case study toward an analysis of the systems, boundaries, relationships and perspectives which starts mapping the areas where the private sector, public institutions and civil society can meet to fast-track sustainable transition activities, particularly in the context of food systems. This article presents some possible approaches to facilitating these activities, inviting policy makers to bridge the gap with businesses and use business agility to support common societal goals to achieve sustainable food systems."
Influence of input values on the prediction model error using artificial neural network based on Taguchi's orthogonal array,"Rankovic, N; Rankovic, D; Ivanovic, M; Lazic, L",10.1002/cpe.6831,2022,"Rapid and accurate assessment of software project development using artificial intelligence tools can be essential for success in the software industry. This article has two objectives: to reduce the magnitude relative error (MRE) value in estimating the effort and cost of software development using the proposed artificial neural network architecture based on the Taguchi method and examine the influence of input variables on the change in relative error value. Clustering and fuzzification methods further mitigate the heterogeneous structure of the different project values of the datasets used. Taguchi method contributes to the reduction of the number of iterations by 99%, which achieves a significant reduction in estimation and value of MRE. By monitoring additional criteria, such as prediction, correlation, and comparing two activation functions, such as sigmoid and radial basis function, the proposed model's correctness, reliability, and stability are confirmed. Significantly better results are expected using the sigmoid activation function and a decrease in the value of the mean (MRE)."
Automatic Detection and Analysis of Technical Debts in Peer-Review Documentation of R Packages,"Khan, JY; Uddin, C",10.1109/SANER53432.2022.00094,2022,"Technical debt (TD) is a metaphor for code-related problems that arise as a result of prioritizing speedy delivery over perfect code. Given that the reduction of TDs can have long-term positive impact in the software engineering life-cycle (SDLC), TDs are studied extensively in the literature. However, very few of the existing research focused on the technical debts of R programming language despite its popularity and usage. Recent research by Codabux et al. [21] finds that R packages can have 10 diverse TD types analyzing peer-review documentation. However, the findings are based on the manual analysis of a small sample of R package review comments. In this paper, we develop a suite of Machine Learning (ML) classifiers to detect the 10 TDs automatically. The best performing classifier is based on the deep ML model BERT, which achieves F1-scores of 0.71 - 0.91. We then apply the trained BERT models on all available peer-review issue comments from two platforms, rOpenSci and BioConductor (13.5K review comments coming from a total of 1297 R packages). We conduct an empirical study on the prevalence and evolution of 10 TDs in the two R platforms. We discovered documentation debt is the most prevalent among all types of TD, and it is also expanding rapidly. We also find that R packages of generic platform (i.e. rOpenSci) are more prone to TD compared to domain-specific platform (i.e. BioConductor). Our empirical study findings can guide future improvements opportunities in R package documentation. Our ML models can be used to automatically monitor the prevalence and evolution of TDs in R package documentation."
Swarm intelligence-based model for improving prediction performance of low-expectation teams in educational software engineering projects,"Al-Ahmad, BI; Al-Zoubi, AA; Kabir, MF; Al-Tawil, M; Aljarah, I",10.7717/peerj-cs.857,2022,"Software engineering is one of the most significant areas, which extensively used in educational and industrial fields. Software engineering education plays an essential role in keeping students up to date with software technologies, products, and processes that are commonly applied in the software industry. The software development project is one of the most important parts of the software engineering course, because it covers the practical side of the course. This type of project helps strengthening students' skills to collaborate in a team spirit to work on software projects. Software project involves the composition of software product and process parts. Software product part represents software deliverables at each phase of Software Development Life Cycle (SDLC) while software process part captures team activities and behaviors during SDLC. The low-expectation teams face challenges during different stages of software project. Consequently, predicting performance of such teams is one of the most important tasks for learning process in software engineering education. The early prediction of performance for low-expectation teams would help instructors to address difficulties and challenges related to such teams at earliest possible phases of software project to avoid project failure. Several studies attempted to early predict the performance for low-expectation teams at different phases of SDLC. This study introduces swarm intelligence-based model which essentially aims to improve the prediction performance for low-expectation teams at earliest possible phases of SDLC by implementing Particle Swarm Optimization-K Nearest Neighbours (PSO-KNN), and it attempts to reduce the number of selected software product and process features to reach higher accuracy with identifying less than 40 relevant features. Experiments were conducted on the Software Engineering Team Assessment and Prediction (SETAP) project dataset. The proposed model was compared with the related studies and the state-of-the-art Machine Learning (ML) classifiers: Sequential Minimal Optimization (SMO), Simple Linear Regression (SLR), Naive Bayes (NB), Multilayer Perceptron (MLP), standard KNN, and J48. The proposed model provides superior results compared to the traditional ML classifiers and state-of-the-art studies in the investigated phases of software product and process development."
A Literature Review of Automatic Traceability Links Recovery for Software Change Impact Analysis,"Aung, TWW; Huo, H; Sui, YL",10.1145/3387904.3389251,2020,"In large-scale software development projects, change impact analysis (CIA) plays an important role in controlling software design evolution. Identifying and accessing the effects of software changes using traceability links between various software artifacts is a common practice during the software development cycle. Recently, research in automated traceability-link recovery has received broad attention in the software maintenance community to reduce the manual maintenance cost of trace links by developers. In this study, we conducted a systematic literature review related to automatic traceability link recovery approaches with a focus on CIA. We identified 33 relevant studies and investigated the following aspects of CIA: traceability approaches, CIA sets, degrees of evaluation, trace direction and methods for recovering traceability link between artifacts of different types. Our review indicated that few traceability studies focused on designing and testing impact analysis sets, presumably due to the scarcity of datasets. Based on the findings, we urge further industrial case studies. Finally, we suggest developing traceability tools to support fully automatic traceability approaches, such as machine learning and deep learning."
Machine Learning in Software Development Life Cycle: A Comprehensive Review,"Navaei, M; Tabrizi, N",10.5220/0011040600003176,2022,"This research concludes an overall summary of the publications so far on the applied Machine Learning (ML) techniques in different phases of Software Development Life Cycle (SDLC) that includes Requirement Analysis, Design, Implementation, Testing, and Maintenance. We have performed a systematic review of the research studies published from 2015-2021 and revealed that Software Requirements Analysis phase has the least number of papers published; in contrast, Software Testing is the phase with the greatest number of papers published."
OnSRAM: Efficient Inter-Node On-Chip Scratchpad Management in Deep Learning Accelerators,"Pal, S; Venkataramani, S; Srinivasan, V; Gopalakrishnan, K",10.1145/3530909,2022,"Hardware acceleration of Artificial Intelligence (AI) workloads has gained widespread popularity with its potential to deliver unprecedented performance and efficiency. An important challenge remains in how AI accelerators are programmed to sustain high utilization without impacting end-user productivity. Prior software optimizations start with an input graph and focus on node-level optimizations, viz. dataflows and hierarchical tiling, and graph-level optimizations such as operation fusion. However, little effort has been devoted to inter-node on-chip scratchpad memory (SPM) management in Deep Learning (DL) accelerators, whose significance is bolstered by the recent trends in complex network topologies and the emergence of eager execution in DL frameworks. We characterize and show that there exists up to a 5.2x performance gap in DL inference to be bridged using SPM management and propose OnSRAM, a novel SPM management framework integrated with the compiler runtime of a DL accelerator. We develop two variants, viz. OnSRAM-Static, which works on static graphs to identify data structures that can be lucratively held on-chip based on their size, liveness and significance, and OnSRAM-Eager, which targets an eager execution model (no graph) and uses a history-based speculative scheme to hold/discard data structures. We integrate OnSRAM with TensorFlow and analyze it on multiple accelerator configurations. Across a suite of 12 images, objects, and language networks, on a 3 TFLOP system with a 2 MB SPM and 32 GBps external memory bandwidth, OnSRAM-Static and OnSRAM-Eager achieve 1.02-4.8x and 1.02-3.1x reduction in inference latency (batch size of 1), over a baseline with no SPM management. In terms of energy savings, we observe average reductions of 1.51x (up to 4.1x) and 1.23x (up to 2.9x) for the static and eager execution scenarios, respectively."
VIBE: Looking for Variability In amBiguous rEquirements?,"Fantechi, A; Gnesi, S; Semini, L",10.1016/j.jss.2022.111540,2023,"Variability is a characteristic of a software project and describes the fact that a system can be configured in different ways, obtaining different products (variants) from a common code base, accordingly to the software product line paradigm. This paradigm can be conveniently applied in all phases of the software process, starting from the definition and analysis of the requirements. We observe that often requirements contain ambiguities which can reveal an unintentional and implicit source of variability, that has to be detected.To this end we define VIBE, a tool supported process to identify variability aspects in requirements documents. VIBE is defined on the basis of a study of the different sources of ambiguity in natural language requirements documents that are useful to recognize potential variability, and is character-ized by the use of a NLP tool customized to detect variability indicators. The tool to be used in VIBE is selected from a number of ambiguity detection tools, after a comparison of their customization features. The validation of VIBE is conducted using real-world requirements documents.(c) 2022 Elsevier Inc. All rights reserved."
User Interface Design for AI-Based Clinical Decision-Support System Preliminary Study,"Beltrao, G; Paramonova, I; Sousa, S",,2022,"This paper presents a case study about the initial phases of the interface design for an artificial intelligence-based decision-support system for clinical diagnosis. The study presents challenges and opportunities in implementing a human-centered design (HCD) approach during the early stages of the software development of a complex system. These methods are commonly adopted to ensure that the systems are designed based on users' needs. For this project, they are also used to investigate the users' potential trust issues and ensure the creation of a trustworthy platform. However, the project stage and heterogeneity of the teams can pose obstacles to their implementation. The results of the implementation of HCD methods have shown to be effective and informed the creation of low fidelity prototypes. The outcomes of this process can assist other designers, developers, and researchers in creating trustworthy AI solutions."
Roadmap for development of skills in Artificial Intelligence by means of a Reinforcement Learning model using a DeepRacer autonomous vehicle,"Cota, JL; RodrÃ­guez, JAT; Alonso, BG; Hurtado, CV",10.1109/EDUCON52537.2022.9766659,2022,"Using Deepracer, through experimentation and simulation, theoretical concepts can be applied to a practical application of reinforcement learning (RL) in a real-life problem. It was considered as a highly useful tool to develop many direct and transversal competences that students need to work within the field of artificial intelligence (AI). Nowadays the combination of Hardware, Computer Vision methods and Machine Learning (ML) algorithms for the development of controllers for vehicle driving automation have facilitated the development of solutions for this problem. The intention of this work is to show a Roadmap that was formulated to learn AI, ML and RL competencies required to prepare undergraduate students for the industry of this area, following a structured mostly practical learning plan using Deepracer AWS platform and local alternatives for training; and a physical vehicle as primary tools that have made an incredibly compact setup process and reduced complexity in the educational researching field related to learning autonomous vehicles (AV) software development process. The AWS DeepRacer framework includes all needed hardware based on a two front-camera vehicle for stereo vision and a LiDAR sensor, besides it provides a powerful computation computer for high performance in scale autonomous vehicles. The roadmap was executed testing and comparing different RL models over the modalities that Deepracer provides (by comparing both local and AWS console training) documentation of the execution of the generated Roadmap is shown to ensure that should be considered as a stable learning system that could be followed by college programs. Finally, models were tested on a physical track built, and limitations, considerations and improvements for this Roadmap are explained as a contribution for future work."
Problematizing the Adoption of Formal Methods in the 4IR-5IR Transition,"van der Poll, JA",10.3390/asi5060127,2022,"The adoption of formal methods (FMs) as a software development methodology remains low. Advocates of FMs point to the advantages to be gained by producing highly dependable systems, while critics refer to the steep learning curve required to master the underlying mathematics and logic. The situation was similar for artificial intelligence (AI), but the advent of 4IR-5IR technologies has recently made AI a feasible technology for computing. We believe that the same could hold for FMs. In this article, we considered both the advantages and disadvantages of the use of FMs and unpacked them by problematizing the aspects that need to be considered in the 4IR-5IR worlds to facilitate the use of FMs as a viable software development methodology. We made the case that the 5IR embedding of harmonious collaboration between humans and machines could assist with difficult FM interfaces, similar to how human-computer interaction (HCI) has influenced technical and inflexible systems in the past. Since we view FMs as a technology, we further considered the role to be played by technology adoption, exemplified by the various technology adoption models, e.g., the TOE framework. This article culminates in the formulation of a problematization framework for the adoption of FMs in 4IR-5IR."
This is Just a Prototype: How Ethics Are Ignored in Software Startup-Like Environments,"Vakkuri, V; Kemell, KK; Jantunen, M; Abrahamsson, P",10.1007/978-3-030-49392-9_13,2020,"Artificial Intelligence (AI) solutions are becoming increasingly common in software development endeavors, and consequently exert a growing societal influence as well. Due to their unique nature, AI based systems influence a wide range of stakeholders with or without their consent, and thus the development of these systems necessitates a higher degree of ethical consideration than is currently carried out in most cases. Various practical examples of AI failures have also highlighted this need. However, there is only limited research on methods and tools for implementing AI ethics in software development, and we currently have little knowledge of the state of practice. In this study, we explore the state of the art in startup-like environments where majority of the AI software today gets developed. Based on a multiple case study, we discuss the current state of practice and highlight issues. The cases underline the complete ignorance of ethical consideration in AI endeavors. We also outline existing good practices that can already support the implementation of AI ethics, such as documentation and error handling."
From User Stories to Models: A Machine Learning Empowered Automation,"Kochbati, T; Li, S; GÃ©rard, S; Mraidha, C",10.5220/0010197800280040,2021,"In modern software development, manually deriving architecture models from software requirements expressed in natural language becomes a tedious and time-consuming task particularly for more complex systems. Moreover, the increase in size of the developed systems raises the need to decompose the software system into sub-systems at early stages since such decomposition aids to better design the system architecture. In this paper, we propose a machine learning based approach to automatically break-down the system into sub-systems and generate preliminary architecture models from natural language user stories in the Scrum process. Our approach consists of three pillars. Firstly, we compute word level similarity of requirements using word2vec as a prediction model. Secondly, we extend it to the requirement level similarity computation, using a scoring formula. Thirdly, we employ the Hierarchical Agglomerative Clustering algorithm to group the semantically similar requirements and provide an early decomposition of the system. Finally, we implement a set of specific Natural Language Processing heuristics in order to extract relevant elements that are needed to build models from the identified clusters. Ultimately, we illustrate our approach by the generation of sub-systems expressed as UML use-case models and demonstrate its applicability using three case studies."
Project smells - Experiences in Analysing the Software Quality of ML Projects with mllint,"van Oort, B; Cruz, L; Loni, B; van Deursen, A",10.1145/3510457.3513041,2022,"Machine Learning (ML) projects incur novel challenges in their development and productionisation over traditional software applications, though established principles and best practices in ensuring the project's software quality still apply. While using static analysis to catch code smells has been shown to improve software quality attributes, it is only a small piece of the software quality puzzle, especially in the case of ML projects given their additional challenges and lower degree of Software Engineering (SE) experience in the data scientists that develop them. We introduce the novel concept of project smells which consider deficits in project management as a more holistic perspective on software quality in ML projects. An open-source static analysis tool mllint was also implemented to help detect and mitigate these. Our research evaluates this novel concept of project smells in the industrial context of ING, a global bank and large software- and data-intensive organisation. We also investigate the perceived importance of these project smells for proof-of-concept versus production-ready ML projects, as well as the perceived obstructions and benefits to using static analysis tools such as mllint. Our findings indicate a need for context-aware static analysis tools, that fit the needs of the project at its current stage of development, while requiring minimal configuration effort from the user."
Building an integrated requirements engineering process based on Intelligent Systems and Semantic Reasoning on the basis of a systematic analysis of existing proposals,"Corral, A; Sanchez, LE; Antonelli, L",10.3897/jucs.78776,2022,"Requirements Engineering is one of the fundamental activities in the software development process and is oriented toward what should be produced. One of the development team's most common problems is a lack of communication regarding an understanding of the discourse domain and how to integrate and process excessive information originating from different sources. This may lead to errors of omission and the consequent production of incomplete and inconsistent artifacts, which will have a direct effect on the quality of the software. The use of machine learning techniques helps the development team produce successful software on the basis of the acquisition of knowledge and human experience with which to understand the domain of the application. This paper, therefore, presents a proposal for a new methodological process oriented toward the construction of a vocabulary concerning the application domain. The authors propose to do this by employing Natural Language Processing (NLP), ontologies and heuristics that will lead to the production of a Lexicon that is common to analysts and customers, both of whom will understand the universe of discourse, thus mitigating problems of completeness. This objective has been achieved by carrying out a Systematic Literature Review of the artificial intelligence techniques employed in the requirements engineering process, which led to the discovery that 41.37% use NLP, while 55.71% apply ontologies such as semantic reasoners which help solve the problem of language ambiguity, the structures in specifications or the identification of key concepts with which to establish traceability links. However, the review also showed that the problems regarding the comprehension and completeness of requirements problems have yet to be resolved."
PAISEÂ® - process model for AI systems engineering,"Hasterok, C; Stompe, J",10.1515/auto-2022-0020,2022,"The application of artificial-intelligence-(AI)-based methods within the context of complex systems poses new challenges within the product life cycle. The process model for AI systems engineering, PAISE((R)), addresses these challenges by combining approaches from the disciplines of systems engineering, software development and data science. The general approach builds on a component-wise development of the overall system including an AI component. This allows domain specific development processes to be parallelized. At the same time, component dependencies are tested within interdisciplinary checkpoints, thus resulting in a refinement of component specifications."
PrivacyStory: Tool Support for Extracting Privacy Requirements from User Stories,"Herwanto, GB; Quirchmayr, G; Tjoa, AM",10.1109/RE54965.2022.00036,2022,"Privacy by design requires that developers address privacy concerns from the early stage of software development life cycle. It encourages them to take a proactive approach to privacy engineering by identifying personal data, creating conceptual data flow diagrams, and identifying privacy threats. We argue that by providing a tool that automates some of the steps can reduce the burden on development teams. We develop a tool called PrivacyStory, including an end-to-end privacy requirement generation from a set of user stories. The tool provides some automation, utilizing a current state-of-the art natural language processing model. The core aim of our tool is to assist development teams in becoming more agile in their approach to privacy requirements engineering."
Incorporating Pre-trained Transformer Models into TextCNN for Sentiment Analysis on Software Engineering Texts,"Sun, KX; Shi, XB; Gao, H; Kuang, HY; Ma, XX; Rong, GP; Shao, D; Zhao, Z; Zhang, H",10.1145/3545258.3545273,2022,"Software information sites (e.g., Jira, Stack Overflow) are now widely used in software development. These online platforms for collaborative development preserve a large amount of Software Engineering (SE) texts. These texts enable researchers to detect developers' attitudes toward their daily development by analyzing the sentiments expressed in the texts. Unfortunately, recent works reported that neither off-the-shelf tools nor SE-specified tools for sentiment analysis on SE texts can provide satisfying and reliable results. In this paper, we propose to incorporate pre-trained transformer models into the sentence-classification oriented deep learning framework named TextCNN to better capture the unique expression of sentiments in SE texts. Specifically, we introduce an optimized BERT model named RoBERTa as the word embedding layer of TextCNN, along with additional residual connections between RoBERTa and TextCNN for better cooperation in our training framework. An empirical evaluation based on four datasets from different software information sites shows that our training framework can achieve overall better accuracy and generalizability than the four baselines."
Current and Future Bots in Software Development,"Erlenhov, L; Neto, FGD; Scandariato, R; Leitner, P",10.1109/BotSE.2019.00009,2019,"Bots that support software development (DevBots) are seen as a promising approach to deal with the ever-increasing complexity of modern software engineering and development. Existing DevBots are already able to relieve developers from routine tasks such as building project images or keeping dependencies up-to-date. However, advances in machine learning and artificial intelligence hold the promise of future, significantly more advanced, DevBots. In this paper, we introduce the terminology of contemporary and ideal DevBots. Contemporary DevBots represent the current state of practice, which we characterise using a facet-based taxonomy. We exemplify this taxonomy using 11 existing, industrial-strength bots. We further provide a vision and definition of future (ideal) DevBots, which are not only autonomous, but also adaptive, as well as technically and socially competent. These properties may allow ideal DevBots to act more akin to artificial team mates than simple development tools."
Constructing reusable knowledge for machine learning projects based on project practices,"Takeuchi, H; Imazaki, K; Kuno, N; Doi, T; Motohashi, Y",10.3233/IDT-220252,2022,"Recently, machine learning (ML) techniques have been introduced into various domains. This study focuses on projects for the development of ML-based service systems in which ML techniques are applied to enterprise functions. In these projects, constructing reusable knowledge on projects that develop ML-based service systems is important to effectively implement such projects. Here, the collection of insights and development of architecture and design patterns for ML-based service systems are considered. We propose a method for collecting insights by referring to a development model based on project practices and developing patterns for ML projects as an enterprise architecture model. Through a practice, we attempt to collect insights as best practices and construct design patterns for ML projects using the proposed method."
Intelligent Software Engineering: The Significance of Artificial Intelligence Techniques in Enhancing Software Development Lifecycle Processes,"Kulkarni, V; Kolhe, A; Kulkarni, J",10.1007/978-3-030-96308-8_7,2022,"In every sphere of technology nowadays, the world has been moving away from manual procedures towards more intelligent systems that minimize human error and intervention, and software engineering is no exception. This paper is a study on the amalgamation of artificial intelligence with software engineering. Software Development Lifecycle is the foundation of this paper, and each phase of it - Requirements Engineering, Design and Architecture, Development and Implementation, and Testing - serves as a building block. This work elucidates the various techniques of intelligent computing that have been applied to these stages of software engineering, as well as the scope for some of these techniques to solve existing challenges and optimize SDLC processes. This paper demonstrates in-depth, comprehensive research into the current state, advantages, limitations and future scope of artificial intelligence in the domain of software engineering. It is significant for its contributions to the field of intelligent software engineering by providing industry-oriented, practical applications of techniques like natural language processing, meta programming, automated data structuring, self-healing testing etc. This paper expounds upon some open issues and inadequacies of software engineering tools today, and proposes ways in which intelligent applications could present solutions to these challenges."
Modern Integrated Development Environment (IDEs),"Alizadehsani, Z; Gomez, EG; Ghaemi, H; GonzÃ¡lez, SR; Jordan, J; FernÃ¡ndez, A; PÃ©rez-Lancho, B",10.1007/978-3-030-78901-5_24,2022,"One of the important objectives of smart cities is to provide electronic services to citizens, however, this requires the building of related software which is a time-consuming process. In this regard, smart city infrastructures require development tools that can help accelerate and facilitate software development (mobile, IoT, and web applications). Integrated Development Environments (IDEs) are well-known tools that have brought together the features of various tools within one package. Modern IDEs include the advantages of Artificial Intelligence (AI) and Cloud Computing. These technologies can help the developer overcome the complexities associated with multi-platform software products. This paper has explored AI techniques that are applied in IDEs. To this end, the Eclipse Theia (cloud-based IDE) and its AI-based extensions are explored as a case study. The findings show that recommender system models, language modeling, deep learning models, code mining, and attention mechanisms are used frequently to facilitate programming Furthermore, some researches have used NLP techniques and AI-based virtual assistance to promote the interaction between developers and projects."
Trusted Artificial Intelligence: Challenges and Promising Solutions,"Turdakov, DY; Avetisyan, AI; Arkhipenko, KV; Antsiferova, AV; Vatolin, DS; Volkov, SS; Gasnikov, AV; Devyatkin, DA; Drobyshevsky, MD; Kovalenko, AP; Krivonosov, MI; Lukashevich, NV; Malykh, VA; Nikolenko, SI; Oseledets, IV; Perminov, AI; Sochenkov, IV; Tikhomirov, MM; Fedotov, AN; Khachay, MY",10.1134/S1064562422060205,2022,"Wide applications of artificial intelligence technologies have led to new threats that cannot be effectively addressed using current tools for secure software development. To meet the challenge, the Research Center for Trusted Artificial Intelligence based on the Institute for Systems Analysis of the Russian Academy of Sciences was founded within the federal project Artificial Intelligence in 2021. Its objectives are the creation of scientific and technological basis for building trust in AI technologies. This paper discusses the risks and threats of applying artificial intelligence technologies, and describes the research directions and intermediate results produced at the Research Center for Trusted Artificial Intelligence."
Tags' Recommender to Classify Architectural Knowledge Applying Language Models,"Borrego, G; Gonzalez-Lopez, S; Palacio, RR",10.3390/math10030446,2022,"Agile global software engineering challenges architectural knowledge (AK) management since face-to-face interactions are preferred over comprehensive documentation, which causes AK loss over time. The AK condensation concept was proposed to reduce AK losing, using the AK shared through unstructured electronic media. A crucial part of this concept is a classification mechanism to ease AK recovery in the future. We developed a Slack complement as a classification mechanism based on social tagging, which recommends tags according to a chat/message topic, using natural language processing (NLP) techniques. We evaluated two tagging modes: NLP-assisted versus alphabetical auto-completion, in terms of correctness and time to select a tag. Fifty-two participants used the complement emulating an agile and global scenario and gave us their complement's perceptions about usefulness, ease of use, and work integration. Messages tagged through NLP recommendations showed fewer semantic errors, and participants spent less time selecting a tag. They perceived the component as very usable, useful, and easy to be integrated into the daily work. These results indicated that a tag recommendation system is necessary to classify the shared AK accurately and quickly. We will improve the NLP techniques to evaluate AK condensation in a long-term test as future work."
Requirements Elicitation Techniques and Tools in the Context of Artificial Intelligence,"Silva, AFD; Silva, GRS; Canedo, ED",10.1007/978-3-031-21686-2_2,2022,"Context: During software development in the context of Artificial Intelligence (AI), just like any other software, there is the requirements elicitation phase. In this phase, developers alongside stakeholders make use of various techniques, methodologies, and tools available to elicit software requirements. Problem: The need of understanding which techniques, methodologies, and tools are suitable in requirements elicitation in the context of AI, taking into consideration ethical issues. Solution: Investigation of the ICT practitioners' perception about their approaches regarding the requirements elicitation process for AI systems. Method: We have conducted a survey with ICT practitioners and reviewed the literature to identify requirements elicitation practices in the context of AI. Summary of Results: Most ICT practitioners work with the techniques and methodologies found in the literature. Regarding tools, our findings were inconclusive, as most practitioners do not use the tools identified in the literature, or even do not use any tools. As for ethical requirements, some were well consolidated with practitioners but others were not, such as the principle of equity and inclusion. Contributions and Impact in the IS area: An overview of how AI systems are being developed across organizations and the treatment that is given to ethical requirements. Our findings reveal that there is a need for organizations to consolidate ethical and legal notions with developers so that they can be applied during the requirements elicitation phase."
On the Track to Application Architectures in Public Transport Service Companies,"JÃ¼ngling, S; Fetai, I; Rogger, A; Morandi, D; Peraic, M",10.3390/app12126073,2022,"There are quite some machine learning (ML) models, frameworks, AI-based services or products from different IT solution providers available, which can be used as building blocks to embed and use in IT solution architectures of companies. However, the path from initial prototypical proof of concept solutions until the deployment of proven systems into the operational environment remains a major challenge. The potential of AI-based software components using ML or knowledge engineering (KE) is huge and the majority of small to medium enterprises are still unsure whether their internal developer teams should be extended by additional ML or KE skills to enrich their IT solution architectures with novel AI-based components where appropriate. How can enterprises manage the change and visualize the current state and foreseeable road-map? In the current paper, we propose an AI system landscape for the public transport sector, which is based on existing AI-domains and AI-categories defined by different technical reports of the European Commission. We collect use-cases from three different enterprises in the transportation sector and visualize them on the proposed domain specific AI-landscape. We provide some insights into different maturity levels of different AI-based components and how the different ML and KE based components can be embedded into an AI-based software development life-cycle (SDLC). We visualize, how the AI-based IT-solution architecture evolved over the last decades with respect to coupling and decoupling of layers and tiers in the overall Enterprise Architecture."
Exploring the Impact of Toxic Comments in Code Quality,"Sayago-Heredia, J; Chango, G; PÃ©rez-Castillo, R; Piattini, M",10.5220/0011039700003176,2022,"Software development has an important human-side, which implies that developers' feelings have a significant impact to software development and could affect developers' quality, productivity, and performance. In this paper, we explore the process to find, understand and relate the effects of toxic emotions on code quality. We propose a tool and sentiments dataset, a clean set of commit messages, extracted from SonarQube code quality metrics and toxic comments obtained from GitHub. Moreover, we perform a preliminary statistical analysis of the dataset. We apply natural language processing techniques to identify toxic developer sentiments on commits that could impact code quality. Our study describes data retrieval process along with tools used for performing a preliminary analysis. The preliminary dataset is available in CSV format to facilitate queries on the data and to investigate in depth factors that impact developer emotions. Preliminary results imply that there is a relationship between toxic comments and code quality that may affect the quality of the software project. Future research will be the development of a complete dataset and an in-depth analysis for efficiency validation experiments along with a linear regression. Finally, we will estimate the code quality as a function of developers' toxic comments."
Software Artifact Mining in Software Engineering Conferences: A Meta-Analysis,"Abou Khalil, Z; Zacchiroli, S",10.1145/3544902.3546239,2022,"Background: Software development results in the production of various types of artifacts: source code, version control system meta-data, bug reports, mailing list conversations, test data, etc. Empirical software engineering (ESE) has thrived mining those artifacts to uncover the inner workings of software development and improve its practices. But which artifacts are studied in the field is a moving target, which we study empirically in this paper. Aims: We quantitatively characterize the most frequently mined and co-mined software artifacts in ESE research and the research purposes they support. Method: We conduct a meta-analysis of artifact mining studies published in 11 top conferences in ESE, for a total of 9621 papers. We use natural language processing (NLP) techniques to characterize the types of software artifacts that are most often mined and their evolution over a 16-year period (2004-2020). We analyze the combinations of artifact types that are most often mined together, as well as the relationship between study purposes and mined artifacts. Results: We find that: (1) mining happens in the vast majority of analyzed papers, (2) source code and test data are the most mined artifacts, (3) there is an increasing interest in mining novel artifacts, together with source code, (4) researchers are most interested in the evaluation of software systems and use all possible empirical signals to support that goal. Conclusions: Our study presents a meta analysis of the usage of software artifacts in the field over a period of 16 years using NLP techniques."
Example Driven Code Review Explanation,"Rahman, S; Koana, UA; Nayebi, M",10.1145/3544902.3546639,2022,"Background: Code reviewing is an essential part of software development to ensure software quality. However, the abundance of review tasks and the intensity of the workload for reviewers negatively impact the quality of the reviews. The short review text is often unactionable. Aims: We propose the Example Driven Review Explanation (EDRE) method to facilitate the code review process by adding additional explanations through examples. EDRE recommends similar code reviews as examples to further explain a review and help a developer to understand the received reviews with less communication overhead. Method: Through an empirical study in an industrial setting and by analyzing 3,722 Code reviews across three open-source projects, we compared five methods of data retrieval, text classification, and text recommendation. Results: EDRE using TF-IDF word embedding along with an SVM classifier can provide practical examples for each code review with 92% F-score and 90% Accuracy. Conclusions: The example-based explanation is an established method for assisting experts in explaining decisions. EDRE can accurately provide a set of context-specific examples to facilitate the code review process in software teams."
Enhanced CUSUM control charts for monitoring Coefficient of Variation: A case study in Textile industry,"Tran, PH; Heuchenne, C; Thomassey, S",10.1016/j.ifacol.2022.09.552,2022,"The recent blooming developments of Artificial Intelligence (AI), Internet of Things (IoT), and Data Science (DS) have put Smart Manufacturing (SM) into a new context. This leads to more attractions on control charts as one of the useful tools that contribute to the success in SM by anomaly detection (AD) approach. Coefficient of variation (CV) is a recent popular statistic that is used in the quality control of SM. In this paper, we propose investigating the performance of Cumulative sum (CUSUM) control charts monitoring CV with a fast initial response (FIR) strategy. The chart parameters are also optimized according to the random shift size in a given interval with the proposed Nelder-Mead optimization algorithm. The numerical results show that the performance of FIR CUSUM-gamma(2) charts are greater than the initial CUSUM-gamma(2) ones. An example in monitoring yarn quality at the spinning mill with the design of FIR CUSUM-gamma(2) charts is also proposed. These findings are useful for practitioners as well as managers and researchers. The proposed design of FIR CUSUM-gamma(2) charts could be applied in other processes of various domains such as finance, business, industrial processes, etc. Copyright (C) 2022 The Authors."
Exploring Automated GDPR-Compliance in Requirements Engineering: A Systematic Mapping Study,"Aberkane, AJ; Poels, G; Broucke, SV",10.1109/ACCESS.2021.3076921,2021,"The General Data Protection Regulation (GDPR), adopted in 2018, profoundly impacts information processing organizations as they must comply with this regulation. In this research, we consider GDPR-compliance as a high-level goal in software development that should be addressed at the outset of software development, meaning during requirements engineering (RE). In this work, we hypothesize that natural language processing (NLP) can offer a viable means to automate this process. We conducted a systematic mapping study to explore the existing literature on the intersection of GDPR, NLP, and RE. As a result, we identified 448 relevant studies, of which the majority (420) were related to NLP and RE. Research on the intersection of GDPR and NLP yielded nine studies, while 20 studies were related to GDPR and RE. Even though only one study was identified on the convergence of GDPR, NLP, and RE, the mapping results indicate opportunities for bridging the gap between these fields. In particular, we identified possibilities for introducing NLP techniques to automate manual RE tasks in the crossing of GDPR and RE, in addition to possibilities of using NLP-based machine learning techniques to achieve GDPR-compliance in RE."
Ethics-based auditing of automated decision-making systems: intervention points and policy implications,"MÃ¶kander, J; Axente, M",10.1007/s00146-021-01286-x,2023,"Organisations increasingly use automated decision-making systems (ADMS) to inform decisions that affect humans and their environment. While the use of ADMS can improve the accuracy and efficiency of decision-making processes, it is also coupled with ethical challenges. Unfortunately, the governance mechanisms currently used to oversee human decision-making often fail when applied to ADMS. In previous work, we proposed that ethics-based auditing (EBA)-that is, a structured process by which ADMS are assessed for consistency with relevant principles or norms-can (a) help organisations verify claims about their ADMS and (b) provide decision-subjects with justifications for the outputs produced by ADMS. In this article, we outline the conditions under which EBA procedures can be feasible and effective in practice. First, we argue that EBA is best understood as a 'soft' yet 'formal' governance mechanism. This implies that the main responsibility of auditors should be to spark ethical deliberation at key intervention points throughout the software development process and ensure that there is sufficient documentation to respond to potential inquiries. Second, we frame AMDS as parts of larger sociotechnical systems to demonstrate that to be feasible and effective, EBA procedures must link to intervention points that span all levels of organisational governance and all phases of the software lifecycle. The main function of EBA should, therefore, be to inform, formalise, assess, and interlink existing governance structures. Finally, we discuss the policy implications of our findings. To support the emergence of feasible and effective EBA procedures, policymakers and regulators could provide standardised reporting formats, facilitate knowledge exchange, provide guidance on how to resolve normative tensions, and create an independent body to oversee EBA of ADMS."
Automatic Detection of Ambiguous Software Requirements: An Insight,"Riaz, MQ; Butt, WH; Rehman, S",10.1109/infoman.2019.8714682,2019,"Requirements Engineering is one of the most important phases of the software development lifecycle. The success of the whole software project depends upon the quality of the requirements. But as we know that mostly the software requirements are stated and documented in the natural language. The requirements written in natural language can be ambiguous and inconsistent. These ambiguities and inconsistencies can lead to misinterpretations and wrong implementations in design and development phase. To address these issues a number of approaches, tools and techniques have been proposed for the automatic detection of natural language ambiguities form software requirement documents. However, to the best of our knowledge, there is very little work done to compare and analyze the differences between these tools and techniques. In this paper, we presented a state of art survey of the currently available tools and techniques for the automatic detection of natural language ambiguities from software requirements. We also focused on figuring out the popularity of different tools and techniques on the basis of citations. This research will help the practitioners and researchers to get the latest insights in the above-mentioned context."
A Tool For Software Requirement Allocation Using Artificial Intelligence Planning,"Pereira, FC; Neto, GB; de Lima, LF; Silva, F; Peres, LM",10.1109/RE54965.2022.00032,2022,"This paper presents the AI Task Allocation tool (ATA') for allocating software requirements into versions using the artificial intelligence (AI) planning technique. A model using AI planning language is proposed for a software project representation containing a set of requirements and one or more development teams. The generated plan indicates the allocation of requirements considering as criteria: requirement development time, priority levels, and dependency relationships. A case study was carried out to assess the use of the ATA' tool to provide a plan that organizes the software requirements. The preliminary results indicated that plans allocate requirements according to the assigned criteria. Thus, the results suggested that ATA' can contribute to the planning of incremental development projects, allowing the requirement allocation among teams."
"Panel: Software development methods in the IoT-laden, AI/ML-driven era","Marco, J",10.1109/SCC55611.2022.00057,2022,"The growing of Internet of Things (IoT) driven with rapid advances in Artificial Intelligence (AI) with special emphasis in Machine Learning (ML) techniques herald a new era of software systems. As we enter this new era traditional software development methods are no longer fully adequate to deal with new issues inherent to the complexity of the new software systems. Hence, new software development methods are needed. This panel will explore these new methods."
Generating Abstract Test Cases from User Requirements using MDSE and NLP,"Allala, SC; Sotomayor, JP; Santiago, D; King, TM; Clarke, PJ",10.1109/QRS57517.2022.00080,2022,"Model-driven software engineering (MDSE) has emerged as a popular and commonly used method for designing software systems in which models are the primary development artifact over the last decade. MDSE has resulted in the trend toward further automating the software process. However, the generation of test cases from user requirements still lags in reaching the required level of automation. Given that most user requirements are written in natural language, the recent advances in natural language processing (NLP) provide an opportunity to further automate the test generation process. In this paper, we exploit the advances in MDSE and NLP to generate abstract test cases from user requirements written in structured natural language and the respective data model. We accomplish this by creating meta-models for user requirements and abstract test cases and defining the appropriate transformation rules. To support this transformation, helper methods are defined to extract the relevant information from user requirements related to testing. To show the feasibility of the approach, we developed a prototype and conducted a case study with use cases and test cases from a Payroll Management System."
Lyra: A Benchmark for Turducken-Style Code Generation,"Liang, QY; Sun, ZY; Zhu, QH; Zhang, WJ; Yu, L; Xiong, YF; Zhang, L",,2022,"Recently, neural techniques have been used to generate source code automatically. While promising for declarative languages, these approaches achieve much poorer performance on datasets for imperative languages. Since a declarative language is typically embedded in an imperative language (i.e., the turducken-style programming) in real-world software development, the promising results on declarative languages can hardly lead to significant reduction of manual software development efforts. In this paper, we define a new code generation task: given a natural language comment, this task aims to generate a program in a base imperative language with an embedded declarative language. To our knowledge, this is the first turducken-style code generation task. For this task, we present Lyra: a dataset in Python with embedded SQL. This dataset contains 2,000 carefully annotated database manipulation programs from real-world projects. Each program is paired with both a Chinese comment and an English comment. In our experiment, we adopted Transformer, BERT-style, and GPT-style models as baselines. In the best setting, the generation performance of GPT-style models is better than others, where the AST exact matching accuracy is 24% and 25.5% when using Chinese and English comments, respectively. Therefore, we believe that Lyra provides a new challenge for code generation. Yet, overcoming this challenge may significantly boost the applicability of code generation techniques for real-world software development."
Mathematical foundations based statistical modeling of software source code for software system evolution,"Althar, RR; Alahmadi, A; Samanta, D; Khan, MZ; Alahmadi, AH",10.3934/mbe.2022170,2022,"Source code is the heart of the software systems; it holds a wealth of knowledge that can be tapped for intelligent software systems and leverage the possibilities of reuse of the software. In this work, exploration revolves around making use of the pattern hidden in various software development processes and artifacts. This module is part of the smart requirements management system that is intended to be built. This system will have multiple modules to make the software requirements management phase more secure from vulnerabilities. Some of the critical challenges bothering the software development community are discussed. The background of Machine Learning approaches and their application in software development practices are explored. Some of the work done around modeling the source code and approaches used for vulnerabilities understanding in software systems are reviewed. Program representation is explored to understand some of the principles that would help in understanding the subject well. Further deeper dive into source code modeling possibilities are explored. Machine learning best practices are explored inline with the software source code modeling."
Educating AI Software Engineers: Challenges and Opportunities,"Bublin, M; Schefer-Wenzl, S; Miladinovic, I",10.1007/978-3-030-93907-6_26,2022,"To properly develop, test and use Artificial Intelligence (AI) applications, students and professionals need a well-defined AI software engineering (AISE) process and the appropriate tools. However, AISE, which is today mainly based on the use of deep learning (DL) neural networks, is still under development. This makes the education of AI software engineers particularly challenging, since there are no well-established methodologies, tools and practices, like in traditional Software Engineering (SE) education drawing on decades of experience and methods in all phases of software development, from requirements analysis over design and implementation to integration and testing. We analyze the main differences between traditional SE and AISE education and address challenges in AISE education. Our methodology is based on literature survey, analysis of own industry experience and statistical analysis of students works on AI applications. Our goal is to provide guidelines for an AISE process and propose a curriculum path for AISE education, which can be used to update a traditional SE curriculum. According to results of our analysis, the main challenges for the students are: Dealing with data and taking into account that algorithms change (learn) by data, selection and re-use of AI algorithms, model test, maintenance and automatizing the AISE process. We propose to address these challenges in SE curricula by teaching more statistical thinking with connections to software development, developing re-engineering capabilities, teaching a model-based AI approach and combining AI with virtual reality simulations. In the whole process, we consider an optimal division of work between humans and AI systems by explicitly including humans in the AISE loop."
Neural Model for Generating Method Names from Combined Contexts,"Varner, Z; OguztÃ¼zÃ¼n, Ã; Long, F",10.1109/STC55697.2022.00009,2022,"The names given to methods within a software system are critical to the success of both software development and maintenance. Meaningful and concise method names save developers both time and effort when attempting to understand and use the code. Our study focuses on learning concise and meaningful method names from word tokens found within the contexts of a method, including the method documentation, input parameters, return type, method body, and enclosing class. Combining the approaches of previous studies, we constructed both an RNN encoder-decoder model with attention as well as a Transformer model, each tested using different combinations of contextual information as input. Our experiments demonstrate that a model that uses all of the mentioned contexts will have a higher performance than a model that uses any subset of the contexts. Furthermore, we demonstrate that the Transformer model outperforms the RNN model in this scenario."
Cognitive Human Factors in the Artificial Intelligence of Things,"Cecilio, A",10.1109/SCC55611.2022.00058,2022,"Internet of Things (IoT) systems are increasingly becoming complex. Heterogeneity in terms of hardware, software, computing capacity and connectivity is a source of complexity. The conversion of IoT systems into cyber-physical systems, including devices that are able not only to collect but also to process and take decisions, in real-time is a second source of complexity. Moreover, not only sensors should be considered, but also actuators, especially robots in the industry domain. In this context Artificial Intelligence (AI) technologies provide powerful capabilities to endow IoT devices with intelligent services, leading to the so-called Artificial Intelligence of Things (AIoT). In this context, the operator/user is in the middle of this complexity trying to understand the current situation and make effective real-time decisions. Hence, human factors, especially the cognitive ones, is a major issue to be addressed. New software development methods in the form of assistants and wizards are necessary to help operators/users to be context-aware and reduce their technical workload about coding or computer-oriented skills, focusing on the task/service at hands."
The Design and Implementation of Quantum Finance Software Development Kit (QFSDK) for AI Education,"Wang, LC; Lee, RST",10.1109/ITHET56107.2022.10031712,2022,"For the past several decades, with the rapid development of internet technologies and online transaction platforms, financial institutions and investors are facing huge challenges from the global economic environment that financial markets are becoming more unpredictable and volatile than before, especially in the stock markets, commodity markets and cryptocurrency markets. Interest and awareness of Artificial Intelligence and Quantum Finance are growing so fast that both academia and higher education are struggling to keep up with the accelerating demand of financial markets. Quantum finance is a newly developed interdisciplinary program with the integration of quantum theory, computational finance, and even computer science, which requires students to have comprehensive knowledge reserves. Meanwhile, it is extremely complicated for students to use a programming language to realize quantum finance calculations from scratch. To facilitate curricula teaching, and hands-on usage of quantum finance and AI, a Quantum Finance Software Development Kit (QFSDK) is proposed based on the author's previous research on Quantum Finance Theory and other AI research findings. The QFSDK was prepared in python programming language as the first step in introducing students to the concepts and applications of quantum finance. The QFSDK bridges the theoretical and practical chasm for learners by developing a quantum finance calculator library. It serves as an open-source template that encourages heavy contextual modification, and it supports any online platforms in the python programming language."
Challenges in Requirements Engineering and Its Solutions: A Systematic Review,"Mello, OD; Fontoura, LM",10.5220/0011079000003179,2022,"Software development projects are susceptible to many adversities throughout their life cycle that can be originated, among several reasons, of a low-quality specification and management of requirements. To ensure the Requirements Engineering activities are conducted correctly, researchers study and apply various techniques to predict and avoid the negative impacts that may occur in projects. The main goal of this research is to identify which techniques have been used to solve problems related to requirements management in software projects. We retrieved and reviewed studies published across various scientific databases to answer research questions that were previously defined. From this work, it was possible to obtain a better understanding of the most common problems in the Requirements Engineering field, as well as some techniques that currently exist to solve them. We also identified that Artificial Intelligence has been widely explored to improve the activities of the field."
A Comparison of Recent Requirements Gathering and Management Tools in Requirements Engineering for IoT-Enabled Sustainable Cities,"Nadeem, MA; Lee, SUJ; Younus, MU",10.3390/su14042427,2022,"The Internet of Things (IoT) is a paradigm that facilitates the proliferation of different devices such as sensors and Radio Frequency Identification (RFIDs) for real-time applications such as healthcare and sustainable cities. The growing popularity of IoT opens up new possibilities, and one of the most notable applications is related to the evolving sustainable city paradigm. A sustainable city is normally designed in such a way to consider the environmental impact and a social, economic, and resilient habitat for existing populations without compromising the ability of future generations to experience the same, while the process of managing project requirements is known as requirements management. To design a high-quality project, effective requirements management is imperative. A number of techniques are already available to perform the requirement gathering process, and software developers apply them to collect the requirements. Nevertheless, they are facing many issues in gathering requirements due to a lack of literature on the selection of appropriate methods, which affects the quality of the software. The software design quality can be improved by using requirements capture and management techniques. Some tools are used to comprehend the system accurately. In this paper, a qualitative comparison of requirements-gathering tools using Artificial Intelligence (AI) and requirements-management tools is presented for sustainable cities. With all the tools and techniques available for capturing and managing requirements, it has been proven that software developers have a wide range of alternatives for selecting the best tool that fits their needs, such as chosen by the AI agent. This effort will aid in the development of requirements for IoT-enabled sustainable cities."
A Grey Literature Review on the Impacts of Covid-19 in Software Development,"Quadros, E; Prikladnicki, R; Lahm, R",10.5220/0011116100003179,2022,"The workplace has been changed by Covid-19. But what is the meaning of the work from home phenomenon in software development? This paper aims to investigate the work from home pandemic phenomenon in software development. Between October 2019 and December 2021, the Grey Literature review was carried out to investigate 25,251 records, collected through a scraper written in python language. Descriptive analysis was performed using data science and artificial intelligence techniques. We developed a methodology to optimize the collection and extraction of insights from the Grey Literature and reveal perceptions or cognitive distances from the social representation of the impacts of Covid-19 in software development. The main contributions of this paper are to show how Grey Literature may contribute to anticipate findings, reveal changes in the discourse regarding the effects of the pandemic on the work model, and show that in early 2021 the desire for flexibility pressed for a hybrid model. This type of literature review can assist in strategies to deal with events such as Covid-19."
Semantic Similarity-Based Clustering of Findings From Security Testing Tools,"Schneider, P; Voggenreiter, M; Gulraiz, A; Matthes, F",,2022,"Over the last years, software development in domains with high security demands transitioned from traditional methodologies to uniting modern approaches from software development and operations (DevOps). Key principles of DevOps gained more importance and are now applied to security aspects of software development, resulting in the automation of security-enhancing activities. In particular, it is common practice to use automated security testing tools that generate reports after inspecting a software artifact from multiple perspectives. However, this raises the challenge of generating duplicate security findings. To identify these duplicate findings manually, a security expert has to invest resources like time, effort, and knowledge. A partial automation of this process could reduce the analysis effort, encourage DevOps principles, and diminish the chance of human error. In this study, we investigated the potential of applying Natural Language Processing for clustering semantically similar security findings to support the identification of problem-specific duplicate findings. Towards this goal, we developed a web application for annotating and assessing security testing tool reports and published a human-annotated corpus of clustered security findings. In addition, we performed a comparison of different semantic similarity techniques for automatically grouping security findings. Finally, we assess the resulting clusters using both quantitative and qualitative evaluation methods."
Reuse of Test Case based on Attributes Weight Optimization,"Shi, YQ; Huang, S; Wan, JY",10.1109/QRS57517.2022.00054,2022,"Software testing is complicated and requires a lot of manpower and material resource in the software life cycle. The design of test cases costs a lot of time. In order to improve the efficiency of software testing in the test cases design stage, this paper uses historical test assets to assist the design of test cases in new project, and proposes a test case reuse method based on attribute weight optimization. Firstly, the text vector of test data is obtained by using Natural Language Processing. The test case package is formed based on the keyword extraction and the test case clustering, and the test case vector library is constructed. Then, a test case attribute weight optimization method based on the Genetic Simulated Annealing Algorithm is proposed. Combined with the optimized attribute weights, the test case reuse is realized by using the similarity calculation of the test case data vector. Finally, the test case reuse method is experimentally verified by two projects with different types. Experimental results show that this method can effectively improve the efficiency of test cases' design. It has better understandability and maintainability, and improve the quality of test cases."
DoMoBOT: A Bot for Automated and Interactive Domain Modelling,"Saini, R; Mussbacher, G; Guo, JLC; Kienzle, J",10.1145/3417990.3421385,2020,"Domain modelling transforms domain problem descriptions written in natural language (NL) into analyzable and concise domain models (class diagrams) during requirements analysis or the early stages of design in software development. Since the practice of domain modelling requires time in addition to modelling skills and experience, several approaches have been proposed to automate or semi-automate the construction of domain models from problem descriptions expressed in NL. Despite the existing work on domain model extraction, some significant challenges remain unaddressed: (i) the extracted domain models are not accurate enough to be used directly or with minor modifications in software development, (ii) existing approaches do not facilitate the tracing of the rationale behind the modelling decisions taken by the model extractor, and (iii) existing approaches do not provide interactive interfaces to update the extracted domain models. Therefore, in this paper, we introduce a domain modelling bot called DoMoBOT, explain its architecture, and implement it in the form of a web-based prototype tool. The bot automatically extracts a domain model from a problem description written in NL with an accuracy higher than existing approaches. Furthermore, the bot enables modellers to update a part of the extracted domain model and in response the bot re-configures the other parts of the domain model pro-actively. To improve the accuracy of extracted domain models, we combine the techniques of Natural Language Processing and Machine Learning. Finally, we evaluate the accuracy of the extracted domain models."
Improving bug report triage performance using artificial intelligence based document generation model,"Lee, DG; Seo, YS",10.1186/s13673-020-00229-7,2020,"Artificial intelligence is one of the key technologies for progression to the fourth industrial revolution. This technology also has a significant impact on software professionals who are continuously striving to achieve high-quality software development by fixing various types of software bugs. During the software development and maintenance stages, software bugs are the major factor that can affect the cost and time of software delivery. To efficiently fix a software bug, open bug repositories are used for identifying bug reports and for classifying and prioritizing the reports for assignment to the most appropriate software developers based on their level of interest and expertise. Owing to a lack of resources such as time and manpower, this bug report triage process is extremely important in software development. To improve the bug report triage performance, numerous studies have focused on a latent Dirichlet allocation (LDA) using the k-nearest neighbors or a support vector machine. Although the existing approaches have improved the accuracy of a bug triage, they often cause conflicts between the combined techniques and generate incorrect triage results. In this study, we propose a method for improving the bug report triage performance using multiple LDA-based topic sets by improving the LDA. The proposed method improves the existing topic sets of the LDA by building two adjunct topic sets. In our experiment, we collected bug reports from a popular bug tracking system, Bugzilla, as well as Android bug reports, to evaluate the proposed method and demonstrate the achievement of the following two goals: increase the bug report triage accuracy, and satisfy the compatibility with other state-of-the-art approaches."
Towards Effective AI-powered Agile Project Management,"Dam, HK; Tran, T; Grundy, J; Ghose, A; Kamei, Y",10.1109/ICSE-NIER.2019.00019,2019,"The rise of Artificial intelligence (AI) has the potential to significantly transform the practice of project management. Project management has a large socio-technical element with many uncertainties arising from variability in human aspects, e.g. customers' needs, developers' performance and team dynamics. AI can assist project managers and team members by automating repetitive, high-volume tasks to enable project analytics for estimation and risk prediction, providing actionable recommendations, and even making decisions. AI is potentially a game changer for project management in helping to accelerate productivity and increase project success rates. In this paper, we propose a framework where AI technologies can be leveraged to offer support for managing agile projects, which have become increasingly popular in the industry."
Conceptual Mappings of Conventional Software and Machine Learning-based Applications Development,"Angel, S; Namin, AS",10.1109/COMPSAC54236.2022.00193,2022,"This paper presents two concept models to compare and contrast conventional software development life cycle(SDLC) and the development of smart applications where Artificial Intelligence/Machine Learning (AI/ML) modules are integrated into the software products. The first concept model illustrates the intersections and differences between SDLC and AI/ML-based application development; whereas, the second concept model depicts the processes/concepts involved in data preprocessing stage of AI/ML-based application development. The concept models show that while there are some similarities between SDLC and AI/ML-based application development; AI/ML-based development follows its own unique features and challenges that need development of further software engineering techniques."
Towards Automatic Classification of Design Decisions from Developer Conversations,"Josephs, A; Gilson, F; Galster, M",10.1109/ICSA-C54293.2022.00009,2022,"Documentation of architecture design decisions can be cumbersome, though it is critical to ongoing software maintenance and integration of new features. In practice, developers often discuss design decisions in online communication tools, such as Slack, Microsoft Teams or Gitter. In this paper, we introduce a design decision bot for Slack to reduce the workload for documenting design decisions. The bot heralds the integration of machine learning and language processing to decision recording. Using available communication tools and interactions (Slack and Slack bots) allows developers to document design decisions mid-stride without diverting attention from other development tasks. The bot utilises a transformer for decision classification, avoiding rigid and onerous interactions between the bot and developers. The transformer builds on Bi-directional Encoder Representations from Transformers (BERT), a machine learning technique for Natural Language Processing (NLP). In this paper, we present the theoretical concepts of the bot with early implementation details and the results of preliminary evaluations."
ArtBrain-K: AI Processor based-on 5-PetaFLOPS AI Server System,"Han, J; Lyuh, CG; Shin, K; Kim, HM; Kwon, H; Chung, J; Cho, CP; Kim, J; Suk, J; Kim, C; Choi, M; Kwon, Y",10.1109/AICAS54282.2022.9869983,2022,"We developed ArtBrain-K, an artificial intelligence server system with a performance of 5 PetaFLOPS per rack that can have a small form factor with low power, 2400W based on a proprietary artificial intelligence processor with 40 TeraFLOPS performance, and 15W power consumption. It consists of 8 artificial intelligence compute nodes with a performance of 590 TeraFLOPS and 300W power consumption which loaded with 20 ABrain-S NPU boards based on the artificial intelligence processor. Also, we developed A1WareRT, a software development environment for independent artificial intelligence processor. Compared to the existing GPU system, the performance is 3 times higher and the power efficiency is 7 times higher. It has been applied to the next-generation airport automatic immigration system and image recognition-based security system, and it can be used in fields that require enormous computing resources for data processing and learning, such as a huge neural network such as a transformer-based artificial intelligence algorithm."
A real-world case study for automated ticket team assignment using natural language processing and explainable models,"Pavelski, LM; Braga, RD",10.1145/3551349.3561164,2022,"In the context of software development, managing and organizing agile boards of multi-disciplinary teams distributed around the world is a great challenge, especially regarding the process of assigning tickets to the correct team roles. Incorrectly assigned tickets can result in significant resource waste in any project and directly influence delivery outcomes and project costs. This work proposes a method for ticket analysis and automatic team assignment using Natural Language Processing and explainable Machine Learning models. Results show that the models perform well on a real-world team assignment task and provide insights into their decision."
Efficient Parallel Wikipedia Internal Link Extraction for NLP-Assisted Requirements Understanding,"Allen, J; Reddivari, S",10.1109/COMPSAC54236.2022.00077,2022,"Requirements engineering (RE) is a critical set of activities in the software development lifecycle (SDLC). Without effective requirements elicitation, organization, communication, and understanding software engineers cannot build quality software. Thus, it is necessary for software stakeholders to facilitate the SDLC by following best practices and utilizing software tools as needed to ensure requirements are well understood. One area where RE still faces issues, despite stakeholders' best efforts, is the communication of requirements amongst the various stakeholders. Software stakeholders consist of the customers, developers, managers, end users, and others with a vested interest in the software, and they typically all have different skillsets, backgrounds, vernaculars, and understanding of the requirements. These differences naturally lead to miscommunications which can lead to redundant, missing, or conflicting requirements, especially when customer and end user domains include complex vocabularies developers may not be accustomed to, and vice versa, e.g., biology, physics, and medicine. One approach in recent works to address this challenge has been to bridge the communication gap between stakeholders by constructing domain-specific ontologies using natural language processing (NLP) and Wikipedia [1]. With these ontologies, stakeholders have a convenient tool they can use to translate and understand specific requirements in the terminologies they're accustomed to. These techniques have shown promising potential, however there are computational challenges associated with efficiently handling a large dataset like Wikipedia. In particular, parsing internal links from Wikipedia article metadata can be a bottleneck in such ontology-construction systems. In this work we address this issue by implementing a program for memoryefficient parallel internal link extraction from Wikipedia articles. This builds on the work of Rodriguez et al. [2] by optimizing additional phases in the knowledge acquisition process."
How to manage a task-oriented virtual assistant software project: an experience report,"Li, SY; Guo, JQ; Gao, Y; Lou, JG; Yang, DJ; Xiao, Y; Zhou, YD; Liu, T",10.1631/FITEE.2100467,2022,"Task-oriented virtual assistants are software systems that provide users with a natural language interface to complete domain-specific tasks. With the recent technological advances in natural language processing and machine learning, an increasing number of task-oriented virtual assistants have been developed. However, due to the well-known complexity and difficulties of the natural language understanding problem, it is challenging to manage a task-oriented virtual assistant software project. Meanwhile, the management and experience related to the development of virtual assistants are hardly studied or shared in the research community or industry, to the best of our knowledge. To bridge this knowledge gap, in this paper, we share our experience and the lessons that we have learned at managing a task-oriented virtual assistant software project at Microsoft. We believe that our practices and the lessons learned can provide a useful reference for other researchers and practitioners who aim to develop a virtual assistant system. Finally, we have developed a requirement management tool, named SpecSpace, which can facilitate the management of virtual assistant projects."
Specific Aspects of Software Development Process for AI/ML-based Systems,"Liubchenko, V",10.1109/CSIT56902.2022.10000821,2022,AI/ML-based software systems have spread in different business domains because they provide solutions to various problems. But the development process requires performing additional activities and incorporating additional team members' skills. It creates new challenges for software engineers. We described the core differences in the software development process for AI/ML-based systems and the known approaches to working with them.
Natural Language Processing Application on Commit Messages: A Case Study on HEP Software,"Yang, Y; Ronchieri, E; Canaparo, M",10.3390/app122110773,2022,"Version Control and Source Code Management Systems, such as GitHub, contain a large amount of unstructured historical information of software projects. Recent studies have introduced Natural Language Processing (NLP) to help software engineers retrieve information from a very large collection of unstructured data. In this study, we have extended our previous study by increasing our datasets and machine learning and clustering techniques. We have followed a complex methodology made up of various steps. Starting from the raw commit messages we have employed NLP techniques to build a structured database. We have extracted their main features and used them as input of different clustering algorithms. Once each entry was labelled, we applied supervised machine learning techniques to build a prediction and classification model. We have developed a machine learning-based model to automatically classify commit messages of a software project. Our model exploits a ground-truth dataset that includes commit messages obtained from various GitHub projects belonging to the High Energy Physics context. The contribution of this paper is two-fold: it proposes a ground-truth database and it provides a machine learning prediction model that automatically identifies the more change-prone areas of code. Our model has obtained a very high average accuracy (0.9590), precision (0.9448), recall (0.9382), and F1-score (0.9360)."
An Effective Low-Dimensional Software Code Representation using BERT and ELMo,"Majumdar, S; Varshney, A; Das, PP; Clough, PD; Chattopadhyay, S",10.1109/QRS57517.2022.00082,2022,"Contextualised word representations (e.g., ELMo and BERT) have been shown to outperform static representations (e.g., Word2vec, Fasttext, and GloVe) for many NLP tasks. In this paper, we investigate the use of contextualised embeddings for code search and classification, an area receiving less attention. We construct CodeELMo by training ELMo from scratch and fine tuning CodeBERT embeddings using masked language modeling based on natural language (NL) texts related to software development concepts and programming language (PL) texts consisting of method comment pairs from open source code bases. The dimensionality of the Finetuned Code BERT embeddings is reduced using linear transformations and augmented with a CodeELMo representation to develop CodeELBE - a low-dimensional contextualised software code representation. Results for binary classification and retrieval tasks show that CodeELBE(1) considerably improves retrieval performance on standard deep code search datasets compared to CodeBERT and baseline BERT models."
Natural language processing: a proposal for word-per-minute evaluation in students' performance within the Classroom,"Reyes, SFV; Arias-Castillo, J; MelÃ©ndez-Armenta, RA; Arguijo, P; LÃ³pez, JAHV",10.1109/ROPEC55836.2022.10018720,2022,"The objective of this research is to evaluate the number of words per minute by using a mobile application that implements the natural language processing and that contributes to the reading comprehension of students of 5th and 6th grade of elementary education in Mexico. The application was developed with the software development kit Flutter to do the interface programming and the library Vosk that implements the natural language processing. The results demonstrate the ability of the mobile application to detect the words that student is capable to read during the aloud reading. In addition, the application registers the time of the reading and finally, ask questions at the end of each proof. These results allow to calculate the reading comprehension of the student by the natural language processing that give a general result of the performance at the end of the test."
Industrial Machine Learning for Enterprises (IML4E),"Grossmann, J; Nurminen, JK",,2022,"Smart software solutions, i.e., software that includes artificial intelligence (AI) and machine learning (ML), have shown a great potential to automate processes that were previously not accessible to automation. These areas include predictive maintenance, the creation of clinical diagnoses, recommendations systems, speech, image and scenario recognition, automated driving etc. Since AI and ML differ from classical software development regarding fundamental activities and processes, it is currently unclear how AI and ML can be integrated into existing industrial-grade software development processes. Addressing the industrialization of ML development and operations, the IML4E project will directly address the specifics of AI and ML by providing interoperability, automation and reusability in the data and the training pipeline. Moreover, IML4E enables continuous quality assurance and supervision for different types of machine learning (supervised learning, unsupervised learning, etc.) throughout the whole life cycle of a smart software solution. In this project presentation, we will focus in particular on the quality assurance and testing research planned in the project."
Utilizing AI in Test Automation to Perform Functional Testing on Web Application,"Alamleh, D",10.1007/978-3-031-10464-0_24,2022,"Artificial Intelligence is the trend in software development. Unfortunately, Artificial Intelligence algorithms and technologies are still not utilized enough in software testing. Designing Test automation has become the main job for quality engineers and software testers. Mainly, Test Automation is beneficial in reducing manual testing efforts Utilizing AI in test automation can form a huge benefit in code optimization and test oracle problem. The primary objective of the research was to approve the usability of the Fuzzy Inference System in providing a test oracle for web application functional testing. The secondary objective was to utilize Artificial Intelligence techniques like self-healing for the test Automation using web scraping. Also, to compare the web scraping approach and the Image processing approach in locating the web elements on the websites dynamically. I have addressed the problem by developing Test Automation that verifies the search functionality for a given website. The hypothesis is mainly to check if the Fuzzy Inference System can predict if the search functionality for a given website is working or not. I tested the hypothesis on ten different websites. Then, after I analysed the results, I have found that implementing the Fuzzy Inference System in test automation can form a reasonable solution for the test oracle problem. Furthermore, using the Fuzzy Inference System is as efficient as the manually prepared test oracle that covers all the possible cases for the inputs using if-else statements. Finally, I have demonstrated how web scraping can be utilized to perform self-healing for the test Automation."
RETRACTED: Design of Integrated Management System of IPE Instructional Resources Information Based on Artificial Intelligence Environment (Retracted Article),"Wang, D",10.1155/2022/4594090,2022,"With the development of modern networks and their fast and convenient characteristics, the construction of instructional resources has become the focus of information construction in universities. Based on AI (Artificial intelligence) technology, this paper discusses the software development technology used in the comprehensive management system of IPE instructional resources. According to the application environment and application objects of the system, a comprehensive information management system of IPE instructional resources based on AI technology is constructed. In this paper, a concept similarity algorithm based on semantic similarity and semantic correlation is proposed to calculate the distance between words and establish concept mapping pairs. The semantic analysis of the information input by students is realized, and the proposed concept similarity algorithm is used in the domain ontology knowledge base to search for similar courses and recommend them to students for their choice. The test results show that the stability of this system can reach about 96%, which is about 10% higher than other systems. The comprehensive management system of instructional resources information proposed in this paper will promote educators and learners to find high-quality IPE instructional resources."
Intelligent Covid-19 Vaccine Supplychain Management System,"Abdelgani, Y; Makki, QH; Ali, H; AlZu'bi, S",10.1109/ICICS55353.2022.9811172,2022,"In light of the trend of countries to give vaccines to their people, it was concluded that the best solution to confront the COVID-19 pandemic is to give the vaccine, as it prevents its transmission from person to person, and if the infection is infected, the symptoms are much less than the degree of danger of the person who did not meet the vaccine. In this paper, we presented tables on the types of vaccines, a brief explanation of the Coronavirus, and the idea of our software project. Vaccine handling skills, where gives users the opinions of the people who received the vaccine and the doctors also according to the age group and the disease."
VIA: A Virtual Informative Assistant for Smart Tourism,"LÃ³pez, MC; HernÃ¡ndez, D; Navarro-Newball, AA; Prakash, EC",10.1007/978-3-030-96753-6_2,2022,"Tourism can be approached from the technological virtual continuum, calling it smart tourism which is related to cultural computing. To achieve that, we implemented an application for mobile devices to enrich the visit of locals and foreigners to the San Antonio neighbourhood in the city of Santiago de Cali, Colombia using concepts such as augmented reality, natural language processing and techniques such as speech-to-text, text-to-speech, skeletal animation and blend shapes. The main objective is to guide and inform about the neighbourhood in order to provide awareness of it as a heritage site. We developed the application following a software development cycle which included analysis, design, prototyping, implementation, and validation. Particularly, in validation, we performed unitary tests, functional tests and user tests. The resulting application was well received among participants in the test."
Technical solutions for automation of distribution networks based on SPM technology,"Piskunov, SA; Mokeev, AV; Popov, AI; Ulyanov, DN; Rodionov, AV",10.1109/SGSMA51733.2022.9805844,2022,"The paper considers the automation of 6-20 kV distribution networks based on synchronized phasor measurement (SPM) technology. The first part of the paper is devoted to the technical system providing localization of faults in the network with isolated and compensated neutral. The paper describes the system, its main functions, architecture, advantages. The second part of the paper relates to the automation of power supply centers of the distribution network - step-down substations (SS) and distribution points (DP). The authors propose using SPM to implement new protection principles of busbars and power transformers. In addition, multifunctional systems are considered that ensure the execution of several network automation tasks at once: monitoring the state of a power transformer, energy monitoring, energy metering, telemechanics, relay protection, and automation (RPA). The paper presents and describes technical devices based on which the operation of the considered systems is carried out. Presented solutions provide integration with digital substation technology, WAMPAC systems."
NEKO: Proposal of the first super-agile methodology to improve work efficiency,"Gavelan, JPM; Mezones, PPP; Rodriguez, C",10.1109/ICCI54321.2022.9756085,2022,"Agile methodologies are the preferred strategy for work teams around the world thanks to their rapid response to changes and adaptability. However, this prioritization of efficiency and speed has neglected the well-being of the most important element of any project: the teams. This article presents NEKO as a super-agile methodology focused on people and that aims to prioritize the mental health of workers without neglecting effectiveness, relying on new technologies such as Artificial Intelligence to enhance its processes. During the research stage, using the methodology proposed by Kitchenham, fifteen articles were analyzed and allowed us to rescue the strengths and identify the problems presented by the agile methodologies most used today. Then, the proposal was developed dividing it into three phases; each including events, artifacts, and tools to achieve your goal. Later, during the discussion, the proposal was contrasted with the methodologies initially analyzed and it was concluded that taking the points in favor of these and adding state-of-the-art technology focused on the well-being of the teams is synonymous with efficiency, meaning a possible change in the paradigm of the current world of work."
A Comparison of Different Source Code Representation Methods for Vulnerability Prediction in Python,"Bagheri, A; Hegedus, P",10.1007/978-3-030-85347-1_20,2021,"In the age of big data and machine learning, at a time when the techniques and methods of software development are evolving rapidly, a problem has arisen: programmers can no longer detect all the security flaws and vulnerabilities in their code manually. To overcome this problem, developers can now rely on automatic techniques, like machine learning based prediction models, to detect such issues. An inherent property of such approaches is that they work with numeric vectors (i.e., feature vectors) as inputs. Therefore, one needs to transform the source code into such feature vectors, often referred to as code embedding. A popular approach for code embedding is to adapt natural language processing techniques, like text representation, to automatically derive the necessary features from the source code. However, the suitability and comparison of different text representation techniques for solving Software Engineering (SE) problems is rarely studied systematically. In this paper, we present a comparative study on three popular text representation methods, word2vec, fastText, and BERT applied to the SE task of detecting vulnerabilities in Python code. Using a data mining approach, we collected a large volume of Python source code in both vulnerable and fixed forms that we embedded with word2vec, fastText, and BERT to vectors and used a Long Short-Term Memory network to train on them. Using the same LSTM architecture, we could compare the efficiency of the different embeddings in deriving meaningful feature vectors. Our findings show that all the text representation methods are suitable for code representation in this particular task, but the BERT model is the most promising as it is the least time consuming and the LSTM model based on it achieved the best overall accuracy (93.8%) in predicting Python source code vulnerabilities."
Towards a Generation of Class Diagram from User Stories in Agile Methods,"Nasiri, S; Rhazali, Y; Lahmer, M; Chenfour, N",10.1016/j.procs.2020.03.148,2020,"Model-Driven Architecture (MDA) is a framework for software development processes that allows an automatic transformation from a business process model to the code model. In MDA there are two transformation kinds: Transformation from the Computation independent model (CIM) to platform-independent model (PIM), and transformation from PIM to platform-specific model (PSM). In this paper, we based on CIM to PIM transformation. This transformation is done by developing a platform that generates a class diagram, presented in XMI file, from specifications that are presented in user stories, which are written in natural language (English). We used a natural language processing (NLP) tool named Stanford CoreNLP for extracting of the object-oriented design elements. Applying our approach to several case studies has given good results. (C) 2020 The Authors. Published by Elsevier B.V."
Designing a Risk Assessment Tool for Artificial Intelligence Systems,"Nagbol, PR; MÃ¼ller, O; Krancher, O",10.1007/978-3-030-82405-1_32,2021,"Notwithstanding its potential benefits, organizational AI use can lead to unintended consequences like opaque decision-making processes or biased decisions. Hence, a key challenge for organizations these days is to implement procedures that can be used to assess and mitigate the risks of organizational AI use. Although public awareness of AI-related risks is growing, the extant literature provides limited guidance to organizations on how to assess and manage AI risks. Against this background, we conducted an Action Design Research project in collaboration with a government agency with a pioneering AI practice to iteratively build, implement, and evaluate the Artificial Intelligence Risk Assessment (AIRA) tool. Besides the theory-ingrained and empirically evaluated AIRA tool, our key contribution is a set of five design principles for instantiating further instances of this class of artifacts. In comparison to existing AI risk assessment tools, our work emphasizes communication between stakeholders of diverse expertise, estimating the expected real-world positive and negative consequences of AI use, and incorporating performance metrics beyond predictive accuracy, including thus assessments of privacy, fairness, and interpretability."
AI in Software Engineering at Facebook,"Bader, J; Kim, SS; Luan, FS; Chandra, S; Meijer, E",10.1109/MS.2021.3061664,2021,"How can artificial intelligence help software engineers better do their jobs and advance the state of the practice? We describe three productivity tools that learn patterns from software artifacts: code search using natural language, code recommendation, and automatic bug fixing."
Advancing Motivational Interviewing Training with Artificial Intelligence: ReadMI,"Hershberger, PJ; Pei, Y; Bricker, DA; Crawford, TN; Shivakumar, A; Vasoya, M; Medaramitta, R; Rechtin, M; Bositty, A; Wilson, JF",10.2147/AMEP.S312373,2021,"Background: Motivational interviewing (MI) is an evidence-based, brief interventional approach that has been demonstrated to be highly effective in triggering change in high-risk lifestyle behaviors. MI tends to be underutilized in clinical settings, in part because of limited and ineffective training. To implement MI more widely, there is a critical need to improve the MI training process in a manner that can provide prompt and efficient feedback. Our team has developed and tested a training tool, Real-time Assessment of Dialogue in Motivational Interviewing (ReadMI), that uses natural language processing (NLP) to provide immediate MI metrics and thereby address the need for more effective MI training. Methods: Metrics produced by the ReadMI tool from transcripts of 48 interviews conducted by medical residents with a simulated patient were examined to identify relationships between physician-speaking time and other MI metrics, including the number of open- and closed-ended questions. In addition, interrater reliability statistics were conducted to determine the accuracy of the ReadMI's analysis of physician responses. Results: The more time the physician spent talking, the less likely the physician was engaging in MI-consistent interview behaviors (r = -0.403, p = 0.007), including open-ended questions, reflective statements, or use of a change ruler. Conclusion: ReadMI produces specific metrics that a trainer can share with a student, resident, or clinician for immediate feedback. Given the time constraints on targeted skill development in health professions training, ReadMI decreases the need to rely on subjective feedback and/or more time-consuming video review to illustrate important teaching points."
TaskAllocator: A Recommendation Approach for Role-based Tasks Allocation in Agile Software Development,"Shafiq, S; Mashkoor, A; Mayr-Dorn, C; Egyed, A",10.1109/ICSSP-ICGSE52873.2021.00014,2021,"In this paper, we propose a recommendation approach - TaskAllocator - in order to predict the assignment of incoming tasks to potential befitting roles. The proposed approach, identifying team roles rather than individual persons, allows project managers to perform better tasks allocation in case the individual developers are over-utilized or moved on to different roles/projects. We evaluated our approach on ten agile case study projects obtained from the Taiga.io repository. In order to determine the TaskAllocator's performance, we have conducted a benchmark study by comparing it with contemporary machine learning models. The applicability of the TaskAllocator was assessed through a plugin that can be integrated with JIRA and provides recommendations about suitable roles whenever a new task is added to the project. Lastly, the source code of the plugin and the dataset employed have been made public."
RETRACTED: Software Systems Security Vulnerabilities Management by Exploring the Capabilities of Language Models Using NLP (Retracted Article),"Althar, RR; Samanta, D; Kaur, M; Alnuaim, AA; Aljaffan, N; Ullah, MA",10.1155/2021/8522839,2021,Security of the software system is a prime focus area for software development teams. This paper explores some data science methods to build a knowledge management system that can assist the software development team to ensure a secure software system is being developed. Various approaches in this context are explored using data of insurance domain-based software development. These approaches will facilitate an easy understanding of the practical challenges associated with actual-world implementation. This paper also discusses the capabilities of language modeling and its role in the knowledge system. The source code is modeled to build a deep software security analysis model. The proposed model can help software engineers build secure software by assessing the software security during software development time. Extensive experiments show that the proposed models can efficiently explore the software language modeling capabilities to classify software systems' security vulnerabilities.
Elicitation of Nonfunctional Requirements in Agile Development Using Cloud Computing Environment,"Younas, M; Jawawi, DNA; Shah, MA; Mustafa, A; Awais, M; Ishfaq, MK; Wakil, K",10.1109/ACCESS.2020.3014381,2020,"Nonfunctional requirements get less attention because functional requirements are considered more important in the domain of agile software methodologies. This is due to the lack of mature requirement elicitation methodologies and the nature of the software agile software development process. The less attention caused few solutions in the domain which lead to software project failure. Cloud computing helps to practice twelve (12) agile principles including nonfunctional requirement elicitation. This study proposed a semi-automated methodology which will help analyst and developers in eliciting nonfunctional requirements in agile development and cloud computing environment. The methodology used an NLP based automatic NFR extraction approach to fast the NFR elicitation process. The methodology is evaluated by applying on eProcurement dataset. The results are improved by 8.77% and 1.76% in terms of Successful NFR. It is decreased by 7.02% and 1.75% in term of Partial success, and 1.76% to 0.0% in term of Failure as compared to existing studies."
Vessel AIS Trajectory Online Compression Based on Scan-Pick-Move Algorithm Added Sliding Window,"Sun, S; Chen, Y; Piao, ZJ; Zhang, JS",10.1109/ACCESS.2020.3001934,2020,"The trajectory data of vessel AIS (automatic identification system) has important theoretical and application value for information supporting decisions. However, large sizes lead to difficulties in storing, querying, and processing. To solve the problems of high compression ratio and longtime consumption of the existing online trajectory compression algorithm, an SPM (scan-pick-move) trajectory data compression algorithm added sliding window is proposed. In order to better compress vessel trajectory data regarding compression efficiency, the sliding window is added to the classical SPM algorithm. In order to reduce trajectory data storage space, the maximum offset distance reference trajectory point is used as the criterion of whether the current trajectory point can be compressed. In this paper, the multi-dimensional space-time characteristics of trajectory data, such as distance error, compression ratio and compression time, are selected to evaluate the trajectory compression method from three levels: geometric characteristics, motion characteristics and compression efficiency. Compared with the existing SPM trajectory data compression algorithm, parallel experiments are conducted based on AIS data gathered over the duration of a month in the Japan Osaka Bay. The SPM trajectory compression algorithm added sliding window can significantly reduce the compression time and outperforms other existing trajectory compression algorithms in term of average compression error at high compression strengths. Also, the proposed method has high compression efficiency in the range of commonly used compression thresholds."
Non-Functional Requirements Orienting the Development of Socially Responsible Software,"Cysneiros, LM; Leite, JCSD",10.1007/978-3-030-49418-6_23,2020,"Nowadays, software is ubiquitous and present in almost everything we buy and use. Artificial intelligence (AI) is becoming prevalent in software products. The use of AI entices consumer inquisitiveness, promising software products that can make our lives easier, productive, and in some mission-critical applications safer. Similar reasoning can be applied to systems exploring Internet of Things, cloud services, and mobile technologies. However, there is a trust deficit when it comes to accepting AI as well as the other above-mentioned features, as a reliable technology platform. This paper argues that the more critical the domain is, the less consumers seem to trust software to make decisions on their behalf or even to be used. Aspects such as safety, privacy, and ethics challenges the perception of trustworthy computing. In the past two decades, several works have suggested that Corporate Social Responsibility (CSR) may play an essential role in creating a trust paradigm between customers and businesses promoting loyalty, customer retention and thus enhancing customer trust and increasing corporate profit. We believe that the software industry will need soon rather than later to encourage trust in their embedded software. Apromising approach lies in adapting principles associated with CSR to guide the software development processes. Such an approach could help to achieve two goals: Deliver trustworthy software and, if desired, deliver socially responsible software. We believe that Non-Functional Requirements (NFR) will play a crucial role in this endeavor. This paper highlights a first approach to establishing a basic set of NFRs that should always be carefully considered when developing software, as to aim socially responsible software."
AutoKG - An Automotive Domain Knowledge Graph for Software Testing: A position paper,"Kesri, V; Nayak, A; Ponnalagu, K",10.1109/ICSTW52544.2021.00047,2021,"Industries have a significant amount of data in semi-structured and unstructured formats which are typically captured in text documents, spreadsheets, images, etc. This is especially the case with the software description documents used by domain experts in the automotive domain to perform tasks at various phases of the Software Development Life Cycle (SDLC). In this paper, we propose an end-to-end pipeline to extract an Automotive Knowledge Graph (AutoKG) from textual data using Natural Language Processing (NLP) techniques with the application of automatic test case generation. The proposed pipeline primarily consists of the following components: 1) AutoOntology, an ontology that has been derived by analyzing several industry scale automotive domain software systems, 2) AutoRE, a Relation Extraction (RE) model to extract triplets from various sentence types typically found in the automotive domain, and 3) AutoVec, a neural embedding based algorithm for triplet matching and context-based search. We demonstrate the pipeline with an application of automatic test case generation from requirements using AutoKG."
DebtHunter: A Machine Learning-based Approach for Detecting Self-Admitted Technical Debt,"Sala, I; Tommasel, A; Fontana, FA",10.1145/3463274.3464455,2021,"Due to limited time, budget or resources, a team is prone to introduce code that does not follow the best software development practices. This code that introduces instability in the software projects is known as Technical Debt (TD). Often, TD intentionally manifests in source code, which is known as Self-Admitted Technical Debt (SATD). This paper presents DebtHunter, a natural language processing (NLP)- and machine learning (ML)- based approach for identifying and classifying SATD in source code comments. The proposed classification approach combines two classification phases for differentiating between the multiple debt types. Evaluations over 10 open source systems, containing more than 259k comments, showed that the approach was able to improve the performance of others in the literature. The presented approach is supported by a tool that can help developers to effectively manage SATD. The tool complements the analysis over Java source code by allowing developers to also examine the associated issue tracker. DebtHunter can be used in a continuous evolution environment to monitor the development process and make developers aware of how and where SATD is introduced, thus helping them to manage and resolve it."
"AIDOaRt: AI -augmented Automation for DevOps, a Model -based Framework for Continuous Development in Cyber-Physical Systems","Eramo, R; Muttillo, V; Berardinelli, L; Bruneliere, H; Gomez, A; Bagnato, A; Sadovykh, A; Cicchetti, A",10.1109/DSD53832.2021.00053,2021,"With the emergence of Cyber-Physical Systems (CPS), the increasing complexity in development and operation demands for an efficient engineering process. In the recent years DevOps promotes closer continuous integration of system development and its operational deployment perspectives. In this context, the use of Artificial Intelligence (AI) is beneficial to improve the system design and integration activities, however, it is still limited despite its high potential. AIDOaRT is a 3 years long 112020-ECSEL European project involving 32 organizations, grouped in clusters from 7 different countries, focusing on AIaugmented automation supporting modelling, coding, testing, monitoring and continuous development of Cyber-Physical Systems (CPS). The project proposes to apply Model-Driven Engineering (MDE) principles and techniques to provide a framework offering proper AI-enhanced methods and related tooling for building trustable CPSs. The framework is intended to work within the DevOps practices combining software development and information technology (IT) operations. In this regard, the project points at enabling AI for IT operations (AIOps) to automate decision making process and complete system development tasks. This paper presents an overview of the project with the aim to discuss context, objectives and the proposed approach."
NLP4IP: Natural Language Processing-based Recommendation Approach for Issues Prioritization,"Shafiq, S; Mashkoor, A; Mayr-Dorn, C; Egyed, A",10.1109/SEAA53835.2021.00022,2021,"This paper proposes a recommendation approach for issues (e.g., a story, a bug, or a task) prioritization based on natural language processing, called NLP4IP. The proposed semi-automatic approach takes into account the priority and story points attributes of existing issues defined by the project stakeholders and devises a recommendation model capable of dynamically predicting the rank of newly added or modified issues. NLP4IP was evaluated on 19 projects from 6 repositories employing the JIRA issue tracking software with a total of 29,698 issues. A comprehensive benchmark study was also conducted to compare the performance of various machine learning models. The results of the study showed an average top @3 accuracy of 81% and a mean squared error of 2.2 when evaluated on the validation set. The applicability of the proposed approach is demonstrated in the form of a JIRA plug-in illustrating predictions made by the newly developed machine learning model. The dataset has also been made publicly available in order to support other researchers working in this domain."
Identifying bot activity in GitHub pull request and issue comments,"Golzadeh, M; Decan, A; Constantinou, E; Mens, T",10.1109/BotSE52550.2021.00012,2021,"Development bots are used on Github to automate repetitive activities. Such bots communicate with human actors via issue comments and pull request comments. Identifying such bot comments allows to prevent bias in socio-technical studies related to software development. To automate their identification, we propose a classification model based on natural language processing. Starting from a balanced ground-truth dataset of 19,282 PR and issue comments, we encode the comments as vectors using a combination of the bag of words and TF-IDF techniques. We train a range of binary classifiers to predict the type of comment (human or bot) based on this vector representation. A multinomial Naive Bayes classifier provides the best results. Its performance on a test set containing 50% of the data achieves an average precision, recall, and F-1 score of 0.88. Although the model shows a promising result on the pull request and issue comments, further work is required to generalize the model on other types of activities, like commit messages and code reviews."
Machine learning based success prediction for crowdsourcing software projects,"Illahi, I; Liu, H; Umer, Q; Niu, N",10.1016/j.jss.2021.110965,2021,"Competitive Crowdsourcing Software Development is an online software development paradigm, promises the innovative, cost effective and high quality solutions on time. However, the paradigm is still in infancy and does not address the key challenges such as low rate of submissions and high risk of project failure. A significant number of software projects fail to receive a satisfactory solution and end up wasting the time and efforts of stakeholders. Therefore, the success prediction of a new software project may help stakeholders in the project crowdsourcing decision, saving their time and efforts. To this end, this study proposes a novel approach based on machine learning to predict the success of a software project for crowdsourcing platforms in terms of whether the given project will reach its completion or otherwise. First, the textual description and important attributes of software projects from TopCoder is extracted. Next, the description is preprocessed using natural language processing technologies. Then, keywords are identified using a modified keyword ranking algorithm and each software project is awarded a ranking score. Every software project is modeled as a vector that is based on the extracted attributes, its identified keywords and ranking scores. Using these vectors with their associated solution status, a support vector machine classifier is trained to predict the success of a given software project. Different machine learning classifiers are applied and it turns out that support vector machine yields the highest performance on the given dataset. Finally, the proposed approach is evaluated with history data of real software projects. The results of hold-out validation suggest that the average precision, recall, and f-measure are up to 94.53%, 99.30% and 96.85%, respectively. (C) 2021 Elsevier Inc. All rights reserved."
Quality Monitoring and Assessment of Deployed Deep Learning Models for Network AIOps,"Yang, LX; Rossi, D",10.1109/MNET.001.2100227,2021,"Artificial intelligence (AI) has recently attracted a lot of attention, transitioning from research labs to a wide range of successful deployments in many fields, which is particularly true for deep learning (DL) techniques. Ultimately, DL models, being software artifacts, need to be regularly maintained and updated: AIOps is the logical extension of the DevOps software development practices to AI software applied to network operation and management. In the life cycle of a DL model deployment, it is important to assess the quality of deployed models, to detect stale models and prioritize their update. In this article, we cover the issue in the context of network management, proposing simple but effective techniques for quality assessment of individual inference, and for overall model quality tracking over multiple inferences, that we apply to two use cases, representative of the network management and image recognition fields."
"Data Cleaning for Accurate, Fair, and Robust Models: A Big Data - AI Integration Approach","Tae, KH; Roh, Y; Oh, YH; Kim, H; Whang, SE",10.1145/3329486.3329493,2019,"The wide use of machine learning is fundamentally changing the software development paradigm (a.k.a. Software 2.0) where data becomes a first-class citizen, on par with code. As machine learning is used in sensitive applications, it becomes imperative that the trained model is accurate, fair, and robust to attacks. While many techniques have been proposed to improve the model training process (in-processing approach) or the trained model itself (post-processing), we argue that the most effective method is to clean the root cause of error: the data the model is trained on (pre-processing). Historically, there are at least three research communities that have been separately studying this problem: data management, machine learning (model fairness), and security. Although a significant amount of research has been done by each community, ultimately the same datasets must be preprocessed, and there is little understanding how the techniques relate to each other and can possibly be integrated. We contend that it is time to extend the notion of data cleaning for modern machine learning needs. We identify dependencies among the data preprocessing techniques and propose MLClean, a unified data cleaning framework that integrates the techniques and helps train accurate and fair models. This work is part of a broader trend of Big data - Artificial Intelligence (AI) integration."
"Detecting Requirements Smells With Deep Learning: Experiences, Challenges and Future Work","Habib, MK; Wagner, S; Graziotin, D",10.1109/REW53955.2021.00027,2021,"Requirements Engineering (RE) is one of the initial phases when building a software system. The success or failure of a software project is firmly tied to this phase, based on communication among stakeholders using natural language. The problem with natural language is that it can easily lead to different understandings if it is not expressed precisely by the stakeholders involved. This results in building a product which is different from the expected one. Previous work proposed to enhance the quality of the software requirements by detecting language errors based on ISO 29148 requirements language criteria. The existing solutions apply classical Natural Language Processing (NLP) to detect them. NLP has some limitations, such as domain dependability which results in poor generalization capability. Therefore, this work aims to improve the previous work by creating a manually labeled dataset and using ensemble learning, Deep Learning (DL), and techniques such as word embeddings and transfer learning to overcome the generalization problem that is tied with classical NLP and improve precision and recall metrics using a manually labeled dataset. The current findings show that the dataset is unbalanced and which class examples should be added more. It is tempting to train algorithms even if the dataset is not considerably representative. Whence, the results show that models are overfitting; in Machine Learning this issue is adressed by adding more instances to the dataset, improving label quality, removing noise, and reducing the learning algorithms complexity, which is planned for this research."
Colloidal Stabilization of Sodium Dilauraminocystine for Selective Nanoparticle-Nanoparticle Interactions: Their Screening and Extraction by Iron Oxide Magnetic Nanoparticles,"Kaur, A; Sandhu, RK; Khullar, P; Singh, K; Ahluwalia, GK; Bakshi, MS",10.1021/acs.langmuir.1c00956,2021,"Nanoparticle-nanoparticle (NP-NP) interactions between Au and Ag NPs were studied by using sodium dilauraminocystine (SDLC)- and Gemini surfactant-stabilized NPs to demonstrate the unique NP surface adsorption behavior of SDLC in controlling and mimicking such interactions in complex mixtures. They were significantly affected by the spacer as well as the polymeric nature of the head group of Gemini surfactants. A longer spacer impeded while a polymeric head group facilitated the interactions. The Au-Ag NPs interactions in an aqueous phase were also controlled by placing surface-active magnetic NPs at an aqueous-air interface, which interacted with either or both kinds of interacting NPs in an aqueous phase and reduced their ability to interact with each other. On the other hand, water-soluble zwitterionic magnetic NPs proved to be excellent extractants of both Au and Ag NPs from the aqueous phase. Extraction efficiency depended on the strength of interactions between the water-soluble magnetic NPs and aqueous-solubilized Au and/or Ag NPs."
Long- and Short-Term Conductance Control of Artificial Polymer Wire Synapses,"Hagiwara, N; Sekizaki, S; Kuwahara, Y; Asai, T; Akai-Kasaya, M",10.3390/polym13020312,2021,"Networks in the human brain are extremely complex and sophisticated. The abstract model of the human brain has been used in software development, specifically in artificial intelligence. Despite the remarkable outcomes achieved using artificial intelligence, the approach consumes a huge amount of computational resources. A possible solution to this issue is the development of processing circuits that physically resemble an artificial brain, which can offer low-energy loss and high-speed processing. This study demonstrated the synaptic functions of conductive polymer wires linking arbitrary electrodes in solution. By controlling the conductance of the wires, synaptic functions such as long-term potentiation and short-term plasticity were achieved, which are similar to the manner in which a synapse changes the strength of its connections. This novel organic artificial synapse can be used to construct information-processing circuits by wiring from scratch and learning efficiently in response to external stimuli."
Democratizing AI in biomedical image classification using virtual reality,"VanHorn, K; Ãobanoglu, MC",10.1007/s10055-021-00550-1,2022,"Artificial intelligence models can produce powerful predictive computer vision tools for healthcare. However, their development simultaneously requires computational skill as well as biomedical expertise. This barrier often impedes the wider utilization of AI in professional environments since biomedical experts often lack software development skills. We present the first development environment where a user with no prior training can build near-expert level convolutional neural network classifiers on real-world datasets. Our key contribution is a simplified environment in virtual reality where the user can build, compute, and critique a model. Through a controlled user study, we show that our software enables biomedical researchers and healthcare professionals with no AI development experience to build AI models with near-expert performance. We conclude that the potential role for AI in the biomedical domain can be realized more effectively by making its development more intuitive for non-technical domain experts using novel modes of interaction."
Large scale quality transformation in hybrid development organizations - A case study,"Pradhan, S; Nanniyur, V",10.1016/j.jss.2020.110836,2021,"As the software industry transitions to a subscription-based software-as-a-service (SaaS) model, soft-ware development companies are transforming to hybrid development organizations with increased adoption of Agile and Continuous Integration/ Continuous Delivery (CI/CD) development practices for newer products while continuing to use Waterfall methods for older products. This transformation is a huge undertaking impacting all aspects of the software development life cycle (SDLC), including the quality management system. This paper presents a case study of a large-scale transformation of a legacy quality management system to a modern system developed and implemented at Cisco Systems. The framework for this transformation is defined by six distinct areas: metrics, process, measurement, reporting, quality analytics, and culture & leadership. Our implementation leveraged recent advances in Machine Learning (ML), Artificial Intelligence (AI), connected data, integrated operations, and big data technologies to solve the challenges created by a hybrid software development organization. We believe this case study will help researchers and industry leaders understand the benefits and potential challenges of such sizeable transformations. (c) 2020 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)."
REAL-TIME MOVING OBJECTS DETECTION AND TRACKING USING DEEP-STREAM TECHNOLOGY,"Abdulghafoor, NH; Abdullah, HN",,2021,"Recently, the deep learning strategy has outperformed the rest of the algorithms related to object detection and tracking in terms of performance and efficiency. It has demonstrated the adaptation of many scientific and practical applications to artificial intelligence (AI) systems. The aim of this paper is primarily to build and develop a real-time object detection and tracking algorithm embedded in an AI computing device known as Nvidia Jetson TX2. It improves the performance of the proposed algorithm. It brings its work closer to reality. DeepStream-Software Development Kit (DS-SDK) was used to achieve high performance and interact with multiple video sources at the same time as well. Many convolutional neural networks were used inside the proposed algorithm, such as those based on Fast Region-Convolution Neural Network (Fast R-CNN), Single Shot Detector (SSD), and You Only Look Once (YOLO) network. Its performance in treating different video clips with deep streams of piping compared. The experimental results showed the excellent accuracy and speed of the proposed algorithm in achieving the desired goal."
A Personalized Learning Framework for Software Vulnerability Detection and Education,"Taeb, M; Chi, HM",10.1109/ISCSIC54682.2021.00032,2021,"The software has become a necessity for many different societal industries including, technology, health care, public safety, education, energy, and transportation. Therefore, training our future software developers to write secure source code is in high demand. With the advent of data-driven techniques, there is now a growing interest in leveraging machine learning and natural language processing (NLP) as a source code assurance method to build trustworthy systems. In this work, we propose a framework including learning modules and hands-on labs to guide future IT professionals towards developing secure programming habits and mitigating source code vulnerabilities at the early stages of the software development lifecycle following the concept of Secure Software Development Life Cycle (SSDLC). In this research, our goal is to prepare a set of hands-on labs that will introduce students to secure programming habits using source code and log file analysis tools to predict, identify, and mitigate vulnerabilities. In summary, we develop a framework which will (1) improve students' skills and awareness on source code vulnerabilities, detection tools and mitigation techniques (2) integrate concepts of source code vulnerabilities from Function, API and library level to bad programming habits and practices, (3) leverage deep learning, NLP and static analysis tools for log file analysis to introduce the root cause of source code vulnerabilities."
Leveraging Code Clones and Natural Language Processing for Log Statement Prediction,"Gholamian, S",10.1109/ASE51524.2021.9678596,2021,"Software developers embed logging statements inside the source code as an imperative duty in modern software development as log files are necessary for tracking down runtime system issues and troubleshooting system management tasks. Prior research has emphasized the importance of logging statements in the operation and debugging of software systems. However, the current logging process is mostly manual and ad hoc, and thus, proper placement and content of logging statements remain as challenges. To overcome these challenges, methods that aim to automate log placement and log content, i.e., 'where, what, and how to log', are of high interest. Thus, we propose to accomplish the goal of this research, that is to predict the log statements by utilizing source code clones and natural language processing (NLP), as these approaches provide additional context and advantage for log prediction. We pursue the following four research objectives: (RO1) investigate whether source code clones can be leveraged for log statement location prediction, (RO2) propose a clone-based approach for log statement prediction, (RO3) predict log statement's description with code-clone and NLP models, and (RO4) examine approaches to automatically predict additional details of the log statement, such as its verbosity level and variables. For this purpose, we perform an experimental analysis on seven open-source java projects, extract their method-level code clones, investigate their attributes, and utilize them for log location and description prediction. Our work demonstrates the effectiveness of log-aware clone detection for automated log location and description prediction and outperforms the prior work."
Metrics for software requirements specification quality quantification,"Ramesh, MRR; Reddy, CS",10.1016/j.compeleceng.2021.107445,2021,"The initial and crucial phase of the automation and software development is identifying requirements and documenting them in an appropriate format that denotes software requirement specification (SRS). The quality and productivity at different phases of the software development depend on requirements specification. The quality of the end product of software development is proportionate to the quality of SRS. Hence, estimating the quality of the SRS is essential. Though the numerous metrics have been defined in contemporary research, the IEEE standard metrics are considered authentic to scale SRS quality. This manuscript endeavored to redefine the IEEE standard metrics' measuring approaches to improvise SRS quality assessment. The experimental study exhibiting the significance and robustness of the proposed approach that compared to the contemporary contributions of the recent literature"
The Semantic of Business Vocabulary and Business Rules: An Automatic Generation From Textual Statements,"Haj, A; Jarrar, A; Balouki, Y; Gadir, T",10.1109/ACCESS.2021.3071623,2021,"In the early phases of the software development process, specifications are mostly written in a natural language rather than formal models, which is not supported by the Model Driven Architecture (MDA). For this reason, the Semantic of Business Vocabulary and Rules (SBVR) is proposed by the Object Management Group to represent the textual specifications in a language comprehensible by both of humans and machines, to facilitate its integration in the MDA lifecycle. However, businesspeople are usually not familiar with SBVR standard. In this paper we present an approach to automatically transform textual business rules to an SBVR model, to facilitate its integration in nowadays information technology infrastructures. Our approach is distinguished from existing works in that it uses an in-depth Natural Language Processing to extract a more comprehensible SBVR model that includes the semantic formulation of each business rule statement, coupled with a Terminological Dictionary of extracted concepts, to which we have added further specifications such as definitions and synonyms. The evaluation of our approach shows that for three sets of business rules statements taken from different domains, we could generate the correct meaning with an average of F1-score exceeding 87%."
Towards an automatic model-based Scrum Methodology,"Chantit, S; Essebaa, I",10.1016/j.procs.2021.03.099,2021,"Software systems evolve continuously and must be developed quickly to fit user requirements and new advances in technology. This has led the software engineering to propose several methods and approaches to overcome the development and maintenance of these software systems. In this regard, Agile Methodologies and Model-Driven Engineering (MDE) are two main approaches that have emerged in recent years and suggest a solution to some of the issues associated with Software systems developments. MDE focuses on software reuse through models and on generative approaches based on separation of concerns whereas Agile Methods promote the use of simpler models and best practices for programming to achieve quick feedback from clients within a development process. However, these two approaches have evolved separately and there are only a few works related to their combination. This paper presents a customized V development life cycle based on models which combines the two MDE variants: The MDA approach in the V left branch with the MBT approach to generate tests of the V right branch. In addition, we integrate this customized V life cycle in the agile Scrum methodology to facilitate the management of each Scrum sprint. (C) 2021 The Authors. Published by Elsevier B.V."
VeriDevOps: Automated Protection and Prevention to Meet Security Requirements in DevOps,"Sadovykh, A; Widforss, G; Truscan, D; Enoiu, EP; Mallouli, W; Iglesias, R; Bagnto, A; Hendel, O",,2021,"Current software development practices are increasingly based on using both COTS and legacy components which make such systems prone to security vulnerabilities. The modern practice addressing ever changing conditions, DevOps, promotes frequent software deliveries, however, verification methods artifacts should be updated in a timely fashion to cope with the pace of the process. VeriDevOps, Horizon 2020 project, aims at providing a faster feedback loop for verifying the security requirements and other quality attributes of large scale cyber-physical systems. VeriDevOps focuses on optimizing the security verification activities, by automatically creating verifiable models directly from security requirements formulated in natural language, using these models to check security properties on design models and then generating artefacts such as, tests or monitors that can be used later in the DevOps process. The main drivers for these advances are: Natural Language Processing, a combined formal verification and model-based testing approach, and machine-learning-based security monitors. VeriDevOps is in its initial stage - the project started on 1.10.2020 and it will run for three years. In this paper we will present the major conceptual ideas behind the project approach as well as the organizational settings."
Extracting Quality Attributes from User Stories for Early Architecture Decision Making,"Gilson, F; Galster, M; Georis, F",10.1109/ICSA-C.2019.00031,2019,"Software quality attributes (e.g., security, performance) influence software architecture design decisions, e.g., when choosing technologies, patterns or tactics. As software developers are moving from big upfront design to an evolutionary or emerging design, the architecture of a system evolves as more functionality is added. In agile software development, functional user requirements are often expressed as user stories. Quality attributes might be implicitly referenced in user stories. To support a more systematic analysis and reasoning about quality attributes in agile development projects, this paper explores how to automatically identify quality attributes from user stories. This could help better understand relevant quality attributes (and potential architectural key drivers) before analysing product backlogs and domains in detail and provides the bigger picture of potential architectural drivers for early architecture decision making. The goal of this paper is to present our vision and preliminary work towards understanding whether user stories do include information about quality attributes at all, and if so, how we can identify such information in an automated manner."
Artificial Intelligence and Social Simulation: Studying Group Dynamics on a Massive Scale,"Hoey, J; SchrÃ¶der, T; Morgan, J; Rogers, KB; Rishi, D; Nagappan, M",10.1177/1046496418802362,2018,"Recent advances in artificial intelligence and computer science can be used by social scientists in their study of groups and teams. Here, we explain how developments in machine learning and simulations with artificially intelligent agents can help group and team scholars to overcome two major problems they face when studying group dynamics. First, because empirical research on groups relies on manual coding, it is hard to study groups in large numbers (the scaling problem). Second, conventional statistical methods in behavioral science often fail to capture the nonlinear interaction dynamics occurring in small groups (the dynamics problem). Machine learning helps to address the scaling problem, as massive computing power can be harnessed to multiply manual codings of group interactions. Computer simulations with artificially intelligent agents help to address the dynamics problem by implementing social psychological theory in data-generating algorithms that allow for sophisticated statements and tests of theory. We describe an ongoing research project aimed at computational analysis of virtual software development teams."
An Additional Set of (Automated) Eyes: Chatbots for Agile Retrospectives,"Matthies, C; Dobrigkeit, F; Hesse, G",10.1109/BotSE.2019.00017,2019,"Recent advances in natural-language processing and data analysis allow software bots to become virtual team members, providing an additional set of automated eyes and additional perspectives for informing and supporting teamwork. In this paper, we propose employing chatbots in the domain of software development with a focus on supporting analyses and measurements of teams' project data. The software project artifacts produced by agile teams during regular development activities, e.g. commits in a version control system, represent detailed information on how a team works and collaborates. Analyses of this data are especially relevant for agile retrospective meetings, where adaptations and improvements to the executed development process are discussed. Development teams can use these measurements to track the progress of identified improvement actions over development iterations. Chatbots provide a convenient user interface for interacting with the outcomes of retrospectives and the associated measurements in a chat-based channel that is already being employed by team members."
A Super-Resolution Convolutional-Neural-Network-Based Approach for Subpixel Mapping of Hyperspectral Images,"Ma, XF; Hong, YT; Song, YZ; Chen, YJ",10.1109/JSTARS.2019.2941089,2019,"A new subpixel mapping (SPM) method based on a super-resolution convolutional neural network (SRCNN) is proposed to generate subpixel land cover maps for hyperspectral images. The SRCNN is used to restore the image spatial resolution from a coarse input image, which is equivalent to interpolation. First, an efficient subpixel convolutional neural network, which is a state-of-the-art SRCNN, is utilized to calculate the subpixel soft class value via a transfer learning strategy. Then, a classifier is used to transform the subpixel soft class values to hard-classified land cover maps with the constraint of fraction images. Experiments on three different hyperspectral images demonstrate that the SPM accuracy of the proposed SRCNN-based method is significantly better than those of three traditional SPM methods. In addition, the SRCNN-based SPM method has a simplified calculation process, does not require training data, and is less time consuming. This article provides a new solution for SPM of hyperspectral images."
Building A Platform for Machine Learning Operations from Open Source Frameworks,"Liu, Y; Ling, ZJ; Huo, BY; Wang, BQ; Chen, TAN; Mouine, E",10.1016/j.ifacol.2021.04.161,2020,"Machine Learning Operations (MLOps) aim to establish a set of practices that put tools, pipelines, and processes to build fast time-to-value machine learning development projects. The lifecycle of machine learning project development encompasses a set of roles, stacks of software frameworks and multiple types of computing resources. Such complexity makes MLOps support usually bundled with commercial cloud platforms that is referred as vendor lock. In this paper, we provide an alternative solution that devises a MLOps platform with open source frameworks on any virtual resources. Our MLOps approach is driven by the development roles of machine learning models. The tool chain of our MLOps connects to the typical CI/CD workflow of machine learning applications. We demonstrate a working example of training and deploying a model for the application of detecting software repository code vulnerability. Copyright (C) 2020 The Authors."
Generating Use Case Scenarios from User Stories,"Gilson, F; Galster, M; Georis, F",10.1145/3379177.3388895,2020,"Textual user stories capture interactions of users with the system as high-level requirements. However, user stories are typically rather short and backlogs can include many stories. This makes it hard to (a) maintain user stories and backlogs, (b) fully understand the scope of a software project without a detailed analysis of the backlog, and (c) analyse how user stories impact design decisions during sprint planning and implementation. This paper proposes a technique to automatically transform textual user stories into visual use case scenarios in the form of robustness diagrams (a semi-formal scenario-based visualisation of workflows). In addition to creating diagrams for individual stories, the technique allows combining diagrams of multiple stories into one diagram to visualise workflows within sets of stories (e.g., a backlog). Moreover, the technique supports viewpoint-based diagrams, i.e., diagrams that show relationships between actors, domain entities and user interfaces starting from a diagram element (e.g., an actor) selected by the analyst. The technique utilises natural language processing and rule-based transformations. We evaluated the technique with more than 1,400 user stories from 22 backlogs and show that (a) the technique generates syntactically valid robustness diagrams, and ( b) the quality of automatically generated robustness diagrams compares to the quality of diagrams created by human experts, but depends on the quality of the textual user stories."
Towards Queryable and Traceable Domain Models,"Saini, R; Mussbacher, G; Guo, JLC; Kienzle, J",10.1109/RE48521.2020.00044,2020,"Model-Driven Software Engineering encompasses various modelling formalisms for supporting software development. One such formalism is domain modelling which bridges the gap between requirements expressed in natural language and analyzable and more concise domain models expressed in class diagrams. Due to the lack of modelling skills among novice modellers and time constraints in industrial projects, it is often not possible to build an accurate domain model manually. To address this challenge, we aim to develop an approach to extract domain models from problem descriptions written in natural language by combining rules based on natural language processing with machine learning. As a first step, we report on an automated and tool-supported approach with an accuracy of extracted domain models higher than existing approaches. In addition, the approach generates trace links for each model element of a domain model. The trace links enable novice modellers to execute queries on the extracted domain models to gain insights into the modelling decisions taken for improving their modelling skills. Furthermore, to evaluate our approach, we propose a novel comparison metric and discuss our experimental design. Finally, we present a research agenda detailing research directions and discuss corresponding challenges."
Cross-project bug type prediction based on transfer learning,"Du, XT; Zhou, ZH; Yin, BB; Xiao, GP",10.1007/s11219-019-09467-0,2020,"The prediction of bug types provides useful insights into the software maintenance process. It can improve the efficiency of software testing and help developers adopt corresponding strategies to fix bugs before releasing software projects. Typically, the prediction tasks are performed through machine learning classifiers, which rely heavily on labeled data. However, for a software project that has insufficient labeled data, it is difficult to train the classification model for predicting bug types. Although labeled data of other projects can be used as training data, the results of the cross-project prediction are often poor. To solve this problem, this paper proposes a cross-project bug type prediction framework based on transfer learning. Transfer learning breaks the assumption of traditional machine learning methods that the training set and the test set should follow the same distribution. Our experiments show that the results of cross-project bug type prediction have significant improvement by adopting transfer learning. In addition, we have studied the factors that influence the prediction results, including different pairs of source and target projects, and the number of bug reports in the source project."
Multiple-components weights model for cross-project software defect prediction,"Qiu, SJ; Lu, L; Jiang, SY",10.1049/iet-sen.2017.0111,2018,"Software defect prediction (SDP) technology is receiving widely attention and most of SDP models are trained on data from the same project. However, at an early phase of the software lifecycle, there are little to no within-project training data to learn an available supervised defect-prediction model. Thus, cross-project defect prediction (CPDP), which is learning a defect predictor for a target project by using labelled data from a source project, has shown promising value in SDP. To better perform the CPDP, most current studies focus on filtering instances or selecting features to weaken the impact of irrelevant cross-project data. Instead, the authors propose a novel multiple-components weights (MCWs) learning model to analyse the varying auxiliary power of multiple components in a source project to construct a more precise ensemble classifiers for a target project. By combining the MCW model with kernel mean matching algorithm, their proposed approach adjusts the source-instance weights and source-component weights to jointly alleviate the negative impacts of irrelevant cross-project data. They conducted comprehensive experiments by employing 15 real-world datasets to demonstrate the advantages and effectiveness of their proposed approach."
Ethical AI-Powered Regression Test Selection,"Strandberg, PE; Frasheri, M; Enoiu, EP",10.1109/AITEST52744.2021.00025,2021,"Test automation is common in software development; often one tests repeatedly to identify regressions. If the amount of test cases is large, one may select a subset and only use the most important test cases. The regression test selection (RTS) could be automated and enhanced with Artificial Intelligence (AI-RTS). This however could introduce ethical challenges. While such challenges in AI are in general well studied, there is a gap with respect to ethical AI-RTS. By exploring the literature and learning from our experiences of developing an industry AI-RTS tool, we contribute to the literature by identifying three challenges (assigning responsibility, bias in decision-making and lack of participation) and three approaches (explicability, supervision and diversity). Additionally, we provide a checklist for ethical AI-RTS to help guide the decision-making of the stakeholders involved in the process."
DiffTech: Differencing Similar Technologies From Crowd-Scale Comparison Discussions,"Wang, H; Chen, CY; Xing, ZC; Grundy, J",10.1109/TSE.2021.3059885,2021,"Developers use different technologies for many software development tasks. However, when faced with several technologies with comparable functionalities, it is not easy to select the most appropriate one, as trial and error comparisons among such technologies are time-consuming. Instead, developers can resort to expert articles, read official documents or ask questions in Q&A sites. However, it still remains difficult to get a comprehensive comparison as online information is often fragmented or contradictory. To overcome these limitations, we propose the DiffTech system that exploits crowdsourced discussions from Stack Overflow, and assists technology comparison with an informative summary of different aspects. We first build a large database of comparable technologies in software engineering by mining tags in Stack Overflow. We then locate comparative sentences about comparable technologies with natural language processing methods. We further mine prominent comparison aspects by clustering similar comparative sentences and representing each cluster with its keywords and aggregate the overall opinion towards the comparable technologies. Our evaluation demonstrates both the accuracy and usefulness of our model, and we have implemented our approach as a practical website for public use."
A Prediction Model for Software Requirements Change Impact,"Zamani, K",10.1109/ASE51524.2021.9678582,2021,"Software requirements Change Impact Analysis (CIA) is a pivotal process in requirements engineering (RE) since changes to requirements are inevitable. When a requirement change is requested, its impact on all software artefacts has to be investigated to accept or reject the request. Manually performed CIA in large-scale software development is time-consuming and error-prone so, automating this analysis can improve the process of requirements change management. The main goal of this research is to apply a combination of Machine Learning (ML) and Natural Language Processing (NLP) based approaches to develop a prediction model for forecasting the requirement change impact on other requirements in the specification document. The proposed prediction model will be evaluated using appropriate datasets for accuracy and performance. The resulting tool will support project managers to perform automated change impact analysis and make informed decisions on the acceptance or rejection of requirement change requests."
On the Defect Prediction for Large Scale Software Systems - From Defect Density to Machine Learning,"Pradhan, S; Nanniyur, V; Vissapragada, PK",10.1109/QRS51102.2020.00056,2020,"As the software industry transitions to software-as-a-service (SAAS) model, there has been tremendous competitive pressure on companies to improve software quality at a much faster rate than before. The software defect prediction (SDP) plays an important role in this effort by enabling predictive quality management during the entire software development lifecycle (SDLC). The SDP has traditionally used defect density and other parametric models. However, recent advances in machine learning and artificial intelligence (ML/AI) have created a renewed interest in ML-based defect prediction among academic researchers and industry practitioners. Published studies on this subject have focused on two areas, i.e. model attributes and ML algorithms, to develop SDP models for small to medium sized software (mostly opensource). However, as we present in this paper, ML-based SDP for large scale software with hundreds of millions of lines of code (LOC) needs to address challenges in additional areas called Data Definition and SDP Lifecycle. We have proposed solutions for these challenges and used the example of a large-scale software (IOS-XE) developed by Cisco Systems to show the validity of our solutions."
Automating the Classification of Requirements Data,"Dave, D; Anu, V; Varde, AS",10.1109/BigData52589.2021.9671548,2021,This paper proposes a pilot approach based on the comparative analysis of supervised Machine Learning models coupled with basic Natural Language Processing concepts for classifying Functional and Non-Functional Requirements from huge collections of data relevant to the Requirements Engineering (RE) phase within software development. The publicly available PROMISE Software Engineering Repository dataset is used in the execution of this approach. Non-Functional Requirements are further classified into subclasses based on attributes they address since they are not directly related to the core functions of the concerned software. This overall research initiative helps to make the RE phase more efficient and reduces human effort in software development. It leverages Big Data in Software Engineering.
Automated Requirements Formalisation for Agile MDE,"Lano, K; Yassipour-Tehrani, S; Umar, MA",10.1109/MODELS-C53483.2021.00030,2021,"Model-driven engineering (MDE) of software systems from precise specifications has become established as an important approach for rigorous software development. However, the use of MDE requires specialised skills and tools, which has limited its adoption. In this paper we describe techniques for automating the derivation of software specifications from requirements statements, in order to reduce the effort required in creating MDE specifications, and hence to improve the usability and agility of MDE. Natural language processing (NLP) and Machine learning (ML) are used to recognise the required data and behaviour elements of systems from textual and graphical documents, and formal specification models of the systems are created. These specifications can then be used as the basis of manual software development, or as the starting point for automated software production using MDE."
ONNC Compiler Used in Fault-Mitigating Mechanisms Analysis on NVDLA-Based and ReRAM-Based Edge AI Chip Design,"Liu, S; Kuo, JH; Tang, LB; Huang, NC; Tsai, DY; Yang, MH; Wu, KC",10.1109/VLSI-DAT52063.2021.9427328,2021,"The calculation of the convolution is considered the most important part in convolutional neural network-based artificial intelligence (AI) deep learning accelerator (DLA) architecture. Regardless of employing the traditional von Neumann architecture or the non-von Neumann compute-in-memory architecture, the variations arising from the integrated circuit (IC) process at the mass production stage must be considered. The AI model accuracy also needs to be improved. The open neural network compiler (ONNC) is an architecture-aware AI software development kit toolchain, which is capable of evaluating the AI model on fixed-point Edge AI DLAs for the open neural network exchange model. In this paper, the ONNC toolchain and the NVIDIA DLA (NVDLA) field-programmable gate array are employed to investigate the NVDLA static random-access memory stuck-at fault impact on model accuracy. The fault-mitigating results after applying a recovery mechanism in the IC process hard failure are also discussed. A method for utilizing the ONNC framework and the recovery guidelines for the resistive random-access memory -based computer-integrated manufacturing DLA architecture is also proposed."
Continuous Build Outcome Prediction: A Small-N Experiment in Settings of a Real Software Project,"Kawalerowicz, M; Madeyski, L",10.1007/978-3-030-79463-7_35,2021,"We explain the idea of Continuous Build Outcome Prediction (CBOP) practice that uses classification to label the possible build results (success or failure) based on historical data and metrics (features) derived from the software repository. Additionally, we present a preliminary empirical evaluation of CBOP in a real live software project. In a small-n repeated-measure with two conditions and replicates experiment, we study whether CBOP will reduce the Failed Build Ratio (FBR). Surprisingly, the result of the study indicates a slight increase in FBR while using the CBOP, although the effect size is very small. A plausible explanation of the revealed phenomenon may come from the authority principle, which is rarely discussed in the software engineering context in general, and AI-supported software development practices in particular."
Constructing Dependable Data-Driven Software With Machine Learning,"Pahl, C; Azimi, S",10.1109/MS.2021.3067940,2021,"Many data-driven software systems present dependability challenges caused by the distribution and dynamicity of their application environments. We investigate machine learning-driven construction techniques combined with pattern-based architecture, showing that system dependability is linked to data and function quality."
Software Design and Artificial Intelligence: A Systematic Mapping Study,"Robles-Aguilar, A; OcharÃ¡n-HernÃ¡ndez, JO; SÃ¡nchez-Garcia, AJ; LimÃ³n, X",10.1109/CONISOFT52520.2021.00028,2021,"Software Design (SD from now on) is a critical process in the Software Development Life Cycle. Like all processes, there is a constant search for the improvement of SD activities. Artificial Intelligence (AI) has proven to be an effective tool to perform activities of any kind, this includes SD assisting. This study presents the state of the art related to the application of AI techniques in SD activities, in order to achieve this goal, we carried out a systematic mapping study consisting of 36 papers. The studies reported the use of 28 AI techniques, such as Neural Networks and Decision Trees; considering 20 SD activities, like diagram generation and design pattern detection. The inclusion of AI techniques in SD is an increasing trend in software development. The collected studies demonstrate the effectiveness of this inclusion, showing successful applications, which in turn may create more interest in the coupling of AI and SD in further studies."
Developing and Implementing Artificial Intelligence-Based Classifier for Requirements Engineering,"Myllynen, S; Suominen, I; Raunio, T; Karell, R; Lahtinen, J",10.1115/1.4049722,2021,"In nuclear power plant (NPP) projects, requirements engineering manages the sheer volume of requirements, typically characterized by descriptive and nonharmonized requirements. Large projects may have tens of thousands to hundreds of thousands of requirements to be managed and fulfilled. Two main issues impede requirements analysis: tortuous requirements to be interpreted; and humans' very limited ability to concentrate on a specific task. It has therefore been recognized that artificial intelligence (AI) algorithms have the potential to support designers' decision making in classifying and allocating NPP requirements into predefined classes. This paper presents our work on developing an AI-based requirements classifier utilizing natural language processing (NLP) and supervised machine-learning (ML). In addition, the paper presents the integration of the classifier with the requirements management system. The focus is on the classification of nuclear power industry-specific requirements utilizing deep-learning-based NLP. Three classifiers are compared, and the corresponding results are presented. The results include predetermined requirement classes, manually gathered and classified data, a comparison of three models and their classification accuracies, microservice system architecture, and integration of the established classifier with the requirements management system. As the performance of the requirements classifier and related system has been successfully demonstrated, future AI-specific development and studies are suggested to focus on atomizing multiclass requirements, combining similar requirements into one, checking requirements syntax, and utilizing unsupervised learning for clustering. Furthermore, new and advantageous requirement classes and hierarchies are suggested for development while improving current datasets both quantitatively and qualitatively."
Automating and Optimizing Software Testing using Artificial Intelligence Techniques,"Job, MA",,2021,"The final product of software development process is a software system and testing is one of the important stages in this process. The success of this process can be determined by how well it accomplishes its goal. Due to the advancement of technology, various software testing tools have been introduced in the software engineering discipline. The use of software is increasing day-by-day and complexity of software functions are challenging and there is need to release the software within the short quality evaluation period, there is a high demand in adopting automation in software testing. Emergence of automatic software testing tools and techniques helps in quality enhancement and reducing time and cost in the software development activity. Artificial Intelligence (AI) techniques are widely applied in different areas of Software engineering (SE). Application of AI techniques can help in achieving good performance in software Testing and increase the productivity of the software development firms. This paper briefly presents the state of the art in the field of software testing by applying AI techniques in software testing."
Regulatory approaches to managing artificial intelligence systems in autonomous vehicles in Australia,"Thiele-Evans, L; Pepper, B; Zeleznikow, J; Foster, N; Sourdin, T",10.1093/ijlit/eaab002,2021,"This article explores the emergence of Automated Vehicles (AVs) in Australia. It will investigate the legal and regulatory terrain. International and domestic approaches are examined to determine potential responses. The regulatory issues emerge partly due to the varied nature of artificial intelligence systems and processes that enable AVs to function. The variations may be due to the chosen domain model, software development processes, or the development of biases that may occur during code development for the underlying artificial intelligence system. Such variations can create difficulties in the application of road rules, safety requirements, and the legal and regulatory requirements. They may give rise to significant issues relating to driver classification and licensing for AVs, due to the varied levels of control and involvement in the driving process. For this reason, legislative reform at specific jurisdictional levels is suggested together with clearer international standards as a pathway to ensure the safe and effective integration of autonomous vehicles into society."
A Systematic Literature Review on Machine Learning for Automated Requirements Classification,"PÃ©rez-Verdejo, JM; SÃ¡nchez-GarcÃ­a, AJ; OcharÃ¡n-HernÃ¡ndez, JO",10.1109/CONISOFT50191.2020.00014,2020,"The development of quality software begins with the correct identification of the system needs. These requirements represent the basis of the subsequent activities in the software life cycle. The correct identification of these requirements in their different categories impacts on the actions taken to meet them. However, this classification can be often time-consuming or error-prone when it comes to large-scale systems, so different proposals have been made to assist in this process automatically. This systematic literature review identifies those applications of Machine Learning techniques in the classification of software requirements. In this regard, 13 articles were identified, from which relevant information on the applied algorithms, their training process, and their evaluation metrics are analyzed. From the results obtained, it is identified that the most recurrent classification algorithms featured on the identified studies are Naive Bayes, Decision Trees, and Natural Language Processing algorithms. The most frequent training datasets are academic databases and collected user reviews."
Self-driving car ISEAUTO for research and education,"Sell, R; Rassolkin, A; Leier, M; Emits, JP",,2018,"The paper introduces a research and educational project targeted to self-driving cars and urban mobility issues. The project focuses on design and development of the self-driving car in cooperation with a partner company, university researchers, and students. Research and implementation results of simulations, software development and mechanical design are presented, and a potential smart city use case discussed."
Reference Model for Agile Development of Machine Learning-based Service Systems,"Takeuchi, H; Kaiya, H; Nakagawa, H; Ogata, S",10.1109/APSECW53869.2021.00014,2021,"In this study, as a reusable knowledge for the effective system development we built a reference agile development model for machine learning(ML)-based systems. For this purpose, we collected project data on ML-based service system development projects (ML projects). From the collected practice data, we identified the common activities in these projects to extend the existing development models such as a generic agile development model and the workflow model for ML projects. Through the reference model represented by ArchiMate that is an enterprise modeling language, it was confirmed that the project participants from the business division can understand the activities that they should be involved in."
Software Requirements Change Prediction Model,"Fatima, R; Zeshan, F; Ahmad, A; Hamid, M; Ahmad, A; Tahir, SA",10.1109/DASA53625.2021.9682217,2021,"The software industry is rising gradually; in this modern era software projects are complex due to the involvement of many factors, so change is unavoidable in projects. The change in requirements predicted earlier can preserve precious resources. Therefore, factors related to stakeholders, developers, and experts having a higher impact on requirements change should be analyzed in detail to predict requirements change at the early stages of software development. To address these limitations, a change prediction process model is proposed based on the weaknesses of existing literature. The knowledge related to these factors is incorporated in the Bayesian network; the results obtained are promising."
Combining Natural Language Processing and Blockchain for Smart Contract Generation in the Accounting and Legal Field,"Monteiro, E; Righi, R; Kunst, R; da Costa, C; Singh, D",10.1007/978-3-030-68449-5_31,2021,"The growth of legislation and the demand for systems for automation of tasks has increased the demand for software development, so that we have greater assertiveness and speed in the reading and interpretation of laws in legal activities. Currently, written legislation must be interpreted by an analyst to be encoded in computer programs later, which is often error-prone. In this context, with the popularization of cryptocurrencies, interest was aroused for the use of Blockchain in the legal area. In particular, the use of smart contracts can house business rules in legislation and automate block control. Still, fast, quality code writing can benefit from the use of natural language processing (NLP) to help developers. After revisiting the state-of-the-art, it is perceived that there are no works that unite intelligent contracts and natural language processing in the context of the analysis of legislation. This work presents a computational model to generate intelligent codes from the analysis of legislation, using NLP and Blockchain for such a procedure. The practical and scientific contribution is pertinent for the law to be interpreted in the correct way and in accordance with the latest updates. Also, a prototype and initial tests are presented, which are encouraging and show the relevance of the research theme."
Emotion-Core: An Open Source framework for emotion detection research,"Alvarez-Gonzalez, N; Kaltenbrunner, A; GÃ³mez, V",10.1016/j.simpa.2021.100179,2021,"Identifying emotions from text is crucial for a variety of real world tasks. We describe Emotion-Core, an Open-Source framework for training, evaluating, and showcasing textual Emotion Detection models. Our framework is composed of two components: Emotion Classification and EmotionUI, which allow researchers to easily extend and reuse existing emotion detection solutions. We discuss the potential impact of our software project, including a recent publication in the findings of the International conference on Empirical Methods in Natural Language Processing (EMNLP 2021). Our code is available and free to use for interested researchers."
Conceptual Framework for Next-Generation Software Ecosystems,"Matsumoto, K",10.1109/SNPD51163.2021.9705010,2021,"This paper proposes a conceptual framework for developing new technologies that will solve today's technical issues in software development and operations (DevOps) and support the future software ecosystems. The proposed framework perceives resources essential for software DevOps from three perspectives: products, people, and technical information, and actively utilizes and link the latest digital technologies such as AI, natural language processing, microservices, and blockchain. The goal is not to aim fully automate software DevOps, but also to achieve high economic efficiency and sustainability by eliminating waste in software DevOps, assuming a human-centered society. The principal approaches of new technology development in the framework are product up-cycling, placement of the right people and AI in the right places, and quality control linked to external technical information. New technologies to be developed with these approaches will expand conventional concepts in software DevOps with three dimensions of reuse, human resources, and quality control."
Investigating the Benefits of Applying Artificial Intelligence Techniques to Enhance Learning Experiences in Capstone Courses,"Gonzalez, LA",10.1145/3446871.3469770,2021,"This research seeks to improve the learning experiences in Software Engineering Programs using Virtual Assistants based on Artificial Intelligence (AI) models. Students of Software Engineering Capstone Courses face real world situations and challenges that grant them valuable experiences for their professional preparation. However, since this knowledge is acquired through real-life exposure projects, it is difficult to transmit it among different generations of students. In consequence, all the gained knowledge, experiences, and computer codes developed are lost and cannot be reused outside the project context when they finish their assignment at the end of the semester. To address this challenge, this thesis considers the development of AI based virtual assistants applied in higher education, in a form of a lesson learned system, a recommender system integrated with a chatbot, to help students, solve problems similar to those they face in the different stages of their software project development by recommending previous lessons learned. The innovative contribution lies in the implementation of the described techniques from the state-of-art artificial intelligence field in an educational platform with the goal to leverage the experience gained during years of the teaching a Capstone Course in Software Engineering to new student generations who might benefit from this universal knowledge gained previously, in order to assist software engineering students to enhance their learning experience."
A Closer Look at Machine Learning Code,"Weber, T; Winiker, C; Hussmann, H",10.1145/3411763.3451679,2021,"Software using Machine Learning algorithms is becoming ever more ubiquitous making it equally important to have good development processes and practices. Whether we can apply insights from software development research remains open though, since it is not yet clear, whether data-driven development has the same requirements as its traditional counterpart. We used eye tracking to investigate whether the code reading behaviour of developers differs between code that uses Machine Learning and code that does not. Our data shows that there are differences in what parts of the code people consider of interest and how they read it. This is a consequence of differences in both syntax and semantics of the code. This reading behaviour already shows that we cannot take existing solutions as universally applicable. In the future, methods that support Machine Learning must iterate on existing knowledge to meet the challenges of data-driven development."
Towards Efficient Use Case Modeling with Automated Domain Classification and Term Recommendation,"Qi, ZW; Wang, TX; Yue, T",10.1109/REW53955.2021.00011,2021,"In requirements engineering, it takes significant time to specify requirements of various formats. Quality of specified requirements has direct impact on subsequent activities of software development, such as analysis and design. Motivated by this, in the paper, we aim to reduce effort required for specifying use case models and meanwhile improve their quality (in terms of consistency and correctness, for instance). Specifically, we investigate how to automatically classify a domain and recommend domain terminologies with natural language processing and information retrieval techniques, in the context of applying Restricted Use Case Modeling (RUCM) for developing use case models in natural language. To evaluate our approach (named RUCMBot), we evaluate it with seven subject systems. Results indicate that RUCMBot can help RUCM users by recommending domain terms with the accuracy being 0.6 in terms of F-score, on average. Moreover, RUCMBot is able to 100% correctly classify domains. RUCMBot also demonstrates its capability of constructing the domain terminology dictionary, and subsequently enhancing its recommendation accuracy along with the continuous use of RUCM for use case modeling."
Decision models for information systems planning using primitive cognitive network process: comparisons with analytic hierarchy process,"Yuen, KKF",10.1007/s12351-021-00628-3,2022,"The well-planned investment in a robust Information System (IS) is essential for the sustainability of a firm's competitive advantage. The careful selection of a suitable adoption plan for the IS investment is vital, especially in the early preparedness stage of a system development life cycle (SDLC), as this has a long-lasting impact on the SDLC. The selection process involves a complex, multiple criteria decision making process. The adoption of a multiple criteria decision tool, the Primitive Cognitive Network Process (PCNP), an alternative of the Analytic Hierarchy Process (AHP), can be challenging due to the minor differences among objects which are not appropriately evaluated by multiplication or ratio. This commonly results in rating judgement that occurs during the selection of alternatives. To address the challenges with IS planning, this paper proposes the use of the PCNP in various decision models. Three established studies of IS projects using the AHP are revisited using the proposed PCNP to demonstrate the feasibility and usability of the PCNP. The paper discusses data conversion from the AHP to the PCNP, its merits, and limitations. The proposed method can be a applied as an alternative decision tool for IS planning for various projects including Artificial Intelligence adoption projects, cloud sourcing planning projects, and mobile deployment projects."
Gemini surfactant mediated HIPE template for the preparation of highly porous monolithic chitosan-g-polyacrylamide with promising adsorption performances,"Zhao, YL; Zhao, Z; Zhang, J; Wei, MZ; Jiang, XC; Hou, LX",10.1016/j.eurpolymj.2018.11.002,2019,"In this work, a gemini-surfactant (i.e. sodium dilauramino cysteine, SDLC) stabilized high internal phase emulsion (HIPE) template was utilized to prepare monolithic polyHIPEs of chitosan-g-polyacrylamide (CT-g-PAAm) for the first time. The influences of internal phase fraction and emulsifier concentration on the porous structures were studied. The morphologies of polyHIPEs were observed via scanning electron microscope (SEM) while structure parameters were investigated by combining nitrogen adsorption/desorption measurements with mercury intrusion porosimetry (MIP). Highly porous polyHIPEs of CT-g-PAAm with porosity varying from 86 to 93% were obtained at different internal phase fractions. PoIyHIPEs were also prepared using Tween 20 as the emulsifier for comparison. It was found the polyHIPE using SDLC emulsifier (S-polyHIPE) showed higher porosity and consequent smaller foam density than that using Tween 20 emulsifier (T-polyHIPE), implying the gemini-surfactant can act as a novel type of promising emulsifier for the preparation of porous polyHIPEs. In addition, the adsorption performances of polyHIPEs were evaluated by adsorbing methylene blue (MB). At the same internal phase fraction, S-polyHIPE showed higher adsorption capacity and faster adsorption rate than T-polyHIPE. The adsorption data were analyzed by isotherm models and dynamic models, respectively. The results indicated that the adsorption occurs as a monolayer on the adsorbent surface with equal adsorption sites."
Towards software reuse through an enterprise architecture-based software capability profile,"Belfadel, A; Amdouni, E; Laval, J; Cherifi, CB; Moalla, N",10.1080/17517575.2020.1843076,2022,"Most of today's software development projects depend on the usage of existing solutions to save time and development costs. We target in this research work the design of a software capability profile that provides a broader view of an organization's software, along with an exploitation model in line with requirements engineering and enterprise architecture to fill the gap between the goals of the stakeholders and what can be delivered as a practical solution. Our contribution aims to improve the reuse of existing software, by upgrading these technical components to the level of end-user's requirements for accelerating future business application development"
Reviewer assignment based on sentence pair modeling,"Duan, Z; Tan, SC; Zhao, S; Wang, QQ; Chen, J; Zhang, YP",10.1016/j.neucom.2019.06.074,2019,"Assigning appropriate reviewers to a manuscript from a pool of candidate reviewers is an important task in the academic community. Recent researches have focused mainly on text processing methods based on natural language processing, such as topic model, word embedding and so on. However, it is difficult for the computer to understand the research fields of reviewers and manuscripts. In this paper, a novel supervisory information that is expressed as sentence pairs constructed by titles and abstracts is adopted to solve reviewer assignment problem. We propose a sentence pair modeling-based reviewer assignment (SPM-RA) method, which models the relationship of sentence pairs by supervising information. The supervisory information makes the model accurately learn the field features of reviewers and manuscripts. Firstly, we construct the training set by the field relationship between title and abstract. We use TF-IDF sampling to solve the problem of unbalanced data set. Then, we use neural network models, such as the BERT (bidirectional encoder representations from transformers), CNN (convolution neural network), biL-STM (bidirectional long short term memory), or various combinations of them to do modeling sentence pair and learn the field features of the paper through the training set. Finally, we predict the similarity between reviewers and manuscripts by training model. We evaluate SPM-RA on two real datasets and compare its performance to that of seven existing methods. The experimental results show that SPM-RA improves the recommendation precision by at least 18% on the public dataset. (C) 2019 Elsevier B.V. All rights reserved."
Creation of Multiple Conceptual Models from User Stories - A Natural Language Processing Approach,"Gupta, A; Poels, G; Bera, P",10.1007/978-3-030-34146-6_5,2019,"While Agile methodologies are used in software development, researchers have identified many issues related to requirements engineering in Agile approaches. Some of these issues relate to ambiguity in user stories, which is a widely-used requirements specification mechanism in Agile methodologies. This research proposes the use of conceptual models while developing user stories. We posit that the use of conceptual models helps reducing ambiguity in user stories. An important aspect of our research is the creation of an algorithm for automatic generation of such models while developing the user stories."
Intelligent Chatbot for Requirements Elicitation and Classification,"Surana, CSRK; Shriya; Gupta, DB; Shankar, SP",,2019,"Software Requirements (SR) are considered as the foundation for a supreme quality software development process and each step of the software development process is dependent and is related to the SR. Software requirements elicitation may be the most important area of requirements engineering and possibly of the entire software development process. There is a lot of human work required in the process of software requirements elicitation and software requirements classification and in cases where the requirements are huge in number, this requirements elicitation and classification process becomes tedious and is prone to errors. We propose a novel approach to automate Requirements Elicitation and Classification using an intelligent conversational chatbot. Utilizing Machine Learning and Artificial Intelligence, the chatbot converses with stakeholders in Natural Language and elicits formal system requirements from the interaction, and subsequently classifies the elicited requirements into Functional and Non-functional system requirements."
Joint distribution matching model for distribution-adaptation-based cross-project defect prediction,"Qiu, SJ; Lu, L; Jiang, SY",10.1049/iet-sen.2018.5131,2019,"Using classification methods to predict software defect is receiving a great deal of attention and most of the existing studies primarily conduct prediction under the within-project setting. However, there usually had no or very limited labelled data to train an effective prediction model at an early phase of the software lifecycle. Thus, cross-project defect prediction (CPDP) is proposed as an alternative solution, which is learning a defect predictor for a target project by using labelled data from a source project. Differing from previous CPDP methods that mainly apply instances selection and classifiers adjustment to improve the performance, in this study, the authors put forward a novel distribution-adaptation-based CPDP approach, joint distribution matching (JDM). Specifically, JDM aims to minimise the joint distribution divergence between the source and target project to improve the CPDP performance. By constructing an adaptive weight vector for the instances of the source project, JDM can be effective and robust at reducing marginal distribution discrepancy and conditional distribution discrepancy simultaneously. Extensive experiments verify that JDM can outperform related distribution-adaptation-based methods on 15 open-source projects that are derived from two types of repositories."
Role of Data Mining and Machine Learning in Software Reusability,"Qayyum, R; Rubaab, J; Riaz, U; Arif, F",10.1109/ICIC53490.2021.9693064,2021,"Machine learning and data mining have opened new streams in research and software development. Likewise, integration of machine learning with software development life cycle has provided opportunities for well-organized and effective development. Software reusability is an important aspect of SDLC. Therefore, automation of software reusability plays a dynamic role in SDLC. It curtails the cost and effort required for development of a software product. This paper provides a taxonomical mapping of reuse metrics and corresponding machine learning techniques applicable. Artificial intelligence techniques i.e., neural networks and clustering are nominated in the paper to check their applicability for automation of software reusability. A model is created to identify best machine learning practice amongst the nominated techniques for automation of software reusability. Evaluation of model based on survey results identifies ANN: the best technique among neural networks and clustering."
Artificial Intelligence Within Agile Software Development: Projected Impacts to Cyber Offense-Defense Balance,"Kokotajlo, T; Long, D; Reith, M; Dill, R",10.34190/EAIR.21.030,2021,"The cyber offense-defense balance theory evaluates the relative effort offense and defense forces need to prevail in conflict. Cyber conflict is a large domain, and many factors influence the offense-defense balance. Recent investments into Artificial Intelligence (A.I.) and Agile development can be used to make more accurate predictions using existing cyber offense-defense models. This paper predicts future shifts in the balance of cyber warfare caused by the current and projected capabilities of AI-augmented Agile software development pipelines. Initially, increased investments and capabilities are predicted to give an advantage to aggressing forces. Over time, however, technology advances and more standardized processes will likely create a more significant advantage for defensive capabilities."
Can We Trust Theorem Provers for Industrial AI?,"Howar, F; Mues, M",10.1109/MS.2021.3103448,2021,"There are three technologies a modern AI-aware software engineer needs to know: data mining, theorem proving, and nonlinear optimization (also called search-based SE). While much of the current industrial AI activity is focused on data mining, these other technologies are starting to achieve prominence. Optimization technology is discussed in the editorial A Watershed Moment for Search-Based Software Engineering (see IEEE Software, July-August 2021). Here, we offer a tutorial on theorem proving (what it is, how to use it, how it can sometimes go wrong, and how to fix that)."
Ethics in AI A Software Developmental and Philosophical Perspective,"Chowdhury, T; Oredo, J",10.1007/978-3-030-85447-8_21,2021,"The launch of various AI systems has been one of the main highlights of the industry. Alongside the enormous and revolutionary benefits, AI can cause numerous problems (usually resulting from poor design) and people have recently started to get serious about researching ways to make AI safer. Many of the AI safety concerns sound like science fiction, problems that might occur with very strong AI systems that are still years away, making these issues difficult to investigate. We don't know what such potential AI systems would be like, but similar issues exist with AI systems that are currently in progress or even running in the real world. The author addresses the possible implications in this article, outlining some important approaches in terms of software development methodologies and philosophy that we can start working on right now to support us with current AI systems and, hopefully, future systems"
Software Engineering for Data Mining (ML-Enabled) Software Applications,"Saeed, S; Abubakar, MM; Karabatak, M",10.1109/ISDFS52919.2021.9486319,2021,"As the data increase keeps on getting more extensive due to technology evolvement from the rational database, online transaction, cloud computing, data warehouse to big data analytics. This changes influences organizations to advance from data mining support to machine learning-enabled software platform. Seemingly, the study summarised secondary data from non-grey and grey academic literature as the research field recently started getting attention. Consequently, the work identifies, analyzes, and synthesizes the challenges of ML-enabled software development, which differs from traditional software development. But, with the adoption of the SE technique to engineer ML-enabled software development, the study was able to identify advancement for ML-enabled software likes automation of mismatch detection, which occurs due to the nature of different perspectives of stakeholders involved. Another one is integrating ML and SE data end-to-end pipeline to allow Systematic test mechanism and test automation where necessary when ML is complex in format to enable standard SE test logs. Then, education, training, and cooperation between the stakeholders, especially SE and ML, to gain more experience, knowledge, put rifts aside to join hands, and work together to ascertain user requirements. Finally, the work reframed the traditional SE development process to engineer the ML software development process. Therefore, the study can benefit stakeholders in the ML and SE communities in handling ML development challenges and may benefits academicians in conduction future research on software engineering for artificial intelligence."
Discovering Authorship of Vulnerabilities in Open Source Software,"Ghosh, K; Otero, D",10.1109/APSECW53869.2021.00018,2021,"Software vulnerabilities are, often the outcome of poor programming practices in the software developmental process. Elimination of bugs increases the expenses of the software. Bugs are created during software development process by programmers. Mitigation of flawed code in the programs can be achieved by training programmers to adopt secure code practices. Therefore, identification of flawed code created by authors becomes critical. In this work, a novel network theoretic approach to understand the relationships between programmers and flawed code in open source software projects is created. Experiments are conducted on multiple open source software to evaluate and validate the connections with authors and flaws, using multiple metrics motivated by concepts from natural language processing."
Realization of an Intrusion Detection use-case in ONAP with Acumos,"Sultana, S; Dooze, P; Kumar, VV",10.1109/ICCCN52240.2021.9522281,2021,"With Software-Defined Networking and Machine Learning/Artificial Intelligence (ML/AI) reaching new paradigms in their corresponding fields, both academia and industry have exhibited interests in discovering unique aspects of intelligent and autonomous communication networks. Transforming such intentions and interests to reality involves software development and deployment, which has its own story of significant evolution. There has been a notable shift in the strategies and approaches to software development. Today, the divergence of tools and technologies as per demand is so substantial that adapting a software application from one environment to another could involve tedious redesign and redevelopment. This implies enormous effort in migrating existing applications and research works to a modern industrial setup. Additionally, the struggles with sustainability maintenance of such applications could be painful. Concerning ML/AI, the capabilities to train, deploy, retrain, and re-deploy AI models as quickly as possible will be crucial for AI-driven network systems. An end-to-end workflow using unified open-source frameworks is the need of the hour to facilitate the integration of ML/AI models into the modern software-driven virtualized communication networks. Hence, in our paper, we present such a prototype by demonstrating the journey of a sample SVM classifier from being a python script to be deployed as a micro-service using ONAP and Acumos. While illustrating various features of Acumos and ONAP, this paper intends to make readers familiar with an end-to-end workflow taking advantage of the integration of both open-source platforms."
Software Process Improvement Diagnostic: A Snowballing Systematic Literature Review,"Ecar, M; da Silva, JPS; Amorim, N; Rodrigues, EM; Basso, F; SoldÃ¡, TG",10.1109/CLEI52000.2020.00025,2021,"Software Process Improvement (SPI) consists of a set of changes in a software development company, introducing new and improved methods, techniques, and tools. We call SPI Diagnostic, the process to know the organization current status. Typically, these SPI Diagnostic processes are manually performed, thus demanding consultant practitioner and a high effort from the organization under analysis. Based on this we propose the following question: What solutions have been proposed to SPI Diagnostic? We looked for other Systematic Literature Reviews with the same goal and the found studies do not answer our question. Hence, we performed a Systematic Literature Review (SLR) to investigate solutions to the SPI Diagnostic process. We executed an SLR, based on snowballing technique. This SLR characterizes 14 solutions aiming at systematizing the SPI Diagnostic process through a method, model, or framework. As a result, we advocate that Artificial Intelligence (AI) based solutions should be more explored in research to deal with SPI Diagnostic complexity and, therefore helping the SPI Diagnostic."
Use of Bots to Support Management Software Development and Streamline Client/Producer Communication in the 5.0 Industry,"Caiza, G; Ibarra-Torres, F; Garcia, M; Barona-Pico, V",10.1007/978-3-030-72660-7_39,2021,"Bots built on management software platforms are a newtool to improve daily communication between users of the same application. Its use shows advantages thanks to the possibility of automating the process of communication, production and distribution of products in general, and even to follow live events or news information that is published through the respective modules of the management systems. Gradually, companies are adapting their strategies to the use of these tools and process automation. This research presents the results of a discussion group with forty-six professionals in the software developer, which aims to analyze the bot phenomenon in Ecuador. The initial results reveal that software development and the industry are in a stage of adoption of this technology, although the first success stories can already be seen. The discussion group was formed taking into account the experience in the area of software development and the practice in the implementation of bots."
System for Automatic Evaluation of Bug Report Quality,"Mirziianov, R; Kiamov, A; Ehlakov, EV",10.1109/ElConRus51938.2021.9396261,2021,"Quality of user written bug reports plays a significant role during software development process; therefore, a big challenge developers face is to ensure that their users give informative feedback regarding bugs in their software. As the projects scale, the amount of resources needed to process user feedback may outweighs the amount of resources available. The objective of this study is to find a way to automatically check if a report complies with guidelines set by the developer, to reduce lifetime of reports, by using algorithms to evaluate bug description together with steps to reproduce and give a final empirical score to the report."
Teaching Modelling Literacy: An Artificial Intelligence Approach,"Saini, R; Mussbacher, G; Guo, JLC; Kienzle, J",10.1109/MODELS-C.2019.00108,2019,"In Model-Driven Engineering (MDE), models are used to build and analyze complex systems. In the last decades, different modelling formalisms have been proposed for supporting software development. However, their adoption and practice strongly rely on mastering essential modelling skills to develop a complete and coherent model-based system. Moreover, it is often difficult for novice modellers to get direct and timely feedback and recommendations on their modelling strategies and decisions, particularly in large classroom settings which hinders their learning. Certainly, there is an opportunity to apply Artificial Intelligence (AI) techniques to an MDE learning environment to empower the provisioning of automated and intelligent modelling advocacy. In this paper, we propose a framework called ModBud (a modelling buddy) to educate novice modellers about the art of abstraction. ModBud uses natural language processing (NLP) and machine learning (ML) to create modelling bots with the aim of improving the modelling skills of novice modellers and assisting other practitioners, too. These bots could be used to support teaching with automatic creation or grading of models and enhance learning beyond the traditional classroom-based MDE education with timely feedback and personalized tutoring. Research challenges for the proposed framework are discussed and a research roadmap is presented."
"A Comprehensive Investigation of BPMN Models Generation from Textual Requirements-Techniques, Tools and Trends","Maqbool, B; Azam, F; Anwar, MW; Butt, WH; Zeb, J; Zafar, I; Nazir, AK; Umair, Z",10.1007/978-981-13-1056-0_54,2019,"Business Process Modeling Languages (BPML's) are continuously getting attraction of software development communities due to the fact of specifying complex business requirements with simplicity. However, the development of business process models from textual requirements through existing BPML's is a time consuming task. In this context, Natural Language Processing (NLP) techniques are commonly applied to automatically generate business process models from textual requirements. Business Process Model and Notation (BPMN) is a well-known BPML. This article comprehensively investigates modern techniques, tools and trends for the generation of BPMN models from textual requirements by utilizing NLP techniques. Particularly, a Systematic Literature Review (SLR) is performed to select and evaluate 36 research studies published in the span of 2010-2018. As a result, 11 NLP and 8 BPMN tools are identified. Furthermore, 8 commonly generated BPMN constructs are recognized. Finally, a comparative analysis of NLP and BPMN tools is performed with the help of important evaluation parameters. It is concluded that the existing NLP techniques and tools significantly simplify the process of BPMN models generation from textual requirements. However, the existing approaches are inadequate to be applied in the industries, especially for real-time systems."
Building Sankie: An AI Platform for DevOps,"Kumar, R; Bansal, C; Maddila, C; Sharma, N; Martelock, S; Bhargava, R",10.1109/BotSE.2019.00020,2019,"There has been a fundamental shift amongst software developers and engineers in the past few years. The software development life cycle (SDLC) for a developer has increased in complexity and scale. Changes that were developed and deployed over a matter of days or weeks are now deployed in a matter of hours. Due to greater availability of compute, storage, better tooling, and the necessity to react, developers are constantly looking to increase their velocity and throughput of developing and deploying changes. Consequently, there is a great need for more intelligent and context sensitive DevOps tools and services that help developers increase their efficiency while developing and debugging. Given the vast amounts of heterogeneous data available from the SDLC, such intelligent tools and services can now be built and deployed at a large scale to help developers achieve their goals and be more productive. In this paper, we present Sankie, a scalable and general service that has been developed to assist and impact all stages of the modern SDLC. Sankie provides all the necessary infrastructure (back-end and front-end bots) to ingest data from repositories and services, train models based on the data, and eventually perform decorations or provide information to engineers to help increase the velocity and throughput of changes, bug fixes etc. This paper discusses the architecture as well as some of the key observations we have made from wide scale deployment of Sankie within Microsoft."
From User Stories to Use Case Scenarios Towards a Generative Approach,"Gilson, F; Irwin, C",10.1109/ASWEC.2018.00016,2018,"User stories are increasingly adopted as the basis of requirement engineering artefacts in Agile Software Development. Surveys have shown that user stories are perceived as being effective at describing the main goals of a system. But the continuous management of a product backlog may be particularly time-consuming and error-prone, especially when assessing the quality or scope of user stories and keeping an eye on the system's big picture. On the other hand, models have been recognised as effective tools for communication and analysis purposes. In this research, we propose a generative approach to create robustness diagrams, i.e. a form of semi-formal use case scenarios, from the automated analysis of user stories. Stories are transformed into diagrams, enabling requirement engineers and users to validate the main concepts and functional steps behind stories and discover malformed or redundant stories. Such models also open the door for automated systematic analysis."
Predicting failures in agile software development through data analytics,"Batarseh, FA; Gonzalez, AJ",10.1007/s11219-015-9285-3,2018,"Artificial intelligence-driven software development paradigms have been attracting much attention in academia, industry and the government. More specifically, within the last 5 years, a wave of data analytics is affecting businesses from all domains, influencing engineering management practices in many industries and making a difference in academic research. Several major software vendors have been adopting a form of intelligent development in one or more phases of their software development processes. Agile for example, is a well-known example of a lifecycle used to build intelligent and analytical systems. The agile process consists of multiple sprints; in each sprint a specific software feature is developed, tested, refined and documented. However, because agile development depends on the context of the project, testing is performed differently in every sprint. This paper introduces a method to predict software failures in the subsequent agile sprints. That is achieved by utilizing analytical and statistical methods (such as using Mean Time between Failures and modelling regression). The novel method is called: analytics-driven testing (ADT). ADT predicts errors and their locations (with a certain statistical confidence level). That is done by continuously measuring MTBF for software components, and using a forecasting regression model for estimating where and what types of software system failures are likely to occur. ADT is presented and evaluated."
Group task allocation approach for heterogeneous software crowdsourcing tasks,"Yin, XJ; Huang, JW; He, W; Guo, W; Yu, H; Cui, LZ",10.1007/s12083-020-01000-6,2021,"It is more common for multiple users to collaborate to develop a software application in a P2P collaborative working environment. In collaborative software development, the rational allocation of software development tasks is of great significance. However, heterogeneous of software development tasks, such as the value of the task, the skill required, the effort required and difficulty, increase the complexity of task allocation. This paper proposes an allocation approach of crowd intelligence software development task in which multiple individuals collaborate to complete software development tasks. The heterogeneous task allocation problem in the crowd intelligence software development system is formulated as an optimization problem. Then, the process of task allocation is modelled using the hidden Markov model. In our study, due to the insufficiency of data characteristics, we propose to construct a generator using Generative Adversarial Networks(GANs) to solve this problem. Then, the Baum-Welch algorithm is used for detailed analysis and calculation of model parameters. And on this basis, effective task allocation strategies for maximizing the total value of tasks obtained by the workers are explored through the Viterbi algorithm. Based on the Agile Manager (AM) dataset, which contains a large scale real human task allocation strategy, the model learns from human decision-making strategies that have achieved good outcomes. Based on the Agile Manager dataset, this approach is evaluated experimentally. The results show that it outperforms the artificial intelligence (AI) player in the AM game platform."
Online sequential pattern mining and association discovery by advanced artificial intelligence and machine learning techniques,"Huang, SC; Chiou, CC; Chiang, JT; Wu, CF",10.1007/s00500-019-04100-5,2020,"With the advances in information science, vast amounts of financial time series data can been collected and analyzed. In modern time series analysis, sequential pattern mining (SPM) and association discovery (AD) are the most important techniques to predict the future trends. This study aims at developing advanced SPM and AD for financial data by cutting edge techniques from artificial intelligence and machine learning. The nonlinearity and non-stationarity of financial time series dynamics pose a major challenge for SPM and AD. This study employs time-frequency analysis to extract features for SPM. Then, a sparse multi-manifold clustering (SMMC) is used to partition the feature space into several disjointed regions for better AD. Finally, local relevance vector machines (RVMs) are employed for AD and perform the forecasting. Different from traditional methods, the novel forecasting system operates on multiple resolutions and multiple dynamic regimes. SMMC finds both the neighbors and the weights automatically by a sparse solution, which approximately spans a low-dimensional affine subspace at that point. RVM, the Bayesian kernel machines, can produce parsimonious models with excellent generalization properties. Taking multiple time series data from financial markets as an example, the empirical results demonstrate that the proposed model outperforms traditional models and significantly reduces the forecasting errors. The framework is effective and suitable for other time series forecasting."
Metamorphic Testing of an Artificially Intelligent Chess Game,"Liaqat, A; Sindhu, MA; Siddiqui, GF",10.1109/ACCESS.2020.3024929,2020,"Artificially intelligent (AI) game software incorporates different algorithms to generate intelligent human-like responses to the users playing them. Testing AI game software poses great difficulty because of the complex possibilities that can result at a given point and analysis of said possibilities is a tedious task. Also during software development there are resource constraints due to which testing targets specific parts of the software. An AI game of Chess takes into consideration a large amount of possible outcomes at any given point before deciding a move. Therefore, testing it in its entirety is impractical. In this paper we propose a metamorphic testing approach for testing an AI Chess game i.e. a Chess engine's algorithm of determining and pruning out possible outcomes and ultimately deciding on a final outcome. For validating our approach, we have done error seeding on an open source Chess engine and tested it through our approach. The results for our proposed approach for testing an AI Chess game through metamorphic relations show that it is successful in revealing 71% of the total seeded faults. On comparison of our proposed approach with the existing technique of testing Chess engine i.e., perft function, we have come across situations in which our proposed approach reveals errors overlooked by the existing technique. In the future we aim to extend our approach towards other AI game software."
Continuous Experimentation on Artificial Intelligence Software: A Research Agenda,"Anh, ND; Abrahamsson, P",10.1145/3368089.3417039,2020,"Moving from experiments to industrial level AI software development requires a shift from understanding AI/ ML model attributes as a standalone experiment to know-how integrating and operating AI models in a large-scale software system. It is a growing demand for adopting state-of-the-art software engineering paradigms into AI development, so that the development efforts can be aligned with business strategies in a lean and fast-paced manner. We describe AI development as an unknown unknown problem where both business needs and AI models evolve over time. We describe a holistic view of an iterative, continuous approach to develop industrial AI software basing on business goals, requirements and Minimum Viable Products. From this, five areas of challenges are presented with the focus on experimentation. In the end, we propose a research agenda with seven questions for future studies."
Analysis of Cross-Referencing Artificial Intelligence Topics Based on Sentence Modeling,"Woo, H; Kim, J; Lee, W",10.3390/app10113681,2020,"Artificial intelligence (AI) is bringing about enormous changes in everyday life and today's society. Interest in AI is continuously increasing as many countries are creating new AI-related degrees, short-term intensive courses, and secondary school programs. This study was conducted with the aim of identifying the interrelationships among topics based on the understanding of various bodies of knowledge and to provide a foundation for topic compositions to construct an academic body of knowledge of AI. To this end, machine learning-based sentence similarity measurement models used in machine translation, chatbots, and document summarization were applied to the body of knowledge of AI. Consequently, several similar topics related to agent designing in AI, such as algorithm complexity, discrete structures, fundamentals of software development, and parallel and distributed computing were identified. The results of this study provide the knowledge necessary to cultivate talent by identifying relationships with other fields in the edutech field."
Software Engineering for Data Analytics,"Kim, M",10.1109/MS.2020.2985775,2020,"We are at an inflection point where software engineering meets the data-centric world of big data, machine learning, and artificial intelligence. In this article, I summarize findings from studies of professional data scientists and discuss my perspectives on open research problems to improve data-centric software development."
Configuring use case models in product families,"Hajri, I; Goknil, A; Briand, LC; Stephany, T",10.1007/s10270-016-0539-8,2018,"In many domains such as automotive and avionics, the size and complexity of software systems is quickly increasing. At the same time, many stakeholders tend to be involved in the development of such systems, which typically must also be configured for multiple customers with varying needs. Product Line Engineering (PLE) is therefore an inevitable practice for such systems. Furthermore, because in many areas requirements must be explicit and traceability to them is required by standards, use cases and domain models are common practice for requirements elicitation and analysis. In this paper, based on the above observations, we aim at supporting PLE in the context of use case-centric development. Therefore, we propose, apply, and assess a use case-driven configuration approach which interactively receives configuration decisions from the analysts to generate product-specific (PS) use case and domain models. Our approach provides the following: (1) a use case-centric product line modeling method (PUM), (2) automated, interactive configuration support based on PUM, and (3) an automatic generation of PS use case and domain models from Product Line (PL) models and configuration decisions. The approach is supported by a tool relying on Natural Language Processing (NLP) and integrated with an industrial requirements management tool, i.e., IBM DOORS. We successfully applied and evaluated our approach to an industrial case study in the automotive domain, thus showing evidence that the approach is practical and beneficial to capture variability at the appropriate level of granularity and to configure PS use case and domain models in industrial settings."
Data-driven Elicitation and Optimization of Dependencies between,"Deshpande, G; Arora, C; Ruhe, G",10.1109/RE.2019.00055,2019,"Requirement dependencies affect many activities in the software development life cycle such as design, implementation, testing, release planning and change management. They are the basis for various software development decisions. However, requirements dependencies extraction is not only error-prone but also a cognitively and computationally complex problem that consumes substantial efforts, since most of the requirements are documented in natural language. This paper proposes a novel approach to extracts requirements dependencies utilizing natural-language processing (NLP) and weakly supervised learning (WSL) in two stages. In the first stage, binary dependencies (basic dependencies:dependent/independent) are identified, which are further analyzed to detect the type of the dependency in the second stage. An initial evaluation of this approach on the PURE data set - European Rail Traffic Management System - was carried out using three machine learners (Random Forest, Support Vector Machine and Naive Bayes), which were then compared and tested. Results showed that all the three learners exhibited similar accuracy measures, while SVM needed additional parameter tuning. The machine learners' accuracy was further improved by applying weakly supervised learning to generate pseudo annotations for unlabelled data. Based on these results, agenda is to provide decision support under a dynamic use case scenario that includes (i) continuous updates and analysis of dependencies, (ii) identification of the general types of dependencies, and (iii) dependencies as a key driver of the decision support for the product releases."
Towards Transforming User Requirements to Test Cases Using MDE and NLP,"Allala, SC; Sotomayor, JP; Santiago, D; King, TM; Clarke, PJ",10.1109/COMPSAC.2019.10231,2019,"The behavior, attributes and properties of a software system is represented in a set of requirements that are written in structured natural language and are usually ambiguous. In large development projects, different modeling techniques are used to create and manage these requirements which aid in the analysis of the problem domain. Requirements are later used in the development process to create test cases, which is still mainly a manual process. To automate this process, we plan to use several of the techniques used in model-driven software development and Natural Language Processing(NLP). The approach under consideration is to use a model-to-model transformation to convert requirements into test cases with the support of Stanford CoreNLP techniques. Key to this transformation process is the use of meta-modeling for requirements and test cases. In this paper we focus on creating a comprehensive meta-model for requirements that can represent both use cases and user stories and performing preliminary analysis of the requirements using NLP. In later work we will develop a set of transformation rules to convert requirements into partial test cases. To show the feasibility of our approach we develop a prototype that can accept a cross-section of requirements written as both use cases and user stories."
Agile Release Planning Using Natural Language Processing Algorithm,"Sharma, S; Kumar, D",10.1109/aicai.2019.8701252,2019,"once the requirement is gathered in agile, it is broken down into smaller pre-defined format called user stories. These user stories are then scoped in various sprint releases and delivered accordingly. Release planning in Agile becomes challenging when the number of user stories goes up in hundreds. In such scenarios it is very difficult to manually identify similar user stories and package them together into a release. Hence, this paper suggests application of natural language processing algorithms for identifying similar user stories and then scoping them into a release This paper takes the approach to build a word corpus for every project release identified in the project and then to convert the provided user stories into a vector of string using Java utility for calculating top 3 most occurring words from the given project corpus in a user story. Once all the user stories are represented as vector array then by using RV coefficient NLP algorithm the user stories are clustered into various releases of the software project. Using the proposed approach, the release planning for large and complex software engineering projects can be simplified resulting into efficient planning in less time. The automated commercial tools like JIRA and Rally can be enhanced to include suggested algorithms for managing release planning in Agile."
EMOD: An End-to-End Approach for Investigating Emotion Dynamics in Software Development,"Neupane, KP; Cheung, K; Wang, Y",10.1109/ICSME.2019.00038,2019,"Emotions are an integral part of human nature. Emotion awareness is critical to any form of interpersonal communication and collaboration, including these in the software development process. Recently, the SE community starts having growing interests in emotion awareness in software development. While researchers have accomplished many valuable results, most extant research ignores the dynamic nature of emotion. To investigate the emotion dynamics, SE community needs an effective approach to capture and model emotion dynamics rather than focuses on extracting isolated emotion states. In this paper, we proposed such an approach-EMOD. EMOD is able to automatically collect project teams' communication records, identify the emotions and their intensities in them, model the emotion dynamics into time series, and provide efficient data management. We developed a prototype tool that instantiates the EMOD approach by assembling state-of-the-art NLP, SE, and time series techniques. We demonstrate the utility of the tool using the IPython's project data on GitHub and a visualization solution built on EmoD. Thus, we demonstrate that EMOD can provide end-to-end support for various emotion awareness research and practices through automated data collection, modeling, storage, analysis, and presentation."
ELICA: An Automated Tool for Dynamic Extraction of Requirements Relevant Information,"Abad, ZSH; Gervasi, V; Zowghi, D; Barker, K",10.1109/AIRE.2018.00007,2018,"Requirements elicitation requires extensive knowledge and deep understanding of the problem domain where the final system will be situated. However, in many software development projects, analysts are required to elicit the requirements from an unfamiliar domain, which often causes communication barriers between analysts and stakeholders. In this paper, we propose a requirements ELICitation Aid tool (ELICA) to help analysts better understand the target application domain by dynamic extraction and labeling of requirements-relevant knowledge. To extract the relevant terms, we leverage the flexibility and power of Weighted Finite State Transducers (WFSTs) in dynamic modeling of natural language processing tasks. In addition to the information conveyed through text, ELICA captures and processes non-linguistic information about the intention of speakers such as their confidence level, analytical tone, and emotions. The extracted information is made available to the analysts as a set of labeled snippets with highlighted relevant terms which can also be exported as an artifact of the Requirements Engineering (RE) process. The application and usefulness of ELICA are demonstrated through a case study. This study shows how pre-existing relevant information about the application domain and the information captured during an elicitation meeting, such as the conversation and stakeholders' intentions, can be captured and used to support analysts achieving their tasks."
Searching StackOverflow Questions with Multi-Faceted Categorization,"Liu, MW; Peng, X; Jiang, QT; Marcus, A; Yang, JW; Zhao, WY",10.1145/3275219.3275227,2018,"StackOverflow provides answers for a huge number of software development questions that are frequently encountered by developers. However, searching relevant questions in StackOverflow is not always easy using the keyword based search engine provided by StackOverflow. A software development question can be characterized by multiple attributes, such as, its concern (e.g., configuration problem, error handling, sample code, etc.), programming language, operating system, and involved middleware, framework, library and software technology. We propose a multi-faceted and interactive approach for searching StackOverflow questions (called MFISSO), which leverages these attributes of the questions. Our approach starts with an initial keyword-based query and extracts a multifaceted categorization from all the candidate questions using natural language processing and data mining. It then allows developers to iteratively refine the search results through an interactive process. We evaluated an implementation of MFISSO in a controlled experiments with 20 computing students, solving ten software development tasks using StackOverflow. The experiment shows that MFISSO can help developers find relevant questions faster and with higher accuracy."
RepoSkillMiner: Identifying software expertise from GitHub repositories using Natural Language Processing,"Kourtzanidis, S; Chatzigeorgiou, A; Ampatzoglou, A",10.1145/3324884.3415305,2020,"A GitHub profile is becoming an essential part of a developer's resume enabling HR departments to extract someone's expertise, through automated analysis of his/her contribution to open-source projects. At the same time, having clear insights on the technologies used in a project can be very beneficial for resource allocation and project maintainability planning. In the literature, one can identify various approaches for identifying expertise on programming languages, based on the projects that developer contributed to. In this paper, we move one step further and introduce an approach (accompanied by a tool) to identify low-level expertise on particular software frameworks and technologies apart, relying solely on GitHub data, using the GitHub API and Natural Language Processing (NLP)-using the Microsoft Language Understanding Intelligent Service (LUIS). In particular, we developed an NLP model in LUIS for named-entity recognition for three (3).NET technologies and two (2) front-end frameworks. Our analysis is based upon specific commit contents, in terms of the exact code chunks, which the committer added or changed. We evaluate the precision, recall and f-measure for the derived technologies/frameworks, by conducting a batch test in LUIS and report the results. The proposed approach is demonstrated through a fully functional web application named RepoSkillMiner."
Achiever or Explorer? Gamifying the Creation Process of Training Data for Machine Learning,"Alaghbari, S; Mitschick, A; Blichmann, G; Voigt, M; Dachselt, R",10.1145/3404983.3405519,2020,"The development of artificial intelligence, e. g., for Compute' Vision, through supervised learning requires the 'input of large amounts of annotated or labeled data objects as training data. The creation of high-quality training data is usually done manually which can be repetitive and tiring. Gamyication, the use of game elements in a non-game context, is one method to make tedious tasks more interesting. I'his paper proposes a multi -step process for ga.min,ing the manual creation of training data for machine learning purposes. We choose a user -adapted approach based on the results of a preceding user study with the target group (employees of an AI software development company) which helped us to identify minotation use cases and the users' player characteristics. The resulting concept includes levels of increasing difficulty, tutorials, progress indicators and a narrative built around a robot character which at the same time is a user assistant. The implemented prototype is an extension of the company's existing annotation tool and serves as a basis for fUrtlwr observations."
The Efficacy of Using Social Media Data for Designing Traffic Management Systems,"Noaeen, M; Far, BH",10.1109/CrowdRE51214.2020.00009,2020,"It has long been acknowledged in the context of developing dynamic and reactive systems that users' input during different stages of the development process helps to quickly and incrementally adapt to changes in the system's context and users' needs. Given the data- and communication-intensive nature of developing transportation management systems, utilizing social media data provides a new route for a dynamic collection of needs and experiences in a timely and direct fashion. In this paper, we will explore how and to what extent social media data can support urban traffic management systems. To this end, we have conducted a mixed-method study including both manual qualitative analysis, and automatic information extraction using weighted finite-state transducers (WFST), natural language processing (NLP), and deep neural networks (DNN) on Twitter data. We utilize Canadian traffic information from twitter to look for issues and relevant information that may assist authorities and software development teams in making decisions when designing and developing traffic management systems by leveraging lay people's input. Data triangulation will also be used to help compare our results against other data sources such as Google Trends and scientific material. We found that the self-reported traffic information with lay users on Twitter can be a valuable source to characterize traffic management systems. Moreover, we found that although theory-based publications in the context of traffic management systems can help with traffic estimation, control, and prediction. they are insufficient to characterize the context-sensitive aspects of these systems."
Reusing Software Engineering Knowledge from Developer Communication,"Silva, CMC",10.1145/3368089.3418540,2020,"Software development requires many different types of knowledge, such as knowledge about software development processes, practices and techniques, and about the domain of an application. Software, developers often share knowledge in informal communication channels (e.g., instant messaging tools, e-mails, or online forums). Considering that this informal communication contains knowledge that may be potentially relevant for other developers and given that this knowledge is not necessarily captured and formally documented for reuse, in this work we propose (a) exploring whether developer communication (via instant messaging) is a suitable source of reusable software engineering knowledge; (b) investigating how to identify that knowledge using data mining; (c) and analysing through action research how to present it to developers in a useful way for reuse. The envisioned theories and solutions approaches will analyze existing software development data captured in communication, rather than data that were captured and stored specifically to be reused."
Implementing Ethics in AI: Initial Results of an Industrial Multiple Case Study,"Vakkuri, V; Kemell, KK; Abrahamsson, P",10.1007/978-3-030-35333-9_24,2019,"Artificial intelligence (AI) is becoming increasingly widespread in system development endeavors. As AI systems affect various stakeholders due to their unique nature, the growing influence of these systems calls for ethical considerations. Academic discussion and practical examples of autonomous system failures have highlighted the need for implementing ethics in software development. However, research on methods and tools for implementing ethics into AI system design and development in practice is still lacking. This paper begins to address this focal problem by providing elements needed for producing a baseline for ethics in AI based software development. We do so by means of an industrial multiple case study on AI systems development in the healthcare sector. Using a research model based on extant, conceptual AI ethics literature, we explore the current state of practice out on the field in the absence of formal methods and tools for ethically aligned design."
Study of the Performance of Various Classifiers in Labeling Non- Functional Requirements,"TÃ³th, L; VidÃ¡cs, L",10.5755/j01.itc.48.3.21973,2019,"Software systems are to be developed based on expectations of the customers. These expectations are expressed using natural languages. To design software meeting the needs of the customer and the stakeholders, the intentions, feedback and reviews are to be understood accurately and without ambiguity. These textual inputs often contain inaccuracies, contradictions and are seldom given in a well-structured form. The issues mentioned in the previous thought frequently result in the program not satisfying the expectation of the stakeholders. In particular, for non-functional requirements, clients rarely emphasize these specifications as much as they might be justified. Identifying, classifying and reconciling the requirements is one of the main duty of the System Analyst, which without using a proper tool, can be very demanding and time-consuming. Tools which support text processing are expected to improve the accuracy of identification and classification of requirements even in an unstructured set of inputs. System Analysts can also use them in document archeology tasks where many documents, regulations, standards, etc. have to be processed. Methods elaborated in natural language processing and machine learning offer a solid basis. However, their usability and the possibility to improve the performance utilizing the specific knowledge from the domain of the software engineering are to be examined thoroughly. In this paper, we present the results of our work adapting natural language processing and machine learning methods for handling and transforming textual inputs of software development. The major contribution of our work is providing a comparison of the performance and applicability of the stateof-the-art techniques used in natural language processing and machine learning in software engineering. Based on the results of our experiments, tools which can support System Analysts working on textual inputs can be designed."
Towards Robust Production Machine Learning Systems: Managing Dataset Shift,"Abdelkader, H",10.1145/3324884.3415281,2020,"The advances in machine learning (ML) have stimulated the integration of their capabilities into software systems. However, there is a tangible gap between software engineering and machine learning practices, that is delaying the progress of intelligent services development. Software organisations are devoting effort to adjust the software engineering processes and practices to facilitate the integration of machine learning models. Machine learning researchers as well are focusing on improving the interpretability of machine learning models to support overall system robustness. Our research focuses on bridging this gap through a methodology that evaluates the robustness of machine learning-enabled software engineering systems. In particular, this methodology will automate the evaluation of the robustness properties of software systems against dataset shift problems in ML. It will also feature a notification mechanism that facilitates the debugging of ML components."
Towards Better Technical Debt Detection with NLP and Machine Learning Methods,"Rantala, L",10.1145/3377812.3381404,2020,"Technical debt (TD) is an economical term used to depict non-optimal choices made in the software development process. It occurs usually when developers take shortcuts instead of following agreed upon development practices, and unchecked growth of technical debt can start to incur negative effects for software development processes. Technical debt detection and management is mainly done manually, and this is both slow and costly way of detecting technical debt. Automatic detectionwould solve this issue, but even state-of-the-art tools of today do not accurately detect the appearance of technical debt. Therefore, increasing the accuracy of automatic classification is of high importance, so that we could eliminate significant portion from the costs relating to technical debt detection. This research aims to solve the problem in detection accuracy by bringing in together static code analysis and natural language processing. This combination of techniques will allow more accurate detection of technical debt, when compared to them being used separately from each other. Research also aims to discover themes and topics from written developer messages that can be linked to technical debt. These can help us to understand technical debt from developers' viewpoint. Finally, we will build an open-source tool/plugin that can be used to accurately detect technical debt using both static analysis and natural language processing methods."
ADCR: An Adaptive Tool to select Appropriate Developer for Code Review based on Code Context,"Sadman, N; Ahsan, MM; Mahmud, MAP",,2020,"Code review is one of the crucial steps in the software development process. Despite having many experts, assigning the appropriate one is often challenging, time-consuming, and inefficient for industrial developers and researchers who demand instant solutions. An automated code review system can serve as a proficient and alternative opportunity for those necessities. This paper aims to identify appropriate reviewers for a selected task based on data analysis using Natural Language Processing (NLP) techniques. Appropriate Developer for Code Review (ADCR) is proposed taking into account a set of data that comprises reviewers' information-responsiveness, experience, and acquaintanceship-benefits of the proposed methods including unbiased review accountability and the early feed-back opportunity for the developers. Additionally, a tool is developed to process the automated review and speed up the development cycles."
Cognification of Program Synthesis-A Systematic Feature-Oriented Analysis and Future Direction,"Subahi, AF",10.3390/computers9020027,2020,"Program synthesis is defined as a software development step aims at achieving an automatic process of code generation that is satisfactory given high-level specifications. There are various program synthesis applications built on Machine Learning (ML) and Natural Language Processing (NLP) based approaches. Recently, there have been remarkable advancements in the Artificial Intelligent (AI) domain. The rise in advanced ML techniques has been remarkable. Deep Learning (DL), for instance, is considered an example of a currently attractive research field that has led to advances in the areas of ML and NLP. With this advancement, there is a need to gain greater benefits from these approaches to cognify synthesis processes for next-generation model-driven engineering (MDE) framework. In this work, a systematic domain analysis is conducted to explore the extent to the automatic generation of code can be enabled via the next generation of cognified MDE frameworks that support recent DL and NLP techniques. After identifying critical features that might be considered when distinguishing synthesis systems, it will be possible to introduce a conceptual design for the future involving program synthesis/MDE frameworks. By searching different research database sources, 182 articles related to program synthesis approaches and their applications were identified. After defining research questions, structuring the domain analysis, and applying inclusion and exclusion criteria on the classification scheme, 170 out of 182 articles were considered in a three-phase systematic analysis, guided by some research questions. The analysis is introduced as a key contribution. The results are documented using feature diagrams as a comprehensive feature model of program synthesis showing alternative techniques and architectures. The achieved outcomes serve as motivation for introducing a conceptual architectural design of the next generation of cognified MDE frameworks."
Hacking the AI - the Next Generation of Hijacked Systems,"Hartmann, K; Steup, C",10.23919/cycon49761.2020.9131724,2020,"Within the next decade, the need for automation, intelligent data handling and pre-processing is expected to increase in order to cope with the vast amount of information generated by a heavily connected and digitalised world. Over the past decades, modern computer networks, infrastructures and digital devices have grown in both complexity and interconnectivity. Cyber security personnel protecting these assets have been confronted with increasing attack surfaces and advancing attack patterns. In order to manage this, cyber defence methods began to rely on automation and (artificial) intelligence supporting the work of humans. However, machine learning (ML) and artificial intelligence (AI) supported methods have not only been integrated in network monitoring and endpoint security products but are almost omnipresent in any application involving constant monitoring, complex or large volumes of data. Intelligent IDS, automated cyber defence, network monitoring and surveillance as well as secure software development and orchestration are all examples of assets that are reliant on ML and automation. These applications are of considerable interest to malicious actors due to their importance to society. Furthermore, ML and AI methods are also used in audio-visual systems utilised by digital assistants, autonomous vehicles, face-recognition applications and many others. Successful attack vectors targeting the AI of audio-visual systems have already been reported. These attacks range from requiring little technical knowledge to complex attacks hijacking the underlying AI. With the increasing dependence of society on ML and AI, we must prepare for the next generation of cyber attacks being directed against these areas. Attacking a system through its learning and automation methods allows attackers to severely damage the system, while at the same time allowing them to operate covertly. The combination of being inherently hidden through the manipulation made, its devastating impact and the wide unawareness of AI and ML vulnerabilities make attack vectors against AI and ML highly favourable for malicious operators. Furthermore, AI systems tend to be difficult to analyse post-incident as well as to monitor during operations. Discriminating a compromised from an uncompromised AI in real-time is still considered difficult. In this paper, we report on the state of the art of attack patterns directed against AI and ML methods. We derive and discuss the attack surface of prominent learning mechanisms utilised in AI systems. We conclude with an analysis of the implications of AI and ML attacks for the next decade of cyber conflicts as well as mitigations strategies and their limitations."
ALGORITHMIC EFFICIENCY INDICATOR FOR THE OPTIMIZATION OF ROUTE SIZE,"RodrÃ­guez, C; Sifuentes, M; Kaseng, F; Lezama, P",10.17993/3ctecno/2020.v9n2e34.49-69,2020,"In software development, sometimes experienced programmers have difficulty determining before performing their tests, which algorithm will work best to solve a particular problem, and the answer to the question will always depend on the type of problem and nature of the data to be used, in this situation, it is necessary to identify which indicator is the most useful for a specific type of problem and especially in route optimization. Currently, there are techniques and algorithms used in Artificial Intelligence, which, however, cannot display their potential without a well-defined data set. The paper seeks to explain and propose an algorithm selection indicator to build a consistent data set, given the lack of availability of data that allows the best route size decision to be made."
A data-driven risk measurement model of software developer turnover,"Ma, ZF; Li, RY; Li, T; Zhu, R; Jiang, R; Yang, J; Tang, MJ; Zheng, M",10.1007/s00500-019-04540-z,2020,"During the software development life cycle, the turnover of software developers is one of the critical risks that may lead to severe problems (such as postponement and failure of projects), which is often ignored by many professionals. To address this problem, we focus on the uncertainty of turnover risk of software developer (TRSD) and its loss incurred to projects. To tackle this problem, we propose a method to quantify the uncertain risks related to developer turnover, including resignation and replacement. Additionally, to calculate the extent of loss caused by TRSD, we employed machine learning, natural language processing, and data mining techniques to identify software development activities and establish the importance of developers by mining and analyzing the commit event logs. Moreover, based on the information entropy theory, we established a risk measurement model of TRSD that can be used to measure the risk level of each developer and the holistic risk of ongoing software projects. Finally, we validated the feasibility and efficacy through a case study."
An idea based on sequential pattern mining and deep learning for text summarization,"Maylawati, DS; Kumar, YJ; Kasmin, FB; Ramdhani, MA",10.1088/1742-6596/1402/7/077013,2019,"One of the Natural Language Processing (NLP) studies that has been widely researched is automatic text summarization. There are a lot of techniques and methods that are proposed for text summarization. However, not much attention has been given on the coherence and cohesion in text. The aim of this study is to present an idea to combine Sequential Pattern Mining (SPM) and Deep Learning (DL) for better text summarization process and result. In text summarization, it is important to produce understable and readable summary, and SPM as text representation extracting algorithm is capable to maintain the meaning of text by giving attention of the order of words appearance. Whereas DL is a popular and powerful machine learning technique widely used recently in various data mining studies. This study uses descriptive research methodology that collects all of the facts and information which are related to SPM and DL for text summarization, where NLP as the body of knowledge, SPM and DL as the method, and text summarization as the domain problem that need to be solved. The findings of the study are presented as a logical design and mapping of current text representation that can be implemented to further improve automatic text summarization results, in particular, to improve its coherence and cohesion."
Artificial Intelligence Helps Making Quality Assurance Processes Leaner,"Poth, A; Beck, Q; Riel, A",10.1007/978-3-030-28005-5_56,2019,"Lean processes focus on doing only necessary things in an efficient way. Artificial intelligence and Machine Learning offer new opportunities to optimizing processes. The presented approach demonstrates an improvement of the test process by using Machine Learning as a support tool for test management. The scope is the semi-automation of the selection of regression tests. The proposed lean testing process uses Machine Learning as a supporting machine, while keeping the human test manager in charge of the adequate test case selection."
An instrument to evaluate the maturity of bias governance capability in artificial intelligence projects,"Coates, DL; Martin, A",10.1147/JRD.2019.2915062,2019,"Artificial intelligence (AI) promises unprecedented contributions to both business and society, attracting a surge of interest from many organizations. However, there is evidence that bias is already prevalent in AI datasets and algorithms, which, albeit unintended, is considered to be unethical, suboptimal, unsustainable, and challenging to manage. It is believed that the governance of data and algorithmic bias must be deeply embedded in the values, mindsets, and procedures of AI software development teams, but currently there is a paucity of actionable mechanisms to help. In this paper, we describe a maturity framework based on ethical principles and best practices, which can be used to evaluate an organization's capability to govern bias. We also design, construct, validate, and test an original instrument for operationalizing the framework, which considers both technical and organizational aspects. The instrument has been developed and validated through a two-phase study involving field experts and academics. The framework and instrument are presented for ongoing evolution and utilization."
A dataset to facilitate automated workflow analysis,"Allard, T; Alvino, P; Shing, L; Wollaber, A; Yuen, J",10.1371/journal.pone.0211486,2019,"Data sets that provide a ground truth to quantify the efficacy of automated algorithms are rare due to the time consuming and expensive, although highly valuable, task of manually annotating observations. These datasets exist for niche problems in developed fields such as Natural Language Processing (NLP) and Business Process Mining (BPM), however it is difficult to find a suitable dataset for use cases that span across multiple fields, such as the one described in this study. The lack of established ground truth maps between cyberspace and the human-interpretable, persona-driven tasks that occur therein, is one of the principal barriers preventing reliable, automated situation awareness of dynamically evolving events and the consequences of loss due to cybersecurity breaches. Automated workflow analysis - the machine-learning assisted identification of templates of repeated tasks-is the likely missing link between semantic descriptions of mission goals and observable events in cyberspace. We summarize our efforts to establish a ground truth for an email dataset pertaining to the operation of an open source software project. The ground truth defines semantic labels for each email and the arrangement of emails within a sequence that describe actions observed in the dataset. Identified sequences are then used to define template workflows that describe the possible tasks undertaken for a project and their business process model. We present the overall purpose of the dataset, the methodology for establishing a ground truth, and lessons learned from the effort. Finally, we report on the proposed use of the dataset for the workflow discovery problem, and its effect on system accuracy."
Crowdsourcing Software Development: Task Assignment Using PDDL Artificial Intelligence Planning,"Tunio, MZ; Luo, HY; Wang, C; Zhao, F; Shao, WH; Pathan, ZH",10.3745/JIPS.04.0055,2018,"The crowdsourcing software development (CSD) is growing rapidly in the open call format in a competitive environment. In CSD, tasks are posted on a web-based CSD platform for CSD workers to compete for the task and win rewards. Task searching and assigning are very important aspects of the CSD environment because tasks posted on different platforms are in hundreds. To search and evaluate a thousand submissions on the platform are very difficult and time-consuming process for both the developer and platform. However, there are many other problems that are affecting CSD quality and reliability of CSD workers to assign the task which include the required knowledge, large participation, time complexity and incentive motivations. In order to attract the right person for the right task, the execution of action plans will help the CSD platform as well the CSD worker for the best matching with their tasks. This study formalized the task assignment method by utilizing different situations in a CSD competition-based environment in artificial intelligence (AI) planning. The results from this study suggested that assigning the task has many challenges whenever there are undefined conditions, especially in a competitive environment. Our main focus is to evaluate the AI automated planning to provide the best possible solution to matching the CSD worker with their personality type."
Code Review Comments: Language Matters,"Efstathiou, V; Spinellis, D",10.1145/3183399.3183411,2018,"Recent research provides evidence that effective communication in collaborative software development has significant impact on the software development lifecycle. Although related qualitative and quantitative studies point out textual characteristics of well-formed messages, the underlying semantics of the intertwined linguistic structures still remain largely misinterpreted or ignored. Especially, regarding quality of code reviews the importance of thorough feedback, and explicit rationale is often mentioned but rarely linked with related linguistic features. As a first step towards addressing this shortcoming, we propose grounding these studies on theories of linguistics. We particularly focus on linguistic structures of coherent speech and explain how they can be exploited in practice. We reflect on related approaches and examine through a preliminary study on four open source projects, possible links between existing findings and the directions we suggest for detecting textual features of useful code reviews."
Automating smart recommendation from natural language API descriptions via representation learning,"Xiong, W; Lu, ZH; Li, B; Hang, B; Wu, Z",10.1016/j.future.2018.05.006,2018,"Software reuse through Application Programming Interfaces (APIs) is a common practice in software development. It remains a big challenge to bridge the semantic gap between user requirements and application functionality with the development of Web-based services. This paper proposes a smart service recommendation approach via Representation Learning. To validate our approach, large-scale experiments are conducted based on a real-world accessible service repository, Programmable-Web. The results show the effectiveness of our proposed approach. (C) 2018 Elsevier B.V. All rights reserved."
HPC software capability landscape in China,"Chen, DB; Yuan, L; Zhang, YQ; Yan, JF; Kahaner, D",10.1177/1094342018760614,2020,"Hardware, applications, and software are equally important to the comprehensive strength of a country in the high-performance computing (HPC) arena. China has made significant progress in developing HPC systems in recent years. The nation's first win of the Gordon Bell Prize at Supercomputing 2016 (SC16) also represents an accomplishment in HPC applications. China's subsequent win in 2017 shows that the 2016 accomplishment was no accident. However, lacking adequately reliable and scalable application software remains the biggest challenge for China. Scientists and engineers who can develop algorithms and software to effectively use supercomputers are in short supply. The present report describes the current HPC software development landscape in China, including government projects and leading universities/research organizations/companies in terms of developing application software and programming frameworks (middleware), as well as examples of self-developed software in the application areas of energy and physics, aerospace, manufacturing, weather and climate, biotechnology (biotech), material science, artificial intelligence (AI), and data analytics. In addition, China's demand and supply of HPC experts are analyzed. Data for this report were generated during the first half of 2017. Some modifications to the text have been added to account for new information through the end of 2017."
The Next Generation Tester: Meeting the Challenges of a Changing IT World,"Bath, G",10.1007/978-3-030-29509-7_2,2020,"If we stop and look back at events that are shaping IT, the pace of change is amazing. Things are so different now, even if we just consider the last 5 years. Projects using, for example, artificial intelligence or Internet of Things are no longer over the horizon; they are reality. Testing needs to keep moving with the times even more than in the past. It needs to face the challenges posed by a changing IT landscape and continue to add value. To do that testers must make sure they are providing the right competencies to projects, regardless of the software development lifecycle used or the particular type of project: It's time to shape what the Next Generation (NG) Tester should be."
A Systematic Literature Mapping of Artificial Intelligence Planning in Software Testing,"de Lima, LF; Peres, LM; Gregio, ARA; Silva, F",10.5220/0009829501520159,2020,"Software testing is one of the most expensive software development processes. So, techniques to automate this process are fundamental to reduce software cost and development time. Artificial intelligence (AI) planning technique has been applied to automate part of the software testing process. We present in this paper a systematic literature mapping (SLM), using Petersen et al. (2015) approach of methods, techniques and tools regarding AI planning in software testing. Using the mapping, we identify 16 papers containing methods, techniques, frameworks and tools proposals, besides a survey. We identify testing techniques, testing phases, artifacts, AI planning techniques, AI planning tools, support tools, and generated plans in these selected papers. By mapping data analyses we identify a deficiency in the use of white-box and error-based testing techniques, besides the recent use of AI planning in security testing."
Analysis of trends in scientific publications by an NLP toolkit: A case study in Software Development Methods for Enhanced Living Environment,"Girma, M; Garcia, N; Zdravevski, E; Kifle, M; Pombo, N; Trajkovik, V",10.1109/sds49854.2020.9143896,2020,"As the number of published scientific articles increases, the analysis of trends and state-of-the-art in software engineering is becoming very time-consuming and laborious task. To address the ever-growing demands for systematic literatures review techniques, rapid review and scoping reviews techniques have emerged. We used an NLP powered tool, which employs the PRISMA surveying methodology, to automate most of the review processes. We used it to automatically review relevant articles indexed in IEEE Xplore, PubMed and Springer digital libraries on the topic Software Development for Enhanced Living Environments and Ambient Assisted Living. The relevant articles identified by the NLP toolkit contained up to 21 properties clustered into 3 logical groups. We discovered that Software Development for Enhanced and Assisted living environments attracted an increased attention from the scientific communities over the last 10 years and showed several trends in the specific research topics that fall into this scope. The research uncovered that iterative software methodology had been the most attractive research topic in the field. Despite the enormous empirical evidence on application and success stories of agile development methodologies in many software development engineering, it received a little attention from the scientific community in the software development for Enhanced and Assisted Living Environments. The NLP toolkit identified the most relevant articles that contained the defined properties in the search. Hence, it significantly reduced the manual work, while also generating informative tables, charts and graphs."
Research on the Development of Expert Systems Using Artificial Intelligence,"Burnashev, RA; Gabdrahmanov, RG; Amer, IF; Vakhitov, GZ; Enikeev, AI",10.1007/978-3-030-30604-5_21,2020,"This paper presents a study related to the development of logical expert systems using artificial intelligence on the example of diagnosing leukemia. For the development of the system, various approaches to the design of artificial intelligence systems and medical data used in the diagnosis of leukemia were applied. A distinctive feature of the expert system is that a specialist working with the knowledge base can not only get the answer he needs, but also get access to all the knowledge from the knowledge base by asking the necessary questions to the expert system. The methods are based on obtaining information about the characteristics of the software using specialized software that provides automation. Data sets and logical rules that were used for the initial diagnosis of the disease were identified. In developing and analyzing software requirements, a prototype system was developed. This system includes a perceptron and consists of 3 hidden layers. For software development was used programming languages Python, JavaScript and Prolog."
Fixing state change inconsistency with self regulating particle swarm optimization,"George, R; Samuel, P",10.1007/s00500-020-05124-y,2020,"Software has made a profound influence in all walks of life. Developing quality software is a major challenge, and the consistency and completeness of the design has a prime role in the development of quality software. Many a times, the process of consistency checking in industries is manual. Artificial intelligence techniques can replace many of these manual efforts to make the development of software easier and cost-effective. Software developers use state diagrams to represent the dynamic behavior in the design stage. We propose a novel application of self regulating particle swarm optimization (SRPSO) algorithm to ensure consistency of state diagrams during the design phase of software development. Inconsistency management is modeled as an optimization problem. In this work, we detect two types of state change inconsistency, incompatible behavior inconsistency and disconnected model inconsistency. A fitness function is defined to detect inconsistency. We make use of the SRPSO algorithm to resolve inconsistency. Detecting inconsistencies in the early stages of software development enables phase containment of errors and prevents errors from being propagated to the code. The proposed approach generates consistent and complete state diagrams leading to accurate code generation, meeting time deadlines, reducing cost of production and easy system maintenance."
A Long/Short-Term Memory Based Automated Testing Model to Quantitatively Evaluate Game Design,"Chen, LK; Chen, YH; Chang, SF; Chang, SC",10.3390/app10196704,2020,"The mobile casual game application lifespan is getting shorter. A company has to shorten the game testing procedure to avoid being squeezed out of the game market share. There is no sufficient testing indicator to objectively evaluate the operability of different game designs. Many automated testing methodologies are proposed, but they adopt rule-based approaches and cannot provide quantitative analysis to statistically evaluate gameplay experience. This study suggests applying Learning Time as a testing indicator and using the learning curve to identify the operability of different game designs. This study also proposes a Long/Short-Term Memory based automated testing model (called LSTM-Testing) to statistically testing game experience through end-to-end functionality (Input: game image; Output: game action) without any manual intervention. The experiment results demonstrate LSTM-Testing can provide quantitative testing data by using learning time as the control variable, game design as the independent variable, and time to complete game as the dependent variable. This study also demonstrates how LSTM-Testing evaluates the effectiveness of different gameplay learning strategies, e.g., reviewing the newest decisions, reviewing the correct decision, or reviewing the wrong decisions. The contributions of LSTM-Testing are (1) providing an objective and quantitative analytical game-testing framework, (2) reducing the labor cost of inefficient and subjective manual game testing, and (3) allowing game company boosts software development by focusing on game intellectual property and leaves game testing to artificial intelligence (AI)."
Ontology-Oriented Software Effort Estimation System for E-commerce Applications Based on Extreme Programming and Scrum Methodologies,"Adnan, M; Afzal, M; Asif, KH",10.1093/comjnl/bxy141,2019,"Presently, software industry is severely suffering from inaccurate effort estimation and inadequate unstructured or semi-structured project history management. In fact, both are difficult to accomplish and hence badly impact the software projects. We proposed improvements in the effort estimation and the project history management of e-commerce projects focusing on Extreme Programing (XP) and Scrum methodologies using ontology models in our software effort estimation system. Proposed system infers suitable estimate in the form of time, resources and lessons learnt as per the project leader's requirements by using description logic and HermiT reasoner. To validate our approach, we have performed a case study comprising 20 Business-to-Consumer (B2C) web projects and performed comparative analysis on the collected efforts in both XP and Scrum contexts by applying (Mean Magnitude of Relative Error) MMRE and PRED(25) prediction accuracy measures. Likewise, software functional size of understudy e-commerce projects was measured using COSMIC functional size measurement methodology. Regression analysis of relations among actual COSMIC function points, estimated effort, and actual effort spent for the projects show better significance-F and R-2 values for our approach. The comparative results show that overall proposed approach provides accurate estimates and significantly improves over planning poker and delphi methods by 10% and 30%, respectively."
D7-R4: Software Development Life-Cycle for Intelligent Vision Systems,"Olszewska, JI",10.5220/0008354804350441,2019,"Intelligent Vision Systems (IVS) are omnipresent in our daily life from social media apps to m-health services, from street surveillance cameras to airport e-gates, from drones to companion robots. Hence, IVS encompass any software which has a visual input processed by means of algorithm(s) involving Artificial Intelligence (AI) methods. The design and development of these IVS softwares has become an increasingly complex task, since vision-based systems have evolved into (semi-)autonomous AI systems, usually requiring effective and ethical data processing along with efficient signal processing and real-time hardware/software integration as well as User Experience (UX) and (cyber)security features. Consequently, IVS system development necessitates an adapted software development life-cycle (SDLC) addressing these multi-domain needs, whilst being developer friendly. Hence, we propose in this paper a new SDLC we called D7-R4 which allows developers to produce quality, new-generation IVS to be deployed in real-time and in real-world, unstructured environments."
Improving Requirement Prioritization and Traceability using Artificial Intelligence Technique for Global Software Development,"Haider, W; Hafeez, Y; Ali, S; Jawad, M; Ahmad, FB; Rafi, MN",10.1109/inmic48123.2019.9022775,2019,"Global software development (GSD) organizations encouraged to improve software quality of products. The GSD used highly skilled and advance technology at one place through the internet. The main challenges face in GSD with benefits are communication, coordination and control. These issues create ambiguities and incompleteness in requirements prioritization and traceability process due to lots of stakeholders and human efforts involved. Therefore, we proposed a framework to improve requirements prioritization and traceability process using artificial intelligent technique. The framework was evaluated using experiment and compared with existing solutions. Results described that proposed framework significantly improved requirements prioritization and traceability with less human interaction to reduce GSD challenges."
Recovering semantic traceability between requirements and design for change impact analysis,"Kchaou, D; Bouassida, N; Mefteh, M; Ben-Abdallah, H",10.1007/s11334-019-00330-w,2019,"One of the ultimate challenges in change impact analysis is traceability modeling between several software artifacts in the software life cycle. This paper proposes a traceability approach that relates requirements and design artifacts modeled in UML. Our method faces two essential challenges: the semantic ambiguities of requirement artifacts that could be written in different natural languages and the heterogeneity of the artifacts that have to be traced (textual description, UML diagrams, etc.). To face these challenges, our method determines the semantic relationships between the requirements modeled with the use case diagram and design modeled with the class and the sequence diagrams through a semantic model which is an intelligent natural language processing technique that analyzes the semantics among the sentences, regardless of the language they are written with. Thanks to the semantic model our approach compares similarities between words having the same role, which makes it more efficient than computing similarities between words of different kinds. The empirical investigation demonstrates the advantages of the semantic traceability using a semantic model compared to the use of an information retrieval technique."
Case Study on Requirements Management Tool for Small and Medium Software Projects,"Zainol, A; Jamal, AT",,2020,"In the previous decades, there are many requirements management tools available in the market. However, these tools are expensive, complicated, difficult to learn and too sophisticated for small and medium projects that have resources and budget limitation. Thus, a requirements management tool, known as Requirements Management Tool for Small and Medium Projects (RMT-SMP) is developed on open source platform targeting the small and medium software projects. This paper presents an evaluation of using RMT-SMP during software development projects for small and medium projects in the real industry. This work is steered based on empirical methods in software engineering using case study. In software engineering, case studies are used for validating research, for example, evaluation of new tools, processes, or methods. Thus, the case study research design components are research questions, preposition or hypothesis, unit of analysis, determination of how data are linked to prepositions and criteria interpret the findings. In order to establish quality of case study, we conducted construct validity, internal validity, external validity and reliability. The result of this case study has shown the success of applying the RMT-SMP during software project development for small and medium projects and can be concluded that RMT-SMP is practical and feasible for the small and medium projects. The RMT-SMP encourages the practitioners to have a better approach in managing their requirements during software development projects."
Software Development Effort Estimation from Unstructured Software Project Description by Sequence Models,"Kangwantrakool, T; Viriyayudhakorn, K; Theeramunkong, T",10.1587/transinf.2019IIP0014,2020,"Most existing methods of effort estimations in software development are manual, labor-intensive and subjective, resulting in over-estimation with bidding fail, and underestimation with money loss. This paper investigates effectiveness of sequence models on estimating development effort, in the form of man-months, from software project data. Four architectures; (1) Average word-vector with Multi-layer Perceptron (MLP), (2) Average word-vector with Support Vector Regression (SVR), (3) Gated Recurrent Unit (GRU) sequence model, and (4) Long short-term memory (LSTM) sequence model are compared in terms of man-months difference. The approach is evaluated using two datasets; ISEM (1,573 English software project descriptions) and ISBSG (9,100 software projects data), where the former is a raw text and the latter is a structured data table explained the characteristic of a software project. The LSTM sequence model achieves the lowest and the second lowest mean absolute errors, which are 0.705 and 14.077 man-months for ISEM and ISBSG datasets respectively. The MLP model achieves the lowest mean absolute errors which is 14.069 for ISBSG datasets."
User Stories identification in software's issues records using natural language processing,"Veitia, FJP; Roldan, L; Vegetti, M",10.1109/ARGENCON49523.2020.9505355,2020,"Nowadays most of software development companies have adopted agile development methodologies, which suggest capturing requirements through user stories. The use of these good practices improves the organization of work teams and the quality of the resulting software product. However, user stories are too often poorly written in practice and exhibit inherent quality defects. In addition, it is common to find the user stories of a software project immersed in large volumes of issues request logs from software quality tracking systems, which makes difficult to process them later. In order to solve these defects and to formulate high quality requirements, a current trend is the application of computational linguistic techniques to identify and then process user stories. In this work, we present two recurrent neural network models that were developed for the identification of user stories in issue records from software quality tracking systems for further processing."
First steps in building the Eurasian Latin Archive,"CarbÃ©, E; Giannelli, N",,2020,"The aim of the Eurasian Latin Archive (ELA, ela.unisi.it ) project is to build an open-access digital platform of Latin and multilingual texts from medieval and early modern times concerning East Asia, which includes a Digital library and tools for textual and semantic analysis. Begun in March 2018 and ended in February 2020, the start-up phase of ELA (DAS-MeMo, www.dasmemo.unisi.it) had been co-financed by Regione Toscana and involves the University of Siena, the IT company QuestIT, specialized in Artificial Intelligence, and the publisher Pacini. This article seeks to explain the workflow of the project, its challenges, the first steps in building the prototype of the platform and adds some notes about further developments."
A Report on the First Workshop on Software Engineering for Artificial Intelligence (SE4AI 2020),"Bandyopadhyay, S; Mukherjee, R; Sarkar, S",10.1145/3385032.3385055,2020,"With advancement in technology-driven decision making, the software-intensive systems for decisions have become more robust, dynamic, adaptive, context-aware, dependable. Architectural designs of such systems crave for new approaches where the data-driven decision making has to be incorporated in the solution. Methods for recommendation mechanism, prediction of operation failures, dealing with unsafe conditions etc are going to be part of the solution itself. Integrating such features to conceive an intelligent system that will directly influence the business solution is mostly appreciated. This would not have been possible without the direct interference of Artificial Intelligence which has been a standard procedure of industrial repertoire since 1980s. The direct impact of AI on social and economic life has been been felt mostly in last decade (since 2007) with the advent of smart phone, which contribute largely to big data. The era of big data has witnessed the efficacy of Machine Learning and there is a need of the hour to combine data-driven machine intelligence with human intelligence (insights and domain knowledge) to effectively make the software development (requirement, design, testing, deployment and operation management) intelligent. The research community has shown a keen interest in this emerging field. In this report, we present a pre-organization summary of the workshop to be held on February 27, 2020, at IIIT Jabbalpur (India), co-located with the 13th Innovations in Software Engineering Conference (ISEC 2020)."
Using AI to Improve Product Teams' Customer Empathy,"Grigoreanu, V; Hammontree, M; Lowdermilk, T",10.18573/book3.ag,2020,"During customer conversations, it is important to know both what questions to ask at any point during the development cycle, and how to ask them. Asking the right questions to capture rich, accurate, and relevant customer feedback is not easy, and professionally-trained researchers cannot be a part of every customer conversation. To scale out researchers' knowledge, we built an artificial intelligence system, the VIVID whisper-bot, trained on three theories: the Hypothesis Progression Framework (contextual research questions for each product development phase), the VIVID grammar framework (asking who, what, why, how, where, how much, and when type questions to recreate rich stories), and the syntactical structure of biased and leading questions. The whisper-bot listens in on a customer conversation, highlights customers' key verbalization (e.g., pain points using the product), and suggests follow-up interview questions (e.g., removing bias or enriching a story). It thereby encourages good interview practices for everyone, which we believe will increase empathy on product development teams, and lead to improvements in the products' user experience."
Scenario-Based Microservice Retrieval Using Word2Vec,"Ma, SP; Chuang, Y; Lan, CW; Chen, HM; Huang, CY; Li, CY",10.1109/ICEBE.2018.00046,2018,"Microservice architecture (MSA) is an emerging software architectural style, which differs fundamentally from the monolithic, layered architecture. During the development and maintenance of microservice systems, how to provide an effective service retrieval mechanism is a critical challenge to avoid the problems of rework and duplicate code. Meanwhile, nowadays, using the BDD (Behavior-Driven Development) method to develop microservices becomes more and more popular due to its agility and domain-driven characteristics. BDD is an agile software development approach emphasizing that test cases are written in a common language to include scenarios that describe the features of a target system. In this paper, we propose an approach, referred to as SMSR (Scenario based MicroService Retrieval), to recommend appropriate microservices to users based on the user-written BDD test scenarios. The proposed service retrieval algorithm is based on word2vec, a widely-used machine learning method in NLP (Natural Language Processing), to perform service filtering and service similarity calculation. Experiment results show that SMSR is able to effectively retrieve appropriate microservices from the service repository."
Assuring Virtual PLC in the Context of SysML Models,"Alenazi, M; Reddy, D; Niu, N",10.1007/978-3-319-90421-4_8,2018,"In complex industrial projects, textual information has been recognized as an important factor for automatically recovering trace links in software development. The goal of this paper is to empirically investigate if the trace links in the simulation result can assist in validating a virtual Programmable Logic Controller (PLC) in the context of System Modeling Language (SysML). We integrate the concept of obstacle analysis to recover situations in which a safety requirement will not be satisfied. Therefore, we use fault tree analysis to validate the safety requirements, and further use the elements of the fault tree to evaluate the quality of the automatically recovered trace links. We showed that the identified impacts of assuring virtual PLC (V-PLC) elements using traceability information can be reused to ensure a number of other PLCs or requirements in the systems models. This paper presents our experience of applying our approach using an automatic transmission systems built in SysML models."
Iterative Process for Generating ER Diagram from Unrestricted Requirements,"Javed, M; Lin, YQ",10.5220/0006778701920204,2018,"Requirements analysis for generating a conceptual model such as an Entity Relationship Diagram (ERD) is an essential task in software development life cycle. In this paper, we are presenting a Natural Language Processing (NLP) based approach to generate the ERD from requirements in an unrestricted format such as general requirements, user stories or Use Case Specification (UCS). To assess the performance and correctness of the proposed technique, we compare our approach with existing automated techniques by processing the same requirements. The preliminary results show a significant improvement."
EMPIRICALLY ANALYZING AND EVALUATING SECURITY FEATURES IN SOFTWARE REQUIREMENTS,"Hayrapetian, A; Raje, R",10.1145/3172871.3172879,2018,"Software requirements, for complex projects, often contain specifications of non-functional attributes (e.g., security-related features). The process of analyzing such requirements for compliance is laborious and error prone. Due to the inherent free-flowing nature of software requirements, it is appealing to apply Natural Language Processing (NLP) and Machine Learning (ML)-based techniques for analyzing these documents. In this paper, we propose a semi-automatic methodology that assesses the security requirements of software systems with respect to completeness and ambiguity, creating a bridge between the requirements documents and being in compliance with standards Security standards, such as ISO and OWASP, are compared against software project documents for textual entailment relationships. These entailment results along with the document annotations are used to train a Neural Network model to predict whether a given statement in the document is found within the security standard or not. Hence, this approach aims to identify the appropriate structures that underlie software requirements documents. Once such structures are formalized and empirically validated, they will provide guidelines to software organizations for generating comprehensive and unambiguous requirements specification documents as related to security-oriented features."
Towards a Knowledge warehouse and expert system for the automation of SDLC tasks,"Kapur, R; Sodhi, B",10.1109/ICSSP.2019.00011,2019,"Cost of a skilled and competent software developer is high, and it is desirable to minimize dependency on such costly human resources. One of the ways to minimize such costs is via automation of various software development tasks. Recent advances in Artificial Intelligence (AI) and the availability of a large volume of knowledge bearing data at various software development related venues present a ripe opportunity for building tools that can automate software development tasks. For instance, there is significant latent knowledge present in raw or unstructured data associated with items such as source files, code commit logs, defect reports, comments, and so on, available in the Open Source Software (OSS) repositories. We aim to leverage such knowledge-bearing data, the latest advances in AI and hardware to create knowledge warehouses and expert systems for the software development domain. Such tools can help in building applications for performing various software development tasks such as defect prediction, effort estimation, code review, etc."
Introducing Artificial Intelligence Agents to the Empirical Measurement of Design Properties for Aspect Oriented Software Development,"Velan, SS",10.1109/aicai.2019.8701250,2019,"The proponents of Aspect Oriented Software Development (AOSD) methodology have done a tremendous amount of work to bring out the positive effects of its adoption using quantitative assessment. A structured assessment of the methodology requires a well-defined quality model. In this paper, an AI agent based quality model has been proposed to evaluate the effect of aspectization. The model has been applied on already existing and equivalent versions of object oriented and aspect oriented case study application, university automation software. Specific metrics for the software design properties have been measured using AI agents for the different versions and were used to infer upon the effect on quality. Based on the initial measurement, it was found that aspectization has positively improved all the three quality attributes defined in the quality model. The attributes of evolution and reusability showed significant improvement in quality due to the effect of aspectization."
"SReYantra: Automated Software Requirement Inter-dependencies Elicitation, Analysis and Learning","Deshpande, G",10.1109/ICSE-Companion.2019.00076,2019,"Requirements elicitation is a cognitively difficult task. Rich semantics in natural language based requirements impose challenges in elicitation, analysis and maintenance of requirement inter-dependencies. The challenges intensify further when dependency types and strengths are considered. Ignoring inter-dependencies can adversely impact the design, development and testing of software products. This PhD research proposal addresses three main challenges. First, Natural Language Processing (NLP) is studied to automatically extract dependencies from textual documents. Further verb classifiers are utilized to automate elicitation and analysis of different types of dependencies (e.g: requires, coupling etc). Second, representation and maintenance of changing requirement dependencies from designing graph theoretic algorithms will be explored. Third, the process of providing recommendations of dependencies will be studied. The results are aimed at assisting project managers to evaluate the impact of inter-dependencies and make effective decisions in software development life cycle."
Specification of Requirements and Software Architecture for the Customisation of Enterprise Software A multi-case study based on the RE4SA model,"Spijkman, T; Brinkkemper, S; Dalpiaz, F; Hemmer, AF; van de Bospoort, R",10.1109/REW.2019.00015,2019,"Many failed software projects can be traced to bad requirements management. Additionally, there is a big gap between state of the art and practice in software architecture. For enterprise software customisation, not only do these issues apply, but additional challenges exist too. Instead of one standard software product, vendors often have to deal with customised versions with additional maintenance challenges. In this research, we apply the Requirements Engineering for Software Architecture (RE4SA) model via a multi-case study to show how the requirements engineering and software architecture disciplines can be linked, and in doing so provide improvements to both areas. Our multi-case study regards enterprise software customisation and shows improvements in requirements management and higher alignment between the software architecture and requirements."
An Approach to Similar Software Projects Searching and Architecture Analysis Based on Artificial Intelligence Methods,"Nadezhda, Y; Gleb, G; Pavel, D; Vladimir, S",10.1007/978-3-030-01818-4_34,2019,"Software engineers from all over the world solve independently a lot of similar problems. In this condition the problem of architecture reusing becomes an issue of the day. In this paper, two phase approach to determining software projects with necessary functionality and reusable architecture is proposed. This approach combines two methods of artificial intelligence: natural language clustering technique and a novel method for comparing software projects based on the ontological representation of their architecture automatically obtained from the projects source code. There are 3 metrics presented in this article that allow us to determine the measure of the relevance of the selected projects based on projects architecture indices."
EmbSE: AWord Embeddings Model Oriented Towards Software Engineering Domain,"FÃ¡vero, EMD; Casanova, D; Pimentel, AR",10.1145/3350768.3350785,2019,"The representation of contexts is essential in tasks involving Natural Language Processing (NLP). In the field of software engineering, classifying similar texts within a specific context has been a complex task, considering the informality and the complexity inherent of the texts produced through many software development processes (e.g. agile methods). Word embeddings capture semantic and syntactic information about unique words, allowing them to be represented in a dense and low-dimensional format. This property makes the embeddings vectors an important input feature for machine learning algorithms that aim to classify texts. Although there has been much research around the application of word embeddings in several areas, up to this moment, there is no knowledge about studies that have explored its application in the creation of a specific model for the domain of the area of software engineering. Thus, this article presents the proposal to generate an embedding model, called embeddings model for software engineering (EmbSE), which can recognize specific and relevant terms in the software engineering context. This model can be used as the main entry in the classification of several textual artifacts generated during the software development project process. The results are promising, presenting a 48% improvement in the mAP values for the EmbSE concerning the model trained on the generic corpus. This reinforces the hypothesis that a model of this nature can bring significant improvements in the classification of texts of the area."
When Natural Language Processing Jumps into Collaborative Software Engineering,"Gilson, F; Weyns, D",10.1109/ICSA-C.2019.00049,2019,"Software engineering is an intrinsically collaborative activity, especially in the era of Agile Software Development. Many actors are partaking in development activities, such that a common understanding should be reached at numerous stages during the overall development life-cycle. For a few years now, Natural Language Processing techniques have been employed either to extract key information from free-form text or to generate models from the analysis of text in order to ease the sharing of knowledge across all parties. A significant part of these approaches focuses on retrieving lost domain and architectural knowledge through the analysis of documents, issue management systems or other forms of knowledge management systems. However, these post-processing methods are time-consuming by nature since they require to invest significant resources into the validation of the extracted knowledge. In this paper, inspired by collaborative tools, bots and Natural Language extraction approaches, we envision new ways to collaboratively record and document design decisions as they are discussed. These decisions will be documented as they are taken and, for some of them, static or behavioural models may be generated on-the-fly. Such an interactive process will ensure everyone agrees on critical design aspects of the software. We believe development teams will benefit from this approach because manual encoding of design knowledge will be reduced and will not be pushed to a later stage, when not forgotten."
Analysis of Specification in Japanese Using Natural Language Processing,"Okano, K; Takahashi, K; Ogata, S; Sekizawa, T",10.1007/978-3-319-97679-2_2,2019,"A requirement specification for software is usually described in a natural language and thus may include sentences containing ambiguity and contradiction. Problems due to the ambiguity often occur at the stage of the verification process of software development, and this forces developers to go back to the design process again. In order to prevent this kind of rework, a method of automatically converting a required specification written in Japanese to a state transition model is desired to help detect ambiguity and contradiction points of the specification. This paper proposes a method for this purpose, and reports on the result of applying the method to a specification example of an electric pot."
Software Defect Estimation Using Machine Learning Algorithms,"YalÃ§iner, B; Ãzdes, M",10.1109/ubmk.2019.8907149,2019,"Software Engineering is a comprehensive domain since it requires a tight communication between system stakeholders and delivering the system to be developed within a determinate time and a limited budget. Delivering the customer requirements include procuring high performance by minimizing the system. Thanks to effective prediction of system defects on the front line of the project life cycle, the project's resources and the effort or the software developers can be allocated more efficiently for system development and quality assurance activities. The main aim of this paper is to evaluate the capability of machine learning algorithms in software defect prediction and find the best category while comparing seven machine learning algorithms within the context of four NASA datasets obtained from public PROMISE repository [12]. All in all, the results of ensemble learners category consisting of Random Forests (RF) and Bagging in defect prediction is pretty much its counterparts."
HR Decision-Making Support Based on Natural Language Processing,"Ivaschenko, A; Milutkin, M",10.1007/978-3-030-29743-5_12,2019,"This paper presents an overview and analysis of IT solution of text understanding being applied to a programming professional domain. Conclusions summarize the authors' experience in NLP/NLU in the last years. Binary classification and logistic regression is used to solve typical problems. The results of practical research are presented. The paper develops the ideas of understanding texts in software development domain using standard text processing tools. The proposed solution is recommended for HR professionals who search suitable candidates for a job based on their blogs, online presence and code."
A Built-in Criteria Analysis for Best IT Governance Framework,"Hamzane, I; Abdessamad, B",,2019,"The implementation of IT governance is important to lead and evolve the information system in agreement with stakeholders. This requirement is seriously amplified at the time of the digital area considering all the new technologies that have been launched recently (Big DATA, Artificial Intelligence, Machine Learning, Deep learning, etc.). Thus, without a good rudder, every company risks getting lost in a sea endless and unreachable goal. This paper aims to provide decision-making system that allows professionals to choose IT governance framework suitable to desired criteria and their importance based on a multi-criteria analysis method (WSM), we did implement a case study based on a Moroccan company. Moreover, we present a better understanding of IT Governance aspects such as standards and best practices. This paper goes into a global objective that aims to build an integrated generated meta-model for a better approach of IT Governance."
Comparing Different Estimation Methods for Software Effort,"Abulalqader, FA; Ali, AW",10.1109/AiCIS.2018.00016,2018,"Software project management begins with a set of collected activities called project pluming activity. Before starting a project, the program's team must evaluate the work to be done, the resources to be reorganized, and the time from the beginning computation. When these activities have been completed, the program's team should establish a set of projects that will assign program engineering tasks, key milestones, determine the responsibly for each task, and identify associated dependencies among participants that may have a strong impact on progress. In general, there isn't a complete accurate estimation method, but in this research I tried to discover the best programming methods to find the best programming estimate. The aim of this research is to present a study of the principles for reducing the cost of software and understanding how these techniques are applied to general program divisions. We provide basic algorithms in Artificial Intelligence, Artificial Neural Networks, Genetic Algorithms, and Fuzzy Logic Algorithm to determine which algorithm is the most appropriate to fmd the best estimates as possible in terms of precision. The best results were found in Neural Networks, but competitive results were found between types of Neural Networks (FFNN, CNN, ENN, RBFN and NARX). The NARX network was observed to provide the best accuracy, but Genetic Algorithm proved better than Fuzzy Logic which is the worst compared to Neural Networks and Genetic Algorithms."
Intelligent Planning Methods and Features of Their Usage for Development Automation of Dynamic Integrated Expert Systems,"Rybina, GV; Blokhin, YM; Parondzhanov, SS",10.1007/978-3-319-63940-6_21,2018,"The problems of intellectualization in the development process of dynamic integrated expert systems basing on the problem-oriented methodology and the AT-TECHNOLOGY workbench are considered. The experience from carrying out intellectual planning development plan generating of prototypes in integrated expert systems, the intelligent planner usage, reusable components, typical project procedures, and other components of the intellectual software environment in the AT-TECHNOLOGY workbench is described."
A Multi-Platform Framework for Artificial Intelligence Engines in Automotive Systems,"Marina, LA; Trasnea, B; Grigorescu, SM",,2018,"It is becoming increasingly evident that data science and artificial intelligence are key technologies in the future of automotive industry. Artificial Intelligence (AI) successes are significant, with some limitations in terms of algorithms portability and code industrialization for mass production and deployment. This is because there are many domains where several software development process standards need to be applied. The approach presented in this paper tries to overcome this issue by creating a unique platform with multiple artificial intelligence engines. By using state-of-the-art AI libraries, together with target-independent software, tackles the challenge of driving context recognition in embedded systems, and uses it as a validation method."
Fixing class design inconsistencies using self regulating particle swarm optimization,"George, R; Samuel, P",10.1016/j.infsof.2018.03.005,2018,"Context: The practice of using Unified Modeling Language models during software development and the chances of occurrence of model inconsistencies during software design are increasing. Hence detection of intra-model design inconsistencies is significant in the development of quality software. Objective: The existing approaches of detecting class attribute inconsistencies rely on human decision making. Manual detection of inconsistencies is exhaustive, time consuming and sometimes incomplete. Therefore, we propose an automated and novel approach to perform consistency check of class attributes using artificial intelligence. Method: Inconsistency in attribute definition and specification is detected and fixed with self regulating particle swarm optimization (SRPSO) algorithm that uses a fitness function to optimize the consistency of attributes in class diagram and activity diagrams. SRPSO is preferred since the best particle is not influenced by its or others experience and uses its direction as the best direction and the remaining particles use self and social knowledge to update their velocity and position. Result: The use of artificial intelligence technique for detection and fixing of inconsistencies during the software design phase ensures design completeness through generation of models with consistent attribute definitions and a significant improvement in software quality prediction, accurate code generation, meeting time deadlines, and software production and maintenance cost is achieved. Conclusion: Ensuring consistency and completeness of models is an inevitable aspect in software design and development. The proposed approach automates the process of inconsistency detection and correction in class attribute definition and specification using SRPSO algorithm during the design phase of software development."
An Ontology-based Approach to Automate the Software Development Process,"Athiththan, K; Rovinsan, S; Sathveegan, S; Gunasekaran, N; Gunawardena, KSAW; Kasthurirathna, D",,2018,"Ontologies are well-known for representing knowledge of a particular domain and an upcoming trend in the field of Computer Science to produce intelligent systems. Further, they help to solve traceability issues and transitive dependencies. Creation of software applications and use of software applications are increased due to new and innovative customer requirements and technologies. Development teams are following several Software Development methodologies to support and to produce quality software applications to the outer world. Agile methodologies are becoming more popular in small organizations and development teams to support the feature by feature development with less throughput. User Stories represent the actual user requirements in Agile. This paper proposes a solution named Sponto, which to automate the software development process using ontology-based approaches to produce reliable boilerplates to the developers and the communities to reuse the base and to build the software applications without spending a considerable amount of time on recreating the artifacts. The proposed solution supports and generates database scripts, Business Process Model diagrams, Java code snippets, and test cases from user stories."
"Why the standard methods, 5GL, common platforms and reusable components are the four pillars of the new computational paradigm Programming without programmers","Stanev, I; Koleva, M",,2019,"Two important problems concerning software engineering are identified: low efficiency of the software development process, and lack of programmers. Three key components with considerable impact on the software development process, namely the software engineering method, the problem specification language, and the platform used, are identified. Four state of the art reviews are prepared including software engineering methods, problem specification languages, cloud computing platforms, and related works. This research problem is solved by the realization of the Knowledge Based Automated Software Engineering (KBASE) method for generation of enterprise information systems, based on 5GL specification languages, common platform for automated programming and a set of intelligent components for generation of new applications from well-defined system and business reusable components in three groups - domain independent, domain dependent and problem solving. KBASE concept, KBASE technological process, KBASE specification language, and KBASE platform are described. Key results achieved in the related works are detailed. The efficiency of the new computational paradigm is presented."
InDoCaS: A process for domain engineering in software production lines with a quality approach,"CanelÃ³n, R",,2019,"This article proposes a process for domain engineering in software production lines based on Software quality called InDoCaS. The main disciplines of this process are the analysis, design and implementation of the domain. The process proposed here can be used in the software production line development approach and can be instantiated for a specific domain and the produced software assets can be reused to generate a product from a particular domain family. The concepts of: development processes, reuse of software assets, domain engineering, variability models, software production lines and Bayesian learning techniques are applied to define the way in which these applications are adapted at runtime to the applications. environment changes. These software variability and adaptability requirements represent an interesting challenge within software engineering. The contribution of this paper to the area of software development based on software production lines is to propose a process for domain engineering with a quality approach that will allow the specification of the quality requirements of the domain application families to be formalized, presenting details about the activities of analysis, design and implementation of the base architecture and its refinement."
Crowd-Generated Data Mining for Continuous Requirements Elicitation,"Alwadain, A; Alshargi, M",,2019,"In software development projects, the process of requirements engineering (RE) is one in which requirements are elicited, analyzed, documented, and managed. Requirements are traditionally collected using manual approaches, including interviews, surveys, and workshops. Employing traditional RE methods to engage a large base of users has always been a challenge, especially when the process involves users beyond the organization's reach. Furthermore, emerging software paradigms, such as mobile computing, social networks, and cloud computing, require better automated or semi-automated approaches for requirements elicitation because of the growth in systems users, the accessibility to crowd-generated data, and the rapid change of users' requirements. This research proposes a methodology to capture and analyze crowd-generated data (e.g., user feedback and comments) to find potential requirements for a software system in use. It semi-automates some requirements-elicitation tasks using data retrieval and natural language processing (NLP) techniques to extract potential requirements. It supports requirements engineers' efforts to gather potential requirements from crowd-generated data on social networks (e.g., Twitter). It is an assistive approach that taps into unused knowledge and experiences emphasizing continuous requirements elicitation during systems use."
From Code to Data: AI at Scale for Developer Productivity,"Sundaresan, N",10.1145/3292500.3340410,2019,"The last decade has seen three great phenomena in computing the rebirth of AI algorithms and AI hardware; the evolution of cloud computing and distributed software development; and the explosive growth of open source software that has led to the availability of code as data, and its associated metadata, at scale. In this talk, we will describe how we take advantage of innovations in these dimensions to improve developer productivity and infuse AI and automation into software processes. We will discuss examples of how we built intelligent software by creating AI algorithms driven by deep understanding of code as data. In addition, we will talk about how data can also be treated as code for the next-generation AI-infused software development."
Research on Software Project Schedule Based on Critical Chain,"Xu, HH; Wan, L; Xue, S",10.1088/1755-1315/295/2/012026,2019,"Based on the problems and causes of the system analysis software project schedule, the idea of introducing critical chain technology into the software project schedule is proposed. This paper expounds the basic concept of the critical chain schedule of the software project, describes its core idea in detail, and constructs the critical chain schedule model of the software project. Then the objectives and construction principles of the software project critical chain schedule model are analyzed, and the implementation process is explained. Finally, the feasibility of the software project critical chain schedule is verified by examples."
Simplified Agile Software Project Selection Model Using Natural Language Processing Based Upon Agile Team Skills,"Sharma, S; Kumar, D; Fayad, ME",10.1109/iccike47802.2019.9004392,2019,"every software development project is unique but still shares some similarities with other projects in terms of domain, database used, programming language employed, etc. Based upon which a lot of work has been done so far to predict project size, efforts required, budget required to build the software project using historical data. But, a very less attention has been given in modeling world to analyze the skills of agile team responsible for software project development. Hence, this paper suggests a simplified agile software project selection model using natural language processing based upon the agile team skills named as Stair-case Model. After all it is the capability of agile team to deliver the project on time within budget. A strong skilled agile team having relevant experience has brighter chances to deliver the quality project on time and vice-versa."
A Novel Framework to Automatically Generate IFML Models From Plain Text Requirements,"Hamdani, M; Butt, WH; Anwar, MW; Ahsan, I; Azam, F; Ahmed, MA",10.1109/ACCESS.2019.2959813,2019,"User Interfaces (UI's) are highly important in this era of web and mobile applications. Therefore, an efficient and accurate development of UI's is desirable in early Software Development Life Cycle (SDLC) phases. To achieve this, Object Management Group (OMG) introduced Interaction Flow Modeling Language (IFML) standard in 2013. IFML provides the modeling of manifold UI's for different applications like mobile, web and desktop. Although IFML is based on Model Driven Engineering (MDE) principle, the development of user interface models from initial requirements is still complex and time consuming task. Particularly, it requires domain expertise to understand several IFML concepts like domain model, view container etc. for the proper modeling of user interfaces. Consequently, there is a strong need of an approach to automate the development of IFML models from initial plain text requirements. This article presents a novel framework to automatically generate IFML models from textual requirements by utilizing the features of Natural Language Processing (NLP). Particularly, a set of NLP rules are developed to extract important IFML elements like View Components, Events etc. from textual requirements. Furthermore, a comprehensive algorithm is developed for the systematic execution of NLP rules in order to generate both IFML Domain and Core models. As a part of research, a sophisticated T ext to IFML(T2IF) tool is developed. The feasibility of proposed framework is demonstrated through movie manager and online bookstore case studies. The evaluation results prove that the proposed framework is capable of generating IFML models from textual requirements with high accuracy."
How Artificial Intelligence Can Improve Web Development and Testing (Invited Paper),"Stocco, A",10.1145/3328433.3328447,2019,"The Artificial Intelligence (AI) revolution in software development is just around the corner. With the rise of AI, developers are expected to play a different role from the traditional role of programmers, as they will need to adapt their know-how and skillsets to complement and apply AI-based tools and techniques into their traditional web development workflow. In this extended abstract, some of the current trends on how AI is being leveraged to enhance web development and testing are discussed, along with some of the main opportunities and challenges for researchers."
Research on Artificial Intelligence Software Development Architecture Based on Data Mining Technology,"Cao, ZY",10.25236/iwmecs.2019.060,2019,"In many fields of computer science, artificial intelligence software is the most challenging and creative field. With the birth and development of artificial intelligence, people begin to use computers in the field. The core step is structured processing of unstructured data, which can reduce the volume of data and the difficulty of data mining, analysis and application. Based on the in-depth analysis of the nature of software development automation, based on the ubiquitous model of software systems, the concept of template engineering is introduced, and the automation development process is supported and implemented through the mapping transformation of template engineering and layer language. Generates all rules that contain only the items of the collection, where each rule has only one item to the right, and the definition of the middle rule is used here. Transforming the feature representation of the sample into a new feature space makes classification or prediction easier, and it has become a hot spot to promote this wave of artificial intelligence."
Preconditions of Successful Implementations of Predictive Analytics Solutions,"Meijere, S; Tambovceva, T",,2019,"The research is focused on how companies manage and should manage implementation of artificial intelligence (AI) / machine learning (ML) based predictive analytics solutions. What are the preconditions of successful implementations and lessons learned from failed cases. Research methods used by the authors are literature review, experiments, qualitative and quantitative surveys. The aim of the research is to identify a model / framework to ensure successful implementation of AI solution in an organization. As a result of the study, the authors propose a model / structure that should be followed for the successful implementation of predictive analytics solutions. The main conclusion is that new IT initiatives, as predictive analytics, to some extent, is an IT project, must go through the classic stages of change management, just like any other initiative to change business processes. The authors recommend that companies allocate enough time, human and other necessary resources for the preparation and implementation of predictive analytics solutions. The goal should be clearly defined and measurable, and communication and change in business processes should be carefully planned. Another important step is training."
Sindhi Speech Recognition System,"Khoso, FH; Hakro, DN; Nasir, SZ",,2019,"For languages around the world, one of the most revolutionary and significant technology is the Speech Recognition, which helps in advancement of languages computing. Speech recognition systems for most of the languages around the world have either been already developed or under the process of development. Unlike other languages, the number of characters and the sounds in Sindhi language is more than the other languages, which makes it unique so most of the available approaches cannot be used for Sindhi language. Therefore, a totally different approach is required in correspondence of Sindhi sounds and characters for designing and development of the tools for Sindhi Speech Recognition. The literature review of Sindhi speech recognition reveals that there is a gap in the quality work and no SR system is available (to the best of our knowledge) with respect to software development for recognition of spoken Sindhi characters and sounds, which may be converted in Sindhi writing by the software. If the input is made through Sindhi speaking, it will not only help the people in reducing effort of typing of very difficult character sounds and characters of Sindhi but also be beneficial for developers who need to map the 52 characters of Sindhi Alphabets against 26 alphabets of English keyboard."
Smart Measurements and Analysis for Software Quality Enhancement,"Dahab, S; Maag, S; Mallouli, W; Cavalli, A",10.1007/978-3-030-29157-0_9,2019,"Requests to improve the quality of software are increasing due to the competition in software industry and the complexity of software development integrating multiple technology domains (e.g., IoT, Big Data, Cloud, Artificial Intelligence, Security Technologies). Measurements collection and analysis is key activity to assess software quality during its development live-cycle. To optimize this activity, our main idea is to periodically select relevant measures to be executed (among a set of possible measures) and automatize their analysis by using a dedicated tool. The proposed solution is integrated in a whole PaaS platform called MEASURE. The tools supporting this activity are Software Metric Suggester tool that recommends metrics of interest according several software development constraints and based on artificial intelligence and MINT tool that correlates collected measurements and provides near real-time recommendations to software development stakeholders (i.e. DevOps team, project manager, human resources manager etc.) to improve the quality of the development process. To illustrate the efficiency of both tools, we created different scenarios on which both approaches are applied. Results show that both tools are complementary and can be used to improve the software development process and thus the final software quality."
A Machine Learning Approach for Continuous Development,"Russo, D; Lomonaco, V; Ciancarini, P",10.1007/978-3-319-70578-1_11,2018,"Complex and ephemeral software requirements, short time-to-market plans and fast changing information technologies have a deep impact on the design of software architectures, especially in Agile/ DevOps projects where micro-services are integrated rapidly and incrementally. In this context, the ability to analyze new software requirements and understand very quickly and effectively their impact on the software architecture design becomes quite crucial. In this work we propose a novel and flexible approach for applying machine learning techniques to assist and speed-up the continuous development process, specifically within the mission-critical domain, where requirements are quite difficult to manage. More specifically, we introduce an Intelligent Software Assistant, designed as an open and plug-in based architecture powered by Machine Learning techniques and present a possible instantiation of this architecture in order to prove the viability of our solution."
Extracting concepts from the software requirements specification using natural language processing,"Kuchta, J; Padhiyar, P",,2018,"Extracting concepts from the sofhv are requirements is one of the first step on the way to automating the software development process. This task is difficult due to the ambiguity of the natural language used to express the requirements specification. the methods used so far consist mainly of statistical analysis of words and matching expressions with a specific ontology of the domain in which the planned software will be applicable. This article proposes a method and a tool to extract concepts based on a grammatical analysis of requirements written in English without the need to refer to specialized ontology. These concepts can be further expressed in the class model, which then can be the basis for the object oriented analysis of the problem. This method uses natural language processing (NI,P) techniques to recognize parts of speech and to divide sentences into phrases and also the WordNet dictionary to search for known concepts and recognize relationships between them."
Entailment-Based Intelligent System for Software Project Monitoring and Control,"Chang, YC; Shih, CW; Hsu, WL",10.1109/JSYST.2016.2563463,2018,"Inrecent years, software project managers compare actual completion of activities against the progress reports filled by project members to identify significant deviations from the estimated schedules and manage software project risks. However, quantitativemeasurements are limited due to the format of project documents, which are mostly natural languages. In this paper, we propose an intelligent system for software project monitoring and control by using natural language processing techniques to recognize textual entailment of progress reports to further evaluate the level of project fulfillment in a qualitativemanner. Our experimental results demonstrate that the proposed method can recognize entailment from text efficiently and outperform other textual entailment approaches. Moreover, we successfully apply the textual entailment technique to project monitoring and control, which not only reduces the project cost and human's effort but also provides a basis for project managers to qualitatively evaluate the performance of each project member."
Natural Language Processing for Productivity Metrics for Software Development Profiling in Enterprise Applications,"Delaney, S; Chan, CCK; Smith, D",10.1145/3299819.3299830,2018,"In this paper, we utilize ontology-based information extraction for semantic analysis and terminology linking from a corpus of software requirement specification documents from 400 enterprise-level software development projects. The purpose for this ontology is to perform semi-supervised learning on enterprise-level specification documents towards an automated method of defining productivity metrics for software development profiling. Profiling an enterprise-level software development project in the context of productivity is necessary in order to objectively measure productivity of a software development project and to identify areas of improvement in software development when compared to similar software development profiles or benchmark of these profiles. We developed a semi-novel methodology of applying NLP OBIE techniques towards determining software development productivity metrics, and evaluated this methodology on multiple practical enterprise-level software projects."
Spherical Paragraph Model,"Zhang, RQ; Guo, JF; Lan, YY; Xu, J; Cheng, XQ",10.1007/978-3-319-76941-7_22,2018,"Representing texts as fixed-length vectors is central to many language processing tasks. Most traditional methods build text representations based on the simple Bag-of-Words (BoW) representation, which loses the rich semantic relations between words. Recent advances in natural language processing have shown that semantically meaningful representations of words can be efficiently acquired by distributed models, making it possible to build text representations based on a better foundation called the Bag-of-Word-Embedding (BoWE) representation. However, existing text representation methods using BoWE often lack sound probabilistic foundations or cannot well capture the semantic relatedness encoded in word vectors. To address these problems, we introduce the Spherical Paragraph Model (SPM), a probabilistic generative model based on BoWE, for text representation. SPM has good probabilistic interpretability and can fully leverage the rich semantics of words, the word co-occurrence information as well as the corpus-wide information to help the representation learning of texts. Experimental results on topical classification and sentiment analysis demonstrate that SPM can achieve new state-of-the-art performances on several benchmark datasets."
Analysis of Software Engineering for Agile Machine Learning Projects,"Singla, K; Bose, J; Naik, C",,2018,"The number of machine learning, artificial intelligence or data science related software engineering projects using Agile methodology is increasing. However, there are very few studies on how such projects work in practice. In this paper, we analyze project issues tracking data taken from Scrum (a popular tool for Agile) for several machine learning projects. We compare this data with corresponding data from non-machine learning projects, in an attempt to analyze how machine learning projects are executed differently from normal software engineering projects. On analysis, we find that machine learning project issues use different kinds of words to describe issues, have higher number of exploratory or research oriented tasks as compared to implementation tasks, and have a higher number of issues in the product backlog after each sprint, denoting that it is more difficult to estimate the duration of machine learning project related tasks in advance. After analyzing this data, we propose a few ways in which Agile machine learning projects can be better logged and executed, given their differences with normal software engineering projects."
Factors Influencing the Design of Unbounded Rule-Based Expert Architecture for Selection of Software Development Methodologies,"Vhutshilo, M; Kadyamatimba, A; Ochara, NM; Tutani, D",,2018,"The extent of success of a given project can be increased by using an appropriate Project Management Methodology (PMM) that takes into account the specific characteristics of the project (such as complexity, size, budget, nature of risk, etc.). PMMs have evolved over the years to become more diverse, complex, with evolving and dynamic ICT platforms. Such PMMs have traditionally been used as frameworks to guide the project management process for decision makers (such as Project Managers, Project Owners and Project Teams). Therefore, the choice facing such decision makers in selecting an appropriate project methodology is daunting; apart from other considerations related to project characteristics such as budget, scope, schedule, performance and resource constraints. One of the vital stages of a successful software development project is selecting a good software development methodology that best suits that project. The aim of this research is to investigate the critical factors considered by project managers. These critical factors are then used as a foundation for an architecture for an unbounded rule-based expert system. A survey was conducted amongst project managers to determine the critical factors necessary for the selection of a software development methodology. From the findings of the study, the critical factors revolved around three constructs of Project Excellence Enablers, Excellent Project Management Practices, and Business Value Proposition factors. These constructs formed the basis of an unbounded rule-based architecture anchored on artificial intelligence principles."
Supporting Analysts by Dynamic Extraction and Classification of Requirements-Related Knowledge,"Abad, ZSH; Gervasi, V; Zowghi, D; Far, BH",10.1109/ICSE.2019.00057,2019,"In many software development projects, analysts are required to deal with systems' requirements from unfamiliar domains. Familiarity with the domain is necessary in order to get full leverage from interaction with stakeholders and for extracting relevant information from the existing project documents. Accurate and timely extraction and classification of requirements knowledge support analysts in this challenging scenario. Our approach is to mine real-time interaction records and project documents for the relevant phrasal units about the requirements related topics being discussed during elicitation. We propose to use both generative and discriminating methods. To extract the relevant terms, we leverage the flexibility and power of Weighted Finite State Transducers (WFSTs) in dynamic modelling of natural language processing tasks. We used an extended version of Support Vector Machines (SVMs) with variable-sized feature vectors to efficiently and dynamically extract and classify requirements-related knowledge from the existing documents. To evaluate the performance of our approach intuitively and quantitatively, we used edit distance and precision/recall metrics. We show in three case studies that the snippets extracted by our method are intuitively relevant and reasonably accurate. Furthermore, we found that statistical and linguistic parameters such as smoothing methods, and words contiguity and order features can impact the performance of both extraction and classification tasks."
Ontology for knowledge condensation to support expertise location in the code phase during software development process,"MartÃ­nez-GarcÃ­a, JR; Castillo-Barrera, FE; Palacio, RR; Borrego, G; Cuevas-Tello, JC",10.1049/iet-sen.2019.0272,2020,"Software Development is a complex process, in which every software product is a knowledge representation of all the involved people. In agile software development, knowledge is prone to vaporise, because documentation is not a priority as indicated in the agile manifesto. This condition generates problems such as poor understanding of the requirements, knowledge transfer deficiency among developers, time wasted by developers while searching for knowledge. The objective of this work is to reducearchitectural knowledge vaporisationby means of knowledge condensation to supportexpertiselocation (high-level knowledge at a given time). This through an ontology that will condensate the knowledge in the code phase. This study presents the description of an ontology development process following the Methontology Framework. Results show that the proposed ontology does not present incongruence or inconsistency and answers the competency questions correctly. The main contribution of this study is the ontology which brings several benefits such as a shared concept of the knowledge in the code phase and a way to link the artefacts (resources used by developers in the project) and theexperts(artefacts provider)."
RETRACTED: Design of Multiobjective Dynamic Software Development Based on Data Mining Algorithm (Retracted Article),"Cai, ZH; Li, HB; Cui, WS",10.1155/2022/4444061,2022,"A data mining method finds hidden patterns in massive datasets for study. It is commonly used in high-tech fields such as image processing and artificial intelligence, due to its ability to compute data statistics and pattern processing problems efficiently. This study investigates data mining in multiobjective dynamic software development based on dynamic traffic congestion prediction. Since traffic data can fluctuate at any time, it is typically challenging to develop more accurate mathematical and theoretical models. We integrate data mining techniques into the software for predicting traffic congestion and develop a new algorithm for discriminating traffic congestion. Using a combination of the 3 criteria and the SVM algorithm, along with massive amounts of data, our prediction accuracy is significantly enhanced."
Automating Tiny ML Intelligent Sensors DevOPS Using Microsoft Azure,"Vuppalapati, C; Ilapakurti, A; Chillara, K; Kedari, S; Mamidi, V",10.1109/BigData50022.2020.9377755,2020,"Microsoft Azure DevOps is a robust,cross platform and powerful automation engine for script-based automation tools. Azure DevOPS enables to build, test, and deploy Cloud Native and/or Non-Cloud Native applications. The core principle and chief advantage that Azure DevOps provide are the availability of automation techniques such as infrastructure as code and the seamless integration of verifiable frameworks such as Machine Learning Operations (MLOps) with the DevOps automated pipelines to provision and configure the infrastructure that applications need to run. With the increase in application complexity and with the infusion of Machine Learning (ML) and Artificial Intelligence (AI) techniques as part of the software development lifecycle, the Azure DevOps is the most important framework that many organizations are rapidly progressing to incorporate it in their business processes to reduce the cost of building product and improve customer success. As part of the paper, we would like to propose a novel DevOps framework for building intelligent Tiny ML dairy agriculture sensors and the advantages that DevOps provide to develop high quality product in the most cost-efficient manner and serve small scale farmers who are at the bottom economic pyramid."
How deadline orientation and architectural modularity influence software quality and job satisfaction,"Kude, T; Foerderer, J; Mithas, S; Heinzl, A",10.1002/joom.1230,2023,"The implementation of digital transformation programs requires careful allocation of software developers to a variety of digital products and services with different levels of modularity. This paper investigates how deadline orientation (an individual-level preference of developers for completing work close to deadlines) and architectural modularity (a characteristic of products) influence central outcomes in software development. We argue that architectural modularity positively interacts with deadline orientation to influence software quality and the job satisfaction of developers. Our empirical analyses, using rare and high-quality data from 131 software developers and 29 product owners working at a captive software development center in India of a leading global software firm, confirm our hypotheses. We contribute to the literature on software development by showing that the fit between the technological characteristics of the software product (i.e., architectural modularity) and people factors (i.e., the temporal work style preferences of developers) plays an important role in shaping both software quality and job satisfaction. Our study has wider implications for the literature on software development, temporal work styles, and architectural modularity. It is instructive for practitioners tasked with hiring or allocating software developers for software products with varying technological characteristics in their digital transformation efforts."
Office Personal Assistant. Towards a Design and AI Approach,"Terroso, M; Sampaio, J; VilaÃ§a, J",10.1007/978-3-030-61671-7_7,2021,"This article aims to lay the foundations for the development of an office system that incorporates essential equipment to work in an office and that through a mobile application and a robotic system allows users to configure the space according to the needs, as well as the use of artificial intelligence in adapting the space to the physical, lighting and ergonomic characteristics of the workers. The article describes the problems inherent in the current workspace and the way this work is carried out, as well as some of the future trends for the configuration of space and equipment that need to be reformulated and adapted to the needs of the digital evolution present in the way we relate and perform the work now and in the future. It also describes the state of the art of solutions proposed by the main brands of office equipment, as well as automation solutions present in the literature. Based on this information, a working concept is presented for the development of an office personal assistant, equipped by a robotic brain that, through a mobile application, positions a series of equipment in space, such as a workstation, a pod, a wall system and lighting. The method used to develop the concept in its different phases is also described, as well as the activities inherent to each phase. The proposed method combines development techniques specific to product and interaction design, automation and software development, as well as techniques for extracting needs used by the social sciences."
Towards a Semantic Representation for Functional Software Requirements,"Sonbol, R; Rebdawi, G; Ghneim, N",10.1109/AIRE51212.2020.00007,2020,"Requirements are core elements in any software project. Therefore, understanding and representing the meaning of requirements play an essential role in automating any requirement engineering task. In this paper, we propose a semantic representation, called ReqVec, for functional software requirements. ReqVec is calculated based on three main phases: First, a set of lexical and syntactic steps are performed to analyze textual requirements. Then, semantic dimensions for requirements are calculated based on a words classifier and the well-known word embedding model Word2vec. Finally, ReqVec is constructed based on the representations of these dimensions. Two experiments have been conducted to evaluate how the proposed ReqVec can capture meaningful semantic information to solve two well-known Requirements Engineering tasks: detecting semantic relation between requirements, and requirements categorization. The proposed representation was efficient enough to detect related requirements with 0.92 F-measure and to categorize requirements with 0.88 F-measure."
Generating educational mobile applications using UIDPs identified by artificial intelligence techniques,"SÃ¡nchez-Morales, LN; Alor-HernÃ¡ndez, G; Rosales-Morales, VY; Cortes-Camarillo, CA; SÃ¡nchez-Cervantes, JL",10.1016/j.csi.2019.103407,2020,"Commercial mobile devices vary in brand, size, and functionalities, but they all allow people to interact with educational applications. In software engineering, application development techniques, approaches, methodologies, and processes (e.g., BBD, FDD, RAD, DDD) are often time consuming, costly, or aim at expert developers - which implies that users outside the software development field (e.g., teachers) need great practice to become experienced application developers. This work proposes an artificial-intelligence-based process for generating educational mobile apps from freehand-generated images. The images' design is based on User Interface Design Pattern (UIDP) representations. As a proof of concept, we introduce EduMatic, an application development tool. To test our process, we assessed Wiki-Comp, an application built with EduMatic, along with three other external educational applications: Khan Academy, Wiki encyclopedia, and Kiwix. According to the evaluation results, Wiki-Comp outstands in functionality, usability, and performance aspects."
Application and Analysis of Artificial Intelligence Graphic Element Algorithm in Digital Media Art Design,"Tian, H",10.1155/2022/6946616,2022,"With the development of information technology, digital media art came into being. Digital media art interface design, as a window for direct communication with users, has a direct impact on the user's experience, so it has attracted more and more people's attention. In the context of the continuous development of artificial intelligence, artificial intelligence algorithms have penetrated into all fields of social development. This article is based on the integration of graphic elements with artificial intelligence algorithms to build a digital media art interface design system. According to the analysis of system requirements, combined with the algorithm of artificial intelligence graphic elements, the system hierarchy is constructed from the user model, window model, and display model; software development and driver development are realized through the media library, and the window and common controls are realized. On this basis, the creation and display of windows and common controls are realized, and the design of the core functions of the system is completed to ensure the effect of system functions. The final conclusion shows that in actual design applications, the algorithm in this paper has a very high probability of finding the optimal solution, about 90%, when the number of iterations is small. Adjust parameters alpha and beta, when alpha=0.6 and beta=0.4, are the best parameters; the image reaches the highest PSNR value, and the image restoration quality is the best; after system testing, the running time of this system is shortened by an average of 1-2 seconds, and the target detection task accuracy rate is 100%; the system is evaluated by questionnaires. The overall satisfaction of the design interface remains at about 80%, which is a great improvement compared to the previous one. Finally, it satisfies the requirements of the digital media art interface design better, and it is worthy of further exploration."
Machine Learning and Deep Learning frameworks and libraries for large-scale data mining: a survey,"Nguyen, G; Dlugolinsky, S; BobÃ¡k, M; Tran, V; GarcÃ­a, AL; Heredia, I; Malik, P; Hluchy, L",10.1007/s10462-018-09679-z,2019,"The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, Machine Learning and especially its subfield Deep Learning have seen impressive advances. Techniques developed within these two fields are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software development in this field is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source communities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Artificial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efficiently in the era of Big Data."
Smart Tools in Software Engineering: A Systematic Mapping Study,"Savchenko, D; Kasurinen, J; Taipale, O",10.23919/mipro.2019.8756975,2019,"Software development processes such as waterfall development model have been around for over fifty years, but still, even modern software development approaches, such as DevOps or Test-driven development, fundamentally rely on the same principles and phases as everything before them. Yet, the modern world imposes new challenges for software businesses, and new ways of digital distribution require new ways of resource provisioning and ability to reduce the time-to-market to its absolute minimum. In this study, we identify and analyze the technologies which may be useful for software companies to ease the development and maintenance work by assisting the experts to collect relevant information and observe issues before they cause process disturbances. As a result, we describe a mapping study, which identifies different approaches to developing a smart software engineering tools applying potential technologies such as artificial intelligence, cloud-based service models, adaptive measurement, and other approaches, which could offer significant benefits to the software development process."
Discovering software developer's coding expertise through deep learning,"Javeed, F; Siddique, A; Munir, A; Shehzad, B; Lali, MIU",10.1049/iet-sen.2019.0290,2020,"The field of software development is growing rapidly and prevailing in every walk of life. The role of software developers in such a challenging and complex activity is very much important. The allocation of right software developers (i.e. who possesses appropriate coding skills) to projects is one of the crucial factors for successful software development. The problem is that it is very difficult for a client, project manager, as well as for software development organisations to find out an appropriate developer and assign him/her to a particular project. To achieve this, there is a need for such a sound mechanism that could detect the level of software developer coding expertise. This study has formulated criteria for novice and expert developers and carried out such criteria to discover the level of coding expertise of software developers using three different models of deep learning. These models include long short-term memory (LSTM), convolution 1D and hybrid (a combination of LSTM and convolution 1D). The deep learning models have analysed software developers' previously written source code collected from the GitHub repository. An experiment was conducted to evaluate the performance of models. The results showed that the LSTM model performed better in comparison to other models by achieving 96.25% accuracy."
Software Fault Prediction Using Data Mining Techniques on Software Metrics,"Kumar, R; Chaturvedi, A",10.1007/978-3-030-82469-3_27,2022,"Software industries have enormous demand for fault prediction of the faulty module and fault removal techniques. Many researchers have developed different fault prediction models to predict the fault at an early stage of the software development life cycle (SDLC). But the state-of-the-art model still suffers from the performance and generalize validation of the models. However, some researchers refer to data mining techniques, machine learning, and artificial intelligence play crucial roles in developing fault prediction models. A recent study stated that metric selection techniques also help to enhance the performance of models. Hence, to resolve the issue of improving the fault prediction model's performance and validation, we have used data mining, instance selection, metric selection, and ensemble methods to beat the state-of-the-art results. For the validation, we have collected the 22 software projects from the four different software repositories. We have implemented three machine learning algorithms and three ensemble methods with two metric selection methods on 22 datasets. The statistical evaluation of the implemented model performed using Wilcoxon signed-rank test and the Friedman test followed by the Nemenyi test to find the significant model. As a result, the Random forest algorithm produces the best result with an average median of 95.43% (accuracy) and 0.96 (f-measure) on 22 software projects. Based on the Nemenyi test, Random forest (RF) is performing better with 4.54 (accuracy mean score) and 4.41 (f-measure mean score) shown in the critical diagram. Experimental study shows that data mining techniques with PCA provide better accuracy and f-measure."
Automatic detection of Feature Envy and Data Class code smells using machine learning,"Skipina, M; Slivka, J; Luburic, N; Kovacevic, A",10.1016/j.eswa.2023.122855,2024,"Code smells in software indicate poor design and implementation choices. Detecting and removing them is critical for sustainable software development. Machine learning (ML) can automate code smell detection. Most ML solutions train models from scratch on code smell datasets, using handcrafted source code metrics as features. Pretrained language models, like BERT, fueled a paradigm shift in natural language processing: from handcrafted features to automatically inferred features and from training models from scratch to using pretrained models. Code embeddings offer the potential to bring a similar paradigm shift to code analysis. Nevertheless, the potential of using pretrained neural code embeddings for code smell detection has yet to be fully explored. To this end, we evaluated ML models trained using different code representations: code metrics and state-of-the-art neural code embeddings (CodeT5 and CuBERT). We experimented with CodeT5 variants (base and small) and explored multiple ways of embedding code snippets (by combining line-level embeddings or passing the entire code snippet as input). We tested our approaches on the tasks of detecting Data Class and Feature Envy on the MLCQ dataset. Considering the results of this study and our previous research, performance-wise, there is no clear winner between using code metrics or code embeddings for different code smell types and programming languages. However, given that, in contrast to code metrics, code embeddings can automatically adapt to new programming constructs and are expected to scale better with dataset size, these models are likely to become the future state-of-the-art feature generation technique for code smell detection."
Cognitive robotics software development aspects based on experiments of future software engineers,"Demeter, R; Kovari, A; Katona, J; Heldal, I; Costescu, C; Rosan, A; Thill, S; Stefanut, T",10.1109/coginfocom50765.2020.9237849,2020,"Machine learning algorithms are widely used in the domain of robotics. In particular, applications using machine learning and artificial intelligence algorithms have led to promising results in industrial applications, cognitive robotics and thus gained attention in recent years. In this context, the purpose of this article is to present the technologies and architectures used in the design and development of cognitive robots by students. This study highlights the difficulties encountered by future engineers in developing research projects in robotics."
Deep Learning-Based Code Auto-Completion for Distributed Applications,"Alizadehsani, Z; Pinto-Santos, F; Alonso-Moro, D; MacÃ­as, DB; GonzÃ¡lez-Briones, A",10.1007/978-3-031-20859-1_14,2023,"Distributed computing has been gaining a continually increasing interest over the past years in research and industrial communities. One of the significant objectives of distributed computing is to provide the infrastructure for performing tasks on independent systems. Utilizing this approach in software development can reduce costs. Consequently, there has been an increasing interest in distributed applications. However, distributed applications need to meet main features, such as scalability, availability, and compatibility. In this context, service-based systems provide an architecture that can support mentioned features. Nevertheless, current services use various technologies and languages, which bring complexity to development. This work aims to facilitate web service development by introducing a deep Learning-based code auto-complete model. This model is used in the toolkit called SmartCLIDE, which provides features to accelerate development using Artificial Intelligence and cloud deployment. The contribution of this work can fall into two steps: First, the top web APIs from a benchmark web service data-set has been identified. Afterward, a data optimization approach has been proposed to systematically augment and improve available web service codes. Second, the service code auto-completion model has been trained, which takes advantage of text generation trends and deep learning methods. The experimental results on web service codes demonstrate that the proposed approach outperforms another general-purpose code-completion model."
A Case Study: Comparison of Software Cost Estimation of Smart Shopping List Application,"Yigit, T; Coskun, H",10.1007/978-3-030-36178-5_50,2020,"The aim of this study is to evaluate development cost of smart shopping software which is going to be planned to developed using cost estimation methods of software development in software engineering. The smart shopping software aims that faster and easier preparing of shopping lists which is prepared by consumer with the aims of buying product from supermarket with lower cost. The most of consumers demand that a products price which they are going to buy or total amount of shopping price are lowest while shopping. Therefore, product prices of supermarkets need to be known. They try to find out which supermarket lowest price of products is in by researching of product price before shopping and they go shopping with the aim of lowest total cost of shopping. The smart shopping software planned to develop in this study aims to make consumers to shop with lower total cost within the help of artificial intelligence and applied mathematics. It is going to provided that to be found out supermarkets where optimal product prices are. The cost of best shopping route for shopping from these supermarkets is going to be determined. In this study, it has been made an estimate of development cost of smart shopping software. Therefore, the attributes of the software were firstly defined. These attributes are classified to determine complexity of them in following categories; interface design, database, controller classes, web API, artificial intelligence. So, the difficulties of performing these tasks have been obtained. After that, the tasks required to develop these attributes were determined within the scope of the functional points. In conclusion, the required workforce and time costs and project size were estimated by the methods using different software development cost estimation approaches and the results of the methods were evaluated."
DeepDev-PERF: A Deep Learning-Based Approach for Improving Software Performance,"Garg, S; Moghaddam, RZ; Clement, CB; Sundaresan, N; Wu, C",10.1145/3540250.3549096,2022,"Improving software performance is an important yet challenging part of the software development cycle. Today, the majority of performance inefficiencies are identified and patched by performance experts. Recent advancements in deep learning approaches and the wide-spread availability of open-source data creates a great opportunity to automate the identification and patching of performance problems. In this paper, we present DeepDev-PERF, a transformer-based approach to suggest performance improvements for C# applications. We pretrain DeepDev-PERF on English and Source code corpora, followed by finetuning for the task of generating performance improvement patches for C# applications. Our evaluation shows that our model can generate the same performance improvement suggestion as the developer fix in similar to 53% of the cases, getting similar to 34% of them verbatim in our expert-verified dataset of performance changes made by C# developers. Additionally, we evaluate DeepDev-PERF on 50 open-source C# repositories on GitHub using both benchmark and unit tests and find that our model is able to suggest valid performance improvements that can improve both CPU usage and Memory allocations. So far we've submitted 19 pull-requests with 28 different performance optimizations and 11 of these PRs have been approved by the project owners."
Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure,"Hutchinson, B; Smart, A; Hanna, A; Denton, E; Greer, C; Kjartansson, O; Barnes, P; Mitchell, M",10.1145/3442188.3445918,2021,"Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes."
Modelling and Simulation Framework for ATR Design Evaluation,"Hickman, DL; Koh, E; Lee, J; Yoo, H; Baek, J",10.1117/12.2632617,2022,"Automatic Target Recognition (ATR) and target tracking are fundamental functions in many military systems and so have a significant impact on a sensor system's performance. In response to the demand for increased capability, ATR designs have evolved from relatively simple filters to increasingly complex algorithms, using techniques such as artificial intelligence. Assessing the performance of image processing algorithms is a significant challenge, particularly with their increased design complexity. Increasing the complexity of a processing design tends to result in greater sensitivity to variations in scene conditions, rendering the system performance more nuanced with respect to the image content. There is a need to develop modelling and simulation design tools that better reflect the impact of image processing on the overall system performance when subjected to a wide variation in input scene and sensor platform characteristics. In this paper, the development and design of the System Performance Model (SPM) is described which provides the modelling and simulation of different image processing algorithms such as ATR. The approach taken within the SPM is to use real imagery and convert this to imagery that would be generated by the modelled camera. This process, which is described in the paper, is critical to the design of the SPM and underpins its effectiveness and accuracy. Example results are given that illustrate the design of the SPM's image conversion process."
ArduCode: Predictive Framework for Automation Engineering,"Canedo, A; Goyal, P; Huang, D; Pandey, A; Quiros, G",10.1109/TASE.2020.3008055,2021,"Automation engineering is the task of integrating, via software, various sensors, actuators, and controls to automate a real-world process. Today, automation engineering is supported by a suite of software tools, including integrated development environments (IDEs), hardware configurators, compilers, and runtimes. These tools focus on the automation code itself but leave the automation engineer unassisted in their decision-making. This can lead to longer software development cycles due to the imperfections in the decision-making, which arise when integrating software and hardware. To address this problem, this article addresses multiple challenges often faced in automation engineering and proposes machine learning-based solutions to assist engineers tackle these challenges. We show that machine learning can be leveraged to assist the automation engineer in classifying automation code, finding similar code snippets, and reasoning about the hardware selection of sensors and actuators. We validate our architecture on two real data sets consisting of 2927 Arduino projects and 683 programmable logic controller (PLC) projects. Our results show that paragraph embedding techniques can be utilized to classify automation using code snippets with precision close to human annotation, giving an F-1-score of 72%. Furthermore, we show that such embedding techniques can help us find similar code snippets with high accuracy. Finally, we use autoencoder models for hardware recommendation and achieve a p@3 of 0.79 and P@5 of 0.95. We also present the implementation of ArduCode in a proof-of-concept user interface integrated into an existing automation engineering system platform. Note to Practitioners-This article is motivated by the use of artificial intelligence methods to improve the efficiency and quality of the automation engineering software development process. Our goal is to develop and integrate intelligent assistants in existing automation engineering development tools to minimally disrupt existing workflows. Practitioners should be able to adapt our framework to other tools and data. Our contributions address important practical problems: 1) we address the lack of realistic data sets in automation engineering with two publicly available data sources; 2) we make the reference implementation of our algorithms publicly available on GitHub for other practitioners to have a starting point for future research; and 3) we demonstrate the integration of our framework as an add-on to an existing automation engineering toolchain."
Sequential Recommendations on GitHub Repository,"Kim, J; Wi, J; Kim, Y",10.3390/app11041585,2021,"The software development platform is an increasingly expanding industry. It is growing steadily due to the active research and sharing of artificial intelligence and deep learning. Further, predicting users' propensity in this huge community and recommending a new repository is beneficial for researchers and users. Despite this, only a few researches have been done on the recommendation system of such platforms. In this study, we propose a method to model extensive user data of an online community with a deep learning-based recommendation system. This study shows that a new repository can be effectively recommended based on the accumulated big data from the user. Moreover, this study is the first study of the sequential recommendation system that provides a new dataset of a software development platform, which is as large as the prevailing datasets. The experiments show that the proposed dataset can be practiced in various recommendation tasks."
Construction of Software Supply Chain Threat Portrait Based on Chain Perspective,"Wang, MY; Wu, P; Luo, Q",10.3390/math11234856,2023,"With the rapid growth of the software industry, the software supply chain (SSC) has become the most intricate system in the complete software life cycle, and the security threat situation is becoming increasingly severe. For the description of the SSC, the relevant research mainly focuses on the perspective of developers, lacking a comprehensive understanding of the SSC. This paper proposes a chain portrait framework of the SSC based on a resource perspective, which comprehensively depicts the threat model and threat surface indicator system of the SSC. The portrait model includes an SSC threat model and an SSC threat indicator matrix. The threat model has 3 levels and 32 dimensions and is based on a generative artificial intelligence model. The threat indicator matrix is constructed using the Attack Net model comprising 14-dimensional attack strategies and 113-dimensional attack techniques. The proposed portrait model's effectiveness is verified through existing SSC security events, domain experts, and event visualization based on security analysis models."
AN APPROACH TO EXTEND THE DIGITAL THREAD FROM REQUIREMENTS TO MODEL GEOMETRY,"Zhou, JW; Camba, JD; Hartman, N; Li, ZT",,2022,"As organizations embrace Industry 4.0 and its corresponding digital transformation, new technologies and practices are enabling more resilient, integrated, and sustainable approaches to product development. Researchers have explored the information flows and data relationships between requirements management (RQM) practices and Computer-Aided Design (CAD) to improve New Product Development (NPD) processes. Similarly, Life Cycle Assessment (LCA) tools can be used to assess the environmental impact of a product at the early stages of development. In this paper, we propose a novel approach to integrate RQM, CAD, and LCA in the NPD process in a manner that extends the digital thread of information from the definition of design requirements to the geometry of the digital product model. Specifically, we demonstrate the seeding of mechanical design models directly from design requirements as a starting point for parametrization, the linking of data items to facilitate subsequent design changes involving geometry, and the use of data connections between requirements and 3D models for continuous design verification. Our approach is supported by a Product Lifecycle Management (PLM) system and involves a workflow with several stages and various inputs from stakeholders. We validate our approach through the implementation of a case study involving a mechanical assembly and a commercial PLM system."
Big Data analytics in Agile software development: A systematic mapping study,"Biesialska, K; Franch, X; MuntÃ©s-Mulero, V",10.1016/j.infsof.2020.106448,2021,"Context: Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity. Objective: Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA). Method: As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019. Results: In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics. Conclusions: As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives."
Translating Code Comments to Procedure Specifications,"Blasi, A; Goffi, A; Kuznetsov, K; Gorla, A; Ernst, MD; PezzÃ¨, M; Castellanos, SD",10.1145/3213846.3213872,2018,"Procedure specifications are useful in many software development tasks. As one example, in automatic test case generation they can guide testing, act as test oracles able to reveal bugs, and identify illegal inputs. Whereas formal specifications are seldom available in practice, it is standard practice for developers to document their code with semi-structured comments. These comments express the procedure specification with a mix of predefined tags and natural language. This paper presents Jdoctor, an approach that combines pattern, lexical, and semantic matching to translate Javadoc comments into executable procedure specifications written as Java expressions. In an empirical evaluation, Jdoctor achieved precision of 92% and recall of 83% in translating Javadoc into procedure specifications. We also supplied the Jdoctor-derived specifications to an automated test case generation tool, Randoop. The specifications enabled Randoop to generate test cases that produce fewer false alarms and reveal more defects."
TSAI - Test Selection using Artificial Intelligence for the Support of Continuous Integration,"Meyer, MLB",10.1109/ISSREW53611.2021.00092,2021,"The agile methodology has been increasingly deployed in the industry world, breaking the process into cycles of planning, executing, and evaluating. In the software development domain, an agile method named continuous integration is widely used to automatically integrate code changes from different developers into the same software. Then, each new build can be tested to make sure that the modifications did not interfere with the rest of the already verified code. Despite being very important, regression tests are usually the costliest part of a project. It is laborious to retest all tests of each new software version due to the time it takes to perform and often, before all tests are finished, a new software version is ready to be tested. To improve regression tests results, a selection can be done. By selecting the right tests at the right moment, the use of all test catalogs can be avoided to find faults in the software tested. The aim of this work is to develop a method to select tests to be executed for each version using artificial intelligence algorithms. Learning algorithms can find patterns and similarities between test cases to help knowing which one has a higher probability to expose a fault."
DevOps for AI - Challenges in Development of AI-enabled Applications,"Lwakatare, LE; Crnkovic, I; Bosch, J",,2020,"When developing software systems that contain Machine Learning (ML) based components, the development process become significantly more complex. The central part of the ML process is training iterations to find the best possible prediction model. Modern software development processes, such as DevOps, have widely been adopted and typically emphasise frequent development iterations and continuous delivery of software changes. Despite the ability of modern approaches in solving some of the problems faced when building ML-based software systems, there are no established procedures on how to combine them with processes in ML workflow in practice today. This paper points out the challenges in development of complex systems that include ML components, and discuss possible solutions driven by the combination of DevOps and ML workflow processes. Industrial cases are presented to illustrate these challenges and the possible solutions."
Improving Code Summarization with Block-wise Abstract Syntax Tree Splitting,"Lin, C; Ouyang, ZC; Zhuang, JQ; Chen, JQ; Li, H; Wu, RX",10.1109/ICPC52881.2021.00026,2021,"Automatic code summarization frees software developers from the heavy burden of manual commenting and benefits software development and maintenance. Abstract Syntax Tree (AST), which depicts the source code's syntactic structure, has been incorporated to guide the generation of code summaries. However, existing AST based methods suffer from the difficulty of training and generate inadequate code summaries. In this paper, we present the Block-wise Abstract Syntax Tree Splitting method (BASTS for short), which fully utilizes the rich tree-form syntax structure in ASTs, for improving code summarization. BASTS splits the code of a method based on the blocks in the dominator tree of the Control Flow Graph, and generates a split AST for each code split. Each split AST is then modeled by a Tree-LSTM using a pre-training strategy to capture local non-linear syntax encoding. The learned syntax encoding is combined with code encoding, and fed into Transformer to generate high-quality code summaries. Comprehensive experiments on benchmarks have demonstrated that BASTS significantly outperforms state-of-the-art approaches in terms of various evaluation metrics. To facilitate reproducibility, our implementation is available at https://github.com/XMUDM/BASTS."
Towards Integrating Data-Driven Requirements Engineering into the Software Development Process: A Vision Paper,"Franch, X; Seyff, N; Oriol, M; Fricker, S; Groher, I; Vierhauser, M; Wimmer, M",10.1007/978-3-030-44429-7_10,2020,"[Context and motivation] Modern software engineering processes have shifted from traditional upfront requirements engineering (RE) to a more continuous way of conducting RE, particularly including data-driven approaches. [Question/problem] However, current research on data-driven RE focuses more on leveraging certain techniques such as natural language processing or machine learning than on making the concept fit for facilitating its use in the entire software development process. [Principal ideas/results] In this paper, we propose a research agenda composed of six distinct research directions. These include a data-driven RE infrastructure, embracing data heterogeneity, context-aware adaptation, data analysis and decision support, privacy and confidentiality, and finally process integration. Each of these directions addresses challenges that impede the broader use of data-driven RE. [Contribution] For researchers, our research agenda provides topics relevant to investigate. For practitioners, overcoming the underlying challenges with the help of the proposed research will allow to adopt a data-driven RE approach and facilitate its seamless integration into modern software engineering. For users, the proposed research will enable the transparency, control, and security needed to trust software systems and software providers."
Understanding the Automated Parameter Optimization on Transfer Learning for Cross-Project Defect Prediction: An Empirical Study,"Li, K; Xiang, ZL; Chen, T; Wang, S; Tan, KC",10.1145/3377811.3380360,2020,"Data-driven defect prediction has become increasingly important in software engineering process. Since it is not uncommon that data from a software project is insufficient for training a reliable defect prediction model, transfer learning that borrows data/konwledge from other projects to facilitate the model building at the current project, namely cross-project defect prediction (CPDP), is naturally plausible. Most CPDP techniques involve two major steps, i.e., transfer learning and classification, each of which has at least one parameter to be tuned to achieve their optimal performance. This practice fits well with the purpose of automated parameter optimization. However, there is a lack of thorough understanding about what are the impacts of automated parameter optimization on various CPDP techniques. In this paper, we present the first empirical study that looks into such impacts on 62 CPDP techniques, 13 of which are chosen from the existing CPDP literature while the other 49 ones have not been explored before. We build defect prediction models over 20 real-world software projects that are of different scales and characteristics. Our findings demonstrate that: (1) Automated parameter optimization substantially improves the defect prediction performance of 77% CPDP techniques with a manageable computational cost. Thus more efforts on this aspect are required in future CPDP studies. (2) Transfer learning is of ultimate importance in CPDP. Given a tight computational budget, it is more cost-effective to focus on optimizing the parameter configuration of transfer learning algorithms (3) The research on CPDP is far from mature where it is 'not difficult' to find a better alternative by making a combination of existing transfer learning and classification techniques. This finding provides important insights about the future design of CPDP techniques."
SPECMATE: Automated Creation of Test Cases from Acceptance Criteria,"Fischbach, J; Vogelsang, A; Spies, D; Wehrle, A; Junker, M; Freudenstein, D",10.1109/ICST46399.2020.00040,2020,"In the agile domain, test cases are derived from acceptance criteria to verify the expected system behavior. How-ever, the design of test cases is laborious and has to be done manually due to missing tool support. Existing approaches for automatically deriving tests require semi-formal or even formal notations of acceptance criteria, though informal descriptions are mostly employed in practice. In this paper, we make three contributions: (1) a case study of 961 user stories providing an insight into how user stories are formulated and used in practice, (2) an approach for the automatic extraction of test cases from informal acceptance criteria and (3) a study demonstrating the feasibility of our approach in cooperation with our industry partner. In our study, out of 604 manually created test cases, 56 % can be generated automatically and missing negative test cases are added."
A Transfer Learning-Based Multivariate Control Chart for Dengue Surveillance in Hong Kong,"Wang, ZZ; Zwetsloot, IM",10.1109/ACCESS.2023.3290324,2023,"Dengue is a severe mosquito-borne epidemic disease. There is no effective vaccine for dengue, so a real-time surveillance system becomes crucial to detect dengue outbreaks. Control charts have been widely used as efficient tools to identify changes in health-related data. In Hong Kong, the environmental protection department uses the area ovitrap index to survey monthly the number of mosquitoes in different areas. Some areas have limited historic area ovitrap index records since the survey started only recently. Parameter estimation for designing control charts is challenging with little historic data. This paper proposes a transfer learning-based estimator to increase parameter estimation accuracy for areas with limited historic data points. We study the questions on what and how to transfer useful knowledge from related source data. A multivariate control chart based on transfer-learned parameters is developed for online monitoring. A real example of dengue surveillance demonstrates its effectiveness in application."
A Model to Helping the Construction of Creative Service-Based Software,"Huang, PS; Fahmi, F; Wang, FJ",10.1109/COMPSAC51774.2021.00171,2021,"With the advent of the Service Oriented Architecture (SOA) in system design, various domain knowledges are included in a service-based application, such as the design of Artificial Intelligence (AI) or augmented reality (AR) systems. While merging one or multiple domains into computation systems, the computation systems can be widely applied in various domain usages with novelty, useful, and surprising properties, which are defined as systems of creative computing. In creative computing, several theoretical evaluation metrics and verification approaches have been proposed for system design in several domains. However, a solid practical design environment for creative service-based systems is rarely considered in current researches. In this paper, we propose a model for creative service software development based on semantic web, which is applied in two phases: (1) requirement specification and (2) service design. In order to bridge the knowledge gap between domain experts and software engineers, and provide a machine-readable format for creative computing, two sub-models, Requirement Specification and Service Structure Models, are constructed in both phases, sequentially. After the latter sub-model is validated, the creative service software is well-constructed based on the services definition and composition represented by the model."
"The adoption of Design Thinking, Agile Software Development and Co-creation concepts A case study of Digital Banking innovation","Indriasari, E; Prabowo, H; Gaol, FL; Purwandari, B",10.1109/PlatCon53246.2021.9680763,2021,"Acceleration of technology, especially the mobile internet, causes changes all aspects of human life, including in the banking sector. New emerging technology such as Artificial Intelligence, Blockchain, Big Data, and Cloud computing change the business and operation of the bank. The bank's services have become more personalized, furthermore change customers' lifestyles. Banks are competing to create innovations and breakthroughs to create added value and building a digital ecosystem with fintech and big tech companies in the era of sharing economy. This case study explores the process of creating digital innovation in banking institutions by focusing on adopting design thinking (DT), agile software development (ASD), and co-creation concepts for building digital banking platforms. The case study involved IT executives from four banks in Indonesia. Data were taken through semi-structured interviews and analyzed using NVIVO12. The implication of this research is to accelerate the process of digital banking innovation and produce high-quality digital banking platforms in terms of features and technology."
Comparing Stacking Ensemble and Deep Learning for Software Project Effort Estimation,"Hoc, HT; Silhavy, R; Prokopova, Z; Silhavy, P",10.1109/ACCESS.2023.3286372,2023,"This study focuses on improving the accuracy of effort estimation by employing ensemble, deep learning, and transfer learning techniques. An ensemble approach is utilized, incorporating XGBoost, Random Forest, and Histogram Gradient Boost as generators to enhance predictive capabilities. The performance of the ensemble method is compared against both the deep learning approach and the PFA-IFPUG technique. Statistical criteria including MAE, SA, MMRE, PRED(0.25), MBRE, MIBRE, and relevant information related to MMRE and PRED(0.25) are employed for evaluation. The results demonstrate that combining regression models with Random Forest as the final regressor and XGBoost and Histogram Gradient Boost as prior generators yields more accurate effort estimation than other combinations. Furthermore, the findings highlight the potential of transfer learning in the deep learning method, which exhibits superior performance over the ensemble approach. This approach leverages pre-trained models and continuously improves performance by training on new datasets, providing valuable insights for cross-company and cross-time effort estimation problems. The ISBSG dataset is used to build the pre-trained model, and the inductive transfer learning approach is verified based on the Desharnais, Albrecht, Kitchenham, and China datasets. The study underscores the significance of transfer learning and the integration of domain-specific knowledge from existing models to enhance the performance of new models, thereby improving accuracy, reducing errors, and enhancing predictive capabilities in effort estimation."
RETRACTED: Construction of Intelligent Textbook Courseware Management System Based on Artificial Intelligence Technology (Retracted Article),"Zhao, Q",10.1155/2022/9993183,2022,"On the one hand, the lack of specialized knowledge and knowledge of software development is serious. Second, the development process takes a lot of time, and the development cost is high. Shortening the development time, reducing the development cost, and reducing the dependence on artificial intelligence can be realized by developing an intelligent courseware writing system. The research of intelligent teaching system based on Web and multiagent technology in this paper will surely promote the continuous development and progress of intelligent teaching and related disciplines, which has profound theoretical and practical significance. The design of the model may need to be emphasized and improved in the understanding and reform of classroom management concepts. The comprehensive improvement has an increase of 86.3%, which is of great significance for courseware management. In terms of courseware management, due to the model design, it is also well reflected in personalization, and the improvement of management is 74.6%."
Research and Application of Machine Learning in Automatic Program GenerationInspec keywordsOther keywordsKey words,"Zhang, XJ; Jiang, Y",10.1049/cje.2020.10.006,2020,"With the development of artificial intelligence, machine learning has been applied in more and more domains. In order to improve the quality and efficiency of software, automatic program generation is becoming a research hotspot. In recent years, machine learning has also been gradually applied in automatic program generation. Decision trees, language models, and cyclic neural networks have been applied in code generation, code completion and code knowledge mining. The efficiency of software development has been improved to a certain extent using machine learning. Aimed at the automatic program generation, this paper analyzes and summarizes the models of machine learning, the modifications involved in the models and the application effects. The research direction is discussed from the aspects of programmer behavior and automatic program generation of machine learning."
An Artificial Intelligence Radio Propagation Model Based on Geographical Information,"Zhang, H; Dong, JB; Liu, XX; Liu, JF; Zhang, XC",10.1109/TAP.2022.3215818,2022,"In this article, a new wireless network planning propagation model is proposed, the datasets are continuous wave data from different cities, and the model is constructed based on the artificial neural network in machine learning and uses geographical information to do feature engineering. Six geographical information features are used as the input feature vectors of the model. The terrain type near the receiving point is a corrective factor affecting the propagation field intensity. To make the physical implications more explicit, two multilayer perceptron networks were designed, and the one-hot encoding of terrain types of five grids near the receiving point on the line between the receiving and transmitting points is taken as one of the input features of the second network. Through the model training and validation, and tested in actual scene, the model on the datasets of different cities has achieved good results. Compared with the traditional propagation model, such as the standard propagation model (SPM), this artificial intelligence model is more suitable for correction of data containing random disturbance and has a better simulation accuracy; the random disturbance includes external random interference sources, global positioning system offsets, and inaccurate maps."
CellS: A Cell-Inspired Efficient Software Framework for AI-Enabled Application on Resources-Constrained Mobile System,"Chen, CH; Wu, MC",10.3390/electronics10050568,2021,"Today's mobile processors generally have multiple cores and sufficient hardware resources to support AI-enabled software operation. However, very few AI applications make full use of the computing performance of mobile multiprocessors. This is because the typical software development is sequential, and the degree of parallelism of the program is very low. In the increasingly complex AI-driven and software development projects with natural human-computer interaction, this will undoubtedly cause a waste of mobile computing resources that are originally limited. This paper proposes an intelligent system software framework, CellS, to improve smart software development on multicore mobile processor systems. This software framework mimics the cell system. In this framework, each cell can autonomously aware changes in the environment (input) and reaction (output) and may change the behavior of other cells. Smart software can be regarded as a large number of cells interacting with each other. Software developed based on the CellS framework has a high degree of scalability and flexibility and can more fully use multicore computing resources to achieve higher computing efficiency."
ONNC-based Software Development Platform for Configurable NVDLA Designs,"Lin, WF; Hsieh, CT; Chou, CY",10.1109/vlsi-dat.2019.8741778,2019,"With the proliferation of deep learning and the increasing pressure to deploy inference applications at the edge, many AI chip makers integrate the open source NVIDIA Deep Learning Accelerator (NVDLA) design in their AI solutions. Lack of open source compiler support and having only limited configurability support in the software stacks erect a barrier for developers to freely explore the NVDLA design space at system level. This paper presents an ONNC-based software development platform that includes the first open source compiler for NVDLA-based designs, a virtual platform with various CPU models as well as configurable NVDLA models, and auxiliary tools for debugging. The platform is tightly coupled with the hardware design tradeoffs and provides extendibility for compiler optimization, more CPU types, and more NVDLA hardware configurations. It lifts many restrictions of software development for those who like to leverage the NVDLA design in inference applications."
Drivers of Automation and Consequences for Jobs in Engineering Services: An Agent-Based Modelling Approach,"NordÃ¥s, HK; KlÃ¼gl, F",10.3389/frobt.2021.637125,2021,"New technology is of little use if it is not adopted, and surveys show that less than 10% of firms use Artificial Intelligence. This paper studies the uptake of AI-driven automation and its impact on employment, using a dynamic agent-based model (ABM). It simulates the adoption of automation software as well as job destruction and job creation in its wake. There are two types of agents: manufacturing firms and engineering services firms. The agents choose between two business models: consulting or automated software. From the engineering firms' point of view, the model exhibits static economies of scale in the software model and dynamic (learning by doing) economies of scale in the consultancy model. From the manufacturing firms' point of view, switching to the software model requires restructuring of production and there are network effects in switching. The ABM matches engineering and manufacturing agents and derives employment of engineers and the tasks they perform, i.e. consultancy, software development, software maintenance, or employment in manufacturing. We find that the uptake of software is gradual; slow in the first few years and then accelerates. Software is fully adopted after about 18 years in the base line run. Employment of engineers shifts from consultancy to software development and to new jobs in manufacturing. Spells of unemployment may occur if skilled jobs creation in manufacturing is slow. Finally, the model generates boom and bust cycles in the software sector."
SPM: Sparse Persistent Memory Attention-Based Model for Network Traffic Prediction,"Ma, XS; Jiang, GH; Zheng, B",10.3390/sym14112319,2022,"The network traffic prediction (NTP) model can help operators predict, adjust, and control network usage more accurately. Meanwhile, it also reduces network congestion and improves the quality of the user service experience. However, the characteristics of network traffic data are quite complex. NTP models with higher prediction accuracy tend to have higher complexity, which shows obvious asymmetry. In this work, we target the conflict between low complexity and high prediction performance and propose an NTP model based on a sparse persistent memory (SPM) attention mechanism. SPM can accurately capture the sparse key features of network traffic and reduce the complexity of the self-attention layer while ensuring prediction performance. The symmetric SPM encoder and decoder replace the high complexity feed-forward sub-layer with an attention layer to reduce the complexity. In addition, by adding an attention layer to persistently memorize key features, the prediction performance of the model could be further improved. We evaluate our method on two real-world network traffic datasets. The results demonstrate that the SPM-based method outperforms the state-of-the-art (SOTA) approaches in NTP results by 33.0% and 21.3%, respectively. Meanwhile, the results of RMSE and R-2 are also optimal. When measured by temporal performance, SPM reduces the complexity and reduces the training time by 22.2% and 30.4%, respectively, over Transformer."
Detecting Resource Release Bugs with Analogical Reasoning,"Liang, WT; Wang, L; She, JL; Liu, YQ",10.1155/2022/3518673,2022,"The resource release bugs are a common type of serious programming bug. However, it is hard to catch them by using static detection for the lacking of comprehensive prior knowledge about the release functions. In this paper, a resource release bug detection method is proposed by introducing analogical reasoning on word vectors. First, the functions of the target source code are encoded into word vectors by the word embedding technique in natural language processing. Second, a two-stage reasoning method is developed for automatically identifying unknown resource release functions according to a few well-known seed functions. 3CosAvg algorithm is employed for the first stage, and a new algorithm is designed for the latter, called 3CosAddExchange. Finally, the identified release functions are translated into static analysis rules to detect potential bugs. The experiment shows that the proposed method is effective and efficient for the large-scale software project. Five unknown resource release bugs are successfully detected in the Linux kernel and confirmed by kernel developers."
Towards Requirement Change Management for Global Software Development using Case Base Reasoning,"Ali, S; Iqbal, N; Hafeez, Y",10.22581/muet1982.1803.17,2018,"Software development globally an important business nowadays and also there are different issues related to software development process especially in change management process when there is change in requirement due to lack of communication, coordination and expert knowledge not properly manage. The aim of this paper is to provide and proposed framework to manage RCM (Requirement Change Management) using CBR (Cased Based Reasoning) technique effectively and intelligently. CBR to solve change in requirement on the base of previous knowledge and experience evaluated the proposed framework through experimental study. The experimental study depicted that proposed framework improved requirement change management in GSD (Globally Software Development) instead of other methods for change management of requirements in globally dispersed environment effectively."
Multicore processors and GPUs: the power of parallel computing in the Cloud,"Bennett, KW; Robertson, J",10.1117/12.2558600,2020,"Sensors used in intelligence, surveillance and reconnaissance (ISR) operations and activities have the ability to generate vast amounts of data. High-volume analytical capabilities are needed to process data from multi-modal sensors to develop and test complex computational and deep learning models in support of the U.S. Army Multi-Domain Operations (MDO). The Army Research Laboratory designs, develops and tests Artificial Intelligence and Machine Learning (AI/ML) algorithms employing large repositories of in-house data. To efficiently process the data as well as design, build, train and deploy models, parallel and distributed algorithms are needed. Deep learning frameworks provide language-specific, container-based building blocks associated with deep learning neural networks applied to specific target applications. This paper discusses applications of AI/ML deep learning frameworks and Software Development Kits (SDKs) and demonstrates and compares specific multi-core processor and NVidia Graphics Processing Unit (GPU) implementations for desktop and Cloud environments. Frameworks selected for this research include PyTorch and Matlab. Amazon Web Services (AWS) SageMaker was used to launch Machine Learning instances ranging from general purpose computing to GPU instances. Detailed processes, example code, performance enhancements, best practices and lessons learned are included for publicly available acoustic and image datasets. Research results indicate parallel implementations of data preprocessing steps saved significant time but more expensive GPUs did not provide any processing time advantages for the machine learning algorithms tested."
Software Development for Agricultural Tillage Robot Based on Technologies of Machine Intelligence,"Panarin, RN; Khvorova, LA",10.1007/978-3-030-94141-3_28,2022,"The article is devoted to the development of software for robots designed for spot mechanical tillage. The need to develop software for the digital twin of the agro-robot with the use of artificial intelligence technologies is dictated by the need of farmers in its practical use. The article describes four high-level nodes of an agricultural robot: the control unit, which is an NVIDIA Jetson NANO computing module; the executive mechanism, which is a 6-axis desktop robotic arm; the machine vision unit, consisting of an Intel RealSense camera; the chassis unit, represented as crawler tracks and drivers for their control. The implementation of the software is carried out independently of the manufacture of the robot, so for the developer there is a task to minimize the risk of its implementation in the manufactured robot. The developed software fully meets the requirements imposed by the customer. For instance, the digital robot twin takes into account the environmental conditions, as well as the terrain in which the prototype robot will work, and then the serial device. Second, the use of ROS (Robot Operating System) in software development will allowone with minimal effort to transfer the digital model to the physical one (prototype and serial robot), without changing the source code. Third, taking into account the physical environment conditions when programming the digital robot twin allowed one to build mathematical models of device control that are close to reality, as well as to debug and test them."
Automatic Detection of Architectural Bad Smells through Semantic Representation of Code,"Pigazzini, I",10.1145/3344948.3344951,2019,"Bad design decisions in software development can progressively affect the internal quality of a software system, causing architecture erosion. Such bad decisions are called Architectural Smells (AS) and should be detected as soon as possible, because their presence heavily hinders the maintainability and evolvability of the software. Many detection approaches rely on software analysis techniques which inspect the structure of the system under analysis and check with rules the presence of AS. However, some recent approaches leverage natural language processing techniques to recover semantic information from the system. This kind of information is useful to detect AS which violate conceptual design principles, such as the separation of concerns one. In this research study, I propose two detection strategies for AS detection based on code2vec, a neural model which is able to predict semantic properties of given snippets of code."
"Iteration Causes, Impact, and Timing in Software Development Lifecycle: An SLR","MUMTAZ, M; AHMAD, N; ASHRAF, MU; ALGHAMDI, AM; BAHADDAD, AA; ALMARHABI, KA",10.1109/ACCESS.2022.3182703,2022,"Context: Iteration-performing an activity once it has already been done-is unavoidable and omnipresent during software development. Management of iteration is a challenging task due to the lack of detailed analysis and use of different terms for the iteration at different places in software engineering. Objective: In order to manage iteration in a better way during software development processes, we investigate different iterative situations, the causes, the stages at which it can occur during the development, and its impacts. Method: We use the systematic literature review (SLR) method to search using six bibliographic databases. The SLR includes 153 primary studies published from 2007 to February 2017. Results: We identify twenty-two leading causes, five stages, and positive and negative impacts of iteration. Then, we develop a lucid taxonomy of iteration perspectives based on the causes and timing at which it occurs during the software development lifecycle. Conclusions: The frequently reported causes of iteration are defects, code smell, and conflicts, whereas the least referenced causes are poor management and different methods followed by teams. The most cited phase at which iteration occurs during the software development is maintenance. The most cited positive consequence of iteration is quality improvement, whereas the negative impacts of iteration are increasing time, effort, and cost. Our study provides a framework to understand the nature of iteration, what sources can lead to which iterative perspective, a particular iterative situation can have what kind of impacts on project milestones, and also provide research directions."
AlLiveSim: An Extensible Virtual Environment for Training Autonomous Vehicles,"Leudet, J; Christophe, F; Mikkonen, T; MÃ¤nnistÃ¶, T",10.1109/COMPSAC.2019.00074,2019,"Virtualization technologies have become commonplace both in software development as well as engineering in a more general sense. Using virtualization offers other benefits than simulation and testing as a virtual environment can often be more liberally configured than the corresponding physical environment. This, in turn, introduces new possibilities for education and training, including both for humans and artificial intelligence (AI). To this end, we are developing a simulation platform AlLiveSim. The platform is built on top of the Unreal Engine(1) game development system, and it is dedicated to training and testing autonomous systems, their sensors and their algorithms in a simulated environment. In this paper, we describe the elements that we have built on top of the engine to realize a Virtual Environment (VE) useful for the design, implementation, application and analysis of autonomous systems. We present the architecture that we have put in place to transform our simulation platform from automotive specific to be domain agnostic and support two new domains of applications: autonomous ships and autonomous mining machines. We describe the important specificity of each domain in regard to simulation. In addition, we also report the challenges encountered when simulating those applications, and the decisions taken to overcome these challenges."
Negative Transfer in Cross Project Defect Prediction: Effect of Domain Divergence,"Omondiagbe, OP; Licorish, SA; MacDonell, SG",10.1109/SEAA56994.2022.00010,2022,"Cross-project defect prediction (CPDP) models are used in new software project prediction tasks to improve defect prediction rates. The development of these CPDP models could be challenging in cases where there is little or no historical data. For this reason, researchers may need to rely on multiple sources and use transfer learning-based CPDP for building defect prediction models. These data are typically taken from similar and related projects, but their distributions can be different from the new software project (target data). Although, transfer learning-based CPDP models are designed to handle these distribution differences, but if not correctly handled by the model, may lead to negative transfer. To this end, recent works have focused on building transfer CPDP models, but little is known about how similar or dissimilar sources should be to avoid negative transfer. This paper provides the first empirical investigation to understand the effect of combining different sources with different levels of similarities in transfer CPDP. We introduce the use of the Population Stability Index (PSI) to interpret whether the distribution of the combined or single-source data is similar to the target data. This was validated using an adversarial approach. Experimental results on three public datasets reveal that when the source and target distribution are very similar, the probability of false alarm is improved by 3% to 7% and the recall indicator is reduced from 1% to 8%. Interestingly, we also found that when dissimilar source data are combined with different source datasets, the overall domain divergence is lowered, and the performance is improved. The results highlight the importance of using the right source to aid the learning process."
ISPY: Automatic Issue-Solution Pair Extraction from Community Live Chats,"Shi, L; Jiang, ZY; Yang, Y; Chen, X; Zhang, YM; Mu, FW; Jiang, HZ; Wang, Q",10.1109/ASE51524.2021.9678894,2021,"Collaborative live chats are gaining popularity as a development communication tool. In community live chatting, developers are likely to post issues they encountered (e.g., setup issues and compile issues), and other developers respond with possible solutions. Therefore, community live chats contain rich sets of information for reported issues and their corresponding solutions, which can be quite useful for knowledge sharing and future reuse if extracted and restored in time. However, it remains challenging to accurately mine such knowledge due to the noisy nature of interleaved dialogs in live chat data. In this paper, we first formulate the problem of issue-solution pair extraction from developer live chat data, and propose an automated approach, named ISPY, based on natural language processing and deep learning techniques with customized enhancements, to address the problem. Specifically, ISPY automates three tasks: 1) Disentangle live chat logs, employing a feedforward neural network to disentangle a conversation history into separate dialogs automatically; 2) Detect dialogs discussing issues, using a novel convolutional neural network (CNN), which consists of a BERT-based utterance embedding layer, a context-aware dialog embedding layer, and an output layer; 3) Extract appropriate utterances and combine them as corresponding solutions, based on the same CNN structure but with different feeding inputs. To evaluate ISPY, we compare it with six baselines, utilizing a dataset with 750 dialogs including 171 issue-solution pairs and evaluate ISPY from eight open source communities. The results show that, for issue-detection, our approach achieves the F1 of 76%, and outperforms all baselines by 30%. Our approach achieves the F1 of 63% for solution-extraction and outperforms the baselines by 20%. Furthermore, we apply ISPY on three new communities to extensively evaluate ISPY's practical usage. Moreover, we publish over 30K issue-solution pairs extracted from 11 communities. We believe that ISPY can facilitate community-based software development by promoting knowledge sharing and shortening the issue-resolving process."
Analysis of various modulation techniques for high-frequency isolated single-phase modified quasi-Z-source AC-AC converter-based solid-state transformer,"Jeelani, N; Bhat, AH",10.1007/s00202-023-02025-9,2024,"Solid-state transformers (SSTs) are expected to become one of the most powerful and adaptable devices that allow controllable voltage, power factor correction, fault isolation, compact size as compared to their low-frequency (50 Hz/60 Hz) counterparts. The high-frequency isolation of single-phase modified quasi-Z-source AC-AC converter (SPM-qZAC) employing bidirectional switches to create the single-stage SPM-qZAC-based SST is proposed in this paper. The proposed topology offers all the benefits of conventional impedance source topologies, including single-stage power conversion with a small footprint, buck-boost operation and retaining or reversing the phase angle. Moreover, the presented converter topology allows to share the same ground between input and output voltage, continuous input current, no input-output LC filters and performs AC-AC power conversion without the use of DC storage, making it suitable for AC voltage regulation. This study introduces two modulation schemes for SPM-qZAC-based SST. Finite control set model predictive control (FCS-MPC) which is a current control technique at variable switching frequency is employed owing to the capabilities of modern digital signal processing. Further, an adaptive hysteresis band-based delta sigma modulation (DSM) technique that offers the benefit of constant switching frequency (CSF) is proposed as an alternative voltage control-based modulation of the same topology. Various performance indices including steady-state response, total harmonic distortion of source current and dynamic response are assessed through simulation studies using MATLAB/Simulink software and real-time simulation environment using RT-Lab with OPAL-RT OP4510. It is observed that SPM-qZAC exhibits good performance when modulated using either modulation techniques; however, CSF-DSM technique offers an additional benefit of constant switching frequency."
Extended hyperspectral characterization of plastic automotive parts via Acousto-optic Tunable Filter and Fourier Transform Infrared Spectrometry,"Anaya, K; de Paz, JPZ; Rizzo-Sierra, JA; Ramirez-Gutierrez, CF; Isaza, C",10.1016/j.infrared.2022.104402,2022,"The importance of plastics in the design and construction of a new generation of vehicles is fundamental for the automotive industry. Typically characterization of these components is done by using techniques such as Fourier Transform Infrared Spectrometer (FTIR), and spectrometer in the visible bands (SPM), among others. It is also possible to get spectral information with high resolution using Acousto-optic Tunable Filters (AOTF). Since the characterization of plastic or polymer materials has been studied extensively, one problem is still open in order to be understood -Cosmetic Degradation. Particularly in the automotive industry, this information is fundamental to guarantee the quality and security of components that are used to build vehicles. Considering the above, we propose a new strategy based on an extended hyperspectral characterization of plastics by using transform and juxtaposition operations of reflectance information values gathered from three techniques: FTIR, AOTF, and SPM. Experimental results show that the proposed method contributes significantly to understanding the problem of cosmetic degradation in plastics, and opens a new branch to understanding the phenomenon through modern computational techniques based on artificial intelligence."
Identification of specialized pro-resolving mediator clusters from healthy adults after intravenous low-dose endotoxin and omega-3 supplementation: a methodological validation,"Norris, PC; Skulas-Ray, AC; Riley, I; Richter, CK; Kris-Etherton, PM; Jensen, GL; Serhan, CN; Maddipati, KR",10.1038/s41598-018-36679-4,2018,"Specialized pro-resolving mediator(s) (SPMs) are produced from the endogenous omega-3 polyunsaturated fatty acids (PUFA), eicosapentaenoic acid (EPA) and docosahexaenoic acid (DHA), and accelerate resolution of acute inflammation. We identified specific clusters of SPM in human plasma and serum using LC-MS/MS based lipid mediator (LM) metabololipidomics in two separate laboratories for inter-laboratory validation. The human plasma cluster consisted of resolvin (Rv)E1, RvD1, lipoxin (LX)B-4, 18-HEPE, and 17-HDHA, and the human serum cluster consisted of RvE1, RvD1, AT-LXA(4), 18-HEPE, and 17-HDHA. Human plasma and serum SPM clusters were increased after omega-3 supplementation (triglyceride dietary supplements or prescription ethyl esters) and low dose intravenous lipopolysaccharide (LPS) challenge. These results were corroborated by parallel determinations with the same coded samples in a second, separate laboratory using essentially identical metabololipidomic operational parameters. In these healthy subjects, two omega-3 supplementation protocols (Study A and Study B) temporally increased the SPM cluster throughout the endotoxin-challenge time course. Study A and Study B were randomized and Study B also had a crossover design with placebo and endotoxin challenge. Endotoxin challenge temporally regulated lipid mediator production in human serum, where pro-inflammatory eicosanoid (prostaglandins and thromboxane) concentrations peaked by 8 hours post-endotoxin and SPMs such as resolvins and lipoxins initially decreased by 2 h and were then elevated at 24 hours. In healthy adults given omega-3 supplementation, the plasma concentration of the SPM cluster (RvE1, RvD1, LXB4, 18-HEPE, and 17-HDHA) peaked at two hours post endotoxin challenge. These results from two separate laboratories with the same samples provide evidence for temporal production of specific pro-resolving mediators with omega-3 supplementation that together support the role of SPM in vivo in inflammation-resolution in humans."
DocToModel: Automated Authoring of Models from Diverse Requirements Specification Documents,"Rajbhoj, A; Nistala, P; Kulkarni, V; Soni, S; Pathan, A",10.1109/ICSE-SEIP58684.2023.00024,2023,"Early stages of Software Development Life Cycle (SDLC) namely requirement elicitation and requirements analysis have remained document-centric in the industry for marketdriven, complex, large-scale business applications and products. The documentation typically runs into hundreds of Natural Language (NL) text documents which requirements engineers need to sift looking for the relevant information and also maintain these documents in-sync over time - a time-consuming and error-prone activity. Much of this difficulty can be overcome if the information is available in a structured form that is amenable to automated processing. Purposive models offer a way out. However, for easy adoption by industry practitioners, these models must be populated from NL text documents in a largely automated manner. This task is characterized by high variability with several documents containing different information conforming to different structures and styles. As a result, purposive information extractors need to be developed for each project/ product. Moreover, being an open-ended space there is no upper bound on the information extractors that need to be developed. To overcome this difficulty, we propose a document structure agnostic and meta-model agnostic tool, DocToModel, for the automated authoring of models from NL text documents. It provides a pattern mapping language to specify a mapping of structured and unstructured document information to meta-model elements, and a pattern interpreter to automate model authoring. The configurable and extensible architecture of DocToModel makes it generic and amenable to easy repurposing for other NL documents. This paper, describes the approach and illustrates its utility and efficacy on multiple real-world case studies."
Cross-Project Transfer Learning on Lightweight Code Semantic Graphs for Defect Prediction,"Fang, DB; Liu, SY; Li, Y",10.1142/S0218194023500262,2023,"A deep learning system (DLS) developed based on one software project for defect prediction may well be applied to the related code on the same project but is usually difficult to be applied to new or unknown software projects. To address this problem, we propose a Transferable Graph Convolutional Neural Network (TGCNN) that can learn defects from the lightweight semantic graphs of code and transfer the learned knowledge from the source project to the target project. We discuss how the semantic graph is constructed from code; how the TGCNN can learn from the graph; and how the learned knowledge can be transferred to a new or unknown project. We also conduct a controlled experiment to evaluate our method. The result shows that despite some limitations, our method performs considerably better than existing methods."
An ontology-based KBE application for supply chain sustainability assessment,"Zhang, L; Olsen, A; Lobov, A",10.1016/j.resenv.2022.100086,2022,"Product Lifecycle Management (PLM) plays a key role in digital transformation demanded by Industry 4.0 and life cycle assessment, including sustainability assessment. Knowledge Based Engineering (KBE) applications can support PLM by integrating heterogeneous knowledge from different stages throughout the product life. However, the integration of knowledge from different stages and teams can cause misunderstanding if not represented in a unified form. Furthermore, different forms of knowledge used by different software are neither machine-readable nor human-readable, which also sets obstacles to knowledge integration in KBE applications. Supply chain sustainability assessment is such a scenario that entails integrating knowledge from different sources. This paper firstly implements a sustainability assessment method from other scholar to calculate the supply chain sustainability performance and adapts a sustainability assessment ontology for supply chain sustainability assessment. Then, an example KBE application is developed by implementing the sustainability assessment ontology and calculation method to simulate the knowledge sharing and integration between different teams. Finally, through this example application, it is discussed that the implementation of ontology to represent knowledge in PLM application for collaborative tasks like sustainability assessment can increase the efficiency of data sharing and integration. This paper is a proof of concept for the ontology-based framework. This framework can facilitate to represent knowledge but not create new knowledge, which means it can increase the efficiency of the software development, but cannot provide a better calculation method and assessment framework for supply chain sustainability assessment."
A Framework for Evaluating the Standards for the Production of Airborne and Ground Traffic Management Software,"Andres-Jimenez, J; Medina-Merodio, JA; Fernandez-Sanz, L; Martinez-Herraiz, JJ; Gonzalez-De-Lope, J",10.1109/ACCESS.2020.3014794,2020,"The development of Airborne and Ground systems is framed by specific regulations, usually expressed as standards. A disadvantage of those standards is the inherent complexity for its application and the verification of compliance given the high number of requirements to be checked in many different situations of the application and which are highly dependent on the applicable level of criticality. When the development of this type of system requires the incorporation of new personnel without enough knowledge about the standards, risks of mistakes in their application grow exponentially. The objective of this work is to develop an Expert System (ES) that helps to evaluate the application of the standards DO-178C and DO-278A throughout the project life cycle, at the same time it serves to facilitate both its use and the learning of its application to a wide group of professionals. The proposed underlying method for the ES will allow evaluating the set of development processes to check coverage of the standards DO-178C and DO-278A without depending on a specific life cycle model. The method involves a model of the set of processes, so they can be evaluated by the ES. Additionally, the ES will require a minimum configuration to evaluate the development of systems based on these two standards. The main result is a new generic Expert System based on rules capable of being adapted to different environment of evaluation, whichh minor configuration operations thus allowing that a Generic ES can act as a Specific ES for each situation. This configurable ES has been customized to evaluate the software life cycle based on the standards under study."
Metabololipidomic profiling of functional immunoresolvent clusters and eicosanoids in mammalian tissues,"Norris, PC; Serhan, CN",10.1016/j.bbrc.2018.03.037,2018,"Metabolomics enables a systems approach to interrogate the bioactive mediators, their pathways and further metabolites involved in the physiology and pathophysiology of human and animal tissues. New metabololipidomic approaches with mass spectrometry presented in this brief review can now be utilized for the identification and profiling of lipid mediator networks that control inflammation-resolution in human blood and healthy and diseased solid tissues. Coagulation of blood is a protective response that prevents excessive bleeding on injury of blood vessels. Here, we review novel approaches to understand the relationship(s) between coagulation and resolution of inflammation and infection. To determine whether coagulation is involved in host-protective actions by lipid mediators, we used a metabololipidomic-based profiling approach with human whole blood (WB) during coagulation. We identified recently temporal clusters of endogenously produced pro-thrombotic and proinflammatory lipid mediators (eicosanoids), as well as specialized proresolving mediators (SPMs) in this vital process. In addition to the classic eicosanoids (prostaglandins, thromboxanes and leukotrienes), a specific SPM cluster was identified that consists of resolvin El (RvE1), RvD1, RvD5, lipoxin B-4, and maresin 1, each of which present at bioactive concentrations (0.1-1 nM). The removal of adenosine from coagulating blood samples significantly enhances SPM amounts and unleashes the biosynthesis of RvD3, RvD4, and RvD6 evident following rapid snap freezing with centrifugation before extraction and LC-MS-MS. The classic cyclooxygenase inhibitors, celecoxib and indomethacin, that block thromboxanes and prostanoids do not block production of the clot-driven SPM cluster. Unbiased mass cytometry analysis demonstrated that the SPM cluster produced in human blood targets leukocytes at the single-cell level, directly activating extracellular signaling in human neutrophils and monocytes. Human whole blood treated with the components of this SPM cluster enhanced both phagocytosis and killing of Escherichia coli by leukocytes. Thus, we identified a pro-resolving lipid mediator circuit and specific SPM cluster that promotes host defense. This new lipid mediator (LM)-SPM metabololipidomic approach now provides accessible metabolomic profiles in healthy and diseased human tissues, including cancer, for precision and personalized medicine. (C) 2018 Elsevier Inc. All rights reserved."
ADDRESSING SOFTWARE/HARDWARE CHALLENGES BY INCORPORATING SYSTEM SOFTWARE INTEGRATOR CERTIFICATION,"Kemper, B; Laplante, P",,2023,"Software is embedded in most electromechanical systems and complex facilities and has changed our lives in the same way that electricity and the internal combustion engine once did. These changes are overwhelmingly beneficial, but recent events, such as the 737 MAX crashes and problems with autonomous vehicles, demonstrate the serious consequences of software failures. The challenges are greater than isolated incidents, however. Three quarters of software projects fail. There has been a 742% average annual increase in SCC attacks over the past three years for open-source software (OSS), in many cases costing millions per incident. There have been significant increases in cyber-attacks and ransomware incidents. Human threats are adaptive, pernicious, and persistent and two US presidents have issued Executive Orders regarding software-related threats to the nation. In response to these events, a task force within the National Society of Professional Engineers proposes the creation of a certified System Software Integrator (SSI) role - a person designated to take responsibility for a system in a manner similar to other professionals with responsible in-charge duties. This proposal has been championed by a multi-discipline team from diverse industries, including power generation, software development, and Artificial Intelligence development. The challenge of building critical embedded software systems is not a software problem, but an engineering problem that goes well beyond writing lines of code. This is an engineering solution."
Data Science with Semantic Technologies: Application to Information Systems Development,"Ben Sassi, S; Yanes, N",10.1080/08874417.2023.2220294,2024,"Various semantic technologies such as ontologies, machine learning, or artificial intelligence-based are being used today with data science for the purpose of explaining the meaning of data, and making this explanation exploitable by computer processing. Although some quick and brief reports do exist, to the best of our knowledge, the literature lacks a detailed study reporting why, when and how semantic technologies are used with data science. This paper is a theoretical review aiming at providing an insight into data science with semantic technologies. We characterize this research topic through a framework called DS2T helping to understand data science with semantic technologies and giving a comprehensive overview of the field through different, but complementary views. The proposed framework may be used to position research studies integrating semantic technologies with data science, compare them, understand new trends, and identify opportunities and open issues related to a given application domain. Software development processes are used as illustration domain."
Semantic-Based Representation Binary Clone Detection for Cross-Architectures in the Internet of Things,"Luo, ZH; Wang, BS; Tang, Y; Xie, W",10.3390/app9163283,2019,"Code reuse is widespread in software development as well as internet of things (IoT) devices. However, code reuse introduces many problems, e.g., software plagiarism and known vulnerabilities. Solving these problems requires extensive manual reverse analysis. Fortunately, binary clone detection can help analysts mitigate manual work by matching reusable code and known parts. However, many binary clone detection methods are not robust to various compiler optimization options and different architectures. While some clone detection methods can be applied across different architectures, they rely on manual features based on human prior knowledge to generate feature vectors for assembly functions and fail to consider the internal associations between features from a semantic perspective. To address this problem, we propose and implement a prototype GeneDiff, a semantic-based representation binary clone detection approach for cross-architectures. GeneDiff utilizes a representation model based on natural language processing (NLP) to generate high-dimensional numeric vectors for each function based on the Valgrind intermediate representation (VEX) representation. This is the first work that translates assembly instructions into an intermediate representation and uses a semantic representation model to implement clone detection for cross-architectures. GeneDiff is robust to various compiler optimization options and different architectures. Compared to approaches using symbolic execution, GeneDiff is significantly more efficient and accurate. The area under the curve (AUC) of the receiver operating characteristic (ROC) of GeneDiff reaches 92.35%, which is considerably higher than the approaches that use symbolic execution. Extensive experiments indicate that GeneDiff can detect similarity with high accuracy even when the code has been compiled with different optimization options and targeted to different architectures. We also use real-world IoT firmware across different architectures as targets, therein proving the practicality of GeneDiff in being able to detect known vulnerabilities."
Toward Supporting the Classification of Software Requirements with an Intelligent Semantic Approach,"Alrumaih, H; Mirza, A; Alsalamah, H",,2020,"With the growing awareness of the effects of requirements on software development processes, requirements engineering is increasingly becoming an important field of focus in software engineering research. Many studies show that failures in understanding and classifying requirements are the main reasons for exceeding project costs and allocated time, which in turn may cause the failure of a project. Successful software systems development requires consistent and classified requirements. The classification of requirements represents an early but critical phase in the requirements analysis stage. While the literature sheds light on distinctions between different types of requirements, the detection of such differences in practice is not always an easy task. This paper provides an overview of requirements classification, presents some of the existing research studies on requirements classification, and discusses their limitations in terms of yielding suggestions for improvement. Additionally, this work takes a different approach to address requirements classification. It proposes a semantic model to classify requirements automatically, using a hybrid artificial intelligence approach. In addition, the paper discusses evaluation methods for each part of the proposed model."
ART for Agile Autonomous Real-Time Testing in the Product Development Cycle,"Fehlmann, T; Kranich, E",10.1007/978-3-030-85521-5_25,2021,"Autonomous Real-time Testing (ART) is a concept for generating test cases that can be executed automatically, without interaction by testers, while the system under test physically remains in operation. The concept is also useful to continuously testing software while being developed. ART complements Test-Driven Development (TDD) because part of ART is the use of Digital Twins that emulate sensors, actuators, and Cloud services during software development, thus improving the ability of the development team to test systems under development. Moreover, because test cases are constantly being extended using an user-driven Artificial Intelligence approach, requirements, especially for security and privacy protection can be kept under control on a nightly base. This paper describes the theoretical background and its consequences for the software process and future improvements needed for digitalization."
Scientific programming using optimized machine learning techniques for software fault prediction to improve software quality,"Shafiq, M; Alghamedy, FH; Jamal, N; Kamal, T; Daradkeh, YI; Shabaz, M",10.1049/sfw2.12091,2023,"The amount of time and money required to finish a software project and distribute the final product increases when there are bugs in the programme. Software procedures like defect monitoring and repair may be both costly and time-consuming to complete. Because it is difficult to locate and correct every defect in a product, it is essential that the negative effect of those defects be minimised in order to provide a result that is of better overall quality. The process of identifying troublesome sections of software code is known as software defect prediction. This paper presents an optimized machine learning-enabled model for software fault prediction to improve software quality. PC1 data set is fed as input data in this model. Important features are selected by ant colony optimization (ACO) technique. Selected features are fed as input to support vector machine. Training and testing of SVM is performed by PC1 data set. Performance of ACO SVM Ant Colony Optimization Support Vector Machine is compared with SVM, Naive Bayes classifier and K-Nearest Neighbour classifier. The performance of ACO-based SVM is better for software fault classification and prediction."
SPMNet: A light-weighted network with separable pyramid module for real-time semantic segmentation,"Gao, SW; Zhang, CZ; Wang, ZP; Zhang, H; Huang, C",10.1080/0952813X.2021.1908432,2022,"Real-time semantic segmentation aims to generate high-quality prediction in limited time. Recently, with the development of many related potential applications, such as autonomous driving, robot sensing and augmented reality devices, semantic segmentation is desirable to make a trade-off between accuracy and inference speed with limited computation resources. This paper introduces a novel effective and light-weighted network based on Separable Pyramid Module (SPM) to achieve competitive accuracy and inference speed with fewer parameters and computation. Our proposed SPM unit utilises factorised convolution and dilated convolution in the form of a feature pyramid to build a bottleneck structure, which extracts local and context information in a simple but effective way. Experiments on Cityscapes and Camvid datasets demonstrate our superior trade-off between speed and precision. Without pre-training or any additional processing, our SPMNet achieves 71.22% mIoU on Cityscapes test set at the speed of 94 FPS on a single GTX 1080Ti GPU card."
Solving Separation-of-Concerns Problems in Collaborative Design of Human-AI Systems through Leaky Abstractions,"Subramonyam, H; Im, J; Seifert, C; Adar, E",10.1145/3491102.3517537,2022,"In conventional software development, user experience (UX) designers and engineers collaborate through separation of concerns (SoC): designers create human interface specifications, and engineers build to those specifications. However, we argue that Human-AI systems thwart SoC because human needs must shape the design of the AI interface, the underlying AI sub-components, and training data. How do designers and engineers currently collaborate on AI and UX design? To find out, we interviewed 21 industry professionals (UX researchers, AI engineers, data scientists, and managers) across 14 organizations about their collaborative work practices and associated challenges. We find that hidden information encapsulated by SoC challenges collaboration across design and engineering concerns. Practitioners describe inventing ad-hoc representations exposing low-level design and implementation details (which we characterize as leaky abstractions) to puncture SoC and share information across expertise boundaries. We identify how leaky abstractions are employed to collaborate at the AI-UX boundary and formalize a process of creating and using leaky abstractions."
"Detecting Cryptography Misuses With Machine Learning: Graph Embeddings, Transfer Learning and Data Augmentation in Source Code Related Tasks","Rodrigues, GED; Braga, AM; Dahab, R",10.1109/TR.2023.3237849,2023,"Cryptography is a ubiquitous tool in secure software development in order to guarantee security requirements in general. However, software developers have scarce knowledge about cryptography and rely on limited support tools that cannot properly detect bad uses of cryptography, thus generating vulnerabilities in software. In this work, we extend the scarcely use of machine learning to detect cryptography misuse in source code by using a state of the art deep learning model (i.e., code2vec) through transfer learning to generate features that feed machine learning models. In addition, we compare this approach to previous ones in different types of binary models. Also, we adapt code obfuscation to serve as data augmentation in machine learning source code related tasks. Finally, we show that through transfer learning code2vec can be a competitive feature generator for cryptography misuse detection and simple code obfuscation can be used to generate data to enhance machine learning models training in source code related tasks."
Data-Oriented Software Development: The Industrial Landscape through Patent Analysis,"Georgiou, K; Mittas, N; Ampatzoglou, A; Chatzigeorgiou, A; Angelis, L",10.3390/info14010004,2023,"tau he large amounts of information produced daily by organizations and enterprises have led to the development of specialized software that can process high volumes of data. Given that the technologies and methodologies used to develop software are constantly changing, offering significant market opportunities, organizations turn to patenting their inventions to secure their ownership as well as their commercial exploitation. In this study, we investigate the landscape of data-oriented software development via the collection and analysis of information extracted from patents. To this regard, we made use of advanced statistical and machine learning approaches, namely Latent Dirichlet Allocation and Brokerage Analysis for the identification of technological trends and thematic axes related to software development patent activity dedicated to data processing and data management processes. Our findings reveal that high-profile countries and organizations are engaging in patent granting, while the main thematic circles found in the retrieved patent data revolve around data updates, integration, version control and software deployment. The results indicate that patent grants in this technological domain are expected to continue their increasing trend in the following years, given that technologies evolve and the need for efficient data processing becomes even more present."
"Designing Adaptive Developer-Chatbot Interactions: Context Integration, Experimental Studies, and Levels of Automation","Melo, G",10.1109/ICSE-COMPANION58688.2023.00064,2023,"The growing demand for software developers and the increasing development complexity have emphasized the need for support in software engineering projects. This is especially relevant in light of advancements in artificial intelligence, such as conversational systems. A significant contributor to the complexity of software development is the multitude of tools and methods used, creating various contexts in which software developers must operate. Moreover, there has been limited investigation into the interaction between context-based chatbots and software developers through experimental user studies. Assisting software developers in their work becomes essential. In particular, understanding the context surrounding software development and integrating this context into chatbots can lead to novel insight into what software developers expect concerning these human-chatbot interactions and their levels of automation. In my research, I study the design of context-based adaptive interactions between software developers and chatbots to foster solutions and knowledge to support software developers at work."
A Maturity Model for Trustworthy AI Software Development,"Cho, S; Kim, I; Kim, J; Woo, H; Shin, W",10.3390/app13084771,2023,"Recently, AI software has been rapidly growing and is widely used in various industrial domains, such as finance, medicine, robotics, and autonomous driving. Unlike traditional software, in which developers need to define and implement specific functions and rules according to requirements, AI software learns these requirements by collecting and training relevant data. For this reason, if unintended biases exist in the training data, AI software can create fairness and safety issues. To address this challenge, we propose a maturity model for ensuring trustworthy and reliable AI software, known as AI-MM, by considering common AI processes and fairness-specific processes within a traditional maturity model, SPICE (ISO/IEC 15504). To verify the effectiveness of AI-MM, we applied this model to 13 real-world AI projects and provide a statistical assessment on them. The results show that AI-MM not only effectively measures the maturity levels of AI projects but also provides practical guidelines for enhancing maturity levels."
HTL-DP: Homogeneous Transfer Learning for Defect Prediction,"Nevendra, M; Singh, P",10.1007/978-3-031-13150-9_22,2023,"Most approaches to predict software defects require extensive projects under training and testing. However, sufficient training information is often not available for a new project. Therefore, a predictive model should be built using historical data from additional relevant projects, followed by using the model to predict defects in the target project. The performance of this cross-project technique still has to be increased, mainly because of the distribution divergences among the source and target projects and the ambiguity about which source project should be chosen to train the model. This study proposes a Homogeneous Transfer Learning Model for Defect Prediction (HTL-DP), which covers two phases: the conversion of images and the prediction of defects. To improve the overall performance of this system, we use the popular ResNet18 to compare it with several state-of-the-art techniques and five different pre-defined deep models. We demonstrate that the framework proposed in this paper provides higher performance with exterior analysis of a 10-software project image data set than that available in the existing literature. Literature also shows that transforming multidimensional software metrics into images is more significant than direct conversion from code into images."
Do Users Write More Insecure Code with AI Assistants?,"Perry, N; Srivastava, M; Kumar, D; Boneh, D",10.1145/3576915.3623157,2023,"AI code assistants have emerged as powerful tools that can aid in the software development life-cycle and can improve developer productivity. Unfortunately, such assistants have also been found to produce insecure code in lab environments, raising significant concerns about their usage in practice. In this paper, we conduct a user study to examine how users interact with AI code assistants to solve a variety of security related tasks. Overall, we find that participants who had access to an AI assistant wrote significantly less secure code than those without access to an assistant. Participants with access to an AI assistant were also more likely to believe they wrote secure code, suggesting that such tools may lead users to be overconfident about security flaws in their code. To better inform the design of future AI-based code assistants, we release our user-study apparatus to researchers seeking to build on our work."
A Multiport Converter Interfacing Solar Photovoltaic Modules and Energy Storage With DC Microgrid,"Vettuparambil, A; Chatterjee, K; Fernandes, BG",10.1109/TIE.2020.2978709,2021,"In this article, a novel multiport converter (MPC) to interface different solar photovoltaic modules (SPM), and the battery with a 380 V dc microgrid is proposed. It is ensured that all the photovoltaic modules are operated at their respective maximum power points (MPPs), which is a unique feature of the proposed scheme. The boosting of the low voltages of the SPMs and that of the battery to 380 V is accomplished by involving a dc-dc converter along with a high-frequency transformer. The battery banks are charged directly from the power of the SPM without involving the high-frequency transformer. This significantly reduces the power flow path within the system. The MPC can operate in stand-alone mode or in microgrid connected mode as and when required. In the microgrid-connected mode, it is capable of realizing the MPP tracking, and at the same time, it is able to control the charging current of the battery as per the requirement of its charge controller. When MPC is operated in stand-alone mode, the voltage across the local loads are regulated at 380 V. The proposed MPC is modeled using the first component approximation method to facilitate the design of the appropriate controllers. The effectiveness of the proposed scheme is established by performing detailed simulation studies. A hardware prototype of the MPC is fabricated. Detailed experimental studies are carried out utilizing the developed prototype to confirm the viability of the proposed scheme."
An Integrated Approach to Ensure Requirements Traceability During the Product Development Process,"Viapiana, D; Riggio, G; Barbieri, L; Bruno, F",10.1007/978-3-030-91234-5_33,2022,"Today's Product Lifecycle Management (PLM) platforms are a fundamental strategy for the development of complex products because of their capability to integrate processes, business systems, and information getting a complete and clear vision of all the various stages of the product development process and supporting the design and management of an extensive set of requirements. Nevertheless, common PLM platforms lack efficient traceability of design requirements within the product development process. Rather than a stand-alone tool for requirements management, it is desirable to take a design method that places requirements in the spotlight of the design process. For this reason, the paper presents a method, based on the Requirement - Functional - Logical - Physical (RFLP) design approach, that ensures requirements traceability during each stage of the product development process and provides decision support in a multidisciplinary collaborative digital environment. In particular, Dassault Systemes' 3DEXPERIENCE platform has been adopted for the case study of the suspension and tyre systems of a vehicle to assess the efficacy of the proposed method and validate the integration within the collaborative business experience platform."
iMER: Iterative process of entity relationship and business process model extraction from the requirements,"Javed, M; Lin, YQ",10.1016/j.infsof.2021.106558,2021,"Context: Extracting conceptual models, e.g., entity relationship model or Business Process model, from software requirement document is an essential task in the software development life cycle. Business process model presents a clear picture of required system's functionality. Operations in business process model together with the data entity consumed, help the software developers to understand the database design and operations to be implemented. Researchers have been aiming at automatic extraction of these artefacts from the requirement document. Objective: In this paper, we present an automated approach to extract the entity relationship and business process models from requirements, which are possibly in different formats such as general requirements, use case specification and user stories. Our approach is based on the efficient natural language processing techniques. Method: It is an iterative approach of Models Extraction from the Requirements (iMER). iMER has multiple iterations where each iteration is to address a sub-problem. In the first iteration, iMER extracts the data entities and attributes. Second iteration is to find the relationships between data entities, while extracting cardinalities is in the third step. Business process model is generated in the fourth iteration, containing the external (actors') and internal (system's) operations. Evaluation: To evaluate the performance and accuracy of iMER, experiments are conducted on various formats of the requirement documents. Additionally, we have also evaluated our approaches using the requirement documents which been modified by shuffling the sentences and by merging with other requirements. Comparative study is also performed. The preliminary results show a noticeable improvement. Conclusion: The iMER is an efficient automated iterative approach that is able to extract the conceptual models from the various formats of requirements."
Application of Data Analytics Techniques for Decision Making in the Retrospective Stage of the Agile Scrum Methodology,"Sandoval-Alfaro, OE; Quintero-Meza, RR",10.1109/ENC53357.2021.9534800,2021,"The Scrum retrospective is the last activity within the Scrum methodology, and it is the previous one to Scrum planning within the activities of the method. To propose improvements for future Sprints, the team performs frequent inspection and adaptation processes analyzing what has worked and what has not worked with respect to the current Sprint and the different related tasks such as the estimation of the Product Backlog, assignment of Story Points and prediction of team velocity. These practices are usually supported by empirical team approaches, implying taking risks that can cost development resources. The adoption of data analytics within the business world has gained relevance in recent years, since through this it is possible to process data for decision-making based on predictive models. Today these models are used based on the management and processing of historical data, using data analytics and artificial intelligence techniques. Taking advantage of the fact that software project managers publicly offer Datasets with information on their Sprints and their characteristics, these could be used with data analytics, in conjunction with supervised learning models, both prediction and classification to support a more efficient decision making, thereby achieving an improvement in the process with a more efficient use of resources."
From SATD Recognition to an Interpretation Method Based on the Dataset,"Meng, Y; Tie, B; Lin, DW",10.1142/S0218194022500693,2023,"Technical debt describes a trade-off between short-term goals and long-term code quality during software development. Self-admitted technical debt (SATD), a type of technical debt, is intentionally introduced by developers. The existence of SATD is likely to leave hidden dangers for future changes in software systems, so identifying SATD is an essential task. Before this, many methods for recognizing SATD (such as pattern matching-based, natural language processing-based, text mining-based, etc.) have been proposed. This paper will present a pre-trained deep learning model to complete the SATD recognition task. An efficient deep learning model interpretation tool Captum can be used to understand the experimental results. At the same time, a new interpretation view is proposed for the matching-based model. Finally, combined with the research in this paper, reasonable suggestions are put forward for future SATD recognition tasks."
Predicting the objective and priority of issue reports in software repositories,"Izadi, M; Akbari, K; Heydarnoori, A",10.1007/s10664-021-10085-3,2022,"Software repositories such as GitHub host a large number of software entities. Developers collaboratively discuss, implement, use, and share these entities. Proper documentation plays an important role in successful software management and maintenance. Users exploit Issue Tracking Systems, a facility of software repositories, to keep track of issue reports, to manage the workload and processes, and finally, to document the highlight of their team's effort. An issue report is a rich source of collaboratively-curated software knowledge, and can contain a reported problem, a request for new features, or merely a question about the software product. As the number of these issues increases, it becomes harder to manage them manually. GitHub provides labels for tagging issues, as a means of issue management. However, about half of the issues in GitHub's top 1000 repositories do not have any labels. In this work, we aim at automating the process of managing issue reports for software teams. We propose a two-stage approach to predict both the objective behind opening an issue and its priority level using feature engineering methods and state-of-the-art text classifiers. To the best of our knowledge, we are the first to fine-tune a Transformer for issue classification. We train and evaluate our models in both project-based and cross-project settings. The latter approach provides a generic prediction model applicable for any unseen software project or projects with little historical data. Our proposed approach can successfully predict the objective and priority level of issue reports with 82% (fine-tuned RoBERTa) and 75% (Random Forest) accuracy, respectively. Moreover, we conducted human labeling and evaluation on unlabeled issues from six unseen GitHub projects to assess the performance of the cross-project model on new data. The model achieves 90% accuracy on the sample set. We measure inter-rater reliability and obtain an average Percent Agreement of 85.3% and Randolph's free-marginal Kappa of 0.71 that translate to a substantial agreement among labelers."
Improving Organizational Pattern Comprehensibility by Varying Their Form and Relationships,"Frtala, T; Vranic, V",10.1109/ZINC55034.2022.9840528,2022,"Based on various contexts, one can find that an organizational pattern can fit diversely in use. This is due to their naturally generalized written descriptions that allow flexible application and minor adjustments. In this study, we examine multiple ways of perceived organizational patterns variants that can be present not only as text differences but also as manners of applying them. We created a supporting tool in the form of a web portal that provides the possibility to track, manage, and depict organizational patterns, pattern languages, and their variants. The separate module recorded participants' interactions with the web portal as statistics. From the gathered statistics, we evaluated how and where the pattern variants emerge and form. The presented results show that some pattern elements are prone to produce more pattern variants than others besides pattern forms. The created web portal can serve as a community hub around organizational patterns."
A Convolutional Neural Network for Language-Agnostic Source Code Summarization,"Moore, J; Ben Gelman; Slater, D",10.5220/0007678100150026,2019,"Descriptive comments play a crucial role in the software engineering process. They decrease development time, enable better bug detection, and facilitate the reuse of previously written code. However, comments are commonly the last of a software developer's priorities and are thus either insufficient or missing entirely. Automatic source code summarization may therefore have the ability to significantly improve the software development process. We introduce a novel encoder-decoder model that summarizes source code, effectively writing a comment to describe the code's functionality. We make two primary innovations beyond current source code summarization models. First, our encoder is fully language-agnostic and requires no complex input preprocessing. Second, our decoder has an open vocabulary, enabling it to predict any word, even ones not seen in training. We demonstrate results comparable to state-of-the-art methods on a single-language data set and provide the first results on a data set consisting of multiple programming languages."
S4 Features and Artificial Intelligence for Designing a Robot against COVID-19-Robocov,"Ponce, P; Mata, O; Perez, E; Lopez, JR; Molina, A; McDaniel, T",10.3390/fi14010022,2022,"Since the COVID-19 Pandemic began, there have been several efforts to create new technology to mitigate the impact of the COVID-19 Pandemic around the world. One of those efforts is to design a new task force, robots, to deal with fundamental goals such as public safety, clinical care, and continuity of work. However, those characteristics need new products based on features that create them more innovatively and creatively. Those products could be designed using the S4 concept (sensing, smart, sustainable, and social features) presented as a concept able to create a new generation of products. This paper presents a low-cost robot, Robocov, designed as a rapid response against the COVID-19 Pandemic at Tecnologico de Monterrey, Mexico, with implementations of artificial intelligence and the S4 concept for the design. Robocov can achieve numerous tasks using the S4 concept that provides flexibility in hardware and software. Thus, Robocov can impact positivity public safety, clinical care, continuity of work, quality of life, laboratory and supply chain automation, and non-hospital care. The mechanical structure and software development allow Robocov to complete support tasks effectively so Robocov can be integrated as a technological tool for achieving the new normality's required conditions according to government regulations. Besides, the reconfiguration of the robot for moving from one task (robot for disinfecting) to another one (robot for detecting face masks) is an easy endeavor that only one operator could do. Robocov is a teleoperated system that transmits information by cameras and an ultrasonic sensor to the operator. In addition, pre-recorded paths can be executed autonomously. In terms of communication channels, Robocov includes a speaker and microphone. Moreover, a machine learning algorithm for detecting face masks and social distance is incorporated using a pre-trained model for the classification process. One of the most important contributions of this paper is to show how a reconfigurable robot can be designed under the S3 concept and integrate AI methodologies. Besides, it is important that this paper does not show specific details about each subsystem in the robot."
Sentence pair modeling based on semantic feature map for human interaction with IoT devices,"Yu, R; Lu, WP; Lu, HM; Wang, SJ; Li, FF; Zhang, X; Yu, JG",10.1007/s13042-021-01349-x,2021,"The rapid development of Internet of Things (IoT) brings an urgent requirement on intelligent human-device interactions using natural language, which are critical for facilitating people to use IoT devices. The efficient interactive approaches depend on various natural language understanding technologies. Among them, sentence pair modeling (SPM) is essential, where neural networks have achieved great success in SPM area due to their powerful abilities in feature extraction and representation. However, as sentences are one-dimensional (1D) texts, the available neural networks are usually limited to 1D sequential models, which prevents the performance improvement of SPM task. To address this gap, in this paper, we propose a novel neural architecture for sentence pair modeling, which utilizes 1D sentences to construct multi-dimensional feature maps similar to images containing multiple color channels. Based on the feature maps, more kinds of neural models become applicable on SPM task, including 2D CNN. In the proposed model, first, the sentence on a specific granularity is encoded with BiLSTM to generate the representation on this granularity, which is viewed as a special channel of the sentence. The representations from different granularity are merged together to construct semantic feature map of the input sentence. Then, 2D CNN is employed to encode the feature map to capture the deeper semantic features contained in the sentence. Next, another 2D CNN is utilized to capture the interactive matching features between sentences, followed by 2D max-pooling and attention mechanism to generate the final matching representation. Finally, the matching degree of sentences are judged with a sigmoid function according to the matching representation. Extensive experiments are conducted on two real-world data sets. In comparison with benchmarks, the proposed model achieved remarkable results, and performed better or comparably with BERT-based models. Our work is beneficial to building a more powerful humanized interaction system with IoT devices."
An ontology-based approach to engineering ethicality requirements,"Guizzardi, R; Amaral, G; Guizzardi, G; Mylopoulos, J",10.1007/s10270-023-01115-3,2023,"In a world where Artificial Intelligence (AI) is pervasive, humans may feel threatened or at risk by giving up control to machines. In this context, ethicality becomes a major concern to prevent AI systems from being biased, making mistakes, or going rogue. Requirements Engineering (RE) is the research area that can exert a great impact in the development of ethical systems by design. However, proposing concepts, tools and techniques that support the incorporation of ethicality into the software development processes as explicit requirements remains a great challenge in the RE field. In this paper, we rely on Ontology-based Requirements Engineering (ObRE) as a method to elicit and analyze ethicality requirements ('Ethicality requirements' is adopted as a name for the class of requirements studied in this paper by analogy to other quality requirements studied in software engineering, such as usability, reliability, and portability, etc. The use of this term (as opposed to 'ethical requirements') highlights that they represent requirements for ethical systems, analogous to how 'trustworthiness requirements' represent requirements for trustworthy systems. To put simply: the predicates 'ethical' or 'trustworthy' are not meant to be predicated over the requirements themselves). ObRE applies ontological analysis to ontologically unpack terms and notions that are referred to in requirements elicitation. Moreover, this method instantiates the adopted ontology and uses it to guide the requirements analysis activity. In a previous paper, we presented a solution concerning two ethical principles, namely Beneficence and Non-maleficence. The present paper extends the previous work by targeting two other important ethicality principles, those of Explicability and Autonomy. For each of these new principles, we do ontological unpacking of the relevant concepts, and we present requirements elicitation and analysis guidelines, as well as examples in the context of a driverless car case. Furthermore, we validate our approach by analysing the requirements elicitation made for the driverless car case in contrast with a similar case, and by assessing our method's coverage w.r.t European Union guidelines for Trustworthy AI."
MLOps: A Guide to its Adoption in the Context of Responsible AI,"Matsui, BMA; Goya, DH",10.1145/3526073.3527591,2022,"DevOps practices have increasingly been applied to software development as well as the machine learning lifecycle, in a process known as MLOps. Currently, many professionals have written about this topic, but still few results can be found in the academic and scientific literature on MLOps and how to to implement it effectively. Considering aspects of responsible AI, this number is even lower, opening up a field of research with many possibilities. This article presents five steps to guide the understanding and adoption of MLOps in the context of responsible AI. The study aims to serve as a reference guide for all those who wish to learn more about the topic and intend to implement MLOps practices to develop their systems, following responsible AI principles."
Automatic Release Notes Generation,"Ali, M; Aftab, A; Buttt, WH",10.1109/icsess49938.2020.9237671,2020,"Release Notes (RNs) are one of the important artifacts in software development and maintenance. As, RNs are required when a new release of a software is planned to deploy. They contain all the changes made to the new release of project i.e. description of new features, improvements, bug fixes, deprecated features, etc. Generating these notes manually is a very complex,and time-consuming task. In this paper, we present an approach for generating RNs automatically. We implemented the approach in python and generate these notes for node.js projects. Our system extracts changes from Git repository, summarize changes, get deprecated features, get library changes, fetch issues from issue tracker, and link these issues to code, etc. Our system hierarchically set up these changes and produce an output in a document. We evaluated our results manually from 14 industry developers. The results obtained from our system shows that these RNs are very good and accurate than ones always produced manually."
An empirical evaluation of Lex/Yacc and ANTLR parser generation tools,"Ortin, F; Quiroga, J; Rodriguez-Prieto, O; Garcia, M",10.1371/journal.pone.0264326,2022,"Parsers are used in different software development scenarios such as compiler construction, data format processing, machine-level translation, and natural language processing. Due to the widespread usage of parsers, there exist different tools aimed at automizing their generation. Two of the most common parser generation tools are the classic Lex/Yacc and ANTLR. Even though ANTLR provides more advanced features, Lex/Yacc is still the preferred choice in many university courses. There exist different qualitative comparisons of the features provided by both approaches, but no study evaluates empirical features such as language implementor productivity and tool simplicity, intuitiveness, and maintainability. In this article, we present such an empirical study by conducting an experiment with undergraduate students of a Software Engineering degree. Two random groups of students implement the same language using a different parser generator, and we statistically compare their performance with different measures. Under the context of the academic study conducted, ANTLR has shown significant differences for most of the empirical features measured."
Deriving the minimum staff number requirement for intelligent staff scheduling: An efficient constructive method and application,"Cao, B; Chen, H; Wang, ZJ; Wang, T; Fan, J",10.1111/exsy.12975,2022,"Effective staff scheduling is a critical activity of successful software development management. Due to its difficulty and broad applications in many service delivery scenarios, staff scheduling has been studied for several decades. However, most existing work focus on constructing the working schedules based on a given workforce size. This paper tries to solve a prerequisite issue before performing staff scheduling, i.e., testing whether the already existed manpower can meet the scheduling requirements. Though it is possible to use network flow theory or artificial intelligence (AI) methods like genetic algorithms to solve this problem, their time complexities could be too high to be used for large problem sizes. This paper proposes a constructive method that can derive the minimum staff number for three scheduling problem variants in a linear running time, and in the meantime a corresponding working schedule that can satisfy all the problem constraints can be produced. We not only theoretically show the lower bound for the computation time complexity of our proposed method but also prove its correctness. Moreover, based on the derived minimum staff number, we further explore the genetic algorithm for generating the schedule and compare its performance with our method. The experiments show that our method outperforms the baselines in terms of both effectiveness and efficiency."
Automatically classifying user requests in crowdsourcing requirements engineering,"Li, CY; Huang, LG; Ge, JD; Luo, B; Ng, V",10.1016/j.jss.2017.12.028,2018,"In order to make a software project succeed, it is necessary to determine the requirements for systems and to document them in a suitable manner. Many ways for requirements elicitation have been discussed. One way is to gather requirements with crowdsourcing methods, which has been discussed for years and is called crowdsourcing requirements engineering. User requests forums in open source communities, where users can propose their expected features of a software product, are common examples of platforms for gathering requirements from the crowd. Requirements collected from these platforms are often informal text descriptions and we name them user requests. In order to transform user requests into structured software requirements, it is better to know the class of requirements that each request belongs to so that each request can be rewritten according to corresponding requirement templates. In this paper, we propose an effective classification methodology by employing both project-specific and non-project specific keywords and machine learning algorithms. The proposed strategy does well in achieving high classification accuracy by using keywords as features, reducing considerable manual efforts in building machine learning based classifiers, and having stable performance in finding minority classes no matter how few instances they have. (C) 2018 Elsevier Inc. All rights reserved."
Linking poverty with water and sanitation in targeting households for achieving sustainable development,"Hijazi, H; Duraes, J; Couceiro, R; Castelhano, J; Barbosa, R; Medeiros, J; Castelo-Branco, M; de Carvalho, P; Madeira, H",10.1109/TSE.2022.3158543,2023,"Code review is an essential practice in software engineering to spot code defects in the early stages of software development. Modern code reviews (e.g., acceptance or rejection of pull requests with Git) have become less formal than classic Fagan's inspections, lightweight, and more reliant on individuals (i.e., reviewers). However, reviewers may encounter mentally demanding challenges during the code review, such as code comprehension difficulties or distractions that might affect the code review quality. This work proposes a novel approach that evaluates the quality of code reviews in terms of bug-finding effectiveness and provides the reviewers with a clear message of whether the review should be repeated, indicating the code regions that may not have been well-reviewed. The proposed approach utilizes biometric information collected from the reviewer during the review process using non-intrusive biofeedback devices (e.g., smartwatches). Biometric measures such as Heart Rate Variability (HRV) and task-evoked pupillary response are captured as a surrogate of the cognitive state of the reviewer (e.g., mental workload) and inexpensive desktop eye-trackers compatible with the software development settings. This work uses Artificial Intelligence techniques to predict the cognitive load from the extracted biomarkers and classify each code region according to a set of features. The final evaluation considers various factors such as code complexity, time of the code review, the experience level of the reviewer, and other factors. Our experimental results show the approach could predict the review quality with 87.77%& PLUSMN;4.65 accuracy and a Spearman correlation coefficient of 0.85 (p-value < 0.001) between the predicted and the actual review performance. This evaluation validates the cognitive load measurement using electroencephalography (EEG) signals as ground truth for the HRV and pupil signals."
Context-Based Aspect-Oriented Requirement Engineering Model,"Idate, SR; Rao, TS; Mali, DJ",,2023,"Mobile applications are context-oriented systems that involve the use of context information while operating. Mobile applications demand tackling context information in the early phase of software engineering. A context-aware system demands a different approach to handling the influence of the context on a system's requirements. Aspect-oriented Requirement Engineering separates concerns throughout requirements, called crosscutting concerns, in the early phase of software development to improve the modularity of complex applications. Capturing requirements embedded within context is a challenging procedure. This study aimed to identify such contextual characteristics of requirements in the early phase of software engineering, using natural language processing techniques, by proposing Context-Based Aspect-Oriented Requirement Engineering (CB-AORE) to visualize the existence of crosscutting concerns. CB-AORE performs context modeling to analyze the context dependency with base requirements and helps the analyst to visualize the correlation of functional and non-functional requirements with context. A case study analyzed the identification of context and its use to identify crosscutting concerns."
A dataset for identifying actionable feedback in collaborative software development,"Meyers, BS; Munaiah, N; Prud'hommeaux, E; Meneely, A; Alm, CO; Wolff, J; Murukannaiah, PK",,2018,"Software developers and testers have long struggled with how to elicit proactive responses from their coworkers when reviewing code for security vulnerabilities and errors. For a code review to be successful, it must not only identify potential problems but also elicit an active response from the colleague responsible for modifying the code. To understand the factors that contribute to this outcome, we analyze a novel dataset of more than one million code reviews for the Google Chromium project, from which we extract linguistic features of feedback that elicited responsive actions from coworkers. Using a manually-labeled subset of reviewer comments, we trained a highly accurate classifier to identify acted-upon comments (AUC = 0.85). Our results demonstrate the utility of our dataset, the feasibility of using NLP for this new task, and the potential of NLP to improve our understanding of how communications between colleagues can be authored to elicit positive, proactive responses."
Machine learning based approval prediction for enhancement reports,"Nafees, SA; Rehman, FAU",10.1109/IBCAST51254.2021.9393180,2021,"In modern times, the maintenance of the software application plays a vital role in its success. Software applications obtain enhancement requests on a large scale to fulfil user requirements through different Issue Tracking Systems. Issue tracking system provides an effective way for keeping the bugs records in the software development system. Conventionally, developers used to manually check these requests themselves. However, manual inspection of these requests turns out to be a boring, hectic and time-consuming activity.Therefore, there is dire need of developing an automatically prediction system, that can help in decision making for further improvement. In this work, we propose a Support Vector Machine-based classifier to automatically approve or reject an enhancement report. Our approach can be divided into different steps. Firstly, we perform the pre-processing on each enhancement report using natural language processing (NLTK) techniques. Secondly, we generate a feature vector for each pre-processed enhancement report. Finally, we train a Support Vector Machine-based classifier that automatically predicts the rejection or approval of the enhancement report. In order to have a thorough analysis, we also evaluate and compare other well-known machine learning algorithms e.g. Multinomial Naive Bayes and Logistic Regression. We use a well-known open-source dataset extracted from the Bugzilla software application for our experiments. Our experiments suggest that Support Vector Machine-based classifier outperforms other approaches and achieves high accuracy on 35 different open-source applications which include 40,000 enhancement reports. The evaluated results of tenfold cross-validation show that the proposed approach can increase the accuracy as compared to the state-of-the-art accuracy. We believe that our approach will help developers save time and address user-requirements in a more efficient manner."
HINT: Integration Testing for AI-based features with Humans in the Loop,"Chen, QZ; Schnabel, T; Nushi, B; Amershi, S",10.1145/3490099.3511141,2022,"The dynamic nature of AI technologies makes testing human-AI interaction and collaboration challenging - especially before such features are deployed in the wild. This presents a challenge for designers and AI practitioners as early feedback for iteration is often unavailable in the development phase. In this paper, we take inspiration from integration testing concepts in software development and present HINT (Human-AI INtegration Testing), a crowd-based framework for testing AI-based experiences integrated with a humans-in-the-loop workflow. HINT supports early testing of AI-based features within the context of realistic user tasks and makes use of successive sessions to simulate AI experiences that evolve over-time. Finally, it provides practitioners with reports to evaluate and compare aspects of these experiences. Through a crowd-based study, we demonstrate the need for overtime testing where user behaviors evolve as they interact with an AI system. We also show that HINT is able to capture and reveal these distinct user behavior patterns across a variety of common AI performance modalities using two AI-based feature prototypes. We further evaluated HINT's potential to support practitioners' evaluation of human-AI interaction experiences pre-deployment through semi-structured interviews with 13 practitioners."
SMARTKT: A Search Framework to assist Program Comprehension using Smart Knowledge Transfer,"Majumdar, S; Papdeja, S; Das, PP; Ghosh, SK",10.1109/QRS.2019.00026,2019,"Regardless of attempts to extract knowledge from code bases to aid in program comprehension, there is an absence of a framework to extract and integrate knowledge to provide a near-complete multifaceted understanding of a program. To bridge this gap, we propose SMARTKT (Smart Knowledge Transfer) to extract and transfer knowledge related to software development and application-specific characteristics and their interrelationships in form of a knowledge graph. For an application, the knowledge graph provides an overall understanding of the design and implementation and can be used by an intelligent natural language query system to convert the process of knowledge transfer into a developer-friendly Google-like search. For validation, we develop an analyzer to discover concurrency-related design aspects from runtime traces in a machine learning framework and obtain a precision and recall of around 97% and 95% respectively. We extract application specific knowledge from code comments and obtain 72% match against human-annotated ground truth."
Documentation-based functional constraint generation for library methods,"Jiang, RH; Chen, ZZ; Pei, Y; Pan, MX; Zhang, T; Li, XD",10.1002/stvr.1785,2021,"Although software libraries promote code reuse and facilitate software development, they increase the complexity of programme analysis tasks. To effectively analyse programmes built on top of software libraries, it is essential to have specifications for the library methods that can be easily processed by analysis tools. However, the availability of such specifications is seriously limited at the moment. Manually writing the specifications can be prohibitively expensive and error-prone, while existing automated approaches to inferring the specifications seldom produce results that are strong enough to be used in programme analysis. In this work, we propose the Doc2smt approach to generating strong functional constraints in SMT for library methods based on their documentations. Doc2smt first applies natural language processing (NLP) techniques and a set of rules to translate a method's natural language documentation into a large number of candidate constraint clauses in OCL. Then, it utilizes a manually enhanced domain model to identify OCL candidate constraint clauses that comply with the problem domain in static validation, translates well-formed OCL constraints into the SMT-LIB format, and checks whether each SMB-LIB constraint rightly abstracts the functionalities of the method under consideration via testing in dynamic validation. In the end, it reports the first functional constraint that survives both validations to the user as the result. We have implemented the approach into a supporting tool with the same name. In experiments conducted on 451 methods from the Java Collections Framework and the Java IO library, Doc2smt generated correct constraints for 309 methods, with the average generation time for each correct constraint being merely 2.7 min. We have also applied the generated constraints to facilitate symbolic-execution-based test generation with the Symbolic Java PathFinder (SPF) tool. For 24 utility methods manipulating Java container and IO objects, SPF with access to the generated constraints produced 51.2 times more test cases than SPF without the access."
Quality Metrics in Software Design: A Systematic Review,"Hernandez-Gonzalez, EY; Sanchez-Garcia, AJ; Cortes-Verdin, MK; Perez-Arriaga, JC",10.1109/CONISOFT.2019.00021,2019,"This paper presents the results of a systematic literature review, which aimed to identify metrics for the quality of software that are applied in the design stage. Fifteen papers from different electronic databases were selected to answer three questions. The analysis allowed us to provide an overview of the metrics used to design artifacts. These metrics will serve as the basis for generating models based on artificial intelligence techniques (Neuronal Networks, Regression, or some other), that help to estimate quality in the early stages of the software development process. It is concluded that most of the design metrics are object oriented. In addition, the design metrics are applied to class diagrams, package diagrams and sequence diagrams."
Determinants of Bank Closures: What Ensures Sustainable Profitability in Mobile Banking?,"Cho, S; Lee, Z; Hwang, S; Kim, J",10.3390/electronics12051196,2023,"Owing to the recent increase in mobile banking customers, studies exploring self-service channels and customer responses as distribution channels in the retail banking industry are also rapidly expanding. Moreover, with the emergence of big data and a series of artificial intelligence (AI) technologies, customer pattern analysis using deep learning, insurance fraud prevention, software development and various types of blockchain-based FinTech technologies, offline banks are disappearing. Accordingly, many commercial banks are attempting to find technological alternatives. However, maintaining a profitable bank branch is a crucial factor in the relationship between service quality and customer satisfaction because excellent service quality prevents existing customers from leaving. This study sought to quantitatively prove the change in customer service quality and profit due to the introduction of technology in the financial industry. We microscopically compared the effects between bank branch closures and changes in net profit using a time-series analysis. Specifically, we quantitatively analyzed actual customer attrition behavior with a time-series analysis across the three quarters before and after the closure of 88 branches of major commercial banks in South Korea in the Seoul metropolitan area and nearby cities. The findings prove that branch closures and multi-channel effects in the financial sector are gradually being resolved through immediate technology acceptance, contrary to popular concern."
A Rapid Seismic Damage Assessment (RASDA) Tool for RC Buildings Based on an Artificial Intelligence Algorithm,"Morfidis, K; Stefanidou, S; Markogiannaki, O",10.3390/app13085100,2023,"In the current manuscript, a novel software application for rapid damage assessment of RC buildings subjected to earthquake excitation is presented based on artificial neural networks. The software integrates the use of a novel deep learning methodology for rapid damage assessment into modern software development platforms, while the developed graphical user interface promotes the ease of use even from non-experts. The aim is to foster actions both in the pre- and post-earthquake phase. The structure of the source code permits the usage of the application either autonomously as a software tool for rapid visual inspections of buildings prior to or after a strong seismic event or as a component of building information modelling systems in the framework of digitizing building data and properties. The methodology implemented for the estimation of the RC buildings' damage states is based on the theory and algorithms of pattern recognition problems. The effectiveness of the developed software is successfully tested using an extended, numerically generated database of RC buildings subjected to recorded seismic events."
AdaptivePaste: Intelligent Copy-Paste in IDE,"Liu, XY; Jang, JN; Sundaresan, N; Allamanis, M; Svyatkovskiy, A",10.1145/3611643.3613895,2023,"In software development, it is common for programmers to copy-paste or port code snippets and then adapt them to their use case. This scenario motivates the code adaptation task - a variant of program repair which aims to adapt variable identifiers in a pasted snippet of code to the surrounding, preexisting context. However, no existing approach has been shown to effectively address this task. In this paper, we introduce AdaptivePaste, a learning-based approach to source code adaptation, based on transformers and a dedicated dataflow-aware deobfuscation pre-training task to learn meaningful representations of variable usage patterns. We demonstrate that AdaptivePaste can learn to adapt Python source code snippets with 67.8% exact match accuracy. We study the impact of confidence thresholds on the model predictions, showing the model precision can be further improved to 85.9% with our parallel-decoder transformer model in a selective code adaptation setting. To assess the practical use of AdaptivePaste we perform a user study among Python software developers on real-world copy-paste instances. The results show that AdaptivePaste reduces dwell time to nearly half the time it takes to port code manually, and helps to avoid bugs. In addition, we utilize the participant feedback to identify potential avenues for improvement."
Automatic examining of software architectures on mobile applications codebases,"Dobrean, D",10.1109/ICSME.2019.00094,2019,"Mobile applications have grown to become complex software systems and some of the most used pieces of software by end users all around the world. With the increase in their complexity, software architecture has become an important topic and a pressure point in their development lifecycle. The purpose of this work is to define an automatic method for extracting and examining the software architecture of mobile applications by leveraging the use of Software Development Kits (SDKs) and Artificial Intelligence algorithms. The proposed system is used for finding architectural issues on the analysed codebase early in the development phase and provides insightful information for both software developers, architects as well as for the management team."
Certification Compliant Performance Analysis and Requirements Management of an Electrically Powered General Aviation Aircraft,"Hein, L; Panchal, P; Myschik, S",10.1109/RAST57548.2023.10197897,2023,"This paper presents an implementation of a requirements validation toolchain for certification compliant aircraft performance analysis of an electrically powered general aviation aircraft. As part of the dtec. bw project ELAPSED, a novel electric propulsion system approach for a CS-22 class aircraft is being developed. In order to comply with CS-22 certification rules, pre-defined aircraft requirements must be fulfilled by the design, such as a maximum take-off distance of 500 m. To validate the feasibility of aircraft and system requirements and their accordance with the certification standards, a toolchain providing bidirectional traceability from requirements to test results has been set up. This toolchain is comprised of Polarion PLM for requirements management and MATLAB/Simulink providing mission evaluation capabilities using a nonlinear simulation model of the specific aircraft. Two inhouse tools named SimPol and Tico provide connectivity and round-tripping between Polarion and Simulink. Application of this toolchain is presented along a test run of 3 requirements."
Mining Experienced Developers in Open-source Projects,"Perez, Q; Urtado, C; Vauttier, S",10.5220/0011071800003176,2022,"Experienced developers are key for the success of software development projects. In open-source software development, due to openness and distance, one cannot always rely on interpersonal interactions to know who these key people are. Automating the mining of experienced developers is not an easy task either, because of the subjectivity and relativity of what experience is and also because the material to search from (code and development-related metadata) does not obviously relate developers to their capabilities. Some research works propose developer profiling or clustering solutions though, from which we take inspiration. This paper advocates that it is possible to learn from tangible metrics extracted from code and development-related artifacts who are the experienced developers. It uses a supervised learning-based approach trained with a manually labeled dataset of 703 developers from 17 open-source projects from GitHub for which 23 metrics are automatically extracted. Experienced developers classification results show a high F1 measure. A companion explainability study analyzes which metrics are the most influential."
Looking for related posts on GitHub discussions,"Lima, M; Steinmacher, I; Ford, D; Liu, E; Vorreuter, G; Conte, T; Gadelha, B",10.7717/peerj-cs.1567,2023,"Software teams increasingly adopt different tools and communication channels to aid the software collaborative development model and coordinate tasks. Among such resources, software development forums have become widely used by developers. Such environments enable developers to get and share technical information quickly. In line with this trend, GitHub announced GitHub Discussions-a native forum to facilitate collaborative discussions between users and members of communities hosted on the platform. Since GitHub Discussions is a software development forum, it faces challenges similar to those faced by systems used for asynchronous communication, including the problems caused by related posts (duplicated and near-duplicated posts). These related posts can add noise to the platform and compromise project knowledge sharing. Hence, this article addresses the problem of detecting related posts on GitHub Discussions. To achieve this, we propose an approach based on a Sentence-BERT pre-trained general-purpose model: the RD-Detector. We evaluated RD-Detector using data from three communities hosted in GitHub. Our dataset comprises 16,048 discussion posts. Three maintainers and three Software Engineering (SE) researchers manually evaluated the RD-Detector results, achieving 77-100% of precision and 66% of recall. In addition, maintainers pointed out practical applications of the approach, such as providing knowledge to support merging the discussion posts and converting the posts to comments on other related posts. Maintainers can benefit from RD-Detector to address the laborintensive task of manually detecting related posts."
Symptom Prediction and Mortality Risk Calculation for COVID-19 Using Machine Learning,"Jamshidi, E; Asgary, A; Tavakoli, N; Zali, A; Dastan, F; Daaee, A; Badakhshan, M; Esmaily, H; Jamaldini, SH; Safari, S; Bastanhagh, E; Maher, A; Babajani, A; Mehrazi, M; Kashi, MAS; Jamshidi, M; Sendani, MH; Rahi, SJ; Mansouri, N",10.3389/frai.2021.673527,2021,"Background: Early prediction of symptoms and mortality risks for COVID-19 patients would improve healthcare outcomes, allow for the appropriate distribution of healthcare resources, reduce healthcare costs, aid in vaccine prioritization and self-isolation strategies, and thus reduce the prevalence of the disease. Such publicly accessible prediction models are lacking, however. Methods: Based on a comprehensive evaluation of existing machine learning (ML) methods, we created two models based solely on the age, gender, and medical histories of 23,749 hospital-confirmed COVID-19 patients from February to September 2020: a symptom prediction model (SPM) and a mortality prediction model (MPM). The SPM predicts 12 symptom groups for each patient: respiratory distress, consciousness disorders, chest pain, paresis or paralysis, cough, fever or chill, gastrointestinal symptoms, sore throat, headache, vertigo, loss of smell or taste, and muscular pain or fatigue. The MPM predicts the death of COVID-19-positive individuals. Results: The SPM yielded ROC-AUCs of 0.53-0.78 for symptoms. The most accurate prediction was for consciousness disorders at a sensitivity of 74% and a specificity of 70%. 2,440 deaths were observed in the study population. MPM had a ROC-AUC of 0.79 and could predict mortality with a sensitivity of 75% and a specificity of 70%. About 90% of deaths occurred in the top 21 percentile of risk groups. To allow patients and clinicians to use these models easily, we created a freely accessible online interface at www. aicovid.net. Conclusion: The ML models predict COVID-19-related symptoms and mortality using information that is readily available to patients as well as clinicians. Thus, both can rapidly estimate the severity of the disease, allowing shared and better healthcare decisions with regard to hospitalization, self-isolation strategy, and COVID-19 vaccine prioritization in the coming months."
Detection of Frog Virus 3 by Integrating RPA-CRISPR/Cas12a-SPM with Deep Learning,"Lei, ZY; Lian, LJ; Zhang, LK; Liu, CY; Zhai, SY; Yuan, X; Wei, JZ; Liu, H; Liu, Y; Du, ZC; Gul, I; Zhang, HH; Qin, ZF; Zeng, SL; Jia, P; Du, K; Deng, L; Yu, DM; He, Q; Qin, PW",10.1021/acsomega.3c02929,2023,"A fast, easy-to-implement, highly sensitive, and point-of-care (POC) detection system for frog virus 3 (FV3) is proposed. Combining recombinase polymerase amplification (RPA) and CRISPR/Cas12a, a limit of detection (LoD) of 100 aM (60.2 copies/mu L) is achieved by optimizing RPA primers and CRISPR RNAs (crRNAs). For POC detection, smartphone microscopy is implemented, and an LoD of 10 aM is achieved in 40 min. The proposed system detects four positive animal-derived samples with a quantitation cycle (Cq) value of quantitative PCR (qPCR) in the range of 13 to 32. In addition, deep learning models are deployed for binary classification (positive or negative samples) and multiclass classification (different concentrations of FV3 and negative samples), achieving 100 and 98.75% accuracy, respectively. Without temperature regulation and expensive equipment, the proposed RPA-CRISPR/Cas12a combined with smartphone readouts and artificial-intelligence-assisted classification showcases the great potential for FV3 detection, specifically POC detection of DNA virus."
"Dancing, not Wrestling: Moving from Compliance to Concordance for Secure Software Development","Ashenden, D; Ollis, G; Reid, I",10.1145/3551349.3561145,2022,"Secure software development has become an increasingly important focus for research in recent years, not least because of advances in technology such as AI, machine learning (AI/ML), robotics, and autonomous systems (RAS). AI/ML and RAS facilitate automated decision-making and have the capability to have a significant impact on society. As such this technology needs to be trustworthy, and secure software development is a key attribute for trustworthiness. Software developers frequently have responsibility and accountability for delivering secure code but limited authority over how this is achieved. Authority tends to lie with cyber security professionals who mandate security processes, tools and training, often with limited success. Our research objective was to better understand how to bridge this gap between software developers and cyber security practitioners so that authority, responsibility and accountability are shared equally. We took inspiration from healthcare research that looks at the relationship between compliance, adherence and concordance. We use this research as a lens through which to analyse qualitative data from 35 interviews with professional software developers. Our research suggests that if software developers and cyber security professionals move to a point of concordance in their interactions it could lead to the negotiation of more realistic cyber security solutions, as well as removing friction from the practice of software developers and ultimately lead to more secure and trustworthy systems."
Conception of a Conversational Interface to Provide a Guided Search of Study Related Data,"Berger, R; Ebner, M; Ebner, M",10.3991/ijet.v14i07.10137,2019,"Since the beginning of software development, solution approaches and technologies have changed massively, including the requirements for a user interface. At the very beginning, it was the desktop application, with a classic Graphical User Interface (GUI), which fulfilled the needs of a user. Nowadays, many applications moved to web respectively mobile and the user behavior changed. A very modern concept to handle the communication between a computer and a user is a chatbot. The range of functions of a chatbot can be very simple up to complex artificial intelligence based solutions. This publication focuses on a chatbot solution for Graz University of Technology (TU Graz), which should support the student by finding study related information via a conversational interface."
User Story Extraction from Online News for Software Requirements Elicitation: A Conceptual Model,"Raharjana, IK; Siahaan, D; Fatichah, C",10.1109/jcsse.2019.8864199,2019,"To specify good requirements, system analysts need to understand the domain knowledge of the system. There are several techniques in requirements elicitation to improve domain knowledge understanding, such as user interviews, questioners, document analysis, and brainstorming. Most of these techniques require profound stakeholder involvement. However, not all software projects can do this task due to limited time or availability of stakeholders. In agile software development, the user story is the de facto standard used for capturing and writing functional requirements. The user story is an appropriate format and easy to understand for writing the results of requirements elicitation. This study purposes a conceptual model to extract user story from online news for improving domain knowledge understanding. The information in the online news contained lesson learned related to certain events. This information may improve the functionality of the software products. The user story consists of three aspects, namely: who, what, and why. Aspect of who represents the role or user, aspect of what shows the purpose or feature, while the aspect of why explains the reason. This format can summarize the lessons learned in the news. Our experimental results indicate that this conceptual model can extract user story from online news. The model manages to extract 105 user stories from 92 aspects of what/why candidate and 109 aspects of who candidate."
An Empirical Study of Deep Transfer Learning-Based Program Repair for Kotlin Projects,"Kim, M; Kim, Y; Jeong, H; Heo, J; Kim, S; Chung, H; Lee, E",10.1145/3540250.3558967,2022,"Deep learning-based automated program repair (DL-APR) can automatically fix software bugs and has received significant attention in the industry because of its potential to significantly reduce software development and maintenance costs. The Samsung mobile experience (MX) team is currently switching from Java to Kotlin projects. This study reviews the application of DL-APR, which automatically fixes defects that arise during this switching process; however, the shortage of Kotlin defect-fixing datasets in Samsung MX team precludes us from fully utilizing the power of deep learning. Therefore, strategies are needed to effectively reuse the pretrained DL-APR model. This demand can be met using the Kotlin defect-fixing datasets constructed from industrial and open-source repositories, and transfer learning. This study aims to validate the performance of the pretrained DL-APR model in fixing defects in the Samsung Kotlin projects, then improve its performance by applying transfer learning. We show that transfer learning with open source and industrial Kotlin defect-fixing datasets can improve the defect-fixing performance of the existing DL-APR by 307%. Furthermore, we confirmed that the performance was improved by 532% compared with the baseline DL-APR model as a result of transferring the knowledge of an industrial (non-defect) bug-fixing dataset. We also discovered that the embedded vectors and overlapping code tokens of the code-change pairs are valuable features for selecting useful knowledge transfer instances by improving the performance of APR models by up to 696%. Our study demonstrates the possibility of applying transfer learning to practitioners who review the application of DL-APR to industrial software."
Classification of UML Diagrams to Support Software Engineering Education,"Tavares, JF; Costa, YMG; Colanzi, TE",10.1109/ASEW52652.2021.00030,2021,"There is a huge necessity for tools that implement accessibility in Software Engineering (SE) education. The use of diagrams to teach software development is a very common practice, and there are a lot of UML diagrams represented as images in didactic materials that need an accessible version for visually impaired or blind students. Machine learning techniques, such as deep learning, can be used to automate this task. The practical application of deep learning in many classification problems in the context of SE is problematic due to the large volumes of labeled data required for training. Transfer learning techniques can help in this type of task by taking advantage of pre-trained models based on Convolutional Neural Networks (CNN), so that better results may be achieved even with few images. In this work, we applied transfer learning and data augmentation for UML diagrams classification on a dataset specially created for the development of this work, containing six types of UML diagrams. The dataset was also made available as a contribution of this work. We experimented three widely-known CNN architectures: VGG16, RestNet50, and InceptionV3. The results demonstrated that the use of transfer learning contributes for achieving good results even using scarce data. However, there is still a room for improvement regarding the successful classification of the UML diagrams addressed in this work."
Developing a Release Management Tool to Support Global Software Development,"Barbosa, HO; Albuquerque, WCX; Bandeira, AI; Albuquerque, LH; Pivoto, UE; Pires, FB; Bonifacio, BA",10.1145/3372787.3390437,2020,"Global Software Development (GSD) has been a trend as the software industry is experiencing increasing commercial globalization. On the other hand, working with distributed teams also face new difficulties and challenges, resulting from geographic separation such as time zone, culture, and activities synchronism. Sidia is a R&D Institute, located in Manaus-Brazil, that develops innovative software solutions on Android Platform in all Latin America. The institute works collaborating with Samsung Mobile Division, located in South Korea, and external stakeholders provided by Mobile Network Operators (MNO) from Latin America countries (e.g., Brazil, Mexico, Chile, Peru). For this reason, to meet the demands of MNOs, Sidia works in GSD environment. In this context, the project management process becomes difficult due to the coordination of many different stakeholders in a distributed environment, such as tracking requirements, wrong releases, tracking issues. To minimize these difficulties, we developed a tool to support our project management process, called Release Manager (RM). This paper describes the introduction of the RM Tool to improve management in distributed projects for the Android Platform Update."
Enhancing IOT based software defect prediction in analytical data management using war strategy optimization and Kernel ELM,"Zada, I; Alshammari, A; Mazhar, AA; Aldaeej, A; Qasem, SN; Amjad, K; Alkhateeb, JH",10.1007/s11276-023-03591-3,2024,"The existence of software problems in IoT applications caused by insufficient source code, poor design, mistakes, and insufficient testing poses a serious risk to functioning and user expectations. Prior to software deployment, thorough testing and quality assurance methods are crucial to reducing these risks. This study advances the field of IoT-based software quality assessment while also showcasing the viability and benefits of incorporating AI methods into Software Defect Prediction (SDP), particularly the Kernel-based Extreme Learning Machine (KELM) and the War Strategy Optimisation (WSO) algorithm. These efforts are essential to maintain the dependability and performance of IoT applications given the IoT's rising significance in our linked world. The chosen keywords, such as Software defect prediction, IoT, KELM, and WSO, capture the multidimensional nature of this novel technique and serve as an important source of information for upcoming study in this area. One of the main issues that needs to be addressed in order to overcome the difficulties of developing IoT-based software is how time and resource-consuming it is to test the programme in order to ensure its effectiveness. Software Defect Prediction (SDP) assumes a crucial function in this context in locating flaws in software components. Manual defect analysis grows more inefficient and time-consuming as software projects become more complicated. This research introduces a fresh method to SDP by utilising artificial intelligence (AI) to address these issues. The suggested methodology includes the War Strategy Optimisation (WSO) algorithm, which is cleverly used to optimise classifier hyperparameters, together with a Kernel Extreme Learning Machine (KELM) for SDP. The main objective is to improve softw. This innovative combination, grounded in previous studies [1, 2], promises superior capabilities in predicting software defects. Notably, it represents the inaugural endeavor to integrate the WSO algorithm with KELM for SDP, introducing a unique and advanced approach to software quality assessment. The proposed methodology undergoes rigorous evaluation using a diverse set of real-world software project datasets, including the renowned PROMISE dataset and various open-source datasets coded in Java. Performance assessment is conducted through multiple metrics, including Efficiency Accuracy, Reliability, Sensitivity, and F1-score, collectively illuminating the effectiveness of this approach. The outcome of our experiments underscores the potency of the Kernel Extreme Learning Machine coupled with the War Strategy Optimization algorithm in enhancing the accuracy of SDP and consequently elevating defect detection efficiency within software components. Remarkably, our methodology consistently outperforms existing techniques, registering an average increase of over 90% in accuracy across the parameters examined. This promising result underscores the potential of our approach to effectively tackle the challenges associated with IoT-based software development and software defect prediction. In conclusion, this study significantly contributes to the field of IoT-based software quality assessment, introducing an innovative methodology that substantially bolsters accuracy and reliability in SDP."
Blended Learning implementation in introduction to artificial intelligence courses using the System Development Life Cycle method,"Prasetyo, D; Wibawa, B; Dima, AO",10.1088/1757-899X/1098/4/042001,2021,"Indonesia is an island nation with geographical conditions that are bounded by many seas and mountains. The situation causes the condition of the islands that are far apart from one another. This certainly affects the conditions and social, economic, technological, and related interactions in daily life, one of which is the level of educational progress in one region / island with other regions / islands. The level of educational progress is certainly influenced by how the learning system is implemented in that place. To overcome the obstacles in improving the quality of learning and equal distribution of learning opportunities in different geographical, cultural, socioeconomic conditions, one solution is created, namely Blended Learning by utilizing devices connected to the internet. Methods that can be used in this research include SDLC (System Development Life Cycle). The optimism of the successful implementation of the Blended Learning system is very high because the facilities and atmosphere of various parties are very supportive plus the needs will become priority needs for everyone in this case students. In the province of East Nusa Tenggara has the geographical conditions of many islands, therefore the Blended Learning system is good enough to be applied, because in addition to the above mentioned transportation limitations for students and educators also become obstacles in conventional learning."
Historical Alphabet Transliteration Software Using Computer Vision Classification Approach,"HamplovÃ¡, A; Franc, D; Tyrychtr, J",10.1007/978-3-031-09076-9_4,2022,"The article presents the problem of developing mobile software for classification and automatic transliteration of historical alphabets to Latin alphabet using OCR Computer Vision algorithms and is presented on Palmyrene Alphabet. Our suggested solution of semi-automatic transliteration of historical alphabets speeds up and simplifies the process of ancient text analysis and makes reading historical alphabets available to the public. We created a mobile application template for field use and proved the functionality on our own photographic and digitized hand-written datasets of Palmyrene letters, using a MobileNet Artificial Neural Network for character recognition. Such an application helps archaeologists with a faster character transliteration of newly discovered, archived, but untranslated tablets, columns etc., and for checking hand-transliterated texts."
Code Librarian: A Software Package Recommendation System,"Tao, LL; Cazan, AP; Ibraimoski, S; Moran, S",10.1109/ICSE-SEIP58684.2023.00023,2023,"The use of packaged libraries can significantly shorten the software development life cycle by improving the quality and readability of code. In this paper, we present a recommendation engine called Code Librarian for open source libraries. A candidate library package is recommended for a given context if: 1) it has been frequently used with the imported libraries in the program; 2) it has similar functionality to the imported libraries in the program; 3) it has similar functionality to the developer's implementation, and 4) it can be used efficiently in the context of the provided code. We apply the state of the art CodeBERT-based model for analysing the context of the source code to deliver relevant library recommendations to users."
Is Evolutionary Computation Evolving Fast Enough?,"Kendall, G",10.1109/MCI.2018.2807019,2018,"Evolutionary Computation (EC) has been an active research area for over 60 years, yet its commercial/home uptake has not been as prolific as we might have expected. By way of comparison, technologies such as 3D printing, which was introduced about 35 years ago, has seen much wider uptake, to the extent that it is now available to home users and is routinely used in manufacturing. Other technologies, such as immersive reality and artificial intelligence have also seen commercial uptake and acceptance by the general public. In this paper we provide a brief history of EC, recognizing the significant contributions that have been made by its pioneers. We focus on two methodologies (Genetic Programming and Hyper-heuristics), which have been proposed as being suitable for automated software development, and question why they are not used more widely by those outside of the academic community. We suggest that different research strands need to be brought together into one framework before wider uptake is possible. We hope that this position paper will serve as a catalyst for automated software development that is used on a daily basis by both companies and home users."
PASTS: Progress-aware spatio-temporal transformer speaker for vision-and-language navigation,"Wang, LY; Liu, CJ; He, ZT; Li, S; Yan, QQ; Chen, HY; Chen, QJ",10.1016/j.engappai.2023.107487,2024,"Vision-and-language navigation (VLN) is a crucial but challenging cross-modal navigation task. One powerful technique to enhance the generalization performance in VLN is the use of an independent speaker model to provide pseudo instructions for data augmentation. However, current speaker models based on Long-Short Term Memory (LSTM) lack the ability to attend to features relevant at different locations and time steps. To address this, we propose a novel progress-aware spatio-temporal transformer speaker (PASTS) model that uses the transformer as the core of the network. PASTS uses a spatio-temporal encoder to fuse panoramic representations and encode intermediate connections through steps. Besides, to avoid the misalignment problem that could result in incorrect supervision, a speaker progress monitor (SPM) is proposed to enable the model to estimate the progress of instruction generation and facilitate more fine-grained caption results. Additionally, a multifeature dropout (MFD) strategy is introduced to alleviate overfitting. The proposed PASTS is flexible to be combined with existing VLN models. The experimental results demonstrate that PASTS outperforms previous speaker models and successfully improves the performance of previous VLN models, achieving state-of-the-art performance on the standard Room-to-Room (R2R) dataset."
Sifu - a cybersecurity awareness platform with challenge assessment and intelligent coach,"Gasiba, TE; Lechner, U; Pinto-Albuquerque, M",10.1186/s42400-020-00064-4,2020,"Software vulnerabilities, when actively exploited by malicious parties, can lead to catastrophic consequences. Proper handling of software vulnerabilities is essential in the industrial context, particularly when the software is deployed in critical infrastructures. Therefore, several industrial standards mandate secure coding guidelines and industrial software developers' training, as software quality is a significant contributor to secure software. CyberSecurity Challenges (CSC) form a method that combines serious game techniques with cybersecurity and secure coding guidelines to raise secure coding awareness of software developers in the industry. These cybersecurity awareness events have been used with success in industrial environments. However, until now, these coached events took place on-site. In the present work, we briefly introduce cybersecurity challenges and propose a novel platform that allows these events to take place online. The introduced cybersecurity awareness platform, which the authors call Sifu, performs automatic assessment of challenges in compliance to secure coding guidelines, and uses an artificial intelligence method to provide players with solution-guiding hints. Furthermore, due to its characteristics, the Sifu platform allows for remote (online) learning, in times of social distancing. The CyberSecurity Challenges events based on the Sifu platform were evaluated during four online real-life CSC events. We report on three surveys showing that the Sifu platform's CSC events are adequate to raise industry software developers awareness on secure coding."
Dynamic vulnerability detection approaches and tools: State of the Art,"Zaazaa, O; El Bakkali, H",10.1109/icds50568.2020.9268686,2020,"Vulnerabilities are everywhere around us. Every device we use in our daily life include a software that may contain vulnerabilities. The growth use of software and devices to automate some of our daily life actions is making these programs more complex and more connected to the internet, which increase the risk of cyber-attacks. To reduce this risk, multiple programming companies are trying to use different approaches to find these vulnerabilities. Some are using static approaches during the software development life cycle while others are using dynamic analysis approaches to find vulnerabilities once the application is correctly working. Unfortunately, both approaches still suffer from multiple limitation and still need improvement. In this paper, we are discussing some of the most recent dynamic approaches and the efficient of the tools that use them."
Leveraging Transformer and Graph Neural Networks for Variable Misuse Detection,"Romanov, V; Dlamini, G; Valeev, A; Ivanov, V",10.5220/0011997300003464,2023,"Understanding source code is a central part of finding and fixing software defects in software development. In many cases software defects caused by an incorrect usage of variables in program code. Over the years researchers have developed data-driven approaches to detect variable misuse. Most of modern existing approaches are based on the transformer architecture, trained on millions of buggy and correct code snippets to learn the task of variable detection. In this paper, we evaluate an alternative, a graph neural network (GNN) architectures, for variable misuse detection. Popular benchmark dataset, which is a collection functions written in Python programming language, is used to train the models presented in this paper. We compare the GNN models with the transformer-based model called CodeBERT."
Toward Imitating Visual Attention of Experts in Software Development Tasks,"Ikutani, Y; Koganti, N; Hata, H; Kubo, T; Matsumoto, K",10.1109/EMIP.2019.00013,2019,"Expert programmers' eye-movements during source code reading are valuable sources that are considered to be associated with their domain expertise. We advocate a vision of new intelligent systems incorporating expertise of experts for software development tasks, such as issue localization, comment generation, and code generation. We present a conceptual framework of neural autonomous agents based on imitation learning (IL), which enables agents to mimic the visual attention of an expert via his/her eye movement. In this framework, an autonomous agent is constructed as a context-based attention model that consists of encoder/decoder network and trained with state-action sequences generated by an experts' demonstration. Challenges to implement an IL-based autonomous agent specialized for software development task are discussed in this paper."
A MODEL FOR BUILDING SKILLS AND KNOWLEDGE NEEDED IN THE JOB MARKET,"Aunimo, L; Huttunen, S",,2020,"Finnish IT companies are facing a shortage of software engineers in several fields of software development. The field evolves quickly as new technologies emerge, as processing power of computers grows and as data available for processing becomes abundant. How can a university of applied sciences keep its teaching relevant from the point of view of companies that need personnel with new skills and knowledge? How do the teachers keep their own professional knowledge and skills up to date to be able to pass the knowledge and skills on to their students? How do the educational institutions know what kind of skills and knowledge the employers need in the first place? This paper presents a model for tackling the above-mentioned challenges in the context of university level education of future and current IT-professionals. The model has been tested in the teaching of artificial intelligence to undergraduate students of business information technology. Experiences from the implementations of the model have been gathered and analysed. The results show that the model clearly is a success among all stakeholders. The main novelty of the model is that it allows the dynamic and timely adjustment of curricula when new skill and knowledge requirements arise from the industry."
Towards Automated Code Reviews: Does Learning Code Structure Help,"Lin, HY; Thongtanunam, P",10.1109/SANER56733.2023.00075,2023,"Code review is a crucial ingredient to quality software development, but requires a large amount of time and effort for developers. To optimise this manual process, recent research on automated code review seeks to leverage Neural Machine Translation (NMT) models to perform tasks such as automated code improvement. A recent work had pretrained the NMT model for automated code review in order to equip the model with general coding knowledge. However, their pretraining approach is generic to natural languages, which does not leverage the unique properties of coding languages. Therefore, we set out to explore two state-of-the-art pretrained NMT models (i.e., CodeT5 and GraphCodeBERT) that were designed to learn code structure. We studied the models' abilities to generate correct code improvement through an empirical evaluation based on five different datasets. Our results showed that in terms of generating correct code sequences, CodeT5, GraphCodeBERT, and the prior work achieved an average accuracy of 22%, 18%, and 10%, respectively. In terms of generating correct dataflow structures, they achieved an average accuracy of 33%, 30%, and 22%, respectively. The results suggested that the code structure focused approaches could outperform the generic pretraining approach. This work contributes towards enhancing automated code review techniques by understanding the effectiveness of code structure focused NMT models."
BTLink : automatic link recovery between issues and commits based on pre-trained BERT model,"Lan, JP; Gong, LA; Zhang, JX; Zhang, HX",10.1007/s10664-023-10342-7,2023,"Data traceability in software development can connect different software artifacts to enhance the observability of developer practices. In particular, traceability links between issues and commits (i.e., issue-commit links) play a key role in software maintenance tasks (e.g., bug localization and bug prediction). In practice, developers typically manually make the issue-commit links by adding the issue identifier into the message of the corresponding commits, which results in missing issue commit links being prevalent in software projects. To recover the missing issue commit links, previous studies have proposed some automatic approaches. However, due to the difference between heuristic rules and real-world behavior, as well as insufficient semantic understanding, these approaches cannot achieve the expected performance. Since the text contained in issues and commits contains highly related information, thorough text understanding can improve traceability links. Meanwhile, pre-trained models (i.e., PTMs) have been successfully used to explore the semantic information of text in various software engineering tasks (e.g., software code generation). Therefore, our study proposes a novel BERT -based method (i.e., BTLink) that employs the pre-trained models to automatically recover the issue-commits links. Our proposed BTlink method includes a BERT embedding layer, a fusion layer, and a classifier layer. First, we build two pre-trained BERT encoders to respectively explore the feature representation of the issue text in combination with commit code and commit text. Then we build the fusion layer to examine the joint feature vector. Finally, we build the classifier layer to identify the links between issue and commit. In addition, to further our investigation and verify the effectiveness of BTLink, we conduct an extensive case study on 12 issue-commit links datasets from open source software projects, and observe that: (i) compared to state-of-the-art approaches, our proposed BTLink improves the performance of automatic issue-commit links recovery on all studied measures; (ii) both text and code information in the issues and commits are effective to recover more accurate issue-commit links; (iii) our proposed BTLink is more applicable to the cross-project context compared to state-of-the-art approaches."
Trusting Machine-Learning Applications in Aeronautics,"Benmeziane, K; Fabiani, P; Herbin, S; Lacaille, J; Ledinot, E",10.1109/AERO55745.2023.10115684,2023,"A general recommendation from the French office for aeronautical and space standardization (BNAE) is being drawn up by experts from Onera, Thales, Dassault and Safran, with the collaboration of Airbus, MBDA and ADP, the main French aeronautical companies. This document is based on mathematical and statistical elements which are reintroduced within a system and software development process considering the specificities of algorithms based on learning methods from data sets or data generators. For each activity in this development process, whether it is data capitalization or the use of artificial intelligence, risks are identified, and mitigation methods proposed. A few application cases are included in the document to illustrate the particularities of certain types of algorithms. Methods of estimation, classification, categorization or even reinforcement learning are mentioned. This paper gives a summary in English of the general recommendation."
Characterizing the Natural Language Descriptions in Software Logging Statements,"He, PJ; Chen, ZB; He, SL; Lyu, MR",10.1145/3238147.3238193,2018,"Logging is a common programming practice of great importance in modern software development, because software logs have been widely used in various software maintenance tasks. To provide high-quality logs, developers need to design the description text in logging statements carefully. Inappropriate descriptions will slow down or even mislead the maintenance process, such as postmortem analysis. However, there is currently a lack of rigorous guide and specifications on developer logging behaviors, which makes the construction of description text in logging statements a challenging problem. To fill this significant gap, in this paper, we systematically study what developers log, with focus on the usage of natural language descriptions in logging statements. We obtain 6 valuable findings by conducting source code analysis on 10 Java projects and 7 C projects, which contain 28,532,975 LOC and 115,159 logging statements in total. Furthermore, our study demonstrates the potential of automated description text generation for logging statements by obtaining up to 49.04 BLEU-4 score and 62.1 ROUGE-L score using a simple information retrieval method. To facilitate future research in this field, the datasets have been publicly released."
Better Data Labelling With EMBLEM (and how that Impacts Defect Prediction),"Tu, H; Yu, Z; Menzies, T",10.1109/TSE.2020.2986415,2022,"Standard automatic methods for recognizing problematic development commits can be greatly improved via the incremental application of human+artificial expertise. In this approach, called EMBLEM, an AI tool first explore the software development process to label commits that are most problematic. Humans then apply their expertise to check those labels (perhaps resulting in the AI updating the support vectors within their SVM learner). We recommend this human+AI partnership, for several reasons. When a new domain is encountered, EMBLEM can learn better ways to label which comments refer to real problems. Also, in studies with 9 open source software projects, labelling via EMBLEM's incremental application of human+AI is at least an order of magnitude cheaper than existing methods (approximate to eight times). Further, EMBLEM is very effective. For the data sets explored here, EMBLEM better labelling methods significantly improved P(opt)20 and G-scores performance in nearly all the projects studied here."
A post COVID Machine Learning approach in Teaching and Learning methodology to alleviate drawbacks of the e-whiteboards,"Jha, S; Ahmad, S; Abdeljaber, HAM; Hamad, AA; Alazzam, MB",10.6180/jase.202204_25(2).0014,2022,"Deep learning has paved the way for critical and revolutionary applications in almost every field of life in general. Ranging from engineering to healthcare, machine learning and deep learning has left its mark as the state-of-the-art technology application which holds the epitome of a reasonable high benchmarked solution. Incorporating neural network architectures into applications has become a common part of any software development process. In this paper, we perform a comparative analysis on the different transfer learning approaches in the domain of hand-written digit recognition. We use two performance measures, loss and accuracy. We later visualize the different results for the training and validation datasets and reach to a unison conclusion. This paper aims to target the drawbacks of the electronic whiteboard with simultaneous focus on the suitable model selection procedure for the digit recognition problem."
Affective Dynamics and Control in Group Processes,"Hoey, J; SchrÃ¶der, T; Morgan, JH; Rogers, KB; Nagappan, M",10.1145/3279981.3279990,2018,"The computational modeling of groups requires models that connect micro-level with macro-level processes and outcomes. Recent research in computational social science has started from simple models of human behaviour, and attempted to link to social structures. However, these models make simplifying assumptions about human understanding of culture that are often not realistic and may be limiting in their generality. In this paper, we present work on Bayesian affect control theory as a more comprehensive, yet highly parsimonious model that integrates artificial intelligence, social psychology, and emotions into a single predictive model of human activities in groups. We illustrate these developments with examples from an ongoing research project aimed at computational analysis of virtual software development teams."
Extraction of non-functional requirement using semantic similarity distance,"Younas, M; Jawawi, DNA; Ghani, I; Shah, MA",10.1007/s00521-019-04226-5,2020,"Functional and non-functional requirements are important equally in software development. Usually, the requirements are expressed in natural languages. The functional and non-functional requirements are written inter-mixed in software requirement document. The extraction of requirement from the software requirement document is a challenging task. Most of the recent studies adopted a supervised learning approach for the extraction of non-functional requirements. However, there is a drawback of supervised learning such as training of model and retrain if the domain changed. The proposed approach manipulates the textual semantic of functional requirements to identify the non-functional requirements. The semantic similarity is calculated based on co-occurrence of patterns in large human knowledge repositories of Wikipedia. This study finds the similarity distance between the popular indicator keywords and requirement statements to identify the type of non-functional requirement. The proposed approach is applied to PROMISE NFR dataset. The performance of the proposed approach is measured in terms of precision, recall and F-measure. Furthermore, the research applies three pre-processing approaches (traditional, part of speech tagging and word augmentation) to increase the performance of NFR extraction. The proposed approach outperforms the results of existing studies."
Teamwork in Software Development and What Personality Has to Do with It - An Overview,"ZÃ¤hl, PM; Theis, S; Wolf, MR; KÃ¶hler, K",10.1007/978-3-031-35634-6_10,2023,"Due to the increasing complexity of software projects, software development is becoming more and more dependent on teams. The quality of this teamwork can vary depending on the team composition, as teams are always a combination of different skills and personality types. This paper aims to answer the question of how to describe a software development team and what influence the personality of the team members has on the team dynamics. For this purpose, a systematic literature review (n = 48) and a literature search with the AI research assistant Elicit (n = 20) were conducted. Result: A person's personality significantly shapes his or her thinking and actions, which in turn influences his or her behavior in software development teams. It has been shown that team performance and satisfaction can be strongly influenced by personality. The quality of communication and the likelihood of conflict can also be attributed to personality."
Modelling Agile Backlogs as Composable Artifacts to support Developers and Product Owners,"Mosser, S; Pulgar, C; Reinharz, V",10.5381/jot.2022.21.3.a3,2022,"The DevOps paradigm combines (agile) software development and IT operations to deliver high-quality software, thanks to a feedback loop where ops feed devs and vice versa. In this context, a central challenge is to reduce as much as possible the duration of the feedback loop, allowing stakeholders to reduce their time-to-market and release process duration. This paper describes how to model a product backlog (usually expressed as informal user stories in plain text in an agile context) as a queryable graph-based model. This graph is automatically extracted from existing artifacts thanks to natural language processing techniques. Then, developers and product owners can support their iteration planning process by leveraging the model, enacting a short-range impact analysis feedback loop of their planning decisions. The approach considers the iterative and incremental nature of agile methods through the definition of composition operators to incrementally build the models. We have validated this approach on five industrial scenarios, on top of a reference open-source dataset of 22 product backlogs, representing 1,671 user stories."
Using text-to-image generation for architectural design ideation,"Paananen, V; Oppenlaender, J; Visuri, A",10.1177/14780771231222783,2024,"Text-to-image generation has become very popular in various domains requiring creativity. This article investigates the potential of text-to-image generators in supporting creativity during the early stages of the architectural design process. We conducted a laboratory study with 17 architecture students, who developed a concept for a culture center using three popular text-to-image generators: Midjourney, Stable Diffusion, and DALL-E. Through standardized questionnaires and group interviews, we found that image generation could be a meaningful part of the design process when design constraints are carefully considered. Generative tools support serendipitous discovery of ideas and an imaginative mindset, enriching the design process. We identified several challenges of image generators and provided considerations for software development and educators to support creativity and emphasize designers' imaginative mindset. By understanding the limitations and potential of text-to-image generators, architects and designers can leverage this technology in their design process and education, facilitating innovation and effective communication of concepts."
Research and Application of Machine Learning in Automatic Program Generation,"Zhang, XJ; Jiang, Y",10.1049/cje.2020.10.006,2020,"With the development of artificial intelligence, machine learning has been applied in more and more domains. In order to improve the quality and efficiency of software, automatic program generation is becoming a research hotspot. In recent years, machine learning has also been gradually applied in automatic program generation. Decision trees, language models, and cyclic neural networks have been applied in code generation, code completion and code knowledge mining. The efficiency of software development has been improved to a certain extent using machine learning. Aimed at the automatic program generation, this paper analyzes and summarizes the models of machine learning, the modifications involved in the models and the application effects. The research direction is discussed from the aspects of programmer behavior and automatic program generation of machine learning."
Tell Them Apart: Distilling Technology Differences from Crowd-Scale Comparison Discussionsy,"Huang, Y; Chen, CY; Xing, ZC; Lin, T; Liu, Y",10.1145/3238147.3238208,2018,"Developers can use different technologies for many software development tasks in their work. However, when faced with several technologies with comparable functionalities, it is not easy for developers to select the most appropriate one, as comparisons among technologies are time-consuming by trial and error. Instead, developers can resort to expert articles, read official documents or ask questions in Q&A sites for technology comparison, but it is opportunistic to get a comprehensive comparison as online information is often fragmented or contradictory. To overcome these limitations, we propose the diffTech system that exploits the crowdsourced discussions from Stack Overflow, and assists technology comparison with an informative summary of different comparison aspects. We first build a large database of comparable software technologies by mining tags in Stack Overflow, and locate comparative sentences about comparable technologies with NLP methods. We further mine prominent comparison aspects by clustering similar comparative sentences and represent each cluster with its keywords. The evaluation demonstrates both the accuracy and usefulness of our model and we implement a practical website for public use."
An Approach to Support Human-in-the-Loop Big Data Software Development Projects,"Nascimento, N; Alencar, P; Cowan, D",10.1109/BigData52589.2021.9671670,2021,"There is a lack of approaches and tools to support the development of projects in which humans and machines (e.g., machine learning algorithms) need to collaborate to achieve a specified goal. Specifically, given a set of software development tasks to develop a project collaboratively, how can these tasks be assigned to humans or machines to perform each task most efficiently and effectively? Such understanding is essential to support new methodologies for developing human-in-the-loop approaches in which machine learning automated procedures assist software developers in achieving their tasks. This paper describes our work in progress towards providing an approach to guide the assignment of tasks in developing human-in-the-loop big data (science) software development projects. The paper provides several contributions, including the provision of (i) a human-in-the-loop approach for the development of big data software development projects; (ii) the application of the approach to two case studies; (iii) a discussion of implications and research opportunities."
Study on Automatic Defect Report Classification System with Self Attention Visualization,"Hirakawa, R; Tominaga, K; Nakatoh, Y",,2020,"In recent years, software in devices such as smartphones and tablets has become increasingly multifunctional, and the use of OSS has become essential. In software development using large-scale OSS, it is important to report defects to appropriate personnel promptly. In this paper, we propose a method to classifying defect reports into appropriate categories using fine-tuned BERT and visualize self-attention information. In the evaluation, category classification was performed using defect reports of the actual OSS project. The F1 score was 0.87, which indicated that high-accuracy classification was possible. Also, the visualization results show that category-specific words can be extracted."
INNOVATIVE PROPOSALS FOR DATABASE STORAGE AND MANAGEMENT,"Petkov, YI; Chikalanov, AI",10.53656/math2022-1-6-inn,2022,"At present, the problem of storing large data sets as a source of artificial intelligence acquires a geopolitical and strategic character. The most well-known and used type of databases so far are the relational (SQL databases) and nonrelational (No SQL databases. The both approaches have some principle problems, which are described below. That publication presents two original approaches to overcoming some of these shortcomings. First one is Object-oriented model for storing data in a relational database. The second is Storage of non-relational data in a relational database according to previously freely created by the user models. Presented models were used as base for software development of more than ten middle and large size national and European scientific and industrial projects."
Toward an Effective Bug Triage System Using Transformers to Add New Developers,"Zaidi, SFA; Woo, H; Lee, CG",10.1155/2022/4347004,2022,"As defects become more widespread in software development and advancement, bug triaging has become imperative for software testing and maintenance. The bug triage process assigns an appropriate developer to a bug report. Many automated and semiautomated systems have been proposed in the last decade, and some recent techniques have provided direction for developing an effective triage system. However, these techniques still require improvement. Another open challenge related to this problem is adding new developers to the existing triage system, which is challenging because the developers have no listed triage history. This paper proposes a transformer-based bug triage system that uses bidirectional encoder representation from transformers (BERT) for word representation. The proposed model can add a new developer to the existing system without building a training model from scratch. To add new developers, we assumed that new developers had a triage history created by a manual triager or human triage manager after learning their skills from the existing developer history. Then, the existing model was fine-tuned to add new developers using the manual triage history. Experiments were conducted using datasets from well-known large-scale open-source projects, such as Eclipse and Mozilla, and top-k accuracy was used as a criterion for assessment. The experimental outcome suggests that the proposed triage system is better than other word-embedding-based triage methods for the bug triage problem. Additionally, the proposed method performs the best for adding new developers to an existing bug triage system without requiring retraining using a whole dataset."
PatentInspector: An Open-Source Tool for Applied Patent Analysis and Information Extraction,"Petrakis, K; Georgiou, K; Mittas, N; Angelis, L; Branco, F; Martins, J; Mamede, H",10.3390/app132413147,2023,"Featured Application This work concerns a fully functional and deployed framework for patent analysis. The potential applications of this work range from the exploratory analysis and scoping of patent technologies and themes to the discovery of key companies that invest in specific patent domains. In our study, we present an exploration of patents related to human and project management and demonstrate how the developed tool enables the rapid interpretation of the findings.Abstract Patent analysis is a field that concerns the analysis of patent records, for the purpose of extracting insights and trends, and it is widely used in various fields. Despite the abundance of proprietary software employed for this purpose, there is currently a lack of easy-to-use and publicly available software that can offer simple and intuitive visualizations, while advocating for open science and scientific software development. In this study, we attempt to fill this gap by offering PatentInspector, an open-source, public tool that, by leveraging patent data from the United States Trademark and Patent Office, is able to produce descriptive analytics, thematic axes and citation network analysis. The use and interpretability of PatentInspector is illustrated through a use case on human resource management-related patents, highlighting its functionalities. The results indicate that PatentInspector is a practical resource for conducting patent analytics and can be used by individuals with a limited or no background in coding and software development."
Fault-Prone Software Requirements Specification Detection Using Ensemble Learning for Edge/Cloud Applications,"Muhamad, FNJ; Ab Hamid, SH; Subramaniam, H; Rashid, RA; Fahmi, F",10.3390/app13148368,2023,"Ambiguous software requirements are a significant contributor to software project failure. Ambiguity in software requirements is characterized by the presence of multiple possible interpretations. As requirements documents often rely on natural language, ambiguity is a frequent challenge in industrial software construction, with the potential to result in software that fails to meet customer needs and generates issues for developers. Ambiguities arise from grammatical errors, inappropriate language use, multiple meanings, or a lack of detail. Previous studies have suggested the use of supervised machine learning for ambiguity detection, but limitations in addressing all ambiguity types and a lack of accuracy remain. In this paper, we introduce the fault-prone software requirements specification detection model (FPDM), which involves the ambiguity classification model (ACM). The ACM model identifies and selects the optimal algorithm to classify ambiguity in software requirements by employing the deep learning technique, while the FPDM model utilizes Boosting ensemble learning algorithms to detect fault-prone software requirements specifications. The ACM model achieved an accuracy of 0.9907, while the FPDM model achieved an accuracy of 0.9750. To validate the results, a case study was conducted to detect fault-prone software requirements specifications for 30 edge/cloud applications, as edge/cloud-based applications are becoming crucial and significant in the current digital world."
Artificial Intelligence-based Solution for the Prediction for Power Consumption in Electronics and Software Applications,"Savitha, C; Khampariya, P; Singh, KU; Kumar, A; Singh, T; Swarup, C",10.1080/03772063.2022.2131638,2024,"As new computing paradigms such as mobile grids and clouds become more commonplace, mobile devices are becoming increasingly attractive to scientists and HPC users who need high-performance computing capabilities. There are still many challenges to designing software that uses mobile devices, such as their restricted capabilities compared to traditional devices such as PCs and servers. It's even more critical to remember that batteries are the primary power source for mobile devices. Thus people delete programmes that severely reduce their battery life. The upshot is that it is common for developers to have no idea how much power specific hardware and software components use. So even if they have been trained in software development, they still require specific standards and skills to design energy-efficient apps. This Paper investigates ways to minimize mobile device energy usage by restructuring source code. However, we did not overlook the prospect of applying our findings to other domains, such as recurring demanding computational cores in scientific applications. Specifically, this research contributes to the field of Green Computing and mobile software development."
Hidden product knowledge: problems and potential solutions,"Jokinen, L; Leino, SP",10.1016/j.promfg.2020.01.099,2019,"Requirements and constraints form the base for each new design but in the design phase only a limited amount of them is known; more requirements and constraints appear during the later phases of product lifecycle. At the moment, the information that is collected after the main design phase is not often stored in a re-usable way. On the other hand, large amount of decisions made by design engineers during the design phase is either not stored or documented at all. This lack of lifecycle knowledge causes large amount of unnecessary costs in a form of scrapping, re-work and additional work hours, therefore providing a potential for large savings. This paper explains the current situation, the challenges, the problems and the consequences of poor knowledge management. The problem of hidden product knowledge is a complex one and solving it requires changes to processes, tools, and maybe even to ways certain functions in companies are organized and managed. This paper proposes ways to at least partially solve the problem by using the Product Lifecycle Management (PLM) tools and modifying the way engineers work. (C) 2019 The Authors. Published by Elsevier B.V."
The Strategic Advantages of Artificial Intelligence System for Product Design Teams with Diverse Cross-Domain Knowledge,"Hsu, Y; Chaing, YH",10.1007/978-3-030-77074-7_31,2021,"New product development is often promoted and managed by enterprises in the form of projects. A new product development project involves a knowledge-intensive process and a series of complex team-working procedures. Therefore, enterprises can establish new product development process or model through practical experience of projects, which can not only serve as the basis for continuous learning and progress of R&D organizations, but also serve as the benchmark for the management of new product development activities. In this study, Construction Ontology-based NPD Process Recommendation Smart System (ONPS) consistent knowledge base architecture. ONPS assist the company, department quickly build and easy to maintain the body of knowledge; at the same time build a graphical user interface for presenting Find knowledge in knowledge, enhance the efficiency of reuse of knowledge. And with three desktop computers as a case study; the original will-depth interviews and expert designers to take advantage of this study ONPS to build ontologies validation framework; and requested the original expert designers use SUS ease of use in the assessment of verification graphical user interface. The results showed that ONPS is feasible, the corporate sector can help quickly build a structure consistent body of knowledge, reasoning ability and possess the knowledge, easy to maintain, but also have a high degree of scalability."
"Evolving Software to be ML-Driven Utilizing Real-World A/B Testing: Experiences, Insights, Challenges","Li, PL; Chai, XY; Campbell, F; Liao, JL; Abburu, N; Kang, M; Niculescu, I; Brake, G; Patil, S; Dooley, J; Paddock, B",10.1109/ICSE-SEIP52600.2021.00026,2021,"ML-driven software is heralded as the next major advancement in software engineering; existing software today can benefit from being evolved to be NIL-driven. In this paper. we contribute practical knowledge about evolving software to be NW-driven. utilizing real-world A/B testing. We draw on experiences oohing two software features from the Windows operating system to he MI.-driven, with more than ten real-world A/B tests on millions of PCs over more than two wars. We discuss practical reasons for using A/B testing to engineer ML-driven software. insights for success, as well as on-going real-world challenges. This knowledge may help practitioners, as well as help direct future research and innovations."
ZuSE-KI-AVF: Application-Specific AI Processor for Intelligent Sensor Signal Processing in Autonomous Driving,"Thieu, GB; Gesper, S; PayÃ¡-VayÃ¡, G; Riggers, C; Renke, O; Fiedler, T; Marten, J; Stuckenberg, T; Blume, H; Weis, C; Steiner, L; Sudarshan, C; Wehn, N; Reimann, LM; Leupers, R; Beyer, M; KÃ¶hler, D; Jauch, A; Borrmann, JM; Jaberansari, S; Berthold, T; Blawat, M; Kock, M; Schewior, G; Benndorf, J; Kautz, F; Bluethgen, HM; Sauer, C",10.23919/DATE56975.2023.10136978,2023,"Modern and future AI-based automotive applications, such as autonomous driving, require the efficient real-time processing of huge amounts of data from different sensors, like camera, radar, and LiDAR. In the ZuSE-KI-AVF project, multiple university, and industry partners collaborate to develop a novel massive parallel processor architecture, based on a customized RISC-V host processor, and an efficient high-performance vertical vector coprocessor. In addition, a software development framework is also provided to efficiently program AI-based sensor processing applications. The proposed processor system was verified and evaluated on a state-of-the-art UltraScale+ FPGA board, reaching a processing performance of up to 126.9 FPS, while executing the YOLO-LITE CNN on 224x224 input images. Further optimizations of the FPGA design and the realization of the processor system on a 22nm FDSOI CMOS technology are planned."
Enhancing Software Reliability and Fault Detection Using Hybrid Brainstorm Optimization-Based LSTM Model,"Raamesh, L; Jothi, S; Radhika, S",10.1080/03772063.2022.2069603,2023,"An essential attribute of software quality is software reliability. To achieve higher reliability, the testing phase with detected and corrected flaws is incorporated in the software development. The fault correction process (FCP) includes the fault detection process (FDP) to develop the software reliability growth model (SRGM). This is difficult to integrate because due to several reasons, including the effects of staffing levels and the interdependence of faults. It limits the applicability of the analytical model. Because of the adoption of data-driven methodologies such as Artificial Intelligence (AI) technology, no precise FCP and FDP assumptions are necessary. In this article, we proposed a hybrid long short-term memory (LSTM) with BrainStorm Optimization and Late Acceptance Hill Climbing (BSO-LAHC) algorithm of a stepwise prediction model for software fault detection and correction. The fault detection and correction procedure has great influence by considering the testing effort. While compared to the existing methods, the proposed hybrid with the BSO-LAHC algorithm demonstrated superior results by using Firefox and bug tracking system Bugzilla datasets. The proposed model's effectiveness is confirmed via empirical study. Based on the Bugzilla and firefox datasets, the proposed mean square error performance is 1.92 and 21.44 respectively. Additionally, the proposed method is less expensive and takes less time to execute. In Bugzilla version 5.0.4, releases 2 and 3 had a determination coefficient of 99.2% and 98.9%, respectively. The FCP is 27% more effective than previous approaches, and the FDP is 32% more effective."
Classification of Non-functional Requirements Using Convolutional Neural Networks,"GarcÃ­a, SEM; FernÃ¡ndez-y-FernÃ¡ndez, CA; PÃ©rez, EGR",10.1134/S0361768823080133,2023,"The requirements phase is the core of software development, if it is not carried out correctly it can cause its failure. To combat this problem, analysts have used requirements engineering (ER, for its acronym in English), which is characterized by producing a list of quality requirements called requirements specification (RS, for its acronym in English). The SR performs the requirements classification activity, which consists of identifying the class to which each requirement belongs so that analysts face the challenge of classifying them properly. This work is focused on improving the performance of the classification of non-functional requirements (NFR); that is, with the help of a convolutional neural network. It also seeks to show the importance of preprocessing, the implementation of sampling strategies, and the use of previously trained matrices such as Fasttext, Glove, and Word2vec. The results were obtained by evaluating the metrics Recall, Precision, and F1 with an average increase of up to 30% over related work. Finally, the evaluation of the model is presented with respect to the pre-trained matrices with the ANOVA analysis."
A bankruptcy based approach to solving multi-agent credit assignment problem,"Yarahmadi, H; Shiri, ME; Navidi, H; Sharifi, A",10.22075/ijnaa.2021.24783.2825,2021,"Multi-agent systems (MAS) are one of the prominent symbols of artificial intelligence (AI) that, in spite of having smaller entities as agents, have many applications in software development, complex system modeling, intelligent traffic control, etc. Learning of MAS, which is commonly based on Reinforcement Learning (RL), is one of the problems that play an essential role in the performance of such systems in an unknown environment. A major challenge in Multi-Agent Reinforcement Learning (MARL) is the problem of credit assignment in them. In this paper, in order to solve Multi-agent Credit Assignment (MCA) problem, we present a bottom-up method based on the bankruptcy concept for the effective distribution of the credits received from the environment in a MAS so that its performance is increased. In this work, considering the Task Start Threshold (TST) of the agents as a new constraint and a multi-score environment, as well as giving priority to agents of lower TST, three methods PTST, T-MAS and T-KAg are presented, which are based on the bankruptcy concept as a sub branch of game theory. In order to evaluate these methods, seven criteria were used among which density was a new one. The simulation results of the proposed methods indicated that the performance of the proposed methods was enhanced in comparison with those of the existing methods in six parameters while it proved a weaker performance in only one parameter."
Preference discovery from wireless social media data in APIs recommendation,"Xu, YS; Zhang, H; Gao, HH; Song, SL; Yin, YY; Hei, L; Ding, YP; Barroso, RJD",10.1007/s11276-021-02543-z,2021,"In recent years, with the development of software development, a large number of developers develop software by invoking API. With the increasing number of APIs, how to accurately recommend the APIs to developers has become a urgently necessary task. In this paper, we discover that there is a relationship between the user and the API, and use such relationships and collaborative learning techniques to finish APIs recommendation. We propose a holistic framework that contains three models. In the models, we design a joint matrix factorization technique and try to discover the preference among APIs invocation process. In natural language processing, word embedding is widely used. In our models, we use doc2vec to turn the representation of users and APIs into vector representation and calculate the similarity separately to generate the relationships. Besides the two modes in users side and APIs side, we also propose an ensemble model fully leveraging the preference mined from both users side and APIs side. We conduct the experiments on a real-world dataset and the experimental results show that our models perform better than all compared methods."
LineVD: Statement-level Vulnerability Detection using Graph Neural Networks,"Hin, D; Kan, A; Chen, HM; Babar, MA",10.1145/3524842.3527949,2022,"Current machine-learning based software vulnerability detection methods are primarily conducted at the function-level. However, a key limitation of these methods is that they do not indicate the specific lines of code contributing to vulnerabilities. This limits the ability of developers to efficiently inspect and interpret the predictions from a learnt model, which is crucial for integrating machine-learning based tools into the software development workflow. Graph-based models have shown promising performance in function-level vulnerability detection, but their capability for statement-level vulnerability detection has not been extensively explored. While interpreting function-level predictions through explainable AI is one promising direction, we herein consider the statement-level software vulnerability detection task from a fully supervised learning perspective. We propose a novel deep learning framework, LineVD, which formulates statement-level vulnerability detection as a node classification task. LineVD leverages control and data dependencies between statements using graph neural networks, and a transformer-based model to encode the raw source code tokens. In particular, by addressing the conflicting outputs between function-level and statement-level information, LineVD significantly improve the prediction performance without vulnerability status for function code. We have conducted extensive experiments against a large-scale collection of real-world C/C++ vulnerabilities obtained from multiple real-world projects, and demonstrate an increase of 105% in F1-score over the current state-of-the-art."
Enhanced Feature Selection Using Word Embeddings for Self-Admitted Technical Debt Identification,"Flisar, J; Podgorelec, V",10.1109/SEAA.2018.00045,2018,"Technical debt (TD) is a term used to describe a trade off between code quality and timely software release. Since technical debt has negative impact on software development, identification of such debt is an important task in the software engineering domain. Sometimes, technical debt is annotated in source code comments. This kind of debt is referred to as self-admitted technical debt (SATD). Recently, some studies have focused on automated detection and classification of SATD using natural language processing methods. However, these methods have only used manually annotated data to train their classifiers. In this paper, we present the results of a performed exploratory study for using large corpus of unlabeled code comments, extracted from open source projects on git-hub, to train word embeddings, in order to improve detection of SATD. Our approach aims to enhance the feature selection method by taking advantage of the pre-trained word embeddings to detect similar features in source code comments. The experimental results show a significant improvement in SATD classification. With achieved 82% of correct predictions of SATD, the method seems to be a good candidate to be adopted in practice."
A systems approach to a resilience assessment for agility,"Hayes, RB",10.1080/21642583.2022.2148138,2022,"This work proposes a theoretical approach to assessing agility in terms of a modified version of resilience during large-scale crisis to sustain operational reliability. The proposed method could be used on subsystem optimization or eventually scaled up to global interconnectedness enabling decision makers to optimize resource allocation and so obtain resilience and agility in troubling times along with long-term sustained prosperity. Introducing weights to various parameters can also allow customizing outcomes such as insuring equitable outcomes, environmental stewardship and proper response to emergencies or any national crisis. The provided mathematical formalism can then become a decision maker tool to predict corrective action outcomes from various responses to a crisis or alternatively to determine sensitivity and potential risk for a crisis from apparently ambient or slowly changing conditions. Ad-hoc examples are considered to demonstrate the generality of the approach."
Prioritizing User Requirements for Agile Software Development,"Sachdeva, S; Arya, A; Paygude, P; Chaudhary, S; Idate, S",,2018,"With the accelerating rising business needs and also developing technology, user requirements evolve almost every day due to which they cannot be perfected at once. Therefore, agile methodology helps in modeling these user requirements through user stories iteratively and incrementally. However, this can only be done through continuous prioritization. This prioritization is essential for meeting the budgetary constraints and time to market deadlines, and is done by the Product Owner (PO) on the basis of Business Value without considering the dependence of one story on other due to execution order. This dependence can be understood by visualizing the process flow using UML Activity Diagram. The conversion of user requirements into UML Diagram can be done via NLP. Once the UML is ready, it helps to prioritize the stories according to process flow. This paper aims to provide an approach to tackle the issue of prioritization considering both the business value and process flow aspects and help the PO in making decisions about which requirements to be considered first and arranging them in Product Backlog."
An efficient approach for reviewing security-related aspects in agile requirements specifications of web applications,"Villamizar, H; Kalinowski, M; Garcia, A; Mendez, D",10.1007/s00766-020-00338-w,2020,"Defects in requirement specifications can have severe consequences during the software development life cycle. Some of them may result in poor product quality and/or time and budget overrun due to incorrect or missing quality characteristics, such as security. This characteristic requires special attention in web applications because they have become a target for manipulating sensible data. Several concerns make security difficult to deal with. For instance, security requirements are often misunderstood and improperly specified due to lack of security expertise and emphasis on security during early stages of software development. This often leads to unspecified or ill-defined security-related aspects. These concerns become even more challenging in agile contexts, where lightweight documentation is typically produced. To tackle this problem, we designed an approach for reviewing security-related aspects in agile requirements specifications of web applications. Our proposal considers user stories and security specifications as inputs and relates those user stories to security properties via natural language processing. Based on the related security properties, our approach identifies high-level security requirements from the Open Web Application Security Project (OWASP) to be verified and generates a reading technique to support reviewers in detecting defects. We evaluate our approach via three experimental trials conducted with 56 novice software engineers, measuring effectiveness, efficiency, usefulness and ease of use. We compare our approach against using: (1) the OWASP high-level security requirements and (2) a perspective-based approach as proposed in contemporary state of the art. The results strengthen our confidence that using our approach has a positive impact (with large effect size) on the performance of inspectors in terms of effectiveness and efficiency."
Classification and Prioritisation of Software Requirements using Machine Learning - A Systematic Review,"Thiele, P; Phalnikar, R",10.1109/Confluence51648.2021.9377190,2021,"Requirement Engineering (RE) plays an integral role throughout the process of software development. Requirement identification and prioritisation arc the foremost phases of the RE process. Latest RE research work uses Machine Learning (ML) algorithms to tackle RE problems such as identifying requirements and assigning priorities to requirements, which have given better results than that of traditional natural language processing methods. An adequate understanding of these ML methods, however, is still lacking. The aim of this study is to understand which of the ML algorithms is likely to classify and prioritise the requirements efficiently and how they can be evaluated. It is observed that the current approaches are having constraints of scalability and complexity. Different methods used for the text preprocessing of requirements front SRS and user reviews are also proposed. 6 different ML algorithms and 6 different prioritisation algorithms, which are most common methods, are found. The most popular performance parameters used are accuracy, precision and recall. The limitations of these ML approaches arc irrespective of dependency of requirements, priorities are assigned to requirements, the results with respect to scalability and speed is inferior."
"A Cloud-Based Architecture for an Interoperable, Resilient, and Scalable C2 Information System","Bau, N; Endres, S; Gerz, M; GÃ¶kgÃ¶z, F",,2018,"Recent advancements in Cloud Computing pave the way for a new generation of C2 Information Systems (C2IS) that provide better scalability and resilience and improved decision support through big data analytics and artificial intelligence. In this paper, we present a cloud-based system architecture for a C2IS. We explain how the cloud architecture ensures system functioning even when critical components fail temporarily. In our architecture, C2 data are not mapped onto a single data model and persisted in a central data store; instead, the design allows for a variety of distinct stores for different purposes and interoperability standards. The concepts have been implemented in a C2IS prototype at the Fraunhofer FKIE. We illustrate how semantically rich interoperability standards support a model-driven development approach."
Toward a More Robust Home Automated System: Introducing a Voice Activation Technique via Pubnub,"Aina, S; Okegbile, SD; Oluwaranti, AI; Okoro, OB; Obasanya, T",10.4018/IJITWE.2019070105,2019,"The work reported in this article developed a home automated system using voice activation. This is with a view to providing users complete control over electrical appliances using simple easy to remember voice commands on an Android mobile device. This work was implemented using the Atmega 328 microcontroller, Relays and a Wi-Fi shield. The human voice is first converted to text using a Natural language processing tool from the Android based application. Thereafter, the text is sent over the internet via the PubNub to the microcontroller. The Atmega 328 microcontroller was programmed on an Arduino using C programming language and the Android based application was developed using Android Software Development Kit. Results obtained from the testing show that the implemented system achieves the mean scores of 8, 7.6, and 7.2 for ease of use, learnability and effectiveness respectively justifying the fact that the system is capable of controlling appliances by changing their state (ON/OFF) from remote a location with a response time within the reasonable limit."
IntelliCode Compose: Code Generation using Transformer,"Svyatkovskiy, A; Deng, SK; Fu, SY; Sundaresan, N",10.1145/3368089.3417058,2020,"In software development through integrated development environments (IDEs), code completion is one of the most widely used features. Nevertheless, majority of integrated development environments only support completion of methods and APIs, or arguments. In this paper, we introduce IntelliCode Compose - a general-purpose multilingual code completion tool which is capable of predicting sequences of code tokens of arbitrary types, generating up to entire lines of syntactically correct code. It leverages stateof-the-art generative transformer model trained on 1.2 billion lines of source code in Python, C#, JavaScript and TypeScript programming languages. IntelliCode Compose is deployed as a cloud-based web service. It makes use of client-side tree-based caching, efficient parallel implementation of the beam search decoder, and compute graph optimizations to meet edit-time completion suggestion requirements in the Visual Studio Code IDE and Azure Notebook. Our best model yields an average edit similarity of 86.7% and a perplexity of 1.82 for Python programming language."
Deep Neural Network-Based Severity Prediction of Bug Reports,"Ramay, WY; Umer, Q; Yin, XC; Zhu, C; Illahi, I",10.1109/ACCESS.2019.2909746,2019,"Software maintenance is an essential phase of software development. Developers employ issue tracking systems to collect bugs for software improvement. Users submit bugs through such issue tracking systems and decide the severity of reported bugs. The severity is an important attribute of a bug that decides how quickly it should be solved. It helps developers to solve important bugs on time. However, manual severity assessment is a tedious job and could be incorrect. To this end, in this paper, we propose a deep neural network-based automatic approach for the severity prediction of bug reports. First, we apply natural language processing techniques for text preprocessing of bug reports. Second, we compute and assign an emotion score for each bug report. Third, we create a vector for each preprocessed bug report. Forth, we pass the constructed vector and the emotion score of each bug report to a deep neural network based classifier for severity prediction. We also evaluate the proposed approach on the history-data of bug reports. The results of cross-product suggest that the proposed approach outperforms the state-of-the-art approaches. On average, it improves the f-measure by 7.90%."
"The impact of varying doses of moringa leaf methanolic extract supplementation in the cryopreservation media on sperm quality, oxidants, and antioxidant capacity of frozen-thawed ram sperm","El-Seadawy, IE; Kotp, MS; El-Maaty, AMA; Fadl, AM; El-Sherbiny, HR; Abdelnaby, EA",10.1007/s11250-022-03344-y,2022,"To increase rams' post-thaw semen quality following cryopreservation, this study used enriched Tris-based diluent with varying amounts of moringa leaf methanolic extract (MLME). The antioxidant activity, total phenolic, and total flavonoid content were all assessed in MLME. The sperm of five healthy Awassi rams were collected, divided into 4 equal aliquots, and diluted [1:5; (v/v)] in Tris-citrate-glucose extender supplemented with 0.48, 0.56, and 0.64 mg MLME/ml or without MLME supplementation (control). The percentages of sperm total motility (STM, %), sperm progressive motility (SPM, %) and viability (V, %), abnormal morphology (AM, %), membrane functional integrity (MFI, %), and acrosome integrity (AI %) were measured. Malondialdehyde (MDA), nitric oxide (NO), ascorbic acid (AA), superoxide dismutase (SOD), glutathione peroxidase (GPx), total cholesterol (TC), low-density lipoproteins (LDL), lactate dehydrogenase (LDH), alkaline phosphatase (ALP), zinc (Zn), and copper (Cu) were measured. The total phenolic gallic acid and flavonoid catechin (equivalent) contents were 19.78 mg/g and 11.94 mg/g, respectively. 2,2-Diphenyl-1-picrylhydrazyl (34.37 mM TE/g) and 2,2 '-azino-bis/3-ethylbenzothiazoline-6-sulfonic acid (53.47 mM TE/g) were found in MLME. MLME had a 64.59 mM TE/g ferric-reducing power. In comparison to control, the addition of 0.64 mg/ml MLME to Tris-based extender resulted in the highest (P < 0.001) STM (55.22 +/- 0.98), SPM (45.41 +/- .70), SV (60.01 +/- 1.05), MFI (75.23 +/- 0.77), and AI (73.13 +/- 0.72) and the lowest (P < 0.001) AM (21.34 +/- 0.72) values. In comparison to the control, the addition of 0.56 mg/ml semen extender resulted in lower STM, SPM, SV, MFI, and AI with higher AM percentages. MDA (P = 0.03), NO (P = 0.012), CHO (P = 0.0001), and LDL (P = 0.004) were reduced by 0.64 mg/ml MLME, while AA (P = 0.017) and SOD (P = 0.0001) were elevated. In conclusion, the highest copper (P = 0.006) and lowest zinc concentrations in MLME (0.48 mg/ml extender) deteriorated the post-thaw semen quality, prompting us to suggest the addition of 0.64 mg MLME to rams' Tris-based semen extender."
A Framework for an Automated Development Environment to Support the Data-driven Machine Learning Paradigm,"Bowman, AD; Prabhakar, SP; Jololian, L",10.1109/SoutheastCon48659.2022.9764094,2022,"In recent years a machine learning paradigm has emerged, focusing on a data-driven approach as opposed to traditional development. Advances in machine learning techniques have allowed researchers to make substantial gains in tackling complex problems in diverse fields such as medical diagnosis through image analysis, object detection and tracking, and natural language processing. However, often researchers only employ one or two machine learning algorithms with a static feature set while only testing a single hypothesis. This self-imposed bottleneck often produces suboptimal results because it arises from using machine learning within the classical, algorithmic context using existing development tools. Therefore, there is a need to create new development tools which reflect this change to the machine learning paradigm. In this research, we propose a development environment that allows researchers to leverage those capabilities more fully by shifting not only the tool they use but also their mindset. Our proposed environment serves as an intermediate tool, guiding the researcher and making full adoption of the machine learning paradigm throughout the software development process easier. To accomplish this, our framework is defined by a three-layer structure designed for subject domain assessment, data manipulation and feature set exploration. Supported by parallelism, data cleaning and feature engineering, this research provides a conceptual basis for future creation of development environments for the machine learning paradigm. Future development of such a conceptual design would allow for additional intelligent tools to aid the user in designing solutions and support reusability at the design level."
TransRepair: Context-aware Program Repair for Compilation Errors,"Li, XY; Liu, SQ; Feng, RT; Meng, GZ; Xie, XF; Chen, K; Liu, Y",10.1145/3551349.3560422,2022,"Automatically fixing compilation errors can greatly raise the productivity of software development, by guiding the novice or AI programmers to write and debug code. Recently, learning-based program repair has gained extensive attention and became the state-of-the-art in practice. But it still leaves plenty of space for improvement. In this paper, we propose an end-to-end solution TransRepair to locate the error lines and create the correct substitute for a C program simultaneously. Superior to the counterpart, our approach takes into account the context of erroneous code and diagnostic compilation feedback. Then we devise a Transformer-based neural network to learn the ways of repair from the erroneous code as well as its context and the diagnostic feedback. To increase the effectiveness of TransRepair, we summarize 5 types and 74 fine-grained sub-types of compilations errors from two real-world program datasets and the Internet. Then a program corruption technique is developed to synthesize a large dataset with 1,821,275 erroneous C programs. Through the extensive experiments, we demonstrate that TransRepair outperforms the state-of-the-art in both single repair accuracy and full repair accuracy. Further analysis sheds light on the strengths and weaknesses in the contemporary solutions for future improvement."
Automated Labeling and Classification of Business Rules from Software Requirement Specifications,"Anish, PR; Lawhatre, P; Chatterjee, R; Joshi, V; Ghaisas, S",10.1145/3510457.3513071,2022,"Business Rules (BRs) are a critical artifact in the requirements elicitation phase of the software development life cycle. Several taxonomies have been proposed for classification of BRs. In this paper, we utilize Ross's BR classification schema and present an approach to automatically label and classify BRs along this schema. Our approach uses Data Programming (DP) for generating labeled training data needed for training two deep learning-based models to classify the BRs. We obtained an average labeling accuracy of 0.73 for all the BR classes using DP. Upon evaluating the approach on industry-specific dataset, we obtained highest weighted F-score (0.69) with a Bi-LSTM with attention-based model."
Test Case Prioritization Using Firefly Algorithm for Software Testing,"Khatibsyarbini, M; Isa, MA; Jawawi, DNA; Hamed, HNA; Suffian, MDM",10.1109/ACCESS.2019.2940620,2019,"Software testing is a vital and complex part of the software development life cycle. Optimization of software testing is still a major challenge, as prioritization of test cases remains unsatisfactory in terms of Average Percentage of Faults Detected (APFD) and time execution performance. This is attributed to a large search space to find an optimal ordering of test cases. In this paper, we have proposed an approach to prioritize test cases optimally using Firefly Algorithm. To optimize the ordering of test cases, we applied Firefly Algorithm with fitness function defined using a similarity distance model. Experiments were carried on three benchmark programs with test suites extracted from Software-artifact Infrastructure Repository (SIR). Our Test Case Prioritization (TCP) technique using Firefly Algorithm with similarity distance model demonstrated better if not equal in terms of APFD and time execution performance compared to existing works. Overall APFD results indicate that Firefly Algorithm is a promising competitor in TCP applications."
From Commit Message Generation to History-Aware Commit Message Completion,"Eliseeva, A; Sokolov, Y; Bogomolov, E; Golubev, Y; Dig, D; Bryksin, T",10.1109/ASE56229.2023.00078,2023,"Commit messages are crucial to software development, allowing developers to track changes and collaborate effectively. Despite their utility, most commit messages lack important information since writing high-quality commit messages is tedious and time-consuming. The active research on commit message generation (CMG) has not yet led to wide adoption in practice. We argue that if we could shift the focus from commit message generation to commit message completion and use previous commit history as additional context, we could significantly improve the quality and the personal nature of the resulting commit messages. In this paper, we propose and evaluate both of these novel ideas. Since the existing datasets lack historical data, we collect and share a novel dataset called CommitChronicle, containing 10.7M commits across 20 programming languages. We use this dataset to evaluate the completion setting and the usefulness of the historical context for state-of-the-art CMG models and GPT3.5-turbo. Our results show that in some contexts, commit message completion shows better results than generation, and that while in general GPT-3.5-turbo performs worse, it shows potential for long and detailed messages. As for the history, the results show that historical information improves the performance of CMG models in the generation task, and the performance of GPT-3.5-turbo in both generation and completion."
A Review on Source Code Documentation,"Rai, S; Belwal, RC; Gupta, A",10.1145/3519312,2022,Context: Coding is an incremental activity where a developer may need to understand a code before making suitable changes in the code. Code documentation is considered one of the best practices in software development but requires significant efforts from developers. Recent advances in natural language processing and machine learning have provided enough motivation to devise automated approaches for source code documentation at multiple levels. Objective: The review aims to study current code documentation practices and analyze the existing literature to provide a perspective on their preparedness to address the stated problem and the challenges that lie ahead. Methodology: We provide a detailed account of the literature in the area of automated source code documentation at different levels and critically analyze the effectiveness of the proposed approaches. This also allows us to infer gaps and challenges to address the problem at different levels. Findings: (1) The research community focused on method-level summarization. (2) Deep learning has dominated the past five years of this research field. (3) Researchers are regularly proposing bigger corpora for source code documentation. (4) Java and Python are the widely used programming languages as corpus. (5) Bilingual Evaluation Understudy is the most favored evaluation metric for the research persons.
DEFTri: A Few-Shot Label Fused Contextual Representation Learning For Product Defect Triage in e-Commerce,"Mohanty, I",,2022,"Defect Triage is a time-sensitive and critical process in a large-scale agile software development lifecycle for e-commerce. Inefficiencies arising from human and process dependencies in this domain have motivated research in automated approaches using machine learning to accurately assign defects to qualified teams. This work proposes a novel framework for automated defect triage (DEFTri) using fine-tuned state-of-the-art pre-trained BERT on labels fused text embeddings to improve contextual representations from human-generated product defects. For our multi-label text classification defect triage task, we also introduce a Walmart proprietary dataset of product defects using weak supervision and adversarial learning, in a few-shot setting."
Architecture Design and Code Implementation of Road Network Path Search System,"Shen, HM; Pan, HZ",10.1155/2022/4235523,2022,"Path search is a hot issue in computer science and artificial intelligence science. When the user enters the starting point and ending point to be queried in the road network path search system, the system will return the best path to the user. In this paper, the road network path search system that can run and calculate the optimal navigation path to the test data is developed by designing the software architecture through the comprehensive use of database design, programming language, shortest path algorithm, UML diagram, software development model, GIS system source data, and other methods. Based on the software design principles of scalability, flexibility, and pluggability, the design and code in this paper can be practically applied to various systems such as GIS, GPS, logistics robots, unmanned aerial vehicles, and autonomous vehicles to realize their road network path planning and navigation functions."
Survey of Deep Learning for Autonomous Surface Vehicles in Marine Environments,"Qiao, YY; Yin, JX; Wang, W; Duarte, F; Yang, J; Ratti, C",10.1109/TITS.2023.3235911,2023,"Within the next several years, there will be a high level of autonomous technology that will be available for widespread use, which will reduce labor costs, increase safety, save energy, enable difficult unmanned tasks in harsh environments, and eliminate human error. Compared to software development for other autonomous vehicles, maritime software development, especially in aging but still functional fleets, is described as being in a very early and emerging phase. This presents great challenges and opportunities for researchers and engineers to develop maritime autonomous systems. Recent progress in sensor and communication technology has introduced the use of autonomous surface vehicles (ASVs) in applications such as coastline surveillance, oceanographic observation, multi-vehicle cooperation, and search and rescue missions. Advanced artificial intelligence technology, especially deep learning (DL) methods that conduct nonlinear mapping with self-learning representations, has brought the concept of full autonomy one step closer to reality. This article reviews existing work on the implementation of DL methods in fields related to ASV. First, the scope of this work is described after reviewing surveys on ASV developments and technologies, which draws attention to the research gap between DL and maritime operations. Then, DL-based navigation, guidance, control (NGC) systems and cooperative operations are presented. Finally, this survey is completed by highlighting current challenges and future research directions."
Extracting core requirements for software product lines,"Reinhartz-Berger, I; Kemelman, M",10.1007/s00766-018-0307-0,2020,"Software Product Line Engineering (SPLE) is a promising paradigm for reusing knowledge and artifacts among similar software products. However, SPLE methods and techniques require a high up-front investment and hence are profitable if several similar software products are developed. Thus in practice adoption of SPLE commonly takes a bottom-up approach, in which analyzing the commonality and variability of existing products and transforming them into reusable ones (termed core assets) are needed. These time-consuming and error-prone tasks call for automation. The literature partially deals with solutions for early software development stages, mainly in the form of variability analysis. We aim for further creation of core requirements-reusable requirements that can be adapted for different software products. To this end, we introduce an automated extractive method, named CoreReq, to generate core requirements from product requirements written in a natural language. The approach clusters similar requirements, captures variable parts utilizing natural language processing techniques, and generates core requirements following an ontological variability framework. Focusing on cloning scenarios, we evaluated CoreReq through examples and a controlled experiment. Based on the results, we claim that core requirements generation with CoreReq is feasible and usable for specifying requirements of new similar products in cloning scenarios."
Applying a Feature-Oriented Software Development Approach to Model Interaction Diversity,"Gollasch, D; Weber, G",10.1145/3603555.3608539,2023,"This research introduces a novel modelling approach based on methods from feature-oriented software development, aimed at enhancing accessibility and diversity in interactive systems. The method integrates user requirements, particularly accessibility and sensitivity to diversity, into software family development. Utilizing a user model subtree, it allows for customization based on users' needs, constraints, and preferences. A prototypical demonstration is shown through a voice user interface of an assistance robot. Despite an overall satisfying success rate of 96%, results suggest the quality of configuration slightly decreases with an increasing number of user constraints. This innovative approach offers significant potential, especially given the growing need for personalized human-computer interaction in our ageing society. However, it also prompts further research questions, such as its adaptability to non-software family systems and quality of configuration via smart AI models."
Feature Selection and Transfer Learning for Alzheimer's Disease Clinical Diagnosis,"Zhou, K; He, WG; Xu, YH; Xiong, GQ; Cai, J",10.3390/app8081372,2018,"Background and Purpose: A majority studies on diagnosis of Alzheimer's Disease (AD) are based on an assumption: the training and testing data are drawn from the same distribution. However, in the diagnosis of AD and mild cognitive impairment (MCI), this identical-distribution assumption may not hold. To solve this problem, we utilize the transfer learning method into the diagnosis of AD. Methods: The MR (Magnetic Resonance) images were segmented using spm-Dartel toolbox and registrated with Automatic Anatomical Labeling (AAL) atlas, then the gray matter (GM) tissue volume of the anatomical region were computed as characteristic parameter. The information gain was introduced for feature selection. The TrAdaboost algorithm was used to classify AD, MCI, and normal controls (NC) data from Alzheimer's Disease Neuroimaging Initiative (ADNI) database, meanwhile, the knowledge learned from ADNI was transferred to AD samples from local hospital. The classification accuracy, sensitivity and specificity were calculated and compared with four classical algorithms. Results: In the experiment of transfer task: AD to MCI, 177 AD and 40NC subjects were grouped as training data; 245 MCI and 45 remaining NC subjects were combined as testing data, the highest accuracy achieved 85.4%, higher than the other four classical algorithms. Meanwhile, feature selection that is based on information gain reduced the features from 90 to 7, controlled the redundancy efficiently. In the experiment of transfer task: ADNI to local hospital data, the highest accuracy achieved 93.7%, and the specificity achieved 100%. Conclusions: The experimental results showed that our algorithm has a clear advantage over classic classification methods with higher accuracy and less fluctuation."
Improved Prioritization of Software Development Demands in Turkish With Deep Learning-Based NLP,"Tunali, V",10.1109/ACCESS.2022.3167269,2022,"Management of software development demands including bug or defect fixes and new feature or change requests is a crucial part of software maintenance. Failure to prioritize demands correctly might result in inefficient planning and use of resources as well as user or customer dissatisfaction. In order to overcome the difficulty and inefficiency of manual processing, many automated prioritization approaches were proposed in the literature. However, existing body of research generally focused on bug report repositories of open-source software, where textual bug descriptions are in English. Additionally, they proposed solutions to the problem using mostly classical text mining methods and machine learning (ML) algorithms. In this study, we first introduce a demand prioritization dataset in Turkish, which is composed of manually labeled demand records taken from the demand management system of a private insurance company in Turkey. Second, we propose several deep learning (DL) architectures to improve software development demand prioritization. Through an extensive experimentation, we compared the effectiveness of our DL architectures trained with several combinations of different optimizers and activation functions in order to reveal the best combination for demand prioritization in Turkish. We empirically show that DL models can achieve much higher accuracy than classical ML models even with a small amount of training data."
A hybrid approach of process reasoning and artificial intelligence-based intelligent decision system framework for fatigue life of belt grinding,"Zhang, YD; Xiao, GJ; Ma, J; Gao, H; Zhu, B; Huang, Y",10.1007/s00170-023-12597-2,2024,"Belt grinding is widely used as the final step in the fabrication of fatigue-resistant surfaces of nickel-based superalloy components, and fatigue life after grinding is one of the most concerning issues. However, the response mechanism of fatigue life under different grinding parameter excitation conditions is not well understood for a long time. In this study, a system framework of fatigue life prediction for nickel-based superalloy abrasive belt based on process reasoning and artificial intelligence algorithm is proposed. Based on the process reasoning method, the mathematical relationship between grinding parameters and fatigue life is established. The equation is solved by RNN and LSMT algorithms embedded in the system, and the excitation response model of process parameters to fatigue life is obtained. The results show that the prediction accuracy of the system is high. The mean squared error (MSE) of the LSTM algorithm is below 0.0441, and the R-squared can be above 0.9956. In addition, experimental verification has been carried out, the observation of the specimen section shows that the process parameters have an effect on the initiation position, distribution, and crack length of the fatigue crack source, which are related to the stress concentration and residual stress distribution at the depth of the grinding scratches. Furthermore, using Spring Boot framework, an intelligent decision-making system based on this system framework is developed by using java and python."
Statistical API Completion Based on Code Relevance Mining,"Wang, CP; Yang, YX; Liu, H; Kang, L",10.1109/maint.2019.8666939,2019,"While Application Programming Interface (API) enables an easy and flexible software development process, selecting a best-fit API is often non-straightforward in practice due to misunderstanding on the API specification or a complex programming context etc.. Consequently, the API selection has always been time-consuming and error-prone. In recent years, API recommendation systems have been introduced to help developers choose an API automatically, e.g., Eclipse and IntelliJ can generate an internal or user-defined API on the fly. Other research leveraged language models to capture the regularity in API usage and further guide the completion of APIs. While existing approaches provided a general support for API usage, they suffer from the lack of semantic awareness (e.g., Eclipse) and code relevance (e.g., language model based methods). To overcome these limitations, we proposed CRMAC in this paper. The key insight of CRMAC is a combination of a cache language model which learns code regularity from both open-source projects and local projects, as well as a relevance mining engine that identifies similar code to enable a weighted language model training. In our empirical evaluation, CRMAC overwhelmed n-gram approaches, with an improvement of 5.28% in terms of top 10 accuracy. Moreover, over 79% APIs were correctly predicted in the top 10 guesses of CRMAC."
Selection of Significant Metrics for Improving the Performance of Change-Proneness Modules,"Satapathy, SC; Jena, AK; Singh, J; Bilgaiyan, S",10.1007/978-3-030-38006-9_1,2020,"Change-proneness modules are described as the programming parts in the source code which has a high likelihood to modify later on. Change-proneness prediction causes programming analyzers to streamline and focus their testing resources on the modules which have a higher likelihood of modification. Accurate estimation of characteristics such as effort, quality, and risk which are the major concerns for change proneness, is of significant worry in the software life cycle. Regression analysis and the neural network techniques are the commonly used methods for attribute estimation, as per the literature. Chidamber and Kemerer metric (CKJM) suite have been used in this study as an input for model training using neural network with various algorithms to optimal weights and ensemble techniques. These models are validated using five different version of eBay web services. The efficiency of the developed models are computed using three different performance parameters such as AUC, F-Measure, and Accuracy. The information present in the experimental results suggested that the models trained using levenberg marquardt (LM) method achieved better results when compared to the model built using the other classifiers."
Quantum Tree Search with Qiskit,"Wichert, A",10.3390/math10173103,2022,"We indicate the quantum tree search qiskit implementation by popular examples from symbolical artificial intelligence, the 3-puzzle, 8-puzzle and the ABC blocks world. Qiskit is an open-source software development kit (SDK) for working with quantum computers at the level of circuits and algorithms from IBM. The objects are represented by symbols and adjectives. Two principles are presented. Either the position description (adjective) is fixed and the class descriptors moves (is changed) or, in the reverse interpretation, the class descriptor is fixed and the position descriptor (adjective) moves (is changed). We indicate how to decompose the permutation operator that executes the rules by the two principles. We demonstrate that the the branching factor is reduced by Grover's amplification to the square root of the average branching factor and not to the maximal branching factor as previously assumed."
FUZZY LOGIC SYSTEM BASED ON DISSOLVED GAS ANALYSIS AND FURAN ANALYSIS FOR POWER TRANSFORMER FAULT DIAGNOSIS,"Nicola, CI; Nicola, M; Nitu, MC; Aciu, AM",,2019,"This paper presents a diagnosis fuzzy logic software system implemented in LabVIEW, based on DGA (Dissolved Gas Analysis) and are making the correlation with the analysis of furan derivatives concentration for a better accuracy of transformer faults prediction. The developed system allows the values resulting from the laboratory chemical analysis to be entered in a MySQL database, use the fuzzy logic and LabVIEW software development to implement the DGA and furan analysis methods, enters the results in a MySQL database, generates automatic reports, sends emails with reports to default addresses and provides string packages with the results obtained for an OPC UA (OLE Object linking and embedding for Process Control Unified Architecture) server for integration into the associated SCADA (Supervisory Control and Data Acquisition) system of an electrical substation. The validation of the presented system consists in testing on a very large number of power transformers of the Romanian Hydro-Power System. The results have proven the consistency between the software decision and the expert evaluation decision."
Multiclass Classification for Self-Admitted Technical Debt Based on XGBoost,"Chen, X; Yu, DJ; Fan, XL; Wang, L; Chen, J",10.1109/TR.2021.3087864,2022,"In software development, due to the demands from users or the limitations of time and resources, developers tend to adopt suboptimal solutions to achieve quick software development. In such a way, the released software usually involves not-quite-right code that is called technical debt, which will significantly decrease the quality of software and increase the maintenance cost. Recently, the concept of self-admitted technical debt (SATD) is proposed and refers to technical debt that is self-admitted by developers in code comments. Existing studies mainly focus on detecting technical debt by classifying code comments into either SATD or non-SATD. However, different types of SATD has different impacts on software maintenance and needs to be handled by different developers. Therefore, the detected SATD should be further classified so that developers can understand and remove technical debt better. In this article, we propose a new method based on eXtreme Gradient Boosting (XGBoost) to classify SATD into multiple classes. In our approach, we first preprocess the original code comments and adopt the easy data augmentation strategy to overcome the class unbalance problem. Then, chi-square is leveraged to select representative features from the textual feature set. Finally, we apply XGBoost to train a classifier and use the trained classifier to partition each comment into the corresponding class. We experimentally investigate the effectiveness of our approach on a public dataset, including 62 566 code comments from 10 open-source projects. Experimental results show that our approach achieves 56.66% in terms of macroaveraged precision, 59.07% in terms of macroaveraged recall, and 55.77% in terms of macroaveraged F-measure on average, and outperforms the natural language processing based method by 4.98%, 5.32%, and 3.17%, respectively. In addition, the experimental results also demonstrate that the data augmentation strategy is effective in improving the effectiveness of our approach."
Banknote Recognizer: From Theory to Application,"Yung, LY; Xia, MH; Wu, YC",,2018,"In recent years, Deep Convolutional Neural Network (CNN) has demonstrated a robust performance and reaches the state-of-the-art performance in many image processing related tasks, such as object detection, image classification or some natural language processing tasks. However, most of the studies tend to focus on the development of the model architecture design, especially with some standard datasets such as MNIST or ImageNet and only a few implementing the advanced technologies into real-life application [1]. In this study, we further introduce the well-designed model in image classification and demonstrate the designed model applied into a mobile environment. In data preparation, different data collection methods were evaluated and examining different methods for creating a dataset. We further investigate the advantages and restrictions of the mobile neural network model. Results illustrated the consistency of performance when transferring from computer environment to mobile environment. In addition, a real-life product was made based on the theory and investigation by co-operating with local blind society and software development company, forming the first real-time A. I. application for visually impaired with high mobility and built-in neural network model, called Hong Kong Banknote Recognizer."
Optimizing Temporal Capital: How Big Tech Imagines Time as Auditable,"Erickson, I; Wajcman, J",10.1177/00027642221127243,2023,"The belief that technology can be profitably employed to control and manage time has a long history. In this article we show how electronic calendaring systems have become emblematic of the contemporary vision of mastering time, codifying a distinctive quantitative orientation to time. Drawing on interviews with calendar designers at four prominent software development companies, we explore the quest among knowledge workers in Silicon Valley to embed a culture of temporal optimization through the use of calendaring software. Their collective response to this issue reveals that there is a specific kind of technoscientific world being developed: one fixated with solving the problem of time scarcity in contexts organized around maximizing productivity. Furthermore, this world is increasingly embracing the power of predictive data analytics and artificial intelligence. Yet, rather than being the progressive act that many Silicon Valley designers believe they are engaging in, this move toward automating time is the latest in a series of long-standing moral attempts to subject time to a particular brand of rationalization. This orientation to, and valorization of, the fast-paced, full life requires incessant performance on our part and the relentless pursuit of self-enhancement. In other words, positing that time has now become fodder for pattern recognition, we argue that calendaring software configures time events as auditable data that is ripe for accounting in the service of both old and new forms of socially-constructed optimization. We conclude by drawing out the implications of treating time as auditable data, most importantly, that it reinforces asymmetrical relations of power and devalues relations of care."
A Systematic Approach of Dataset definition for a Supervised Machine Learning using NFR Framework,"Marinho, M; Arruda, D; Wanderley, F; Lins, A",10.1109/QUATIC.2018.00024,2018,"Non-functional requirements describe important constraints upon the software development and should therefore be considered and specified as early as possible during the system analysis. Effective elicitation of requirements is arguably among the most important of the resulting recommended RE practices. Recent research has shown that artificial intelligence techniques such as Machine Learning and Text Mining perform the automatic extraction and classification of quality attributes from text documents with relevant results. This paper aims to define a systematic process of dataset generation through NFR Framework catalogues improving the NFR's classification process using Machine Learning techniques. A well-known dataset (Promise) was used to evaluate the precision of our approach reaching interesting results. Regarding to security and performance we obtained a precision and recall ranging between similar to 85% and similar to 98%. And we achievement a F1 above similar to 79% when classified the security, performance and usability together."
Heterogeneous Cross-Project Defect Prediction via Optimal Transport,"Zong, X; Li, GY; Zheng, S; Zou, HT; Yu, HL; Gao, S",10.1109/ACCESS.2023.3241924,2023,"Heterogeneous cross-project defect prediction (HCPDP) aims to learn a prediction model from a heterogeneous source project and then apply the model to a target project. Existing HCPDP works mapped the data of the source and target projects in a common space. However, the pre-defined forms of mapping methods often limit prediction performance and it is difficult to measure the distance between two data instances from different feature spaces. This paper introduced optimal transport (OT) theory for the first time to build the relationship between source and target data distributions, and two prediction algorithms were proposed based on OT theory. In particular, an algorithm based on the entropic Gromov-Wasserstein (EGW) discrepancy was developed to perform the HCPDP model. The proposed EGW model measures the distance between two metric spaces by learning an optimal transfer matrix with the minimum data transfer cost and avoids measuring the distance of two instances of different feature spaces. Then, to improve EGW performance, an EGW+ transport algorithm based on EGW was developed by integrating target labels. Experimental results showed the effectiveness of EGW and EGW+ methods, and proved that our methods can support developers to find the defects in the early phase of software development."
Incentive Design for Crowdsourced Development of Selective AI for Human and Machine Data Processing: A Case Study,"Hayashi, M; Kobayashi, M; Matsubara, M; Amagasa, T; Morishima, A",,2019,"The most typical approach today to data processing which does not have proven algorithms is to first request humans to provide labels to a small set of data and then develop artificial intelligences (AIs) with the data to perform all the remaining tasks. This development is sometimes crowdsourced through platforms such as Kaggle. The approach, however, is not always effective; if the AI does not meet the quality requirement, we may have to give up the development and all the data items have to he done manually. In order to avoid this all-or-nothing situation, selective AI programs that perform tasks which they are confident to do will be effective. This study addresses the problem of designing an incentive structure for crowdsourcing the development of such selective AI programs. This paper shows the results of our real-world experiment with a stair-step incentive structure and the behavior of a worker who developed the AI agent under the incentive. This paper also discusses the limitations of the proposed incentive design."
A software engineering perspective on engineering machine learning systems: State of the art and challenges,"Giray, G",10.1016/j.jss.2021.111031,2021,"Context: Advancements in machine learning (ML) lead to a shift from the traditional view of software development, where algorithms are hard-coded by humans, to ML systems materialized through learning from data. Therefore, we need to revisit our ways of developing software systems and consider the particularities required by these new types of systems. Objective: The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of software engineering (SE) research for engineering ML systems. Method: I performed a systematic literature review (SLR). I systematically selected a pool of 141 studies from SE venues and then conducted a quantitative and qualitative analysis using the data extracted from these studies. Results: The non-deterministic nature of ML systems complicates all SE aspects of engineering ML systems. Despite increasing interest from 2018 onwards, the results reveal that none of the SE aspects have a mature set of tools and techniques. Testing is by far the most popular area among researchers. Even for testing ML systems, engineers have only some tool prototypes and solution proposals with weak experimental proof. Many of the challenges of ML systems engineering were identified through surveys and interviews. Researchers should conduct experiments and case studies, ideally in industrial environments, to further understand these challenges and propose solutions. Conclusion: The results may benefit (1) practitioners in foreseeing the challenges of ML systems engineering; (2) researchers and academicians in identifying potential research questions; and (3) educators in designing or updating SE courses to cover ML systems engineering. (C) 2021 Elsevier Inc. All rights reserved."
SEOSS-Queries - a software engineering dataset for text-to-SQL and question answering tasks,"Tomova, MT; Hofmann, M; MÃ¤der, P",10.1016/j.dib.2022.108211,2022,"Stakeholders of software development projects have various information needs for making rational decisions during their daily work. Satisfying these needs requires substantial knowledge of where and how the relevant information is stored and consumes valuable time that is often not available. Easing the need for this knowledge is an ideal text-to-SQL benchmark problem, a field where public datasets are scarce and needed. We propose the SEOSSQueries dataset consisting of natural language utterances and accompanying SQL queries extracted from previous studies, software projects, issue tracking tools, and through expert surveys to cover a large variety of information need perspectives. Our dataset consists of 1,162 English utterances translating into 166 SQL queries; each query has four precise utterances and three more general ones. Furthermore, the dataset contains 393,086 labeled utterances extracted from issue tracker comments. We provide pre-trained SQLNet and RatSQL baseline models for benchmark comparisons, a replication package facilitating a seamless application, and discuss various other tasks that may be solved and evaluated using the dataset. The whole dataset with paraphrased natural language utterances and SQL queries is hosted at figshare.com/s/75ed49ef01ac2f83b3e2. (C) 2022 The Authors. Published by Elsevier Inc."
Towards adaptive technology in routine mental health care,"Lamo, Y; Mukhiya, SK; Rabbi, F; Aminifar, A; Lillehaug, S; Torresen, J; Pham, M; Cote-Allard, U; Noori, FM; Guribye, F; Inal, Y; Flobakk, E; Wake, JD; Myklebost, S; Lundervold, AJ; Hammar, A; Nordby, E; Kahlon, S; Kenter, R; Sekse, RJ; Griffin, KF; Jakobsen, P; Odegaard, KJ; Skar, YS; Nordgreen, T",10.1177/20552076221128678,2022,"This paper summarizes the information technology-related research findings after 5 years with the INTROducing Mental health through Adaptive Technology project. The aim was to improve mental healthcare by introducing new technologies for adaptive interventions in mental healthcare through interdisciplinary research and development. We focus on the challenges related to internet-delivered psychological treatments, emphasising artificial intelligence, human-computer interaction, and software engineering. We present the main research findings, the developed artefacts, and lessons learned from the project before outlining directions for future research. The main findings from this project are encapsulated in a reference architecture that is used for establishing an infrastructure for adaptive internet-delivered psychological treatment systems in clinical contexts. The infrastructure is developed by introducing an interdisciplinary design and development process inspired by domain-driven design, user-centred design, and the person based approach for intervention design. The process aligns the software development with the intervention design and illustrates their mutual dependencies. Finally, we present software artefacts produced within the project and discuss how they are related to the proposed reference architecture. Our results indicate that the proposed development process, the reference architecture and the produced software can be practical means of designing adaptive mental health care treatments in correspondence with the patients' needs and preferences. In summary, we have created the initial version of an information technology infrastructure to support the development and deployment of Internet-delivered mental health interventions with inherent support for data sharing, data analysis, reusability of treatment content, and adaptation of intervention based on user needs and preferences."
DECODER - DEveloper COmpanion for Documented and annotatEd code Reference,"Torres, V; Gil, M; Pelechano, V",10.1007/978-3-030-35333-9_44,2019,"Software is everywhere and the productivity of Software Engineers has increased radically with the advent of new specifications, design and programming paradigms and languages. The main objective of the DECODER project is to introduce radical solutions to increase productivity by increasing the abstraction level, at specification stage, using requirements engineering techniques to integrate more complete specifications into the development process, and formal methods to reduce the time and efforts for integration testing. DECODER project will develop a methodology and tools to improve the productivity of the software development process for medium-criticality applications in the domains of IoT, Cloud Computing, and Operating Systems by combining Natural Language Processing techniques, modelling techniques and Formal Methods. A radical improvement is expected from the management and transformation of informal data into material (herein called knowledge) that can be assimilated by any party involved in a development process. The project expects an average benefit of 20% in terms of efforts on several use cases belonging to the beforehand mentioned domains and will provide recommendations on how to generalize the approach to other medium-critical domains."
Vagus nerve stimulation promotes resolution of inflammation by a mechanism that involves Alox15 and requires the Î±7nAChR subunit,"Caravaca, AS; Gallina, AL; Tarnawski, L; Shavva, VS; Colas, RA; Dalli, J; Malin, SG; Hult, H; Arnardottir, H; Olofsson, PS",10.1073/pnas.2023285119,2022,"Nonresolving inflammation underlies a range of chronic inflammatory diseases, and therapeutic acceleration of resolution of inflammation may improve outcomes. Neural reflexes regulate the intensity of inflammation (for example, through signals in the vagus nerve), but whether activation of the vagus nerve promotes the resolution of inflammation in vivo has been unknown. To investigate this, mice were subjected to electrical vagus nerve stimulation (VNS) or sham surgery at the cervical level followed by zymosan-induced peritonitis. The duration of inflammation resolution was significantly reduced and efferocytosis was significantly increased in mice treated with VNS as compared with sham. Lipid mediator (LM) metabololipidomics revealed that mice treated with VNS had higher levels of specialized proresolving mediators (SPMs), particularly from the omega-3 docosahexaenoic (DHA) and docosapentaenoic (n-3 DPA) metabolomes, in peritoneal exudates. VNS also shifted the ratio between proinflammatory and proresolving LMs toward a proresolving profile, but this effect by VNS was inverted in mice deficient in 12/15-lipoxgenase (Alox15), a key enzyme in this SPM biosynthesis. The significant VNS-mediated reduction of neutrophil numbers in peritoneal exudates was absent in mice deficient in the cholinergic alpha 7-nicotinic acetylcholine receptor subunit (alpha 7nAChR), an essential component of the inflammatory reflex. Thus, VNS increased local levels of SPM and accelerated resolution of inflammation in zymosan-induced peritonitis by a mechanism that involves Alox15 and requires the alpha 7nAChR."
Automated Identification of Toxic Code Reviews Using ToxiCR,"Sarker, J; Turzo, AK; Dong, M; Bosu, A",10.1145/3583562,2023,"Toxic conversations during software development interactions may have serious repercussions on a Free and Open Source Software (FOSS) development project. For example, victims of toxic conversations may become afraid to express themselves, therefore get demotivated, and may eventually leave the project. Automated filtering of toxic conversations may help a FOSS community maintain healthy interactions among its members. However, off-the-shelf toxicity detectors perform poorly on a software engineering dataset, such as one curated from code review comments. To counter this challenge, we present ToxiCR, a supervised learning based toxicity identification tool for code review interactions. ToxiCR includes a choice to select one of the 10 supervised learning algorithms, an option to select text vectorization techniques, eight preprocessing steps, and a large-scale labeled dataset of 19,651 code review comments. Two out of those eight preprocessing steps are software engineering domain specific. With our rigorous evaluation of the models with various combinations of preprocessing steps and vectorization techniques, we have identified the best combination for our dataset that boosts 95.8% accuracy and an 88.9% F1-score in identifying toxic texts. ToxiCR significantly outperforms existing toxicity detectors on our dataset. We have released our dataset, pre-trainedmodels, evaluation results, and source code publicly, which is available at https://github.com/WSU- SEAL/ToxiCR."
Knowledge Graph based Explainable Question Retrieval for Programming Tasks,"Liu, MW; Yu, SM; Peng, X; Du, XY; Yang, TY; Xu, HJ; Zhang, GY",10.1109/ICSME58846.2023.00023,2023,"Developers often seek solutions for their programming problems by retrieving existing questions on technical Q&A sites such as Stack Overflow. In many cases, they fail to find relevant questions due to the knowledge gap between the questions and the queries or feel it hard to choose the desired questions from the returned results due to the lack of explanations about the relevance. In this paper, we propose KGXQR, a knowledge graph based explainable question retrieval approach for programming tasks. It uses BERT-based sentence similarity to retrieve candidate Stack Overflow questions that are relevant to a given query. To bridge the knowledge gap and enhance the performance of question retrieval, it constructs a software development related concept knowledge graph and trains a question relevance prediction model to re-rank the candidate questions. The model is trained based on a combined sentence representation of BERT-based sentence embedding and graph-based concept embedding. To help understand the relevance of the returned Stack Overflow questions, KGXQR further generates explanations based on the association paths between the concepts involved in the query and the Stack Overflow questions. The evaluation shows that KGXQR outperforms the baselines in terms of accuracy, recall, MRR, and MAP and the generated explanations help the users to find the desired questions faster and more accurately."
Transfer Convolutional Neural Network for Cross-Project Defect Prediction,"Qiu, SJ; Xu, H; Deng, JH; Jiang, SY; Lu, L",10.3390/app9132660,2019,"Cross-project defect prediction (CPDP) is a practical solution that allows software defect prediction (SDP) to be used earlier in the software lifecycle. With the CPDP technique, the software defect predictor trained by labeled data of mature projects can be applied for the prediction task of a new project. Most previous CPDP approaches ignored the semantic information in the source code, and existing semantic-feature-based SDP methods do not take into account the data distribution divergence between projects. These limitations may weaken defect prediction performance. To solve these problems, we propose a novel approach, the transfer convolutional neural network (TCNN), to mine the transferable semantic (deep-learning (DL)-generated) features for CPDP tasks. Specifically, our approach first parses the source file into integer vectors as the network inputs. Next, to obtain the TCNN model, a matching layer is added into convolutional neural network where the hidden representations of the source and target project-specific data are embedded into a reproducing kernel Hilbert space for distribution matching. By simultaneously minimizing classification error and distribution divergence between projects, the constructed TCNN could extract the transferable DL-generated features. Finally, without losing the information contained in handcrafted features, we combine them with transferable DL-generated features to form the joint features for CPDP performing. Experiments based on 10 benchmark projects (with 90 pairs of CPDP tasks) showed that the proposed TCNN method is superior to the reference methods."
An Intelligent DevOps Platform Research and Design Based on Machine Learning,"Wang, ZQ; Shi, MY; Li, CF",10.1109/CBD51900.2020.00017,2020,"With the continuous deepens and expansion of IT business based on AI, machine learning and blockchain technologies, there are many developments in intelligent communication and Internet industries. Matured IT business cause daily DevOps (Development & Operations) works must deal with huge amounts of data. What the trickier work gradually emerged is that these data have complex sources, various formats, and other issues. Efficient and inexpensive DevOps of computer software and hardware systems become an important task which needs to be resolved. In SLC (Software Life Cycle), DevOps occupies more than half proportion. It impact entire IT business reflected in the business overall control, business risk control, and business cost control. In order to improve the efficiency of DevOps engineers and ensure the high-quality intelligence level of DevOps, this project starts with the DevOps theoretical framework, use machine learning method to do research, and design an intelligent DevOps platform, which can help engineers analyze huge amounts of multifarious system alarms, promotes the development of DevOps in the direction of informatization."
EDA for Domain Specific Computing An Introduction for the Panel,"Jiang, IHR; Chinnery, D",10.1145/3569052.3580221,2023,"This panel explores domain-specific computing from hardware, software, and electronic design automation (EDA) perspectives. Hennessey and Patterson signaled a new golden age of computer architecture in 2018 [1]. Process technology advances and general-purpose processor improvements provided much faster and more efficient computation, but scaling with Moore's law has slowed significantly. Domain-specific customization can improve power-performance efficiency by orders-of-magnitude for important application domains, such as graphics, deep neural networks (DNN) for machine learning [2], simulation, bioinformatics [3], image processing, and many other tasks. The common features of domain-specific architectures are: 1) dedicated memories to minimize data movement across chip; 2) more arithmetic units or bigger memories; 3) use of parallelism matching the domain; 4) smaller data types appropriate for the target applications; and 5) domain-specific software languages. Expediting software development with optimized compilation for efficient fast computation on heterogeneous architectures is a difficult task, and must be considered with the hardware design. For example, GPU programming has used CUDA and OpenCL. The hardware comprises application-specific integrated circuits (ASICs) [4] and systems-of-chips (SoCs). General-purpose processor cores are often combined with graphics processing units (GPUs) for stream processing, digital signal processors, field programmable gate arrays (FPGAs) for configurability [5], artificial intelligence ( AI) acceleration hardware, and so forth. Domain-specific computers have been deployed recently. For example: the Google Tensor Processing Unit (DNN ASIC) [6]; Microsoft Catapult (FPGA-based cloud domain-service solution) [7]; Intel Crest (DNN ASIC) [8]; Google Pixel Visual Core (image processing and computer vision for cell phones and tablets) [9]; and the RISC-V architecture and open instruction set for heterogeneous computing [10]."
Using BiLSTM with attention mechanism to automatically detect self-admitted technical debt,"Yu, DJ; Wang, L; Chen, X; Chen, J",10.1007/s11704-020-9281-z,2021,"Technical debt is a metaphor for seeking short-term gains at expense of long-term code quality. Previous studies have shown that self-admitted technical debt, which is introduced intentionally, has strong negative impacts on software development and incurs high maintenance overheads. To help developers identify self-admitted technical debt, researchers have proposed many state-of-the-art methods. However, there is still room for improvement about the effectiveness of the current methods, as self-admitted technical debt comments have the characteristics of length variability, low proportion and style diversity. Therefore, in this paper, we propose a novel approach based on the bidirectional long short-term memory (BiLSTM) networks with the attention mechanism to automatically detect self-admitted technical debt by leveraging source code comments. In BiLSTM, we utilize a balanced cross entropy loss function to overcome the class unbalance problem. We experimentally investigate the performance of our approach on a public dataset including 62, 566 code comments from ten open source projects. Experimental results show that our approach achieves 81.75% in terms of precision, 72.24% in terms of recall and 75.86% in terms of F1-score on average and outperforms the state-of-the-art text mining-based method by 8.14%, 5.49% and 6.64%, respectively."
Clustering Crowdsourced Test Reports of Mobile Applications Using Image Understanding,"Liu, D; Feng, Y; Zhang, XF; Jones, JA; Chen, ZY",10.1109/TSE.2020.3017514,2022,"Crowdsourced testing has been widely used to improve software quality as it can detect various bugs and simulate real usage scenarios. Crowdsourced workers perform tasks on crowdsourcing platforms and present their experiences as test reports, which naturally generates an overwhelming number of test reports. Therefore, inspecting these reports becomes a time-consuming yet inevitable task. In recent years, many text-based prioritization and clustering techniques have been proposed to address this challenge. However, in mobile testing, test reports often consist of only short test descriptions but rich screenshots. Compared with the uncertainty of textual information, well-defined screenshots can often adequately express the mobile application's activity views. In this paper, by employing image-understanding techniques, we propose an approach for clustering crowdsourced test reports of mobile applications based on both textual and image features to assist the inspection procedure. We employ Spatial Pyramid Matching (SPM) to measure the similarity of the screenshots and use the natural-language-processing techniques to compute the textual distance of test reports. To validate our approach, we conducted an experiment on 6 industrial crowdsourced projects that contain more than 1600 test reports and 1400 screenshots. The results show that our approach is capable of outperforming the baselines by up to 37 percent regarding the APFD metric. Further, we analyze the parameter sensitivity of our approach and discuss the settings for different application scenarios."
Study on Virtual Experience Marketing Model Based on Augmented Reality: Museum Marketing (Example),"Zhu, Y; Wang, C",10.1155/2022/2485460,2022,"With the development of emerging digital technologies such as Augmented Reality and Artificial Intelligence, Augmented Reality (AR) technology-enabled experience marketing model can bring brand new virtual experience to the users, improve the brand attitudes of users, and increase the use and purchase intention of users. Based on the theoretical basis of experience marketing and AR, the AR application of Guilin Museum was designed and developed by using Unity as the software development tool and using AR Foundation as the AR development framework. The implementation of this application was mainly based on face detection and tracking, image detection, and tracking in the underlying API of AR Foundation. Subsequently, an AR virtual experience marketing model was constructed based on the Schmitt strategic experience module, and the usage data of AR applications were collected. Furthermore, the collected data were analyzed and evaluated using SPSS and AMOS software, and the relationships and influences of sensory experience, emotional experience, thinking experience, action experience, and association experience on the brand attitudes of users and use intention and purchase intention in AR application were tested."
An Empirical Analysis on Just-In-Time Defect Prediction Models for Self-driving Software Systems,"Choi, J; Manikandan, S; Ryu, D; Baik, J",10.1007/978-3-031-25380-5_3,2023,"Just-in-time (JIT) defect prediction has been used to predict whether a code change is defective or not. Existing JIT prediction has been applied to different kind of open-source software platform for cloud computing, but JIT defect prediction has never been applied in self-driving software. Unlike other software systems, self-driving system is an AI-enabled system and is a representative system to which edge cloud service is applied. Therefore, we aim to identify whether the existing JIT defect prediction models for traditional software systems also work well for self-driving software. To this end, we collect and label the dataset of open-source self-driving software project using SZZ (Sliwerski, Zimmermann and Zeller) algorithm. And we select four traditional machine learning methods and state-of-the-art research (i.e., JIT-Line) as our baselines and compare their prediction performance. Our experimental results show that JITLine and logistic regression produce superior performance, however, there exists a room to be improved. Through XAI (Explainable AI) analysis it turned out that the prediction performance is mainly affected by experience and history-related features among change-level metrics. Our study is expected to provide important insight for practitioners and subsequent researchers performing defect prediction in AI-enabled system."
Optimization of Favourable Test Path Sequences Using Bio-Inspired Natural River System Algorithm,"Rathee, N; Chhillar, RS",10.4018/JITR.2021040105,2021,"Testing of software requires a great amount of time and effort. The tester's main aim is to design optimized test sequences with a minimum amount of time, effort, and with less redundancy. Testers have used artificial intelligence meta-heuristic algorithms for optimization of test sequences. The model-driven approach is helpful in the generation of test sequences at early designing phase only. The model-driven approach uses UML diagram to represent the system's behavior and design test cases for the system at design stage of software development life cycle. The proposed approach uses natural river system for optimizing favourable non-redundant test path sequences using UML activity diagrams and sequence diagrams. The implementation of proposed approach has been done using python and results show that the proposed approach provides full coverage of test paths with less redundant test nodes compared to other meta heuristic algorithms."
Medium Voltage Solid State Transformer for Extreme Fast Charging Applications,"Awal, MA; Montes, OA; Teng, F; Wang, DK; Bipu, MRH; Yu, WS; Lukic, S; Husain, I",10.1109/APEC43580.2023.10131285,2023,"A modular and scalable solid state transformer (SST) with direct medium voltage (MV) AC connectivity is proposed to enable DC extreme fast charging (XFC) of electric vehicles. Single-phase-modules (SPMs), each consisting of an active-front-end (AFE) stage and an isolated DC-DC stage, are connected in input-series-output-parallel (ISOP) configuration. The modular hardware is co-designed with decentralized control of the DC-DC stages where voltage and power balancing are achieved by each SPM using only its local sensor feedback; a centralized controller (CC) regulates the low voltage (LV) DC bus through the AFE stages without any sensor feedback form the SPMs. The controller architecture contrasts sharply with the prior art for MV AC to LV DC SSTs where highspeed bidirectional communication among SPMs and a CC are required for module-level voltage and power balancing, which severely limits the scalability and practical realization of higher voltage and higher power units. Detailed small-signal analysis and controller design guidelines are developed. Furthermore, a soft start-up strategy is presented. The proposed converter and control structure are validated through simulation and experimental results."
Personalized API Recommendations,"Yang, WH; Zhou, Y; Huang, ZQ",10.1142/S021819402150042X,2021,"Application Programming Interfaces (APIs) play an important role in modern software development. Developers interact with APIs on a daily basis and thus need to learn and memorize those APIs suitable for implementing the required functions. This can be a burden even for experienced developers since there exists a mass of available APIs. API recommendation techniques focus on assisting developers in selecting suitable APIs. However, existing API recommendation techniques have not taken the developers personal characteristics into account. As a result, they cannot provide developers with personalized API recommendation services. Meanwhile, they lack the support for self-defined APIs in the recommendation. To this end, we aim to propose a personalized API recommendation method that considers developers' differences. Our API recommendation method is based on statistical language. We propose a model structure that combines the N-gram model and the long short-term memory (LSTM) neural network and train predictive models using API invoking sequences extracted from GitHub code repositories. A general language model trained on all sorts of code data is first acquired, based on which two personalized language models that recommend personalized library APIs and self-defined APIs are trained using the code data of the developer who needs personalized services. We evaluate our personalized API recommendation method on real-world developers, and the experimental results show that our approach achieves better accuracy in recommending both library APIs and self-defined APIs compared with the state-of-the-art. The experimental results also confirm the effectiveness of our hybrid model structure and the choice of the LSTM's size."
Exploratory analysis on drivers and barriers to Canadian prairie agricultural technology innovation and adoption,"Lassoued, R; Phillips, PWB; Smyth, SJ",10.1016/j.atech.2023.100257,2023,"Emerging technologies are transforming agriculture from a labour-intensive to a technology-intensive sector that meets current and future societal needs without compromising environmental integrity. Using primary data collected through a survey, this study sheds light on the development and commercialization of advanced agricultural technologies in the Canadian Prairies by investigating the objectives, drivers, barriers, co-operation and funding for innovation activities. Results show that Prairie agtech enterprises have undertaken various innovation activities ranging from internal R&D, creative work, software development, to personal training. Those activities are financed mainly with internal funds but also trigger some public subsidies. Not only are customers, educational institutions and suppliers key valued cooperation partners, but also the greatest sources of information when it comes to innovation. Research facilities, including research parks, incubators and accelerators are identified as concurrent drivers for innovation. This exploratory analysis illustrates the opportunities and potential challenges agtech enterprises face during the technology commercialization process."
An Automated Post-Mortem Analysis of Vulnerability Relationships using Natural Language Word Embeddings,"Meyers, BS; Meneely, A",10.1016/j.procs.2021.04.018,2021,"The daily activities of cybersecurity experts and software engineers-code reviews, issue tracking, vulnerability reporting-are constantly contributing to a massive wealth of security-specific natural language. In the case of vulnerabilities, understanding their causes, consequences, and mitigations is essential to learning from past mistakes and writing better, more secure code in the future. Many existing vulnerability assessment methodologies, like CVSS, rely on categorization and numerical metrics to glean insights into vulnerabilities, but these tools are unable to capture the subtle complexities and relationships between vulnerabilities because they do not examine the nuanced natural language artifacts left behind by developers. In this work, we want to discover unexpected relationships between vulnerabilities with the goal of improving upon current practices for post-mortem analysis of vulnerabilities. To that end, we trained word embedding models on two corpora of vulnerability descriptions from Common Vulnerabilities and Exposures (CVE) and the Vulnerability History Project (VHP), performed hierarchical agglomerative clustering on word embedding vectors representing the overall semantic meaning of vulnerability descriptions, and derived insights from vulnerability clusters based on their most common bigrams. We found that (1) vulnerabilities with similar consequences and based on similar weaknesses are often clustered together, (2) clustering word embeddings identified vulnerabilities that need more detailed descriptions, and (3) clusters rarely contained vulnerabilities from a single software project. Our methodology is automated and can be easily applied to other natural language corpora. We release all of the corpora, models, and code used in our work. (C) 2021 The Authors. Published by Elsevier B.V."
Ontology-Based Verification of UML Class/OCL Model,"Hafeez, A; Musavi, SHA; Rehman, AU",10.22581/muet1982.1804.07,2018,"Software models describe structures, relationships and features of the software. Modern software development methodologies such as MDE (Model Driven Engineering) use models as core elements. In MDE, the code is automatically generated from the model and model errors can implicitly shift into the code, which are difficult to find and fix. Model verification is a promising solution to this problem. However, coverage of all facets of model verification is a painful job and existing formal/semi-formal verification methods are greatly inspired by mathematics and difficult to understand by the software practitioners. This work considers particularly UML Class/OCL (Unified Modeling Language Class/Object Constraint Language) model and presents an ontology-based verification method. In the proposed method, a class diagram is transformed into ontology specified in OWL (Web Ontology Language) and constraints into SPARQL NAF (Negation as Failure) queries. This work tries to demonstrate that the proposed approach can efficiently cover all aspects of UML Class/OCL model verification."
DeepVS: an efficient and generic approach for source code modelling usage,"Hussain, Y; Huang, ZQ; Zhou, Y; Wang, SZ",10.1049/el.2020.0500,2020,"The source code suggestions provided by current integrated development environment (IDEs) are mostly dependent on static type learning. These suggestions often end up proposing irrelevant suggestions for a peculiar context. Recently, deep learning-based approaches have shown great potential in the modelling of source code for various software engineering tasks. However, these techniques lack adequate generalisation and resistance to acclimate the use of such models in a real-world software development environment. This Letter presents DeepVS, an end-to-end deep neural code completion tool that learns from existing codebases by exploiting the bidirectional gated recurrent unit (BiGRU) neural net. The proposed tool is capable of providing source code suggestions instantly in an IDE by using pre-trained BiGRU neural net. The evaluation of this work is two-fold, quantitative and qualitative. Through extensive evaluation on ten real-world open-source software systems, the proposed method shows significant performance enhancement and its practicality. Moreover, the results also suggest that DeepVS tool is capable of suggesting zero-day (unseen) code tokens by learning coding patterns from real-world software systems."
Intelligent Software Mining with Business Intelligence Tools for Automation of Micro services in SOA: A Use Case for Analytics,"Wangoo, DP",10.23919/indiacom49435.2020.9083682,2019,"Business Intelligence as a platform is of great assistance to software engineers for analyzing and amassing data to search for information through queries. Object oriented business engineering advances reusability by considering reusable components at the business level. Systematic reuse is the major factor governing the success of an object -oriented software business. Software mining or mining of software engineering data is the successful mining of software engineering data. The integration of mining of software engineering data and Business Intelligence tools gives rise to automation of software services at business level. Automation is the key to Industry 4.0 that we witness today. Therefore, automation of software intelligence is the need of the hour for service delivery. Today's cloud -based applications are working on delivery of Anything as a Service (XaaS) that accounts for provision of anything as a service like database, software, microservices, security on the cloud platform. Service-oriented Architecture (SoA) is based on Software as a Service (SaaS) of the cloud computing. Software intelligence is the form of artificial intelligence which automates the process of mining software engineering data for useful business applications. Not only it integrates the amalgamation of the above fields but at the same time associates the field with industry standards. The aim of this paper is to analyze the various Business Intelligence tools available for promoting intelligence at the business level with benefits to the SoA services particularly microservices for the cloud platform. With the analysis the goal of Business Oriented Software Engineering is enhanced with the analysis and proper usage of business intelligence tools at all the levels of software development process."
Auto-Logging: AI-centred Logging Instrumentation,"Bogatinovski, J; Kao, O",10.1109/ICSE-NIER58687.2023.00023,2023,"Logging in software development plays a crucial role in bug-fixing, maintaining the code and operating the application. Logs are hints created by human software developers that aim to help human developers and operators in identifying root causes for application bugs or other misbehaviour types. They also serve as a bridge between the Devs and the Ops, allowing the exchange of information. The rise of the DevOps paradigm with the CI/CD pipelines led to a significantly higher number of deployments per month and consequently increased the logging requirements. In response, AI-enabled methods for IT operation (AIOps) are introduced to automate the testing and run-time fault tolerance to a certain extent. However, using logs tailored for human understanding to learn (automatic) AI methods poses an ill-defined problem: AI algorithms need no hints but structured, precise and indicative data. Until now, AIOps researchers adapt the AI algorithms to the properties of the existing human-centred data (e.g., log sentiment), which are not always trivial to model. By pointing out the discrepancy, we envision that there exists an alternative approach: the logging can be adapted such that the produced logs are better tailored towards the strengths of the AI-enabled methods. In response, in this vision paper, we introduce auto-logging, which devises the idea of how to automatically insert log instructions into the code that can better suit AI-enabled methods as end-log consumers."
Blockchained Adaptive Federated Auto MetaLearning BigData and DevOps CyberSecurity Architecture in Industry 4.0,"Demertzis, K; Iliadis, L; Pimenidis, E; Tziritas, N; Koziri, M; Kikiras, P",10.1007/978-3-030-80568-5_29,2021,"Maximizing the production process in modern industry, as proposed by Industry 4.0, requires extensive use of Cyber-Physical Systems (CbPS). Artificial intelligence technologies, through CbPS, allow monitoring of natural processes, making autonomous, decentralized and optimal decisions. Collection of information that optimizes the effectiveness of decisions, implies the need for big data management and analysis. This data is usually coming from heterogeneous sources and it might be non-interoperable. Big data management is further complicated by the need to protect information, to ensure business confidentiality and privacy, according to the recent General Data Protection Regulation - GDPR. This paper introduces an innovative holistic Blockchained Adaptive Federated Auto Meta Learning Big Data and DevOps Cyber Security Architecture in Industry 4.0. The aim is to fill the gap found in the ways of handling and securing industrial data. This architecture, combines the most modern software development technologies under an optimal and efficient framework. It successfully achieves the prediction and assessment of threat-related conditions in an industrial ecosystem, while ensuring privacy and secrecy."
"A Survey on Information and Communication Technologies for Industry 4.0: State-of-the-Art, Taxonomies, Perspectives, and Challenges","Aceto, G; Persico, V; PescapÃ©, A",10.1109/COMST.2019.2938259,2019,"A new industrial revolution is undergoing, based on a number of technological paradigms. The will to foster and guide this phenomenon has been summarized in the expression Industry 4.0 (I4.0). Initiatives under this term share the vision that many key technologies underlying Cyber-Physical Systems and Big Data Analytics are converging to a new distributed, highly automated, and highly dynamic production network, and that this process needs regulatory and cultural advancements to effectively and timely develop. In this work, we focus on the technological aspect only, highlighting the unprecedented complexity of I4.0 emerging from the scientific literature. While previous works have focused on one or up to four related enablers, we consider ten technological enablers, including besides the most cited Big Data, Internet of Things, and Cloud Computing, also others more rarely considered as Fog and Mobile Computing, Artificial Intelligence, Human-Computer Interaction, Robotics, down to the often overlooked, very recent, or taken for granted Open-Source Software, Blockchain, and the Internet. For each we explore the main characteristics in relation to I4.0 and its interdependencies with other enablers. Finally we provide a detailed analysis of challenges in leveraging each of the enablers in I4.0, evidencing possible roadblocks to be overcome and pointing at possible future directions of research. Our goal is to provide a reference for the experts in some of the technological fields involved, for a reconnaissance of integration and hybridization possibilities with other fields in the endeavor of I4.0, as well as for the laymen, for a high-level grasp of the variety (and often deep history) of the scientific research backing I4.0."
Analysis of teaching reform mode based on cognitive computing system - An example of dragon boat teaching,"Liu, PH; He, SB",10.1016/j.cogsys.2018.09.013,2018,"It is a technical difficulty for the cognitive system to quickly analyze the characteristic data in massive data. A new method of cognitive fusion is proposed. In the embedded ARM server system, the software development and design of cognitive computing system are designed, the database of cognitive computing system is constructed, the hierarchical structure design algorithm of process constraint is used for cognitive information fusion, and the fuzzy scheduling and depth calculation are used to cluster the information, and it is carried out in the embedded ARM server environment. The integrated development and design of the system takes the reform of dragon boat teaching technology as an example. The test results show that using this system to reform the Dragon Boat teaching technology improves the artificial intelligence of dragon boat teaching, the error rate of teaching resources transmission is low, and the overall reliability and balance of the system is good. (C) 2018 Elsevier B.V. All rights reserved."
Accelerating Transformer Neural Networks on FPGAs for High Energy Physics Experiments,"Wojcicki, F; Que, ZQ; Tapper, AD; Luk, W",,2022,"High Energy Physics studies the fundamental forces and elementary particles of the Universe. With the unprecedented scale of experiments comes the challenge of accurate, ultra-low latency decision-making. Transformer Neural Networks (TNNs) have been proven to accomplish cutting-edge accuracy in classification for hadronic jet tagging. Nevertheless, softwarecentered solutions targeting CPUs and GPUs lack the inference speed required for real-time particle triggers, most notably those at the CERN Large Hadron Collider. This paper proposes a novel TNN-based architecture, efficiently mapped to FieldProgrammable Gate Arrays, that outperforms GPU inference capabilities involving state-of-the-art neural network models by approximately 1000 times while preserving comparable classification accuracy. The design offers high customizability and aims to bridge the gap between hardware and software development by using High-Level Synthesis. Moreover, we propose a novel model-independent post-training quantization search algorithm that works in general hardware environments according to user-defined constraints. Experimental evaluation yields a 64% reduction in overall bit-widths with a 2% accuracy loss."
'R: Towards Detecting and Understanding Code-Document Violations in Rust,"Ouyang, WR; Hua, BJ",10.1109/ISSREW53611.2021.00063,2021,"Documentation and comments are important for any software project. Although documentation is not executed, it is useful for many purposes, such as code comprehension, reuse, and maintenance. As a project evolves, the code and documentation can easily grow out-of-sync, and inconsistencies are introduced, which can mislead developers and introduce new bugs in subsequent developments. Recent studies have shown it is promising to use natural language processing and machine learning to detect inconsistencies between code and documentation. However, it's challenging to apply existing techniques to detect code-document inconsistency in Rust programs, as Rustdoc supports advanced document features like document testing, which makes existing solutions inapplicable. This paper presents the first software tool prototype, 'R, to detect and understand code-document inconsistencies in Rust. To perform such analysis, 'R leverages static program analysis, not only on Rust source code, but also on document testing code, to detect inconsistency indicating either bugs or bad documentation. To evaluate the effectiveness of 'R, we applied it to 37 open source Rust projects from 9 domains, with a total of 6,192,251 lines of Rust source code (with 322,330 lines of comments). The results of the analysis give interesting insights, for example: the cryptocurrency domain has the highest documentation ratio (58.23%), documentation testing is rarely used (ratio 2.30% on average) in real-world Rust projects in all domains, etc. Based on these findings, we propose recommendations to guide the construction of better Rust documentation, better Rust documentation quality detection tools, and boarder adoption of the language."
SMART GRID @ HOME - PROBLEM-ORIENTED LEARNING BASED ON A CURRENT SOCIAL THEME,"Doersam, B",,2019,"Smart Grid @ Home systems are approaches that try to self-consume the electricity generated from renewable energy sources as much as possible through intelligent energy management systems. Hence, owners and operators of this kind of systems become CO2 neutral considering their power consumption. For the teaching project presented here, a simulation and a demonstration environment for a smart grid are being developed. Both systems should introduce students to topics of information technology, topics of digitization (e.g. artificial intelligence) and economic issues in the context of problem-oriented learning. This learning process is set up on the topic of climate protection / CO2 neutrality. The students will first build the simulation and the demonstration environment and later use it to develop, evaluate and compare algorithms. The confrontation with the topic of climate protection, which is relevant for the most of students in their everyday life, should help them to develop an intrinsic motivation to learn the basic information technologies needed for the project."
Design and Implementation of Massive Information Management System Based on Big Data,"Li, Y; Shao, MC",,2018,"In order to improve the artificial intelligence of mass information management system and carry out the optimization design of mass information management system, a design method of mass information management system is proposed based on big data analysis and Web. The function module of the system includes computer aided management module, big data information processing module, program loading module, data classification module and data output module. The fuzzy clustering method is used to design big data's classification algorithm, and the association rule mining method is used to mine big data and integrate scheduling. The big data processing algorithm is used to load the control instructions through the program loading module, and the software development and design of the mass information management system is realized under the embedded Web environment. The simulation results show that the design of mass information management system has a good mining ability of big data and the intelligence of big data information management has been improved"
A Vulnerability Detection Algorithm Based on Transformer Model,"Hou, FJ; Zhou, K; Li, LB; Yuan, T; Li, J; Li, J",10.1007/978-3-031-06791-4_4,2022,"In today's Internet background and the rapid development of computer science and technology, new software is born every day, whether it is on the computer or mobile phone and on the hardware. In order to meet people's various daily needs, developers need to continuously develop new software and firmware. The software development process requires the reuse of shared codes and the realization of the middle-station module codes. These reusable codes can save developers' development time and improve efficiency. The code of the middle-station model is highly complex, and the vulnerabilities hidden in it are not easy to be discovered. A large number of vulnerabilities are inevitably introduced, which leads to immeasurable losses in downstream task modules. In order to enable these middle-station codes to better serve downstream tasks and discover the vulnerabilities hidden in them in time, it is first necessary to extract the defined software method body from the source code. We build an abstract syntax tree for the method to form a statement set; then, the variable names, function names, and strings in the method are replaced. Each statement in the code is given a number to construct a node set. The dependency between functions and variables includes data dependency and control dependency extraction and the node set itself as the input feature of the model. This paper uses Transformer model to model the sequence information. Transformer model can make the information of each node in the sequence fully interact. Based on the Transformer model, this paper further attempts to add the attention structure to improve the probability of detecting vulnerabilities. In the final experimental results, the model can detect vulnerabilities in the code with an accuracy of 95.04% and a recall rate of 88.89%, which also proves that transformer can accurately detect vulnerabilities in the sequence."
Antecedents of Organizational Agility During Business Uncertainty in Noninformation Technology Sectors,"Batra, D",10.4018/JDM.309433,2022,"The prolonged COVID-19 pandemic, economic stress, and geopolitical tensions have caused market disruptions and other forces that have likely increased organizational agility. This article focuses on the antecedents of organizational agility under such business uncertainty in the noninformation technology (IT) sectors. The research model stems from the uncertainty reduction theory and the following three frameworks: (1) dynamic capabilities; (2) decision making; and (3) business intelligence and analytics (BI&A) competitive advantage maturity model. It considers intelligence (risk and opportunity) and aligned decision making as agility predictors. It lists employee capability and IT flexibility as antecedents of intelligence, aligned decision making, and organizational agility. The results indicate that employee capability affects agility through the mediating variables of intelligence and aligned decision making. IT flexibility impacts agility only through intelligence. Both intelligence and aligned decision making have significant direct effects on agility."
The Influence of Deep Learning Algorithms Factors in Software Fault Prediction,"Al Qasem, O; Akour, M; Alenezi, M",10.1109/ACCESS.2020.2985290,2020,"The discovery of software faults at early stages plays an important role in improving software quality; reduce the costs, time, and effort that should be spent on software development. Machine learning (ML) have been widely used in the software faults prediction (SFP), ML algorithms provide varying results in terms of predicting software fault. Deep learning achieves remarkable performance in various areas such as computer vision, natural language processing, speech recognition, and other fields. In this study, two deep learning algorithms are studied, Multi-layer perceptron & x2019;s (MLPs) and Convolutional Neural Network (CNN) to address the factors that might have an influence on the performance of both algorithms. The experiment results show how modifying parameters is directly affecting the resulting improvement, these parameters are manipulated until the optimal number for each of them is reached. Moreover, the experiments show that the effect of modifying parameters had an important role in prediction performance, which reached a high rate in comparison with the traditional ML algorithm. To validate our assumptions, the experiments are conducted on four common NASA datasets. The result shows how the addressed factors might increase or decrease the fault detection rate measurement. The improvement rate was as follows up to 43.5 & x0025; for PC1, 8 & x0025; for KC1, 18 & x0025; for KC2 and 76.5 & x0025; for CM1."
Crowdtesting Practices and Models: An Empirical Approach,"Tsai, WT; Zhang, L; Hu, SF; Fan, ZZ; Wang, QY",10.1016/j.infsof.2022.107103,2023,"Context: Crowdsourced software testing (CST) has received significant attention. After these years, CST has made new progress and changes.Objective: While current literature lists many CST challenges, this paper analyzes industrial CST practices, finds that many challenges already have practical solutions, summarizes their commonalities, and comes up with new CST models and processes.Method: We look for well-known CST websites to participate in and take a secret and unobtrusive approach where customers, platform managers, and fellow workers do not know that we are mainly interested in CST research. We then register at selected CST websites, collect any public documents such as whitepapers, open rules, and public training materials, and join as many test tasks as possible.Results: We analyze the confrontation and collaboration among clients, platforms, and workers in the CST sessions. Clients want to get as much bug information as possible for a small amount of pay, but workers want to get paid as much as possible for a small amount of bug information. We also study the process and method of selecting suitable CST workers. Based on these, this paper proposes three future research directions.Conclusion: Data security and privacy at CST are paramount. If this problem can be overcome, CST will have wider applications. Additionally, the integration of workers, internal workers, software automation, and artificial intelligence will be major drivers for CST. It is also critical to develop a standardized CST structure and processes, and this will push the field to grow significantly."
"Exploring the resources, competencies, and capabilities needed for successful machine learning projects in digital marketing","Blomster, M; KoivumÃ¤ki, T",10.1007/s10257-021-00547-y,2022,"This study aimed to explore the organizational resources, competencies, and capabilities needed for the successful implementation of machine learning development projects for digital marketing operations in marketing organizations. The structure of the machine learning development project was investigated via the Agile-Stage-Gate model to identify the workflow, tasks, and roles of the marketing management and development teams during the project. With the accomplished project illustration, the necessary resources, competencies, and capabilities were identified. The findings suggest that marketing organizations' capability to understand and refine data by taking into the notion the impact of the marketing environment is the most crucial competence of machine learning development projects because it forms a solid base for algorithm execution and successful project implementation for marketing purposes. Marketing organizations must develop rigorous business processes and management procedures to support data governance and thus provide suitable data for machine learning purposes. Personnel's understanding of the data's characteristics and capabilities for running successful machine learning projects were also seen as key competencies for marketing organizations."
Programming skills in the industry 4.0: are chemical engineering students able to face new problems?,"dos Santos, MT; Vianna, AS; Le Roux, GAC",10.1016/j.ece.2018.01.002,2018,"A reflection on teaching programming at undergraduate level using advanced tools is presented. Advanced digital tools and computational evolution have shaped different areas, such as industrial process, communications, education and innovation. New technologies, such as the Internet of Things-IoT, cloud computing and artificial intelligence, have boosted software development and computational skills in different areas. The chemical engineering knowledge acquired by senior students and programming skills can be integrated to develop computational tools, favoring chemical engineers to take advantage of new opportunities in digital area. Normally, programming courses are offered at the beginning of chemical engineering program, with examples not related to chemical engineering problems. To fill this gap, an elective discipline was created for final year undergraduate students in the Department of Chemical Engineering at University of Sao Paulo (Brazil). This paper discusses the main motivations for a new programming course, presents the structure of the course and shows some outcomes from a students' survey about what they have learned. (C) 2018 Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved."
CodeGen-Search: A Code Generation Model Incorporating Similar Sample Information,"Li, HW; Kuang, JL; Zhong, MS; Wang, ZX; Liu, G; Liu, GL; Xiao, YJ",10.1142/S0218194023500584,2023,"Code generation has a positive significance in supporting software development, reducing labor intensity, and improving development efficiency. Some scholars use similar code information to enhance the quality of code generation. However, to improve the efficiency and accuracy of programming in daily development tasks, developers often search for similar samples as references. They get the code's syntactic structure and semantic information from similar samples to assist in programming development. Inspired by this, we argue that similar samples are helpful for code generation. This paper proposes a CodeGen-Search model to improve code generation quality by incorporating similar samples. To fully utilize the information of similar samples, the model adopts the pre-training + fine-tuning pattern. The model uses a minimum edit distance algorithm to find some similar samples with natural language (NL), and uses different encoders to extract the features of the NL and the code in similar samples. Experimental results show that our model efficiently improves the quality of the generated code. Compared to the state-of-the-art model, the CodeGen-Search model improves the BLEU by 1.5%, the Rough by 0.8% on the HS dataset, and the StrAcc by 0.5% on the ATIS dataset."
NEXTFLOAT: DISRUPTING FLOATING WIND,"Cahay, M; Mahmoudi, H",,2023,"Since the Hywind Demo project, the first Floating Offshore Wind Turbine (FOWT) of 2.3 MW installed 12 km offshore Karmoy island, South West of Norway by Equinor in 2009, up to the ongoing Hywind Tampen floating wind farm composed of 11 units of 8.6 MW to provide electricity for the Snorre and Gullfaks oil and gas fields in the Norwegian North Sea, most of the FOWTs deployed are spread moored to the seabed and use upwind 3 bladed turbines. Some other small-scale prototypes using catenary or tension legs, spread or single point moorings, single or multi-turbine configurations and upwind or downwind turbines were also tested during this period in a variety of environments. The NextFloat project (European Commission Grant Agreement n degrees 101084300 under HORIZON-CL5-2021-D3-03) started in November 2022. NextFloat will engineer and build an innovative weathervanning FOWT concept that will be deployed and operated on the Mistral test site in the South of France near Marseille. The unit will be a 6 MW version of the PivotBuoy currently deployed by X1- Wind on the PLOCAN site in the Canary Islands. The paper presents the objectives, the key challenges, the project design basis and the different innovations to be studied. It also highlights how this platform can be more suitable for the future 14MW+ turbines."
Automatic summarising of user stories in order to be reused in future similar projects,"Resketi, MR; Motameni, H; Nematzadeh, H; Akbari, E",10.1049/iet-sen.2019.0182,2020,"User stories play an important role in agile development systems. In this study, a method of summarising user stories is proposed to reuse them in the future. To enhance the results, quality improvement should be made on user stories. It would help developers build better results, and it may also lead to omitting some essential information. To avoid such issues, user stories are duplicated in two exact similar groups, and quality improvement is made on one set while the other set remains unattained. With the help of a modified bag of words and a verb parser, a collection of keywords and key verbs are extracted for both groups. Afterwards, automatic user stories are made, and then an expert improves them. Next, some experts choose between the results and select the better ones. The result is evaluated by applying different experiments on the framework and prototype implementation on 14 data sets of a user story from industry and a fake data set from Duke University. The result showed 97% of micro F-measure and 93% of macro F-measure, which are promising. These new user stories can be used as the base user stories in future similar projects."
Fairway: A Way to Build Fair ML Software,"Chakraborty, J; Majumder, S; Yu, Z; Menzies, T",10.1145/3368089.3409697,2020,"Machine learning software is increasingly being used to make decisions that affect people's lives. But sometimes, the core part of this software (the learned model), behaves in a biased manner that gives undue advantages to a specific group of people (where those groups are determined by sex, race, etc.). This algorithmic discrimination in the AI software systems has become a matter of serious concern in the machine learning and software engineering community. There have been works done to find algorithmic bias or ethical bias in software system. Once the bias is detected in the AI software system, mitigation of bias is extremely important. In this work, we a) explain how ground truth bias in training data affects machine learning model fairness and how to find that bias in AI software, b) propose a method Fairway which combines preprocessing and in-processing approach to remove ethical bias from training data and trained model. Our results show that we can find bias and mitigate bias in a learned model, without much damaging the predictive performance of that model. We propose that (1) testing for bias and (2) bias mitigation should be a routine part of the machine learning software development life cycle. Fairway offers much support for these two purposes."
APPLYING THE KNOWLEDGE BASE OF CWE WEAKNESSES IN SOFTWARE DESIGN,"Sartabanova, ZE; Dimitrov, VT; Sarsimbaeva, SM",10.26577/JMMCS.2020.v108.i4.06,2020,"The article deals with the issues of organizing software weaknesses by the software architect at the stage of its design using the developed ontological knowledge base of CWE weaknesses. The main goal of this research is to analyze the software defect system based on CWE and develop an ontology model (knowledge base) of this system for software architects. The use of artificial intelligence tools, in particular the development of knowledge bases based on weaknesses, will provide new opportunities for searching and researching software weaknesses. This model being developed will be useful for application by software developers, researchers in the field of software design and cybersecurity, as well as teachers of educational institutions that conduct courses in software development technology and information security. For developers, this model can serve as an assistant and reference when designing software, since weaknesses are organized by a well-known security tactic, helping the designer to embed security during the design process instead of detecting weaknesses after the software has been created. Researchers will be interested in studying and applying software weaknesses in their work. Teachers can use this model as a reference when studying and discussing security vulnerabilities in software design or architecture, as well as the types of errors that can be made during software development. The functions of the software architect are analyzed, and an example of the built ontological knowledge base of CWE weaknesses is given."
Bi-LSTM-Based Neural Source Code Summarization,"Aljumah, S; Berriche, L",10.3390/app122412587,2022,"Featured Application Code comment generation. Code summarization is a task that is often employed by software developers for fixing code or reusing code. Software documentation is essential when it comes to software maintenance. The highest cost in software development goes to maintenance because of the difficulty of code modification. To help in reducing the cost and time spent on software development and maintenance, we introduce an automated comment summarization and commenting technique using state-of-the-art techniques in summarization. We use deep neural networks, specifically bidirectional long short-term memory (Bi-LSTM), combined with an attention model to enhance performance. In this study, we propose two different scenarios: one that uses the code text and the structure of the code represented in an abstract syntax tree (AST) and another that uses only code text. We propose two encoder-based models for the first scenario that encodes the code text and the AST independently. Previous works have used different techniques in deep neural networks to generate comments. This study's proposed methodologies scored higher than previous works based on the gated recurrent unit encoder. We conducted our experiment on a dataset of 2.1 million pairs of Java methods and comments. Additionally, we showed that the code structure is beneficial for methods' signatures featuring unclear words."
An Intelligent Automation of Power Transformer using PLC and SCADA at Substation,"Shamshad, A; Riaz, MT; Raza, A; Khan, MA",10.1145/3593434.3594242,2023,"In this paper, SCADA, and PLC (programmable logic controller) are used in the substation to automate the control of transformers. The substation is where the power for the generator and the primary supply comes from the generating station to the distributor or the end user via the transmission line. Each substation has its own set of activities, such as switching transmission lines, measuring parameters, detecting faults, and storing historical data. All these tasks were performed by one person, as the system becomes more advanced it moves to automation. Monitoring, controlling, and acquiring of data can be performed by SCADA and a PLC (Programmable Logic Controller) in an automated substation. In the case of a low voltage, a transformer set at the substation is quickly turned on by the power management system using SCADA. A PLC, or programmed logic controller, regulates the voltage in a substation. Substation automation has three categories, First RTU data is sent to controllers for system monitoring or management. The second PLC generates control commands based on system parameters, and the third SCADA allows for human-machine interaction."
Augmented Intelligence Multilingual Conversational Service for Smart Enterprise Management Software,"Adeniyi, AE; Olagunju, M; Awotunde, JB; Abiodun, MK; Awokola, J; Lawrence, MO",10.1007/978-3-031-10548-7_35,2022,"Conversational agents are gaining popularity in the corporate world as a way to increase customer experience and economic competitiveness. Additionally, developments in augmented intelligence systems employ natural language processing to provide the industry with natural and clear interaction experiences. Multilingual conversational bot or Chabot is important in every area of life, especially in a multicultural community recognized for its numerous accents and slang among many social groupings Moreover, most present Chabot systems only handle one language at a time, and the training session is cumbersome since it needs various dialects for different purposes. This research presents a multilingual chatbot that allows clients to converse in many languages as if they were conversing with a real person to achieve a smart enterprise management software. The proposed system was implemented using React.JS and python programming language on a Pentium III processor speed of 600 MHz minimum. The proposed multilingual service will deal with the limitation of the existing system by developing a system that will allow multiple languages on chatbot agents. The system will allow the users to converse in their languages, which will make communication easy between the system and the users. The proposed system will include a user-friendly interface that will assist in guiding each user on how to utilize it effectively without any specialized training."
Pattern Analysis Software Tools (PAST) for Written Artefacts,"Mohammed, H; Helman-Wazny, A; Colini, C; Beyer, W; Bosch, S",10.1007/978-3-031-06555-2_15,2022,"The research of ancient written artefacts results in an ever-increasing amount of digital data in different forms, ranging from raw images of artefacts, to automatically generated data from advanced acquisition techniques. The manual analysis of this data is typically time consuming, and can be subject to human error and bias. Therefore, we present in this work a set of Pattern Analysis Software Tools (PAST), which are dedicated to the automatic analysis of visual and tabular patterns in the research data from the study of ancient written artefacts. These software tools have been developed to facilitate a more efficient study of written artefacts and to help the scholars benefit from the rapid advancements in the fields of pattern analysis and artificial intelligence. Furthermore, these tools can provide new insights which can only be emerged from the statistical analysis of research data. Each tool in PAST is developed and tested in close collaboration with experts from relevant fields of research, and presented here with actual use cases in order to demonstrate its usability and applicability to real research questions."
Code comment generation based on graph neural network enhanced transformer model for code understanding in open-source software ecosystems,"Kuang, L; Zhou, C; Yang, XX",10.1007/s10515-022-00341-1,2022,"In open-source software ecosystems, the scale of source code is getting larger and larger, and developers often use various methods (good code comments or method names, etc.) to make the code easier to read and understand. However, high-quality code comments or method names are often unavailable due to tight project schedules or other reasons in open-source software ecosystems such as Github. Therefore, in this work, we try to use deep learning models to generate appropriate code comments or method names to help software development and maintenance, which requires a non-trivial understanding of the code. Therefore, we propose a Graph neural network enhanced Transformer model (GTrans for short) to learn code representation to understand code better. Specifically, GTrans learns code representation from code sequences and graphs. We use a Transformer encoder to capture the global representation from code sequence and a graph neural network (GNN) encoder to focus on the local details in the code graph, and then use a decoder to combine both global and local representations by attention mechanism. We use three public datasets collected from GitHub to evaluate our model. In an extensive evaluation, we show that GTrans outperforms the state-of-the-art models up to 3.8% increase in METEOR metrics on code comment generation and outperforms the state-of-the-art models by margins of 5.8%-9.4% in ROUGE metrics on method name generation after some adjustments on the structure. Empirically, we find the method name generation task depends on more local information than global, and the code comment generation task is in contrast. Our data and code are available at https://github.com/zc-work/GTrans."
Machine Learning in Computer Vision,"Khan, AI; Al-Habsi, S",10.1016/j.procs.2020.03.355,2020,"During last few years the computer applications have gone dramatic transfoi tation from simple data processing to machine learning, thanks to the availability and accessibility of huge volume of data collected through sensors and internet. The idea of machine learning demonstrates and propagates the facts that computer has the ability to improve itself with the passage of time. The western countries have shown great interest on the topic of machine learning, computer vision, and pattern recognition via organizing conferences, workshops, collective discussion, experimentation, and real life implementation. This study on machine learning and computer vision explores and analytically evaluates the machine learning applications in computer vision and predicts future prospects. The study has found that the machine learning strategies in computer vision are supervised, un-supervised, and semi-supervised. The commonly used algorithms are neural networks, k-means clustering, and support vector machine. The most recent applications of machine learning in computer vision are object detection, object classification, and extraction of relevant infounation from images, graphic documents, and videos. Additionally, Tensor flow, Faster-RCNN-Inception-V2 model, and Anaconda software development environment used to identify cars and persons in images. (C) 2020 The Authors. Published by Elsevier B.V."
The Concept of Digital Social Control: Results of Public Opinion,"Nikitina, AS; Ruchkin, AV; Isacov, AS",10.14456/ITJEMAST.2022.68,2022,"Digital technologies currently have a significant impact on most areas of society, including the sphere of state and municipal government. The most popular and in demand today are the forms and mechanisms of social control over the activities of government bodies using digital technologies. The article examines the theoretical, legal, and empirical aspects of social control in the public administration system in the context of digitalization. Toward the active development of the digital society, the issues of digitalization are gaining more and more popularity in the modern world, including the improvement of the activities of public authorities using advanced digital technologies, artificial intelligence, information, and telecommunication systems. The article analyzes the regulatory, financial, personnel, organizational, and managerial aspects of effective social control in the public administration system based on sociological surveys and experts' interviews, and offers practical digital solutions to improve the effectiveness of social control. The digital software project of the Open Government is proposed, which is aimed at increasing the efficiency of interaction between public authorities and the population, representatives of the business environment, and the public. This work concludes and recommends improving social control over the activities of the authorities in the context of digitalization."
Study of Various Classifiers for Identification and Classification of Non-functional Requirements,"TÃ³th, L; VidÃ¡cs, L",10.1007/978-3-319-95174-4_39,2018,"Identification of non-functional requirements in an early phase of software development process is crucial for creating a proper software design. These requirements are often neglected or given in too general forms. However, interviews and other sources of requirements often include important references also to non-functional requirements which are embedded in a bigger textual context. The non-functional requirements have to be extracted from these contexts and should be presented in a formulated and standardized way to support software design. The set of requirements extracted from their textual context have to be classified to formalize them. This task is to be accomplished manually but it can be very demanding and error-prone. Several attempts have been made to support identification and classification tasks using supervised and semi-supervised learning processes. These efforts have achieved remarkable results. Researchers were mainly focused on the performance of classification measured by precision and recall. However, creating a tool which can support business analysts with their requirements elicitation tasks, execution time is also an important factor which has to be taken into account. Knowing the performance and the results of bench-marks can help business analysts to choose a proper method for their classification tasks. Our study presented in this article focuses on both the comparison of performances of the classification processes and their execution time to support the choice among the methods."
Fast Detection of Duplicate Bug Reports using LDA-based Topic Modeling and Classification,"Akilan, T; Shah, D; Patel, N; Mehta, R",10.1109/smc42975.2020.9283289,2020,"A bug tracking system continuously monitors the status of a software environment, like an Operating System (OS) or a user application. Whenever it detects an anomaly situation, it generates a bug report and sends it to the software developer or maintenance center. However, the newly reported bug can be an already existing issue that was reported earlier and may have a solution in the master report repository. This condition brings an avalanche of duplicate bug reports, posing a big challenge to the software development life cycle. Thus, early detection of duplicate bug reports has become an extremely important task in the software industry. To address this issue, this work proposes a double-tier approach using clustering and classification, whereby it exploits Latent Dirichlet Allocation (LDA) for topic-based clustering, multimodal text representation using Word2Vec (W2V), FastText (FT) and Global Vectors for Word Representation (GloVe), and a unified text similarity measure using Cosine and Euclidean metrics. The proposed model is tested on the Eclipse dataset consisting over 80,000 bug reports, which is the amalgamation of both master and duplicate reports. This work considers only the description of the reports for detecting duplicates. The experimental results show that the proposed two-tier model achieves a recall rate of 67% for Top-N recommendations with 3 times faster computation than the conventional one-on-one classification model."
Rapid Prototyping Framework for Intelligent Arrays with Heterogeneous Computing,"Vanhoy, G; Lichtman, M; Hoare, RR; Brevik, C",10.1109/PAST49659.2022.9975034,2022,"The availability of highly capable RF platforms with integrated heterogeneous compute has necessitated the need for software development frameworks that allow application designers to take advantage of these newly available capabilities. By leveraging open-source libraries and frameworks including GNU Radio, PyTorch, and Vitis AI, we have developed a framework that enables rapid prototyping and development of intelligent array processing applications and other SDR applications. This framework enables researchers and developers to deploy applications that make use of CPU, GPU, and Xilinx DPU resources, without the need to be a GPU or FPGA expert. Our initial kernel performance metrics show that this new ability will allow system designers to efficiently leverage trade-offs in system performance by effectively allocating resources across several processors."
Research on the Construction of the Quality Maturity Evaluation in the Product R&D Phase,"Lei, S; Gu, ZW; Cui, YX; Tang, HB",10.1007/978-3-031-05014-5_12,2022,"With the popularization of the concept of comprehensive quality management in Chinese enterprises, more and more enterprises begin to realize that doing a good job in quality management needs to start from the source. The design and development stage is an important process for the formation and finalization of product quality, and the quality management work at this stage is crucial to determine the product quality level. With digital mobile communication products as the research background, this paper summarizes the international methodology of product design and development process, such as ISO9001 international quality management system, advanced product quality planning (APQP), project management knowledge system (PMBOK), door diameter management system (SGS), integrated product development (IPD) and product lifecycle management (PLM), etc. Through a systematic summary and research on the quality management method of the above design and development process, the quality maturity evaluation index system is constructed. Help the product to carry out quantitative product development quality maturity evaluation in the concept definition stage, sample verification stage, pilot test confirmation stage, small batch climbing stage and mass production stage of the design and production process. Through the product maturity evaluation system, the quality maturity status of the product design and development process can be easily measured, so that the project stakeholders can have clear objectives and risk management for each process of the product design and development. Easy solve the opacity of quality management and problem exposure lag in the design development process, so that the design quality is well prevented in advance."
Vec2graph: A Python Library for Visualizing Word Embeddings as Graphs,"Katricheva, N; Yaskevich, A; Lisitsina, A; Zhordaniya, T; Kutuzov, A; Kuzmenko, E",10.1007/978-3-030-39575-9_20,2020,"Visualization as a means of easy conveyance of ideas plays a key role in communicating linguistic theory through its applications. User-friendly NLP visualization tools allow researchers to get important insights for building, challenging, proving or rejecting their hypotheses. At the same time, visualizations provide general public with some understanding of what computational linguists investigate. In this paper, we present vec2graph: a ready-to-use Python 3 library visualizing vector representations (for example, word embeddings) as dynamic and interactive graphs. It is aimed at users with beginners' knowledge of software development, and can be used to easily produce visualizations suitable for the Web. We describe key ideas behind vec2graph, its hyperparameters, and its integration into existing word embedding frameworks."
A Novel UCP Model Based on Artificial Neural Networks and Orthogonal Arrays,"Rankovic, N; Rankovic, D; Ivanovic, M; Lazic, L",10.3390/app11198799,2021,"Adequate estimation is a crucial factor for the implementation of software projects within set customer requirements. The use of Case Point Analysis (UCP) is the latest and most accurate method for estimating the effort and cost of realizing software products. This paper will present a new, improved UCP model constructed based on two different artificial neural network (ANN) architectures based on Taguchi Orthogonal Vector Plans. ANNs are an exceptional artificial intelligence tool that have been proven to be reliable and stable in this area of software engineering. The Taguchi method of Orthogonal Vector Plans is an optimization method that reduces the number of iterations required, which significantly shortens estimation time. The goal is to construct models that give a minimum magnitude relative error (MRE) value concerning previous approaches and techniques. A minimum number of iterations (less than six) and a minimum value of MMRE (less than 10%) have been achieved. The obtained results significantly improve the accuracy and reliability of estimating the effort and cost involved in the implementation of software projects."
"Transcriptional dynamics of maize leaves, pollens and ovules to gain insights into heat stress-related responses","Jagtap, AB; Yadav, IS; Vikal, Y; Praba, UP; Kaur, N; Gill, AS; Johal, GS",10.3389/fpls.2023.1117136,2023,"Heat stress (HS) is one of the alarming issues today due to global warming and is the foremost detrimental to crop production. Maize is one of the versatile crops grown over different agro-climatic conditions. However, it is significantly sensitive to heat stress, especially during the reproductive phase. The heat stress tolerance mechanism is yet to be elucidated at the reproductive stage. Thus, the present study focused on identifying transcriptional changes in two inbreds, LM 11 (sensitive to HS) and CML 25 (tolerant to HS), under intense heat stress at 42 degrees C during the reproductive stage from three tissues viz. flag leaf, tassel, and ovule. Samples from each inbred were collected after 5 days of pollinations for RNA isolation. Six cDNA libraries were constructed from three separate tissues of LM 11 and CML 25 and sequenced using an Illumina HiSeq2500 platform. A total of 2,164 (1127 up-regulated and 1037 down-regulated) differentially expressed genes (DEGs) were identified with 1151, 451, and 562 DEGs in comparisons of LM 11 and CML 25, corresponding to a leaf, pollen, and ovule, respectively. Functional annotated DEGs associated with transcription factors (TFs) viz. AP2, MYB, WRKY, PsbP, bZIP, and NAM, heat shock proteins (HSP20, HSP70, and HSP101/ClpB), as well as genes related to photosynthesis (PsaD & PsaN), antioxidation (APX and CAT) and polyamines (Spd and Spm). KEGG pathways analyses showed that the metabolic overview pathway and secondary metabolites biosynthesis pathway, with the involvement of 264 and 146 genes, respectively, were highly enriched in response to heat stress. Notably, the expression changes of the most common HS-responsive genes were typically much more significant in CML 25, which might explain why CML 25 is more heat tolerant. Seven DEGs were common in leaf, pollen, and ovule; and involved in the polyamines biosynthesis pathway. Their exact role in maize heat stress response would warrant further studies. These results enhanced our understanding to heat stress responses in maize."
What Quality Attributes Can We Find in Product Backlogs? A Machine Learning Perspective,"Galster, M; Gilson, F; Georis, F",10.1007/978-3-030-29983-5_6,2019,"Automatically identifying quality attributes (e.g., security, performance) in agile user stories could help architects reason about early architecture design decisions before analyzing a product backlog in detail (e.g., through a manual review of stories). For example, architects may already get the bigger picture of potential architectural key drivers and constraints. Applying a previously developed method to automatically identify quality attributes in user stories, in this paper we investigate (a) what quality attributes are potentially missed in an automatic analysis of a backlog, and (b) how the importance of quality attributes (based on the frequency of their occurrence in a backlog) differs to that of quality attributes identified in a manual review of a backlog. As in previous works, we analyzed the backlogs of 22 publicly available projects including 1,675 stories. For most backlogs, automatically identified quality attributes are a subset of quality attributes identified manually. On the other hand, the automatic identification would usually not find more (and therefore potentially irrelevant) quality attributes than a manual review. We also found that the ranking of quality attributes differs between the automatically and manually analyzed user stories, but the overall trend of rankings is consistent. Our findings indicate that automatically identifying quality attributes can reduce the effort of an initial backlog analysis, but still provide useful (even though high-level and therefore potentially incomplete) information about quality attributes."
Toward Refactoring Evaluation with Code Naturalness,"Arima, R; Higo, Y; Kusumoto, S",10.1145/3196321.3196362,2018,"Refactoring evaluation is a challenging research topic because right and wrong of refactoring depend on various aspects of development context such as developers' skills, development cost, deadline and so on. Many techniques have been proposed to evaluate refactoring objectively. However, those techniques do not consider individual contexts of software development. Currently, the authors are trying to evaluate refactoring automatically and objectively with considering development contexts. In this paper, we propose to evaluate refactoring with code naturalness. Our technique is based on a hypothesis: if a given refactoring raises the naturalness of existing code, the refactoring is beneficial. In this paper, we also report our pilot study on open source software."
Latent variable-based multiple instance learning towards label-free polarity detection,"Ihasz, PL; Kovacs, M; Kryssanov, VV",10.1145/3322645.3322654,2019,"Extracting information from the audio content of the users' dialogic utterances would provide an easily-perturbed set of features that could serve as a reliable and inexpensive mean for emotion recognition, suitable to be applied in commercial software development. Owing to the diversity of audio features, however, emotion recognition in spontaneous dialogues is a complex task, typically requiring the pre-training of classifiers on large collections of labeled data. To escape the necessity of hand labeling, a novel multiple instance learning method is proposed. It performs the bag-label-based instance classification through the extraction of latent variables with variational autoencoders. In this semi-supervised method, the bags themselves are gathered from audio features of weakly labeled YouTube videos, thus training is fully automated and does not require manual annotation."
Early increase of specialized pro-resolving lipid mediators in patients with ST-elevation myocardial infarction,"Fosshaug, LE; Colas, RA; Anstensrud, AK; Gregersen, I; Nymo, S; Sagen, EL; Michelsen, A; Vinge, LE; Oie, E; Gullestad, L; Halvorsen, B; Hansen, TV; Aukrust, P; Dalli, J; Yndestad, A",10.1016/j.ebiom.2019.07.024,2019,"Background: Termination of acute inflammation is an active process orchestrated by lipid mediators (LM) derived from polyunsaturated fatty acids, referred to as specialized pro-resolving mediators (SPM). These mediators also provide novel therapeutic opportunities for treating inflammatory disease. However, the regulation of these molecules following acute myocardial infarction (MI) remains of interest. Methods: In this prospective observational study we aimed to profile plasma levels of SPMs in ST-elevation MI (STEMI) patients during the first week following MI. Plasma LM concentrations were measured in patients with STEMI (n = 15) at three time points and compared with stable coronary artery disease (CAD; n = 10) and healthy controls (n = 10). Findings: Our main findings were: (i) Immediately after onset of MI and before peak troponin T levels, STEMI patients had markedly increased levels of SPMs as compared with healthy controls and stable CAD patients, with levels of these mediators declining during follow-up. (ii) The increase in SPMs primarily reflected an increase in docosapentaenoic acid- and docosahexaenoic add-derived protectins. (iii) Several individual protectins were correlated with the rapid increase in neutrophil counts, but not with CRP. (iv) A shift in 5-LOX activity from the leukotriene B4 pathway to the pro-resolving RvTs was observed. Interpretation: The temporal regulation of SPMs indicates that resolution mechanisms are activated early during STEMI as part of an endogenous mechanism to initiate repair. Thus strategies to boost the activity and/or efficacy of these endogenous mechanisms may represent novel therapeutic opportunities for treatment of patients with MI."
Reducing Requirements Ambiguity via Gamification: Comparison with Traditional Techniques,"Dar, HS; Imtiaz, S; Lali, MI",10.1155/2022/3183411,2022,"Requirements elicitation is one of the most significant activities of requirements engineering (RE) process. Poorly specified requirements can lead to a failed project. Various elicitation techniques are used to elicit requirements from the users and other stakeholders, each having its own pros and cons. Lack of user engagement, less user involvement, textual nature of the requirements, time taking process are some of the major problems that make it difficult to perform elicitation via traditional techniques. Moreover, these problems further create other challenges such as ambiguity, inconsistency, and incompleteness in requirements. Currently, researchers have focused on reducing ambiguity in requirements with the help of different techniques such as natural language processing techniques, requirement templates, and formal methods; however, these techniques work on reducing ambiguity during specification or from specified requirements. One of the young' and exciting way of engaging users in requirements elicitation of a system is Gamification', which helps in user engagement into the system. We intend to discover how gamification helps in reducing ambiguity by engaging stakeholders in an interactive manner. In this review study, we have reviewed traditional techniques used to detect and reduce requirements ambiguity. On the contrary, we have also presented the significance of gamification in requirements elicitation and the popular but effective game elements used in similar systems. Furthermore, this study highlights the significance of using gamification in requirements elicitation, which is beneficial to software development team as well as the users involved in the system."
Classifying Non-functional Requirements using RNN Variants for Quality Software Development,"Rahman, MA; Haque, MA; Tawhid, MNA; Siddik, MS",10.1145/3340482.3342745,2019,"Non-Functional Requirements (NFR), a set of quality attributes, required for software architectural design. Which are usually scattered in SRS and must be extracted for quality software development to meet user expectations. Researchers show that functional and non-functional requirements are mixed together within the same SRS, which requires a mammoth effort for distinguishing them. Automatic NFR classification would be a feasible way to characterize those requirements, where several techniques have been recommended e.g. IR, linguistic knowledge, etc. However, conventional supervised machine learning methods suffered for word representation problem and usually required hand-crafted features, which will be overcome by proposed research using RNN variants to categories NFR. The NFR are interrelated and one task happens after another, which is the ideal situation for RNN. In this approach, requirements are processed to eliminate unnecessary contents, which are used to extract features using word2vec to fed as input of RNN variants LSTM and GRU. Performance has been evaluated using PROMISE dataset considering several statistical analysis. Among those models, precision, recall, and f1-score of LSTM validation are 0.973, 0.967 and 0.966 respectively, which is higher over CNN and GRU models. LSTM also correctly classified minimum 60% and maximum 80% unseen requirements. In addition, classification accuracy of LSTM is 6.1% better than GRU, which concluded that RNN variants can lead to better classification results, and LSTM is more suitable for NFR classification from textual requirements."
Voice-Driven Modeling: Software Modeling Using Automated Speech Recognition,"Black, D; Rapos, EJ; Stephan, M",10.1109/MODELS-C.2019.00040,2019,"Voice-driven programming allows engineers to alleviate physical discomfort, pain, and injury. It also has the potential to be faster than typing and assist those with disabilities. While there are a number of solutions to voice-driven programming, Model-Driven Engineering (MDE) has yet to exploit this non-conventional but high-potential approach to software development. Standard convention in MDE practice involves creating software models using a traditional mouse and keyboard combination, or whiteboard sketch hardware. In this position paper, we introduce our vision and ideas for a Voice-Driven Modeling (VDM) approach. Our vision involves a framework that includes 3 phases: Speech Processing, Natural Language Processing, and Context Specific Modeling. We describe these 3 phases in this paper, which others can apply in their attempts to realize VDM. We additionally include our research plans for developing a VDM solution targeted at Simulink models and our early proof of concept capable of implementing several example commands. We establish the pertinence of this work through a survey that finds negligible work on VDM and highlights the potential impact this can have on the field of MDE as a whole. Specifically, it is our position that it can have a positive impact on modelers in general, modelers with disabilities, and domain experts not familiar with modeling. It is our hope that this work helps fuel research in this area, allowing for a new way to develop software models."
Integrating project management and systems engineering to transition to remote operations,"Gajadhar, S; Stomski, P",,2018,"Remote operation of observatories has been a topic of interest for many years. This paper discusses a general approach to determining what it will take to transition from on-site summit nighttime operation to remote nighttime operation of a facility. It is informed by involvement in projects at Canada-France-Hawaii Telescope, Gemini Observatory, and W. M. Keck Observatory. While these projects had differences, they all shared the goals of upgrading an operating observatory that is on sky every night to improve efficiency of operations without negative impact on science. The approach combines project management (PMI) and systems engineering (INCOSE) methodologies and tools to develop an understanding of the impact on operations, determine scope and requirements for new capabilities as well as additional functionality for existing systems, identify and manage risks, and how to incrementally move toward remote operation by integrating changes into current operations along the way."
Asset Management of Wastewater Interceptors Adjacent to Bodies of Water,"Fawwaz, MDB; Najafi, M; Kaushal, V",10.3390/w15234176,2023,"Pipeline asset management derives from pipelines' physical conditions, condition rating, and serviceability through investigating, monitoring, and analyzing the rupture history. The remaining asset life and structural condition of the pipeline network running near and under bodies of water are often hard to predict. In case of a pipeline failure, major damage may occur to the surrounding environment, adding up to disruptions in service and repair costs. This paper develops multinomial logistic regression (MLR) and binary logistic regression models to predict how the bodies of water could affect the soil surrounding wastewater interceptors. The models were developed based on data from the City of Fort Worth, Texas. This study concludes that the pipe diameter, pipe age, location of the pipeline with reference to bodies of water (far or near), and the pipe material are the most significant variables that affect the surrounding conditions and remaining life of wastewater interceptors. In future, a clearer perception through increased software development and machine learning for managing pipeline asset management would provide impacts on different parameters on pipelines' expected life."
A User-friendly Semi-automatic iStar Modeling Approach,"Zhou, QX; Li, T",10.1109/RE54965.2022.00033,2022,"iStar modeling is beneficial in the early stage of requirements engineering, helping requirements analysts to analyze requirements and improve the efficiency and quality of the software development procedure. However, it is time-consuming and hard to learn to perform the iStar modeling manually, which can be more practical if the modeling process is automated. To facilitate the distribution of iStar practices, we designed a user-friendly semi-automatic iStar modeling approach to assist users in iStar modeling by extracting model elements from natural language requirement artifacts. Specifically, based on the analysis of the actual modeling process via interviewing, this work proposed an iStar modeling process, and automated three modeling steps: the actor entity extraction, the actor relation extraction, and the intention entity extraction. Then, this work proposes a hybrid method for natural language processing to extract the model elements in requirement sentences to automate the modeling steps. This hybrid method consists of two parts: the deep learning-based method and the logical reasoning method, which utilizes both methods simultaneously, ensuring the high accuracy of the results. Overall, this work proposed a user-friendly semi-automatic approach for aiding the iStar modeling, which proposes an iStar modeling process and automates many steps with hybrid natural language method during the process. We evaluated our proposed approach, and the results show that our proposed approach is efficient and helpful."
Enhancing the Performance of Software Authorship Attribution Using an Ensemble of Deep Autoencoders,"Czibula, G; Lupea, M; Briciu, A",10.3390/math10152572,2022,"Software authorship attribution, defined as the problem of software authentication and resolution of source code ownership, is of major relevance in the software engineering field. Authorship analysis of source code is more difficult than the classic task on literature, but it would be of great use in various software development activities such as software maintenance, software quality analysis or project management. This paper addresses the problem of code authorship attribution and introduces, as a proof of concept, a new supervised classification model AutoSoft for identifying the developer of a certain piece of code. The proposed model is composed of an ensemble of autoencoders that are trained to encode and recognize the programming style of software developers. An extension of the AutoSoft classifier, able to recognize an unknown developer (a developer that was not seen during the training), is also discussed and evaluated. Experiments conducted on software programs collected from the Google Code Jam data set highlight the performance of the proposed model in various test settings. A comparison to existing similar solutions for code authorship attribution indicates that AutoSoft outperforms most of them. Moreover, AutoSoft provides the advantage of adaptability, illustrated through a series of extensions such as the definition of class membership probabilities and the re-framing of the AutoSoft system to address one-class classification."
A Standard Baseline for Software Defect Prediction: Using Machine Learning and Explainable AI,"Bommi, NS; Negi, A",10.1109/COMPSAC57700.2023.00278,2023,"The utility of ubiquitous computing systems drives large-scale software development with millions of lines of code (LOC). As there are vast code sets, it also increases the possibility of coding errors since it is difficult for even highly trained software engineers to write flawless code. Such flawed software can lead to severe issues once deployed. McCabe and Halstead proposed feature extractors for source code to define software quality. Based on the static features proposed by McCabe and Halstead, we run a series of feature engineering techniques and different machine learning models to detect code defects and use explainable algorithms to assess the prediction quality. We report different processing pipeline combinations to detect defects and compare the approaches. We conclude the paper with comments on the nature of the dataset and establish a baseline for further research."
Investigating Interracial Pair Coordination During Remote Pair Programming,"Mason, SA; Kuttal, SK",10.1109/VL-HCC57772.2023.00047,2023,"Remote pair programming is a popular software development method that lacks evidence on how race may affect pair dynamics. Past computer science studies demonstrated how race impacts various fields such as AI, education, politics and jobs. A complex history of interracial interactions in the United States has led to differences in collaborative styles. We recruited 12 professional developers and investigated how same-and mixed-race pairs (Black-White) coordinated during remote pair programming interactions. Our results revealed that Black developers in mixed-race pairs were more democratic while in same-race pairs were more authoritative."
Exploring and Evaluating Personalized Models for Code Generation,"Zlotchevski, A; Drain, D; Svyatkovskiy, A; Clement, CB; Sundaresan, N; Tufano, M",10.1145/3540250.3558959,2022,"Large Transformer models achieved the state-of-the-art status for Natural Language Understanding tasks and are increasingly becoming the baseline model architecture for modeling source code. Transformers are usually pre-trained on large unsupervised corpora, learning token representations and transformations relevant to modeling generally available text, and are then fine-tuned on a particular downstream task of interest. While fine-tuning is a tried-and-true method for adapting a model to a new domain - for example, question-answering on a given topic - generalization remains an on-going challenge. In this paper, we explore and evaluate transformer model fine-tuning for personalization. In the context of generating unit tests for Java methods, we evaluate learning to personalize to a specific software project using several personalization techniques. We consider three key approaches: (i) custom fine-tuning, which allows all the model parameters to be tuned; (ii) lightweight fine-tuning, which freezes most of the model's parameters, allowing tuning of the token embeddings and softmax layer only or the final layer alone; (iii) prefix tuning, which keeps model parameters frozen, but optimizes a small project-specific prefix vector. Each of these techniques offers a trade-off in total compute cost and predictive performance, which we evaluate by code and task-specific metrics, training time, and total computational operations. We compare these fine-tuning strategies for code generation and discuss the potential generalization and cost benefits of each in various deployment scenarios."
"Radiomics Analysis of Brain [18F]FDG PET/CT to Predict Alzheimer's Disease in Patients with Amyloid PET Positivity: A Preliminary Report on the Application of SPM Cortical Segmentation, Pyradiomics and Machine-Learning Analysis","Alongi, P; Laudicella, R; Panasiti, F; Stefano, A; Comelli, A; Giaccone, P; Arnone, A; Minutoli, F; Quartuccio, N; Cupidi, C; Arnone, G; Piccoli, T; Grimaldi, LME; Baldari, S; Russo, G",10.3390/diagnostics12040933,2022,"Background: Early in-vivo diagnosis of Alzheimer's disease (AD) is crucial for accurate management of patients, in particular, to select subjects with mild cognitive impairment (MCI) that may evolve into AD, and to define other types of MCI non-AD patients. The application of artificial intelligence to functional brain [F-18]fluorodeoxyglucose (FDG) positron emission tomography (PET)/computed tomography(CT) aiming to increase diagnostic accuracy in the diagnosis of AD is still undetermined. In this field, we propose a radiomics analysis on advanced imaging segmentation method Statistical Parametric Mapping (SPM)-based completed with a Machine-Learning (ML) application to predict the diagnosis of AD, also by comparing the results with following Amyloid-PET and final clinical diagnosis. Methods: From July 2016 to September 2017, 43 patients underwent PET/CT scans with FDG and Florbetaben brain PET/CT and at least 24 months of clinical/instrumental follow-up. Patients were retrospectively evaluated by a multidisciplinary team (MDT = Neurologist, Psychologist, Radiologist, Nuclear Medicine Physician, Laboratory Clinic) at the G. Giglio Institute in Cefalu, Italy. Starting from the cerebral segmentations applied by SPM on the main cortical macro-areas of each patient, Pyradiomics was used for the feature extraction process; subsequently, an innovative descriptive-inferential mixed sequential approach and a machine learning algorithm (i.e., discriminant analysis) were used to obtain the best diagnostic performance in prediction of amyloid deposition and the final diagnosis of AD. Results: A total of 11 radiomics features significantly predictive of cortical beta-amyloid deposition (n = 6) and AD (n = 5) were found. Among them, two higher-order features (original_glcm_Idmn and original_glcm_Id), extracted from the limbic enthorinal cortical area (ROI-1) in the FDG-PET/CT images, predicted the positivity of Amyloid-PET/CT scans with maximum values of sensitivity (SS), specificity (SP), precision (PR) and accuracy (AC) of 84.92%, 75.13%, 73.75%, and 79.56%, respectively. Conversely, for the prediction of the clinical-instrumental final diagnosis of AD, the best performance was obtained by two higher-order features (original_glcm_MCC and original_glcm_Maximum Probability) extracted from ROI-2 (frontal cortex) with a SS, SP, PR and AC of 75.16%, 80.50%, 77.68%, and 78.05%, respectively, and by one higher-order feature (original_glcm_Idmn) extracted from ROI-3 (medial Temporal cortex; SS = 80.88%, SP = 76.85%, PR = 75.63%, AC = 78.76%. Conclusions: The results obtained in this preliminary study support advanced segmentation of cortical areas typically involved in early AD on FDG PET/CT brain images, and radiomics analysis for the identification of specific high-order features to predict Amyloid deposition and final diagnosis of AD."
Ethics in Digital Health: a deontic accountability framework,"Milosevic, Z",10.1109/EDOC.2019.00022,2019,"We present key ethics concerns in digital health and introduce related ethics principles to address these concerns. We propose mappings of these principles into deontic logic concepts to support ethics -based analysis. This provides input into detailed design of deontic and accountability constraints using semantic foundation from the ODP enterprise language standard [11. We refer to our approach as 'ethics by design' because it aims at explicit inclusion of ethics principles into contemporary software development and tooling. The paper is focused on digital health, but the approach has broader applicability."
The New Era of Drug Discovery: The Power of Computer-aided Drug Design (CADD),"Nascimento, IJD; de Aquino, TM; Da Silva, EF",10.2174/1570180819666220405225817,2022,"Drug design and discovery is a process that requires high financial costs and is time-consuming. For many years, this process focused on empirical pharmacology. However, over the years, the target-based approach allowed a significant discovery in this field, initiating the rational design era. In view, to decrease the time and financial cost, rational drug design is benefited by increasing computer engineering and software development, and computer-aided drug design (CADD) emerges as a promising alternative. Since the 1970s, this approach has been able to identify many important and revolutionary compounds, like protease inhibitors, antibiotics, and others. Many anticancer compounds identified through this approach have shown their importance, being CADD essential in any drug discovery campaign. Thus, this perspective will present the prominent successful cases utilizing this approach and entering into the next stage of drug design. We believe that drug discovery will follow the progress in bioinformatics, using high-performance computing with molecular dynamics protocols faster and more effectively. In addition, artificial intelligence and machine learning will be the next process in the rational design of new drugs. Here, we hope that this paper generates new ideas and instigates research groups worldwide to use these methods and stimulate progress in drug design."
"Alteration of intracranial blood perfusion in temporal lobe epilepsy, an arterial spin labeling study","Rahimzadeh, H; Kamkar, H; Hoseini-Tabatabaei, N; Mobarakeh, NM; Habibabadi, JM; Hashemi-Fesharaki, SS; Nazem-Zadeh, MR",10.1016/j.heliyon.2023.e14854,2023,"Background: A critical necessity before surgical resection in mesial temporal lobe epilepsy (mTLE) is lateralizing the seizure focus in the temporal lobe. This study aimed to investigate the differ-ences in perfusion pattern changes in right and left mTLE.Methods: 42 mTLE patients (22 left and 20 right mTLE) and 14 controls were surveyed with pulsed arterial spin labeling at 3.0 T. The mean cerebral blood flow (CBF) and asymmetry index (AI) were calculated in the bilateral temporal lobe, amygdala, hippocampus, parahippocampus, and nine bilateral vascular territories ROIs. The alterations in whole-brain CBF were identified using statistical parametric mapping (SPM).Results: CBF decreased in ipsilateral sides in both epilepsy subcohorts, with right mTLE showing a significant difference in most ROIs while left mTLE exhibiting no significant change. CBF com-parison of left mTLE and controls showed a significant drop in ROI analysis in left middle tem-poral and left intermediate posterior cerebral artery and in AI analysis in parahippocampus, distal anterior cerebral artery, distal middle cerebral artery, and intermediate anterior cerebral artery. CBF hypoperfusion was seen in ROI analysis in the left intermediate anterior cerebral artery, left middle temporal, right middle temporal, left superior temporal in the right mTLE compared to controls. Left mTLE CBF differed significantly from right mTLE CBF in right distal middle cerebral artery ROI and AI of proximal middle cerebral artery."
Recovering Semantic Traceability between Requirements and Source Code Using Feature Representation Techniques,"Zhang, M; Tao, CQ; Guo, HJ; Huang, ZQ",10.1109/QRS54544.2021.00096,2021,"Requirement traceability is essential for software development and maintenance, thereby effectively recovering the requirements traceability has become an important issue for requirement engineering. With the development of software systems, it is always unrealistic to maintain traceability links between requirements and source code manually. Therefore, researchers have proposed information retrieval-based approaches to recover the links automatically. Although these methods reduce human labor, they do not fully extract the specific features, resulting in poor traceability accuracy. In this paper, we propose an approach to recovering traceability between requirements and source code, which combines word embedding and self-attention model to extract features and generate text vectors. These technologies make full use of the semantic information of the context and feature representation. In addition, the paper discusses the impact of code content and comments on the results and improves the results on weight. Finally, the proposed approach is compared with the commonly-used baselines, and the study results show that the proposed approach outperforms others."
English Translation Teaching Model of Flipped Classroom Based on the Fusion Algorithm of Network Communication and Artificial Intelligence,"Li, L",10.1155/2021/7520862,2021,"With the advent of the information age and the rapid development of communication technology, traditional teaching methods and methods can no longer meet people's needs for education in the information age. This research mainly discusses the flipped classroom English translation teaching model based on the fusion algorithm of network communication and artificial intelligence. This research proposes an Internet learning platform based on Tencent's QQ communication software and mobile terminals. This learning platform can meet the learning needs of flipped classrooms, without spending a lot of investment in hardware and software development. This is a relatively ideal Internet learning platform suitable for students to carry out flipped classroom learning. Most of these mobile terminals have functions such as video playback, document reading and editing, wireless network connection, QQ communication, and large-capacity storage. These functions provide support for the smooth implementation of flip class. The video playback function can meet the needs of students to download and watch microvideos. Students can use mobile terminals to watch anytime, anywhere. The document reading and editing function can help students read related learning materials online, including self-study test questions, task lists, and teaching evaluation forms. Under this platform, students and teachers who are participants in the learning process are linked to each other through the QQ of the network terminal. The learning resources are uploaded by the teacher to the group file for students to download and watch. Students use the QQ group to give feedback and discuss difficult problems. This kind of learning network that flexibly chooses the learning time and place according to the individual student's own situation is very efficient and convenient. After the English translation test, the failure rate of the experimental class was only 3.3%, while in the control class, the failure rate was 8.3%. This research will better integrate the flipped classroom teaching mode and English translation teaching by building a learning exchange platform, which will help improve the quality of teaching."
Ginkgolic Acid is a Multi-Target Inhibitor of Key Enzymes in Pro-Inflammatory Lipid Mediator Biosynthesis,"Gerstmeieri, J; Seegers, J; Witt, F; Waltenberger, B; Temml, V; Rollinger, JM; Stuppner, H; Koeberle, A; Schuster, D; Werz, O",10.3389/fphar.2019.00797,2019,"Introduction: Lipid mediators (LMs) comprise bioactive metabolites of polyunsaturated fatty acids, including pro-inflammatory prostaglandins (PGs), thromboxanes (TXs), and leukotrienes (LTs), as well as specialized pro-resolving mediators (SPMs). They are essentially biosynthesized via cyclooxygenase (COX) and lipoxygenase (LO) pathways in complex networks and regulate the progression as well as the resolution of inflammatory disorders including inflammation-triggered cancer. Ginkgolic acid (GA) is a phenolic acid contained in Ginkgo biloba L. with neuroprotective, antimicrobial, and antitumoral properties. Although LMs regulate microbial infections and tumor progression, whether GA affects LM biosynthesis is unknown and was investigated here in detail. Methods: Pharmacophore-based virtual screening was performed along with docking simulations. Activity assays were conducted for isolated human recombinant 5-LO, cytosolic phospholipase (PLA)(2)alpha, COX-2, and ovine COX-1. The activity of human mPGES-1 and thromboxane A(2) synthase (TXAS) was determined in crude cellular fractions. Cellular LM formation was studied using human monocytes, neutrophils, platelets, and M1- and M2-like macrophages. LMs were identified after (ultra) high-performance liquid chromatography by UV detection or ESI-tandem mass spectrometry. Results: GA was identified as virtual hit in an mPGES-1 pharmacophore-based virtual screening. Cell-free assays revealed potent suppression of mPGES-1 activity (IC50 = 0.7 mu M) that is fully reversible and essentially independent of the substrate concentration. Moreover, cell-free assays revealed COX-1 and TXAS as additional targets of GA with lower affinity (IC50 = 8.1 and 5.2 mu M). Notably, 5-LO, the key enzyme in LT biosynthesis, was potently inhibited by GA (IC50 = 0.2 mu M) in a reversible and substrate-independent manner. Docking simulations support the molecular interaction of GA with mPGES-1 and 5-LO and suggest concrete binding sites. Interestingly, interference of GA with mPGES-1, COX-1, TXAS, and 5-LO was evident also in intact cells with IC50 values of 2.1-3.8 mu M; no radical scavenging or cytotoxic properties were obvious. Analysis of LM profiles from bacteria-stimulated human M1- and M2-like macrophages confirmed the multi-target features of GA and revealed LM redirection towards the formation of 12-/15-LO products including SPM. Conclusions: We reveal GA as potent multi-target inhibitor of key enzymes in the biosynthesis of pro-inflammatory LMs that contribute to the complex pharmacological and toxicological properties of GA."
Tracing and Visualizing Human-ML/AI Collaborative Processes through Artifacts of Data Work,"Rogers, J; Crisan, A",10.1145/3544548.3580819,2023,"Automated Machine Learning (AutoML) technology can lower barriers in data work yet still requires human intervention to be functional. However, the complex and collaborative process resulting from humans and machines trading off work makes it difficult to trace what was done, by whom (or what), and when. In this research, we construct a taxonomy of data work artifacts that captures AutoML and human processes. We present a rigorous methodology for its creation and discuss its transferability to the visual design process. We operationalize the taxonomy through the development of AutoML Trace a visual interactive sketch showing both the context and temporality of human-ML/AI collaboration in data work. Finally, we demonstrate the utility of our approach via a usage scenario with an enterprise software development team. Collectively, our research process and findings explore challenges and fruitful avenues for developing data visualization tools that interrogate the sociotechnical relationships in automated data work."
VulSeeker: A Semantic Learning Based Vulnerability Seeker for Cross-Platform Binary,"Gao, J; Yang, X; Fu, Y; Jiang, Y; Sun, JG",10.1145/3238147.3240480,2018,"Code reuse improves software development efficiency, however, vulnerabilities can be introduced inadvertently. Many existing works compute the code similarity based on CFGs to determine whether a binary function contains a known vulnerability. Unfortunately, their performance in cross-platform binary search is challenged. This paper presents VulSeeker, a semantic learning based vulnerability seeker for cross-platform binary. Given a target function and a vulnerable function, VulSeeker first constructs the labeled semantic flow graphs and extracts basic block features as numerical vectors for both of them. Then the embedding vector of the whole binary function is generated by feeding the numerical vectors of basic blocks to the customized semantics aware DNN model. Finally, the similarity of the two binary functions is measured based on the Cosine distance. The experimental results show that VulSeeker outperforms the state-of-the-art approaches in terms of accuracy. For example, compared to the most recent and related work Gemini, VulSeeker finds 50.00% more vulnerabilities in the top-10 candidates and 13.89% more in the top-50 candidates, and improves the values of AUC and ACC for 8.23% and 12.14% respectively."
An Adversarial Discriminative Convolutional Neural Network for Cross-Project Defect Prediction,"Sheng, L; Lu, L; Lin, JH",10.1109/ACCESS.2020.2981869,2020,"Cross-project defect prediction (CPDP) is a promising approach to help to allocate testing efforts efficiently and guarantee software reliability in the early software lifecycle. A CPDP method usually trains a software defect classifier based on labeled data sets. Then the trained classifier can predict new projects without labeled data. Most previous CPDP techniques focused on manually designing handcrafted features. However, these handcrafted features ignore the programs & x2019; semantic information. Moreover, some other existing defect prediction approaches learned semantic features from source code to build classifiers directly. However, they did not consider the distribution divergence between source and target projects. To address these limitations, we put forward a new method called Adversarial Discriminative Convolutional Neural Network (ADCNN). It can extract the transferable semantic features from source code for CPDP tasks. Specifically, we first parse source files into token vectors and then map them to integer vectors via word embedding. Second, we combine adversarial learning with discriminative feature learning to train the ADCNN model. The key of the ADCNN model is to learn the discriminative mapping of the target project to the source feature space by deceiving a domain discriminator. A domain discriminator tries to distinguish the target project files from the source project files. Finally, we use the extracted transferable semantic features to build a classifier for CPDP tasks. We evaluate our method on ten benchmark projects in terms of F-measure, AUC, and PofB20 (an effort-aware evaluation metric). The experimental results demonstrate that our ADCNN method performs better compared with other related CPDP methods."
An Approach for Reviewing Security-Related Aspects in Agile Requirements Specifications of Web Applications,"Villamizar, H; Neto, AA; Kalinowski, M; Garcia, A; MÃ©ndez, D",10.1109/RE.2019.00020,2019,"Defects in requirements specifications can have severe consequences during the software development lifecycle. Some of them result in overall project failure due to incorrect or missing quality characteristics such as security. There are several concerns that make security difficult to deal with; for instance, (1) when stakeholders discuss general requirements in meetings, they are often unaware that they should also discuss security-related topics, and (2) they typically do not have enough expertise in security. This often leads to unspecified or ill-defined security-related aspects. These concerns become even more challenging in agile contexts, where lightweight documentation is typically involved. The goal of this paper is to design and evaluate an approach for reviewing security-related aspects in agile requirements specifications of web applications. The approach considers user stories and security specifications as input and relates those user stories to security properties via Natural Language Processing. Based on the related security properties, our approach then identifies high-level security requirements from the Open Web Application Security Project to be verified and generates a reading technique to support reviewers in detecting defects. We evaluate our approach via two controlled experiment trials. We compare the effectiveness and efficiency of novice inspectors verifying security aspects in agile requirements using our approach against using the complete list of high-level security requirements. The (statistically significant) results indicate that using our approach has a positive impact (with large effect size) on the performance of inspectors in terms of effectiveness and efficiency."
Transfer Learning from Partial Annotations for Whole Brain Segmentation,"Dai, CL; Mo, YH; Angelini, E; Guo, YK; Bai, WJ",10.1007/978-3-030-33391-1_23,2019,"Brain MR image segmentation is a key task in neuroimaging studies. It is commonly conducted using standard computational tools, such as FSL, SPM, multi-atlas segmentation etc, which are often registration-based and suffer from expensive computation cost. Recently, there is an increased interest using deep neural networks for brain image segmentation, which have demonstrated advantages in both speed and performance. However, neural networks-based approaches normally require a large amount of manual annotations for optimising the massive amount of network parameters. For 3D networks used in volumetric image segmentation, this has become a particular challenge, as a 3D network consists of many more parameters compared to its 2D counterpart. Manual annotation of 3D brain images is extremely time-consuming and requires extensive involvement of trained experts. To address the challenge with limited manual annotations, here we propose a novel multi-task learning framework for brain image segmentation, which utilises a large amount of automatically generated partial annotations together with a small set of manually created full annotations for network training. Our method yields a high performance comparable to state-of-the-art methods for whole brain segmentation."
User story extraction from natural language for requirements elicitation: Identify software-related information from online news,"Siahaan, D; Raharjana, IK; Fatichah, C",10.1016/j.infsof.2023.107195,2023,"Context: The user story is a popular artifact in agile software development. Extracting user stories is helpful for process improvement in requirements elicitation, closing limitations such as limited access, and uncovering new and unique domains. Most sources of requirements elicitation are available in natural language form. However, the approach to extracting user stories from natural language is still limited. Objective: This study aims to extract user stories from natural language. It includes identifying the aspect of who (stakeholder), aspect of what (stakeholder's wants), and aspect of why (the reason why the aspect of what exists). Method: This study used online news as a case study because information related to stakeholders and their needs is available. Aspects of who, what, and why are obtained using a rule-based approach using part-of-speech (POS) chunking, named entity recognition (NER), dependency parsing, WordNet, and BloomSoft. Result: We found that online news tends to generate requirements with hard-goals or soft-goals types. In identifying aspects of who, we succeeded in increasing the F-score value by combining stakeholder identification methods according to the characteristics of online news. We also found that PUblic REquirements (PURE), domain specificity, and WordNet lexical names can significantly improve the extraction of software-related information in identifying the aspects of what. Conclusion: This study demonstrates that information related to software requirements could arise from nonsoftware-related artifacts such as online news."
An Improved Software Defect Prediction Algorithm Using Self-organizing Maps Combined with Hierarchical Clustering and Data Preprocessing,"Shakhovska, N; Yakovyna, V; Kryvinska, N",10.1007/978-3-030-59003-1_27,2020,"An improved software defects prediction algorithm based on combination of Kohonen map and hierarchical clustering is presented in this paper. The need for software reliability assessment and analysis growths rapidly due to increasing dependence of our day-to-day life on software-controlled devices and systems. Software reliability prediction is the only tool available at early stage of software development lifecycle when the debugging cost risk of faulty operation is minimal. Artificial intelligence and machine learning in particular are promising techniques to solve this task. Various classification methods have been used previously to build software defect prediction models, ranging from simple, like logistic regression, to advanced methods, e.g. multivariate adaptive regression splicing. However, the available literature still does not allow to make unambiguous conclusion concerning the choice of the best classifier and trying different dimensions to overcome potential bias is suggested. The purpose of the paper is to analyze the software code metrics to find dependences be-tween software module's defect-proneness and its metrics. JM1 public NASA dataset from PROMISE Software Engineering Repository was used in this study. To increase the classification accuracy, we combine self-organizing maps with hierarchical clustering and data preprocessing."
"Automatic requirements extraction, analysis, and graph representation using an approach derived from computational linguistics","Mokammel, F; CoatanÃ©a, E; CoatanÃ©a, J; Nenchev, V; Blanco, E; Pietola, M",10.1002/sys.21461,2018,"The quality of requirements is fundamental in engineering projects. Requirements are usually expressed partly or totally in a natural language (NL) format and come from different documents. Their qualities are difficult to analyze manually, especially when hundreds of thousands of them have to be considered. The assistance of software tools is becoming a necessity. In this article, the goal was to develop a set of metrics supported by NL processing (NLP) methods supporting different types of analysis of requirements and especially the dependencies between requirements. An NLP approach is used to extract requirements from text; to analyze their quality, links, similarities, and contradictions; and to cluster them automatically. The analysis framework includes different combinations of methods such as cosine similarity, singular value decomposition, and K-means clustering. One objective is to assess the possible combinations and their impacts on detections to establish optimal metrics. Three case studies exemplify and support the validation of the work. Graphs are used to represent the automatically clustered requirements, as well as similarities and contradictions. A new contradiction analysis process that includes a rules-based approach is proposed. Finally, the combined results are presented as graphs, which unveil the semantic relationships between requirements. Subsection 4.8 compares the results provided by the tool and the results obtained from experts. The proposed methodology and network presentation not only support the understanding of the semantics of the requirements but also help requirements engineers to review the interconnections and consistency of requirements systems and manage traceability. The approach is valuable during the early phases of projects when requirements are evolving dynamically and rapidly."
ReACC: A Retrieval-Augmented Code Completion Framework,"Lu, S; Duan, N; Han, HJ; Guo, DY; Hwang, SW; Svyatkovskiy, A",,2022,"Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development. Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets. However, current approaches focus only on code context within the file or project, i.e. internal context. Our distinction is utilizing external context, inspired by human behaviors of copying from the related code snippets when writing code. Specifically, we propose a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval. We adopt a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language. We evaluate our approach in the code completion task in Python and Java programming languages, achieving a state-of-the-art performance on CodeXGLUE benchmark."
Software Engineering for Data Intensive Scalable Computing and Heterogeneous Computing,"Kim, MRY",10.1109/ICSE-FoSE59343.2023.00006,2023,"With the development of big data, machine learning, and AI, existing software engineering techniques must be re-imagined to provide the productivity gains that developers desire. Furthermore, specialized hardware accelerators like GPUs or FPGAs have become a prominent part of the current computing landscape. However, developing heterogeneous applications is limited to a small subset of programmers with specialized hardware knowledge. To improve productivity and performance for data-intensive and compute-intensive development, now is the time that the software engineering community should design new waves of refactoring, testing, and debugging tools for big data analytics and heterogeneous application development. In this paper, we overview software development challenges in this new data-intensive scalable computing and heterogeneous computing domain. We describe examples of automated software engineering (debugging, testing, and refactoring) techniques that target this data and compute intensive domain and share lessons learned from building these techniques."
Community-Driven Methods for Open and Reproducible Software Tools for Analyzing Datasets from Atom Probe Microscopy,"KÃ¼hbach, M; London, AJ; Wang, J; Schreiber, DK; Martin, FM; Ghamarian, I; Bilal, H; Ceguerra, A",10.1017/S1431927621012241,2022,"Atom probe tomography, and related methods, probe the composition and the three-dimensional architecture of materials. The software tools which microscopists use, and how these tools are connected into workflows, make a substantial contribution to the accuracy and precision of such material characterization experiments. Typically, we adapt methods from other communities like mathematics, data science, computational geometry, artificial intelligence, or scientific computing. We also realize that improving on research data management is a challenge when it comes to align with the FAIR data stewardship principles. Faced with this global challenge, we are convinced it is useful to join forces. Here, we report the results and challenges with an inter-laboratory call for developing test cases for several types of atom probe microscopy software tools. The results support why defining detailed recipes of software workflows and sharing these recipes is necessary and rewarding: Open source tools and (meta)data exchange can help to make our day-to-day data processing tasks become more efficient, the training of new users and knowledge transfer become easier, and assist us with automated quantification of uncertainties to gain access to substantiated results."
Screening for obstructive sleep apnea with novel hybrid acoustic smartphone app technology,"Tiron, R; Lyon, G; Kilroy, H; Osman, A; Kelly, N; O'Mahony, N; Lopes, C; Coffey, S; McMahon, S; Wren, M; Conway, K; Fox, N; Costello, J; Shouldice, R; Lederer, K; Fietze, I; Penzel, T",10.21037/jtd-20-804,2020,"Background: Obstructive sleep apnea (OSA) has a high prevalence, with an estimated 425 million adults with apnea hypopnea index (AHI) of _15 events/hour, and is significantly underdiagnosed. This presents a significant pain point for both the sufferers, and for healthcare systems, particularly in a post COVID-19 pandemic world. As such, it presents an opportunity for new technologies that can enable screening in both developing and developed countries. In this work, the performance of a non-contact OSA screener App that can run on both Apple and Android smartphones is presented. Methods: The subtle breathing patterns of a person in bed can be measured via a smartphone using the Firefly app technology platform [and underpinning software development kit (SDK)], which utilizes advanced digital signal processing (DSP) technology and artificial intelligence (AI) algorithms to identify detailed sleep stages, respiration rate, snoring, and OSA patterns. The smartphone is simply placed adjacent to the subject, such as on a bedside table, night stand or shelf, during the sleep session. The system was trained on a set of 128 overnights recorded at a sleep laboratory, where volunteers underwent simultaneous full polysomnography (PSG), and Firefly smartphone app analysis. A separate independent test set of 120 recordings was collected across a range of Apple iOS and Android smartphones, and withheld for performance evaluation by a different team. An operating point tuned for mid-sensitivity (i.e., balancing sensitivity and specificity) was chosen for the screener. Results: The performance on the test set is comparable to ambulatory OSA screeners, and other smartphone screening apps, with a sensitivity of 88.3% and specificity of 80.0% [with receiver operating characteristic (ROC) area under the curve (AUC) of 0.92], for a clinical threshold for the AHI of >_15 events/ hour of detected sleep time. Conclusions: The Firefly app based sensing technology offers the potential to significantly lower the barrier of entry to OSA screening, as no hardware (other than the user's personal smartphone) is required. Additionally, multi-night analysis is possible in the home environment, without requiring the wearing of a portable PSG or other home sleep test (HST)."
INCORPORATING ETHICS THROUGHOUT THE SOFTWARE DEVELOPMENT PROCESS,"Gordon, D; Collins, M; Becevel, A; O'Mahony, W",,2020,"The media is reporting scandals associated with computer companies with increasing regularity; whether it is the misuse of user data, breach of privacy concerns, the use of biased artificial intelligence, or the problems of automated vehicles. Because of these complex issues, there is a growing need to equip computer science students with a deep appreciation of ethics, and to ensure that in the future they will develop computer systems that are ethically-based. One particularly useful strand of their education to incorporate ethics into is when teaching them about the formal approaches to developing computer systems. There are a number of specific processes and methodologies that incorporate these stages in different ways into their approaches. Some take a linear approach to these stages, whereas others take a more iterative and/or incremental approach. These models include the Waterfall Model, the V-Model, the Spiral Model, and the Agile family of models. For each of these models this paper will present a way to include ethics in the Specifying stage, and well as threaded throughout the model, and as an explicit stage in a final review process at the end of the implementation stage. These formal models are understood (and used) by computer companies all over the world, and therefore are a natural means of incorporating ethics into software development in a manner that would not seem overly arduous or unwieldy to developers. These techniques are also taught in the computer science departments of universities all over the world, it is therefore vitally important that lecturers incorporate an ethical dimension into their systems development teaching, and we believe that these newly refined models provide them with a simple means of achieving this task, and this will make a new generation of software developers ethically-aware."
Bioeconomic analysis and management aspects of metapenaeus shrimp fisheries in Pakistan,"Mehak, A; Mu, YT; Mohsin, M; Noman, M; Memon, AM",,2018,"Fishery input (FI) and fishery output (FO) data, 1984-2009, related to Metapenaeus shrimp fisheries sector in Pakistan is analyzed in order to access its bioeconomic and management aspects. The maximum, minimum and average catch of this fishery resource remained 14824 t (1984), 5004 t (2006) and 7925 t y(-1) , correspondingly. Data was analyzed by using two specialized fishery software i.e. CEDA and ASPIC. Three surplus production models (SPMs) viz. Fox (FM), Schaefer (SM) and Pella-Tomlinson (PTM) were used in CEDA. Furthermore, error assumptions (EAs) viz. normal (NEA), log normal (LNEA) and gamma (GEA), for each SPM were also applied in CEDA. Two SPMs, FM and LM, were used in ASPIC. In CEDA by using an initial proportion (IP) of 1, FM estimated MSY, CV and R-2 as 6474 t (tons), 0.048 and 0.670 for NEA. While, the computed values of these parameters for LNEA remained as 6108 t, 0.061 and 0.574, correspondingly. For this model, GEA produced minimization failure (MF). Estimated MSY for all EAs by SM and PTM were remained same viz. 7731 t, 6959 t and 7102 t, correspondingly. FM results showed the highest value of R-2 (0.670). In ASPIC, FM estimated MSY, Fmsy, CV and R-2 as 6200 t, 0.219 y(-1) , 0.054 and 0.896, respectively, whereas, LM computed same parameters as 6769 t, 0.207 y(-1), 0.100 and 0.846, correspondingly. The obtained results suggest that this fishery resource is overexploited. Thus, harvest levels of this fishery resource must be lowered for their sustainable maximum economic contribution and conservation as well."
Requirements Derivation for the Goalkeeper of the RoboCup Small Size League,"Farias, GB; Maximo, MROA; Afonso, RJM",10.1007/s40313-021-00872-0,2022,"In systems engineering, requirements derivation is an important step to achieve a successful product design. This process consists of obtaining technical specifications that meet the client's needs. Some applications, however, do not have a clear relationship between these two. Therefore, we contribute with a requirements derivation method by studying the trade-off between product performance and specification demand. We apply this method to the RoboCup Small Size League, a robot competition that promotes research in artificial intelligence and robot systems. In this work, we focus in the goal interception task. The development of a team for this competition requires a hardware project to build the robots and a software project to make game decisions. In this context, we derive requirements both for the robot's parameters (maximum velocity, maximum acceleration, robot's bandwidth, robot's phase margin) and for the perception system's parameters (information update frequency, information delay). During this study's development, we model the robots and game dynamics to determine the influence of the system's parameters in the task and evaluate the goalkeeper's performance by using Monte Carlo methods to estimate the goalkeeper's performance. Moreover, we observe that the performance estimation depends not only on the system's parameters but also on the Monte Carlo sampling model and the selected game strategy. Lastly, realistic ranges for the design parameters are obtained using the proposed methods."
VDCNet: A Vulnerability Detection and Classification System in Cross-Project Scenarios,"Zhang, DP; Xian, HQ; Chen, JY; Xu, ZG",10.1007/978-3-031-44207-0_26,2023,"The existence of software vulnerabilities is the primary cause of most security incidents in cyberspace. Timely detection of potential vulnerabilities from source code during the software development stage is a critical issue for developers. With the increasing scale of open-source projects, traditional static analysis tools are becoming more and more unreliable and stagnant in their development. Meanwhile, approaches for vulnerability detection based on deep learning are being investigated. This paper introduces a novel deep learning-based vulnerability detection system, VDCNet, to identify and classify multiple vulnerabilities more effectively. We extract advanced semantic information from AST representations of source code and capture patterns of vulnerable functions by training neural networks. VDCNet constructs a BERT model for embedding and a Bi-LSTM network for prediction. The experimental results on a comprehensive dataset demonstrate that our method is more efficient in binary vulnerability detection than other deep learning-based methods, with outstanding multi-classification performance in cross-project scenarios."
PUMiner: Mining Security Posts from Developer Question and Answer Websites with PU Learning,"Le, THM; Hin, D; Croft, R; Babar, MA",10.1145/3379597.3387443,2020,"Security is an increasing concern in software development. Developer Question and Answer (Q&A) websites provide a large amount of security discussion. Existing studies have used human-defined rules to mine security discussions, but these works still miss many posts, which may lead to an incomplete analysis of the security practices reported on Q&A websites. Traditional supervised Machine Learning methods can automate the mining process; however, the required negative (non-security) class is too expensive to obtain. We propose a novel learning framework, PUMiner, to automatically mine security posts from Q&A websites. PUMiner builds a context-aware embedding model to extract features of the posts, and then develops a two-stage PU model to identify security content using the labelled Positive and Unlabelled posts. We evaluate PUMiner on more than 17.2 million posts on Stack Overflow and 52,611 posts on Security StackExchange. We show that PUMiner is effective with the validation performance of at least 0.85 across all model configurations. Moreover, Matthews Correlation Coefficient (MCC) of PUMiner is 0.906, 0.534 and 0.084 points higher than one-class SVM, positive-similarity filtering, and one-stage PU models on unseen testing posts, respectively. PUMiner also performs well with an MCC of 0.745 for scenarios where string matching totally fails. Even when the ratio of the labelled positive posts to the unlabelled ones is only 1:100, PUMiner still achieves a strong MCC of 0.65, which is 160% better than fully-supervised learning. Using PUMiner, we provide the largest and up-to-date security content on Q&A websites for practitioners and researchers."
Pre-Trained Model-Based NFR Classification: Overcoming Limited Data Challenges,"Rahman, K; Ghani, A; Alzahrani, A; Tariq, MU; Rahman, AU",10.1109/ACCESS.2023.3301725,2023,"Machine learning techniques have shown promising results in classifying non-functional requirements (NFR). However, the lack of annotated training data in the domain of requirement engineering poses challenges to the accuracy, generalization, and reliability of ML-based methods, including overfitting, poor performance, biased models, and out-of-vocabulary issues. This study presents an approach for the classification of NFR in software requirements specification documents by extracting features from word embedding pre-trained models. The novel algorithms are specifically designed to extract relevant representative features from pre-trained word embedding models. In addition, each pre-trained model is paired with the four tailored neural network architectures for NFR classification including RPCNN, RPBiLSTM, RPLSTM, and RPANN. This combination results in the creation of twelve unique models, each with its unique configuration and characteristics. The results show that the integration of pre-trained GloVe models with RPBiLSTM demonstrates the highest performance, achieving an impressive average Area Under the Curve (AUC) score of 96%, a precision of 85%, and recall of 82%, highlighting its strong ability to accurately classify NFRs. Furthermore, among the integration of pre-trained Word2Vec models, RPLSTM achieved notable results, with an AUC score of 95%, precision of 86%, and recall of 80%. Similarly, integrated fastText-based pre-trained models the RPBiLSTM yield competitive performance, with an AUC score of 95%, precision of 85%, and recall of 80%. This comprehensive and integrated approach provides a practical solution for effectively analyzing and classifying NFR, thereby facilitating improved software development practices."
How to make intelligent automation projects agile? Identification of success factors and an assessment approach,"Josyula, SS; Suresh, M; Raman, RR",10.1108/IJOA-05-2021-2749,2023,"Purpose Organizations are fast adopting new technologies such as automation, analytics and artificial intelligence, collectively called intelligent automation, to drive digital transformation. When adopting intelligent automation, there is a need to understand the success factors of these new technologies and adapt agile software development (ASD) practices to meet customer expectations. The purpose of this paper is to explore the success factors of intelligent automation and create a framework for managers and practitioners to meet dynamic business demands. Total interpretive structural modeling (TISM) framework is a suitable approach to integrate quantitative measurement with qualitative semi-structured interviews capturing the context of the individual organization environment. Design/methodology/approach This paper identified agility factors and their interrelationships using a TISM framework. TISM results were validated using a one-tailed t-test to confirm the interrelationships between factors. Furthermore, the agility index of a case project organization was assessed using a graph-theoretic approach (GTA) to identify both the triggering factors for agility success and improvement proposals. Findings Results showed that leadership vision, organization structure and program methodology were driving factors. The TISM model was validated statistically and the agility index of the intelligent automation case project organization was calculated to be79.5%. Here, a GTA was applied and the triggering factors for improvement of the agility index were identified. Research limitations/implications The limitations of the study are described along with the opportunities for future research as the field evolves through the rapid innovation of technology and products. Practical implications The increasing role of digital transformation in enterprise strategy and operations requires practitioners to understand how ASD practices must be planned, measured and/or improved over time through the implementation of automation, analytics and artificial intelligence programs. The TISM digraph provides a framework of hierarchical structure to organize the influencing factors, which assists in achieving organizational goals. This study highlights the driving factors which contribute to the success of intelligent automation projects and project organizations. Originality/value This is a first attempt to analyze the interrelationships among agility factors in intelligent automation projects (IAP) using TISM and the assessment of the agility index of a case IAP organization using a GTA."
A Fusion of Java Domain Knowledge Base and Siamese Network for Java API Recommendation,"Li, H; Li, T; Zhong, S; Kang, Y; Chen, T",10.1109/QRS-C51114.2020.00074,2020,"APIs play an important role in modern software development. Programmers need to frequently search for the appropriate APIs according to different tasks. With the development of the information industry, API reference documents have become larger and larger. Due to redundant and erroneous information on the Internet, traditional search methods can also cause inconvenience to programmers' queries. At the same time, there is a gap in terms of vocabulary and knowledge between the natural language description of the programming task and the description in the API documentation, so it is difficult to find a suitable API. To solve these problems, this paper proposes a Java API recommendation model by fusing the Java domain knowledge base and the Siamese Network to improve the accuracy of API recommendation. Experiments on the BIKER data set show that our method has better recommendation results than the state-of-art DeepAPI and BIKER model."
Towards the Development of a Multi-Agent Cognitive Networking System for the Lunar Environment,"Dudukovich, R; Wagner, K; Kancharla, S; Fantl, J; Fung, A",10.1109/WiSEE50203.2021.9613839,2021,"This paper details the development of a multi-agent cognitive system intended to optimize networking performance in the lunar environment. NASA's current concept of the future of lunar communication, LunaNet [1], outlines a complex network of networks. Challenges such as scalability, interoperability and reliability must first be addressed to successfully fulfill this vision. Machine intelligence can greatly reduce the reliance on human operators and enable efficient operations for tasks such as scheduling and network management. The application of machine learning, artificial intelligence, and other automated decision-making techniques can be used to allow network nodes to intelligently sense and adapt to changes in the environment such as link disruptions, new nodes joining the network, and support for a diverse range of protocols. Cognitive networking seeks to evolve these technologies into an autonomous system with improved science data return, reliability, and scalability. In this paper, we study three main areas a means to further develop cognitive networking capabilities: networking and flight software development, analysis of wireless data for modeling and simulation, and development of algorithms for a multi-agent system."
MLOps for evolvable AI intensive software systems,"Moreschini, S; Lomio, F; Hastbacka, D; Taibi, D",10.1109/SANER53432.2022.00155,2022,"DevOps practices are the de facto sandard when developing software. The increased adoption of machine learning (ML) to solve problems urges us to adapt all the current approaches to developing a new standard that can take full benefit from the new solution. In this work we propose a graphical representation for DevOps for ML-based applications, namely MLOps, and also outline open research challenges. The pipeline aims to get the best of both worlds by maintaining the simple and iconic pipeline of DevOps, yet improving it by adding new circular steps for ML incorporation. This aims to create an ML-based development subsystem that can be self-maintained, and is capable of evolving side-by-side with the software development."
Atomic-Scale Modeling for Materials and Chemistry,"Le, NQ; Domenico, J; Salerno, KM; Stiles, CD",,2023,"Atomic and molecular modeling techniques have developed over the past 75 years into a vibrant field of computational science, used to understand and predict materials properties and phenomena in academic, industrial, and government labs. Researchers today have the benefit of decades of Moore's law growth in computer processors, decades of algorithm and software development, experiments capable of atomic-scale characterization for validation, and a deeper understanding of the strengths, limitations, and complementary features of different computational methods. It is not surprising then that important problems in many fields-battery chemistry, drug design, mechanics of materials, biocompatibility, and catalyst design-are routinely studied using atomic-scale simulation and modeling. In this article, we first outline a brief history and background of the density functional theory and molecular dynamics methods. Next, we discuss several case studies that exemplify how scientists and engineers at the Johns Hopkins University Applied Physics Laboratory (APL) use these computational methods to attain APL's broader goals and mission. Finally, we discuss future directions for atomic-scale modeling and calculations, such as integration with modeling methods at other scales and with artificial intelligence-enabled frameworks, to meet the next generation of sponsor challenges."
Software Challenges in Heterogeneous Computing: A Multiple Case Study in Industry,"Andrade, H; Lwakatare, LE; Crnkovic, I; Bosch, J",10.1109/SEAA.2019.00031,2019,"One way to improve the performance of embedded systems is through heterogeneous platforms, i.e., using hardware containing more than one type of processor, like CPU + GPU or CPU + FPGA. This approach has shown improved performance, particularly in the domain of artificial intelligence, in which computationally demanding models must be trained and executed. However, these computational environments pose various challenges to software engineering, since applications must be designed and developed differently while accounting for target hardware architectures that are inherently different. Companies interested in migrating to heterogeneous platforms must be aware of the changes to the software development processes that are required to accommodate such solution. In this paper, we conducted a multiple case study that aims to discover the challenges and common practices when developing software for heterogeneous platforms in industrial contexts. First, we organized semi-structured interviews with companies on the automotive, automation, and telecommunication domains. Then, we analyzed and structured the data in order to create a classification describing the software engineering challenges faced by companies using heterogeneous platforms. The companies involved in this study are in different stages of maturity concerning the use of heterogeneous platforms. Thus, the classification takes into consideration these levels to describe the challenges accordingly."
"ICT Innovations, entrepreneurship and hubs in East Africa: The case of Ethiopia","Desta, T",10.1080/20421338.2018.1473064,2018,"This research set out to assess the state of ICT innovation in Ethiopia, then to profile and analyse the two Ethiopian ICT hubs (iceaddis and xHub). It also assessed the challenges to ICT innovation in Ethiopia and recommend solutions. The research used a range of qualitative research methods, which combined semi-structured interviews, participant observation, SWOT analysis and a review of secondary materials. The findings of the research have revealed that the state and nature of ICT innovation in Ethiopia can be described as low scale or household' type of innovation engaged in the modification or improvement of foreign products or processes rather than inventive innovations. Nevertheless, this is changing as more and more advanced industrial ICT companies are emerging such as iCog Labs, which work in the areas of cloud computing and Artificial Intelligence. Most of the innovations are in the areas of entertainment, information, financial and app software development. Ethiopian ICT hubs iceaddis and xHub are filling the innovation gap by offering a working space, training, investment and mentorship until the startups mature despite some internal weaknesses and possible threats. Infrastructure (connectivity) was the biggest challenge to innovation followed by government policies, investment, societal and educational shortcomings."
Natural chalcones elicit formation of specialized pro-resolving mediators and related 15-lipoxygenase products in human macrophages,"Kretzer, C; Jordan, PM; Meyer, KPL; Hoff, D; Werner, M; Hofstetter, RK; Koeberle, A; Peralta, AC; Viault, G; Seraphin, D; Richomme, P; Helesbeux, JJ; Stuppner, H; Temml, V; Schuster, D; Werz, O",10.1016/j.bcp.2021.114825,2022,"Specialized pro-resolving mediators (SPMs) comprise lipid mediators (LMs) produced from polyunsaturated fatty acids (PUFAs) via stereoselective oxygenation particularly involving 12/15-lipoxygenases (LOXs). In contrast to pro-inflammatory LMs such as leukotrienes formed by 5-LOX and prostaglandins formed by cyclooxygenases, the SPMs have anti-inflammatory and inflammation-resolving properties. Although glucocorticoids and nonsteroidal anti-inflammatory drugs (NSAIDs) that block prostaglandin production are still prime therapeutics for inflammation-related diseases despite severe side effects, novel concepts focus on SPMs as immunoresolvents for anti-inflammatory pharmacotherapy. Here, we studied the natural chalcone MF-14 and the corresponding dihydrochalcone MF-15 from Melodorum fruticosum, for modulating the biosynthesis of LM including leukotrienes, prostaglandins, SPM and their 12/15-LOX-derived precursors in human monocyte-derived macrophage (MDM) Ml- and M2-like phenotypes. In MDM challenged with Staphylococcus aureus-derived exotoxins both compounds (10 04) significantly suppressed 5-LOX product formation but increased the biosynthesis of 12/15-LOX products, especially in M2-MDM. Intriguingly, in resting M2-MDM, MF-14 and MF-15 strikingly evoked generation of 12/15-LOX products and of SPMs from liberated PUFAs, along with translocation of 15-LOX-1 to membranous compartments. Enhanced 12/15-LOX product formation by the chalcones was evident also when exogenous PUFAs were supplied, excluding increased substrate supply as sole underlying mechanism. Rather, MF-14 and MF-15 stimulate the activity of 15-LOX-1, supported by experiments with HEK293 cells transfected with either 5-LOX, 15-LOX-1 or 15-LOX-2. Together, the natural chalcone MF-14 and the dihydrochalcone MF-15 favorably modulate LM biosynthesis in human macrophages by suppressing pro-inflammatory leukotrienes but stimulating formation of SPMs by differential interference with 5-LOX and 15-LOX-1."
Automated data function extraction from textual requirements by leveraging semi-supervised CRF and language model,"Li, MY; Shi, L; Wang, YW; Wang, JJ; Wang, Q; Hu, J; Peng, XH; Liao, WM; Pi, GZ",10.1016/j.infsof.2021.106770,2022,"Context: Function Point Analysis (FPA) provides an objective, comparative measure for size estimation in the early stage of software development. When practicing FPA, analysts typically abide by the following steps: data function (DF) extraction, transactional function extraction, function type classification and adjustment factor determination. However, due to lack of approach and tool support, these steps are usually conduct by human efforts in practice. Related approaches can hardly be applied in the FPA due to the following three challenges, i.e., FPA rule-driven extraction, domain-specific parsing, and expensive labeled resources. Objective: In this paper, we aim to automate the extraction of DFs, which is the starting and fundamental step in FPA. Method: We propose an automated approach named DEX to extract data functions from textual requirements. Specifically, DEX introduces the popularly-used conditional random field (CRF) model to predict the boundary of a data function. Besides, DEX employs the bootstrapping-based algorithm and DF-oriented language model to further boost the performance. Results: We evaluate DEX from two aspects: evaluation on a real industrial dataset and a manual review by domain experts. The evaluation on the real industrial dataset shows that DEX could achieve 80% precision, 84% recall, and 82% F1, and outperforms three state-of-the-art baselines. The expert review suggests that DEX could increase 16% precision and 13% recall, compared with those produced by engineers. Conclusion: DEX could achieve promising results under a small number of labeled requirements and outperform the state-of-the-art approaches. Moreover, DEX could help engineers produce more accurate and complete DFs in the industrial environment."
"Morals, ethics, and the technology capabilities and limitations of automated and self-driving vehicles","Siegel, J; Pappas, G",10.1007/s00146-021-01277-y,2023,"We motivate the desire for self-driving and explain its potential and limitations, and explore the need for-and potential implementation of-morals, ethics, and other value systems as complementary capabilities to the Deep Technologies behind self-driving. We consider how the incorporation of such systems may drive or slow adoption of high automation within vehicles. First, we explore the role for morals, ethics, and other value systems in self-driving through a representative hypothetical dilemma faced by a self-driving car. Through the lens of engineering, we explain in simple terms common moral and ethical frameworks including utilitarianism, deontology, and virtue ethics before characterizing their relationship to the fundamental algorithms enabling self-driving. The concepts of behavior cloning, state-based modeling, and reinforcement learning are introduced, with some algorithms being more suitable for the implementation of value systems than others. We touch upon the contemporary cross-disciplinary landscape of morals and ethics in self-driving systems from a joint philosophical and technical perspective, and close with considerations for practitioners and the public, particularly as individuals may not appreciate the nuance and complexity of using imperfect information to navigate diverse scenarios and tough-to-quantify value systems, while typical software development reduces complex problems to black and white decision-making."
MicroMILTS: Fault Location for Microservices Based Mutual Information and LSTM Autoencoder,"Yang, LW; Li, J; Shi, KZ; Yang, SL; Yang, QF; Sun, JG",,2022,"Driven by the development of cloud computing and artificial intelligence, architecture has dramatically improved in terms of flexibility and scalability in software development. Therefore, it is increasingly being used to build large-scale applications for agile development. However, along with the technology heterogeneity, the dynamics of running instances, and the complexity of service dependencies, fault localization is extraordinarily difficult. In this paper, we present MicroMILTS, a microservice fault location method based on mutual information and an LSTM Autoencoder. MicroMILTS first uses BIRCH for anomaly detection based on the analysis of the performance metrics data correlated to microservice anomalies. Once anomalies are detected, a service dependency property graph is constructed based on the real-time microservice invocation relationships and the reconstructed deviations of performance metrics with the LSTM Autoencoder. Next, MicroMILTS dynamically updates the weight of each node in the service dependency property graph. Then, a PageRank-based random walk is applied for further ranking root causes. Finally, a Sock-shop microservice system is built on the Huawei Cloud to evaluate the performance of MicroMILTS. The experiment shows that MicroMILTS achieves a good root cause location result, with 90.4% in precision and 91.6% in mean average precision, outperforming state-of-the-art methods."
Predicting Multiple Domain Queue Waiting Time via Machine Learning,"Loureiro, C; Pereira, PJ; Cortez, P; Guimaraes, P; Moreira, C; Pinho, A",10.1007/978-3-031-36805-9_27,2023,"This paper describes an implementation of the CrossIndustry Standard Process for Data Mining (CRISP-DM) methodology for a demonstrative case of human queue waiting time prediction. We collaborated with a multiple domain (e.g., bank, pharmacies) ticket management service software development company, aiming to study a Machine Learning (ML) approach to estimate queue waiting time. A large multiple domain database was analyzed, which included millions of records related with two time periods (one year, for the modeling experiments; and two year, for a deployment simulation). The data was first preprocessed (including data cleaning and feature engineering tasks) and then modeled by exploring five state-of-the-art ML regression algorithms and four input attribute selections (including newly engineered features). Furthermore, the ML approaches were compared with the estimation method currently adopted by the analyzed company. The computational experiments assumed two main validation procedures, a standard cross-validation and a Rolling Window scheme. Overall, competitive and quality results were obtained by an Automated ML (AutoML) algorithm fed with newly engineered features. Indeed, the proposed AutoML model produces a small error (from 5 to 7 min), while requiring a reasonable computational effort. Finally, an eXplainable Artificial Intelligence (XAI) approach was applied to a trained AutoML model, demonstrating the extraction of useful explanatory knowledge for this domain."
Transferable Student Performance Modeling for Intelligent Tutoring Systems,"Schmucker, R; Mitchell, TM",,2022,"Millions of students worldwide are now using intelligent tutoring systems (ITSs). At their core, ITSs rely on student performance models (SPMs) to trace each student's changing ability level over time, in order to provide personalized feedback and instruction. Crucially, SPMs are trained using interaction sequence data of previous students to analyze data generated by future students. This induces a cold-start problem when a new course is introduced, because no students have yet taken the course and hence there is no data to train the SPM. Here, we consider transfer learning techniques to train accurate SPMs for new courses by leveraging log data from existing courses. We study two settings: (i) In the naive transfer setting, we first train SPMs on existing course data and then apply these SPMs to new courses without modification. (ii) In the inductive transfer setting, we fine tune these SPMs using a small amount of training data from the new course (e.g., collected during a pilot study). We evaluate the proposed techniques using student interaction sequence data from five different mathematics courses taken by over 47,000 students. The naive transfer models that use features provided by human domain experts (e.g., difficulty ratings for questions in the new course) but no student interaction training data for the new course, achieve prediction accuracy on par with standard BKT and PFA models that use training data from thousands of students in the new course. In the inductive setting our transfer approach yields more accurate predictions than conventional SPMs when only limited student interaction training data (<100 students) is available to both."
Mining Project Failure Indicators From Big Data Using Machine Learning Mixed Methods,"Strang, KD; Vajjhala, NR",10.4018/IJITPM.317221,2023,"The literature revealed approximately 50% of IT-related projects around the world fail, which must frustrate a sponsor or decision maker since their ability to forecast success is statistically about the same as guessing with a random coin toss. Nonetheless, some project success/failure factors have been identified, but often the effect sizes were statistically negligible. A pragmatic mixed methods recursive approach was applied, using structured programming, machine learning (ML), and statistical software to mine a large data source for probable project success/failure indicators. Seven feature indicators were detected from ML, producing an accuracy of 79.9%, a recall rate of 81%, an F1 score of 0.798, and a ROCa of 0.849. A post-hoc regression model confirmed three indicators were significant with a 27% effect size. The contributions made to the body of knowledge included: A conceptual model comparing ML methods by artificial intelligence capability and research decision making goal, a mixed methods recursive pragmatic research design, application of the random forest ML technique with post hoc statistical methods, and a preliminary list of IT project failure indicators analyzed from big data."
eTagger - An Energy Pattern Tagging Tool for GitHub Issues in Android Projects,"Shanbhag, S; Chimalakonda, S; Sharma, VS; Kaulgud, V",10.1109/ICSME55016.2022.00064,2022,"Energy efficiency is an essential consideration in mobile application development, given that these apps run on battery-powered devices. This has led the researchers to develop a set of energy design patterns that can help the developers improve the energy efficiency of their applications. However, the adoption of these energy patterns in projects remains a challenge, given the lack of awareness about these patterns among the developers. To bridge this gap, we propose our tool eTagger, a Google Chrome extension that tags GitHub issues from Android repositories with associated energy patterns. eTagger works based on the embeddings generated by Sentence-BERT. We believe that labeling the GitHub issues with energy patterns may help towards their larger adoption as GitHub is a prominent platform in collaborative software development. A preliminary evaluation of eTagger achieved an AUC-ROC of 0.73 with a precision of 0.58, recall of 0.53 and an F1-score of 0.5. The demonstration of the tool is available at https://youtu.be/hP4pWJ4AKxE"
Re-engineering Software Engineering for a Data-centric World,"Kim, M",,2019,"With the development of big data, machine learning, and AI, existing software engineering techniques must be re-imagined to provide the productivity gains that developers desire. This talk will review emerging roles of data scientists and the tools they need to build scalable, correct, and efficient software for a data centric world. Kim will present a large-scale study of about 800 data scientists in collaboration with Microsoft Research, which looked at data scientists' educational background, problem topics that they work on, tools they use, and activities. From the gathered data, she has identified nine distinct clusters of data scientists and best practices and challenges faced by each cluster. In the second half of this talk, she will discuss the needs of re-targeting SE research community's directions to address new challenges in the era of data-centric software development. In particular, she will detail some examples of her group's work that re-invents debugging and testing for big data distributed systems such as Apache Spark. She will conclude with open SE problems in ML and heterogeneous computing that support data-centric software development."
Analysis of Selected Characteristics of Open Data Inception Portals in the Context of Smart Cities IoT Data Accessibility,"Dymora, P; Mazurek, M; Kowal, B",10.5220/0010117600670074,2020,"In this study, we focus on Open Government Data, which is the sphere of public services where such type of data can be useful. In the Industry 4.0 concept, the primary data source is the IoT infrastructure. Open Data is of considerable importance for the software development process. The issue of Open Data is becoming a significant challenge nowadays. Especially when it comes to preparing data for sharing, analyzing it, and searching for hidden dependencies, which opens up new possibilities for computing and artificial intelligence. The paper shows that the architecture of solutions existing, e.g., in Poland, follows global trends. Together with statistics based on the Socrata portal, it can be noticed that these data can be and are successfully used for data processing. New methods and software are being developed for processing data as we write. The vast majority of software is data-driven, and data are needed for verification and validation. The article presents a comprehensive analysis of available open data portals with data. j son files as also the analysis of the most commonly used data formats for Open Data Network portal databases."
SampleFix: Learning to Generate Functionally Diverse Fixes,"Hajipour, H; Bhattacharyya, A; Staicu, CA; Fritz, M",10.1007/978-3-030-93733-1_8,2021,"Automatic program repair holds the potential of dramatically improving the productivity of programmers during the software development process and correctness of software in general. Recent advances in machine learning, deep learning, and NLP have rekindled the hope to eventually fully automate the process of repairing programs. However, previous approaches that aim to predict a single fix are prone to fail due to uncertainty about the true intend of the programmer. Therefore, we propose a generative model that learns a distribution over potential fixes. Our model is formulated as a deep conditional variational autoencoder that can efficiently sample fixes for a given erroneous program. In order to ensure diverse solutions, we propose a novel regularizer that encourages diversity over a semantic embedding space. Our evaluations on common programming errors show for the first time the generation of diverse fixes and strong improvements over the state-of-the-art approaches by fixing up to 45% of the erroneous programs. We additionally show that for the 65% of the repaired programs, our approach was able to generate multiple programs with diverse functionalities."
More Effective Test Case Generation with Multiple Tribes of AI,"Olsthoorn, M",10.1145/3510454.3517066,2022,"Software testing is a critical activity in the software development life cycle for quality assurance. Automated Test Case Generation (TCG) can assist developers by speeding up this process. It accomplishes this by evolving an initial set of randomly generated test cases over time to optimize for predefined coverage criteria. One of the key challenges for automated TCG approaches is navigating the large input space. Existing state-of-the-art TCG algorithms struggle with generating highly-structured input data and preserving patterns in test structures, among others. I hypothesize that combining multiple tribes of AI can improve the effectiveness and efficiency of automated TCG. To test this hypothesis, I propose using grammar-based fuzzing and machine learning to augment evolutionary algorithms for generating more structured input data and preserving promising patterns within test cases. Additionally, I propose to use behavioral modeling and interprocedural control dependency analysis to improve test effectiveness. Finally, I propose integrating these novel approaches into a testing framework to promote the adoption of automated TCG in industry."
RETRACTED: General Industrial Environment and Health Design Software Using a Small Data-Driven Neural Network Model (Retracted Article),"Xu, LJ; Zheng, J; Gao, J; Chen, W; Chen, L",10.1155/2022/1768446,2022,"The developed enterprise intellectual brain neural network platform for industrial environment and health design driven by small data converts the wisdom knowledge of enterprise designers into data and helps enterprises retain design experience, accumulate design knowledge results, sort out the design process, and shorten the design cycle. The goal of this project is to combine artificial intelligence, big data, the Internet of things, parameterization, and other computer technologies with the industrial environment and health design and development process of manufacturing enterprises to solve pain points for businesses and designers, as well as to develop and create a large-scale product-level general parameter intelligent design software. The development of this software project can greatly improve the work efficiency of designers, save the time spent by designers on low-end repetitive labor, and in turn promote designers to engage in more valuable creative work and realize the digitization of entire product life cycle. It can also improve the inefficient and redundant work efficiency of designers, and designers can switch between multiple roles. At the same time, a design think tank is formed through the accumulation of enterprise design knowledge through data, and the neural network platform development of big data drives the design brain, that is, an efficient and intelligent industrial environment and health design expert system is generated, just like the enterprise brain, including expert think tanks, internal technical data encryption of enterprises interface, enterprise core technology think tank, enterprise production resources, risk control standard library, etc."
Using Commonsense Knowledge and Text Mining for Implicit Requirements Localization,"Onyeka, E; Varde, AS; Anu, V; Tandon, N; Daramola, O",10.1109/ICTAI50040.2020.00146,2020,"This paper addresses identification of implicit requirements (IMRs) in software requirements specifications (SRS). IMRs, as opposed to explicit requirements, are not specified by users but are more subtle. It has been noticed that IMRs are crucial to the success of software development. In this paper, we demonstrate a software tool called COTIR developed by us as a system that integrates Commonsense knowledge, Ontology and Text mining for early identification of Implicit Requirements. This relieves human software engineers from the tedious task of manually identifying IMRs in huge SRS documents. Our evaluation reveals that COTIR outperforms existing IMR tools. This demo paper would be useful to Software Engineers since it deals with automation in the requirements analysis phase, thus contributing to Requirements Engineering. It would interest AI scientists as it entails multi-disciplinary work encompassing text mining, ontology and commonsense knowledge. It makes a broader impact on Smart Cities, because automated identification of IMRs would offer inputs to Smart City Tools, where requirements may often be implicit given that Smart Cities are an emerging and growing paradigm."
Broken Promises: Measuring Confounding Effects in Learning-based Vulnerability Discovery,"Imgrund, E; Ganz, T; HÃ¤rterich, M; Pirch, L; Risse, N; Rieck, K",10.1145/3605764.3623915,2023,"Several learning-based vulnerability detection methods have been proposed to assist developers during the secure software development life-cycle. In particular, recent learning-based large transformer networks have shown remarkably high performance in various vulnerability detection and localization benchmarks. However, these models have also been shown to have difficulties accurately locating the root cause of flaws and generalizing to out-of-distribution samples. In this work, we investigate this problem and identify spurious correlations as the main obstacle to transferability and generalization, resulting in performance losses of up to 30% for current models. We propose a method to measure the impact of these spurious correlations on learning models and estimate their true, unbiased performance. We present several strategies to counteract the underlying confounding bias, but ultimately our work highlights the limitations of evaluations in the laboratory for complex learning tasks such as vulnerability discovery."
Deep Learning Based Program Generation From Requirements Text: Are We There Yet?,"Liu, H; Shen, MZ; Zhu, JQ; Niu, N; Li, G; Zhang, L",10.1109/TSE.2020.3018481,2022,"To release developers from time-consuming software development, many approaches have been proposed to generate source code automatically according to software requirements. With significant advances in deep learning and natural language processing, deep learning-based approaches are proposed to generate source code from natural language descriptions. The key insight is that given a large corpus of software requirements and their corresponding implementations, advanced deep learning techniques may learn how to translate software requirements into source code that fulfill such requirements. Although such approaches are reported to be highly accurate, they are evaluated on datasets that are rather small, lack of diversity, and significantly different from real-world software requirements. To this end, we build a large scale dataset that is composed of longer requirements as well as validated implementations. We evaluate the state-of-the-art approaches on this new dataset, and the results suggest that their performance on our dataset is significantly lower than that on existing datasets concerning the common metrics, i.e., BLEU. Evaluation results also suggest that the generated programs often contain syntactic and semantical errors, and none of them can pass even a single predefined test case. Further analysis reveals that the state-of-the-art approaches learn little from software requirements, and most of the successfully generated statements are popular statements in the training programs. Based on this finding, we propose a popularity-based approach that always generates the most popular statements in training programs regardless of the input (software requirements). Evaluation results suggest that none of the state-of-the-art approaches can outperform this simple statistics-based approach. As a conclusion, deep learning-based program generation requires significant improvement in the future, and our dataset may serve as a basis for future research in this direction."
A Comprehensive Study on Code Clones in Automated Driving Software,"Mo, R; Jiang, YJ; Zhan, WJ; Wang, DY; Li, ZY",10.1109/ASE56229.2023.00053,2023,"With the continuous improvement of artificial intelligence technology, autonomous driving technology has been greatly developed. Hence automated driving software has drawn more and more attention from both researchers and practitioners. Code clone is a commonly used to speed up the development cycle in software development, but many studies have shown that code clones may affect software maintainability. Currently, there is little research investigating code clones in automated driving software. To bridge this gap, we conduct a comprehensive experience study on the code clones in automated driving software. Through the analysis of Apollo and Autoware, we have presented that code clones are prevalent in automated driving software. about 30% of code lines are involved in code clones and more than 50% of files contain code clones. Moreover, a notable portion of these code clones has caused bugs and co-modifications. Due to the high complexity of autonomous driving, the automated driving software is often designed to be modular, with each module responsible for a single task. When considering each module individually, we have found that Perception, Planning, Canbus, and Sensing modules are more likely to encounter code clones, and more likely to have bug-prone and co-modified clones. Finally, we have shown that there exist cross-module clones to propagate bugs and co-modifications in different modules, which undermine the software's modularity."
Cross-Project Transfer Representation Learning for Vulnerable Function Discovery,"Lin, GJ; Zhang, J; Luo, W; Pan, L; Xiang, Y; De Vel, O; Montague, P",10.1109/TII.2018.2821768,2018,"Machine learning is now widely used to detect security vulnerabilities in the software, even before the software is released. But its potential is often severely compromised at the early stage of a software project when we face a shortage of high-quality training data and have to rely on overly generic hand-crafted features. This paper addresses this cold-start problem of machine learning, by learning rich features that generalize across similar projects. To reach an optimal balance between feature-richness and generalizability, we devise a data-driven method including the following innovative ideas. First, the code semantics are revealed through serialized abstract syntax trees (ASTs), with tokens encoded by Continuous Bag-of-Words neural embeddings. Next, the serialized ASTs are fed to a sequential deep learning classifier (Bi-LSTM) to obtain a representation indicative of software vulnerability. Finally, the neural representation obtained from existing software projects is then transferred to the new project to enable early vulnerability detection even with a small set of training labels. To validate this vulnerability detection approach, we manually labeled 457 vulnerable functions and collected 30 000+ nonvulnerable functions from six open-source projects. The empirical results confirmed that the trained model is capable of generating representations that are indicative of program vulnerability and is adaptable across multiple projects. Compared with the traditional code metrics, our transfer-learned representations are more effective for predicting vulnerable functions, both within a project and across multiple projects."
DiffTech: A Tool for Differencing Similar Technologies from Question-and-Answer Discussions,"Wang, H; Chen, CY; Xing, ZC; Grundy, J",10.1145/3368089.3417931,2020,"Developers can use different technologies for different software development tasks in their work. However, when faced with several technologies with comparable functionalities, it can be challenging for developers to select the most appropriate one, as trial and error comparisons among such technologies are time-consuming. Instead, developers resort to expert articles, read official documents or ask questions in Q&A sites for technology comparison. However, it is still very opportunistic whether they will get a comprehensive comparison, as online information is often fragmented, contradictory and biased. To overcome these limitations, we propose the DIFFTECH system that exploits the crowd sourced discussions from Stack Overflow, and assists technology comparison with an informative summary of different comparison aspects. We found 19,118 comparative sentences from 2,410 pairs of comparable technologies. We released our DIFFTECH website for public use. Our website attracts over 1800 users and we also receive some positive comments on social media. A walkthrough video of the tool demo: https://www.youtube.com/ watch?v= ixX41DXRNsI Website link: https://difftech.herokuapp.com/"
Program Merge Conflict Resolution via Neural Transformers,"Svyatkovskiy, A; Fakhoury, S; Ghorbani, N; Mytkowicz, T; Dinella, E; Bird, C; Jang, J; Sundaresan, N; Lahiri, SK",10.1145/3540250.3549163,2022,"Collaborative software development is an integral part of the modern software development life cycle, essential to the success of large-scale software projects. When multiple developers make concurrent changes around the same lines of code, a merge conflict may occur. Such conflicts stall pull requests and continuous integration pipelines for hours to several days, seriously hurting developer productivity. To address this problem, we introduce MergeBERT, a novel neural program merge framework based on token-level three-way differencing and a transformer encoder model. By exploiting the restricted nature of merge conflict resolutions, we reformulate the task of generating the resolution sequence as a classification task over a set of primitive merge patterns extracted from real-world merge commit data. Our model achieves 63-68% accuracy for merge resolution synthesis, yielding nearly a 3x performance improvement over existing semi-structured, and 2x improvement over neural program merge tools. Finally, we demonstrate that MergeBERT is sufficiently flexible to work with source code files in Java, JavaScript, TypeScript, and C# programming languages. To measure the practical use of MergeBERT, we conduct a user study to evaluate MergeBERT suggestions with 25 developers from large OSS projects on 122 real-world conflicts they encountered. Results suggest that in practice, MergeBERT resolutions would be accepted at a higher rate than estimated by automatic metrics for precision and accuracy. Additionally, we use participant feedback to identify future avenues for improvement of MergeBERT."
PYMT5: multi-mode translation of natural language and PYTHON code with transformers,"Clement, CB; Drain, D; Timcheck, J; Svyatkovskiy, A; Sundaresan, N",,2020,"Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PYMT5, the PYTHON method text-to-text transfer transformer, which is trained to translate between all pairs of PYTHON method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million PYTHON methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PYMT5 outperforms similarlysized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CODE-SEARCHNET test set, our best model predicts 92.1% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation."
"Advances in Designing a Student-Centered Learning Process using Cutting-Edge Methods, Tools, and Artificial Intelligence: An E-Learning Platform","Serban, C; Vescan, A",10.1145/3340435.3342716,2019,"It is well known the fact that learning process is difficult for learners and at the same time it raises problems for those who teach. Teaching Software Engineering for undergraduate students is an assiduous and a challenging task due to its level of abstraction, to frequently changes that appear in programming paradigms and in software development methodologies. In this paper we provide a novel approach in teaching Advanced Programming Methods, the third introductory course in Software Engineering that is being taught at our faculty within the Computer Science Curriculum for undergraduate students. The contribution of this paper is threefold: firstly, we design a student-centered learning process intertwining cutting edge methods like for instance project-based learning, self assessment-based learning and students engagement. Secondly, we design an E-learning platform to provide for students an automated assessment and appropriate feedback and, most important, to offer them support throughout the learning process. Thirdly, we provide a quantitative and qualitative analysis over 3 years of teaching Advanced Programming Methods course, by applying the proposed methodology. Our analysis results show the effectiveness of our approach. Key contributions in this paper are our proposed E-learning platform and the analysis findings."
Farming on the edge: Architectural Goals,"Carvalho, A; O' Mahony, N; Krpalkova, L; Campbell, S; Walsh, J; Doody, P",10.1109/agro-geoinformatics.2019.8820424,2019,"This research investigates how advances in Internet of Things (IoT) and availability of internet connection would enable Edge Solutions to promote smart utilization of existing machines at the edge. The presented results are based on experiments performed in real scenarios using the proposed solution. Whereas scenarios were cloned from real environments it is important to have in mind that experiments were performed with low load in terms of data and small number of devices in terms of distribution. As result of extensive architecture investigation for an optimal edge solution and its possible correlation to industrial applications, this paper will provide evidences supporting the use of edge solutions in challenging conditions which arise at the edge, including smart factories and smart agriculture. The present work assumes that the reader has some exposition to Edge computing, Cloud computing and software development. The paper will present some important findings on this area, compare main architectural aspects and will provide a broad view of how edge solutions might be built for this particular scenario. Having discussed how the ideal architecture works and having provided an overview about how it may be applied to industrial plants, the final section of this paper addresses how artificial intelligence will fit into edge solutions, forming a new source of smart capabilities to existing environments."
Duplicate Bug Report Detection and Classification System Based on Deep Learning Technique,"Kukkar, A; Mohana, R; Kumar, Y; Nayyar, A; Bilal, M; Kwak, KS",10.1109/ACCESS.2020.3033045,2020,"Duplicate bug report detection is a process of finding a duplicate bug report in the bug tracking system. This process is essential to avoid unnecessary work and rediscovery. In typical bug tracking systems, more than thousands of duplicate bug reports are reported every day. In turn, human cost, effort and time are increased. This makes it an important problem in the software management process. The solution is to automate the duplicate bug report detection system for reducing the manual effort, thus the productivity of triager's and developer's is increased. It also speeds up the process of software management as a result software maintenance cost is also reduced. However, existing systems are not quite accurate yet, in spite of these systems used various machine learning approaches. In this work, an automatic bug report detection and classification model is proposed using deep learning technique. The proposed system has three modules i.e. Preprocessing, Deep Learning Model and Duplicate Bug report Detection and Classification. Further, the proposed model used Convolutional Neural Network based deep learning model to extract relevant feature. These relevant features are used to determine the similar features of bug reports. Hence, the bug reports similarity is computers through these similar features. The performance of the proposed system is evaluated on six publicly available datasets using six performance metrics. It is noticed that the proposed system outperforms the existing systems by achieving an accuracy rate in the range of 85% to 99 % and recall@k rate in between 79%-94%. Moreover, the effectiveness of the proposed system is also measured on the cross training datasets of same and different domain. The proposed system achieves a good high accuracy rate for same domain data sets and low accuracy rate for different domain datasets."
An Audio Personal Health Library of Clinic Visit Recordings for Patients and Their Caregivers (HealthPAL): User-Centered Design Approach,"Barr, PJ; Haslett, W; Dannenberg, MD; Oh, L; Elwyn, G; Hassanpour, S; Bonasia, KL; Finora, JC; Schoonmaker, JA; Onsando, WM; Ryan, J; Bruce, ML; Das, AK; Arend, R; Piper, S; Ganoe, CH",10.2196/25512,2021,"Background: Providing digital recordings of clinic visits to patients has emerged as a strategy to promote patient and family engagement in care. With advances in natural language processing, an opportunity exists to maximize the value of visit recordings for patients by automatically tagging key visit information (eg, medications, tests, and imaging) and linkages to trustworthy web-based resources curated in an audio-based personal health library. Objective: This study aims to report on the user-centered development of HealthPAL, an audio personal health library. Methods: Our user-centered design and usability evaluation approach incorporated iterative rounds of video-recorded sessions from 2016 to 2019. We recruited participants from a range of community settings to represent older patient and caregiver perspectives. In the first round, we used paper prototypes and focused on feature envisionment. We moved to low-fidelity and high-fidelity versions of the HealthPAL in later rounds, which focused on functionality and use; all sessions included a debriefing interview. Participants listened to a deidentified, standardized primary care visit recording before completing a series of tasks (eg, finding where a medication was discussed in the recording). In the final round, we recorded the patients' primary care clinic visits for use in the session. Findings from each round informed the agile software development process. Task completion and critical incidents were recorded in each round, and the System Usability Scale was completed by participants using the digital prototype in later rounds. Results: We completed 5 rounds of usability sessions with 40 participants, of whom 25 (63%) were women with a median age of 68 years (range 23-89). Feedback from sessions resulted in color-coding and highlighting of information tags, a more prominent play button, clearer structure to move between one's own recordings and others' recordings, the ability to filter recording content by the topic discussed and descriptions, 10-second forward and rewind controls, and a help link and search bar. Perceived usability increased over the rounds, with a median System Usability Scale of 78.2 (range 20-100) in the final round. Participants were overwhelmingly positive about the concept of accessing a curated audio recording of a clinic visit. Some participants reported concerns about privacy and the computer-based skills necessary to access recordings. Conclusions: To our knowledge, HealthPAL is the first patient-centered app designed to allow patients and their caregivers to access easy-to-navigate recordings of clinic visits, with key concepts tagged and hyperlinks to further information provided. The HealthPAL user interface has been rigorously co-designed with older adult patients and their caregivers and is now ready for further field testing. The successful development and use of HealthPAL may help improve the ability of patients to manage their own care, especially older adult patients who have to navigate complex treatment plans."
Development of music teaching software based on neural network algorithm and user analysis,"Xuelian, H",10.1007/s00500-023-08641-8,2023,"At this stage, music teaching is facing an increasingly serious shortage of teacher resources. Therefore, it is particularly important to develop a music teaching software by using computer-assisted music teaching activities. First of all, the operation method program of this system is carefully designed according to the principles of computer network technology. Using the performance characteristics of Fourier transform and its enhanced functions to extract music, the priority key system modules are designed according to the system structure framework and data processing program, and the main design code is provided. With the development of artificial intelligence technology, neural network has gradually become an important research method in this field. And compared with the traditional mechanical learning methods, the neural network-based method has the advantages of simple algorithm mode, good universality, strong robustness, organic and mobility. With the rapid change of in-depth learning technology, music teaching software has shown great overall advantages in the accuracy and speed of detection. In addition, this paper analyzes the specific user level of music teaching programs, focusing on their interest in and specific acceptance of these music teaching programs, as well as the use of user feedback to develop specific and effective music teaching programs. Neural network algorithm and user analysis provide a new strategy for developing music teaching software."
Towards a Gamified Support Tool for Requirements Gathering in Bahasa Indonesia,"Saphira, M; Rusli, A",10.1109/conmedia46929.2019.8981828,2019,"In this modern world, software products have significant impacts on every aspect of peoples' life. Requirements engineering is one of the most critical activities in software development in which the foundation of the product is laid. In order to provide structure in requirements, the user stories concept is often used. The user story concept emphasizes information in three central parts, the stakeholder's role, the requirement, and the business value which will be satisfied by the requirement. Stakeholders' participation is crucial in gathering, eliciting, and analyzing requirements; however, frequently due to circumstances, stakeholders provide minimum information related to the software requirements. In order to increase user's participation in providing stories related to the software requirements, our research aims to develop a gamified web-based support tool for the stakeholders and engineers to use in gathering requirements while implementing the user story concept. The gamified support tool is then evaluated to assess the system adoption regarding its use to support users in providing requirements regarding a specific software product. Results show that the support tool receives scores as follow, 81.1% for Joy, 73.4% for Control, 67.2% for Focused Immersion, 60.6% for Temporal Dissociation, 80% for Curiosity, 76.8% for Perceived Ease of Use, 72.9% for Perceived Usefulness, and 76.2% for Behavioral Intention to Use. Based on the interview with the users, the support tool shows excellent potential to be further developed to support the requirements engineering activity in software development."
RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization,"Kamezawa, H; Nishida, N; Shimizu, N; Miyazaki, T; Nakayama, H",,2022,"A release note is a technical document that describes the latest changes to a software product and is crucial in open source software development. However, it still remains challenging to generate release notes automatically. In this paper, we present a new dataset called RNSum, which contains approximately 82,000 English release notes and the associated commit messages derived from the online repositories in GitHub. Then, we propose classwise extractivethen-abstractive/abstractive summarization approaches to this task, which can employ a modern transformer-based seq2seq network like BART and can be applied to various repositories without specific constraints. The experimental results on the RNSum dataset show that the proposed methods can generate less noisy release notes at higher coverage than the baselines. We also observe that there is a significant gap in the coverage of essential information when compared to human references. Our dataset and the code are publicly available."
Bridging the Gap: The Critical Role of Regulatory Affairs and Clinical Affairs in the Total Product Life Cycle of Pathology Imaging Devices and Software,"Kearney, SJ; Lowe, A; Lennerz, JK; Parwani, A; Bui, MM; Wack, K; Giannini, G; Abels, E",10.3389/fmed.2021.765385,2021,"Manufacturers of pathology imaging devices and associated software engage regulatory affairs and clinical affairs (RACA) throughout the Total Product Life Cycle (TPLC) of regulated products. A number of manufacturers, pathologists, and end users are not familiar with how RACA involvement benefits each stage of the TPLC. RACA professionals are important contributors to product development and deployment strategies because these professionals maintain an understanding of the scientific, technical, and clinical aspects of biomedical product regulation, as well as the relevant knowledge of regulatory requirements, policies, and market trends for both local and global regulations and standards. Defining a regulatory and clinical strategy at the beginning of product design enables early evaluation of risks and provides assurance that the collected evidence supports the product's clinical claims (e.g., in a marketing application), its safe and effective use, and potential reimbursement strategies. It is recommended to involve RACA early and throughout the TPLC to assist with navigating changes in the regulatory environment and dynamic diagnostic market. Here we outline how various stakeholders can utilize RACA to navigate the nuanced landscape behind the development and use of clinical diagnostic products. Collectively, this work emphasizes the critical importance of RACA as an integral part of product development and, thereby, sustained innovation."
TROBO: A Novel Deep Transfer Model for Enhancing Cross-Project Bug Localization,"Zhu, ZY; Wang, Y; Li, Y",10.1007/978-3-030-82136-4_43,2021,"Bug localization, which aims to locate buggy files in the software project by leveraging bug reports, plays an important role in software quality control. Recently, many automatic bug localization methods based on historical bug-fix data (i.e., bug reports labeled with corresponding buggy code files) have been proposed. However, the lack of bug-fix data for software projects in the early stages of development limits the performance of most existing supervised learning methods. To address this issue, we propose a deep transfer bug localization model called TroBo, which can transfer shared knowledge from label-rich source project to the target project. Specifically, we accomplish the knowledge transfer on both the bug report and code file. For processing bug reports, which belong to informal text data, we design a soft attention-based module to alleviate the noise problem. For processing code files, we apply an adversarial strategy to learn the project-shared features, and additionally extract project-exclusive features for each project. Furthermore, a project-aware classifier is introduced in TroBo to avoid redundancy between shared and exclusive features. Extensive experiments on four large-scale real-world projects demonstrate that our model significantly outperforms the state-of-the-art techniques."
API recommendation for event-driven Android application development,"Yuan, WZ; Nguyen, HH; Jiang, LX; Chen, YT; Zhao, JJ; Yu, HB",10.1016/j.infsof.2018.10.010,2019,"Context: Software development is increasingly dependent on existing libraries. Developers need help to find suitable library APIs. Although many studies have been proposed to recommend relevant functional APIs that can be invoked for implementing a functionality, few studies have paid attention to an orthogonal need associated with event-driven programming frameworks, such as the Android framework. In addition to invoking functional APIs, Android developers need to know where to place functional code according to various events that may be triggered within the framework. Objective: This paper aims to develop an API recommendation engine for Android application development that can recommend both (1) functional APIs for implementing a functionality and (2) the event callback APIs that are to be overridden to contain the functional code. Method: We carry out an empirical study on actual Android programming questions from StackOverflow to confirm the need of recommending callbacks. Then we build Android-specific API databases to contain the correlations among various functionalities and APIs, based on customized parsing of code snippets and natural language processing of texts in Android tutorials and SDK documents, and then textual and code similarity metrics are adapted for recommending relevant APIs. Results: We have evaluated our prototype recommendation engine, named LibraryGuru, with about 1500 questions on Android programming from StackOverflow, and demonstrated that our top-5 results on recommending callbacks and functional APIs can on estimate achieve up to 43.5% and 50.9% respectively in precision, 24.6% and 32.5% respectively in mean average precision (MAP) scores, and 51.1% and 44.0% respectively in recall. Conclusion: We conclude that it is important and possible to recommend both functional APIs and callbacks for Android application development, and future work is needed to take more data sources into consideration to make more relevant recommendations for developers needs."
Trends of Intangibles and Intellectual Capital: State of Art and Research,"Matos, F; Vairinhos, V; Matos, AJ",,2019,"Conference proceedings about intellectual capital and knowledge management are important sources of current ideas about intellectual capital, intangibles, knowledge management, authors, institutions, trends and how these are related. Since those meetings are periodic concentrations of the main sources - the papers' authors - of innovative ideas about those subjects, it is believed that an adequate analysis and synthesis of those documents can be useful to identify emerging concepts, topics, trends, directions and relations involving those concepts, their creators and the places where they were presented. The purpose of this paper is to provide, using as a data source the texts of conference proceedings, a comprehensive knowledge about the state of art of the research on intangibles and intellectual capital over the last decade and to identify the trends on those issues for future research. This study consists of a review of abstracts, titles, authors' names, emails and institutions, keywords and main texts of all the papers in the Proceedings of the European Conference on Intellectual Capital, presented between 2009 and 2017. The study also involves the identification and characterization of patterns, such as the main topics subjacent to such texts, including associations of concepts, and the trends of such associations, involving concepts of intellectual capital and intangibles, throughout that period and conference locations. The innovative methodology used in this study is text mining, based on the classic bag-of-words model and in more recent natural language processing approaches, incorporated in R or Python packages. This work also highlights some needs not covered by the present packages and presents directions for future researches and software development. The paper can be classified as a pilot study to support the construction of new computational and knowledge management methodologies in this area."
A Systematic Mapping Study of the Advancement in Software Vulnerability Forecasting,"Gautier, A; Whitehead, C; Dzielski, D; Devine, T; Hernandez, J",10.1109/SoutheastCon51012.2023.10115111,2023,"Developing software securely remains a challenge even with great advancements made in AI, vulnerability databases, dynamic and static code analysis. Research into aspects of detecting, identifying, and refactoring vulnerabilities has shown promise, but further work is needed throughout all stages of the software development lifecycle. We will show that current research suffers from a lack of focus on forecasting where vulnerabilities will occur. Investigations into predicting vulnerabilities in a reliable, scalable way provide developers with insights that could prevent security failures in the future. Increasing software security is an initiative which can aid efforts from national security to saving lives affected by hospital ransomware attacks. Through our first questions, we seek to provide an overview of the vulnerabilities that current researchers are targeting and the methods they use to uncover them. Subsequently, we investigated the fields of impact that the research is contributing to and look for unique contributions. As this paper shows, wide distribution coupled with poor security practices has increased the attack angles available to malicious actors and is statistically linked with increased vulnerabilities. We argue that combinations of automated and manual techniques are necessary to achieve more impactful and adaptable methods to increase security in software development."
Balancing Performance and Portability with Containers in HPC: An OpenSHMEM Example,"Naughton, T; Sorrillo, L; Simpson, A; Imam, N",10.1007/978-3-319-73814-7_9,2018,"There is a growing interest in using Linux containers to streamline software development and application deployment. A container enables the user to bundle the salient elements of the software stack from an application's perspective. In this paper, we discuss initial experiences in using the Open MPI implementation of OpenSHMEM with containers on HPC resources. We provide a brief overview of two container runtimes, Docker & Singularity, highlighting elements that are of interest for HPC users. The Docker platform offers a rich set of services that are widely used in enterprise environments, whereas Singularity is an emerging container runtime that is specifically written for use on HPC systems. We describe our procedure for container assembly and deployment that strives to maintain the portability of the container-based application. We show performance results for the Graph500 benchmark running along the typical continuum of development testbed up to full production supercomputer (ORNL's Titan). The results show consistent performance between the native and Singularity (container) tests. The results also showed an unexplained drop in performance when using the Cray Gemini network with Open MPI's OpenSHMEM, which was unrelated to the container usage."
BugListener: Identifying and Synthesizing Bug Reports from Collaborative Live Chats,"Shi, L; Mu, FW; Zhang, YM; Yang, Y; Chen, JJ; Chen, X; Jiang, HZ; Jiang, ZY; Wang, Q",10.1145/3510003.3510108,2022,"In community-based software development, developers frequently rely on live-chatting to discuss emergent bugs/errors they encounter in daily development tasks. However, it remains a challenging task to accurately record such knowledge due to the noisy nature of interleaved dialogs in live chat data. In this paper, we first formulate the task of identifying and synthesizing bug reports from community live chats, and propose a novel approach, named BugListener, to address the challenges. Specifically, BugListener automates three sub-tasks: 1) Disentangle the dialogs from massive chat logs by using a Feed-Forward neural network; 2) Identify the bug-report dialogs from separated dialogs by leveraging the Graph neural network to learn the contextual information; 3) Synthesize the bug reports by utilizing Transfer Learning techniques to classify the sentences into: observed behaviors (OB), expected behaviors (EB), and steps to reproduce the bug (SR). BugListener is evaluated on six open source projects. The results show that: for bug report identification, BugListener achieves the average F1 of 77.74%, improving the best baseline by 12.96%; and for bug report synthesis task, BugListener could classify the OB, EB, and SR sentences with the F1 of 84.62%, 71.46%, and 73.13%, improving the best baselines by 9.32%, 12.21%, 10.91%, respectively. A human evaluation study also confirms the effectiveness of BugListener in generating relevant and accurate bug reports. These demonstrate the significant potential of applying BugListener in community-based software development, for promoting bug discovery and quality improvement."
Building an Educational Social Media Application for Higher Education,"Weber, F; Dettmer, N; Schurz, K; Thelen, T",10.1007/978-3-031-05064-0_16,2022,"In this paper, we present an overview of an ongoing field study by the Siddata (Studienindividualisierung durch Digitale Datengestuetzte Assistenten [Joint project for Individualization of Studies through Digital, Data-Driven Assistants]) joint research project at the Universities of Bremen, Hannover, and Osnabruck in Northern Germany, with a digital data-driven study assistant (DSA), integrated into the local learning management system (LMS). Some of the included functions, especially those combining data from LMS, OER repositories, and user data with recommendation algorithms, are similar to functions of social media applications. Based on these similarities, we introduce the idea of educational social media applications (edSMA), which implement social media functions for educational purposes. Since 2018, four prototypes (P0, P1, P2, P3) have been developed, deployed, and tested in annual software development cycles. We overview the general user interaction schema, prototype, lifetimes, usage statistics, and features. A remarkable finding is a high demand for digital assistance in the early stages of the student life cycle. For any student differing from the default student, implicitly assumed by education systems, recommender systems can make less frequent educational opportunities accessible and consequently individualize educational pathways and increase equality. We conclude with an outlook on planned developments in the future, such as making the Siddata study assistant available for a broader range of students."
S2LMMD: Cross-Project Software Defect Prediction via Statement Semantic Learning and Maximum Mean Discrepancy,"Liu, WS; Zhu, YT; Chen, X; Gu, Q; Wang, XY; Gu, SK",10.1109/APSEC53868.2021.00044,2021,"Different from within-project software defect prediction (WPDP), cross-project software defect prediction (CPDP) does not require sufficient training data and can help developers in the early stages of software development. Recent studies tried to learn semantic features for CPDP by feeding neural networks with abstract syntax tree (AST) token vectors. However, the ASTs directly parsed from software modules usually have complex structures, which are reflected on more nodes and deeper size, and the transfer learning is not regularly adopted to further reduce the data distribution difference between the source project and the target project. To solve these problems, we aim to joint learn the statement level trees (SLT) and alleviate data distribution difference with maximum mean discrepancy (MMD) to improve defect prediction performance on CPDP. Specifically, we propose a novel cross-project defect prediction method (SLMMD)-L-2 via statement semantic learning and MMD. We first construct the SLT by splitting the original AST on specified node. Then we generate more effective semantic features by learning of sequence embedding with Bi-GRU neural network. Finally, a transfer loss MMD is carried out to keep more common characteristics across different project datasets to further improve CPDP performance. To verify the effectiveness of our proposed method, we conducted experiments on ten widely used open-source projects and evaluated the experimental performance by using AUC measures. Our empirical results show that our proposed method (SLMMD)-L-2 can significantly outperform eight state-of-the-art baselines. In addition, for semantic learning, SLT has a higher influence on CPDP, while MMD is of great significance in transfer learning."
Actionable Analytics: Stop Telling Me What It Is; Please Tell Me What To Do,"Tantithamthavorn, C; Jiarpakdee, J; Grundy, J",10.1109/MS.2021.3072088,2021,"The success of software projects depends on complex decision making (e.g., which tasks should a developer do first, who should perform this task, is the software of high quality, is a software system reliable and resilient enough to deploy, etc.). Bad decisions cost money (and reputation) so we need better tools for making better decisions. This article discusses the why and how of explainable and actionable software analytics. For the task of reducing the risk of software defects, we show initial results from a successful case study that offers more actionable software analytics. Also, we present an interactive tutorial on the subject of Explainable AI tools for SE in our Software Analytics Cookbook (https://xai4se.github.io/book/), and we discuss some open questions that need to be addressed."
A Case Study on Data Science Processes in an Academia-Industry Collaboration,"Silva, SD; de FranÃ§a, BBN",10.1145/3629479.3629514,2023,"Data Science (DS) is emerging in major software development projects and often needs to follow software development practices. Therefore, DS processes will likely continue to attract Software Engineering (SE) practices and vice-versa. This case study aims to map and describe a software development process for Machine Learning(ML)-enabled applications and associated practices used in a real DS project at the Recod.ai laboratory in collaboration with an industrial partner. The focus was to analyze the process and identify the strengths and primary challenges, considering their expertise in robust ML practices and how they can contribute to general software quality. To achieve this, we conducted semi-structured interviews and analyzed them using procedures from the Straussian Grounded Theory. The results showed that the DS development process is iterative, with feedback between activities, which differs from the processes in the literature. Additionally, this process presents a greater involvement of domain experts. Besides, the team prioritizes software quality characteristics (attributes) in these DS projects to ensure some aspects of the final product's quality, i.e., functional correctness and robustness. To achieve those, they use regular accuracy metrics and include explainability and data leakage as quality metrics during training. Finally, the software engineer's role and its responsibilities differ from those of a traditional industry software engineer, as s/he is involved in most of the process steps. These characteristics can contribute to high-quality models achieving the partner needs and, consequently, relevant contributions to the intersection between SE and DS."
Vehicle Software Engineering (VSE): Research and Practice,"Moukahal, LJ; Elsayed, MA; Zulkernine, M",10.1109/JIOT.2020.3001026,2020,"The Internet of Things (IoT) is shaping the future of the automotive industry. Grounded on the advances in everything from sensors, electronic controllers, artificial intelligence, data analytics, to network connectivity, intelligent connected autonomous vehicles (CAVs) have become the essence in IoT applications. The software in CAVs lies at the core of this digital transformation. Faulty software remains the main reason behind the vast number of safety recalls and reputation damage witnessed recently in the automotive industry. The uniqueness of CAVs originates challenges for vehicle software engineering (VSE) that render traditional models and practical solutions for software development ineffective and inapplicable. Despite the raised necessity to adopt a software engineering model that can handle these challenges, there is a lack of studies recognizing the importance of VSE. This article presents an in-depth and comprehensive analysis to perceive the existing software engineering processes detailing their strengths and limitations in the context of CAVs. It also reviews current practical software solutions, including standards, tools, languages, and research efforts to understand the evolution, trends, and current practice in this article area. This article will enable automakers and software providers to better assess and differentiate among the existing software engineering processes and current practical solutions for vehicle software system development. Hence, they would be able to adopt a VSE model and follow best practices that can better meet their challenging needs."
Position Paper: Low-cost Prototyping and Solution Development for Pandemics and Emergencies using Industry 4.0,"Yamanoor, S; Yamanoor, N; Thyagaraja, S",10.5220/0010147001010108,2020,"Pandemics, such as the coronavirus pandemic and other large-scale public emergencies, including floods, volcanic explosions, and earthquakes, require strategic responses for smooth function and restart of industry. Creative, robust, low-cost, scalable solutions must be deployed for underserved and socially disadvantaged communities. This effort requires compressing product and process development from requirements engineering to final testing and deployment, service, and repair, in terms of timeframes, budgets, and related resource constraints. Exceptional circumstances, such as the coronavirus pandemic, add additional pressures such as social-distancing requirements. Several development techniques and tools are available for teams to respond rapidly and effectively to evolving needs in a cost and resource-efficient manner. Industry 4.0 principles can be extended to support frugal development, manufacturing, and operations in diverse communities. Efforts such as the Maker Movement and the availability of licensing techniques for open hardware and software development further add to the abilities of teams to enable virtual collaboration, solution development, customization, and deployment. The paper describes two positions, one that Industry 4.0 can aid in frugal solution development for underserved communities, and two that Industry 4.0 can be implemented frugally to aid production and quality among underserved and vulnerable communities."
DIGITIZATION OF EMBOSSED NUMBERS ON CONTINUOUS STEEL CASTING BILLETS,"David, J; GarzinovÃ¡, R; BarcÃ¡k, T; SlÃ¡cala, J; Shmeleva, N",,2019,"Computer Vision is currently one of the most advanced and fastest growing areas of computing and software development. It can be used to recognize objects from the captured image. It is a visual image and video recognition system coupled with artificial intelligence. Industrial vision using industrial cameras is currently used in many industrial areas. Initial capital investment into the vision system has fast economic return, depending on the cost of the system, the number of human operators replaced, production capacity and other parameters. In the case of a properly designed and configured system that can often fully eliminate the human factor. Typical tasks in machine vision can be recognition and counting of products using industrial cameras, positioning, dimensioning, or optical quality control. The paper will describe the use of machine vision in the metallurgical industry - specifically for the numerical identification of embossed numbers on continuous steel casting billets. The basic requirement of the operation was to create a system for billet identification and archiving of collected data in order to eliminate inaccurate control causing billets to be replaced of each other and fatal manufacturing defects with considerable financial losses. The solution uses a combination of machine vision and neural networks. Combined with automation, advanced data analytics and production management systems, it creates a unique concept of smart metallurgical operation."
"A Fast, Autonomous, Bipedal Walking Behavior over Rapid Regions","Calvert, D; Mishra, B; McCrory, S; Bertrand, S; Griffin, R; Pratt, J",,2022,"In trying to build humanoid robots that perform useful tasks in a world built for humans, we address the problem of autonomous locomotion. Humanoid robot planning and control algorithms for walking over rough terrain are becoming increasingly capable. At the same time, commercially available depth cameras have been getting more accurate and GPU computing has become a primary tool in AI research. In this paper, we present a newly constructed behavior control system for achieving fast, autonomous, bipedal walking, without pauses or deliberation. We achieve this using a recently published rapidly updating planar regions perception algorithm, a height map based body path planner, an A* footstep planner, and a momentum-based walking controller. We put these elements together to form a behavior control system supported by modern software development practices and simulation tools."
CoDesc: A Large Code-Description Parallel Dataset,"Hasan, M; Muttaqueen, T; Al Ishtiaq, A; Mehrab, KS; Haque, MMA; Hasan, T; Ahmad, WU; Iqbal, A; Shahriyar, R",,2021,"Translation between natural language and source code can help software development by enabling developers to comprehend, ideate, search, and write computer programs in natural language. Despite growing interest from the industry and the research community, this task is often difficult due to the lack of large standard datasets suitable for training deep neural models, standard noise removal methods, and evaluation benchmarks. This leaves researchers to collect new small-scale datasets, resulting in inconsistencies across published works. In this study, we present CoDesc - a large parallel dataset composed of 4.2 million Java methods and natural language descriptions. With extensive analysis, we identify and remove prevailing noise patterns from the dataset. We demonstrate the proficiency of CoDesc in two complementary tasks for code-description pairs: code summarization and code search. We show that the dataset helps improve code search by up to 22% and achieves the new state-of-the-art in code summarization. Furthermore, we show CoDesc's effectiveness in pre-training-fine-tuning setup, opening possibilities in building pretrained language models for Java. To facilitate future research, we release the dataset, a data processing tool, and a benchmark at https://github.com/csebuetnlp/CoDesc."
Comparative study of the neural and neuro-fuzzy networks for direct path generation of a new fully spherical parallel manipulator,"Enferadi, J; Safari, H",10.1080/14484846.2018.1495796,2020,"In this paper, the direct kinematics problem of a new 3(RPSP)-S fully spherical parallel manipulator (SPM) is solved using three different models of the artificial intelligence networks, including: a back propagation neural network (BPNN), a radial basis function neural network (RBFNN) and an adaptive neuro-fuzzy inference system (ANFIS). For the proposed networks, we use a training data set which is made from solving the inverse kinematics problem of the robot. After making a database for training the network, different parameters of the networks are changed and finally the best ones for each of BPNN, RBFNN and ANFIS models are derived. Effectiveness of the proposed models is checked by comparing the results of these models with the results of elimination method. As the results show, BPNN has the greatest error and the greatest computational time equal to and 12.4 s, respectively. The next model is RBFNN which has much better precision and less computational time which are equal to and 4.9 s, respectively. Finally, the simulation results by the ANFIS model show that it is the best model for solving the FKP of the 3(RPSP)-S robot. The mean square error (MSE) and computation time of the ANFIS model are and 1.2 s, respectively. These results confirm the reliability of the designed networks."
Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study,"Imai, S",10.1145/3510454.3522684,2022,"This empirical study investigates the effectiveness of pair programming with GitHub Copilot in comparison to human pair-programming. Through an experiment with 21 participants we focus on code productivity and code quality. For experimental design, a participant was given a project to code, under three conditions presented in a randomized order. The conditions are pair-programming with Copilot, human pair-programming as a driver, and as a navigator. The codes generated from the three trials were analyzed to determine how many lines of code on average were added in each condition and how many lines of code on average were removed in the subsequent stage. The former measures the productivity of each condition while the latter measures the quality of the produced code. The results suggest that although Copilot increases productivity as measured by lines of code added, the quality of code produced is inferior by having more lines of code deleted in the subsequent trial."
Data-driven prototyping via natural-language-based GUI retrieval,"Kolthoff, K; Bartelt, C; Ponzetto, SP",10.1007/s10515-023-00377-x,2023,"Rapid GUI prototyping has evolved into a widely applied technique in early stages of software development to facilitate the clarification and refinement of requirements. Especially high-fidelity GUI prototyping has shown to enable productive discussions with customers and mitigate potential misunderstandings, however, the benefits of applying high-fidelity GUI prototypes are accompanied by the disadvantage of being expensive and time-consuming in development and requiring experience to create. In this work, we show RaWi, a data-driven GUI prototyping approach that effectively retrieves GUIs for reuse from a large-scale semi-automatically created GUI repository for mobile apps on the basis of Natural Language (NL) searches to facilitate GUI prototyping and improve its productivity by leveraging the vast GUI prototyping knowledge embodied in the repository. Retrieved GUIs can directly be reused and adapted in the graphical editor of RaWi. Moreover, we present a comprehensive evaluation methodology to enable (i) the systematic evaluation of NL-based GUI ranking methods through a novel high-quality gold standard and conduct an in-depth evaluation of traditional IR and state-of-the-art BERT-based models for GUI ranking, and (ii) the assessment of GUI prototyping productivity accompanied by an extensive user study in a practical GUI prototyping environment."
AnimalAccML: An open-source graphical user interface for automated behavior analytics of individual animals using triaxial accelerometers and machine learning,"Li, GM; Chai, LL",10.1016/j.compag.2023.107835,2023,"Automated collection of accelerometer data and subsequent machine learning modeling are prevalent combined methods for animal behavior recognition. However, there is a lack of customized tools for user-friendly machine learning model development. Meanwhile, existing models in previous research could not be directly used for behavior interpretation. The objective of this study was to design and develop a tool for customized machine learning model development and animal behavior analysis using triaxial accelerometer data. A graphical user interface was programmed with Python and saved in a public repository for open access. The interface mainly consists of pages of 'Manage Project', 'Preprocess Data', 'Develop Models', and 'Analyze Behavior'. An open dataset containing triaxial accelerometer data of six beef cattle was used to test the developed interface. The main results show that users can customize appropriate machine learning models for behavior analytics through several mouse clicks on the interface. A total of 15 models can be selected and trained to determine an optimal one, and model performance can be optimized by adjusting parameters of window size, step size, and training-to -validation ratio. Data imbalance can be solved by merging minority classes into one. The newly developed model has the capacity to analyze overall behavior time budget, statistics (e.g., mean, minimum, maximum, and standard deviation) of each behavior duration, and frequency of behavior sequences. The tool is supportive for automated animal behavior analytics critical to enhancing animal welfare, housing environment, genetics se-lection, and flock management."
How are decisions made in open source software communities? - Uncovering rationale from python email repositories,"Sharma, PN; Savarimuthu, BTR; Stanger, N",10.1002/smr.2526,2024,"Group decision-making (GDM) processes shape the evolution of open source software (OSS) products, thus playing an important role in the governance of open source software communities. While these GDM processes have attracted the attention of researchers, the rationale behind decisions, that is, how decisions are made that enhance the OSS, have not received much attention. This work bridges this gap by extracting these rationales from a large open source repository comprising 1.55 million emails available in Python development archives. This work makes a methodological contribution by presenting a heuristics-based rationale extraction system called Rationale Miner that employs information retrieval, natural language processing, and heuristics-based techniques. Using these techniques, it extracts the rationale behind specific decisions (for example, whether a new module was added based on core developer consensus or a benevolent dictator's pronouncement). This work unearths 11 such rationales behind decisions in the Python community and thus makes a knowledge contribution. It also analyzes the prevalence of these rationales across all PEPs and three sub-types of PEPs: Process, Informational, and Standard Track PEPs. The effectiveness of our contributions has been positively evaluated using quantitative and qualitative approaches (e.g., comparison against baselines for rationale identification showed up to 47% improvement in the most conservative case, and feedback from the Python steering committee showed the accurate identification of rationales respectively). The approach proposed in this work can be used and extended to discover the rationale behind decisions that remain hidden in communication repositories of other OSS projects, which will make the decision-making (DM) process transparent to stakeholders and encourage decision-makers to be more accountable."
Introduction to Artificial Intelligence and Machine Learning for Pathology,"Harrison, JH Jr; Gilbertson, JR; Hanna, MG; Olson, NH; Seheult, JN; Sorace, JM; Stram, MN",10.5858/arpa.2020-0541-CP),2021,"center dot Context.-Recent developments in machine learning have stimulated intense interest in software that may augment or replace human experts. Machine learning may impact pathology practice by offering new capabilities in analysis, interpretation, and outcomes prediction using images and other data. The principles of operation and management of machine learning systems are unfamiliar to pathologists, who anticipate a need for additional education to be effective as expert users and managers of the new tools. Objective.-To provide a background on machine learning for practicing pathologists, including an overview of algorithms, model development, and performance evaluation; to examine the current status of machine learning in pathology and consider possible roles and requirements for pathologists in local deployment and management of machine learning systems; and to highlight existing challenges and gaps in deployment methodology and regulation. Data Sources.-Sources include the biomedical and engineering literature, white papers from professional organizations, government reports, electronic resources, and authors' experience in machine learning. References were chosen when possible for accessibility to practicing pathologists without specialized training in mathematics, statistics, or software development. Conclusions.-Machine learning offers an array of techniques that in recent published results show substantial promise. Data suggest that human experts working with machine learning tools outperform humans or machines separately, but the optimal form for this combination in pathology has not been established. Significant questions related to the generalizability of machine learning systems, local site verification, and performance monitoring remain to be resolved before a consensus on best practices and a regulatory environment can be established. (Arch Pathol Lab Med. 2021;145:1228-1254 ; doi: 10.5858/arpa.2020-0541-CP)"
Automatic resolution of model merging conflicts using quality-based reinforcement learning,"Sharbaf, M; Zamani, B; Sunye, G",10.1016/j.cola.2022.101123,2022,"Modeling is an activity in the software development life cycle in which different experts and stakeholders collaborate as a team. In collaborative modeling, adhering to the optimistic versioning paradigm allows users to apply concurrent changes to the same model. In such a situation, conflicts may arise. To have an integrated yet consistent merged model, conflicts have to be resolved. To this end, automation is currently at its limit or is not supported at all, and user interaction is often required. To alleviate this flaw, there is an opportunity to apply Artificial Intelligence techniques in a collaborative modeling environment to empower the provisioning of automated and intelligent decision-making. In this paper, we propose the use of reinforcement learning algorithms to achieve merging conflict resolution with a high degree of automation. This enables the personalized and quality-based integration of model versions. To evaluate our idea, we demonstrate the resolution of UML class diagram conflicts using a learning process in an illustrative modeling scenario. We also show the applicability of our approach through a proof of concept implementation and assess its accuracy compared to the greedy and search-based algorithms. Moreover, we conducted an experience with five experts to evaluate the satisfaction of actual users with the selection of resolution actions for different conflicts. The result of the assessment validates our proposal with various syntactic and semantic conflicts."
Deep Just-In-Time Defect Localization,"Qiu, FC; Gao, ZP; Xia, X; Lo, DV; Grundy, J; Wang, XY",10.1109/TSE.2021.3135875,2022,"During software development and maintenance, defect localization is an essential part of software quality assurance. Even though different techniques have been proposed for defect localization, i.e., information retrieval (IR)-based techniques and spectrum-based techniques, they can only work after the defect has been exposed, which can be too late and costly to adapt to the newly introduced bugs in the daily development. There are also many JIT defect prediction tools that have been proposed to predict the buggy commit. But these tools do not locate the suspicious buggy positions in the buggy commit. To assist developers to detect bugs in time and avoid introducing them, just-in-time (JIT) bug localization techniques have been proposed, which is targeting to locate suspicious buggy code after a change commit has been submitted. In this paper, we propose a novel JIT defect localization approach, named DeepDL (Deep Learning-based defect localization), to locate defect code lines within a defect introducing change. DeepDL employs a neural language model to capture the semantics of the code lines, in this way, the naturalness of each code line can be learned and converted to a suspiciousness score. The core of our DeepDL is a deep learning-based neural language model. We train the neural language model with previous snapshots (history versions) of a project so that it can calculate the naturalness of a piece of code. In its application, for a given new code change, DeepDL automatically assigns a suspiciousness score to each code line and sorts these code lines in descending order of this score. The code lines at the top of the list are considered as potential defect locations. Our tool can assist developers efficiently check buggy lines at an early stage, which is able to reduce the risk of introducing bugs in time and improve the developers' confidence in the reliability of their software. We conducted an extensive experiment on 14 open source Java projects with a total of 11,615 buggy changes. We evaluate the experimental results considering four evaluation metrics. The experimental results show that our method outperforms the state-of-the-art by a substantial margin."
Bug Triaging Based on Tossing Sequence Modeling,"Xi, SQ; Yao, Y; Xiao, XS; Xu, F; Lv, J",10.1007/s11390-019-1953-5,2019,"Bug triaging, which routes the bug reports to potential fixers, is an integral step in software development and maintenance. To make bug triaging more efficient, many researchers propose to adopt machine learning and information retrieval techniques to identify some suitable fixers for a given bug report. However, none of the existing proposals simultaneously take into account the following three aspects that matter for the efficiency of bug triaging: 1) the textual content in the bug reports, 2) the metadata in the bug reports, and 3) the tossing sequence of the bug reports. To simultaneously make use of the above three aspects, we propose iTriage which first adopts a sequence-to-sequence model to jointly learn the features of textual content and tossing sequence, and then uses a classification model to integrate the features from textual content, metadata, and tossing sequence. Evaluation results on three different open-source projects show that the proposed approach has significantly improved the accuracy of bug triaging compared with the state-of-the-art approaches."
"Arabic calligraphy, typewritten and handwritten using optical character recognition (OCR) system","Al-Barhamtoshy, HM; Jambi, KM; Ahmed, H; Mohamed, S; Abdo, SM; Rashwan, MA",10.21786/bbrc/12.2/11,2019,"This paper describes an Omni OCR system for recognizing typewritten and handwritten Arabic texts documents. The proposed system of the Arabic OCR system can be classified into four main phases. The first phase is the pre-processing phase; it focuses on binarizing, skewing treatment, framing, and noise removing from the prepared documents (dataset). The second phase aims to segment the preprocessed documents into lines and words. Two main tasks are pointed during this phase: language model with the used Arabic dictionary, and the detection of segmented lines and segmented words. The third phase is features extraction phase; it is used to extract features for each segmented line/word according to the used language model. Finally, the classifier or the recognizer will be used to recognize each word/line into a text stream. Therefore, scientific evaluation of the four phases will be applied to measure the accuracy of the Arabic OCR system. The recognition approachis based on Hidden Markov Models (HMM) with the prepared datasets and software development tool are discussed and introduced. State of the art OCR's recognition systems are now capable to perform accuracy of 70% for unconstrained Arabic texts. However, this outline is still far away from what is required in a lot of useful applications. In other words, this paper describes a proposed approach based on language model with ligature and overlap characters for the pro-posed Arabic OCR. Therefore, a posterior word-based approach is used with tri-gram model to recognize the Arabic text. Features are extracted from images of words and generated pattern using the proposed solution. We test our proposed OCR system in different categories of Arabic documents: early printed or typewritten, printed, historical and calligraphy documents. The test bed of our system gives 12.5%-character error rate compared to the best OCR of other systems."
Towards Inclusive Software Engineering Through A/B Testing: A Case-Study at Windows,"Niculescu, I; Hu, HM; Gee, C; Chong, C; Dubey, S; Li, PL",10.1109/ICSE-SEIP52600.2021.00027,2021,"Engineering software to he inclusive of all those that might/could/should use the software is important. flow ex er, today, data used to engineer software can hate inherent biases (e.g. gentler identity) with inclusiveness concerns., While much attention has been given to this topic in the AI/ML. space, in this paper, we examine another data-centric software engineering process, A/B testing. for which we have a dearth of understanding today. Using real-world data from the Windows out of box experience (OOBE) feature, we provide a case-study of how inclusiveness concerns can manifest in A/B testing, practical adjustments to A/B testing towards inclusive software engineering, and insights into ongoing challenges. We discuss implications for research and practice."
The Role of Knowledge Management in IT Projects,"Domagala, P",10.1007/978-3-030-29904-0_4,2019,"Project Management is a field of management dealing with using available knowledge, skills, tools and technology to fill needs and expectations of projects principles. The main goal of this article is to show how knowledge management powered by AI can be used in projects, why it is so important nowadays and how it can impact on the projects effectiveness in a positive way. In the first part of the paper the author focuses on definition and division of knowledge. The second part relates to project management basics. The last one focuses on role of knowledge management in projects."
A machine learning approach to predict DevOps readiness and adaptation in a heterogeneous IT environment,"Sriraman, G; Shriram, R",10.3389/fcomp.2023.1214722,2023,Software and information systems have become a core competency for every business in this connected world. Any enhancement in software delivery and operations will tremendously impact businesses and society. Sustainable software development is one of the key focus areas for software organizations. The application of intelligent automation leveraging artificial intelligence and cloud computing to deliver continuous value from software is in its nascent stage across the industry and is evolving rapidly. The advent of agile methodologies with DevOps has increased software quality and accelerated its delivery. Numerous software organizations have adopted DevOps to develop and operate their software systems and improve efficiency. Software organizations try to implement DevOps activities by taking advantage of various expert services. The adoption of DevOps by software organizations is beset with multiple challenges. These issues can be overcome by understanding and structurally addressing the pain points. This paper presents the preliminary analysis of the interviews with the relevant stakeholders. Ground truths were established and applied to evaluate various machine learning algorithms to compare their accuracy and test our hypothesis. This study aims to help researchers and practitioners understand the adoption of DevOps and the contexts in which the DevOps practices are viable. The experimental results will show that machine learning can predict an organization's readiness to adopt DevOps.
Security testing for web applications: A Systematic Literature Review,"DomÃ­nguez-GarcÃ­a, AD; LimÃ³n, X; OcharÃ¡n-HernÃ¡ndez, JO; PÃ©rez-Arriaga, JC",10.1109/CONISOFT58849.2023.00020,2023,"As the use of the Internet grows, the number and relevance of web applications have also grown, being an integral part of many sectors and businesses. However this growth has adverse effects in the form of increased security threats. Given the large number of current vulnerabilities and the wide variety of testing techniques and tools used to find vulnerabilities, it becomes complex for software developers and application testers to select the proper tools and techniques to test potential threats.This paper aims to collect and classify current security-oriented software testing tools, techniques, and security development models for web systems. According to the STRIDE threat model, our, classification considers software development activities, and associated security threats. To accomplish our goal, we conducted a systematic literature review (SLR), from 2017 to 2022. We identified 18 software testing techniques, 88 tools and four secure development processes, methodologies or models. We found a great variety of tools and techniques, from traditional penetration testing to state-of-the-art Artificial Intelligence supported tools, and we associate different threats found with their respectively testing techniques and STRIDE classification. We believe that our work serves as a foundation for software testers to select proper and modern techniques, tools, and security models, processes or methodologies related to security testing, in accordance with their threat analysis, potentially improving their security testing for web systems."
Identify and Update Test Cases when Production Code Changes: A Transformer-based Approach,"Hu, X; Liu, Z; Xia, X; Liu, ZX; Xu, TT; Yang, XH",10.1109/ASE56229.2023.00165,2023,"Software testing is one of the most essential parts of the software lifecycle and requires a substantial amount of time and effort. During the software evolution, test cases should co-evolve with the production code. However, the co-evolution of test cases often fails due to tight project schedules and other reasons. Obsolete test cases improve the cost of software maintenance and may fail to reveal faults and even lead to future bugs. Therefore, it is essential to detect and update these obsolete test cases in time. In this paper, we propose a novel approach CEPROT (Co-Evolution of Production-Test Code) to identify outdated test cases and update them automatically according to changes in the production code. CEPROT consists of two stages, i.e., obsolete test identification and updating. Specifically, given a production code change and a corresponding test case, CEPROT first identifies whether the test case should be updated. If the test is identified as obsolete, CEPROT will update it to a new version of test case. To evaluate the effectiveness of the two stages, we construct two datasets. Our dataset focuses on method-level production code changes and updates on their obsolete test cases. The experimental results show that CEPROT can effectively identify obsolete test cases with precision and recall of 98.3% and 90.0%, respectively. In addition, test cases generated by CEPROT are identical to the ground truth for 12.3% of samples that are identified as obsolete by CEPROT. We also conduct dynamic evaluation and human evaluation to measure the effectiveness of the updated test cases by CEPROT. 48.0% of updated test cases can be compiled and the average coverage of updated cases is 34.2% which achieves 89% coverage improvement over the obsolete tests. We believe that this study can motivate the co-evolution of production and test code."
AN ON-SITE FECES IMAGE CLASSIFIER SYSTEM FOR CHICKEN HEALTH ASSESSMENT: A PROOF OF CONCEPT,"Li, GM; Gates, RS; Ramirez, BC",10.13031/aea.15607,2023,". Rapid and accurate chicken health assessment can assist producers in making timely decisions, reducing disease transmission, improving animal welfare, and decreasing economic loss. The objective of this research was to develop and evaluate a proof-of-concept mobile application system to assist caretakers in assessing chicken health during their daily flock inspections. A computer server was built to assign users with different usage credentials and receive uploaded fecal images. A dataset containing fecal images from healthy and unhealthy birds (infected with Coccidiosis, Salmonella, and Newcastle disease) was used for classification model development. The modified MobileNetV2 model with additional layers of artificial neural networks was selected after a comparative evaluation of six models. The developed model was embedded into a local server for image classification. An application was developed and deployed, allowing a user with the application on a mobile device to upload a fecal image to a website hosted on the server and receive results processed by the model. Health status is transferred back to the user and can be shared with production managers. The system achieved over 90% accuracy for identifying diseases, and the whole operational procedure took less than one second. This proof-of-concept demonstrates the feasibility of a potential framework for mobile poultry health assessment based on fecal images. However, further development is needed to expand applicability to different production systems through the collection of fecal images from various genetic lines, ages, feed components, housing backgrounds, and flooring types in the poultry industry and improve system performance."
Design and Development of Diabetes Management System Using Machine Learning,"Sowah, RA; Bampoe-Addo, AA; Armoo, SK; Saalia, FK; Gatsi, F; Sarkodie-Mensah, B",10.1155/2020/8870141,2020,"This paper describes the design and implementation of a software system to improve the management of diabetes using a machine learning approach and to demonstrate and evaluate its effectiveness in controlling diabetes. The proposed approach for this management system handles the various factors that affect the health of people with diabetes by combining multiple artificial intelligence algorithms. The proposed framework factors the diabetes management problem into subgoals: building a Tensorflow neural network model for food classification; thus, it allows users to upload an image to determine if a meal is recommended for consumption; implementing K-Nearest Neighbour (KNN) algorithm to recommend meals; using cognitive sciences to build a diabetes question and answer chatbot; tracking user activity, user geolocation, and generating pdfs of logged blood sugar readings. The food recognition model was evaluated with cross-entropy metrics that support validation using Neural networks with a backpropagation algorithm. The model learned features of the images fed from local Ghanaian dishes with specific nutritional value and essence in managing diabetics and provided accurate image classification with given labels and corresponding accuracy. The model achieved specified goals by predicting with high accuracy, labels of new images. The food recognition and classification model achieved over 95% accuracy levels for specific calorie intakes. The performance of the meal recommender model and question and answer chatbot was tested with a designed cross-platform user-friendly interface using Cordova and Ionic Frameworks for software development for both mobile and web applications. The system recommended meals to meet the calorific needs of users successfully using KNN (withk=5) and answered questions asked in a human-like way. The implemented system would solve the problem of managing activity, dieting recommendations, and medication notification of diabetics."
A Simulation-Based Development and Verification Architecture for Micro UAV Teams and Swarms,"AkÃ§akoca, M; Atici, BM; Gever, B; Oguz, S; Demir, M; Saldiran, E; Yuksek, B; Koyuncu, E; Yeniceri, R; Inalhan, G",,2019,"In this work, we present an unmanned aerial vehicle (UAV) simulation-based, hardware and software development and verification architecture structured around the Robot Operating System (ROS). One of the key expectations of such a system is a graceful increase in architectural and computational complexity as the number of vehicles and vehicle complexity increases. In addition, the system is expected to provide the ability to test and verify algorithms both at the software and hardware level before real flight operations. This requirement also couples with the requested flexibility of updating the models and the algorithms based on the results coming from real operations. As such, the designed architecture allows joint simulation and testing at both hardware and software layers for multiple vehicle and swarm operations. Specifically, the architecture consists of distinct and networked layers where hardware elements such as autopilot systems (e.g., Pixhawk, Ardupilot etc.), ground stations and external motion capture/localization systems (e.g., Vicon, Otus Tracker etc.) are integrated around the ROS simulation shell. In addition, the dynamics, sensor models, motion planning and other features can be driven by highly parallel MATLAB/Simulink models. Visualization and visual sensing is obtained through linking of virtual reality with simulation environments such as Gazebo and Airsim. This highly reconfigurable architecture allows research teams to work on multidisciplinary areas such as modeling, control, computer vision, artificial intelligence and machine learning within the same simulation and test environment."
Interrelation of Digitalization and Digital Transformation in a Maritime Company,"Ulfsnes, R; Moe, NB; Hanssen, GK; Buan, TA",10.1007/978-3-031-20706-8_4,2022,"Many organizations must undergo digitalization and digital transformation (DT) simultaneously; in itself, either is daunting. For 15 months, we followed the ongoing digitalization and DT activities at a maritime company with over 3700 employees through a qualitative analysis of 20 interviews, a workshop, and several documents. We see how digitalization and DT are inherently interrelated; DT and digitalization have common enablers through technology such as AI, and common processes in continuous software development. They also share many challenges, including lack of resources and internal resistance against change. Through acquiring data in the digitalization of core services, companies can undergo DT by utilizing data in new and profound ways to build services with new value propositions. In conclusion, digitalization and DT are necessary for incumbent companies and require careful balancing of resources, competence, and technology."
Voice-activated solutions for agile retrospective sessions,"Gaikwad, PK; Jayakumar, CT; Tilve, E; Bohra, N; Yu, WF; Spichkova, M",10.1016/j.procs.2019.09.416,2019,"Retrospective (retro) sessions are an important part of Agile/Scrum process for software development. In theory, conducting retro sessions each sprint should improve team dynamics and productivity. In praxis, retro sessions also have some disadvantages/hurdles that are hard to overcome: they are non-anonymous and time consuming. The goal of our project, conducted in collaboration between Shine Solutions and RMIT University, is to analyse the existing hurdles from industry-prospective and to provide an AI-based solution to overcome them. The two main outcomes of the project are (1) a web-based solution to support retro-sessions, and (2) a qualitative analysis of two speech recognition tools, Google Home and Amazon Alexa, to be connected with the elaborated solution to populate the retro board and help in time boxing a retro by using voice activated commands. (C) 2019 The Authors. Published by Elsevier B.V."
Distribution Awareness for AI System Testing,"Berend, D",10.1109/ICSE-Companion52605.2021.00045,2021,"As Deep Learning (DL) is continuously adopted in many safety critical applications, its quality and reliability start to raise concerns. Similar to the traditional software development process, testing the DL software to uncover its defects at an early stage is an effective way to reduce risks after deployment. Although recent progress has been made in designing novel testing techniques for DL software, the distribution of generated test data is not taken into consideration. It is therefore hard to judge whether the identified errors are indeed meaningful errors to the DL application. Therefore, we propose a new distribution aware testing technique which aims to generate new unseen test cases relevant to the underlying DL system task. Our results show that this technique is able to filter up to 55.44% of error test case on CIFAR-10 and is 10.05% more effective in enhancing robustness."
DizSpec: Digitalization of Requirements Specification Documents to Automate Traceability and Impact Analysis,"Rajbhoj, A; Nistala, P; Kulkarni, V; Soni, S; Pathan, A",10.1109/RE54965.2022.00030,2022,"Requirement engineering in many IT services industries continues to be a document-centric and heavily manual activity, relying on the expertise of business analysts. Requirement specification documents contain details of product features, process flows, activities, rules, parameters, etc. Intricate knowledge of dependencies between these specification elements is necessary for carrying out the effective evolution of the product over time. Today, Business Analysts (BA) are forced to recourse to keyword-based search across multiple requirement specification documents which is a time-, effort- and intellect-intensive endeavor, and vulnerable to the errors of omission and commission. To overcome these lacunae, we propose DizSpec, an automated approach for digitalizing the requirement specification documents into a model form through automatic extraction of specification model elements and the various dependencies between them. The proposed approach creates a digital thread providing machine-processable traceability from product features to its specification elements. It also provides an easy natural language querying mechanism to generate traceability and impact analysis reports of interest. In this paper, we describe the application of this approach to two real-world products thus bringing out its efficacy as well as lessons learned from this transformation journey of the document-centric process to a model-centric and automated process. Though the findings are shared in the specific context of two industry products, we believe, researchers, practitioners, and tool vendors will find the takeaways from this approach and experience applicable in other contexts too."
RETRACTED: Developing algorithmic business resource optimization model for code smells detection: an applied case insight from enterprise level software management system,"Gupta, P; Anand, A; Das, R; Hughes, L; Dwivedi, YK",10.1007/s10479-023-05536-7,2024,"The art of business process optimization and resolution through advanced analytics have gained popularity across all business sectors in recent years. An emerging stream of modern analytics methods have focused on analyzing software development firms and their approach to coding and software development, with a view to optimize key processes through data to enhance organizational value. Despite systematic developments within the field, many high throughput software suffers from incomprehensible bad program structure that results in imbalanced performance throughout its lifecycle. This phenomenon is widely known as Code Smells, i.e., design flaws. A series of AI and ML based algorithms and mathematical tools were developed to tackle the problem in the past. However, at the business and decision-making level there are large uncertainties on how to optimize resource allocation towards each code smell class to maximize benefit of the process. In this paper we propose a novel mathematical business model that will help business and operational managers to optimize budget and resource allocation towards detection of maximum number of smells within a system, with increased output efficiency. Our proposed model benefits from a real life validation of code smell dataset, along with detailed prescription of optimal resource allocation along with reasoning."
An AI Model for Neurodegenerative Diseases,"Mishra, S; Bhargavi, K",10.1109/ICCCI50826.2021.9402493,2021,"Models of neural networks are receiving widespread attention as potential new architectures for computing systems. The models considered here consist of highly interconnected networks of simple computing elements. A computation is performed collectively by the whole network with the activity distributed over all the computing elements. This collective operation results in a high degree of parallel computation and gives the network the potential to solve complex problems quickly. Furthermore, this computational model is meant to be burnt onto a neuromorphic chip and with the use of the surgical procedure, it can then be given to the healthcare department and can thus be used to cure the patients suffering from neurodegenerative diseases. The application software involved maps the patients with the concerned healthcare department for making this model feasible and practical to use."
Cross-Project Issue Classification Based on Ensemble Modeling in a Social Coding World,"Zeng, YR; Yu, Y; Fan, Q; Zhang, XH; Wang, T; Yin, G; Wang, HM",10.1007/978-3-030-04212-7_24,2018,"The simplified and deformalized contribution mechanisms in social coding are attracting more and more contributors involved in the collaborative software development. To reduce the burden on the side of project core team, various kinds of automated and intelligent approaches have been proposed based on machine learning and data mining technologies, which would be restricted by the lack of training data. In this paper, we conduct an extensive empirical study of transferring and aggregating reusable models across projects in the context of issue classification, based on a large-scale dataset including 799 open source projects and more than 795,000 issues. We propose a novel cross-project approach which integrate multiple models learned from various source projects to classify target project. We evaluate our approach through conducting comparative experiments with the within-project classification and a typical cross-project method called Bellwether. The results show that our cross-project approach based on ensemble modeling can obtain great performance, which comparable to the within-project classification and performs better than Bellwether."
Environmental Sustainability of Metaverse: Perspectives from Romanian Developers,"Vladuescu, S; Stanescu, GC",10.3390/su151511704,2023,"The metaverse is currently in the process of development and has applications in various fields, ranging from the gaming industry to art, communication, education, and fashion. Researchers regard the metaverse as the future of the internet, and, in this context, the impact of this new universe's development on the environment needs to be investigated to find viable solutions for its sustainability. We have discussed the economic and social sustainability consequences of the metaverse, and we have largely concentrated on the environmental effects. In conducting this research, we used a quantitative methodology mainly based on a structured questionnaire. We analyzed the impact of the metaverse on the environment from the perspective of professionals in the Romanian software development and programming industries. We selected this sample because the field is newly emerging and because they are experts on how the metaverse functions and evolves. We concluded that IT professionals believe that the new universe could have several negative impacts on the natural environment, such as increased power consumption or increased CO2. emissions and the negative impact can be mitigated by adopting clear regulations and sustainable policies at the international level. This study aims to contribute to the long-term sustainability of the metaverse ecosystem by facilitating a comprehensive understanding of its functioning and evolution, as well as by addressing potential negative impacts on the natural environment. Additionally, the study seeks to make a scholarly contribution to advancing a sustainable metaverse by fostering informed decision-making processes and encouraging responsible practices within the industry. This research might be useful for technology companies, academics, and policymakers."
So who is impacted anyway - a preliminary study of indirect stakeholder identification in practice,"MÃ¼ller, I; Hussain, W; Grundy, J",10.1145/3528579.3529168,2022,"Due to the proliferation of disruptive technologies such as AI into almost every aspect of modern society, software systems increasingly affect the lives of people who do not directly use these systems - with potentially serious and harmful consequences. However, current software development practices do not yet account for this trend sufficiently well and frequently overlook indirect stakeholders. This paper presents the results of a preliminary interview-based study of software professionals aimed at understanding the stateof-practice of indirect stakeholder identification in the software industry. Our initial findings confirm that indirect stakeholders are often overlooked due to customer expectations, project constraints, the prevailing technology-centric software engineering culture and a lack of practical methods and tools. Based on these findings, we outline a roadmap for the investigation of methods and tools for the effective and efficient identification of indirect stakeholders."
An NLP Approach to Estimating Effort in a Work Environment,"Dan, I; Catalin, R; Oliver, O",10.23919/softcom50211.2020.9238219,2020,"Effort estimation in Software development is becoming increasingly hard to do correctly, this is in part due to the growing complexity of software projects, but also due to the higher amount of projects in total. Specialists in multiple fields are required to work together to obtain a realistic estimate, but even then, there is a good amount of risk involved when planning based on the estimate. This leads to cost increases both for the actual estimation process and the losses taken due to wrong estimations. There are already a few project estimation systems out there which take into account presumed system size, development cycles (design, develop, test) or other project related variables. We want to introduce a more granular estimation system, which uses the text descriptions of various tasks but also takes into account available metadata like seniority of the employee which will be working on said tasks, client and project/project type when estimating. The system is built on a deep neural network. The results we are getting are promising so far and we are working on establishing the human baseline accuracy while the tool is available for company employees to use."
Decision-making in sustainable energy transition in Southeastern Europe: probabilistic network-based model,"Hribar, N; Simic, G; Vukadinovic, S; Sprajc, P",10.1186/s13705-021-00315-3,2021,"Background Sustainable energy transition of a country is complex and long-term process, which requires decision-making in all stages and at all levels, including a large number of different factors, with different causality. The main objective of this paper is the development of a probabilistic model for decision-making in sustainable energy transition in developing countries of SE Europe. The model will be developed according to the specificities of the countries for which it is intended-SE Europe. These are countries where energy transition is slower and more difficult due to many factors: high degree of uncertainty, low transparency, corruption, investment problems, insufficiently reliable data, lower level of economic development, high level of corruption and untrained human resources. All these factors are making decision-making more challenging and demanding. Methods Research was done by using content analysis, artificial intelligence methods, software development method and testing. The model was developed by using MSBNx-Microsoft Research's Bayesian Network Authoring and Evaluation Tool. Results Due to the large number of insufficiently clear, but interdependent factors, the model is developed on the principle of probabilistic (Bayesian) networks of factors of interest. The paper presents the first model for supporting decision-making in the field of energy sustainability for the region of Southeastern Europe, which is based on the application of Bayesian Networks. Conclusion Testing of the developed model showed certain characteristics, discussed in paper. The application of developed model will make it possible to predict the short-term and long-term consequences that may occur during energy transition by varying these factors. Recommendations are given for further development of the model, based on Bayesian networks."
"A Comprehensive Investigation of Modern Test Suite Optimization Trends, Tools and Techniques","Kiran, A; Butt, WH; Anwar, MW; Azam, F; Maqbool, B",10.1109/ACCESS.2019.2926384,2019,"Software testing is an important but expensive activity of software development life cycle, as it accounts for more than 52% of entire development cost. Testing requires the execution of all possible test cases in order to find the defects in the software. Therefore, different test suite optimization approaches like the genetic algorithm and the greedy algorithm, etc., are widely used to select the representative test suite without compromising the effectiveness. Test suite optimization is frequently researched to enhance its competences but there is no study published until now that analyzes the latest developments from 2016 to 2019. Hence, in this article, we systematically examine the state-of-the-art optimizations' approaches, tools, and supporting platforms. Principally, we conducted a systematic literature review (SLR) to inspect and examine 58 selected studies that are published during 2016-2019. Subsequently, the selected researches are grouped into five main categories, i.e., greedy algorithm (seven studies), meta-heuristic (28 studies), hybrid (six studies), clustering (five studies), and general (12 studies). Finally, 32 leading tools have been presented, i.e., existing tools (25 tools) and proposed/developed tools (seven tools) along 14 platform supports. Furthermore, it is noted that several approaches aim at solving the single-objective optimization problem. Therefore, researchers should focus on dealing with the multi-objective problem, as multi-objective versions outperform the single-objective ones. Moreover, less attention has been given to clustering-based techniques. Thus, we recommend exploring the machine learning and artificial intelligence-based optimization approaches in the future. A broad exploration of tools and techniques, in this article, will help researchers, practitioners, and developers to opt for adequate techniques, tools, or platforms as per requirements."
Brain metabolic characteristics distinguishing typical and atypical benign epilepsy with centro-temporal spikes,"Li, YT; Feng, JH; Zhang, T; Shi, KX; Ding, Y; Zhang, XH; Jin, CT; Pan, JY; Xue, L; Liao, Y; Wang, XW; Zhuo, C; Zhang, H; Tian, M",10.1007/s00330-021-08051-0,2021,"Objectives Atypical benign epilepsy with centro-temporal spikes (BECTS) have less favorable outcomes than typical BECTS, and thus should be accurately identified for adequate treatment. We aimed to investigate the glucose metabolic differences between typical and atypical BECTS using F-18-fluorodeoxyglucose positron emission tomography ([F-18]FDG PET) imaging, and explore whether these differences can help distinguish. Methods Forty-six patients with typical BECTS, 31 patients with atypical BECTS and 23 controls who underwent [F-18]FDG PET examination were retrospectively involved. Absolute asymmetry index (|AI|) was applied to evaluate the severity of metabolic abnormality. Glucose metabolic differences were investigated among typical BECTS, atypical BECTS, and controls by using statistical parametric mapping (SPM). Logistic regression analyses were performed based on clinical, PET, and hybrid features. Results The |AI| was found significantly higher in atypical BECTS than in typical BECTS (p = 0.040). Atypical BECTS showed more hypo-metabolism regions than typical BECTS, mainly located in the fronto-temporo-parietal cortex. The PET model had significantly higher area under the curve (AUC) than the clinical model (0.91 vs. 0.70, p = 0.006). The hybrid model had the highest sensitivity (0.90), specificity (0.85), and accuracy (0.87) of all three models. Conclusions Atypical BECTS showed more widespread and severe hypo-metabolism than typical BECTS, depending on which the two groups can be well distinguished. The combination of metabolic characteristics and clinical variables has the potential to be used clinically to distinguish between typical and atypical BECTS."
Effective Bug Triage Based on a Hybrid Neural Network,"Wang, HB; Li, Q",10.1109/APSEC53868.2021.00016,2021,"With the increasing scale and complexity of open source software, the quality of software has become a focus to which repairers pay close attention. Due to the inevitable existence of some known or unknown bugs in software,under certain conditions, software bugs may directly cause program running errors, and then produce abnormal running results and wrong program behavior, which will cause huge economic losses. Therefore, software defect repair is an important part of software evolution and quality assurance. Quickly and efficiently assigning defect reports to the right repairer for repair, to ensure efficiency and reduce the cost of open-source software development is an important problem that must be solved in software quality improvement. In this study, we propose a new defect report repair recommendation algorithm, RCNN, which can effectively learn the features of the defect report and recommend the appropriate repairer according to the feature. The proposed algorithm uses a CNN convolution kernel to capture the local information of the text and RNN is used to capture the sequence information of the text. The attention mechanism is introduced to learn the contribution ratio of each part of the text to the overall semantic information of the text. Thus, to a certain extent, it makes up for the defect that RNN cannot effectively learn and monitor remote information. Through experiments on the Eclipse and Mozilla datasets, compared with NB (naive Bayes), SVM (support vector machines), LeeCNN and DBRNNA, the RCNN model can effectively find the appropriate bug repairer among many repairers, and achieve higher classification accuracy."
Model-Driven Engineering in Digital Thread Platforms: A Practical Use Case and Future Challenges,"Chaudhary, HAA; Guevara, I; John, J; Singh, A; Ghosal, A; Pesch, D; Margaria, T",10.1007/978-3-031-19762-8_14,2022,"The increasing complexity delivered by the heterogeneity of the cyber-physical systems is being addressed and decoded by edge technologies, IoT development, robotics, digital twin engineering, and AI. Nevertheless, tackling the orchestration of these complex ecosystems has become a challenging problem. Specially the inherent entanglement of the different emerging technologies makes it hard to maintain and scale such ecosystems. In this context, the usage of model-driven engineering as a more abstract form of glue-code, replacing the boilerplate fashion, has improved the software development lifecycle, democratising the access to and use of the aforementioned technologies. In this paper, we present a practical use case in the context of Smart Manufacturing, where we use several platforms as providers of a high-level abstraction layer, as well as security measures, allowing a more efficient system construction and interoperability."
"Demystifying Practices, Challenges and Expected Features of Using GitHub Copilot","Zhang, BQ; Liang, P; Zhou, XY; Ahmad, A; Waseem, M",10.1142/S0218194023410048,2023,"With the advances in machine learning, there is a growing interest in AI-enabled tools for autocompleting source code. GitHub Copilot, also referred to as the AI Pair Programmer, has been trained on billions of lines of open source GitHub code, and is one of such tools that has been increasingly used since its launch in June 2021. However, little effort has been devoted to understanding the practices, challenges, and expected features of using Copilot in programming for auto-completed source code from the point of view of practitioners. To this end, we conducted an empirical study by collecting and analyzing the data from Stack Overflow (SO) and GitHub Discussions. More specifically, we searched and manually collected 303 SO posts and 927 GitHub discussions related to the usage of Copilot. We identified the programming languages, Integrated Development Environments (IDEs), technologies used with Copilot, functions implemented, benefits, limitations, and challenges when using Copilot. The results show that when practitioners use Copilot: (1) The major programming languages used with Copilot are JavaScript and Python, (2) the main IDE used with Copilot is Visual Studio Code, (3) the most common used technology with Copilot is Node.js, (4) the leading function implemented by Copilot is data processing, (5) the main purpose of users using Copilot is to help generate code, (6) the significant benefit of using Copilot is useful code generation, (7) the main limitation encountered by practitioners when using Copilot is difficulty of integration, and (8) the most common expected feature is that Copilot can be integrated with more IDEs. Our results suggest that using Copilot is like a double-edged sword, which requires developers to carefully consider various aspects when deciding whether or not to use it. Our study provides empirically grounded foundations that could inform software developers and practitioners, as well as provide a basis for future investigations on the role of Copilot as an AI pair programmer in software development."
Recent Advances in Intelligent Source Code Generation: A Survey on Natural Language Based Studies,"Yang, C; Liu, Y; Yin, CQ",10.3390/e23091174,2021,"Source Code Generation (SCG) is a prevalent research field in the automation software engineering sector that maps specific descriptions to various sorts of executable code. Along with the numerous intensive studies, diverse SCG types that integrate different scenarios and contexts continue to emerge. As the ultimate purpose of SCG, Natural Language-based Source Code Generation (NLSCG) is growing into an attractive and challenging field, as the expressibility and extremely high abstraction of the input end. The booming large-scale dataset generated by open-source code repositories and Q&A resources, the innovation of machine learning algorithms, and the development of computing capacity make the NLSCG field promising and give more opportunities to the model implementation and perfection. Besides, we observed an increasing interest stream of NLSCG relevant studies recently, presenting quite various technical schools. However, many studies are bound to specific datasets with customization issues, producing occasional successful solutions with tentative technical methods. There is no systematic study to explore and promote the further development of this field. We carried out a systematic literature survey and tool research to find potential improvement directions. First, we position the role of NLSCG among various SCG genres, and specify the generation context empirically via software development domain knowledge and programming experiences; second, we explore the selected studies collected by a thoughtfully designed snowballing process, clarify the NLSCG field and understand the NLSCG problem, which lays a foundation for our subsequent investigation. Third, we model the research problems from technical focus and adaptive challenges, and elaborate insights gained from the NLSCG research backlog. Finally, we summarize the latest technology landscape over the transformation model and depict the critical tactics used in the essential components and their correlations. This research addresses the challenges of bridging the gap between natural language processing and source code analytics, outlines different dimensions of NLSCG research concerns and technical utilities, and shows a bounded technical context of NLSCG to facilitate more future studies in this promising area."
The Impact of Digital Transformation on Business Models and Strategies of Enterprises: Analysis of Trends and Challenges,"Kravchuk, N; Mykolaichuk, I; Zaika, Y; Melnyk, T; Bocharova, N",,2023,"The extensive advancement of digital technology within the framework of the fourth industrial revolution presents potential consumers and stakeholders with a plethora of tools. However, determining the optimal digital path for transforming business strategies and models becomes challenging. The objective of this study is to pinpoint the most advantageous areas for investing in the digital transformation of business activities. The use of cross-iteration ranking technology allowed us to identify a wide range of relevant independent expert resources (25 organisations), based on their personal ratings, a set of available digital solutions for transforming business models and strategies (36 digital tools) was formed. Among them, the optimal areas for investment financial flows (5 digital technologies) were selected, which have the highest potential for modernising business processes. The obtained potential investment areas are medium and independent. It was found that the key area of investment in the digital transformation of business models and business strategies is automation and cybersecurity. Further research is aimed at expanding the sample of expert organisations (to increase the level of reliability) and modelling the likely development of certain types of business structures affected by digital transformation (to develop tools for preliminary determination of appropriate areas of digital development). Limitations of the study include the methodological framework, as well as the timeframe and scope of the dataset, which may affect the overall context and universality of the results."
TECHNOLOGICAL DEVELOPMENT AND EMPLOYMENT STRUCTURE IN CONTEXT OF ECONOMY DIGITAL TRANSFORMATION,"Akaev, A; Sarygulov, A; Petryakov, A; Podgornaya, V",10.15405/epsbs.2021.04.7,2021,"The article examines the influence of the technological changes on the labor market formation and employment in new forms. It is shown that the employment structure will develop under the increasing influence in market demand for highly qualified workers and wide diffusion of employment alternative forms. Market demand for highly skilled workers in the digital age will grow for several reasons. First, digital technologies can be expected to create a significant number of new jobs in big data analytics, training and management of artificial intelligence, intelligent computing technologies and software development, training, maintenance and management of intelligent robots. Secondly, there is always a new technologies diffusion indirect effect when new jobs are created in related industries. Jobs in these emerging industries will require deep and versatile math, engineering knowledge, job skills that can only be gained through successful graduate and postgraduate studies. Conversely, digital technologies create new opportunities for finding and organizing work for a potential employee, which has already formed a significant market for employment alternative forms. In this regard, the authors aimed to create a mathematical model of jobs technological substitution in the digital economy and to assess the labor force distribution across three forms of employment (full employment, contingent employed and employed on electronic platforms) in economy digital transformation context. For the first time, the authors proposed mathematical models of labor force distribution for three forms of employment, probability distribution curves of labor force technological replacement depending on employment form in context of economy digital transformation. (C) 2021 Published by European Publisher."
An effective approach for plant leaf diseases classification based on a novel DeepPlantNet deep learning model,"Ullah, N; Khan, JA; Almakdi, S; Alshehri, MS; Al Qathrady, M; El-Rashidy, N; El-Sappagh, S; Ali, F",10.3389/fpls.2023.1212747,2023,"IntroductionRecently, plant disease detection and diagnosis procedures have become a primary agricultural concern. Early detection of plant diseases enables farmers to take preventative action, stopping the disease's transmission to other plant sections. Plant diseases are a severe hazard to food safety, but because the essential infrastructure is missing in various places around the globe, quick disease diagnosis is still difficult. The plant may experience a variety of attacks, from minor damage to total devastation, depending on how severe the infections are. Thus, early detection of plant diseases is necessary to optimize output to prevent such destruction. The physical examination of plant diseases produced low accuracy, required a lot of time, and could not accurately anticipate the plant disease. Creating an automated method capable of accurately classifying to deal with these issues is vital.MethodThis research proposes an efficient, novel, and lightweight DeepPlantNet deep learning (DL)-based architecture for predicting and categorizing plant leaf diseases. The proposed DeepPlantNet model comprises 28 learned layers, i.e., 25 convolutional layers (ConV) and three fully connected (FC) layers. The framework employed Leaky RelU (LReLU), batch normalization (BN), fire modules, and a mix of 3x3 and 1x1 filters, making it a novel plant disease classification framework. The Proposed DeepPlantNet model can categorize plant disease images into many classifications.ResultsThe proposed approach categorizes the plant diseases into the following ten groups: Apple_Black_rot (ABR), Cherry_(including_sour)_Powdery_mildew (CPM), Grape_Leaf_blight_(Isariopsis_Leaf_Spot) (GLB), Peach_Bacterial_spot (PBS), Pepper_bell_Bacterial_spot (PBBS), Potato_Early_blight (PEB), Squash_Powdery_mildew (SPM), Strawberry_Leaf_scorch (SLS), bacterial tomato spot (TBS), and maize common rust (MCR). The proposed framework achieved an average accuracy of 98.49 and 99.85in the case of eight-class and three-class classification schemes, respectively.DiscussionThe experimental findings demonstrated the DeepPlantNet model's superiority to the alternatives. The proposed technique can reduce financial and agricultural output losses by quickly and effectively assisting professionals and farmers in identifying plant leaf diseases."
Development of a Word-Level Classification and Vocabulary Learning (WCVL) System,"Baha, K; Shishido, M",10.1109/SCISISIS55246.2022.10001994,2022,"This study aimed to develop a support system to facilitate English vocabulary learning. The Word-level Classification and Vocabulary Learning System (WCVL) was developed utilizing the Waterfall software development methodology, which consists of five stages: requirements analysis, system design, implementation, setup, and evaluation. The NLP-Compromise JavaScript library was used to extract words from an English text using morphological analysis. The extracted vocabulary was also classified by difficulty levels based on the SLV12000 vocabulary database. The system was designed to selectively display words at a higher difficulty level than the student's estimated vocabulary level. Furthermore, it included the Japanese and Thai meanings of words. The WCVL system provided six types of practices for students to learn new words and collected the vocabulary they had acquired in the personal user's database. We evaluated the WCVL system after identifying students' attitudes toward it via an online questionnaire. With the results of this pilot study and the continued use of the WCVL system, it is expected that the WCVL system developed is powerful enough to function as a support system for students in learning and memorizing vocabulary."
SCSS-Net: solar corona structures segmentation by deep learning,"Mackovjak, S; Harman, M; Maslej-KresnÃ¡kovÃ¡, V; Butka, P",10.1093/mnras/stab2536,2021,"Structures in the solar corona are the main drivers of space weather processes that might directly or indirectly affect the Earth. Thanks to the most recent space-based solar observatories, with capabilities to acquire high-resolution images continuously, the structures in the solar corona can be monitored over the years with a time resolution of minutes. For this purpose, we have developed a method for automatic segmentation of solar corona structures observed in the EUV spectrum that is based on a deep-learning approach utilizing convolutional neural networks. The available input data sets have been examined together with our own data set based on the manual annotation of the target structures. Indeed, the input data set is the main limitation of the developed model's performance. Our SCSS-Net model provides results for coronal holes and active regions that could be compared with other generally used methods for automatic segmentation. Even more, it provides a universal procedure to identify structures in the solar corona with the help of the transfer learning technique. The outputs of the model can be then used for further statistical studies of connections between solar activity and the influence of space weather on Earth."
An Unbiased Transformer Source Code Learning with Semantic Vulnerability Graph,"Islam, NT; Parra, GD; Manuel, D; Bou-Harb, E; Najafirad, P",10.1109/EuroSP57164.2023.00018,2023,"Over the years, open-source software systems have become prey to threat actors. Even highly-adopted software has been crippled by unforeseeable attacks, leaving millions of devices exposed. Even as open-source communities act quickly to patch the breach, code vulnerability screening should be an integral part of agile software development from the beginning. Unfortunately, current vulnerability screening techniques are ineffective at identifying novel vulnerabilities or providing developers with code vulnerability and classification. Furthermore, the datasets used for vulnerability learning often exhibit distribution shifts from the real-world testing distribution due to novel attack strategies deployed by adversaries and as a result, the machine learning model's performance may be hindered or biased. To address these issues, we propose a joint interpolated multitasked unbiased vulnerability classifier comprising a transformer RoBERTa and graph convolution neural network (GCN). We present a training process utilizing a semantic vulnerability graph (SVG) representation from source code, created by integrating edges from a sequential flow, control flow, and data flow, as well as a novel flow dubbed Poacher Flow (PF). Poacher flow edges reduce the gap between dynamic and static program analysis and handle complex long-range dependencies. Moreover, our approach reduces biases of classifiers regarding unbalanced datasets by integrating Focal Loss objective function along with SVG. Remarkably, experimental results show that our classifier outperforms state-of-the-art results on vulnerability detection with fewer false negatives and false positives. After testing our model across multiple datasets, it shows an improvement of at least 2.41% and 18.75% in the bestcase scenario. Evaluations using N-day program samples demonstrate that our proposed approach achieves a 93% accuracy and was able to detect 4, zero-day vulnerabilities from popular GitHub repositories. Our code and data are available at https://github.com/pial08/SemVulDet"
"A Layered Model of Comprehension in Collaborative Software Development: Programs, Programming, and Programmers","Rose, CP",10.1109/ICPC52881.2021.00009,2021,"Collaborative software development, whether synchronous or asynchronous, is a creative, integrative process in which something new comes into being through the joint engagement, something new that did not fully exist in the mind of any one person prior to the engagement. One can view this engagement from a macro-level perspective, focusing on large scale development efforts of 100 or more developers, organized into sub-teams, producing collections complex software products like Mozilla. Past work in the area of software engineering has explored the symbiosis between the management structure of a software team and the module structure of the resulting software. In this talk, we focus instead on small scale software teams of between 2 and 5 developers, working on smaller-scale efforts of between one hour and 9 months, through more fine grained analysis of collaborative processes and collaborative products. In this more tightly coupled engagement within small groups, we see again a symbiosis between people, processes, and products. This talk bridges between the field of Computer-Supported Collaborative Learning and the study of software teams in the field of Software Engineering by investigating the inner-workings of small scale collaborative software development. Building on over a decade of AI-enabled collaborative learning experiences in the classroom and online, in this talk we report our work in progress beginning with classroom studies in large online software courses with substantial teamwork components. In our classroom work, we have adapted an industry standard team practice referred to as Mob Programming into a paradigm called Online Mob Programming (OMP) for the purpose of encouraging teams to reflect on concepts and share work in the midst of their project experience. At the core of this work are process mining technologies that enable real time monitoring and just-in-time support for learning during productive work. Recent work on deep-learning approaches to program understanding bridge between investigations of processes and products."
Just-in-Time Defect Prediction for Self-driving Software via a Deep Learning Model,"Choi, J; Kim, T; Ryu, D; Baik, J; Kim, S",10.13052/jwe1540-9589.2225,2023,"computing application. We use pre-trained unified cross-modal pre-training for code representation (UniXCoder) to embed commit messages and code changes. We use bidirectional-LSTM(Bi-LSTM) for context and semantic learning. As a result of the experiment, it was confirmed that the proposed JIT4EA performed better than state-of-the-art methods and could reduce the code inspection effort.Edge computing is applied to various applications and is typically applied to autonomous driving software. As the self-driving system becomes complicated and the proportion of software increases, accidents caused by software defects increase. Just-in-time (JIT) defect prediction is a technique that identifies defects during the software development phase, which helps developers prioritize code inspection. Many researchers have proposed various JIT models, but it is difficult to find a case in which JIT defect prediction was performed on edge computing applications. In particular, due to the characteristic of self-driving software, which is frequently updated, there is a high risk of inducing defects into the update process. In this work, we propose a JIT defect prediction model via deep learning for edge computing applications called JIT4EA. Our research goal is to develop an effective model to predict defects in edge computing applications. To do this, we perform defect prediction on self-driving software, a representative edge"
TOGA: A Neural Method for Test Oracle Generation,"Dinella, E; Ryan, G; Mytkowicz, T; Lahiri, SK",10.1145/3510003.3510141,2022,"Testing is widely recognized as an important stage of the software development lifecycle. Effective software testing can provide benefits such as bug finding, preventing regressions, and documentation. In terms of documentation, unit tests express a unit's intended functionality, as conceived by the developer. A test oracle, typically expressed as an condition, documents the intended behavior of a unit under a given test prefix. Synthesizing a functional test oracle is a challenging problem, as it must capture the intended functionality rather than the implemented functionality. In this paper, we propose TOGA (a neural method for Test Oracle GenerAtion), a unified transformer-based neural approach to infer both exceptional and assertion test oracles based on the context of the focal method. Our approach can handle units with ambiguous or missing documentation, and even units with a missing implementation. We evaluate our approach on both oracle inference accuracy and functional bug-finding. Our technique improves accuracy by 33% over existing oracle inference approaches, achieving 96% overall accuracy on a held out test dataset. Furthermore, we show that when integrated with a automated test generation tool (EvoSuite), our approach finds 57 real world bugs in large-scale Java programs, including 30 bugs that are not found by any other automated testing method in our evaluation."
A Review Of Data-Based Methods For The Development Of An Adaptive Engineering Change System For Automotive Body Shop,"Schuh, G; Bergweiler, G; Fiedler, F; Slawik, V; Ahues, C",10.15488/11295,2021,"The automotive body shop with its high investment costs and resource intensive high automation level is not sufficiently responsive to the demands of agility and adaptivity. The coexistence of combustion engine and e-mobility platforms, but also design modifications on products during production phase, require constant and agile product adaptation in automotive structural components. These changes to the structural car body design have different effects on production planning. However, in some cases they have a considerable influence on the equipment design, e.g., the choice of jig, fixture, and joining technology and the number of robots used. This means that investment decisions for the procurement of new equipment or even entire production systems are also determined. In particular, late introduced new requirements or engineering changes in the product development process (PDP) have a massive impact on the quality of planning and compliance with established premises at the beginning of the PDP. Measures for the holistic consideration of engineering changes and their influences require both a product-side and a production-side review of influences. In this review paper, existing approaches for the product and production-side handling of requirements and engineering changes are analyzed by a systematic literature review. Existing data-based methods are analyzed with regard to their applicability on engineering change management (ECM) in body shop. Based on this, enabling methods are assessed concerning their suitability for the transfer to a holistic, adaptive, and data-based ECM system for the PDP of automotive car bodies."
Detecting coreferent entities in natural language requirements,"Wang, YW; Shi, L; Li, MY; Wang, Q; Yang, Y",10.1007/s00766-022-00374-8,2022,"Requirements are usually written in natural language and evolve continuously during the process of software development, which involves a large number of stakeholders. Stakeholders with diverse backgrounds and skills might refer to the same real-world entity with different linguistic expressions in the natural-language requirements, resulting in requirement inconsistency. We define this phenomenon as Entity Coreference (EC) in the Requirement Engineering (RE) area. It can lead to misconception about technical terminologies, and harm the readability and long-term maintainability of the requirements. In this paper, we propose a DEEP context-wise method for entity COREFerence detection, named DeepCoref. First, we truncate corresponding contexts surrounding entities. Then, we construct a deep context-wise neural network for coreference classification. The network consists of one fine-tuning BERT model for context representation, a Word2Vec-based network for entity representation, and a multi-layer perceptron in the end to fuse and make a trade-off between two representations. Finally, we cluster and normalize coreferent entities. We evaluate our method, respectively, on coreference classification and clustering with 1853 industry data on 21 projects. The former evaluation shows that DeepCoref outperforms three baselines with average precision and recall of 96.10% and 96.06%, respectively. The latter evaluation on six metrics shows that DeepCoref can cluster coreferent entities more accurately. We also conduct ablation experiments with three variants to demonstrate the performance enhancement brought by different components of neural network designed for coreference classification."
FCP2Vec: Deep Learning-Based Approach to Software Change Prediction by Learning Co-Changing Patterns from Changelogs,"Ahmed, HA; Lee, J",10.3390/app13116453,2023,"As software systems evolve, they become more complex and larger, creating challenges in predicting change propagation while maintaining system stability and functionality. Existing studies have explored extracting co-change patterns from changelog data using data-driven methods such as dependency networks; however, these approaches suffer from scalability issues and limited focus on high-level abstraction (package level). This article addresses these research gaps by proposing a file-level change propagation to vector (FCP2Vec) approach. FCP2Vec is a recommendation system designed to aid developers by suggesting files that may undergo change propagation subsequently, based on the file being presently worked on. We carried out a case study utilizing three publicly available datasets: Vuze, Spring Framework, and Elasticsearch. These datasets, which consist of open-source Java-based software development changelogs, were extracted from version control systems. Our technique learns the historical development sequence of transactional software changelog data using a skip-gram method with negative sampling and unsupervised nearest neighbors. We validate our approach by analyzing historical data from the software development changelog for more than ten years. Using multiple metrics, such as the normalized discounted cumulative gain at K (NDCG@K) and the hit ratio at K (HR@K), we achieved an average HR@K of 0.34 at the file level and an average HR@K of 0.49 at the package level across the three datasets. These results confirm the effectiveness of the FCP2Vec method in predicting the next change propagation from historical changelog data, addressing the identified research gap, and show a 21% better accuracy than in the previous study at the package level."
A deep learning framework for 18F-FDG PET imaging diagnosis in pediatric patients with temporal lobe epilepsy,"Zhang, QM; Liao, Y; Wang, XW; Zhang, T; Feng, JH; Deng, JN; Shi, KX; Chen, L; Feng, L; Ma, MD; Xue, L; Hou, HF; Dou, XF; Yu, CC; Ren, L; Ding, Y; Chen, YF; Wu, S; Chen, ZX; Zhang, H; Zhuo, C; Tian, M",10.1007/s00259-020-05108-y,2021,"Purpose Epilepsy is one of the most disabling neurological disorders, which affects all age groups and often results in severe consequences. Since misdiagnoses are common, many pediatric patients fail to receive the correct treatment. Recently, F-18-fluorodeoxyglucose positron emission tomography (F-18-FDG PET) imaging has been used for the evaluation of pediatric epilepsy. However, the epileptic focus is very difficult to be identified by visual assessment since it may present either hypo- or hyper-metabolic abnormality with unclear boundary. This study aimed to develop a novel symmetricity-driven deep learning framework of PET imaging for the identification of epileptic foci in pediatric patients with temporal lobe epilepsy (TLE). Methods We retrospectively included 201 pediatric patients with TLE and 24 age-matched controls who underwent F-18-FDG PET-CT studies. F-18-FDG PET images were quantitatively investigated using 386 symmetricity features, and a pair-of-cube (PoC)-based Siamese convolutional neural network (CNN) was proposed for precise localization of epileptic focus, and then metabolic abnormality level of the predicted focus was calculated automatically by asymmetric index (AI). Performances of the proposed framework were compared with visual assessment, statistical parametric mapping (SPM) software, and Jensen-Shannon divergence-based logistic regression (JS-LR) analysis. Results The proposed deep learning framework could detect the epileptic foci accurately with the dice coefficient of 0.51, which was significantly higher than that of SPM (0.24, P < 0.01) and significantly (or marginally) higher than that of visual assessment (0.31-0.44, P = 0.005-0.27). The area under the curve (AUC) of the PoC classification was higher than that of the JS-LR (0.93 vs. 0.72). The metabolic level detection accuracy of the proposed method was significantly higher than that of visual assessment blinded or unblinded to clinical information (90% vs. 56% or 68%, P < 0.01). Conclusion The proposed deep learning framework for F-18-FDG PET imaging could identify epileptic foci accurately and efficiently, which might be applied as a computer-assisted approach for the future diagnosis of epilepsy patients."
Building Better Digital Twins for Production Systems by Incorporating Environmental Related Functions-Literature Analysis and Determining Alternatives,"Popescu, D; Dragomir, M; Popescu, S; Dragomir, D",10.3390/app12178657,2022,"The digital twin solution is an industry 4.0 specific tool that has grown in the past decade, stemming from the modelling and simulation approaches that existed before, complemented by new sensor capabilities, cloud processing, big data analytics, and implementation mechanisms. As it is being used mostly in the present by manufacturing companies, the primary focus of the solution is to enhance productivity and reduce costs by optimizing processes and enabling real-time problem-solving, sometimes based on decision-making systems and artificial intelligence. However, as companies are being faced with an increasingly steep list of environmental requirements and regulations, ranging from the classical pollution control and waste recycling to full-scale economic models based on circular economy and transformative carbon dioxide elimination programs, the features of the manufacturing digital twins must also evolve to provide an appropriate answer to these challenges. In this paper, the authors propose a framework for building better digital twins for production systems by incorporating environmental-related functions. The demarches start from analysing existing solutions presented in literature from the point of view of environmental suitability, based on the use of the MoSCoW method for differentiating attributes (into Must have, Should have, Could have, Will not have elements) and determining development alternatives based on the employment of Multi-Criteria Decision Analysis (MCDA) for feature selection, and the TRIZ method (Theory of Inventive Problem-Solving) for application guidelines. The MCDA was performed within a focus group of nine production specialists from regionally successful sectors. We arrive at the conclusion that environmental-related functions are poorly implemented in the digital twins of the present (although more so in integrated solutions and custom-built applications) and that the development of the proper tools, databases, and interpretation keys should proceed immediately in the fields of production engineering, industrial ecology, and software development to support them."
Trends in Intelligent and AI-Based Software Engineering Processes: A Deep Learning-Based Software Process Model Recommendation Method,"Alshammari, FH",10.1155/2022/1960684,2022,"In recent years, numerous studies have successfully implemented machine learning strategies in a wide range of application areas. Therefore, several different deep learning models exist, each one tailored to a certain software task. Using deep learning models provides numerous advantages for the software development industry. Testing and maintaining software is a critical concern today. Software engineers have many responsibilities while developing a software system, including coding, testing, and delivering the software to users via the cloud. From this list, it is easy to see that each task calls for extensive organization and preparation, as well as access to a variety of resources. A developer may consult other code repositories, websites with related programming content, and even colleagues for information before attempting to build and test a solution to the problem at hand. In this investigation, we aim to identify the factors that led to developing the recommender. This system analyzes the recommender's performance and provides suggestions for improving the software based on users' opinions."
Identification and Classification of Architecturally Significant Functional Requirements,"Chatterjee, R; Ahmed, A; Anish, PR",10.1109/AIRE51212.2020.00008,2020,"Architecturally Significant Functional Requirements (ASFRs) are those functional requirements that have a significant impact on the architecture of the software system. ASFRs contain comprehensive information to aid architectural decisions; however, their architectural impact is often not explicitly stated in software requirement specification documents. ASFRs are therefore hard to detect, and if missed, can result in expensive refactoring efforts in later stages of software development. Identification and classification of ASFRs using traditional machine learning algorithms have been reported in the past. In this paper, we present our experiments with a deep learning-based model for performing the same task. Our approach is based on a Bidirectional Long Short-Term Memory Network (Bi-LSTM) to capture the context information for each word in the software requirements text, followed by an Attention model to aggregate useful information from these words in order to get the final classification. For ASFR identification, we obtained an f-score of 0.86 and for ASFR classification, we obtained an average f-score of 0.83."
Tool wear segmentation in blanking processes with fully convolutional networks based digital image processing,"Schlegel, C; Molitor, DA; Kubik, C; Martin, DM; Groche, P",10.1016/j.jmatprotec.2023.118270,2024,"The extend of tool wear significantly affects blanking processes and has a decisive impact on product quality and productivity. For this reason, numerous scientists have addressed their research to wear monitoring systems in order to identify or even predict critical wear at an early stage. Existing approaches are mainly based on indirect monitoring using time series, which are used to detect critical wear states via thresholds or machine learning models. Nevertheless, differentiation between types of wear phenomena affecting the tool during blanking as well as quantification of worn surfaces is still limited in practice. While time series data provides partial insights into wear occurrence and evolution, direct monitoring techniques utilizing image data offer a more comprehensive perspective and increased robustness when dealing with varying process parameters. However, acquiring and processing this data in real-time is challenging. In particular, high dynamics combined with increasing strokes rates as well as the high dimensionality of image data have so far prevented the development of direct image-based monitoring systems. For this reason, this paper demonstrates how high -resolution images of tools at 600 spm can be captured and subsequently processed using semantic segmentation deep learning algorithms, more precisely Fully Convolutional Networks (FCN). 125,000 images of the tool are taken from successive strokes. Selected images are labeled pixel by pixel according to their wear condition and used to train a FCN (U-Net). The approach presented offers the possibility of spatially monitoring wear in high-speed blanking operations in real-time."
A Protocol for Preventing Transaction Commitment Without Recipient's Authorization on Blockchain and It's Implementation,"Kamidoi, Y; Yamauchi, R; Wakabayashi, S",10.1109/ACCESS.2021.3056623,2021,"In recent years, blockchain is utilized practically as a distributed secure digital ledger of some sorts of transactions. Blockchain is regarded as one of the most important next generation infrastructure technologies of the financial industry, as well as artificial intelligence and big data. In 2020, cryptocurrencies based on blockchain, such as Bitcoin, Ethereum, or XRP, have a value of more than $450 billion in the market capitalization. Furthermore, on blockchains such as Ethereum, transactions can also represent automatic executions of programs, which are called smart contracts. Thus, many institutes in various categories show their positive attitude toward processing financial transactions or non-financial contracts on blockchain. Although many researchers have studied for various types of issues on blockchain, there always exist security and privacy concerns for blockchain. In this paper, we point out a new concern for abusing the publicity of blockchain and also show the possibility of suspicions aroused by the concern. Then we propose a selective mechanism for self-protecting against the approach from crimes or computer viruses on blockchain, whether the disclosure of user's privacy occurs or not. Next, we also propose a concrete implementation of our proposed selective mechanism with two new address types. We aim to incorporate the mechanism in Bitcoin Core, which is the official Bitcoin client software, and using libbitcoin library functions for Bitcoin software development. We show experimental results to estimate overhead costs for processing our proposed address types toward processing the current standard address type in nodes on the peer-to-peer network."
Artificial Intelligence System for College Students' Physical Fitness and Health Management Based on Physical Measurement Big Data,"Ai, L",10.1155/2021/4727340,2021,"Most of the current health management products are used in medical institutions and generally do not pay enough attention to the student population. Based on this, this paper designs a student-oriented and functional autonomous health management system. This paper proposes a personal health management system based on a multidimensional data model based on the main social characteristics of the population with chronic diseases and the actual needs of personal health management for chronic diseases. The value of various health data for health management is deeply analyzed and mined, and a multidimensional model data warehouse is constructed according to relevant national health data standards to create a standard data platform for intelligent health warning and disease risk assessment. This paper researches and designs a closed-loop personal health management method based on the Plan-Do-Check-Action (PDCA) cycle management model, with detailed functional design in four aspects: health data collection and recording, health assessment, health planning, and tracking and execution. This paper researches health data collection, processing, and storage technologies and adopts HDFS data storage technology, html, css, Java Script, java, and other software development technologies, combined with j Query, UEditor, Date Range Picker, and other plug-ins, as well as SMS email generation interface, wireless Bluetooth transmission interface, etc. This system web and mobile application platforms are designed and developed. Relational database is used as the system database, and a snowflake-type multidimensional data model is designed. Finally, the functions and performance of this system were tested, and the development and trial run of the basic version have been completed."
Android-based Indonesian sign language model using robot hand,"Taryudi, T; Yuliatmojo, P; Paripurna, MA",10.1088/1742-6596/1402/4/044028,2019,"Sign language is one of the most effective ways to communicate with deaf people. However, not everyone is able to use sign language. Therefore, in this study, a sign language display system was created using a hand robot that was moved through an Android-based application on a smartphone to overcome this problem. The design and development of the system begin from the manufacture of hardware consisting of one hand robot with 11 servo motors as the actuators, the Arduino Mega 2560 microcontroller as the main controller and a Bluetooth module for wireless communication between hand robot and smartphone. Then software development is carried out on the hand robot controller and android based application on a smartphone. Furthermore, system testing is carried out by validating the results of the letters and numbers shown by hand robot with gestures in the Indonesian Language Signalling System Dictionary (SIBI). The results show that the hand robot has been able to display letters and numbers according to the instructions given by an Android-based application on a smartphone with an 80% accuracy rate."
CodeGen4Libs: A Two-Stage Approach for Library-Oriented Code Generation,"Liu, MW; Yang, TY; Lou, YL; Du, XY; Wang, Y; Peng, X",10.1109/ASE56229.2023.00159,2023,"Automated code generation has been extensively studied in recent literature. In this work, we first survey 66 participants to motivate a more pragmatic code generation scenario, i.e., library-oriented code generation, where the generated code should implement the functionally of the natural language query with the given library. We then revisit existing learningbased code generation techniques and find they have limited effectiveness in such a library-oriented code generation scenario. To address this limitation, we propose a novel library-oriented code generation technique, CodeGen4Libs, which incorporates two stages: import generation and code generation. The import generation stage generates import statements for the natural language query with the given third-party libraries, while the code generation stage generates concrete code based on the generated imports and the query. To evaluate the effectiveness of our approach, we conduct extensive experiments on a dataset of 403,780 data items. Our results demonstrate that CodeGen4Libs outperforms baseline models in both import generation and code generation stages, achieving improvements of up to 97.4% on EM (Exact Match), 54.5% on BLEU, and 53.5% on Hit@All. Overall, our proposed CodeGen4Libs approach shows promising results in generating high-quality code with specific third-party libraries, which can improve the efficiency and effectiveness of software development."
Simple and cost-effective determination of polychlorinated biphenyls in insulating oils using an ionic liquid-based stationary phase and flow modulated comprehensive two-dimensional gas chromatography with electron capture detection,"Crucello, J; Pierone, DV; Hantao, LW",10.1016/j.chroma.2019.460530,2020,"In this article we describe a method using flow-modulated comprehensive two-dimensional gas chromatography with electron capture detector (FM-GCxGC-ECD) for the determination of polychlorinated biphenyls (PCBs) in complex transformer oils bypassing the need for sample preparation. A two-fold improvement in method development was attained. First, the solvation parameter model (SPM) was used to guide column selection. A highly cohesive ionic liquid-based phase (low l system constant), namely 1,12-di(tripropylphosphonium)dodecane bis(trifluoromethanesulfonyl)imide, was used in the primary stage leading to negligible retention of interfering aliphatic hydrocarbons, which are eluted in the first upper quadrant of the chromatogram. The resulting separation space was used to resolve the critical class of compounds, namely, PCBs and polyaromatic hydrocarbons. Second, a unique combination of column geometries and phase ratios enabled highly efficient reverse fill/flush flow modulation using very low pressure for auxiliary gas flow. The proof of concept method described herein exhibited linearities ranging from 0.990 to 0.994, limits of quantitation (LOQ) from 2.23 and 6.85 mu g mL(-1), precision below 5% relative standard deviation (RSD), and accuracy from 84.2% to 108.9% showcasing the potential of FM-GCxGC for routine analysis. (C) 2019 Elsevier B.V. All rights reserved."
A Deep Introduction to AI Based Software Defect Prediction (SDP) and its Current Challenges,"Pandit, MBR; Varma, N",10.1109/tencon.2019.8929661,2019,"Software systems are driving the quality of human life. Annual investment in software systems development is exceeding 4 trillion USD. Given the increasing complexity of software products and cognitive nature of software development process, the software defects are on the rise resulting in poor quality software that needs about 2.8 trillion USD to fix. To help improve software quality, the field of Software Defect Prediction (SDP) emerged. Its novel attempt to isolate defective software units enables defect removal as well as better utilization of resources in software development and maintenance activities. Having begun with a humble set of statistical models, SDP now employs sophisticated machine learning techniques delivering good results. SDP is now considering use of highly accurate search-based techniques for defects prediction. Also SDP is emerging as a formal methodology having its own process, model and evaluation criteria. Still, the SDP is troubled by a variety of problems such as diversity of datasets, difficulty in selecting ideal set of features to predict software defects, lack of robust methodology to build SDP models and non-availability of feature rich, cross-project industry (commercial) datasets to build ideal SDP models for use across multiple projects etc. There is a plenty of scope to strengthen SDP and also take measures to promote its practical use within the industry by simplifying SDP models and quantifying their outputs and benefits."
An Ontology-Based Approach to Reduce the Negative Impact of Code Smells in Software Development Projects,"Castellano, IL; Aguilar, GFC; Silega, N; Kamal, T; Al-Gaashani, MSAM; Samee, NA; Alabdulhafith, M",10.1109/ACCESS.2023.3300575,2023,"The quality of software systems may be seriously impacted by specific types of source code anomalies. For example, poor programming practices result in Code Smells (CSs), which are a specific type of source code anomalies. They lead to architectural problems that consequently impact some significant software quality attributes, such as maintainability, portability, and reuse. To reduce the risk of introducing CSs and alleviate their consequences, the knowledge and skills of developers and architects is essential. On the other hand, ontologies, which are an artificial intelligence technique, have been used as a solution to deal with different software engineering challenges. Hence, the aim of this paper is to describe an ontological approach to representing and analyzing code smells. Since ontologies are a formal language based on description logics, this approach may contribute to formally analyzing the information about code smells, for example, to detect inconsistencies or infer new knowledge with the support of a reasoner. In addition, this proposal may support the training of software developers by providing the most relevant information on code smells. This ontology can also be a means of representing the knowledge on CSs from different sources (documents in natural language, relational databases, HTML documents, etc.). Therefore, it could be a valuable knowledge base to support the struggle of software developers and architects either to avoid CSs or to detect and remove them. The ontology was developed following a sound methodology. The well-known tool Protege was used to manage the ontology and it was validated by using different techniques. An experiment was conducted to demonstrate the applicability of the ontology and evaluate its impact on speeding up the analysis of CSs."
Ontology-Based Design Pattern Selection,"Naghdipour, A; Hasheminejad, SMH",10.1109/CSICC52343.2021.9420592,2021,"The software design phase is important and challenging due to its high impact on other phases of the software development life cycle. Design patterns are proven solutions based on software developers' experience to solve recurring problems, which used to acquire quality software design. However, selecting an appropriate design pattern is quite difficult. Hence, many studies have been done to automate the design pattern selection process. The existing automated design pattern selection methodologies have certain issues such as the need to have a large sample size, user restrictions on selecting preset concepts, time-consuming, and incomprehensiveness. To address these issues in this paper, a two-phase method for selecting an appropriate design pattern is presented. The proposed method is based on an ontology approach that enables domain knowledge to be modeled in a simple and abstract way and enables queries to be evaluated against a knowledge base. The concepts of ontology are then linked to WordNet. Subsequently, a dataset includes use cases that can be satisfied with GOF design patterns is provided. The set of use cases is then processed in such a way as to make it easy and fast to select the concept-constraint pair to query the ontology. The experimental shows promising and effective results of the proposed method."
Heterogeneous Fault Prediction Using Feature Selection and Supervised Learning Algorithms,"Arora, R; Kaur, A",10.1142/S2196888822500142,2022,"Software Fault Prediction (SFP) is the most persuasive research area of software engineering. Software Fault Prediction which is carried out within the same software project is known as With-In Fault Prediction. However, local data repositories are not enough to build the model of With-in software Fault prediction. The idea of cross-project fault prediction (CPFP) has been suggested in recent years, which aims to construct a prediction model on one project, and use that model to predict the other project. However, CPFP requires that both the training and testing datasets use the same set of metrics. As a consequence, traditional CPFP approaches are challenging to implement through projects with diverse metric sets. The specific case of CPFP is Heterogeneous Fault Prediction (HFP), which allows the program to predict faults among projects with diverse metrics. The proposed framework aims to achieve an HFP model by implementing Feature Selection on both the source and target datasets to build an efficient prediction model using supervised machine learning techniques. Our approach is applied on two open-source projects, Linux and MySQL, and prediction is evaluated based on Area Under Curve (AUC) performance measure. The key results of the proposed approach are as follows: It significantly gives better results of prediction performance for heterogeneous projects as compared with cross projects. Also, it demonstrates that feature selection with feature mapping has a significant effect on HFP models. Non-parametric statistical analyses, such as the Friedman and Nemenyi Post-hoc Tests, are applied, demonstrating that Logistic Regression performed significantly better than other supervised learning algorithms in HFP models."
A comparison between fuzzy AHP and fuzzy TOPSIS methods to software requirements selection,"Nazim, M; Mohammad, CW; Sadiq, M",10.1016/j.aej.2022.04.005,2022,"The fuzzy set theory as one of the key agents of artificial intelligence has been used to deal with vagueness and imprecision during the decision-making process. The software requirements selection is a multicriteria decision making problem which has a key importance for several software development companies. Few methods have been developed to select the software requirements from the list of the elicited requirements using fuzzy analytic hierarchy process (AHP) and fuzzy technique for order of preference by similarity to ideal solution (TOPSIS) methods. Based on our review, we found that little attention is given on the comparison between the fuzzy AHP and fuzzy TOPSIS methods in the context of the software requirements selection problem. To address this issue, this paper presents a comparative analysis of fuzzy AHP and fuzzy TOPSIS methods by considering the small and large set of requirements of an institute examination system based on the following factors: agreement measure, time complexity, rank reversal issue, and number of judgments by decision makers. Based on the comparative study, we found that both fuzzy AHP and fuzzy TOPSIS methods produce the same set of functional requirements based on agreement measure metric in both dataset-1 and dataset-2. Fuzzy AHP requires less time to generate the ranking order in dataset-1; and fuzzy TOPSIS performs better then fuzzy AHP in dataset-2. Fuzzy AHP causes the rank reversal issue; and there is no rank reversal issue in fuzzy TOPSIS and it produces the consistent results. Fuzzy TOPSIS requires less judgment by decision makers then fuzzy AHP. Finally, we discuss the future research directions in the area of SRs selection problem.(c) 2022 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Engineering, Alexandria University This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/ licenses/by-nc-nd/4.0/)."
Supporting Deep Learning-Based Named Entity Recognition Using Cloud Resource Management,"Hartmann, B; Tamla, P; Hemmje, M",10.1007/978-3-031-48057-7_6,2023,"This paper presents a system for managing Cloud Resources such as memory and CPU/GPU that is used to develop, train, and customize Deep Learning-based Named Entity Recognition (NER) models in domains like heath care. The increasing digitization of healthcare services has led to the emergence of electronic health records (EHRs) as a significant component of healthcare data management. NER is a machine learning technique that can be applied to EHRs to extract information such as drug and treatment information, helping to support clinical decision making. The paper is addressing the difficulty domain experts face in using Cloud technologies to perform NER tasks, since they often require technical expertise and technical management overhead. The paper presents a system for the configuration of cloud resources for NER training using the spaCy framework and AWS compute services. The research is structured using Nunamaker's methodology, which provides a structured approach to software development through four phases: observation, theory building, systems development, and experimentation. The paper identifies problem statements and research questions to guide the research and maps them to the objectives of the methodology. The objectives of the methodology include researching the state-of-the-art of NER and cloud technologies, analyzing the architecture of motivating research projects, defining user requirements and the system architecture, and implementing the system. The system is designed using User Centered Systems Design and is based on previously identified user requirements. Two main user groups are considered for the application: NER Experts and Medical Domain Experts. The system is implemented using the Model-View-Controller architecture pattern. It allows for the training of Transformer models, selection of compute resources, and adjusting training configuration and hyperparameters. The system is designed for scalability of compute and storage resources. The paper also discusses the evaluation of the system through experiments and analysis of the results to gain insights. It provides information about the technical implementation and details about the user interface. It is evaluated using cognitive walkthrough and experiments with Transformer-based models."
Seven Principles for Rapid-Response Data Science: Lessons Learned from Covid-19 Forecasting,"Yu, B; Singh, C",10.1214/22-STS855,2022,"In this article, we take a step back to distill seven principles out of our experience in the spring of 2020, when our 12-person rapid-response team used skills of data science and beyond to help distribute 340,000+ units of Covid PPE. This process included tapping into domain knowledge of epidemiology and medical logistics chains, curating a relevant data repository, developing models for short-term county-level death forecasting in the US, and building a website for sharing visualization (an automated AI machine). The principles are described in the context of working with Response4Life, a then-new nonprofit organization, to illustrate their necessity. Many of these principles overlap with those in standard data-science teams, but an emphasis is put on dealing with problems that require rapid response, often resembling agile software development. The technical work from this rapid response project resulted in a paper (Altieri et al. (2021)); see also this interview for more background (Yu and Meng (2021))."
I Don't Understand! : Toward a Model to Evaluate the Role of User Story Quality,"Hallmann, D",10.1007/978-3-030-49392-9_7,2020,"User stories are popular for conveying requirements in agile software projects. Despite existing quality criteria, authors make formal mistakes that result in bad user story quality. If developers have insufficient experience in balancing quality problems, the creation of a shared mental model is impossible, thus increasing the risk of impacts on the project's success. This article provides a work-in-progress research model to set these variables in relation and establish a systematic method to uncover answers regarding their correlation. Details on the effects support research in agile requirements engineering to gain a better understanding of cognitive processes in the comprehension of user stories. In addition, insights can help to develop design recommendations and AI tools to improve user stories. A first evaluation of the model provides promising insights into the behavior and forms a basis for future research."
Enhancements to Neural Language Model for Generating System Configuration Code: A Study with Maven Dependency,"Yang, C; Liu, Y",10.3390/app12168347,2022,"Thanks to the widespread application of software frameworks, OTS components, DSLs, and new-generation software building and construction systems, leveraging system configuration code to assist in development is increasingly common, especially in agile software development. In software system implementation, the system configuration code is used to separate configuration items from the underlying logic, of which Maven dependency code is a typical example. To improve software productivity, developers often reuse existing dependency libraries. However, the large quantity, rapid iteration, and various application scenarios exacerbate the challenge for researchers to reuse the library efficiently and appropriately. Proper reuse of Maven dependencies requires correct importation, which is the research priority of this article; putting it into practical usage at the functional level is the next step. In order to overcome this barrier, researchers have proposed a number of recommendation and intelligent generation models based on deep learning algorithms and code learning strategies. We first introduce an enhancement path for the generation model in order to propose novel models that are more targeted than previous studies. We propose EMGSCC (Enhanced Model for Generating System Configuration Code), which generates accompanying dependency libraries based on the libraries already employed by the current system. EMGSCC uses content-based attention to cope with dependency language features and integrate additional domain information. Finally, we evaluate EMGSCC on the DDDI dataset with extra domain information, and findings show that improvement varies from 1% to 8% on all metrics compared with the baseline. We show empirical evidence of our enhancement path for generating system configuration code based on neural language models, and continuous improvement in this direction would yield promising results."
Lateralization of the crossed cerebellar diaschisis-associated metabolic connectivities in cortico-ponto-cerebellar and cortico-rubral pathways,"Zhu, YK; Ruan, G; Cheng, ZT; Zou, SJ; Zhu, XH",10.1016/j.neuroimage.2022.119487,2022,"This study aimed to explore the glucose metabolic profile of extrapyramidal system in patients with crossed cerebellar diaschisis (CCD). Furthermore, the metabolic connectivities in cortico-ponto-cerebellar and cortico-rubral pathways associated with CCD were also investigated. A total of 130 CCD positive (CCD+) and 424 CCD negative (CCD-) patients with unilateral cerebral hemisphere hypometabolism on F-18-fluorodeoxyglucose positron emission tomography (F-18-FDG PET) were enrolled. Besides, the control group consisted of 56 subjects without any brain structural and metabolic abnormalities. Apart from the autocorrelation, metabolic connectivity pattern of right or left affected cerebellar hemisphere involved unilateral (left or right, respectively) caudate, pallidum, putamen, thalamus and red nucleus, in CCD+ patients with left or right supratentorial lesions, respectively (P-uncorrected < 0.001, cluster size > 200). CCD+ group had significantly lower asymmetry index (AI) in corticoponto-cerebellar pathway (including ipsilateral cerebral white matter, ipsilateral pons, contralateral cerebellum white matter and contralateral cerebellum exterior cortex) and cortico-rubral pathway (including ipsilateral caudate, thalamus proper, pallidum, putamen, ventral diencephalon and red nucleus) than those of both CCD- and control groups (all P < 0.05). AI in contralateral cerebellum exterior cortex was significantly positively correlated with that in ipsilateral caudate, putamen, pallidum, thalamus proper, ventral diencephalon, red nucleus and pons among CCD+ group (all P < 0.01), but only with that in ipsilateral caudate and putamen among CCD- group (both P < 0.001). These results provide additional insight into the involvement of both cortico-ponto-cerebellar and cortico-rubral pathways in the presence of CCD, underlining the need for further investigation about the role of their aberrant metabolic connectivities in the associated symptoms of CCD."
Reimagining Ship Construction through AI KBE Concept: Implementation of Template Oriented Modeling in Detail and Production Ship Design,"Shahzad, T; Wang, P; Hoffmans, J",10.5957/JSPD.05230008,2023,"In shipbuilding, the design and construction processes often involve repetitive tasks and the need for consistent structural elements across various vessels. This research paper presents the implementation of Template Oriented Modeling (TOM), an in-house developed CAD feature that offers an innovative solution to address these challenges. TOM introduces automation and efficiency by streamlining the ship design and construction processes. By utilizing predefined templates and dynamic parameters, TOM significantly reduces the need for manual repetition, resulting in time savings and increased productivity. This paper focuses on the issue of repetitive manual work in shipbuilding and highlights TOM as a transformative approach to overcome this challenge. The motivations, benefits, and innovations associated with TOM are thoroughly examined, emphasizing its potential to revolutionize the shipbuilding industry. We presented a fully functional example demonstrating the effectiveness of TOM in achieving streamlined workflows and improved design consistency."
Computational Thinking Curriculum for Unmanned Aerial Systems,"Zhang, SQ; Stewart, C",,2019,"Unmanned aerial systems (UAS) can explore common, vast and unsafe places at low cost. They could transform multiple sectors from photography to farming to city planning. However, the software underlying UAS is complex and requires multiple distinct programming skills, e.g., AI, machine learning and flight control. Few programmers encompass these skills, hampering software development and dampening the impact of UAS. We contend that early exposure to UAS software could help align workforce skills. However, early exposure requires curriculum that (1) captures the breadth of UAS software, (2) supports multiple levels of depth for diverse programming backgrounds and (3) fits within resource and institutional challenges. We propose a computational thinking framework. In our approach, lessons fit within 20-30 minute instructional blocks, making them usable in short workshop and extended classroom settings. UAV topics and computational thinking depth link lessons. Teachers can trade breadth for in-depth coding and vice versa. In early work, we presented an autonomous UAS to middle school students. Our 1 hour workshop focused on breadth and was received well."
A report on the Workshop Software Engineering for the Uncertain World,"Kulkarni, V",10.1145/3385032.3385053,2020,"Traditionally, software systems have been used to derive mechanical advantage through automation. The underlying assumptions being: objectives for the software system and the environment within which it will operate will remain largely unchanged; and the required information is available fully and with total certainty. Software development is then viewed as a refinement exercise from high-level human-understandable requirements to a deterministic machine-executable implementation. However, for a variety of reasons, these assumptions no longer hold. This calls for a new look at engineering software that's expected to deliver on the stated objectives in an everchanging environment characterized with partial information and inherent uncertainty. The workshop aims to brainstorm this emerging challenge of Software Engineering for the Uncertain World."
CoSS: Leveraging Statement Semantics for Code Summarization,"Shi, CC; Cai, BR; Zhao, Y; Gao, LX; Sood, K; Xiang, Y",10.1109/TSE.2023.3256362,2023,"Automated code summarization tools allow generating descriptions for code snippets in natural language, which benefits software development and maintenance. Recent studies demonstrate that the quality of generated summaries can be improved by using additional code representations beyond token sequences. The majority of contemporary approaches mainly focus on extracting code syntactic and structural information from abstract syntax trees (ASTs). However, from the view of macro-structures, it is challenging to identify and capture semantically meaningful features due to fine-grained syntactic nodes involved in ASTs. To fill this gap, we investigate how to learn more code semantics and control flow features from the perspective of code statements. Accordingly, we propose a novel model entitled CoSS for code summarization. CoSS adopts a Transformer-based encoder and a graph attention network-based encoder to capture token-level and statement-level semantics from code token sequence and control flow graph, respectively. Then, after receiving two-level embeddings from encoders, a joint decoder with a multi-head attention mechanism predicts output sequences verbatim. Performance evaluations on Java, Python, and Solidity datasets validate that CoSS outperforms nine state-of-the-art (SOTA) neural code summarization models in effectiveness and is competitive in execution efficiency. Further, the ablation study reveals the contribution of each model component."
Assessment in Software Development for Competitive Environments: An AI Strategy Development Case Study,"Palomo-Duarte, M; GarcÃ­a-DomÃ­nguez, A; Balderas, A",10.3390/electronics10131566,2021,"Competitions are being widely used to motivate students in diverse learning processes, including those in computer programming. This paper presents a methodology for designing and assessing competitive learning scenarios that allow students to develop three different coding skills: the ability to compete against unknown competitors, the ability to compete against known competitors and the ability to compete against refined versions of known competitors. The proposal is based on peer code review, implemented as an improvement cycle after the dissemination of the code among participants. A case study evaluating the methodology was conducted with two cohorts of students in an undergraduate course. The analysis of the obtained grades suggests that while performance after our assistance was improved, students could still fail or succeed independently of the assistance. Complementary data from student questionnaires and supervisor observations are aligned with this finding. As a conclusion, the evidence supports the validity of the methodology. Additionally, several guidelines based on the experience are provided to transfer the proposal to other environments."
Automated game testing using computer vision methods,"Paduraru, C; Paduraru, M; Stefanescu, A",10.1109/ASEW52652.2021.00024,2021,"Video game development is a growing industry nowadays with high revenues. However, even if there are many resources invested in the software development process, many games still contain bugs or performance issues that affect the user experience. This paper presents ideas on how computer vision methods can be used to automate the process of game testing. The goal is to replace the parts of the testing process that require human users (testers) with machines as much as possible, in order to reduce costs and perform more tests in less time by scaling with hardware resources. The focus is on solving existing real-world problems that have emerged from several discussions with industry partners. We base our methods on previous work in this area using intelligent agents playing video games and deep learning methods that interpret feedback from their actions based on visual output. The paper proposes several methods and a set of open-source tools, independent of the operating system or deployment platform, to evaluate the efficiency of the presented methods."
An Efficient Wikipedia-based Approach for Better Understanding of Natural Language Text Related to User Requirements,"Rodriguez, DV; Carver, DL; Mahmoud, A",,2018,"Requirements engineering is the initial step in the development of software. Eliciting and understanding requirements are crucial in that the quality of the resulting system is highly dependent on a clear understanding of the customers' needs. Customers frequently express requirements in natural language, and software engineers then transform these natural language requirements to a more formalized representation that is useful during the remaining steps of software development. This transformation process has the potential to introduce errors and misunderstandings in the requirements because of ambiguities and incompleteness found in the natural language requirements. A communication gap typically exists between customers and software developers who may not share the same technical expertise. Software developers typically have computer science backgrounds, while the software application areas and thus the expertise of the customers may be from different domains, including such domains as chemistry, aerospace, healthcare, and economics. This communication gap can lead to software failures that can have devastating consequences in many different domains, including the aerospace domain, in terms of time, money and mission-critical efforts. Numerous research efforts are found in the literature that describe how a large textual corpus such as Wikipedia can be useful for helping fill that communication gap between the customer and the developer when natural language requirements are used in the requirements engineering process [1] [2] [3]. Wikipedia is a large online information repository that has an interface that emulates a paper encyclopedia in its coverage of diverse topics. It provides a large number of realworld concepts organized in hierarchical semantic structures. The quantity of the information and the ability to search the information in Wikipedia make it a valuable resource for research related to requirements and natural language processing [4][5] [6]; however, the quantity of information makes searching it time consuming. In this paper, we describe research to determine the most efficient approach to retrieve data from Wikipedia for use in improving the requirements engineering process. We refer to this step of retrieving the information of interest as knowledge acquisition. We present our analysis and compare time efficiency of the use of regular expressions and string search algorithms for decreasing the knowledge acquisition bottleneck caused by the high computational time needed to retrieve these data from Wikipedia."
A Self-Attentional Neural Architecture for Code Completion with Multi-Task Learning,"Liu, F; Li, G; Wei, BL; Xia, X; Fu, ZY; Jin, Z",10.1145/3387904.3389261,2020,"Code completion, one of the most useful features in the Integrated Development Environments (IDEs), can accelerate software development by suggesting the libraries, APIs, and method names in real-time. Recent studies have shown that statistical language models can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from three major drawbacks: a) The hierarchical structural information of the programs is not fully utilized in the program's representation; b) In programs, the semantic relationships can be very long. Existing recurrent neural networks based language models are not sufficient to model the long-term dependency. c) Existing approaches perform a specific task in one model, which leads to the underuse of the information from related tasks. To address these challenges, in this paper, we propose a selfattentional neural architecture for code completion with multi-task learning. To utilize the hierarchical structural information of the programs, we present a novel method that considers the path from the predicting node to the root node. To capture the long-term dependency in the input programs, we adopt a self-attentional architecture based network as the base language model. To enable the knowledge sharing between related tasks, we creatively propose a Multi-Task Learning (MTL) framework to learn two related tasks in code completion jointly. Experiments on three real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods."
Systematic Comprehension for Developer Reply in Mobile System Forum,"Guo, CK; Wang, WJ; Wu, YF; Dong, NP; Ye, QQ; Xu, J; Zhang, S",10.1109/saner.2019.8668016,2019,"Review-based software development has become increasingly prevalent in recent years. Existing efforts aiming at either informative evaluation or sentiment analysis are mainly from the perspective of the reviewers, while neglecting the attitude and behavior of the developers. Such efforts inevitably suffer from recommendation bias in practice, and thus benefit little for the improvement of user reviews. In this paper, we attempt to bridge the gap between user review and developer reply, and conduct a systematic study for review reply in development forums, especially in Chinese mobile system forums. To this end, we concentrate on three research questions: 1) should a targeted review be replied; 2) how long time it should be replied; 3) does traditional review analysis help to pursue a reply for certain review? To answer such questions, given certain review datasets, we perform a systematical study including the following three stages: 1) a binary classification for reply behavior prediction, 2) a regression for prediction of reply time, 3) a systematic factor study for the relationship between traditional review analysis and reply performance. To enhance the accuracy of prediction and analysis, we proposed a CNN-based weak-supervision analysis framework, which exploits manifold techniques from NLP and deep learning. We validate our approach via extensive comparison experiments. The results show that our analysis framework is effective. More importantly, we have uncovered several interesting findings, which provide valuable guidance for further review improvement and recommendation."
Automatic Product Saleability Prediction using Sentiment Analysis on User Reviews,"Kasturia, V; Sharma, S; Sharma, S",10.1109/confluence47617.2020.9058286,2020,"From past few decades information technology industry is on the rise and software development companies thrive to provide the best results for the consumers. Sentiment Analysis is a powerful tool that can help the software industry and company to better evaluate user needs and cater the software in a way to maximise the sales potential. Sentiment Analysis combined with machine learning techniques can help us learn about the industrial trends. Greater than 40 thousand Exabyte (10<^>18) of data is estimated to be a part of the internet out of which 80% is unstructured and can be processed to useful means using NLP techniques. In proposed work sentiment analysis has been applied on user review to predict its saleability or in simpler words: How well a product will sell? Customer feedback was collected from users through a feedback form which required them to express their satisfaction with the product by answering a set of questions which serves as features and input to the machine which evaluates the features such as user interface, Performance, feasibility, cost effectiveness and customer service by extracting the polarity from each. The result shows that sentiment analysis is a viable option to predict the saleability of a product. The empirical results are close to the customer's own expected probability of buying."
A Digital Patient Portal for Patients With Multiple Sclerosis,"Voigt, I; Benedict, M; Susky, M; Scheplitz, T; Frankowitz, S; Kern, R; MÃ¼ller, O; Schlieter, H; Ziemssen, T",10.3389/fneur.2020.00400,2020,"Background: Multiple Sclerosis is a chronic inflammatory disease of the central nervous system that requires a complex, differential, and lifelong treatment strategy, which involves high monitoring efforts and the accumulation of numerous medical data. A fast and broad availability of care, as well as patient-relevant data and a stronger integration of patients and participating care providers into the complex treatment process is desirable. The aim of the ERDF-funded project Integrated Care Portal Multiple Sclerosis (IBMS) was to develop a pathway-based care model and a corresponding patient portal for MS patients and health care professionals (HCPs) as a digital tool to deliver the care model. Methods: The patient portal was created according to a patient-centered design approach which involves both the patients' and the professionals' view. Buurmann's five iterative phases were integrated into a design science research process. A problem analysis focusing on functions and user interfaces was conducted through surveys and workshops with MS patients and HCPs. Based on this, the patient portal was refined and a prototype of the portal was implemented using an agile software development strategy. Results: HCPs and patients already use digital hardware and are open to new technologies. Nevertheless, they desire improved (digital) communication and coordination between care providers. Both groups require a number of functions for the patient portal, which were implemented in the prototype. Usability tests with patients and HCPs are planned to consider whether the portal is deemed as usable, acceptable as well as functional to prepare for any needed ameliorations. Discussion: After testing the patient portal for usability, acceptability, and functionality, it will most likely be a useful and high-quality electronic health (eHealth) tool for patient management from day care to telerehabilitation. It implements clinical pathways in a manner which is comprehensible for patients. Future developments of the patient portal modules could include additional diseases, the integration of quality management and privacy management tools, and the use of artificial intelligence to personalize treatment strategies."
C2AADL_Reverse: A model-driven reverse engineering approach to development and verification of safety-critical software,"Yang, ZB; Qiu, ZK; Zhou, Y; Huang, ZQ; Bodeveix, JP; Filali, M",10.1016/j.sysarc.2021.102202,2021,"The safety-critical system communities have been struggling to manage and maintain their legacy software systems because upgrading such systems has been a complex challenge. To overcome or reduce this problem, reverse engineering has been increasingly used in safety-critical systems. This paper proposes C2AADL_Reverse, a model-driven reverse engineering approach for safety-critical software development and verification. C2AADL_Reverse takes multi-task C source code as input, and generates AADL (Architecture Analysis and Design Language) model of the legacy software systems. Compared with the existing works, this paper considers more reversed construction including AADL component structure, behavior, and multithreaded run-time information. Moreover, two types of activities are proposed to ensure the correctness of C2AADL_Reverse. First, it is necessary to validate the reverse engineering process. Second, the generated AADL models should conform to desired critical properties. We propose the verification of the reverseengineered AADL model by using UPPAAL to establish component-level properties and the Assume Guarantee REasoning Environment (AGREE) to perform compositional verification of the architecture. This combination of verification tools allows us to iteratively explore design and verification of detailed behavioral models, and to scale formal analysis to large models. In addition, the prototype tool and the evaluation of C2AADL_Reverse using a real-world aerospace case study are presented."
Genetic Algorithm-Based Fuzzy Inference System for Describing Execution Tracing Quality,"Galli, T; Chiclana, F; Siewe, F",10.3390/math9212822,2021,"Execution tracing is a tool used in the course of software development and software maintenance to identify the internal routes of execution and state changes while the software operates. Its quality has a high influence on the duration of the analysis required to locate software faults. Nevertheless, execution tracing quality has not been described by a quality model, which is an impediment while measuring software product quality. In addition, such a model needs to consider uncertainty, as the underlying factors involve human analysis and assessment. The goal of this study is to address both issues and to fill the gap by defining a quality model for execution tracing. The data collection was conducted on a defined study population with the inclusion of software professionals to consider their accumulated experiences; moreover, the data were processed by genetic algorithms to identify the linguistic rules of a fuzzy inference system. The linguistic rules constitute a human-interpretable rule set that offers further insights into the problem domain. The study found that the quality properties accuracy, design and implementation have the strongest impact on the quality of execution tracing, while the property legibility is necessary but not completely inevitable. Furthermore, the quality property security shows adverse effects on the quality of execution tracing, but its presence is required to some extent to avoid leaking information and to satisfy legal expectations. The created model is able to describe execution tracing quality appropriately. In future work, the researchers plan to link the constructed quality model to overall software product quality frameworks to consider execution tracing quality with regard to software product quality as a whole. In addition, the simplification of the mathematically complex model is also planned to ensure an easy-to-tailor approach to specific application domains."
Automating the Removal of Obsolete TODO Comments,"Gao, ZP; Xia, X; Lo, D; Grundy, J; Zimmermann, T",10.1145/3468264.3468553,2021,"TODO comments are very widely used by software developers to describe their pending tasks during software development. However, after performing the task developers sometimes neglect or simply forget to remove the TODO comment, resulting in obsolete TODO comments. These obsolete TODO comments can confuse development teams and may cause the introduction of bugs in the future, decreasing the software's quality and maintainability. Manually identifying obsolete TODO comments is time-consuming and expensive. It is thus necessary to detect obsolete TODO comments and remove them automatically before they cause any unwanted side effects. In this work, we propose a novel model, named TDCLEANER (TODO comment Cleaner), to identify obsolete TODO comments in software projects. TDCLEANER can assist developers in just-in-time checking of TODO comments status and avoid leaving obsolete TODO comments. Our approach has two main stages: offline learning and online prediction. During offline learning, we first automatically establish training samples and leverage three neural encoders to capture the semantic features of TODO comment, code change and commit message respectively. TDCLEANER then automatically learns the correlations and interactions between different encoders to estimate the final status of the TODO comment. For online prediction, we check a TODO comment's status by leveraging the offline trained model to judge the TODO comment's likelihood of being obsolete. We built our dataset by collecting TODO comments from the top-10,000 Python and Java Github repositories and evaluated TDCLEANER on them. Extensive experimental results show the promising performance of our model over a set of benchmarks. We also performed an in-the-wild evaluation with real-world software projects, we reported 18 obsolete TODO comments identified by TDCLEANER to Github developers and 9 of them have already been confirmed and removed by the developers, demonstrating the practical usage of our approach."
ZC3: Zero-Shot Cross-Language Code Clone Detection,"Li, J; Tao, CY; Jin, Z; Liu, F; Li, J; Li, G",10.1109/ASE56229.2023.00210,2023,"Developers introduce code clones to improve programming productivity. Many existing studies have achieved impressive performance in monolingual code clone detection. However, during software development, more and more developers write semantically equivalent programs with different languages to support different platforms and help developers translate projects from one language to another. Considering that collecting cross-language parallel data, especially for low-resource languages, is expensive and time-consuming, how designing an effective cross-language model that does not rely on any parallel data is a significant problem. In this paper, we propose a novel method named ZC(3) for Zero-shot Cross-language Code Clone detection. ZC(3) designs the contrastive snippet prediction to form an isomorphic representation space among different programming languages. Based on this, ZC(3) exploits domain-aware learning and cycle consistency learning to further constrain the model to generate representations that are aligned among different languages meanwhile are diacritical for different types of clones. To evaluate our approach, we conduct extensive experiments on four representative cross-language clone detection datasets. Experimental results show that ZC(3) outperforms the state-of-the-art baselines by 67.12%, 51.39%, 14.85%, and 53.01% on the MAP score, respectively. We further investigate the representational distribution of different languages and discuss the effectiveness of our method."
A unified multi-task learning model for AST-level and token-level code completion,"Liu, F; Li, G; Wei, BL; Xia, X; Fu, ZY; Jin, Z",10.1007/s10664-022-10140-7,2022,"Code completion, one of the most useful features in the Integrated Development Environments (IDEs), can accelerate software development by suggesting the next probable tokens based on existing code in real-time. Recent studies have shown that recurrent neural networks based statistical language models can improve the performance of code completion tools through learning from large-scale software repositories. However, most of the existing approaches treat code completion as a single generation task in which the model predicts the value of the tokens or AST nodes based on the contextual source code without considering the syntactic constraints such as the static type information. Besides, the semantic relationships in programs can be very long. Existing recurrent neural networks based language models are not sufficient to model the long-term dependency. In this paper, we tackle the aforementioned limitations by building a unified multi-task learning based code completion model for both AST-level and token-level code completion. To model the relationship and constraints between the type and value of the code elements, we adopt a multi-task learning framework to predict the type and value of the tokens (AST nodes) simultaneously. To capture the long-term dependency in the input programs, we employ a self-attentional architecture based network as the base language model. We apply our approach to both AST-level and token-level code completion. Experimental results demonstrate the effectiveness of our model when compared with state-of-the-art methods."
Surgical Process Modeling for Open Spinal Surgeries,"Carrillo, F; Esfandiari, H; Mueller, S; von Atzigen, M; Massalimova, A; Suter, D; Laux, CJ; Spirig, JM; Farshad, M; Fuernstahl, P",10.3389/fsurg.2021.776945,2022,"Modern operating rooms are becoming increasingly advanced thanks to the emerging medical technologies and cutting-edge surgical techniques. Current surgeries are transitioning into complex processes that involve information and actions from multiple resources. When designing context-aware medical technologies for a given intervention, it is of utmost importance to have a deep understanding of the underlying surgical process. This is essential to develop technologies that can correctly address the clinical needs and can adapt to the existing workflow. Surgical Process Modeling (SPM) is a relatively recent discipline that focuses on achieving a profound understanding of the surgical workflow and providing a model that explains the elements of a given surgery as well as their sequence and hierarchy, both in quantitative and qualitative manner. To date, a significant body of work has been dedicated to the development of comprehensive SPMs for minimally invasive baroscopic and endoscopic surgeries, while such models are missing for open spinal surgeries. In this paper, we provide SPMs common open spinal interventions in orthopedics. Direct video observations of surgeries conducted in our institution were used to derive temporal and transitional information about the surgical activities. This information was later used to develop detailed SPMs that modeled different primary surgical steps and highlighted the frequency of transitions between the surgical activities made within each step. Given the recent emersion of advanced techniques that are tailored to open spinal surgeries (e.g., artificial intelligence methods for intraoperative guidance and navigation), we believe that the SPMs provided in this study can serve as the basis for further advancement of next-generation algorithms dedicated to open spinal interventions that require a profound understanding of the surgical workflow (e.g., automatic surgical activity recognition and surgical skill evaluation). Furthermore, the models provided in this study can potentially benefit the clinical community through standardization of the surgery, which is essential for surgical training."
Enhancing Palliative Care With Mindful Touch: Impact of a Manual and Movement Therapy Training Program in an International Multidisciplinary Integrative Oncology Setting,"Ben-Arye, E; Portalupi, E; Keshet, Y; Bonucci, M; Can, G; Kading, Y; Samuels, N; Livas, M; Gressel, O; Silbermann, M; Breitkreuz, T",10.1016/j.jpainsymman.2020.08.004,2021,"Context. Manual and movement therapies (MMTs) play a central role in the integrative oncology setting, significantly improving patients' quality of life (QOL). Despite research supporting the effectiveness and safety of these modalities, most oncology health care providers (HCPs) lack any MMT training. Objectives. In this study, we examine the impact of an MMT-based integrative oncology training program with the participation of an international and multidisciplinary group of oncology HCPs. The feasibility of implementing these skills in palliative cancer care is examined. Methods. A three-day evidence-based hands-on teaching program was designed to train oncology HCPs working in supportive cancer care MMT modalities from traditional Chinese and anthroposophic medicine. Prequalitative and postqualitative assessments of the trainees' narratives were analyzed using ATLAS.Ti software (Scientific Software Development GmbH, Berlin, Germany) for systematic coding. Results. The training program was attended by 30 participants from Israel (15), Germany (7), Italy (6), Turkey (1), and Cyprus (1). The group included 13 nurses, 10 physicians, 6 complementary/integrative HCPs, and 1 psycho-oncologist. The pretraining expectations that were met at post-training included gaining knowledge and practical QOL-oriented skills, which could be implemented in the palliative and supportive care setting. A significant change in the attitude of trainees to touch therapy was also identified, with respondents seeing MMTs promoting patient-centered palliative care, including nonverbal communication. Conclusion. An MMT training program for oncology HCPs for QOL-related indications is both feasible and likely to be implemented in palliative and supportive cancer care. Nonspecific effects of MMTs were also recognized for their ability to facilitate patient-centered care. (C) 2020 American Academy of Hospice and Palliative Medicine. Published by Elsevier Inc. All rights reserved."
Managing Technical Debt Using Intelligent Techniques-A Systematic Mapping Study,"Albuquerque, D; Guimaraes, E; Tonin, G; RodrÃ­guez, P; Perkusich, M; Almeida, H; Perkusich, A; Chagas, F",10.1109/TSE.2022.3214764,2023,"Technical Debt (TD) is a metaphor reflecting technical compromises that can yield short-term benefits but might hurt the long-term health of a software system. With the increasing amount of data generated when performing software development activities, an emergent research field has gained attention: applying Intelligent Techniques to solve Software Engineering problems. Intelligent Techniques were used to explore data for knowledge discovery, reasoning, learning, planning, perception, or supporting decision-making. Although these techniques can be promising, there is no structured understanding related to their application to support Technical Debt Management (TDM) activities. Within this context, this study aims to investigate to what extent the literature has proposed and evaluated solutions based on Intelligent Techniques to support TDM activities. To this end, we performed a Systematic Mapping Study (SMS) to investigate to what extent the literature has proposed and evaluated solutions based on Intelligent Techniques to support TDM activities. In total, 150 primary studies were identified and analyzed, dated from 2012 to 2021. The results indicated a growing interest in applying Intelligent Techniques to support TDM activities, the most used: Machine Learning and Reasoning under uncertainty. Intelligent Techniques aimed to assist mainly TDM activities related to identification, measurement, and monitoring. Design TD, Code TD, and Architectural TD are the TD types in the spotlight. Most studies were categorized at automation levels 1 and 2, meaning that existing approaches still require substantial human intervention. Symbolists and Analogizers are levels of explanation presented by most Intelligent Techniques, implying that these solutions conclude a general truth after considering a sufficient number of particular cases. Moreover, we also cataloged the empirical research types, contributions, and validation strategies described in primary studies. Based on our findings, we argue that there is still room to improve the use of Intelligent Techniques to support TDM activities. The open issues that emerged from this study can represent future opportunities for practitioners and researchers."
Sentiment analysis tools in software engineering: A systematic mapping study,"Obaidi, M; Nagel, L; Specht, A; KlÃ¼nder, J",10.1016/j.infsof.2022.107018,2022,"Context: Software development is a collaborative task. Previous research has shown social aspects within development teams to be highly relevant for the success of software projects. A team's mood has been proven to be particularly important. It is paramount for project managers to be aware of negative moods within their teams, as such awareness enables them to intervene. Sentiment analysis tools offer a way to determine the mood of a team based on textual communication. Objective: We aim to help developers or stakeholders in their choice of sentiment analysis tools for their specific purpose. Therefore, we conducted a systematic mapping study (SMS). Methods: We present the results of our SMS of sentiment analysis tools developed for or applied in the context of software engineering (SE). Our results summarize insights from 106 papers with respect to (1) the application domain, (2) the purpose, (3) the used data sets, (4) the approaches for developing sentiment analysis tools, (5) the usage of already existing tools, and (6) the difficulties researchers face. We analyzed in more detail which tools and approaches perform how in terms of their performance. Results: According to our results, sentiment analysis is frequently applied to open-source software projects, and most approaches are neural networks or support-vector machines. The best performing approach in our analysis is neural networks and the best tool is BERT. Despite the frequent use of sentiment analysis in SE, there are open issues, e.g. regarding the identification of irony or sarcasm, pointing to future research directions. Conclusion: We conducted an SMS to gain an overview of the current state of sentiment analysis in order to help developers or stakeholders in this matter. Our results include interesting findings e.g. on the used tools and their difficulties. We present several suggestions on how to solve these identified problems."
MUSE: A Multi-Feature Semantic Fusion Method for ROS Node Search Based on Knowledge Graph,"Zhao, YX; Mao, XJ; Zhang, THR; Zhang, Z",10.1109/APSEC60848.2023.00037,2023,"Reusing ROS components, specifically ROS Nodes, is crucial for improving the efficiency and quality of robotic software development. However, developers face challenges in finding the desired ROS Nodes for reuse due to scattered organization of ROS Nodes and the ambiguity in their ROS Node name. To address these challenges, this paper proposes a MUlti-feature SEmantic fusion method (MUSE) that leverages a domain-specific ROS knowledge graph for searching ROS Nodes. Firstly, a large dataset is constructed, comprising code files and textual descriptions related to ROS Nodes obtained from GitHub and ROS Wiki. Secondly, an in-depth analysis of user queries regarding the reuse of ROS Nodes is conducted, leading to the selection of multiple features that provide a comprehensive representation of ROS Node semantics, including Function, Hardware, Input, and Output. Subsequently, a knowledge graph of ROS Nodes is developed based on the dataset, incorporating the selected features. This knowledge graph effectively organizes scattered knowledge and resolves the issue of diverse mentions through entity disambiguation and resolution. To eliminate the semantic gap between the descriptions of features mentioned in user queries and the entities in the knowledge graph, a pre-trained transformer-based model was used to measure the multi-feature semantic similarity between user queries and ROS Nodes knowledge. Finally, we employ a linear regression model to integrate the multi-feature knowledge between user queries and ROS Nodes knowledge. The proposed method has shown a 20% improvement in performance on NDCG@1 compared to other ROS Node search methods. Further evaluations highlight the effectiveness of each feature incorporated in the knowledge graph, as well as the significance of each parameter within the regression model. These findings underscore the robustness of this research in optimizing the reuse of ROS Nodes and facilitating the development of robotics software."
"CzSL: Learning from citizen science, experts, and unlabelled data in astronomical image classification","JimÃ©nez, M; Alfaro, EJ; Torres, MT; Triguero, I",10.1093/mnras/stad2852,2023,"Citizen science is gaining popularity as a valuable tool for labelling large collections of astronomical images by the general public. This is often achieved at the cost of poorer quality classifications made by amateur participants, which are usually verified by employing smaller data sets labelled by professional astronomers. Despite its success, citizen science alone will not be able to handle the classification of current and upcoming surveys. To alleviate this issue, citizen science projects have been coupled with machine learning techniques in pursuit of a more robust automated classification. However, existing approaches have neglected the fact that, apart from the data labelled by amateurs, (limited) expert knowledge of the problem is also available along with vast amounts of unlabelled data that have not yet been exploited within a unified learning framework. This paper presents an innovative learning methodology for citizen science capable of taking advantage of expert- and amateur-labelled data, featuring a transfer of labels between experts and amateurs. The proposed approach first learns from unlabelled data with a convolutional auto-encoder and then exploits amateur and expert labels via the pre-training and fine-tuning of a convolutional neural network, respectively. We focus on the classification of galaxy images from the Galaxy Zoo project, from which we test binary, multiclass, and imbalanced classification scenarios. The results demonstrate that our solution is able to improve classification performance compared to a set of baseline approaches, deploying a promising methodology for learning from different confidence levels in data labelling."
Workflow assessment of an augmented reality application for planning of perforator flaps in plastic reconstructive surgery: Game or game changer?,"Berger, MF; Winter, R; Tuca, AC; Michelitsch, B; Schenkenfelder, B; Hartmann, R; Giretzlehner, M; Reishofer, G; Kamolz, LP; Lumenta, DB",10.1177/20552076231173554,2023,"Objective: In contrast to the rising amount of financial investments for research and development in medical technology worldwide is the lack of usability and clinical readiness of the produced systems. We evaluated an augmented reality (AR) setup under development for preoperative perforator vessel mapping for elective autologous breast reconstruction. Methods: In this grant-supported research pilot, we used magnetic resonance angiography data (MR-A) of the trunk to superimpose the scans on the corresponding patients with hands-free AR goggles to identify regions-of-interest for surgical planning. Perforator location was assessed using MR-A imaging (MR-A projection) and Doppler ultrasound data (3D distance) and confirmed intraoperatively in all cases. We evaluated usability (System Usability Scale, SUS), data transfer load and documented personnel hours for software development, correlation of image data, as well as processing duration to clinical readiness (time from MR-A to AR projections per scan). Results: All perforator locations were confirmed intraoperatively, and we found a strong correlation between MR-A projection and 3D distance measurements (Spearman r = 0.894). The overall usability (SUS) was 67 +/- 10 (=moderate to good). The presented setup for AR projections took 173 min to clinical readiness (=availability on AR device per patient). Conclusion: In this pilot, we calculated development investments based on project-approved grant-funded personnel hours with a moderate to good usability outcome resulting from some limitations: assessment was based on one-time testing with no previous training, a time lag of AR visualizations on the body and difficulties in spatial AR orientation. The use of AR systems can provide new opportunities for future surgical planning, but has more potential for educational (e.g., patient information) or training purposes of medical under- and postgraduates (spatial recognition of imaging data associated with anatomical structures and operative planning). We expect future usability improvements with refined user interfaces, faster AR hardware and artificial intelligence-enhanced visualization techniques."
SiameseQAT: A Semantic Context-Based Duplicate Bug Report Detection Using Replicated Cluster Information,"Rocha, TM; Carvalho, ALD",10.1109/ACCESS.2021.3066283,2021,"In large-scale software development environments, defect reports are maintained through bug tracking systems (BTS) and analyzed by domain experts. Different users may create bug reports in a non-standard manner and may report a particular problem using a particular set of words due to stylistic choices and writing patterns. Therefore, the same defect can be reported with very different descriptions, generating non-trivial duplicates. To avoid redundant work for the development team, an expert needs to look at all new reports while trying to label possible duplicates. However, this approach is neither trivial nor scalable and directly impacts bug fix correction time. Recent efforts to find duplicate bug reports tend to focus on deep neural approaches that consider hybrid representations of bug reports, using both structured and unstructured information. Unfortunately, these approaches ignore that a single bug can have multiple previously identified duplicates and, therefore, multiple textual descriptions, titles, and categorical information. In this work, we propose SiameseQAT, a duplicate bug report detection method that considers information on individual bugs as well as information extracted from bug clusters. The SiameseQAT combines context and semantic learning on structured and unstructured features and corpus topic extraction-based features, with a novel loss function called Quintet Loss, which considers the centroid of duplicate clusters and their contextual information. We validated our approach on the well-known open-source software repositories Eclipse, NetBeans, and Open Office, comprised of more than 500 thousand bug reports. We evaluated both the retrieval and classification of duplicates, reporting a Recall@25 mean of 85% for retrieval and 84% AUROC for classification tasks, results that were significantly superior to previous works."
Estimation of Human Cerebral Atrophy Based on Systemic Metabolic Status Using Machine Learning,"Sakatani, K; Oyama, K; Hu, LZ; Warisawa, S",10.3389/fneur.2022.869915,2022,"BackgroundBased on the assumption that systemic metabolic disorders affect cognitive function, we have developed a deep neural network (DNN) model that can estimate cognitive function based on basic blood test data that do not contain dementia-specific biomarkers. In this study, we used the same DNN model to assess whether basic blood data can be used to estimate cerebral atrophy. MethodsWe used data from 1,310 subjects (58.32 +/- 12.91years old) enrolled in the Brain Doc Bank. The average Mini Mental State Examination score was 28.6 +/- 1.9. The degree of cerebral atrophy was determined using the MRI-based index (GM-BHQ). First, we evaluated the correlations between the subjects' age, blood data, and GM-BHQ. Next, we developed DNN models to assess the GM-BHQ: one used subjects' age and blood data, while the other used only blood data for input items. ResultsThere was a negative correlation between age and GM-BHQ scores (r = -0.71). The subjects' age was positively correlated with blood urea nitrogen (BUN) (r = 0.40), alkaline phosphatase (ALP) (r = 0.22), glucose (GLU) (r = 0.22), and negative correlations with red blood cell counts (RBC) (r = -0.29) and platelet counts (PLT) (r = -0.26). GM-BHQ correlated with BUN (r = -0.30), GLU (r = -0.26), PLT (r = 0.26), and ALP (r = 0.22). The GM-BHQ estimated by the DNN model with subject age exhibited a positive correlation with the ground truth GM-BHQ (r = 0.70). Furthermore, even if the DNN model without subject age was used, the estimated GM-BHQ showed a significant positive correlation with ground truth GM-BHQ (r = 0.58). Age was the most important variable for estimating GM-BHQ. DiscussionAging had the greatest effect on cerebral atrophy. Aging also affects various organs, such as the kidney, and causes changes in systemic metabolic status, which may contribute to cerebral atrophy and cognitive impairment. The DNN model may serve as a new screening test for dementia using basic blood tests for health examinations. Finally, the blood data reflect systemic metabolic disorders in each subject-this method may thus contribute to personalized care."
CORDIC Hardware Acceleration Using DMA-Based ISA Extension,"Manor, E; Ben-David, A; Greenberg, S",10.3390/jlpea12010004,2022,"The use of RISC-based embedded processors aimed at low cost and low power is becoming an increasingly popular ecosystem for both hardware and software development. High-performance yet low-power embedded processors may be attained via the use of hardware acceleration and Instruction Set Architecture (ISA) extension. Recent publications of AI have demonstrated the use of Coordinate Rotation Digital Computer (CORDIC) as a dedicated low-power solution for solving nonlinear equations applied to Neural Networks (NN). This paper proposes ISA extension to support floating-point CORDIC, providing efficient hardware acceleration for mathematical functions. A new DMA-based ISA extension approach integrated with a pipeline CORDIC accelerator is proposed. The CORDIC ISA extension is directly interfaced with a standard processor data path, allowing efficient implementation of new trigonometric ALU-based custom instructions. The proposed DMA-based CORDIC accelerator can also be used to perform repeated array calculations, offering a significant speedup over software implementations. The proposed accelerator is evaluated on Intel Cyclone-IV FPGA as an extension to Nios processor. Experimental results show a significant speedup of over three orders of magnitude compared with software implementation, while applied to trigonometric arrays, and outperforms the existing commercial CORDIC hardware accelerator."
Agency and servitude in platform labour: a feminist analysis of blended cultures,"Komarraju, SA; Arora, P; Raman, U",10.1177/01634437211029890,2022,"Digital labour platforms have become important sites of negotiation between expressions of micro-entrepreneurship, worker freedom and dignity of work. In the Global South, these negotiations are overlaid on an already fraught relationship mediated by the dynamics of caste and culture, to the usual politics of difference. Urban Company (UC), an app-based, on-demand platform in India that connects service providers offering home-based services to potential customers, lists professionalised services that have hitherto been considered part of a 'culture of servitude', performed by historically marginalised groups afforded little dignity of labour. Such platforms offer the possibility of disrupting the entrenched 'master-servant' relationship that exists in many traditional cultures in the Global South by their ostensibly professional approach. While service providers now have the opportunity for self-employment and gain 'respectability' by being associated with the platform, UC claims to have leveraged AI to automate discipline in everything the providers do. Using interviews with UC women service providers involved in beauty work and software development engineers, this paper explores the agency afforded to service partners in both professional and personal spheres. Further, we propose the term blended cultures to think about the ways in which algorithms and human cultures mutually (re)make each other."
DLInfer: Deep Learning with Static Slicing for Python Type Inference,"Yan, YY; Feng, Y; Fan, HC; Xu, BW",10.1109/ICSE48619.2023.00170,2023,"Python programming language has gained enormous popularity in the past decades. While its flexibility significantly improves software development productivity, the dynamic typing feature challenges software maintenance and quality assurance. To facilitate programming and type error checking, the Python programming language has provided a type hint mechanism enabling developers to annotate type information for variables. However, this manual annotation process often requires plenty of resources and may introduce errors. In this paper, we propose a deep learning type inference technique, namely DLInfer, to automatically infer the type information for Python programs. DLInfer collects slice statements for variables through static analysis and then vectorizes them with the Unigram Language Model algorithm. Based on the vectorized slicing features, we designed a bi-directional gated recurrent unit model to learn the type propagation information for inference. To validate the effectiveness of DLInfer, we conduct an extensive empirical study on 700 open-source projects. We evaluate its accuracy in inferring three kinds of fundamental types, including built-in, library, and user-defined types. By training with a large-scale dataset, DLInfer achieves an average of 98.79% Top-1 accuracy for the variables that can get type information through static analysis and manual annotation. Further, DLInfer achieves 83.03% type inference accuracy on average for the variables that can only obtain the type information through dynamic analysis. The results indicate DLInfer is highly effective in inferring types. It is promising to apply it to assist in various software engineering tasks for Python programs."
Virtual Infrastructure Twins: Software Testing Platforms for Computing-Instrument Ecosystems,"Rao, NSV; Al-Najjar, A; Zandi, H; Sankaran, R; Hicks, S; Roccapriori, K; Mukherjee, D",10.1007/978-3-031-23606-8_10,2022,"Science ecosystems are being built by federating computing systems and instruments located at geographically distributed sites over wide-area networks. These computing-instrument ecosystems are expected to support complex workflows that incorporate remote, automated AI-driven science experiments. Their realization, however, requires various designs to be explored and software components to be developed, in order to support the orchestration of distributed computations and experiments. It is often too expensive, infeasible, or disruptive for the entire ecosystem to be available during the typically long software development and testing periods. We propose a Virtual Infrastructure Twin (VIT) of the ecosystem that emulates its network and computing components, and incorporates its instrument software simulators. It provides a software environment nearly identical to the ecosystem to support early development and testing, and design space exploration. We present a brief overview of previous digital infrastructure twins that culminated in the VIT concept, including (i) the virtual science network environment for developing software-defined networking solutions, and (ii) the virtual federated science instrument environment for testing the federation software stack and remote instrument control software. We briefly describe VITs for Nion microscope steering and access to GPU systems."
Technological Transformation of Telco Operators towards Seamless IoT Edge-Cloud Continuum,"Oztoprak, K; Tuncel, YK; Butun, I",10.3390/s23021004,2023,"This article investigates and discusses challenges in the telecommunication field from multiple perspectives, both academic and industry sides are catered for, surveying the main points of technological transformation toward edge-cloud continuum from the view of a telco operator to show the complete picture, including the evolution of cloud-native computing, Software-Defined Networking (SDN), and network automation platforms. The cultural shift in software development and management with DevOps enabled the development of significant technologies in the telecommunication world, including network equipment, application development, and system orchestration. The effect of the aforementioned cultural shift to the application area, especially from the IoT point of view, is investigated. The enormous change in service diversity and delivery capabilities to mass devices are also discussed. During the last two decades, desktop and server virtualization has played an active role in the Information Technology (IT) world. With the use of OpenFlow, SDN, and Network Functions Virtualization (NFV), the network revolution has got underway. The shift from monolithic application development and deployment to micro-services changed the whole picture. On the other hand, the data centers evolved in several generations where the control plane cannot cope with all the networks without an intelligent decision-making process, benefiting from the AI/ML techniques. AI also enables operators to forecast demand more accurately, anticipate network load, and adjust capacity and throughput automatically. Going one step further, zero-touch networking and service management (ZSM) is proposed to get high-level human intents to generate a low-level configuration for network elements with validated results, minimizing the ratio of faults caused by human intervention. Harmonizing all signs of progress in different communication technologies enabled the use of edge computing successfully. Low-powered (from both energy and processing perspectives) IoT networks have disrupted the customer and end-point demands within the sector, as such paved the path towards devising the edge computing concept, which finalized the whole picture of the edge-cloud continuum."
An Effective Approach for Routing the Bug Reports to the Right Fixers,"Xi, SQ; Yao, Y; Xiao, XS; Xu, F; Lu, J",10.1145/3275219.3275228,2018,"Routing the bug reports to potential fixers (i.e., bug triaging), is an integral step in software development and maintenance. However, manually inspecting and assigning bug reports is tedious and time-consuming, especially in those software projects that have a large amount of bug reports and developers. To make bug triaging more efficient, many machine learning and information retrieval based approaches have been proposed to automatically assign bug reports for suitable developers to fix. However, these techniques typically ignore two important facts in bug fixing. First, for some bug reports, the bug reporter himself/herself is one of the developers in the project, and he/she is likely to fix his/her reported bugs in the future. Second, for some bug reports, there may be a tossing sequence which contains several developers from the first potential fixer to the last actual fixer. Such tossing sequences encode valuable information such as the dependency of developers for the bug triaging task. To make use of the above facts, we propose a sequence to sequence model named SeqTriage to automatically route a given bug report to its responsible fixer. Evaluation results on three different open-source projects show that the proposed approach has significantly improved the accuracy of bug triaging compared with the state-of-the-art approaches (20% at best and 5% at least)."
Towards Low-Resource Automatic Program Repair with Meta-Learning and Pretrained Language Models,"Wang, WS; Wang, Y; Hoi, SCH; Joty, S",,2023,"Automatic program repair (APR) has gained increasing attention as an essential technique in software development to reduce manual debugging efforts and boost developers' productivity. Recent advances in deep learning (DL) based models have demonstrated promising results by learning from large-scale bug-fix examples in a data-driven manner. However, in practical scenarios, software bugs have an imbalanced distribution, and the fixing knowledge learned by APR models often only capture the patterns of frequent error types, making it inapplicable to handle the rare error types. To address this limitation, we investigate a novel task of low-resource APR, and propose Meta-APR, a new meta-learning framework integrated with code pretrained language models to generate fixes for low-resource bugs with limited training samples. Our Meta-APR learns better errorspecific knowledge from high-resource bugs through efficient first-order meta-learning optimization, which allows for a faster adaptation to the target low-resource bugs. Besides, while we adopt CodeT5, a pretrained code-aware encoder-decoder Transformer, as the backbone model for Meta-APR, it is a model-agnostic framework that can be integrated with any neural models. Extensive experimental results on three benchmarks in various programming languages verify the superiority of our method over existing DL-based APR approaches."
Patching as Translation: the Data and the Metaphor,"Ding, YRB; Ray, B; Devanbu, P; Hellendoorn, VJ",10.1145/3324884.3416587,2020,"Machine Learning models from other fields, like Computational Linguistics, have been transplanted to Software Engineering tasks, often quite successfully. Yet a transplanted model's initial success at a given task does not necessarily mean it is well-suited for the task. In this work, we examine a common example of this phenomenon: the conceit that software patching is like language translation. We demonstrate empirically that there are subtle, but critical distinctions between sequence-to-sequence models and translation model: while program repair benefits greatly from the former, general modeling architecture, it actually suffers from design decisions built into the latter, both in terms of translation accuracy and diversity. Given these findings, we demonstrate how a more principled approach to model design, based on our empirical findings and general knowledge of software development, can lead to better solutions. Our findings also lend strong support to the recent trend towards synthesizing edits of code conditional on the buggy context, to repair bugs. We implement such models ourselves as proof-of-concept tools and empirically confirm that they behave in a fundamentally different, more effective way than the studied translation-based architectures. Overall, our results demonstrate the merit of studying the intricacies of machine learned models in software engineering: not only can this help elucidate potential issues that may be overshadowed by increases in accuracy; it can also help innovate on these models to raise the state-of-the-art further. We will publicly release our replication data and materials at https://github.com/ARiSE- Lab/Patch-as-translation."
Building an open-source system test generation tool: lessons learned and empirical analyses with EvoMaster,"Arcuri, A; Zhang, M; Belhadi, A; Marculescu, B; Golmohammadi, A; Galeotti, JP; Seran, S",10.1007/s11219-023-09620-w,2023,"Research in software testing often involves the development of software prototypes. Like any piece of software, there are challenges in the development, use and verification of such tools. However, some challenges are rather specific to this problem domain. For example, often these tools are developed by PhD students straight out of bachelor/master degrees, possibly lacking any industrial experience in software development. Prototype tools are used to carry out empirical studies, possibly studying different parameters of novel designed algorithms. Software scaffolding is needed to run large sets of experiments efficiently. Furthermore, when using AI-based techniques like evolutionary algorithms, care needs to be taken to deal with their randomness, which further complicates their verification. The aforementioned represent some of the challenges we have identified for this domain. In this paper, we report on our experience in building the open-source EvoMaster tool, which aims at system-level test case generation for enterprise applications. Many of the challenges we faced would be common to any researcher needing to build software testing tool prototypes. Therefore, one goal is that our shared experience here will boost the research community, by providing concrete solutions to many development challenges in the building of such kind of research prototypes. Ultimately, this will lead to increase the impact of scientific research on industrial practice."
Beyond 160 applications of an expert system: key to a better usability,"TÃ³th-HaÃ¡sz, G; Baracskai, Z",10.1109/coginfocom50765.2020.9237822,2020,"The most influential relevant thinkers have complained of the poverty of Expert Systems (ES) both in the past (Dreyfus and Dreyfus, 1986) and in recently studies as well (Muller and Bostrom, 2016). We developed our own AI-Based Expert System shell for rule-based and case-based reasoning three decades ago and now there are 160 Knowledge Engineering (KE) process behind us with this system. We hope that this experience give us the right to formulate an opinion about that what is the key to a better usability and user experience in understanding of the result of the decision making process. While we do not think that ES is an omnipotent panacea, we also do not think that its applicability is determined only by the shell capabilities. However, one ability is essential; namely, presenting the result as simply as possible in order to that the decision-maker also can understand it. Our finding is that ES shells are only able to be transparent if they are designed by people who have an understanding of the human thinking process instead of a strong math-based software development approach."
Digital Thinking,"Dix, A",10.1145/3489410.3489412,2021,"Digital technology is ubiquitous and has transformed many aspects of domestic and business life. At a personal level there is an 'app for everything', in commerce banks are shifting online and even the heat and oil of the factory floor is being transformed by industry 4.0. In some cases, the changes are incremental, simply making existing process more efficient, or allowing online access to previous face-to-face services. However, there are also more radical changes. Some of these are within the methods of digital production from the perpetual beta of Web 2.0 and A-B testing of user interfaces to agile software development. Other changes are enabled by digital technology, such as more flexible industrial processes due to digital fabrication and applications of AI in medicine. There is a distinctly digital eye that allows us to think differently about the world, for example greater levels of personalisation in consumer products, or more dynamic sensor-rich industrial processes. Sometimes these innovations happen by accident, but we can explicitly adopt this viewpoint to prompt more radical design practice. In this talk I will draw out some of the facets and design heuristics of this new mode of digital thinking."
Service recommendation based on contrastive learning and multi-task learning,"Yu, T; Zhang, LH; Liu, HL; Liu, HB; Wang, JJ",10.1016/j.comcom.2023.11.018,2024,"Service recommendation is an efficient method for service-oriented software that can improve software quality. Applications often require the integration of multiple services to create more powerful and complex functionality while saving software development time. However, the vast number of available candidate Web services can impose a heavy burden on software developers' selection decisions. The existing service recommendation challenges are mainly come from: (1) the development requirements entered by users are too arbitrary (2) the extreme sparsity of invocation records. To address the above challenges, in this paper, we propose a Service Recommendation method based on Contrastive Learning and Multi-task Learning (SRCLML). Specifically, we utilize the Transformer model to extract the development requirements of users, conduct indepth mining of text descriptions, and extract features of applications. Next, the features are fed into the DNN model to predict the probability that the service will be selected. Moreover, we add a tag judgment task to make it capable of multi-task learning, through which, the training signal information implied can be used as an inductive bias to improve service recommendation capabilities. Additionally, we build three subgraphs based on the global graph, conduct in-depth mining of historical invocation records based on contrastive learning and graph neural network to extract features of applications and services and calculate application preferences for each service. Finally, we combined the above two to obtain the final recommendation service list. Extensive experiments on real-world datasets demonstrate that our method, SRCLML, outperforms several state-of-the-art comparison methods in the domain of service recommendation."
Can Identifier Splitting Improve Open-Vocabulary Language Model of Code,"Shi, JK; Yang, Z; He, JD; Xu, BW; Lo, D",10.1109/SANER53432.2022.00130,2022,"Statistical language models on source code have successfully assisted software engineering tasks. However, developers can create or pick arbitrary identifiers when writing source code. Freely chosen identifiers lead to the notorious out-of-vocabulary (OOV) problem that negatively affects model performance. Recently, Karampatsis et al. showed that using the Byte Pair Encoding (BPE) algorithm to address the OOV problem can improve the language models' predictive performance on source code. However, a drawback of BPE is that it cannot split the identifiers in a way that preserves the meaningful semantics. Prior researchers also show that splitting compound identifiers into sub-words that reflect the semantics can benefit software development tools. These two facts motivate us to explore whether identifier splitting techniques can be utilized to augment the BPE algorithm and boost the performance of open-vocabulary language models considered in Karampatsis et al.'s work. This paper proposes to split identifiers in both constructing vocabulary and processing model inputs procedures, thus exploiting three different settings of applying identifier splitting to language models for the code completion task. We contrast models' performance under these settings and find that simply inserting identifier splitting into the pipeline hurts the model performance, while a hybrid strategy combining identifier splitting and the BPE algorithm can outperform the original open-vocabulary models on predicting identifiers by 3.68% of recall and 6.32% of Mean Reciprocal Rank. The results also show that the hybrid strategy can improve the entropy of language models by 2.02%."
DCServCG: A data-centric service code generation using deep learning,"Alizadehsani, Z; Ghaemi, H; Shahraki, A; Gonzalez-Briones, A; Corchado, JM",10.1016/j.engappai.2023.106304,2023,"Modern software development paradigms, including Service-Oriented Architecture (SOA), tend to make use of available services e.g., web service Application Programming Interfaces (APIs) to generate new software. Thus, for the further advancement of SOA, the development of accurate automatic tasks, such as service discovery and composition, is necessary. Most of these automated tasks rely heavily on web service metadata annotation. The lack of machine-readable documentation and structured metadata reduces the accuracy and volume of automatic data annotation, negatively affecting the performance of automated SOA tasks. This study aims to propose automatic code completion for improving web service-based systems by identifying and capturing service usage collected from public repositories that share Open Source Software (OSS). To this end, a Data-Centric Service Code Generation (DCServCG) model is proposed to improve old-fashioned, general-purpose code generators that neglect essential service-based code characteristics e.g., sequence overlap and bias issues. DCServCG takes advantage of the data-centric concept, i.e., conditional text generation, to overcome the mentioned issues. We have evaluated the approach from the point of view of language modeling metrics. The obtained results indicate that the usage of the data-centric approach reduces perplexity by 1.125. Moreover, the DCServCG model uses de-noising and conditional text generation, which is trained on the transformer by distilling the knowledge, DistilGPT2 (82M parameters) trained faster and its perplexity is 0.363 lower than ServCG (124M parameters) without de-noising and conditional text generation, which lower perplexity value indicates better model generalization performance."
VITAMIN-V: Virtual Environment and Tool-boxing for Trustworthy Development of RISC-V based Cloud Services,"Canal, R; Chenet, C; Arelakis, A; Arnau, JM; Berral, JL; Call, A; Di Carlo, S; Costa, JJ; Gizopoulos, D; Karakostas, V; Lubrano, F; Nikas, K; Nikolakopoulos, Y; Otero, B; Papadimitriou, G; Papaefstathiou, I; Pnevmatikatos, D; Raho, D; Rigo, A; RodrÃ­guez, E; Savino, A; Scionti, A; Tampouratzis, N; Torregrosa, A",10.1109/DSD60849.2023.00050,2023,"VITAMIN-V is a 2023-2025 Horizon Europe project that aims to develop a complete RISC-V open-source software stack for cloud services with comparable performance to the cloud-dominant x86 counterpart and a powerful virtual execution environment for software development, validation, verification, and testing that considers the relevant RISC-V ISA extensions for cloud deployment. VITAMIN-V will specifically support the RISC-V extensions for virtualization, cryptography, and vectorization in three virtual environments: QEMU, gem5, and cloud FPGA prototype platforms. The project will focus on European Processor Initiative (EPI) based RISC-V designs and accelerators. VITAMIN-V will also support the ISA extensions by adding the compiler and toolchain support. Furthermore, it will develop novel software validation, verification, and testing approaches to ensure software trustworthiness. To enable the execution of complete cloud stacks, VITAMIN-V will port all necessary machine-dependent modules in relevant open-source cloud software distributions, focusing on three cloud setups. Finally, VITAMIN-V will demonstrate and benchmark these three cloud setups using relevant AI, big-data, and serverless applications. VITAMIN-V aims to match the software performance of its x86 equivalent while contributing to RISC-V open-source virtual environments, software validation, and cloud software suites."
Supervisory Control and Data Acquisition System for Machines Used for Thermal Processing of Materials,"Patino, D; Preciado, WT; Castrillon, AMS; Castrillon, SAS",,2022,"A supervisory control and data acquisition (SCADA) system has been developed for three machines used for the thermal processing of materials: a hot wire cutter, an induction heater and a welding test stand. The cutter uses a transformer with adjustable voltage between 20 V and 32 V, and current of 8 A, measuring the temperature of the wire with thermal expansion. The heater uses a 24 V, 15 A sources, and a type K thermocouple embedded in the sample in order to measure temperature. In welding, a temperature control system was implemented for the sample using type K thermocouple and a cooling fan using a 12 V and 20 A sources. The SCADA system consists of a PLC and a PC with a graphical interface which serves to select the process to be worked on as it displays the thermal history of the monitored object. The supervisory system uses a PC with a 32-bit Windows 7 operating system and an OPC software package running on the academic LabVIEW platform. It was designed to use a single human-machine interface for different thermal processes. This paper describes the important components of the system, including its architecture, software development and performance testing."
Accurate Library Recommendation Using Combining Collaborative Filtering and Topic Model for Mobile Development,"Zhao, XQ; Li, SP; Yu, H; Wang, Y; Qiu, WW",10.1587/transinf.2018EDP7227,2019,"Background: The applying of third-party libraries is an integral part of many applications. But the libraries choosing is time-onsuming even for experienced developers. The automated recommendation system for libraries recommendation is widely researched to help developers to choose libraries. Aim: from software engineering aspect, our research aims to give developers a reliable recommended list of third-arty libraries at the early phase of software development lifecycle to help them build their development environment faster; and from technical aspect, our research aims to build a generalizable recommendation system framework which combines collaborative filtering and topic modeling techniques, in order to improve the performance of libraries recommendation significantly. Our works on this research: 1) we design a hybrid methodology to combine collaborative filtering and LDA text mining technology; 2) we build a recommendation system framework successfully based on the above hybrid methodology; 3) we make a well-designed experiment to validate the methodology and framework which use the data of 1,013 mobile application projects; 4) we do the evaluation for the result of the experiment. Conclusions: 1) hybrid methodology with collaborative filtering and LDA can improve the performance of libraries recommendation significantly; 2) based on the hybrid methodology, the framework works very well on the libraries recommendation for helping developers' libraries choosing. Further research is necessary to improve the performance of the libraries recommendation including: 1) use more accurate NLP technologies improve the correlation analysis; 2) try other similarity calculation methodology for collaborative filtering to rise the accuracy; 3) on this research, we just bring the time-series approach to the framework and make an experiment as comparative trial, the result shows that the performance improves continuously, so in further research we plan to use time-series data-mining as the basic methodology to update the framework."
De-Identification of Facial Features in Magnetic Resonance Images: Software Development Using Deep Learning Technology,"Jeong, YU; Yoo, S; Kim, YH; Shim, WH",10.2196/22739,2020,"Background: High-resolution medical images that include facial regions can be used to recognize the subject's face when reconstructing 3-dimensional (3D)-rendered images from 2-dimensional (2D) sequential images, which might constitute a risk of infringement of personal information when sharing data. According to the Health Insurance Portability and Accountability Act (HIPAA) privacy rules, full-face photographic images and any comparable image are direct identifiers and considered as protected health information. Moreover, the General Data Protection Regulation (GDPR) categorizes facial images as biometric data and stipulates that special restrictions should be placed on the processing of biometric data. Objective: This study aimed to develop software that can remove the header information from Digital Imaging and Communications in Medicine (DICOM) format files and facial features (eyes, nose, and ears) at the 2D sliced-image level to anonymize personal information in medical images. Methods: A total of 240 cranial magnetic resonance (MR) images were used to train the deep learning model (144, 48, and 48 for the training, validation, and test sets, respectively, from the Alzheimer's Disease Neuroimaging Initiative [ADNI] database). To overcome the small sample size problem, we used a data augmentation technique to create 576 images per epoch. We used attention-gated U-net for the basic structure of our deep learning model. To validate the performance of the software, we adapted an external test set comprising 100 cranial MR images from the Open Access Series of Imaging Studies (OASIS) database. Results: The facial features (eyes, nose, and ears) were successfully detected and anonymized in both test sets (48 from ADNI and 100 from OASIS). Each result was manually validated in both the 2D image plane and the 3D-rendered images. Furthermore, the ADNI test set was verified using Microsoft Azure's face recognition artificial intelligence service. By adding a user interface, we developed and distributed (via GitHub) software named Deface program for medical images as an open-source project. Conclusions: We developed deep learning-based software for the anonymization of MR images that distorts the eyes, nose, and ears to prevent facial identification of the subject in reconstructed 3D images. It could be used to share medical big data for secondary research while making both data providers and recipients compliant with the relevant privacy regulations."
Bridging the Gap Between Java and Python in Mobile Software Development to Enable MLOps,"Dautov, R; Husom, EJ; Gonidis, F; Papatzelos, S; Malamas, N",10.1109/WIMOB55322.2022.9941679,2022,"The role of Machine Learning (ML) engineers in mobile development has become increasingly important in recent years, as more and more business-critical mobile applications depend on AI components. Many development teams already include dedicated ML engineers who aim to follow agile development practices in their work, as part of the larger MLOps concept. However, the availability of MLOps tools tailored specifically towards mobile platforms is scarce, often due the limited support for non-native programming languages such as Python, as well as the unsuitability of native programming languages such as Java and Kotlin to support ML-related programming tasks. This paper aims to address this gap and describes a plug-in architecture for developing, deploying and running data ingestion and processing components written in Python on the Android platform. With the possibility to pass a user-defined schema with the data format and structure, the proposed architecture ensures that time-series datasets are correctly interpreted by multiple ML modules dealing with both data ingestion and processing,. The proposed approach benefits from modularity, extensibility, customisation, and separation of concerns, which enable ML engineers to be fully involved in a mobile development lifecycle following agile MLOps practices."
Coda: An End-to-End Neural Program Decompiler,"Fu, C; Chen, HL; Liu, HL; Chen, XY; Tian, YD; Koushanfar, F; Zhao, JS",,2019,"Reverse engineering of binary executables is a critical problem in the computer security domain. On the one hand, malicious parties may recover interpretable source codes from the software products to gain commercial advantages. On the other hand, binary decompilation can be leveraged for code vulnerability analysis and malware detection. However, efficient binary decompilation is challenging. Conventional decompilers have the following major limitations: (i) they are only applicable to specific source-target language pair, hence incurs undesired development cost for new language tasks; (ii) their output high-level code cannot effectively preserve the correct functionality of the input binary; (iii) their output program does not capture the semantics of the input and the reversed program is hard to interpret. To address the above problems, we propose Coda(1), the first end-to-end neural-based framework for code decompilation. Coda decomposes the decompilation task into of two key phases: First, Coda employs an instruction type-aware encoder and a tree decoder for generating an abstract syntax tree (AST) with attention feeding during the code sketch generation stage. Second, Coda then updates the code sketch using an iterative error correction machine guided by an ensembled neural error predictor. By finding a good approximate candidate and then fixing it towards perfect, Coda achieves superior performance compared to baseline approaches. We assess Coda's performance with extensive experiments on various benchmarks. Evaluation results show that Coda achieves an average of 82% program recovery accuracy on unseen binary samples, where the state-of-the-art decompilers yield 0% accuracy. Furthermore, Coda outperforms the sequence-to-sequence model with attention by a margin of 70% program accuracy. Our work reveals the vulnerability of binary executables and imposes a new threat to the protection of Intellectual Property (IP) for software development."
The Mozart Reuse Exposed Dataflow Processor for AI and Beyond,"Sankaralingam, K; Nowatzki, T; Gangadhar, V; Shah, P; Davies, M; Galliher, W; Guo, ZL; Khare, J; Vijay, D; Palamuttam, P; Punde, M; Tan, A; Thiruvengadam, V; Wang, RY; Xu, SM",10.1145/3470496.3533040,2022,"In this paper we introduce the Mozart Processor, which implements a new processing paradigm called Reuse Exposed Dataflow (RED). RED is a counterpart to existing execution models of Von-Neumann, SIMT, Dataflow, and FPGA. Dataflow and data reuse are the fundamental architecture primitives in RED, implemented with mechanisms for inter-worker communication and synchronization. The paper defines the processor architecture, the details of the microarchitecture, chip implementation, software stack development, and performance results. The architecture's goal is to achieve near-CPU like flexibility while having ASIC-like efficiency for a large-class of data-intensive workloads. An additional goal was software maturity - have large coverage of applications immediately, avoiding the need for a long-drawn hand-tuning software development phase. The architecture was defined with this software-maturity/compiler friendliness in mind. In short, the goal was to do to GPUs, what GPUs did to CPUs - i.e. be a better solution for a large range of workloads, while preserving flexibility and programmability. The chip was implemented with HBM and PCIe interfaces and taken to production on a 16nm TSMC FFC process. For ML inference tasks with batch-size=4, Mozart is integer factors better than state-of-the-art GPUs even while being nearly 2 technology nodes behind. We conclude with a set of lessons learned, the unique challenges of a clean-slate architecture in a commercial setting, and pointers for uncovered research problems."
Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization,"RamÃ©, A; Ahuja, K; Zhang, JY; Cord, M; Bottou, L; Lopez-Paz, D",,2023,"Foundation models are redefining how AI systems are built. Practitioners now follow a standard procedure to build their machine learning solutions: from a pre-trained foundation model, they fine-tune the weights on the target task of interest. So, the Internet is swarmed by a handful of foundation models fine-tuned on many diverse tasks: these individual fine-tunings exist in isolation without benefiting from each other. In our opinion, this is a missed opportunity, as these specialized models contain rich and diverse features. In this paper, we thus propose model ratatouille, a new strategy to recycle the multiple fine-tunings of the same foundation model on diverse auxiliary tasks. Specifically, we repurpose these auxiliary weights as initializations for multiple parallel fine-tunings on the target task; then, we average all fine-tuned weights to obtain the final model. This recycling strategy aims at maximizing the diversity in weights by leveraging the diversity in auxiliary tasks. Empirically, it improves the state of the art on the reference DomainBed benchmark for out-of-distribution generalization. Looking forward, this work contributes to the emerging paradigm of updatable machine learning where, akin to open-source software development, the community collaborates to reliably update machine learning models. Our code is released here."
GECOJAP: A novel source-code preprocessing technique to improve code coverage,"Godboley, S; Dutta, A; Mohapatra, DP; Mall, R",10.1016/j.csi.2017.04.003,2018,"Safety critical standards such as DO178B/DO178C/ RTCA (Radio Technical Commission for Aeronautics) mandates coverage based testing in Aerospace applications. These standards mandate Level A certification for Modified Condition/Decision Coverage (MC/DC). To perform exhaustive and rigorous testing, concolic testing is used in the testing phase of the software development life cycle. But, still some concolic testers need to improve their performance, so that they can achieve higher coverage. We present an automated Java code transformation technique that can be used as a front-end to concolic testing tool for achieving high coverage. We have developed our tool using four modules. The tool named GEaring COverage for JAva Program (GECOJAP) for implementation of our approach. The first module shows a source code preprocessing technique called JEX-NCT (Java Exclusive-NOR Code Transformer) that inserts dummy branches according to Modified Condition / Decision Coverage (MC/DC) criterion. The second module represents a concolic tester named jCUTE (an open source tool) we used to generate test cases. The third module presents computation of MC/DC% using the generated test cases and original program. The fourth module shows the speed calculator that measures speed of test case generation, GECOJAP is more powerful and efficient in comparison to the existing techniques in terms of code transformation. Using GECOJAP one can, achieve higher code coverage. Also, GECOJAP results in the time and speed of the test case generation process. Our experimentation on ten Java programs for thirty executions shows that our approach achieves higher Branch Coverage and MC/DC over traditional concolic testers by 13.79% and 19% respectively. (C) 2017 Elsevier B.V. All rights reserved."
An Investigation of Cross-Project Learning in Online Just-In-Time Software Defect Prediction,"Tabassum, S; Minku, LL; Feng, DY; Cabral, GG; Song, LY",10.1145/3377811.3380403,2020,"Just-In-Time Software Defect Prediction (JIT-SDP) is concerned with predicting whether software changes are defect-inducing or clean based on machine learning classifiers. Building such classifiers requires a sufficient amount of training data that is not available at the beginning of a software project. Cross-Project (CP) JIT-SDP can overcome this issue by using data from other projects to build the classifier, achieving similar (not better) predictive performance to classifiers trained on Within-Project (WP) data. However, such approaches have never been investigated in realistic online learning scenarios, where WP software changes arrive continuously over time and can be used to update the classifiers. It is unknown to what extent CP data can be helpful in such situation. In particular, it is unknown whether CP data are only useful during the very initial phase of the project when there is little WP data, or whether they could be helpful for extended periods of time. This work thus provides the first investigation of when and to what extent CP data are useful for JIT-SDP in a realistic online learning scenario. For that, we develop three different CP JIT-SDP approaches that can operate in online mode and be updated with both incoming CP and WP training examples over time. We also collect 2048 commits from three software repositories being developed by a software company over the course of 9 to 10 months, and use 19,8468 commits from 10 active open source GitHub projects being developed over the course of 6 to 14 years. The study shows that training classifiers with incoming CP+WP data can lead to improvements in G-mean of up to 53.90% compared to classifiers using only WP data at the initial stage of the projects. For the open source projects, which have been running for longer periods of time, using CP data to supplement WP data also helped the classifiers to reduce or prevent large drops in predictive performance that may occur over time, leading to up to around 40% better G-Mean during such periods. Such use of CP data was shown to be beneficial even after a large number of WP data were received, leading to overall G-means up to 18.5% better than those of WP classifiers."
"Implementation of Gamification in Polish Companies-Stages, Elements, Ethics","Witoszek-Kubicka, A",10.3390/info11080371,2020,"Business gamification has been gaining in popularity in Poland in recent years and is indeed appearing in companies, especially large ones. However, the implementation of game-based solutions is still not sufficiently described. The technology allows the use of solutions such as AI or Machine Learning, but gamification is not only an IT project. The aim of the article is to determine the stages of implementation of business gamification according to various models, describe the existing differences and confront the results with business practice in Poland. To this end, a scoping review on the subject was carried out in terms of the existing methodologies for the implementation of gamification solutions. In the next stage, a scenario was created to conduct individual in-depth interviews (IDI) with companies implementing gamification projects in business. As a result of the research, the practice of implementing business gamification in Poland was described against the background of the methodologies proposed in the literature. This has led to the identification of several significant differences in implementation stages both between theory and practice and among the implementations proposed by the companies participating in the interviews. An attempt was made to explain these differences by taking the type of IT solution as a criterion."
On the Relation of Variability Modeling Languages and Non-Functional Properties,"Friesel, D; MÃ¼ller, M; Ferraz, M; Spinczyk, O",10.1145/3503229.3547055,2022,"Non-functional properties (NFPs) such as code size (RAM, ROM), performance, and energy consumption are at least as important as functional properties in many software development domains. When configuring a software product line - especially in the area of resource-constrained embedded systems - developers must be aware of the NFPs of the configured product instance. Several NFP-aware variability modeling languages have been proposed to address this in the past. However, it is not clear whether a variability modeling language is the best place for handling NFP-related concerns, or whether separate NFP prediction models should be preferred. We shine light onto this question by discussing limitations of state-of-the-art NFP-aware variability modeling languages, and find that both in terms of the development process and model accuracy a separate NFP model is favorable. Our quantitative analysis is based on six different software product lines, including the widely used busybox multi-call binary and the x264 video encoder. We use classification and regression trees (CART) and our recently proposed Regression Model Trees [8] as separate NFP models. These tree-based models can cover the effects of arbitrary feature interactions and thus easily outperform variability models with static, feature-wise NFP annotations. For example, when estimating the throughput of an embedded AI product line, static annotations come with a mean generalization error of 114.5 % while the error of CART is only 9.4 %."
Development of INSVAGRAM: An English Subject-Verb Agreement Mobile Learning Application,"Miranda, JPP; Dianelo, RFB; Yabut, AM; Paguio, CAL; Cruz, AGD; Mangahas, HWG; Malabasco, KC",10.3991/ijet.v16i19.24071,2021,"English language proficiency of students in the Philippines is a point of concern by many institutions particularly as it was highlighted when the results of the first participation of the country in the Program for International Student Assessment (PISA) was released. The use of technology-aided educational applications dedicated to language learning, particularly those that can be accessed through mobile devices, is heavily studied internationally. However, in the Philippines, there is a limited number of developed mobile-supported applications that assist acquisition and learning of language, specifically English. Thus, the study aims were to develop a mobile learning application intended to assist students in learning a least mastered topic in English. The study utilized mixed-method descriptive research design. For the quantitative part, it employed a content-validated questionnaire to survey 132 fifth grade students, while for the qualitative part; it interviewed five English language teachers in the Philippines. A mobile learning application intended for smartphones in an Android OS was developed with focus on subject-verb agreement (SVA) as identified topic which was divided into 10 sub-topics. The application comprises two main parts: The Learn and Play Mode. Additionally, the application was also able to integrate an animated character named Alvin to simulate interaction. Alvin interacts with the user by narrating lessons, giving remarks, providing basic corrective feedback in the form of text, and expressing feelings based on the user's inputs when doing exercises. The survey from students aged 10 to 12 years old showed that they have very often access to technological devices (i.e., desktop, laptops, tablet, smartphone) (M = 4.33) and that their technology aptitude in using them is high (M = 3.45). Furthermore, results also showed that students mostly prefer smartphones for their learning as it is their most available device at home. It is recommended to subject the developed mobile application to a user acceptance and usability testing in different educational contexts. Additionally, it is suggested to explore newer technologies like artificial intelligence to make the application intelligent and more responsive to students' needs and inputs. Future studies may also include addition of new topics as well as developing similar applications that are localized and culturally-aware."
Automotive Data Management SPICE Assessment - Comparison of Process Assessment Models,"PÃ¶rtner, L; Riel, A; Leclaire, M; Makkar, SS",10.1007/978-3-031-42307-9_16,2023,"Many of the current innovations in the automotive environment revolve around autonomous driving, digitalization, connectivity, AI, and new services in the context of mobility. These innovations are based on the collection and use of data. However, the handling of data often plays a subordinate, barely visible role in today's development and operations processes, with corresponding risks. ASPICE as an industry-standard guideline for evaluating system and software development processes helps automotive suppliers incorporate best practices to identify defects earlier in development and ensure that OEM requirements are met. With the purpose of the creation of a process model for data management that is aligned with Automotive SPICE (R) 3.1 and other established standards in the industry, a draft version of the Data Management SPICE was initiated by the intacs group. In this work, we are going to provide improvement potential on the content of the pilot draft of Data Management SPICE assessment, based on our industrial and academic experience in the field of Automotive and Data Management. A first comparison between Camelot Data Management Strategy Assessment and Data Management SPICE Assessment is given. Based on expert's knowledge proposals to improve the quality and the content areas of the Data Management SPICE Assessment before issuing the released version of the standard are shown."
Enhancing Crop Mapping Precision through Multi-Temporal Sentinel-2 Image and Spatial-Temporal Neural Networks in Northern Slopes of Tianshan Mountain,"Zhang, XY; Guo, YL; Tian, XY; Bai, YQ",10.3390/agronomy13112800,2023,"Northern Slopes of Tianshan Mountain (NSTM) in Xinjiang hold significance as a principal agricultural hub within the region's arid zone. Accurate crop mapping across vast agricultural expanses is fundamental for intelligent crop monitoring and devising sustainable agricultural strategies. Previous studies on multi-temporal crop classification have predominantly focused on single-point pixel temporal features, often neglecting spatial data. In large-scale crop classification tasks, by using spatial information around the pixel, the contextual relationships of the crop can be obtained to reduce possible noise interference. This research introduces a multi-scale, multi-temporal classification framework centered on ConvGRU (convolutional gated recurrent unit). By leveraging the attention mechanism of the Strip Pooling Module (SPM), a multi-scale spatial feature extraction module has been designed. This module accentuates vital spatial and spectral features, enhancing the clarity of crop edges and reducing misclassifications. The temporal information fusion module integration features from various periods to bolster classification precision. Using Sentinel-2 imagery spanning May to October 2022, datasets for cotton, corn, and winter wheat of the NSTM were generated for the framework's training and validation. The results demonstrate an impressive 93.03% accuracy for 10 m resolution crop mapping using 15-day interval, 12-band Sentinel-2 data for the three crops. This method outperforms other mainstream methods like Random Forest (RF), Long Short-Term Memory (LSTM), Transformer, and Temporal Convolutional Neural Network (TempCNN), showcasing a kappa coefficient of 0.9062, 7.52% and 2.42% improvement in Overall Accuracy compared to RF and LSTM, respectively, which demonstrate the potential of our model for large-scale crop classification tasks to enable high-resolution crop mapping on the NSTM."
DENAS Automated Rule Generation by Knowledge Extraction from Neural Networks,"Chen, SM; Bateni, S; Grandhi, S; Li, XD; Liu, C; Yang, W",10.1145/3368089.3409733,2020,"Deep neural networks (DNNs) have been widely applied in the software development process to automatically learn patterns from massive data. However, many applications still make decisions based on rules that are manually crafted and verified by domain experts due to safety or security concerns. In this paper, we aim to close the gap between DNNs and rule-based systems by automating the rule generation process via extracting knowledge from well-trained DNNs. Existing techniques with similar purposes either rely on specific DNNs input instances or use inherently unstable random sampling of the input space. Therefore, these approaches either limit the exploration area to a local decision-space of the DNNs or fail to converge to a consistent set of rules. The resulting rules thus lack representativeness and stability. In this paper, we address the two aforementioned shortcomings by discovering a global property of the DNNs and use it to remodel the DNNs decision-boundary. We name this property as the activation probability, and show that this property is stable. With this insight, we propose an approach named DENAS including a novel rule-generation algorithm. Our proposed algorithm approximates the non-linear decision boundary of DNNs by iteratively superimposing a linearized optimization function. We evaluate the representativeness, stability and accuracy of DENAS against five state-of-the-art techniques (LEMNA, Gradient, IG, DeepTaylor, and DTExtract) on three software engineering and security applications: Binary analysis, PDF malware detection, and Android malware detection. Our results show that DENAS can generate more representative rules consistently in a more stable manner over other approaches. We further offer case studies that demonstrate the applications of DENAS such as debugging faults in the DNNs and generating signatures that can detect zero-day malware."
Automatic generation of Web service for the Praxeme software aspect from the ReLEL requirements model,"Andrianjaka, RM; Hajarisena, R; Mihaela, I; Thomas, M; Sorin, I; Raft, RN",10.1016/j.procs.2021.03.098,2021,"Praxeme is an enterprise methodology that combines the two approaches SOA and MDA to build an information system. It is based on the ideology of separating the business concern into a homogeneous set called aspect to better control the usual complexity of a system. The instantiation of the ReLEL requirements model in the Praxeme methodology is an interesting approach to improve the quality of the software development process because it not only allows to specify the user requirements but also to represent the very precise conceptual level of the future system to be designed. Therefore, this article deals with the automatic generation of SOAP web services that represent the software aspect of the Praxeme methodology from the intentional aspect specified by the ReLEL requirements model. To achieve this, we proceeded in two steps, namely the proposal of the rules for deriving the Praxeme Logical Factory Model (logical aspect) into a Web Service Description Language model or WSDL (software aspect) classified as Model to Model Transformation (M2M) in the MDA approach and then the proposal of the rules for translating the WSDL model into a WSDL document known as Model to Text Translation (M2T). The result obtained from the approach proposed in this paper is a WSDL file which is an XML language allowing the complete description of a Web service. We used ATL for the implementation of the M2M approach transformation rules and then the Acceleo template engine for the translation of the M2T. (C) 2021 The Authors. Published by Elsevier B.V."
PyScribe-Learning to describe python code,"Guo, JC; Liu, J; Liu, X; Wan, Y; Zhao, YJ; Li, L; Liu, K; Klein, J; Bissyande, TF",10.1002/spe.3291,2024,"Code comment generation, which attempts to summarize the functionality of source code in textual descriptions, plays an important role in automatic software development research. Currently, several structural neural networks have been exploited to preserve the syntax structure of source code based on abstract syntax trees (ASTs). However, they can not well capture both the long-distance and local relations between nodes while retaining the overall structural information of AST. To mitigate this problem, we present a pro-to type tool titled PyScribe, which extends the Transformer model to a new encoder-decoder-based framework. Particularly, the triplet position is designed and integrated into the node-level and edge-level structural features of AST for producing Python code comments automatically. This paper, to the best of our knowledge, makes the first effort to model the edges of AST as an explicit component for improved code representation. By specifying triplet positions for each node and edge, the overall structural information can be well preserved in the learning process. Moreover, the captured node and edge features go through a two-stage decoding process to yield higher qualified comments. To evaluate the effectiveness of PyScribe, we resort to a large dataset of code-comment pairsby mining Jupyter Notebooks from GitHub, for which we have made it publicly available to support further studies. The experimental results reveal that PyScribe is indeed effective, outperforming the state-of the-art by achieving an average BLEU score (i.e., av-BLEU) of approximate to 0.28."
Open Health Imaging Foundation Viewer: An Extensible Open-Source Framework for Building Web-Based Imaging Applications to Support Cancer Research,"Ziegler, E; Urban, T; Brown, D; Petts, J; Pieper, SD; Lewis, R; Hafey, C; Harris, GJ",10.1200/CCI.19.00131,2020,"PURPOSE Zero-footprint Web architecture enables imaging applications to be deployed on premise or in the cloud without requiring installation of custom software on the user's computer. Benefits include decreased costs and information technology support requirements, as well as improved accessibility across sites. The Open Health Imaging Foundation (OHIF) Viewer is an extensible platform developed to leverage these benefits and address the demand for open-source Web-based imaging applications. The platform can be modified to support site-specific workflows and accommodate evolving research requirements. MATERIALS AND METHODS The OHIF Viewer provides basic image review functionality (eg, image manipulation and measurement) as well as advanced visualization (eg, multiplanar reformatting). It is written as a client-only, single-page Web application that can easily be embedded into third-party applications or hosted as a standalone Web site. The platform provides extension points for software developers to include custom tools and adapt the system for their workflows. It is standards compliant and relies on DICOMweb for data exchange and OpenID Connect for authentication, but it can be configured to use any data source or authentication flow. Additionally, the user interface components are provided in a standalone component library so that developers can create custom extensions. RESULTS The OHIF Viewer and its underlying components have been widely adopted and integrated into multiple clinical research platforms (e,g Precision Imaging Metrics, XNAT, LabCAS, ISB-CGC) and commercial applications (eg, Osirix). It has also been used to build custom imaging applications (eg, ProstateCancer.ai, Crowds Cure Cancer [presented as a case study]). CONCLUSION The OHIF Viewer provides a flexible framework for building applications to support imaging research. Its adoption could reduce redundancies in software development for National Cancer Institute-funded projects, including Informatics Technology for Cancer Research and the Quantitative Imaging Network. (c) 2020 by American Society of Clinical Oncology"
Heterogeneous Graph Neural Networks for Software Effort Estimation,"Phan, H; Jannesari, A",10.1145/3544902.3546248,2022,"Background. Software effort can be measured by story point [35]. Story point estimation is important in software projects' planning. Current approaches for automatically estimating story points focus on applying pre-trained embedding models and deep learning for text regression to solve this problem. These approaches require expensive embedding models and confront challenges that the sequence of text might not be an efficient representation for software issues which can be the combination of text and code. Aims. We propose HeteroSP, a tool for estimating story points from textual input ofAgile software project issues. We select GPT2SP [12] and Deep-SE [8] as the baselines for comparison. Method. First, from the analysis of the story point dataset [8], we conclude that software issues are actually a mixture of natural language sentences with quoted code snippets and have problems related to large-size vocabulary. Second, we provide a module to normalize the input text including words and code tokens of the software issues. Third, we design an algorithm to convert an input software issue to a graph with different types of nodes and edges. Fourth, we construct a heterogeneous graph neural networks model with the support of fastText [6] for constructing initial node embedding to learn and predict the story points of new issues. Results. We did the comparison over three scenarios of estimation, including within project, cross-project within the repository, and cross-project cross repository with our baseline approaches. We achieve the average Mean Absolute Error (MAE) as 2.38, 2.61, and 2.63 for three scenarios. We outperform GPT2SP in 2/3 of the scenarios while outperforming Deep-SE in the most challenging scenario with significantly less amount of running time. We also compare our approaches with different homogeneous graph neural network models and the results show that the heterogeneous graph neural networks model outperforms the homogeneous models in story point estimation. For time performance, we achieve about 570 seconds as the time performance in both three processes: node embedding initialization, model construction, and story point estimation. HeterpSP's artifacts are available at [22]. Conclusion. HeteroSP, a heterogeneous graph neural networks model for story point estimation, achieved good accuracy and running time."
MTDL-EPDCLD: A Multi-Task Deep-Learning-Based System for Enhanced Precision Detection and Diagnosis of Corn Leaf Diseases,"Dai, DK; Xia, PW; Zhu, ZY; Che, HL",10.3390/plants12132433,2023,"Corn leaf diseases lead to significant losses in agricultural production, posing challenges to global food security. Accurate and timely detection and diagnosis are crucial for implementing effective control measures. In this research, a multi-task deep learning-based system for enhanced precision detection and diagnosis of corn leaf diseases (MTDL-EPDCLD) is proposed to enhance the detection and diagnosis of corn leaf diseases, along with the development of a mobile application utilizing the Qt framework, which is a cross-platform software development framework. The system comprises Task 1 for rapid and accurate health status identification (RAHSI) and Task 2 for fine-grained disease classification with attention (FDCA). A shallow CNN-4 model with a spatial attention mechanism is developed for Task 1, achieving 98.73% accuracy in identifying healthy and diseased corn leaves. For Task 2, a customized MobileNetV3Large-Attention model is designed. It achieves a val_accuracy of 94.44%, and improvements of 4-8% in precision, recall, and F1 score from other mainstream deep learning models. Moreover, the model attains an area under the curve (AUC) of 0.9993, exhibiting an enhancement of 0.002-0.007 compared to other mainstream models. The MTDL-EPDCLD system provides an accurate and efficient tool for corn leaf disease detection and diagnosis, supporting informed decisions on disease management, increased crop yields, and improved food security. This research offers a promising solution for detecting and diagnosing corn leaf diseases, and its continued development and implementation may substantially impact agricultural practices and outcomes."
Implementing a high-efficiency similarity analysis approach for firmware code,"Wang, YS; Wang, RM; Jing, J; Wang, HW",10.1371/journal.pone.0245098,2021,"The rapid expansion of the open-source community has shortened the software development cycle, but the spread of vulnerabilities has been accelerated, especially in the field of the Internet of Things. In recent years, the frequency of attacks against connected devices is increasing exponentially; thus, the vulnerabilities are more serious in nature. The state-of-the-art firmware security inspection technologies, such as methods based on machine learning and graph theory, find similar applications depending on the known vulnerabilities but cannot do anything without detailed information about the vulnerabilities. Moreover, model training, which is necessary for the machine learning technologies, requires a significant amount of time and data, resulting in low efficiency and poor extensibility. Aiming at the above shortcomings, a high-efficiency similarity analysis approach for firmware code is proposed in this study. First, the function control flow features and data flow features are extracted from the functions of the firmware and of the vulnerabilities, and the features are used to calculate the SimHash of the functions. The mass storage and fast query capabilities of the SimHash are implemented by the pigeonhole principle. Second, the similarity function pairs are analyzed in detail within and among the basic blocks. Within the basic blocks, the symbolic execution is used to generate the basic block semantic information, and the constraint solver is used to determine the semantic equivalence. Among the basic blocks, the local control flow graphs are analyzed to obtain their similarity. Then, we implemented a prototype and present the evaluation. The evaluation results demonstrate that the proposed approach can implement large-scale firmware function similarity analysis. It can also get the location of the real-world firmware patch without vulnerability function information. Finally, we compare our method with existing methods. The comparison results demonstrate that our method is more efficient and accurate than the Gemini and StagedMethod. More than 90% of the firmware functions can be indexed within 0.1 s, while the search time of 100,000 firmware functions is less than 2 s."
"Effects of Four-Week Kayak Training on Three-Dimensional Paddling Kinetics, Body Kinematics, and Electromyography Activity in a Novice Paddler: A Case Study","Kinugasa, R; Kubo, S; Endo, K",10.3389/fspor.2021.694989,2021,"From a biomechanical viewpoint, no longitudinal quantitative studies have been conducted on inexperienced paddlers. The present study aimed to investigate changes in three-dimensional paddling kinetics and kinematics, whole-body kinematics, and muscle activity with four-week on-water kayak training in a novice paddler. The participant practiced kayak paddling on river for four weeks. Before and after training, paddling kinetics and kinematics, body kinematics, and electromyography (EMG) activity were measured using a kayak ergometer. After the four-week training, the time required for on-water paddling for 270 m was reduced by 7.3% from pre to post training, while the average impulse in the x-direction significantly (P < 0.001, partial eta squared [eta(2)] = 0.82) increased from 71.9 +/- 1.9 to 91.1 +/- 5.4 N kg(-1) s(-1). Furthermore, with training, the stroke rate and stroke length in the x-direction significantly (P < 0.001, partial eta(2) = 0.80 and 0.79, respectively) increased from 62.8 +/- 1.2 to 81.0 +/- 2.9 spm and from 1.53 +/- 0.04 to 1.71 +/- 0.02 m, respectively. After training, the transition time significantly (P < 0.001, partial eta(2) = 0.32) decreased (from 0.04 +/- 0.01 to 0.01 +/- 0.01 s), and there was an increase in paddle catch position (from -0.88 +/- 0.01 to -1.04 +/- 0.03 m). The pull time was not significantly changed (P = 0.077, partial eta(2) = 0.08) because of the increasing stroke length after training, meaning that substantial pull time, which defined as pull time relative to the stroke displacement, was shorter in post-training than in pre-training. The relative change in average impulse in the x-direction with training was significantly (r = 0.857, P = 0.014) correlated with that of vastus lateralis EMG. These results indicated that after four-week kayak training of the novice paddler, the key mechanism underlying time reduction to perform on-water paddling for 270 m was associated with (1) increased average impulse along the propulsive direction caused by an increase in vastus lateralis EMG and (2) a higher stroke rate, which was attributed to a reduction in the pull and transition times."
Supporting program comprehension by generating abstract code summary tree,"Bhattacharjee, A; Roy, B; Schneider, KA",10.1145/3510455.3512793,2022,"Reading through code, finding relevant methods, classes and files takes a significant portion of software development time. Having good tool support for this code browsing activity can reduce human effort and increase overall developer productivity. To help with program comprehension activities, building an abstract code summary of a software system from its call graph is an active research area. A call graph is a visual representation of the caller-callee relationships between different methods of a software system. Call graphs can be difficult to comprehend for a large code-base. Previous work by Gharibi et al. on abstract code summarizing suggested using the Agglomerative Hierarchical Clustering (AHC) tree for understanding the codebase. Each node in the tree is associated with the top five method names. When we replicated the previous approach, we observed that the number of nodes in the AI IC tree is burdensome for developers to explore. We also noticed only five method names for each node is not sufficient to comprehend an abstract node. We propose a technique to transform the AHC tree using cluster flattening for natural grouping and reduced nodes. We also generate a natural text summary for each abstract node derived from method comments. In order to evaluate our proposed approach, we collected developers' opinions about the abstract code summary tree based on their codebase. The evaluation results confirm that our approach can not only help developers get an overview of their codebases but also could assist them in doing specific software maintenance tasks."
NEW TECHNOLOGIES AND THEIR INTERCONNECTION IN THE CREATION AND PROCESSING OF 3DMODELS AND SCENES,"Janovsky, M",10.5194/isprs-archives-XLVI-5-W1-2022-129-2022,2022,"This article focuses on the content and processing of the prepared dissertation thesis, which deals with the possibilities of using new technologies and software tools such as procedural generation of 3D models, game engines, rendering programs, virtual and augmented reality, eye tracking and BIM in cartographic processing and 3D landscape visualization. The dissertation focuses on the interconnection of these technologies, on new methods of processing and on the combination of the use of modern software equipment with already existing methods of processing into one complete process. For these purposes, the possibilities of using, among other things, software development kits (SDK), plugins and extensions of existing programs will be explored. Great emphasis will be placed on the Unity game engine and the ability to use its individual parts (rendering engine, physics engine, scripting, simulation, and AI) to create new workflows and new types of processing / outputs. The main goal of this dissertation is to find and document the possibilities of interconnection of individual technologies, especially those that are not commonly used or can be considered new (no scientific articles were found that would use these methods of interconnection). The result of the dissertation will be new working procedures and more realistic visualizations of 3D models and scenes corresponding to the technologies available today. An equally important part of the dissertation will be the creation of sample projects for individual programs, test data, work procedures and general teaching materials to simplify the use of these technologies in teaching and in departmental projects."
A Lightweight End-to-End Speech Recognition System on Embedded Devices,"Wang, Y; Nishizaki, H",10.1587/transinf.2022EDP7221,2023,"In industry, automatic speech recognition has come to be a competitive feature for embedded products with poor hardware resources. In this work, we propose a tiny end-to-end speech recognition model that is lightweight and easily deployable on edge platforms. First, instead of sophisticated network structures, such as recurrent neural networks, trans-formers, etc., the model we propose mainly uses convolutional neural net-works as its backbone. This ensures that our model is supported by most software development kits for embedded devices. Second, we adopt the basic unit of MobileNet-v3, which performs well in computer vision tasks, and integrate the features of the hidden layer at different scales, thus com-pressing the number of parameters of the model to less than 1 M and achiev-ing an accuracy greater than that of some traditional models. Third, in order to further reduce the CPU computation, we directly extract acoustic repre-sentations from 1-dimensional speech waveforms and use a self-supervised learning approach to encourage the convergence of the model. Finally, to solve some problems where hardware resources are relatively weak, we use a prefix beam search decoder to dynamically extend the search path with an optimized pruning strategy and an additional initialism language model to capture the probability of between-words in advance and thus avoid prema-ture pruning of correct words. In our experiments, according to a number of evaluation categories, our end-to-end model outperformed several tiny speech recognition models used for embedded devices in related work."
"REDUCT: Keep it Close, Keep it Cool!","Nori, AV; Bera, R; Balachandran, S; Rakshit, J; Omer, OJ; Abuhatzera, A; Kuttanna, B; Subramoney, S",10.1109/ISCA52012.2021.00022,2021,"Deep Neural Networks (DNN) are used in a variety of applications and services. With the evolving nature of DNNs, the race to build optimal hardware (both in datacenter and edge) continues. General purpose multi-core CPUs offer unique attractive advantages for DNN inference at both datacenter [60] and edge [71]. Most of the CPU pipeline design complexity is targeted towards optimizing general-purpose single thread performance, and is overkill for relatively simpler, but still hugely important, data parallel DNN inference workloads. Addressing this disparity efficiently can enable both raw performance scaling and overall performance/Watt improvements for multi-core CPU DNN inference. We present REDUCT, where we build innovative solutions that bypass traditional CPU resources which impact DNN inference power and limit its performance. Fundamentally, REDUCT's Keep it close policy enables consecutive pieces of work to be executed close to each other. REDUCT enables instruction delivery/decode close to execution and instruction execution close to data. Simple ISA extensions encode the fixed-iteration count loop-y workload behavior enabling an effective bypass of many power-hungry front-end stages of the wide Out-of-Order (OoO) CPU pipeline. Per core performance scales efficiently by distributing lightweight tensor compute near all caches in a multi-level cache hierarchy. This maximizes the cumulative utilization of the existing architectural bandwidth resources in the system and minimizes movement of data. Across a number of DNN models, REDUCT achieves a 2.3x increase in convolution performance/Watt with a 2x to 3.94x scaling in raw performance. Similarly, REDUCT achieves a 1.8x increase in inner-product performance/Watt with 2.8x scaling in performance. REDUCT performance/power scaling is achieved with no increase to cache capacity or bandwidth and a mere 2.63% increase in area. Crucially, REDUCT operates entirely within the CPU programming and memory model, simplifying software development, while achieving performance similar to or better than state-of-the-art Domain Specific Accelerators (DSA) for DNN inference, providing fresh design choices in the AI era."
Diagnosis of Alzheimer's Disease Based on the Modified Tresnet,"Xu, ZL; Deng, HM; Liu, J; Yang, Y",10.3390/electronics10161908,2021,"In the medical field, Alzheimer's disease (AD), as a neurodegenerative brain disease which is very difficult to diagnose, can cause cognitive impairment and memory decline. Many existing works include a variety of clinical neurological and psychological examinations, especially computer-aided diagnosis (CAD) methods based on electroencephalographic (EEG) recording or MRI images by using machine learning (ML) combined with different preprocessing steps such as hippocampus shape analysis, fusion of embedded features, and so on, where EEG dataset used for AD diagnosis is usually is large and complex, requiring extraction of a series of features like entropy features, spectral feature, etc., and it has seldom been applied in the AD detection based on deep learning (DL), while MRI images were suitable for both ML and DL. In terms of the structural MRI brain images, few differences could be found in brain atrophy among the three situations: AD, mild cognitive impairment (MCI), and Normal Control (NC). On the other hand, DL methods have been used to diagnose AD incorporating MRI images in recent years, but there have not yet been many selective models with very deep layers. In this article, the Gray Matter (GM) Magnetic Resonance Imaging (MRI) is automatically extracted, which could better distinguish among the three types of situations like AD, MCI, and NC, compared with Cerebro Spinal Fluid (CSF) and White Matter (WM). Firstly, FMRIB Software Library (FSL) software is utilized for batch processing to remove the skull, cerebellum and register the heterogeneous images, and the SPM + cat12 tool kits in MATLAB is used to segment MRI images for obtaining the standard GM MRI images. Next, the GM MRI images are trained by some new neural networks. The characteristics of the training process are as follows: (1) The Tresnet, as the network that achieves the best classification effect among several new networks in the experiment, is selected as the basic network. (2) A multi-receptive-field mechanism is integrated into the network, which is inspired by neurons that can dynamically adjust the receptive fields according to different stimuli. (3) The whole network is realized by adding multiple channels to the convolutional layer, and the size of the convolution kernel of each channel can be dynamically adjusted. (4) Transfer learning method is used to train the model for speeding up the learning and optimizing the learning efficiency. Finally, we achieve the accuracies of 86.9% for AD vs. NC, 63.2% for AD vs. MCI vs. NC respectively, which outperform the previous approaches. The results demonstrate the effectiveness of our approach."
Research on Pattern Extraction Method of Underwater Acoustic Signal Based on Linear Array,"Yu, M; He, YT; Kong, Q",10.1155/2022/1819423,2022,"Underwater acoustic signal is an important reference data for marine information research. The research and application of underwater acoustic signal have been widely concerned and valued by countries and enterprises. With the needs of modern military development and the development of marine industry, the research and application of underwater acoustic signal will develop faster and faster. In order to better understand marine information, it is necessary to collect seawater acoustic signal data. Aiming at the purpose of recording underwater acoustic signals placed in the ocean for a long time, this study innovates the calibration and recording of large dynamic range of long-time underwater acoustic signals, improves the circuit setting methods such as receiving, amplification, and sampling, designs a large dynamic series of long-time underwater acoustic signal recording device, and adopts the linear array extraction method, so that it can monitor the underwater acoustic biological sound under the condition of low power. It can also monitor the blasting sound of offshore engineering. Hardware circuit design mainly includes main control chip selection, amplification circuit design, filter circuit design, analog-to-digital conversion circuit design, storage circuit design, and some auxiliary circuit design. The fourth chapter introduces the software development process of large dynamic range underwater acoustic signal recorder and mainly introduces the system development tool, system clock working method, real-time clock module working method, underwater acoustic signal acquisition method, data storage scheme design, and the use of FatFs file system. The underwater acoustic signal data is stored on a MicroSD in the form of TXT file; linear array extraction method is used for feature extraction. Compared to other methods, the transformer will suppress DC and low-frequency interference signals, thus achieving high-pass filtering characteristics. Finally, the performance and experimental results of the whole underwater acoustic signal recording device are analyzed. After testing, the underwater acoustic signal recording device designed in this paper works stably and can record underwater acoustic signals with large dynamic range for a long time in low-power mode."
"AsHES 2020 Keynote Speaker (5:30 pm CDT) Multi-Hetero Accelerated Supercomputing: System, Programming and Applications","Boku, T",10.1109/IPDPSW50202.2020.00080,2020,"In the Exa-scale era, one of the most important and tough problems is how to enhance the sustained performance against the limited power budget. Traditional multi- or many-core general CPUs are still popular for easy programming and porting of general applications. However, it is getting to face to the limit by semiconductor technology limit, memory capacity per core, network bandwidth, etc. GPU represents the attached accelerator solution in heterogeneous computing thanks to its high peak performance ratio to power consumption, and moreover, recent progress on AI applications such as TensorFlow ready NVIDIA GPUs. However, GPU's extremely high performance is provided by wide width of data parallel computation both in instruction level and core/thread level which requires thousands of SIMD operations simultaneously executed. Many of success stories on GPU acceleration depend on their simple parallel execution and quite low rate of exception (if statements) handling. Another problem is the interconnection network which relies on CPU-bundle high performance network such as InfiniBand. In our research team has been focusing on the FPGA computation, which is one of the hot topics of new type of accelerators for HPC, however it is quite difficult to achieve a comparable performance with GPU especially for SIMD style applications. So, we think that a new generation of accelerated computing supported by multiple heterogeneous accelerator platform including several types of ones together on computation node. The first target is a combination of GPU and FPGA to provide 360-degree solution with SIMD and pipelined parallelism depending on the characteristics of each computation part of a large application. In this talk, I will introduce the current status of our Multi-Hetero Accelerated System running on University of Tsukuba, its hardware and software development, and real application with preliminary performance evaluation."
IMPACT OF PROBLEM-BASED LEARNING IN THE LECTURING OF MECHATRONIC ENGINEERS AT THE NATIONAL AUTONOMOUS UNIVERSITY OF MEXICO,"PeÃ±uelas-Rivas, U; Minami-Koyama, Y; Miranda-Cordero, L",,2019,"In the School of Engineering of the National Autonomous University of Mexico (UNAM) there is an academic space known as Open Robotics Workshop, in which, professors and students participate developing robotics, mechatronics and artificial intelligence projects with which it is intended to enhance the training quality of graduates. The Open Robotics Workshop is the place where the institutional project Mobile robots for the recognition and inspection of areas with restricted mobility is carried out, under the Program of Support for Research and Technological Innovation Projects, PAPIIT by its acronym in Spanish, of which its main goal is: ... create an interdisciplinary group of academics, undergraduate and post graduate students whom will perform social service and theses develop solving specific problems of the project according to methodologies of problem-based learning that foster a solid and significant academic formation. The theoretical structures for the project are based on constructivism and constructionism, which are the pillars in which problem-based learning (PBL) is supported, which recently has taken great importance in formal education, because of its advantages over the called traditional working methods due to the results obtained. The approach to the problems of this project is based on design methodologies applied to mechatronics such as those proposed by Ullman and Ulrich, as well as production and assembly methodologies such as those proposed by Kanban and Boothroyd, and for the design control components and software architectures, agile software development SCRUM-based methodologies have been used, which have given good results. There are two purposes of this article: first one, to describe the way the PBL is applied in the Open Robotics Workshop, the benefits obtained by the students, teachers and the institution itself, and the strengths and weaknesses that have been detected through time; the second one, show the results of a survey applied to graduates, employers and researchers who are directly related to those graduates on the Mechatronic Engineering career, of the aforementioned School of Engineering, on the influence of the PBL used as a didactic strategy in their professional training and improvement of their performance in the workplace. The participants expressed their opinion regarding the quality of the engineers graduated from UNAM who developed projects during their studies, comparing them with those trained under the traditional teaching approach; likewise, they described the capacity shown by the graduates when they have to work on individual, group and/or entrepreneurship projects. From the analysis of the information obtained, it can be seen that the PBL helped to train engineers with greater confidence and methodological knowledge to approach projects, with the ability to work collaboratively by sharing information with their colleagues, in a way that makes it easier for them to lead their work groups. In addition, it provides information that feeds the permanent improvement and transcendence of the application of non-traditional teaching strategies used in the training of engineers. It is concluded that the application of the PBL in the Open Robotics Workshop has been successful from the point of view of training excellent engineering professionals and proposes ideas that could improve their deficiencies to consolidate their impact on the quality of their participants."
Generative AI in Software Development Education: Insights from a Degree Apprenticeship Programme,"Petrovska, O; Clift, L; Moller, F",10.1145/3610969.3611132,2023,
The next step in Requirements Management: Artificial Intelligence,"Woodcock, H",10.1109/RE.2019.00012,2019,
Convergence of AI for Secure Software Development,"Andrade, R; Torres, J; Flores, P; Cabezas, E; Segovia, J",10.1109/CSNET64211.2024.10851473,2024,"The traditional approach to software security often fails to detect vulnerabilities early enough. This research explores how artificial intelligence (AI) can be integrated into the software development cycle to improve the identification of vulnerabilities and antipatterns. In the context of DevSecOps, we propose a new methodology that detects antipatterns automatically using machine learning and natural language processing techniques. Also, we explore the possibility of using a large language model (LLM) in a CI (Continuous Integration) and CD (Continuous Delivery) pipeline, to allow the developers real-time feedback, with it the initial phases of the software development cycle can improve the security and quality of the software. Further study is necessary to determine the AI model's true capability in managing real-world challenges."
Can LLM Replace Stack Overflow? A Study on Robustness and Reliability of Large Language Model Code Generation,"Zhong, L; Wang, ZL",,2024,"Recently, large language models (LLMs) have shown an extraordinary ability to understand natural language and generate programming code. It has been a common practice for software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability, and robustness of the code generation from LLMs have not yet been thoroughly studied. The executable code is not equivalent to reliable and robust code, especially in the context of real-world software development. For example, the misuse of APIs in the generated code could lead to severe problems, such as resource leaks, program crashes, etc. Existing code evaluation benchmarks and datasets focus on crafting small tasks such as programming questions in coding interviews. However, this deviates from the problems developers typically consult LLMs about. To fill the missing piece, we propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs. We collect 1208 coding questions from Stack Overflow on 18 representative Java APIs. We summarize the common misuse patterns of these APIs and evaluate them on current popular LLMs. The evaluation results show that even GPT-4 has 62% of the generated code that contains API misuses. It would cause unexpected consequences if the code is introduced into real-world software."
Large language model evaluation for high-performance computing software development,"Godoy, WF; Valero-Lara, P; Teranishi, K; Balaprakash, P; Vetter, JS",10.1002/cpe.8269,2024,"We apply AI-assisted large language model (LLM) capabilities of GPT-3 targeting high-performance computing (HPC) kernels for (i) code generation, and (ii) auto-parallelization of serial code in C ++, Fortran, Python and Julia. Our scope includes the following fundamental numerical kernels: AXPY, GEMV, GEMM, SpMV, Jacobi Stencil, and CG, and language/programming models: (1) C++ (e.g., OpenMP [including offload], OpenACC, Kokkos, SyCL, CUDA, and HIP), (2) Fortran (e.g., OpenMP [including offload] and OpenACC), (3) Python (e.g., numpy, Numba, cuPy, and pyCUDA), and (4) Julia (e.g., Threads, CUDA.jl, AMDGPU.jl, and KernelAbstractions.jl). Kernel implementations are generated using GitHub Copilot capabilities powered by the GPT-based OpenAI Codex available in Visual Studio Code given simple + + prompt variants. To quantify and compare the generated results, we propose a proficiency metric around the initial 10 suggestions given for each prompt. For auto-parallelization, we use ChatGPT interactively giving simple prompts as in a dialogue with another human including simple prompt engineering follow ups. Results suggest that correct outputs for C++ correlate with the adoption and maturity of programming models. For example, OpenMP and CUDA score really high, whereas HIP is still lacking. We found that prompts from either a targeted language such as Fortran or the more general-purpose Python can benefit from adding language keywords, while Julia prompts perform acceptably well for its Threads and CUDA.jl programming models. We expect to provide an initial quantifiable point of reference for code generation in each programming model using a state-of-the-art LLM. Overall, understanding the convergence of LLMs, AI, and HPC is crucial due to its rapidly evolving nature and how it is redefining human-computer interactions."
LLM4MDG: Leveraging Large Language Model to Construct Microservices Dependency Graph,"Hu, JK; Li, YK; Xiang, ZX; Ma, LP; Jia, XQ; Huang, QJ",10.1109/TrustCom63139.2024.00128,2024,"Microservices architecture has gained popularity in modern software development due to its scalability and flexibility. However, understanding the complexity of interactions and dependencies between services presents significant challenges, which complicates the identification and analysis of errors within microservice applications. To gain insights into the architecture and interdependencies of microservices applications, prior studies have developed dependency graphs to illustrate the relationships among services. However, the methods used to construct these dependency graphs are not suitable for common microservices applications and suffer from insufficient data granularity. To address these shortcomings, we introduce LLM4MDG, an innovative framework for constructing microservices dependency graphs using an LLM-driven multi-agent system. By leveraging optimized prompt engineering and principles of knowledge graphs, LLM4MDG can effectively identify and interpret service interactions across diverse microservice ecosystems, achieving high accuracy and adaptability across various scenarios. We also present a new open-source dataset comprising 47 microservices applications, annotated by domain experts, to validate our framework. Evaluation results demonstrate that LLM4MDG achieves an 88.3% accuracy in identifying data dependencies in the Train Ticket project, a benchmark application with over 80 service instances. This study provides a robust solution for constructing dependency graphs and facilitating better system understanding and management."
Towards LLM-Generated Code Tours for Onboarding,"Balfroid, M; Vanderose, B; Devroey, X",10.1145/3643787.3648033,2024,"Onboarding new developers is a challenge for any software project. Addressing this challenge relies on human resources (e.g., having a senior developer write documentation or mentor the new developer). One promising solution is using annotated code tours. While this approach partially lifts the need for mentorship, it still requires a senior developer to write this interactive form of documentation. This paper argues that a Large Language Model (LLM) might help with this documentation process. Our approach is to record the stack trace between a failed test and a faulty method. We then extract code snippets from the methods in this stack trace using CodeQL, a static analysis tool and have them explained by gpt-3.5-turbo-1106, the LLM behind ChatGPT. Finally, we evaluate the quality of a sample of these generated tours using a checklist. We show that the automatic generation of code tours is feasible but has limitations like redundant and low-level explanations."
Gauging Tech Community Acceptance of Rapid Prototyping in Unfamiliar Programming Languages using LLM Chatbots,"Chusap, K; Liu, C",10.1145/3643795.3648393,2024,"Large Language Model (LLM) chatbots such as ChatGPT possess information not only about human languages but also computer languages. It is now possible to perform programming and software design tasks with assistance from ChatGPT. We are particularly interested in how the software development community views the use of LLM chatbots in rapid prototyping using unfamiliar programming languages. In four different tech events, several example scenarios of how a tech-savvy engineer could use ChatGPT to prototype apps in unfamiliar programming languages were demonstrated, including a health education app. The four events include an IEEE chapter workshop, an IEEE WIE (Woman In Engineering) meeting, an IEEE joint chapter talk, and a university-level Computer Science class. The responses from the tech audience showed that the majority perceived value in the use of LLM chatbots in these contexts, even though there were subtle differences among different groups. This shows the need for further research on how to effectively incorporate LLM chatbots into traditional software design workflow to better serve the software development community."
Developer Behaviors in Validating and Repairing LLM-Generated Code Using IDE and Eye Tracking,"Tang, NZ; Chen, M; Ning, Z; Bansal, A; Huang, Y; McMillan, C; Li, TJJ",10.1109/VL/HCC60511.2024.00015,2024,"The increasing use of large language model (LLM)-powered code generation tools, such as GitHub Copilot, is transforming software engineering practices. This paper investigates how developers validate and repair code generated by Copilot and examines the impact of code provenance awareness during these processes. We conducted a lab study with 28 participants tasked with validating and repairing Copilot-generated code in three software projects. Participants were randomly divided into two groups: one informed about the provenance of LLM-generated code and the other not. We collected data on IDE interactions, eye-tracking, cognitive workload assessments, and conducted semi-structured interviews. Our results indicate that, without explicit information, developers often fail to identify the LLM origin of the code. Developers exhibit LLM-specific behaviors such as frequent switching between code and comments, different attentional focus, and a tendency to delete and rewrite code. Being aware of the code's provenance led to improved performance, increased search efforts, more frequent Copilot usage, and higher cognitive workload. These findings enhance our understanding of developer interactions with LLM-generated code and inform the design of tools for effective human-LLM collaboration in software development."
LLM-based Individual Contribution Summarization in Software Projects,"de Miranda, F; Ferrao, RC; Soler, DP; Graglia, MAV",10.1145/3649409.3691092,2024,"This work in progress is about preliminary results in using a Large Language Model (LLM) to summarize individual student contributions in open-ended software projects. Projects for industry clients are good real-world learning opportunities. Though, if the scope is open and defined based on external clients' needs, each group's project will look unique, what makes a challenge for grading and regular feedback. Distributed code version control systems such as Git and resources such as Git classroom help, but it is still burdensome to have professors and TAs looking at the repositories with a frequency that enables useful, timely feedback for the students. We prototyped a method of summarizing each student's contributions to a project's Git repository using an LLM, indicating how to preprocess and break down repository data in order to get better responses from the system. Each student's contributions were extracted using Pydriller. This technique was tested during a 3-week full-time software development sprint in a class of 28 students. Preliminary results indicate a general agreement of students and faculty with the synthesized summaries and an increase in students' awareness of individual responsibilities within the teams and an improvement in engagement among less active members."
Leveraging LLMs for the Quality Assurance of Software Requirements,"Lubos, S; Felfernig, A; Tran, TNT; Garber, D; El Mansi, M; Erdeniz, SP; Le, VM",10.1109/RE59067.2024.00046,2024,"Successful software projects depend on the quality of software requirements. Creating high-quality requirements is a crucial step toward successful software development. Effective support in this area can significantly reduce development costs and enhance the software quality. In this paper, we introduce and assess the capabilities of a Large Language Model (LLM) to evaluate the quality characteristics of software requirements according to the ISO 29148 standard. We aim to further improve the support of stakeholders engaged in requirements engineering (RE). We show how an LLM can assess requirements, explain its decision-making process, and examine its capacity to propose improved versions of requirements. We conduct a study with software engineers to validate our approach. Our findings emphasize the potential of LLMs for improving the quality of software requirements."
DevGPT: Studying Developer-ChatGPT Conversations,"Xiao, T; Treude, C; Hata, H; Matsumoto, K",10.1145/3643991.3648400,2024,"This paper introduces DevGPT, a dataset curated to explore how software developers interact with ChatGPT, a prominent large language model (LLM). The dataset encompasses 29,778 prompts and responses from ChatGPT, including 19,106 code snippets, and is linked to corresponding software development artifacts such as source code, commits, issues, pull requests, discussions, and Hacker News threads. This comprehensive dataset is derived from shared ChatGPT conversations collected from GitHub and Hacker News, providing a rich resource for understanding the dynamics of developer interactions with ChatGPT, the nature of their inquiries, and the impact of these interactions on their work. DevGPT enables the study of developer queries, the effectiveness of ChatGPT in code generation and problem solving, and the broader implications of AI-assisted programming. By providing this dataset, the paper paves the way for novel research avenues in software engineering, particularly in understanding and improving the use of LLMs like ChatGPT by developers."
Are We Testing or Being Tested? Exploring the Practical Applications of Large Language Models in Software Testing,"Santos, R; Santos, I; Magalhaes, C; Santos, RD",10.1109/ICST60714.2024.00039,2024,"A Large Language Model (LLM) represents a cutting-edge artificial intelligence model that generates content, including grammatical sentences, human-like paragraphs, and syntactically code snippets. LLMs can play a pivotal role in software development, including software testing. LLMs go beyond traditional roles such as requirement analysis and documentation and can support test case generation, making them valuable tools that significantly enhance testing practices within the field. Hence, we explore the practical application of LLMs in software testing within an industrial setting, focusing on their current use by professional testers. In this context, rather than relying on existing data, we conducted a cross-sectional survey and collected data within real working contexts-specifically, engaging with practitioners in industrial settings. We applied quantitative and qualitative techniques to analyze and synthesize our collected data. Our findings demonstrate that LLMs effectively enhance testing documents and significantly assist testing professionals in programming tasks like debugging and test case automation. LLMs can support individuals engaged in manual testing who need to code. However, it is crucial to emphasize that, at this early stage, software testing professionals should use LLMs with caution while well-defined methods and guidelines are being built for the secure adoption of these tools."
Studying the Quality of Source Code Generated by Different AI Generative Engines: An Empirical Evaluation,"Tosi, D",10.3390/fi16060188,2024,"The advent of Generative Artificial Intelligence is opening essential questions about whether and when AI will replace human abilities in accomplishing everyday tasks. This issue is particularly true in the domain of software development, where generative AI seems to have strong skills in solving coding problems and generating software source code. In this paper, an empirical evaluation of AI-generated source code is performed: three complex coding problems (selected from the exams for the Java Programming course at the University of Insubria) are prompted to three different Large Language Model (LLM) Engines, and the generated code is evaluated in its correctness and quality by means of human-implemented test suites and quality metrics. The experimentation shows that the three evaluated LLM engines are able to solve the three exams but with the constant supervision of software experts in performing these tasks. Currently, LLM engines need human-expert support to produce running code that is of good quality."
A Quantitative Analysis of Quality and Consistency in AI-generated Code,"Clark, A; Igbokwe, D; Ross, S; Zibran, MF",10.1109/ICoSSE62619.2024.00014,2024,"With the recent emergence of generative AI (Artificial intelligence), Large Language Model (LLM) based tools such as ChatGPT have become popular assistants to humans in diverse tasks. ChatGPT has also been widely adopted for solving programming problems and for generating source code in software development. This research investigates both the code quality and the consistency of code quality over iterative prompts in 625 ChatGPT-generated Python code samples in the DevGPT dataset and the corresponding code snippets regenerated by manually prompting ChatGPT. Code samples are measured in terms of seven Halstead complexity metrics. We also assess how consistent they are across code snippets generated by different versions of ChatGPT. It was found that while ChatGPT generates good quality code across iterative prompts, it does generate semi-frequent bugs, similar to how humans do, necessitating code review before integration. These traits also remain consistent across code snippets generated by subsequent releases of ChatGPT. These results suggest using AI-generated source code in software development will not hinder the process."
Multi-type requirements traceability prediction by code data augmentation and fine-tuning MS-CodeBERT,"Majidzadeh, A; Ashtiani, M; Zakeri-Nasrabadi, M",10.1016/j.csi.2024.103850,2024,"Requirement traceability is a crucial quality factor that highly impacts the software evolution process and maintenance costs. Automated traceability links recovery techniques are required for a reliable and low-cost software development life cycle. Pre-trained language models have shown promising results on many natural language tasks. However, using such pre-trained models for requirement traceability needs large and quality traceability datasets and accurate fine-tuning mechanisms. This paper proposes code augmentation and finetuning techniques to prepare the MS-CodeBERT pre-trained language model for various types of requirements traceability prediction including documentation-to-method, issue-to-commit, and issue-to-method links. Three program transformation operations, namely, Rename Variable, Swap Operands, and Swap Statements are designed to generate new quality samples increasing the sample diversity of the traceability datasets. A 2stage and 3-stage fine-tuning mechanism is proposed to fine -tune the language model for the three types of requirement traceability prediction on provided datasets. Experiments on 14 Java projects demonstrate a 6.2% to 8.5% improvement in the precision, 2.5% to 5.2% improvement in the recall, and 3.8% to 7.3% improvement in the F1 score of the traceability prediction models compared to the best results from the state-of-the-art methods."
State of the Art of the Security of Code Generated by LLMs: A Systematic Literature Review,"RamÃ­rez, LC; LimÃ³n, X; SÃ¡nchez-GarcÃ­a, AJ; PÃ©rez-Arriaga, JC",10.1109/CONISOFT63288.2024.00050,2024,"AI-assisted programming has experienced a surge in popularity over the past few years, largely thanks to advancements in Large Language Model technologies. This has led to the emergence of tools like ChatGPT and GitHub Copilot. However, the use of AI models for code generation comes with a downside: the resulting code is susceptible to vulnerabilities, thus posing new challenges in the field of secure software development. In this study, we analyze the current state of research regarding the security of LLM generated code from the Software Engineering perspective. We conducted a Systematic Literature Review following the guidelines from Kitchenham et al. The search process included five sources: IEEE Xplore, ACM, Science Direct, Springer Link and Wiley Online Library. We also included an iteration of backward and forward snowballing. We obtained 3104 peer-reviewed studies though Quasi-Gold aided automated search and selected the most relevant ones through 5 stages. The final selection includes 15 primary studies from which we extracted and synthesized data. We identified seven different kinds of security vulnerability present in LLM generated code, six different mitigation strategies and practices, and four tools recommended by authors to use in conjunction with LLM code generation. Security related issues within LLM generated code have only just begun to be explored, initial research has already emphasized the significance of considering the inclusion of AI-powered code generation in software projects, as it carries the risk of introducing vulnerabilities at a higher rate than human-generated code. The vulnerabilities, practices and tools identified in this study, can potentially help developers to use LLM programming assistants more responsibly, making informed decisions when leveraging LLM technology."
Translating Formal Specs: Event-B to English,"Vanhari, FK",10.1109/CASCON62161.2024.10838142,2024,"In software development, formal specifications are essential for providing precision and conciseness in describing system behavior. The Event-B specification language is a powerful tool in this domain, offering a variety of data types and a rich set of operations to model complex systems. However, some of these operations use less common notation, which can be challenging to understand. To assist readers of these specifications, we propose translating Event-B expressions and predicates into English. Our primary goal is to ensure that the translations are both accurate and comprehensible. We employ a rule-based system to generate accurate translations, adhering to predefined linguistic rules, and utilize a large language model to select the most readable translation. This hybrid approach ensures both fidelity and readability. Our evaluation includes measuring perplexity scores and human assessments of fluency and adequacy. The results show that GPT-4 aligns best with human evaluators, followed by GPT-3.5, with LLaMA-3 performing slightly lower."
Enhancing privacy policy comprehension through Privacify: : A user-centric approach using advanced language models,"Woodring, J; Perez, K; Ali-Gombe, A",10.1016/j.cose.2024.103997,2024,"As the digital age advances, the collection, usage, and dissemination of personal data have become critical concerns for users, regulators, and the cybersecurity community. Questions surrounding the extent of identifiable data collection, its usage, sharing, selling, and the mechanisms of consent are increasingly central to discussions on user data privacy. These issues highlight the need for effective management and comprehension of privacy policies. To this end, this paper introduces Privacify- a production-ready web application designed to enhance the accessibility and understandability of privacy policies, thus empowering users to make more informed decisions about their data. At its backend, Privacify leverages a combination of text segmentation, summarization using Large Language Model (LLM), and map-reduce technologies to facilitate BASE analysis for single-document insights and WRT and REV for comprehensive cross-document analysis. Designed with a usercentric approach, Privacify features an intuitive interface that presents all relevant user privacy information in easy-to-understand language, complete with a detailed explainability component. This design not only simplifies privacy policies but also aids users in effortlessly navigating complex privacy terms, significantly boosting their ability to protect and manage their personal information. Our evaluation employs robust methodologies, including reliability and accuracy assessments, alongside rigorous functionality verification through ROUGE metrics and human analysis, validating the system's efficacy and performance. Privacify's architecture promotes scalability, replicability, and seamless deployment, advancing the domain of user data protection through improved privacy comprehension."
Is It Possible to Use ChatGPT to Perform Measurements Using the COSMIC Method?,"ValdÃ©s-Souto, F; Torres-Robledo, D",10.1134/S0361768824700695,2024,"The process of developing software is intricate and time-consuming. Resource estimation is one of the most important responsibilities in software development. Since it is currently the only acceptable metric, the functional size of the program is used to generate estimating models in a widely accepted manner. On the other hand, functional size measurement takes time. The use of artificial intelligence (AI) to automate certain software development jobs has gained popularity in recent years. Software functional sizing and estimation is one area where artificial intelligence may be used. In this study, we investigate how to apply the concepts and guidelines of the COSMIC method to measurements using ChatGPT 4o, a large language model (LLM). To determine whether ChatGPT can perform COSMIC measurements, we discovered that ChatGPT could not reliably produce accurate findings. The primary shortcomings found in ChatGPT include its incapacity to accurately extract data movements, data groups, and functional users from the text. Because of this, ChatGPT's measurements fall short of two essential requirements for measurement: accuracy and reproducibility."
Software Development and Education: Transitioning Towards AI Enhanced Teaching,"Israilidis, J; Chen, WY; Tsakalerou, M",10.1109/EDUCON60312.2024.10578564,2024,"This paper investigates the impact of large language model (LLM) AI tools, such as ChatGPT and Copilot, on software development education, focusing on usability, efficiency, and effectiveness in real-world scenarios. The research employs a quantitative approach, utilizing a survey of 50 software developers with varying levels of experience. Preliminary findings suggest that AI tools have a positive influence on expediting coding tasks and automating text generation, particularly in the early stages of product development. Challenges related to customization, accuracy, and transparency, as well as concerns about their potential impacts on employment, personal privacy, and ethical boundaries, have been identified. Pointers and initial recommendations for transitioning to AI-enhanced teaching and optimizing interactions between learners and generative AI practices are provided."
Integrating Conversational Large Language Models into Student Learning: A Case Study of ChatGPT in Software Engineering Education,"Liu, Y",10.1109/FIE61694.2024.10893046,2024,"This innovative practice full paper describes a pilot study exploring the integration of ChatGPT, a Conversational Large Language Model (LLM), into the student learning process in software engineering education, which emphasizes principles and methodologies in software development. Focused on a software engineering class, the study examines ChatGPT as a tool for problem clarification, modeling assistance, system design feedback, and implementation support in a project on modeling, designing, and implementing a solution using finite state processes and concurrent programming in Java. A survey designed for the case study collects insights into students' experiences with ChatGPT at different stages of the project. Student feedback on using ChatGPT and their performance on the project are analyzed to understand the impact of conversational LLMs on learning outcomes and to address whether there is room for improvement in enhancing the use of conversational LLMs in software engineering education."
Advancing Carbon-Efficient Software Development: A Sustainable Path Forward,"Gupta, S; Gupta, A",10.1145/3700838.3703670,2024,"In today's IT world, where most companies focus on sustainable development, reducing the carbon footprint should be an essential part of the Software Development Lifecycle (SDLC). Companies rely heavily on open-source libraries in enterprise ecosystems, making building an energy-efficient software product complex. The library code may need to be optimised for the specific business use case under development. This brings us to the problem wherein a tool must assess multiple open-source libraries and provide suggestions that best suit one's use case. This paper proposes a PEFT (Parameter Efficient Fine-Tuning) Large Language Model (LLM) to assist developers in selecting libraries that are not only functional but also sustainable and secure. The solution includes creating detailed library profiles based on energy consumption, performance, and typical use cases. This is followed by a comparative analysis using Multi-Criteria Decision Analysis (MCDA) to rank libraries and provide real-time recommendations during the development process. This practical approach ensures that the solution is theoretical and can be implemented in real-world software development scenarios. This solution outlines a comprehensive approach to enhancing the software development lifecycle by integrating energy efficiency and security considerations from the early stages of development. This will reduce potential issues and contribute to the creation of green software."
Icing on the Cake: Automatic Code Summarization at Ericsson,"Sridhara, G; Roychowdhury, S; Soman, S; Ranjani, HG; Britto, R",10.1109/ICSME58944.2024.00073,2024,"This paper presents our findings on the automatic summarization of Java methods within Ericsson, a global telecommunications company. We evaluate the performance of an approach called Automatic Semantic Augmentation of Prompts (ASAP), which uses a Large Language Model (LLM) to generate leading summary comments (Javadocs) for Java methods. ASAP enhances the LLM's prompt context by integrating static program analysis and information retrieval techniques to identify similar exemplar methods along with their developer-written Javadocs, and serves as the baseline in our study. In contrast, we explore and compare the performance of four simpler approaches that do not require static program analysis, information retrieval, or the presence of exemplars as in the ASAP method. Our methods rely solely on the Java method body as input, making them lightweight and more suitable for rapid deployment in commercial software development environments. We conducted experiments on an Ericsson software project and replicated the study using two widely-used open-source Java projects, Guava and Elasticsearch, to ensure the reliability of our results. Performance was measured across eight metrics that capture various aspects of similarity. Notably, one of our simpler approaches performed as well as or better than the ASAP method on both the Ericsson project and the open-source projects. Additionally, we performed an ablation study to examine the impact of method names on Javadoc summary generation across our four proposed approaches and the ASAP method. By masking the method names and observing the generated summaries, we found that our approaches were statistically significantly less influenced by the absence of method names compared to the baseline. This suggests that our methods are more robust to variations in method names and may derive summaries more comprehensively from the method body than the ASAP approach."
WIP: ARTful Insights From A Pilot Study on GPT-based Automatic Code Reviews in Undergraduate Computer Science Programs,"Crandall, AS; Fischer, BJ; Crandall, JL",10.1109/FIE61694.2024.10893407,2024,"This work in progress research paper describes a pilot study using a Large Language Model (LLM) Generative Pre-Trained Transformer-based (GPT) system that generates industry-style code reviews for student feedback on software development projects in Computer Science 2nd, 3rd, and 4th+ semester classes (CS2, CS3, CS4+) at an ABET accredited baccalaureate institution. Code reviews are a valuable, but work-intensive, component of the software engineering process and provide important training to undergraduate students in the form of mentor-peer knowledge transfer. Participants in this study engaged in iterative experiential learning using the Automatic Review Tool (ART), an artificial intelligence tool to support software engineering as an Automatic Static Analysis Tool in the Continuous Integration pipeline alongside software testing harnesses and code style checkers. This pilot study was based on earlier results from a full computer science second semester (CS2) class (n = 74) to develop an ART-generated code review intervention pilot study with a small group of students in CS2 / 3 and CS4. The project underway uses an experiential learning and iterative feedback process to answer research questions including Does ART provide accurate and actionable code reviews for students and Which levels of students are best prepared to receive and use ART-based code reviews? During this pilot study, the project used a mixed methods research approach with a series of surveys, code review interventions, and numerical analysis of the code reviews' accuracy. Results showed a reasonable degree of code review accuracy by ART and the students learned code review skills from interaction with the ART-based reviews they received. Ongoing work includes increasing the scale of data collection, using this work to refine and focus the ART-based reviews onto the categories of feedback that students find the most valuable, and building out a more modular tool for wider release in the academic community."
PR-DupliChecker: detecting duplicate pull requests in Fork-based workflows,"Messaoud, MB; Chekaya, RB; Mkaouer, MW; Jenhani, I; Aljedaani, W",10.1007/s13198-024-02361-4,2024,"Pull requests (PR) are a fundamental aspect of collaborative software development, allowing developers to propose changes to a codebase hosted on platforms like GitHub. They serve as a mechanism for peer review, enabling team members to assess the proposed changes before merging them into the main code repository. Duplicate pull requests occur when multiple contributors submit similar or identical proposed changes to a code repository. Such duplicate pull requests can be problematic because they create redundancy, waste developers' time, and complicate the review process. In this paper, we propose an approach which is based on a pre-trained language model, namely BERT (Bidirectional Encoder Representations from Transformers) to automatically detect duplicate PRs in GitHub repositories. A dataset of 3328 labeled PRs collected from 26 GitHub repositories is built. This data is then fed to a BERT model in order to get the embeddings which represent the contextual relationships between the words used in pairs of pull requests. Then, the BERT's classification outputs are fed to a Multilayer Perceptron (MLP) classifier which represents our final duplicate pull requests detector. Experiments have shown that BERT provided good performance and achieved an accuracy of 92% with MLP classifier. Results have proven that BERT's word representation features achieved an increase of 13% (resp., 17 and 23%) compared to Siamese-BERT model (resp., DC-CNN and Word2Vec) in term of accuracy."
Self-Elicitation of Requirements with Automated GUI Prototyping,"Kolthoff, K; Bartelt, C; Ponzetto, SP; Schneider, K",10.1145/3691620.3695350,2024,"Requirements Elicitation (RE) is a crucial activity especially in the early stages of software development. GUI prototyping has widely been adopted as one of the most effective RE techniques for user-facing software systems. However, GUI prototyping requires (i) the availability of experienced requirements analysts, (ii) typically necessitates conducting multiple joint sessions with customers and (iii) creates considerable manual effort. In this work, we propose SERGUI, a novel approach enabling the Self-Elicitation of Requirements (SER) based on an automated GUI prototyping assistant. SERGUI exploits the vast prototyping knowledge embodied in a large-scale GUI repository through Natural Language Requirements (NLR) based GUI retrieval and facilitates fast feedback through GUI prototypes. The GUI retrieval approach is closely integrated with a Large Language Model (LLM) driving the prompting-based recommendation of GUI features for the current GUI prototyping context and thus stimulating the elicitation of additional requirements. We envision SERGUI to be employed in the initial RE phase, creating an initial GUI prototype specification to be used by the analyst as a means for communicating the requirements. To measure the effectiveness of our approach, we conducted a preliminary evaluation. Video presentation of SERGUI at: https://youtu.be/pzAAB9Uht80"
CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model,"Di, P; Li, JG; Yu, H; Jiang, W; Cai, WT; Cao, Y; Chen, CY; Chen, DJ; Chen, HW; Chen, L; Fan, G; Gong, J; Gong, Z; Hu, W; Guo, TT; Lei, ZC; Li, T; Li, Z; Liang, M; Liao, C; Liu, BC; Liu, JC; Liu, ZW; Lu, SJ; Shen, M; Wang, GP; Wang, H; Wang, Z; Xu, ZG; Yang, JW; Ye, Q; Zhang, GH; Zhang, Y; Zhao, ZL; Zheng, XJ; Zhou, HL; Zhu, LF; Zhu, XY",10.1145/3639477.3639719,2024,"Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CODEFUSE-13B, an open-sourced pre-trained code LLM2. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CODEFUSE achieves its effectiveness by utilizing a highquality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HUMANEVAL-X, and the specially designed CODEFUSEEVAL for Chinese prompts. To assess the effectiveness of CODEFUSE, we actively collected valuable human feedback from the AntGroup's software development process where CODEFUSE has been successfully deployed. The results demonstrate that CODEFUSE-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CODEFUSE performs better than other models when confronted with Chinese prompts."
The Use of Large Language Model in Code Review Automation: An Examination of Enforcing SOLID Principles,"Martins, GF; Firmino, ECM; De Mello, VP",10.1007/978-3-031-60615-1_6,2024,"Within the ever-evolving domain of software development, the practice of having teams located in different geographical locations presents distinct obstacles, including disparate time zones, language hurdles, and differing degrees of experience. This paper presents a novel approach to address these difficulties by using an automated GitHub bot that utilizes Large Language Models (LLMs) to enforce SOLID principles during code reviews. This bot, which incorporates advanced models such as OpenAI's GPT-4 and the locally deployable Mixtral, has the objective of delivering immediate and practical feedback. Its purpose is to improve the quality of code and make learning easier for developers, particularly those who are new to programming. The bot's structure enables effortless incorporation into GitHub, utilizing LLMs to examine code modifications and offer observations regarding adherence to SOLID principles. An important characteristic of this method is the incorporation of Mixtral, which may be operated on-site, providing advantages in terms of data confidentiality and operational adaptability, essential for global enterprises with strict privacy demands. Here, we explores the bot's architecture, its incorporation with LLMs, and its capacity to revolutionize code reviews by offering a secure, efficient, and instructive instrument for geographically dispersed software development teams."
Applying Large Language Model to User Experience Testing,"Hsueh, NL; Lin, HJ; Lai, LC",10.3390/electronics13234633,2024,"The maturation of internet usage environments has elevated User Experience (UX) to a critical factor in system success. However, traditional manual UX testing methods are hampered by subjectivity and lack of standardization, resulting in time-consuming and costly processes. This study explores the potential of Large Language Models (LLMs) to address these challenges by developing an automated UX testing tool. Our innovative approach integrates the Rapi web recording tool to capture user interaction data with the analytical capabilities of LLMs, utilizing Nielsen's usability heuristics as evaluation criteria. This methodology aims to significantly reduce the initial costs associated with UX testing while maintaining assessment quality. To validate the tool's efficacy, we conducted a case study featuring a tennis-themed course reservation system. The system incorporated multiple scenarios per page, allowing users to perform tasks based on predefined goals. We employed our automated UX testing tool to evaluate screenshots and interaction logs from user sessions. Concurrently, we invited participants to test the system and complete UX questionnaires based on their experiences. Comparative analysis revealed that varying prompts in the automated UX testing tool yielded different outcomes, particularly in detecting interface elements. Notably, our tool demonstrated superior capability in identifying issues aligned with Nielsen's usability principles compared to participant evaluations. This research contributes to the field of UX evaluation by leveraging advanced language models and established usability heuristics. Our findings suggest that LLM-based automated UX testing tools can offer more consistent and comprehensive assessments."
Understanding the Performance of Large Language Model to Generate SQL Queries,"Ko, M; Bose, DB; Wang, WL; Seyam, M; Brown, C",10.1109/VL/HCC60511.2024.00048,2024,"Recent developments in Artificial Intelligence (AI) have shifted the software development paradigm. Past studies demonstrated how effective AI can generate code for programming purposes. However, to our knowledge, no prior study has been done to evaluate the effectiveness of SQL queries generated by AI. We utilized nine AI assistants to generate SQL queries. Our results reveal that most AI assistants generate inaccurate SQL queries, and based on the results, we provide possible implications for SQL developers."
Towards Understanding Contracts Grammar: A Large Language Model-based Extractive Question-Answering Approach,"Rejithkumar, G; Anish, PR; Ghaisas, S",10.1109/RE59067.2024.00037,2024,"Software Engineering (SE) contracts play a pivotal role in Information Technology Outsourcing (ITO) projects. The obligations in SE contracts are known to be a useful source for deriving software requirements, thereby contributing to the overall Software Development Life Cycle (SDLC). Making sense of contractual obligations is an important first step in successfully executing software projects. This includes building compliant systems, meeting delivery deadlines, avoiding heavy penalties, and steering clear of expensive litigations. In this work, we present an approach to capture the essence of a contractual clause by extracting its Contracts Grammar. Through an exploratory study, we first identify the constituents of Contracts Grammar. Subsequently, we experiment with multiple approaches for the automated extraction of these constituents, including extractive question-answering, token classification, text-to-text generation, prompting, and regular expressions. The question-answering based approach performed the best in terms of high average ROUGE-L score of 0.81, and faster inference times. The work presented in this paper is a part of the Contracts Governance System (CGS) and is in the process of deployment within a large IT vendor organization."
MONOCODER: Domain-Specific Code Language Model for HPC Codes and Tasks,"Kadosh, T; Hasabnis, N; Vo, VA; Schneider, N; Krien, N; Capota, M; Wasay, A; Tamir, G; Willke, T; Ahmed, N; Pinter, Y; Mattson, T; Oren, G",10.1109/HPEC62836.2024.10938441,2024,"With easier access to powerful compute resources, there is a growing trend in AI for software development to develop large language models (LLMs) to address a variety of programming tasks. Even LLMs applied to tasks from the high-performance computing (HPC) domain are huge in size and demand expensive compute resources for training. This is partly because LLMs for HPC tasks are obtained by finetuning existing LLMs that support several natural and/or programming languages. We found this design choice confusing - why do we need LLMs trained on natural languages and programming languages unrelated to HPC for HPC-specific tasks? In this line of work, we aim to question choices made by existing LLMs by developing smaller language models (LMs) for specific domains - we call them domain-specific LMs. Specifically, we start with HPC as a domain and build an HPC-specific LM, named MONOCODER, which is orders of magnitude smaller than existing LMs but delivers better performance on non-HPC and HPC codes. Specifically, we pre-trained MONOCODER on an HPC-specific dataset (named HPCORPUS) of C and C++ programs mined from GitHub. We evaluated the performance of MONOCODER against state-of-the-art multi-lingual LLMs. Results demonstrate that MONOCODER, although much smaller than existing LMs, outperforms other LLMs on normalized-perplexity tests (in relation to model size) while also delivering competing CodeBLEU scores for high-performance and parallel code generations. In other words, results suggest that MONOCODER understands HPC code better than state-of-the-art LLMs. MONOCODER source code is available at our GitHub repository."
Patient-Friendly Discharge Summaries in Korea Based on ChatGPT: Software Development and Validation,"Kim, H; Jin, HM; Bin Jung, Y; You, SC",10.3346/jkms.2024.39.e148,2024,"Background: Although discharge summaries in patient -friendly language can enhance patient comprehension and satisfaction, they can also increase medical staff workload. Using a large language model, we developed and validated software that generates a patient -friendly discharge summary. Methods: We developed and tested the software using 100 discharge summary documents, 50 for patients with myocardial infarction and 50 for patients treated in the Department of General Surgery. For each document, three new summaries were generated using three different prompting methods (Zero -shot, One-shot, and Few -shot) and graded using a 5 -point Likert Scale regarding factuality, comprehensiveness, usability, ease, and fluency. We compared the effects of different prompting methods and assessed the relationship between input length and output quality. Results: The mean overall scores differed across prompting methods (4.19 +/- 0.36 in Few -shot, 4.11 +/- 0.36 in One-shot, and 3.73 +/- 0.44 in Zero -shot; P < 0.001). Post -hoc analysis indicated that the scores were higher with Few -shot and One-shot prompts than in zero -shot prompts, whereas there was no significant difference between Few -shot and One-shot prompts. The overall proportion of outputs that scored >= 4 was 77.0% (95% confidence interval: 68.8-85.3%), 70.0% (95% confidence interval [CI], 61.0-79.0%), and 32.0% (95% CI, 22.9-41.1%) with Few -shot, One-shot, and Zero -shot prompts, respectively. The mean factuality score was 4.19 +/- 0.60 with Few -shot, 4.20 +/- 0.55 with One-shot, and 3.82 +/- 0.57 with Zero -shot prompts. Input length and the overall score showed negative correlations in the Zero -shot ( r = -0.437, P < 0.001) and One-shot ( r = -0.327, P < 0.001) tests but not in the Few -shot ( r = -0.050, P = 0.625) tests. Conclusion: Large -language models utilizing Few -shot prompts generally produce acceptable discharge summaries without significant misinformation. Our research highlights the potential of such models in creating patient -friendly discharge summaries for Korean patients to support patient -centered care."
CIPHER: Cybersecurity Intelligent Penetration-Testing Helper for Ethical Researcher,"Pratama, D; Suryanto, N; Adiputra, AA; Le, TTH; Kadiptya, AY; Iqbal, M; Kim, H",10.3390/s24216878,2024,"Penetration testing, a critical component of cybersecurity, typically requires extensive time and effort to find vulnerabilities. Beginners in this field often benefit from collaborative approaches with the community or experts. To address this, we develop Cybersecurity Intelligent Penetration-testing Helper for Ethical Researchers (CIPHER), a large language model specifically trained to assist in penetration testing tasks as a chatbot. Unlike software development, penetration testing involves domain-specific knowledge that is not widely documented or easily accessible, necessitating a specialized training approach for AI language models. CIPHER was trained using over 300 high-quality write-ups of vulnerable machines, hacking techniques, and documentation of open-source penetration testing tools augmented in an expert response structure. Additionally, we introduced the Findings, Action, Reasoning, and Results (FARR) Flow augmentation, a novel method to augment penetration testing write-ups to establish a fully automated pentesting simulation benchmark tailored for large language models. This approach fills a significant gap in traditional cybersecurity Q&A benchmarks and provides a realistic and rigorous standard for evaluating LLM's technical knowledge, reasoning capabilities, and practical utility in dynamic penetration testing scenarios. In our assessments, CIPHER achieved the best overall performance in providing accurate suggestion responses compared to other open-source penetration testing models of similar size and even larger state-of-the-art models like Llama 3 70B and Qwen1.5 72B Chat, particularly on insane difficulty machine setups. This demonstrates that the current capabilities of general large language models (LLMs) are insufficient for effectively guiding users through the penetration testing process. We also discuss the potential for improvement through scaling and the development of better benchmarks using FARR Flow augmentation results."
Can Large Language Models Write Parallel Code?,"Nichols, D; Davis, JH; Xie, ZJ; Rajaram, A; Bhatele, A",10.1145/3625549.3658689,2024,"Large language models are increasingly becoming a popular tool for software development. Their ability to model and generate source code has been demonstrated in a variety of contexts, including code completion, summarization, translation, and lookup. However, they often struggle to generate code for complex programs. In this paper, we study the capabilities of state-of-the-art language models to generate parallel code. In order to evaluate language models, we create a benchmark, PAREVAL, consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing. We use PAREVAL to evaluate the effectiveness of several state-of-the-art open- and closed-source language models on these tasks. We introduce novel metrics for evaluating the performance of generated code, and use them to explore how well each large language model performs for 12 different computational problem types and six different parallel programming models."
Guiding ChatGPT for Better Code Generation: An Empirical Study,"Liu, C; Bao, XL; Zhang, HY; Zhang, N; Hu, HB; Zhang, XH; Yan, M",10.1109/SANER60148.2024.00018,2024,"Automated code generation is a powerful technique for software development, which can significantly reduce developers' effort and time for writing code. Recently, OpenAI's large language model ChatGPT has emerged as a powerful tool for generating human-like responses to a wide range of textual inputs (i.e., prompts), including those related to code generation. However, the effectiveness of ChatGPT in code generation is still not well understood. The code generation performance could also be heavily influenced by the choice of prompts, which should be further explored. In this paper, we report an empirical study on ChatGPT's capabilities for two types of code generation tasks, namely text-to-code and code-to-code generation. We investigate different types of prompts by leveraging the chain-of-thought strategy with multi-step optimizations. Our empirical results show that by carefully designing prompts to guide ChatGPT, the code generation performance can be improved substantially. We also analyze the factors that influence the prompt design and provide insights that could guide future research."
Enhancing DevSecOps practice with Large Language Models and Security Chaos Engineering,"Bedoya, M; Palacios, S; DÃ­az-LÃ³pez, D; Laverde, E; Nespoli, P",10.1007/s10207-024-00909-w,2024,"Recently, the DevSecOps practice has improved companies' agile production of secure software, reducing problems and improving return on investment. However, overreliance on security tools and traditional security techniques can facilitate the implementation of vulnerabilities in different stages of the software lifecycle.. Thus, this paper proposes the integration of a Large Language Model to help automate threat discovery at the design stage and Security Chaos Engineering to support the identification of security flaws that may be undetected by security tools. A specific use case is described to demonstrate how our proposal can be applied to a retail company that has the business need to produce rapidly secure software."
Feedback-Driven Automated Whole Bug Report Reproduction for Android Apps,"Wang, DB; Zhao, Y; Feng, SD; Zhang, ZX; Halfond, WGJ; Chen, CY; Sun, XX; Shi, JF; Yu, TT",10.1145/3650212.3680341,2024,"In software development, bug report reproduction is a challenging task. This paper introduces ReBL, a novel feedback-driven approach that leverages GPT-4, a large-scale language model (LLM), to automatically reproduce Android bug reports. Unlike traditional methods, ReBL bypasses the use of Step to Reproduce (S2R) entities. Instead, it leverages the entire textual bug report and employs innovative prompts to enhance GPT's contextual reasoning. This approach is more flexible and context-aware than the traditional step-by-step entity matching approach, resulting in improved accuracy and effectiveness. In addition to handling crash reports, ReBL has the capability of handling non-crash functional bug reports. Our evaluation of 96 Android bug reports (73 crash and 23 non-crash) demonstrates that ReBL successfully reproduced 90.63% of these reports, averaging only 74.98 seconds per bug report. Additionally, ReBL outperformed three existing tools in both success rate and speed."
Neuro-Symbolic Approach to Certified Scientific Software Synthesis,"Bagheri, H; Mirakhorli, M; Fazelnia, M; Mujhid, I; Hasan, MR",10.1145/3664646.3664776,2024,"Scientific software development demands robust solutions to meet the complexities of modern scientific systems. In response, we propose a paradigm-shifting Neuro-Symbolic Approach to Certified Scientific Software Synthesis. This innovative framework integrates large language models (LLMs) with formal methods, facilitating automated synthesis of complex scientific software while ensuring verifiability and correctness. Through a combination of technologies including a Scientific and Satisfiability-Aided Large Language Model (SaSLLM), a Scientific Domain Specific Language (DSL), and Generalized Planning for Abstract Reasoning, our approach transforms scientific concepts into certified software solutions. By leveraging advanced reasoning techniques, our framework streamlines the development process, allowing scientists to focus on design and exploration. This approach represents a significant step towards automated, certified-by-design scientific software synthesis, revolutionizing the landscape of scientific research and discovery."
Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback,"Liu, ZF; Su, J; Cai, J; Yang, JZ; Wu, CF",10.1007/978-981-97-5669-8_11,2024,"General large language models (LLMs), represented by ChatGPT, have demonstrated an impressive ability in software development tasks such as code completion, comment generation, and unit test generation. However, when evaluated on competition level programming problems, these models still underperform. We observed that conventional models prefer to convert problem descriptions directly into codes, which results in the inability to handle some edge cases. We think this problem occurs because the training data contains incorrect code that only passes some of the test cases, but not all. To improve the capabilities of language models in competition level code generation, we introduced a novel fine-tuning approach called Reinforcement Learning with Online Judging Feedback (RLOJF) to align the generated code with the correct code. Then we collect a dataset including online judge feedback from Codefroces and base on this dataset train a new model called Instruct-Code-LLaMA, under the same conditions, the generated code exhibits a 0.29% increase in average accuracy on the APPs dataset."
SPMDSL Language Model Onto a DSL for Agile Use Case Driven Software Project's Management,"Ribeiro, GGG; Barros, AMA; da Cruz, AMR",10.5220/0006326804030410,2017,"Project management involves applying knowledge, skills, tools and techniques to project activities to meet the project requirements. Each project's unique nature implies tailoring that knowledge, skills, tools and techniques to adapt the management activities to cope with project constraints. Management and technical activities meet at some points, namely on activities that have technical and management relevance. This paper proposes SPMDSL and presents its language model and the domain analysis made during its development. SPMDSL aims to be a DSL defining a set of representational primitives with which to model projects in the domain of agile software project management. These primitives are represented as classes and their interrelationships. The proposed DSL focuses on agile use case driven software development project management, and so it also integrates concepts from software modeling. The goal is to enable representing past projects' information to facilitate retrieving information for lessons learned analysis."
Will I be replaced? Assessing ChatGPT's effect on software development and programmer perceptions of AI tools,"Kuhail, MA; Mathew, SS; Khalil, A; Berengueres, J; Shah, SJH",10.1016/j.scico.2024.103111,2024,"ChatGPT is a language model with artificial intelligence (AI) capabilities that has found utility across various sectors. Given its impact, we conducted two empirical studies to assess the potential and limitations of ChatGPT and other AI tools in software development. In the first study, we evaluated ChatGPT 3.5 ' s effectiveness in generating code for 180 coding problems from LeetCode, an online coding interview preparation platform. Our findings suggest that ChatGPT 3.5 is more effective in solving easy and medium coding problems but less reliable for harder problems. Further, ChatGPT 3.5 is somewhat more effective at coding problems with higher popularity scores. In the second study, we administered a questionnaire (N = 99) to programmers to gain insights into their views on ChatGPT and other AI tools. Our findings indicate that programmers use AI tools for various tasks, such as generating boilerplate code, explaining complex code, and conducting research. AI tools also help programmers to become more productive by creating better-performing, shorter, and more readable code, among other benefits. However, AI tools can sometimes misunderstand requirements and generate erroneous code. While most programmers are not currently concerned about AI tools replacing them, they are apprehensive about what the future may hold. Our research has also revealed associations between AI tool usage, trust, perceived productivity, and job security threats caused by the tools."
DP-CCL: A Supervised Contrastive Learning Approach Using CodeBERT Model in Software Defect Prediction,"Sahar, S; Younas, M; Khan, MM; Sarwar, MU",10.1109/ACCESS.2024.3362896,2024,"Software Defect Prediction (SDP) reduces the overall cost of software development by identifying the code at a higher risk of defects at the initial phase of software development. SDP helps the test engineers to optimize the allocation of testing resources more effectively. Traditional SDP models are built using handcrafted software metrics that ignore the structural, semantic, and contextual information of the code. Consequently, many researchers have employed deep learning models to capture contextual, semantic, and structural information from the code. In this article, we propose the DP-CCL (Defect Prediction using CodeBERT with Contrastive Learning) model to predict the defective code. The proposed model employs supervised contrastive learning using this CodeBERT Language model to capture semantic features from the source code. Contrastive learning extracts valuable information from the data by maximizing the similarity between similar data pairs (positive pair) and meanwhile minimizing the similarity between dissimilar data pairs (negative pair). Moreover, The model combines the semantic features with software metrics to obtain the benefits of both semantic and handcrafted features. The combined features are input to the logistic regression model for code classification as either buggy or clean. In this study, ten PROMISE projects were utilized to conduct the experiments. Results show that the DP-CCL model achieved significant improvement i.e., 4.9% to 14.9% increase in F-Score as compared to existing approaches."
Leveraging pre-trained language models for code generation,"Soliman, A; Shaheen, S; Hadhoud, M",10.1007/s40747-024-01373-8,2024,"Code assistance refers to the utilization of various tools, techniques, and models to help developers in the process of software development. As coding tasks become increasingly complex, code assistant plays a pivotal role in enhancing developer productivity, reducing errors, and facilitating a more efficient coding workflow. This assistance can manifest in various forms, including code autocompletion, error detection and correction, code generation, documentation support, and context-aware suggestions. Language models have emerged as integral components of code assistance, offering developers the capability to receive intelligent suggestions, generate code snippets, and enhance overall coding proficiency. In this paper, we propose new hybrid models for code generation by leveraging pre-trained language models BERT, RoBERTa, ELECTRA, and LUKE with the Marian Causal Language Model. Selecting these models based on their strong performance in various natural language processing tasks. We evaluate the performance of these models on two datasets CoNaLa and DJANGO and compare them to existing state-of-the-art models. We aim to investigate the potential of pre-trained transformer language models to revolutionize code generation, offering improved precision and efficiency in navigating complex coding scenarios. Additionally, conducting error analysis and refining the generated code. Our results show that these models, when combined with the Marian Decoder, significantly improve code generation accuracy and efficiency. Notably, the RoBERTaMarian model achieved a maximum BLEU score of 35.74 and an exact match accuracy of 13.8% on CoNaLa, while LUKE-Marian attained a BLEU score of 89.34 and an exact match accuracy of 78.50% on DJANGO. Implementation of this work is available at https://github.com/AhmedSSoliman/Leveraging-Pretrained-Language-Models-for-Code-Generation."
Java Web Programming with ChatGPT,"Guo, MH",10.1109/ICMTIM62047.2024.10629560,2024,"With the rapid development of artificial intelligence (AI), ChatGPT is an important technical breakthrough in the field of natural language processing (NLP). ChatGPT, as a large-scale language model, focuses on the task of conversation generation and has attracted wide attention in academia and industry. Meanwhile, it brings opportunities and challenges to Java Web programming. Compared with previous studies that investigated the ability of ChatGPT to solve small-scale Java programming problems that emphasized basic concepts, this paper explores how well ChatGPT can produce sequential Java code to construct the complete Web project. A series of experiments based on the user-login study case are conducted to evaluate the performance of ChatGPT-generated code. Through thorough experimental analysis, the generated code is characterized by high readability, good quality, and complete functionality. However, ChatGPT's performance degrades when instructions contain limited text information. In conclusion, ChatGPT has the potential to be an important assistant for developers, greatly improving efficiency and productivity in the software development process."
Enhancing the Resiliency of Automated Web Tests with Natural Language,"Ayli, M; Bakouny, Y; Jalloul, N; Kilany, R",10.1145/3700523.3700536,2024,"Web application testing has traditionally been the domain of specialized software professionals, with a significant portion still relying on manual execution by individuals with limited programming expertise. This research introduces a novel proof-of-concept tool that democratizes the creation of automated web tests through a restricted natural language interface. Leveraging the GPT-4 language model and introducing smart web element locators, our tool enables both technical and non-technical professionals to design comprehensive test cases without explicit programming knowledge. The tool comprises three key components: (1) a pseudo-language definition mapping into Selenium actions, (2) a new concept for resilient locators, and (3) a semantic understanding system translating natural language into software assertions. This approach enhances test production speed by offering a no-code interface and improves test resiliency through non-fragile locators. While introducing a slight increase in execution time (approximately 15% on average), the benefits in creation speed and script resilience far outweigh this trade-off. Empirical evaluation demonstrates the tool's effectiveness in real-world scenarios, enabling non-technical team members to contribute meaningfully to the testing process. Results show a significant reduction in test suite creation and maintenance time, as well as improved test coverage due to increased participation of domain experts. This research contributes to software testing by combining natural language processing, smart locators, and automated test generation. By lowering the barrier to entry for test automation, our tool has the potential to revolutionize web application testing practices, leading to more efficient, comprehensive, and reliable testing processes across the software development industry."
Benchmarking Large Language Models for Ethereum Smart Contract Development,"Daspe, E; Durand, M; Hahn, J; Bradai, S",10.1109/BRAINS63024.2024.10732686,2024,"The integration of blockchain technology, particularly Ethereum and its smart contract, has revolutionized software programming. Solidity, Ethereum's main language, is crucial due to its features for blockchain applications. However, the immutable nature of Smart Contract (SC) presents significant security issues, with vulnerabilities leading to financial risks. Meanwhile, Large language models (LLMS) have transformed software development by enhancing coding efficiency and error detection. Despite their potential, current benchmarks often overlook niche languages like Solidity. This paper introduces the first benchmark to evaluate LLMs in Solidity smart contract generation, aiming to improve automated SC development and blockchain deployment reliability using a Test-Driven Development inspired methodology and pass@k metric. This work not only addresses a significant gap in LLM evaluation for blockchain applications but also extends the capabilities of LLMs in this specialized and critical area of software development."
Integrating ChatGPT in Project Management Education: Benefits and Challenges in the Academic Environment,"Oran, AC; Montenegro, LB; Schuster, HA; Duarte, JC; Silva, W; Lima, RR",10.1145/3701625.3701684,2024,"CONTEXT: Teaching project management is complex, and students often do not feel engaged or motivated. Professors can use many initiatives to improve the teaching and learning process. Tools like ChatGPT, when integrated into education, have generated considerable interest due to their potential to enrich students' learning experiences. GOAL: This paper analyzes the impacts of using ChatGPT as a complementary tool in teaching Project Management in the Software Engineering course, highlighting its benefits and challenges. METHOD: We performed an exploratory study to identify the effects of using ChatGPT in teaching project management, evaluating learning, productivity, teamwork, student perceptions, and future expectations. RESULTS: The results indicate that ChatGPT contributed to improving content comprehension, developing critical skills, accelerating production, improving collaboration and communication, and increasing student engagement. However, challenges related to misuse and dependence on the tool were also identified. CONCLUSION: The integration of ChatGPT in teaching project management has shown promise, promoting a richer and more collaborative learning experience. The insights obtained provide directions for future implementations and research on the use of AI in project management education."
Evaluating Human-AI Partnership for LLM-based Code Migration,"Omidvar-Tehrani, B; Ishaani, M; Anubhai, A",10.1145/3613905.3650896,2024,"The potential of Generative AI, especially Large Language Models (LLMs), to transform software development is remarkable. In this paper, we focus on one area in software development called code migration. We define code migration as the process of transitioning the language version of a code repository by converting both the source code and its dependencies. Carefully designing an effective human-AI partnership is essential for boosting developer productivity and faster migrations when performing code migrations. Though human-AI partnerships have been generally explored in the literature, their application to code migrations remains largely unexamined. In this work, we leverage an LLM-based code migration tool called Amazon Q Code Transformation to conduct semi-structured interviews with 11 participants undertaking code migrations. We discuss human's role in the human-AI partnership ( human as a director and a reviewer) and define a trust framework based on various model outcomes to earn trust with LLMs. The guidelines presented in this paper offer a vital starting point for designing human-AI partnerships that effectively augment and complement human capabilities in software development with Generative AI."
LLM-Based Chatbots for Mining Software Repositories: Challenges and Opportunities,"Abedu, S; Abdellatif, A; Shihab, E",10.1145/3661167.3661218,2024,"Software repositories have a plethora of information about software development, encompassing details such as code contributions, bug reports and code reviews. This rich source of data can be harnessed to enhance not only software quality and development velocity but also to gain insights into team collaboration and inform strategic decision-making throughout the software development lifecycle. Previous studies show that many stakeholders cannot benefit from the project information due to the technical knowledge and expertise required to extract the project data. To lower the barrier to entry by automating the process of extracting and analyzing repository data, we explored the potential of using an LLM to develop a chatbot for answering questions related to software repositories. We evaluated the chatbot on 150 software repository-related questions. We found that the chatbot correctly answered one question. This result prompted us to shift our focus to investigate the challenges in adopting LLMs for the out-of-thebox development of software repository chatbots. We identified five main challenges related to retrieving data, structuring the data, and generating the answer to the user's query. Among these challenges, the most frequent (83.3%) is the inaccurate retrieval of data to answer questions. In this paper, we share our experience and challenges in developing an LLM-based chatbot to answer software repository-related questions within the SE community. We also provide recommendations on mitigating these challenges. Our findings will serve as a foundation to drive future research aimed at enhancing LLMs for adoption in extracting useful information from software repositories, fostering advancements in natural language understanding, data retrieval, and response generation within the context of software repository-related questions and analytics."
LLM-Based Agents for Automating the Enhancement of User Story Quality: An Early Report,"Zhang, ZY; Rayhan, M; Herda, T; Goisauf, M; Abrahamsson, P",10.1007/978-3-031-61154-4_8,2024,"In agile software development, maintaining high-quality user stories is crucial, but also challenging. This study explores the application of large language models (LLMs) to improve the quality of user stories within the agile teams of Austrian Post Group IT. We developed an Autonomous LLM-based Agent System (ALAS) and evaluated its impact on user story quality with 11 participants from six agile teams. Our findings reveal the potential of LLMs in improving user story quality, provide a practical example, and lay the foundation for future research into the broad application of LLMs in a variety of industry settings."
Characterizing Developers' Behaviors in LLM-Supported Software Development,"Wang, W; Ning, HL; Qian, S; Zhang, GW; Wang, Y",10.1109/COMPSAC61105.2024.00156,2024,"The emergence of large language models (LLMs) represented by ChatGPT has profoundly influenced the conventional software development process. Nevertheless, there is little research on how developers interact with LLMs. To investigate the interaction between developers and LLMs during software development, we conducted a user study with 56 participants, who were randomly assigned into two groups to perform different types of software development tasks. The first task was solving two simple coding puzzles, and the second was to fix two real-world bugs from a small-scale open source projects. We captured the full screen histories of all participants to construct a Markov activity transition model, depicting transitions among prevalent development activities such as coding, writing prompt and debugging. By characterizing developer behavior of the two groups of participants in the task, our study contributes empirical insights into developer behavior while interacting with LLMs. Such knowledge could guide the development of LLM-specific capabilities in supporting software development tasks, and offer valuable insights for developers aiming to utilize LLMs for more efficient problem-solving in their development practices."
Demystifying RCE Vulnerabilities in LLM-Integrated Apps,"Liu, T; Deng, ZZ; Meng, GZ; Li, YK; Chen, K",10.1145/3658644.3690338,2024,"Large Language Models (LLMs) show promise in transforming software development, with a growing interest in integrating them into more intelligent apps. Frameworks like LangChain aid LLM-integrated app development, offering code execution utility/APIs for custom actions. However, these capabilities theoretically introduce Remote Code Execution (RCE) vulnerabilities, enabling remote code execution through prompt injections. No prior research systematically investigates these frameworks' RCE vulnerabilities or their impact on applications and exploitation consequences. Therefore, there is a huge research gap in this field. In this study, we propose LLMSMITH to detect, validate and exploit the RCE vulnerabilities in LLM-integrated frameworks and apps. To achieve this goal, we develop two novel techniques, including 1) a lightweight static analysis to construct call chains to identify RCE vulnerabilities in frameworks; 2) a systematical prompt-based exploitation method to verify and exploit the found vulnerabilities in LLM-integrated apps. This technique involves various strategies to control LLM outputs, trigger RCE vulnerabilities and launch subsequent attacks. Our research has uncovered a total of 20 vulnerabilities in 11 LLM-integrated frameworks, comprising 19 RCE vulnerabilities and 1 arbitrary file read/write vulnerability. Of these, 17 have been confirmed by the framework developers, with 13 vulnerabilities being assigned CVE IDs, 6 of which have a CVSS score of 9.8, and we were also awarded a bug bounty of $1350. For the 51 apps potentially affected by RCE, we successfully executed attacks on 17 apps, 16 of which are vulnerable to RCE and 1 to SQL injection. Furthermore, we conduct a comprehensive analysis of these vulnerabilities and construct practical attacks to demonstrate the hazards in reality, e.g., app output hijacking, user data leakage, even the potential to take full control of systems. Last, we propose several mitigation measures for both framework and app developers to counteract such attacks."
Test-Driven Development and LLM-based Code Generation,"Mathews, NS; Nagappan, M",10.1145/3691620.3695527,2024,"Recent Large Language Models (LLMs) have demonstrated significant capabilities in generating code snippets directly from problem statements. This increasingly automated process mirrors traditional human-led software development, where code is often written in response to a requirement. Historically, Test-Driven Development (TDD) has proven its merit, requiring developers to write tests before the functional code, ensuring alignment with the initial problem statements. Applying TDD principles to LLM-based code generation offers one distinct benefit: it enables developers to verify the correctness of generated code against predefined tests. This paper investigates if and how TDD can be incorporated into AI-assisted code-generation processes. We experimentally evaluate our hypothesis that providing LLMs like GPT-4 and Llama 3 with tests in addition to the problem statements enhances code generation outcomes. We experimented with established function-level code generation benchmarks such as MBPP and HumanEval. Our results consistently demonstrate that including test cases leads to higher success in solving programming challenges. We assert that TDD is a promising paradigm for helping ensure that the code generated by LLMs effectively captures the requirements."
Exploring LLM Tools through the Eyes of Industry Experts and Novice Programmers,"Tona, C; JuÃ¡rez-RamÃ­rez, R; JimÃ©nez, S; DurÃ¡n, M",10.1109/CONISOFT63288.2024.00048,2024,"At present, Large Language Models (LLM) and Generative AI models have emerged and impacted industry and society. LLMs are Artificial intelligence (AI) systems designed to understand and generate human language. The rise in popularity of LLM-based systems has motivated research into their use in education, including code generation tools, automated feedback systems, and support for student software projects. The release of ChatGPT (TM) marked a significant milestone, providing an accessible tool for IA interaction. ChatGPT (TM) has gained popularity among students, not only in software areas. This study analyzes the perspectives of software engineering students and software engineers on using LLM tools such as ChatGPT (TM) for software development projects. In this study, we use a questionnaire to analyze different viewpoints and graphics to show the experiment results between these groups. The findings of this study are expected to provide valuable contributions to the understanding of how LLM tools are perceived in the context of software development and their potential implications for educational practices and industry standards."
Smart Product Backlog: Automatic Classification of User Stories Using Large Language Models (LLM),"Gaona-Cuevas, M; Bucheli-Guerrero, V; Vera-Rivera, F",10.19503/01211129.v33.n69.2024.18076,2024,"In agile software development processes, specifically within intelligent applications that leverage artificial intelligence (AI), Smart Product Backlog (SPB) serves as an artifact that includes both AI-implementable functionalities and those that do not use AI. Significant work has been done in the development of Natural Language Processing (NLP) models, and Large Language Models (LLMs) have demonstrated exceptional performance. However, whether LLMs can be used in automatic classification tasks without prior annotation, thereby allowing direct extraction from the Smart Product Backlog (SPB) remains an unanswered question. In this study, we compared the effectiveness of fine-tuning techniques with prompting methods to determine the potential of models such as ChatGPT-4o, Gemini Pro 1.5, and ChaGPT-Mini. A dataset was constructed with user stories manually classified by a group of experts, which enabled assembling experiments and creating the respective contingency tables. The classification performance metrics of each LLM were statistically evaluated; accuracy, sensitivity, and F1-Score were used to assess the effectiveness of each model. This comparative approach aimed to highlight the strengths and limitations of each LLM in efficiently and accurately assisting in the construction of the SPB. This comparative analysis demonstrates that ChatGPT-Mini has limitations in balancing precision and sensitivity. Although Gemini Pro 1.5 was superior in accuracy scores and ChatGPT performed well, neither is robust enough to build a fully automated tool for user story classification. Therefore, we identified the need to develop a specialized classifier that enables the construction of an automated tool to recommend viable user stories for AI development, thereby supporting decision-making in agile software projects."
LLM-Enhanced Test Case Prioritization for Complex Software Systems,"Zosimadis, I; Stamelos, I",10.1145/3716554.3716561,2024,"This paper presents a novel approach to test case prioritization using Large Language Models (LLMs) for complex software systems. Traditional prioritization methods often struggle with the dynamic nature of modern software development and the large amounts of unstructured data generated during the software lifecycle. Our method leverages LLMs to analyze diverse data sources, including code changes, user feedback, and system documentation, creating a more adaptive and context-aware prioritization strategy. We applied our approach to an Internet of Things (IoT) based system for motion tracking in ten-pin bowling. The experimental results show significant improvements over a baseline Additional Statement Coverage method. Our LLM-enhanced approach achieved a 12.12% higher Average Percentage of Faults Detected (APFD) score and reduced test suite execution time by 26 %. These findings demonstrate the potential of LLMs to enhance software testing practices, particularly in early fault detection and efficient resource utilization. The paper discusses implementation details, evaluation metrics, and future directions for integrating this approach into continuous integration and deployment pipelines."
LLM4VV: Exploring LLM-as-a-Judge for Validation and Verification Testsuites,"Sollenberger, Z; Patel, J; Munley, C; Jarmusch, A; Chandrasekaran, S",10.1109/SCW63240.2024.00238,2024,"Large Language Models (LLM) continue to improve and are revolutionizing the landscape of software development. These large models have demonstrated potential to generate, debug, test, analyze, document, and even translate code. Thus they are a valuable tool in the software development cycle. If used correctly, such tools can often accelerate the development cycle. Though the tools are powerful and new, the community is cautious of training using biased or sensitive data, which can lead to biased, dangerous, or incorrect outputs along with the inadvertent release of confidential information. Additionally, the carbon footprints and the un-explainability of these black box models continue to raise questions about the reliability of LLMs. With these opportunities and these challenges ahead, this paper explores the idea of judging LLM-generated code to better understand and open up the un-explainable black box models used by LLMs. We probe into the black box of one such LLM that has generated the best compiler tests for the directive-based programming models OpenMP and OpenACC in our earlier research. We challenge DeepSeek's deepseek-coder-33B-instruct model with intentionally-erroneous code, and we also define relevant metrics and adopt an agent-based approach to evaluate the LLM and assess its capabilities as an LLM-as-a-judge. We also develop a pipeline-based approach to streamline the entire workflow. Finally, we make use of all of these strategies together to develop a more reliable method for automatically validating LLM-generated compiler tests. Based on our results, utilizing an agent-based prompting approach and setting up a validation pipeline structure drastically increased the quality of deepseek-coder-33B-instruct evaluation of tests which are used to validate compiler implementations of directive-based parallel programming models."
MissConf: LLM-Enhanced Reproduction of Configuration-Triggered Bugs,"Fu, Y; Wang, T; Li, SS; Ding, JY; Zhou, SL; Jia, ZY; Li, W; Jiang, Y; Liao, XK",10.1145/3639478.3647635,2024,"Bug reproduction stands as a pivotal phase in software development, but the absence of configuration information emerges as the main obstacle to effective bug reproduction. Since configuration options generally control critical branches of the software, many bugs can only be triggered under specific configuration settings. We refer to these bugs as configuration-triggered bugs or CTBugs for short. The reproduction of CTBugs consumes considerable time and manual efforts due to the challenges in deducing the missing configuration options within the vast search space of configurations. This complexity contributes to a form of technical debt in software development. To address these challenges, we first conducted an empirical study on 120 CTBugs from 4 widely used systems to understand the root causes and factors influencing the reproduction of CTBugs. Based on our study, we designed and implemented MISSCONF, the first LLM-enhanced automated tool for CTBug reproduction. MISSCONF first leverages the LLM to infer whether crucial configuration options are missing in the bug report. Once a suspect CTBug is found, MISSCONF employs configuration taint analysis and dynamic monitoring methods to filter suspicious configuration options set. Furthermore, it adopts a heuristic strategy for identifying crucial configuration options and their corresponding values. We evaluated MISSCONF on 5 real-world software systems. The experimental results demonstrate that MISSCONF successfully infers the 84% (41/49) of the CTBugs and reproduces the 65% (32/49) CTBugs. In the reproduction phase, MISSCONF eliminates up to 76% of irrelevant configurations, offering significant time savings for developers."
LCM: LLM-focused Hybrid SPM-cache Architecture with Cache Management for Multi-Core AI Accelerators,"Lai, CT; Zhou, ZC; Poptani, A; Zhang, W",10.1145/3650200.3656592,2024,"The proliferation of large language models (LLMs) with substantial computational requirements and memory footprints has necessitated the design of more capable AI accelerators. Given the long compilation time of scratchpad memory-based (SPM-based) AI accelerators and the challenges brought by LLMs, we have explored the other side of the tradeoff - a multi-core AI accelerator system that incorporates a shared cache and application-specific management strategies - to provide significantly shorter compilation time at the cost of sometimes slightly lower performance than SPM-based systems. Besides, state-of-the-art mixed precision quantization methods also bring dynamic and irregular memory access patterns that do not fit SPMs well. Experiments conducted on cycle-accurate simulators have shown a maximum 30% memory time reduction with our cache management strategies, and up to 1.37x overall execution speedup with our prefetcher. Furthermore, we integrate the cache model into a state-of-the-art AI accelerator analytical model, allowing for a rapid understanding of the impact of caches for large-scale systems and big ML models. Overall, a system with our managed cache performs comparably with that using a conventional global SPM in non-quantized test-cases. However, our managed cache can outperform an SPM by 50.5% when a mixed precision quantization algorithm is used. With our cache management schemes, the proposed system can achieve a 2.57x speedup compared to LRU caches. Finally, we implement the design in RTL and the area of our design is 0.218mm2 with 15nm process, which can run at 1 GHz clock frequency. Our findings explore the potential of a shared cache design to assist the development of future AI accelerator systems."
LLM Security Guard for Code,"Kavian, A; Mehdi, M; Kallehbasti, P; Kazemi, S; Firouzi, E; Ghafari, M",10.1145/3661167.3661263,2024,"Many developers rely on Large Language Models (LLMs) to facilitate software development. Nevertheless, these models have exhibited limited capabilities in the security domain. We introduce LLMSecGuard, a framework to offer enhanced code security through the synergy between static code analyzers and LLMs. LLMSecGuard is open source and aims to equip developers with code solutions that are more secure than the code initially generated by LLMs. This framework also has a benchmarking feature, aimed at providing insights into the evolving security attributes of these models."
LLMs in Web Development: Evaluating LLM-Generated PHP Code Unveiling Vulnerabilities and Limitations,"TÃ³th, R; Bisztray, T; Erdodi, L",10.1007/978-3-031-68738-9_34,2024,"This study evaluates the security of web application code generated by Large Language Models, analyzing 2,500 GPT-4 generated PHP websites. These were deployed in Docker containers and tested for vulnerabilities using a hybrid approach of Burp Suite active scanning, static analysis, and manual review. Our investigation focuses on identifying Insecure File Upload, SQL Injection, Stored XSS, and Reflected XSS in GPT-4 generated PHP code. This analysis highlights potential security risks and the implications of deploying such code in real-world scenarios. Overall, our analysis found 2,440 vulnerable parameters. According to Burp's Scan, 11.56% of the sites can be straight out compromised. Adding static scan results, 26% had at least one vulnerability that can be exploited through web interaction. Certain coding scenarios, like file upload functionality, are insecure 78% of the time, underscoring significant risks to software safety and security. The main contribution of our research is the creation of a methodology, that allows benchmarking LLMs for deployable PHP code generation, where the investigated property is secure code generation. We have made the source codes and a detailed vulnerability record for each GPT-4 generated sample publicly available to support further research: https://github.com/Beckyntosh/ChatPHP. This study emphasizes the crucial need for thorough testing and evaluation if generative AI technologies are used in software development."
Leveraging LLM-based data augmentation for automatic classification of recurring tasks in software development projects,"Wysocki, W; Ochodek, M",10.1016/j.jss.2025.112641,2026,"Background: Issue tracking systems (ITS) store project task data that is valuable for analytics and simulation. Projects typically include two types of tasks: stateful and recurring. While stateful tasks can be automatically categorized with relative ease, categorizing recurring tasks remains challenging. Prior research indicates that a key difficulty may lie in the underrepresentation of certain task types, which leads to severely imbalanced training datasets and hampers the accuracy of machine-learning models for task categorization. Aims: The goal of this study is to evaluate whether leveraging large language models (LLM) for data augmentation can enhance the machine-learning-based categorization of recurring tasks in software projects. Method: We conduct our study on a dataset from six industrial projects comprising 9,589 tasks. To address class imbalance, we up-sample minority classes during training via data augmentation using LLMs and several prompting strategies, assessing their impact on prediction quality. For each project, we perform time-series 5-fold cross-validation and evaluate the classifiers using state-of-the-art metrics - Accuracy, Precision, Recall, F1-score, and MCC - as well as practice-inspired metric called Monthly Classification Error (MCE) that assess the impact of task misclassification on project planning and resource allocation. Our machine-learning pipeline employs Transformer-based sentence embeddings and XGBoost classifiers. Results: The model automatically classifies software process tasks into 14 classes, achieving MCC values between 0.71 and 0.76. We observed higher prediction quality for the largest projects in the dataset and for those managed using traditional project management methodologies. Moreover, employing intra-project data augmentation strategies reduced the MCE error by up to 43%. Conclusions: Our findings indicate that large language models (LLMs) can be used to mitigate the impact of imbalanced task categories, thereby enhancing the performance of classification models even with limited training data."
Exploring Human-AI Collaboration in Agile: Customised LLM Meeting Assistants,"Cabrero-Daniel, B; Herda, T; Pichler, V; Eder, M",10.1007/978-3-031-61154-4_11,2024,"This action research study focuses on the integration of AI assistants in two Agile software development meetings: the Daily Scrum and a feature refinement, a planning meeting that is part of an in-house Scaled Agile framework. We discuss the critical drivers of success, and establish a link between the use of AI and team collaboration dynamics. We conclude with a list of lessons learnt during the interventions in an industrial context, and provide a assessment checklist for companies and teams to reflect on their readiness level. This paper is thus a road-map to facilitate the integration of AI tools in Agile setups."
"DevSec-GPT - Generative-AI (With Custom-Trained Meta's Llama2 LLM), Blockchain, NFT and PBOM Enabled Cloud Native Container Vulnerability Management and Pipeline Verification Platform","Bandara, E; Shetty, S; Mukkamala, R; Rahman, A; Foytik, P; Liang, XP; De Zoysa, K; Keong, NW",10.1109/Cloud-Summit61220.2024.00012,2024,"The ever-evolving realm of cloud-native software development and container-based deployment, although offering exceptional efficiency and scalability, presents an array of complex challenges. These prominent challenges encompass supply-chain attacks, vulnerabilities within open-source tools, difficulties in tracking the development lifecycle and pipelines, and intricacies related to managing data provenance. In response to these pressing concerns, this paper introduces DevSec-GPT, a pioneering solution that harnesses Generative AI, blockchain, NFTs, SBOMs (Software Bill Of Materials), and PBOMs (Pipeline Bill Of Materials) to effectively manage software container vulnerabilities and streamline the intricate intricacies of pipeline and supplychain verification. In the contemporary software development landscape, cloud-native containers, such as Docker and Kubernetes, are the linchpins of the build and deploy process, complemented by CI/CD (Continuous Integration and Continuous Delivery) services such as GitHub Actions. This process entails the creation of pull requests, container generation, test suite execution, verification, approval, merging to the master branch, and eventual deployment. In our innovative system, blockchain smart contracts play a pivotal role in generating vulnerability scans for each pull request through SBOM analysis. Moreover, a custom-trained Llama2 Large Language Model(LLM) from Meta has been integrated to generate PBOMs tailored to each pull request and master builds, thereby preventing supply-chain attacks and data breaches etc. This Llama2-13B LLM has been quantized and fine-tuned using Qlora to ensure optimal performance on consumer-grade hardware. These PBOMs are generated as JSON schemas by the LLM, encapsulating vital details, including pull request information (branch, approver, reviewer, timestamp, etc.), test results, the identified vulnerabilities in the built container, and the status of the pull request and its timestamp. Subsequently, blockchain smart contracts employ these JSON schemas to generate signed NFT tokens, a remarkable method that enables comprehensive tracking of software container states, vulnerabilities, and pipeline details from development to production. We've innovated a tailor-made NFT token schema designed to encapsulate PBOM data within the blockchain. These NFT tokens furnish a resilient mechanism, facilitating retrieval and verification at any point. The end-toend software/pipeline verification data provenance information is handled via ModelCards. The prototype of our proposed system has been constructed atop the Rahasak blockchain, complemented by the GitHub Actions CI/CD platform and Docker containers. The generation of PBOMs functions are handled by custom-trained Llama2-13B LLM. To the best of our knowledge, this is the very first research effort aimed at standardizing PBOM schemas and integrating Language Model algorithms for the generation of PBOMs."
Multi-language Software Development in the LLM Era: Insights from Practitioners Conversations with ChatGPT,"Aguiar, L; Paixao, M; Carmo, R; Soares, E; Leal, A; Freitas, M; Gama, E",10.1145/3674805.3690755,2024,"Non-trivial software systems are commonly developed using more than a single programming language. However, multi-language development is not straightforward. Nowadays, tools powered by Large Language Models (LLMs), such as ChatGPT, have been shown to successfully assist practitioners in several aspects of software development. This paper reports a preliminary study aimed to investigate to what extent ChatGPT is being used in multi-language development scenarios. Hence, we leveraged DevGPT, a dataset of conversations between software practitioners and ChatGPT. In total, we studied data from 3,584 conversations, comprising a total of 18,862 code snippets. Our analyses show that only 18.33% of the code snippets suggested by ChatGPT are written in the same programming language as the primary language in the repository where the conversation was shared. In an in-depth analysis, we observed expected scenarios, such as 31.54% of JavaScript snippets being suggested in CSS repositories However, we also unveiled surprising ones, such as Python snippets being largely suggested in C++ repositories. After a qualitative open card sorting of the conversations, we found that in 70% of them developers were asking for coding support while in 57% developers used ChatGPT as a tool to generate code. Our initial results indicate that not only LLMs are being used in multi-language development but also showcase the contexts in which such tools are assisting developers."
Towards LLM-augmented multiagent systems for agile software engineering,"Chudziak, JA; Cinkusz, K",10.1145/3691620.3695336,2024,"A cognitive multi-agent ecosystem designed for efficient software engineering using Agile methodologies can significantly improve software development processes. Key components include the integration of Multi-Agent Systems (MAS) and Large Language Models (LLMs), utilizing Dynamic Context techniques for agent profiling, and Theory of Mind to enhance collaboration. The CogniSim Ecosystem analyzes problems, proposes solutions, constructs and validates plans, and coordinates specialized agents playing roles such as developers, executors, quality checkers, and methodology reviewers. These agents produce documentation, models, and diagrams (e.g., UML) while adhering to predefined quality and performance measures. The ecosystem also simulates the impact of various team configurations on problem-solving effectiveness, helping organizations identify optimal team structures. Case studies and simulations demonstrate its practical applications."
ThreatFinderAI: Automated Threat Modeling Applied to LLM System Integration,"Von der Assen, J; Huertas, A; Sharif, J; Feng, C; Bovet, G; Stiller, B",10.23919/CNSM62983.2024.10814632,2024,"Artificial Intelligence (AI) is a rapidly integrated technology, significantly contributing to advancements like 6G. However, its swift adoption raises considerable security concerns. Large Language Models (LLMs) pose risks such as spear phishing, code injections, and remote code execution. Conventional threat modeling, used in secure software development, faces challenges when applied to AI systems, as existing methodologies are designed for traditional software. Furthermore, AI-specific threat modeling research is sparse and lacks approaches providing practical support or automation. Thus, this demo paper presents ThreatFinderAI, an asset-centric threat modeling and risk assessment framework. ThreatFinderAI fulfills seven steps aligned with AI system design and transforms AI threat and control knowledge bases into a queryable knowledge graph for automated asset identification and threat elicitation. It also proposes business impact analysis and expert estimates for AI threat impact quantification. In the demonstration, ThreatFinderAI is illustrated by securing a customer care application relying on LLMs. Through this, it is demonstrated how the proposed framework can he used to identify relevant threats and practical countermeasures and communicate strategic risk."
An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation,"Schafer, M; Nadi, S; Eghbali, A; Tip, F",10.1109/TSE.2023.3334955,2024,"Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software development, including their suggested use for automated generation of unit tests, but while requiring additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without requiring additional training or manual effort. Concretely, we consider an approach where the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We implement our approach in TESTPILOT, an adaptive LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project's API. We evaluate TESTPILOT using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a total of 1,684 API functions. The generated tests achieve a median statement coverage of 70.2% and branch coverage of 52.8%. In contrast, the state-of-the feedback-directed JavaScript test generation technique, Nessie, achieves only 51.3% statement coverage and 25.6% branch coverage. Furthermore, experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. We also find that 92.8% of TESTPILOT's generated tests have <= 50% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies. Finally, we run TESTPILOT with two additional LLMs, OpenAI's older code-cushman-002 LLM and StarCoder, an LLM for which the training process is publicly documented. Overall, we observed similar results with the former (68.2% median statement coverage), and somewhat worse results with the latter (54.0% median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model."
Future of software development with generative AI,"Sauvola, J; Tarkoma, S; Klemettinen, M; Riekki, J; Doermann, D",10.1007/s10515-024-00426-z,2024,"Generative AI is regarded as a major disruption to software development. Platforms, repositories, clouds, and the automation of tools and processes have been proven to improve productivity, cost, and quality. Generative AI, with its rapidly expanding capabilities, is a major step forward in this field. As a new key enabling technology, it can be used for many purposes, from creative dimensions to replacing repetitive and manual tasks. The number of opportunities increases with the capabilities of large-language models (LLMs). This has raised concerns about ethics, education, regulation, intellectual property, and even criminal activities. We analyzed the potential of generative AI and LLM technologies for future software development paths. We propose four primary scenarios, model trajectories for transitions between them, and reflect against relevant software development operations. The motivation for this research is clear: the software development industry needs new tools to understand the potential, limitations, and risks of generative AI, as well as guidelines for using it."
Self-Collaboration Code Generation via ChatGPT,"Dong, YH; Jiang, X; Jin, Z; Li, G",10.1145/3672459,2024,"Although large language models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, (1) Multiple LLM agents act as distinct experts, each responsible for a specific subtask within a complex task; (2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other's work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development's analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9-47.1% Pass@1 compared to the base LLM agent. Moreover, we showcase that selfcollaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent."
The Impact of Large Language Models on Programming Education and Student Learning Outcomes,"Jost, G; Taneski, V; Karakatic, S",10.3390/app14104115,2024,"Recent advancements in Large Language Models (LLMs) like ChatGPT and Copilot have led to their integration into various educational domains, including software development education. Regular use of LLMs in the learning process is still not well-researched; thus, this paper intends to fill this gap. The paper explores the nuanced impact of informal LLM usage on undergraduate students' learning outcomes in software development education, focusing on React applications. We carefully designed an experiment involving thirty-two participants over ten weeks where we examined unrestricted but not specifically encouraged LLM use and their correlation with student performance. Our results reveal a significant negative correlation between increased LLM reliance for critical thinking-intensive tasks such as code generation and debugging and lower final grades. Furthermore, a downward trend in final grades is observed with increased average LLM use across all tasks. However, the correlation between the use of LLMs for seeking additional explanations and final grades was not as strong, indicating that LLMs may serve better as a supplementary learning tool. These findings highlight the importance of balancing LLM integration with the cultivation of independent problem-solving skills in programming education."
An LPDDR-based CXL-PNM Platform for TCO-efficient Inference of Transformer-based Large Language Models,"Park, SS; Kim, K; So, J; Jung, J; Lee, J; Woo, K; Kim, N; Lee, Y; Kim, H; Kwon, Y; Kim, J; Lee, J; Cho, Y; Tai, Y; Cho, J; Song, H; Ahn, JH; Kim, NS",10.1109/HPCA57654.2024.00078,2024,"Transformer-based large language models (LLMs) such as Generative Pre-trained Transformer (GPT) have become popular due to their remarkable performance across diverse applications, including text generation and translation. For LLM training and inference, the GPU has been the predominant accelerator with its pervasive software development ecosystem and powerful computing capability. However, as the size of LLMs keeps increasing for higher performance and/or more complex applications, a single GPU cannot efficiently accelerate LLM training and inference due to its limited memory capacity, which demands frequent transfers of the model parameters needed by the GPU to compute the current layer(s) from the host CPU memory/storage. A GPU appliance may provide enough aggregated memory capacity with multiple GPUs, but it suffers from frequent transfers of intermediate values among GPU devices, each accelerating specific layers of a given LLM. As the frequent transfers of these model parameters and intermediate values are performed over relatively slow device-to-device interconnects such as PCIe or NVLink, they become the key bottleneck for efficient acceleration of LLMs. Focusing on accelerating LLM inference, which is essential for many commercial services, we develop CXL-PNM, a processing near memory (PNM) platform based on the emerging interconnect technology, Compute eXpress Link (CXL). Specifically, we first devise an LPDDR5X-based CXL memory architecture with 512GB of capacity and 1.1TB/s of bandwidth, which boasts 16x larger capacity and 10x higher bandwidth than GDDR6and DDR5-based CXL memory architectures, respectively, under a module form-factor constraint. Second, we design a CXLPNM controller architecture integrated with an LLM inference accelerator, exploiting the unique capabilities of such CXL memory to overcome the disadvantages of competing technologies such as HBM-PIM and AxDIMM. Lastly, we implement a CXLPNM software stack that supports seamless and transparent use of CXL-PNM for Python-based LLM programs. Our evaluation shows that a CXL-PNM appliance with 8 CXL-PNM devices offers 23% lower latency, 31% higher throughput, and 2.8x higher energy efficiency at 30% lower hardware cost than a GPU appliance with 8 GPU devices for an LLM inference service."
AutoCodeRover: Autonomous Program Improvement,"Zhang, YT; Ruan, HF; Fan, ZY; Roychoudhury, A",10.1145/3650212.3680384,2024,"Researchers have made significant progress in automating the software development process in the past, decades. Automated techniques for issue summarization, bug reproduction, fault localization, and program repair have been built to ease the workload of developers. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless, software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. program repair to fix bugs) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving Github issues to autonomously achieve program improvement. In our approach called AUTOCODEROVER, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM's understanding of the issue's root cause, and effectively retrieve a context via iterative search. The use of spectrum-based fault localization using tests, further sharpens the context, as long as a test-suite is available. Experiments on the recently proposed SWE-bench-lice (300 real-life Github issues) show increased efficacy in solving Github issues (19% on SWE-bench-lite), which is higher than the efficacy of the recently reported SWE-AGENT. Interestingly, our approach resolved 57 GitHub issues in about 4 minutes each (pass@1), Whereas developers spent more than 2.68 days on average. In addition, AUTOCODEROVER achieved this efficacy with significantly lower cost (on average, $0.43 USD), compared to other baselines. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can he autonomously improved."
An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project,"Rasnayaka, S; Wang, GL; Shariffdeen, R; Iyer, GN",10.1145/3643795.3648379,2024,"Large Language Models (LLMs) represent a leap in artificial intelligence, excelling in tasks using human language(s). Although the main focus of general-purpose LLMs is not code generation, they have shown promising results in the domain. However, the usefulness of LLMs in an academic software engineering project has not been fully explored yet. In this study, we explore the usefulness of LLMs for 214 students working in teams consisting of up to six members. Notably, in the academic course through which this study is conducted, students were encouraged to integrate LLMs into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of LLMs. In this paper, we analyze the AI-generated code, prompts used for code generation, and the human intervention levels to integrate the code into the code base. We also conduct a perception study to gain insights into the perceived usefulness, influencing factors, and future outlook of LLM from a computer science student's perspective. Our findings suggest that LLMs can play a crucial role in the early stages of software development, especially in generating foundational code structures, and helping with syntax and error debugging. These insights provide us with a framework on how to effectively utilize LLMs as a tool to enhance the productivity of software engineering students, and highlight the necessity of shifting the educational focus toward preparing students for successful human-AI collaboration."
Teamwork Conflict Management Training and Conflict Resolution Practice via Large Language Models,"Aggrawal, S; Magana, AJ",10.3390/fi16050177,2024,"This study implements a conflict management training approach guided by principles of transformative learning and conflict management practice simulated via an LLM. Transformative learning is more effective when learners are engaged mentally and behaviorally in learning experiences. Correspondingly, the conflict management training approach involved a three-step procedure consisting of a learning phase, a practice phase enabled by an LLM, and a reflection phase. Fifty-six students enrolled in a systems development course were exposed to the transformative learning approach to conflict management so they would be better prepared to address any potential conflicts within their teams as they approached a semester-long software development project. The study investigated the following: (1) How did the training and practice affect students' level of confidence in addressing conflict? (2) Which conflict management styles did students use in the simulated practice? (3) Which strategies did students employ when engaging with the simulated conflict? The findings indicate that: (1) 65% of the students significantly increased in confidence in managing conflict by demonstrating collaborative, compromising, and accommodative approaches; (2) 26% of the students slightly increased in confidence by implementing collaborative and accommodative approaches; and (3) 9% of the students did not increase in confidence, as they were already confident in applying collaborative approaches. The three most frequently used strategies for managing conflict were identifying the root cause of the problem, actively listening, and being specific and objective in explaining their concerns."
Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation,"Jin, KL; Wang, CY; Pham, HV; Hemmati, H",10.1145/3643991.3645074,2024,"Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers' conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts of modern software development."
Can LLMs Generate Architectural Design Decisions? - An Exploratory Empirical study,"Llhar, R; Vaidhyanathan, K; Varma, V",10.1109/ICSA59870.2024.00016,2024,"Architectural Knowledge Management (AKM) involves the organized handling of information related to architectural decisions and design within a project or organization. An essential artefact of AKM is the Architecture Decision Records (ADR), which documents key design decisions. ADRs are documents that capture decision context, decision made and various aspects related to a design decision, thereby promoting transparency, collaboration, and understanding. Despite their benefits, ADR adoption in software development has been slow due to challenges like time constraints and inconsistent uptake. Recent advancements in Large Language Models (LLMs) may help bridge this adoption gap by facilitating ADR generation. However, the effectiveness of LLM for ADR generation or understanding is something that has not been explored. To this end, in this work, we perform an exploratory study which aims to investigate the feasibility of using LLM for the generation of ADRs given the decision context. In our exploratory study, we utilize GPT and T5-based models with 0-shot, few-shot, and fine-tuning approaches to generate the Decision of an ADR given its Context. Our results indicate that in a 0-shot setting, state-of-the-art models such as GPT-4 generate relevant and accurate Design Decisions, although they fall short of human-level performance. Additionally, we observe that more cost-effective models like GPT-3.5 can achieve similar outcomes in a few-shot setting, and smaller models such as Flan-T5 can yield comparable results after fine-tuning. To conclude, this exploratory study suggests that LLM can generate Design Decisions, but further research is required to attain human-level generation and establish standardized widespread adoption."
An empirical study on the effectiveness of large language models for SATD identification and classification,"Sheikhaei, MS; Tian, Y; Wang, SW; Xu, BW",10.1007/s10664-024-10548-3,2024,"Self-Admitted Technical Debt (SATD), a concept highlighting sub-optimal choices in software development documented in code comments or other project resources, poses challenges in the maintainability and evolution of software systems. Large language models (LLMs) have demonstrated significant effectiveness across a broad range of software tasks, especially in software text generation tasks. Nonetheless, their effectiveness in tasks related to SATD is still under-researched. In this paper, we investigate the efficacy of LLMs in both identification and classification of SATD. For both tasks, we investigate the performance gain from using more recent LLMs, specifically the Flan-T5 family, across different common usage settings. Our results demonstrate that for SATD identification, all fine-tuned LLMs outperform the best existing non-LLM baseline, i.e., the CNN model, with a 4.4% to 7.2% improvement in F1 score. In the SATD classification task, while our largest fine-tuned model, Flan-T5-XL, still led in performance, the CNN model exhibited competitive results, even surpassing four of six LLMs. We also found that the largest Flan-T5 model, i.e., Flan-T5-XXL, when used with a zero-shot in-context learning (ICL) approach that only provides instructions for SATD identification, yields competitive results with traditional approaches but performs 6.4% to 9.2% worse than fine-tuned LLMs. For SATD classification, few-shot ICL approach, incorporating examples and category descriptions in prompts, outperforms the zero-shot approach and even surpasses the fine-tuned smaller Flan-T5 models. Moreover, our experiments demonstrate that incorporating contextual information, such as surrounding code, into the SATD classification task enables larger fine-tuned LLMs to improve their performance. Our study highlights the capabilities and limitations of LLMs for SATD tasks and the role of contextual information in achieving higher performance with larger LLMs, setting a foundation for future efforts to enhance these models for more effective technical debt management."
ChatGPT Code Detection: Techniques for Uncovering the Source of Code,"Oedingen, M; Engelhardt, RC; Denz, R; Hammer, M; Konen, W",10.3390/ai5030053,2024,"In recent times, large language models (LLMs) have made significant strides in generating computer code, blurring the lines between code created by humans and code produced by artificial intelligence (AI). As these technologies evolve rapidly, it is crucial to explore how they influence code generation, especially given the risk of misuse in areas such as higher education. The present paper explores this issue by using advanced classification techniques to differentiate between code written by humans and code generated by ChatGPT, a type of LLM. We employ a new approach that combines powerful embedding features (black-box) with supervised learning algorithms including Deep Neural Networks, Random Forests, and Extreme Gradient Boosting to achieve this differentiation with an impressive accuracy of 98%. For the successful combinations, we also examine their model calibration, showing that some of the models are extremely well calibrated. Additionally, we present white-box features and an interpretable Bayes classifier to elucidate critical differences between the code sources, enhancing the explainability and transparency of our approach. Both approaches work well, but provide at most 85-88% accuracy. Tests on a small sample of untrained humans suggest that humans do not solve the task much better than random guessing. This study is crucial in understanding and mitigating the potential risks associated with using AI in code generation, particularly in the context of higher education, software development, and competitive programming."
Requirements are All You Need: From Requirements to Code with LLMs,"Wei, BY",10.1109/RE59067.2024.00049,2024,"The pervasive use of textual formats in the documentation of software requirements presents a great opportunity for applying large language models (LLMs) to software engineering tasks. High-quality software requirements not only enhance the manual software development process but also position organizations to fully harness the potential of the emerging LLMs technology. This paper introduces a tailored LLM for automating the generation of code snippets from well-structured requirements documents. This LLM is augmented with knowledge, heuristics, and instructions that are pertinent to the software development process, requirements analysis, object-oriented design, and test-driven development, effectively emulating the expertise of a seasoned software engineer. We introduce a Progressive Prompting method that allows software engineers to engage with this LLM in a stepwise manner. Through this approach, the LLM incrementally tackles software development tasks by interpreting the provided requirements to extract functional requirements, using these to create object-oriented models, and subsequently generating unit tests and code based on the object-oriented designs. We demonstrate the LLM's proficiency in comprehending intricate user requirements and producing robust design and code solutions through a case study focused on the development of a web project. This study underscores the potential of integrating LLMs into the software development workflow to significantly enhance both efficiency and quality. The tailored LLM is available at https://chat.openai.com/g/g-bahoiKzkB-software-engineer-gpt."
A Controlled Experiment on the Energy Efficiency of the Source Code Generated by Code Llama,"Cursaru, VA; Duits, L; Milligan, J; Ural, D; Sanchez, BR; Stoico, V; Malavolta, I",10.1007/978-3-031-70245-7_12,2024,"Context. Large Language Models (LLMs) are now crucial for developers to increase productivity and reduce software development time and cost. Code Llama, an LLM from Meta, is one of the most recent LLM tools. However, currently there is no objective assessment of the energy efficiency of the source code generated by Code Llama. Goal. In this paper, we present an empirical study that assesses the energy efficiency of the source code generated by Code Llama with respect to human-written source code. Method. We design an experiment involving three human-written programming problems implemented in C++, JavaScript, and Python. We ask Code Llama to generate the code of the problems using different prompts and temperatures, which sets the predictability of the output of an LLM. Therefore, we execute both implementations and profile their energy efficiency. Results. Our study shows that the energy efficiency of the code generated by Code Llama varies according to the chosen programming language and code characteristics. Human implementations tend to be more energy efficient overall, with generated JavaScript code outperforming its human counterpart. In addition, explicitly asking Code Llama to generate energy-efficient code results in an equal or worse energy efficiency, and using different temperatures does not seem to affect the energy efficiency of generated code. Conclusions. According to our results, code generated using Code Llama does not guarantee energy efficiency, even when prompted to do so. Therefore, software developers should evaluate the energy efficiency of generated code before integrating it into the software system under development."
Turning Low-Code Development Platforms into True No-Code with LLMs,"Hagel, N; Hili, N; Schwab, D",10.1145/3652620.3688334,2024,"The relevance of low-code / no-code development has grown substantially in research and practice over the years to allow nontechnical users to create applications and, therefore, democratise software development. One problem in this domain still persists: many platforms remain low-code as the underlying modeling layer still requires professionals to write/design a model, often using Domain Specific Languages (DSLs). With the rise of generative AI and Large Language Models (LLMs) and their capabilities, new possibilities emerge on how Low Code Development Platforms (LCDPs) can be improved. This paper shows how the capabilities of LLMs can be leveraged to turn DSL-based low-code platforms into true no-code. We analyzed how textual modeling can be replaced by generating the required model using LLMs. We conducted a user experiment to compare textual modeling with the use of LLMs for that task. Our results show that task completion time could be significantly improved, and the majority of users prefer using the LLM-aided modeling. Based on these findings, we discuss the integration of these techniques into an existing low-code platform to transform it into true no-code."
How much SPACE do metrics have in GenAI assisted software development?,"Sikand, S; Phokela, KK; Sharma, VS; Singi, K; Kaulgud, V; Tung, T; Sharma, P; Burden, AP",10.1145/3641399.3641419,2024,"Large Language Models (LLMs) are revolutionizing the way a developer creates software by replacing code with natural language prompts as primary drivers. While many initial assessments of such LLMs suggest that it helps with developer productivity, other research studies have also pointed out areas in the Software Development Life Cycle(SDLC) and developer experience where such tools fail miserably. Currently, there exist many studies dedicated to evaluation of LLM-based AI-assisted software tools but there lacks a standardization of studies and metrics which may prove to be a hindrance to adoption of metrics and reproducible studies. The primary objective of this survey is to assess the recent user studies and surveys, aimed at evaluating different aspects of developer's experience of using code-based LLMs, and highlight any existing gaps among them. We have leveraged the SPACE framework to enumerate and categorise metrics from studies conducting some form of controlled user experiments. In Generative AI assisted SDLC, the developer's experience should encompass the ability to perform the in-hand task efficiently and effectively, with minimal friction using these LLM tools. Our exploration has led to some critical insights regarding complete absence of user studies in Collaborative aspects of teams, bias towards certain LLM models & metrics and lack of diversity in metrics within productivity dimensions. We also propose some recommendations to the research community which will help bring some conformity in the evaluation of such LLMs."
AI-Driven Refactoring: A Pipeline for Identifying and Correcting Data Clumps in Git Repositories,"Baumgartner, N; Iyenghar, P; Schoemaker, T; Pulvermueller, E",10.3390/electronics13091644,2024,"Data clumps, groups of variables that repeatedly appear together across different parts of a software system, are indicative of poor code structure and can lead to potential issues such as maintenance challenges, testing complexity, and scalability concerns, among others. Addressing this, our study introduces an innovative AI-driven pipeline specifically designed for the refactoring of data clumps in software repositories. This pipeline leverages the capabilities of Large Language Models (LLM), such as ChatGPT, to automate the detection and resolution of data clumps, thereby enhancing code quality and maintainability. In developing this pipeline, we have taken into consideration the new European Union (EU)-Artificial Intelligence (AI) Act, ensuring that our pipeline complies with the latest regulatory requirements and ethical standards for use of AI in software development by outsourcing decisions to a human in the loop. Preliminary experiments utilizing ChatGPT were conducted to validate the effectiveness and efficiency of our approach. These tests demonstrate promising results in identifying and refactoring data clumps, but also the challenges using LLMs."
Chain of Targeted Verification Questions to Improve the Reliability of Code Generated by LLMs,"Ngassom, SK; Dakhel, AM; Tambon, F; Khomh, F",10.1145/3664646.3664772,2024,"LLM-based assistants, such as GitHub Copilot and ChatGPT, have the potential to generate code that fulfills a programming task described in a natural language description, referred to as a prompt. The widespread accessibility of these assistants enables users with diverse backgrounds to generate code and integrate it into software projects. However, studies show that code generated by LLMs is prone to bugs and may miss various corner cases in task specifications. Presenting such buggy code to users can impact their reliability and trust in LLM-based assistants. Moreover, significant efforts are required by the user to detect and repair any bug present in the code, especially if no test cases are available. In this study, we propose a self-refinement method aimed at improving the reliability of code generated by LLMs by minimizing the number of bugs before execution, without human intervention, and in the absence of test cases. Our approach is based on targeted Verification Questions (VQs) to identify potential bugs within the initial code. These VQs target various nodes within the Abstract Syntax Tree (AST) of the initial code, which have the potential to trigger specific types of bug patterns commonly found in LLM-generated code. Finally, our method attempts to repair these potential bugs by reprompting the LLM with the targeted VQs and the initial code. Our evaluation, based on programming tasks in the CoderEval dataset, demonstrates that our proposed method outperforms state-of-the-art methods by decreasing the number of targeted errors in the code between 21% to 62% and improving the number of executable code instances to 13%."
How Do Software Developers Use ChatGPT? An Exploratory Study on GitHub Pull Requests,"Chouchen, M; Bessghaier, N; Begoug, M; Ouni, A; AlOmar, EA; Mkaouer, MW",10.1145/3643991.3645084,2024,"Nowadays, Large Language Models (LLMs) play a pivotal role in software engineering. Developers can use LLMs to address software development-related tasks such as documentation, code refactoring, debugging, and testing. ChatGPT, released by OpenAI, has become the most prominent LLM. In particular, ChatGPT is a cutting-edge tool for providing recommendations and solutions for developers in their pull requests (PRs). However, little is known about the characteristics of PRs that incorporate ChatGPT compared to those without it and what developers usually use it for. To this end, we quantitatively analyzed 243 PRs that listed at least one ChatGPT prompt against a representative sample of 384 PRs without any ChatGPT prompts. Our findings show that developers use ChatGPT in larger, time-consuming pull requests that are five times slower to be closed than PRs that do not use ChatGPT. Furthermore, we perform a qualitative analysis to build a taxonomy of the topics developers primarily address in their prompts. Our analysis results in a taxonomy comprising 8 topics and 32 sub-topics. Our findings highlight that ChatGPT is often used in review-intensive pull requests. Moreover, our taxonomy enriches our understanding of the developer's current applications of ChatGPT."
"Can GPT-4 Aid in Detecting Ambiguities, Inconsistencies, and Incompleteness in Requirements Analysis? A Comprehensive Case Study","Mahbub, T; Dghaym, D; Shankarnarayanan, A; Syed, T; Shapsough, S; Zualkernan, I",10.1109/ACCESS.2024.3464242,2024,"Effective software projects hinge on robust requirements, yet flawed requirements often lead to costly delays and revisions. While tools have been developed to identify defects in Software Requirements Specifications (SRS), the advent of Large Language Models (LLMs) like GPT-4 presents new opportunities for enhancing requirements quality. However, the potential of LLMs in this realm remains largely unexplored, particularly in the context of large-scale industrial documents. To bridge this gap, we investigate the efficacy of zero-shot GPT-4 in various requirements analysis tasks using an industrial software specification document. Our study evaluates LLM performance in detecting defects, such as ambiguities, inconsistencies, and incompleteness, while also analyzing GPT-4's ability to identify issues across version iterations and support technical experts in requirements analysis. Qualitatively, we identify key limitations of LLMs in defect detection, notably their inability to cross-reference throughout the document and their constrained understanding of specialized contexts. Quantitatively, we find that while LLMs excel in identifying incomplete requirements (precision 0.61), their performance is less impressive in detecting inconsistencies (precision 0.43) and ambiguities (precision 0.39). Although GPT-4 demonstrates promise in automating early defect detection across versions and providing accurate technical answers, our results underscore that they cannot entirely replace human analysts due to their lack of nuanced domain knowledge in a zero-shot setting. Nevertheless, avenues like few-shot learning and complex prompt design offer the potential to enhance LLM precision in defect detection."
Towards an Intelligent Test Case Generation Framework Using LLMs and Prompt Engineering,"Boukhlif, M; Kharmoum, N; Hanine, M; Kodad, M; Lagmiri, SN",10.1007/978-3-031-66854-8_3,2024,"Software testing is a critical phase in software development, and the research for more intelligent and adaptive test case generation approaches is ever-growing. As software systems grow in complexity, the need for advanced methodologies becomes imperative and the integration of Natural Language Processing (NLP) and Large Language Models (LLMs) into software testing offers unparalleled advantages. In response to this demand, we present an innovative prototype for an intelligent test case generation framework that leverages the strengths of fine-tuning through the TuneReqTest module and user-centric collaboration via the PrompTester module. TuneReqTest refines a pre-trained LLM with Software Requirements Specification (SRS) documents and associated test cases, enabling contextual adaptability to software-specific nuances. PrompTester introduces an intuitive interface for testers to actively participate in the test case generation process, guiding the LLM through tailored prompts. Our framework excels in adaptability to evolving requirements, reduces dependency on predefined test cases, and enhances coverage and quality assurance. Through these advancements, our framework represents a significant leap forward in the landscape of automated software testing, fostering collaboration between advanced language models and human testers for more effective and context-aware test case generation."
Lessons from Building StackSpot AI: A Contextualized AI Coding Assistant,"Pinto, G; de Souza, CRB; Neto, JB; de Souza, A; Gotto, T; Monteiro, E",10.1145/3639477.3639751,2024,"With their exceptional natural language processing capabilities, tools based on Large Language Models (LLMs) like ChatGPT and CoPilot have swiftly become indispensable resources in the software developer's toolkit. While recent studies suggest the potential productivity gains these tools can unlock, users still encounter drawbacks, such as generic or incorrect answers. Additionally, the pursuit of improved responses often leads to extensive prompt engineering efforts, diverting valuable time from writing code that delivers actual value. To address these challenges, a new breed of tools, built atop LLMs, is emerging. These tools aim to mitigate drawbacks by employing techniques like fine-tuning or enriching user prompts with contextualized information. In this paper, we delve into the lessons learned by a software development team venturing into the creation of such a contextualized LLM-based application, using retrieval-based techniques, called StackSpot AI. Over a four-month period, the team, despite lacking prior professional experience in LLM-based applications, built the product from scratch. Following the initial product release, we engaged with the development team responsible for the code generative components. Through interviews and analysis of the application's issue tracker, we uncover various intriguing challenges that teams working on LLM-based applications might encounter. For instance, we found three main group of lessons: LLM-based lessons, User-based lessons, and Technical lessons. By understanding these lessons, software development teams could become better prepared to build LLM-based applications."
Navigating (in)security of AI-generated code,"Ambati, SH; Ridley, N; Branca, E; Stakhanova, N",10.1109/CSR61664.2024.10679468,2024,"The increasing use of large language models (LLMs) such as OpenAI's ChatGPT and Google's Bard in the software development industry raise questions about the security of generated code. Our research evaluates Java, C, and Python code samples that were generated by these LLMs. In our investigation, we assessed the consistency of code samples generated by each LLM, characterized the security of generated code, and asked both LLMs to evaluate and fix the weaknesses of their own generated code as well as the code of the other LLM. Using 133 unique prompts from Google Code Jam competitions, we produced 3,854 code samples across three distinct programming languages. We found that the code produced by these LLMs is frequently insecure and prone to weaknesses and vulnerabilities. This concerns human developers who must exercise caution while employing these LLMs."
"Translation Titans, Reasoning Challenges: Satisfiability-Aided Language Models for Detecting Conflicting Requirements","Fazelnia, M; Mirakhorli, M; Bagheri, H",10.1145/3691620.3695302,2024,"Detecting conflicting requirements early in the software development lifecycle is crucial to mitigating risks of system failures and enhancing overall reliability. While Large Language Models (LLMs) have demonstrated proficiency in natural language understanding tasks, they often struggle with the nuanced reasoning required for identifying complex requirement conflicts. This paper introduces a novel framework, SAT-LLM, which integrates Satisfiability Modulo Theories (SMT) solvers with LLMs to enhance the detection of conflicting software requirements. SMT solvers provide rigorous formal reasoning capabilities, complementing LLMs' proficiency in natural language understanding. By synergizing these strengths, SAT-LLM aims to overcome the limitations of standalone LLMs in handling intricate requirement interactions. The early experiments provide empirical evidence supporting the effectiveness of our SAT-LLM over pure LLM-based methods like ChatGPT in identifying and resolving conflicting requirements. These findings lay a foundation for further exploration and refinement of hybrid approaches that integrate NLP techniques with formal reasoning methodologies to address complex challenges in software development."
The role of library versions in Developer-ChatGPT conversations,"Raj, R; Costa, DE",10.1145/3643991.3645075,2024,"The latest breakthroughs in large language models (LLM) have empowered software development tools, such as ChatGPT, to aid developers in complex tasks. Developers use ChatGPT to write code, review code changes, and even debug their programs. In these interactions, ChatGPT often recommends code snippets that depend on external libraries. However, code from libraries changes over time, invalidating a once-correct code snippet and making it difficult to reuse recommended code. In this study, we analyze DevGPT, a dataset of more than 4,000 Developer-ChatGPT interactions, to understand the role of library versions in code-related conversations. We quantify how often library version constraints are mentioned in code-related conversations and when ChatGPT recommends the installation of specific libraries. Our findings show that, albeit to constantly recommend and analyze code with external dependencies, library version constraints only appear in 9% of the conversations. In the majority of conversations, the version constraints are prompted by users (as opposed to being specified by ChatGPT) as a method for receiving better quality responses. Moreover, we study how library version constraints are used in the conversation through qualitative methods, identifying several potential problems that warrant further research."
Unrestricted Use of LLMs in a Software Project Course: Student Perceptions on Learning and Impact on Course Performance,"Korpimies, K; Laaksonen, A; Luukkainen, M",10.1145/3699538.3699541,2024,"Large language models (LLMs) provide round-the-clock personalized programming assistance, unlike course instructors or traditional online information sources such as Stack Overflow. While LLMs can aid in code generation, concerns about over-reliance and the impact on learning persist. This study discusses students' experiences with LLMs in a software project course where students were allowed to use LLMs freely except for unit test generation. We conducted surveys during course instances in autumn 2023 and spring 2024. The surveys assessed the extent of LLM usage, methods of application, and perceived impact on learning. Results indicate diverse usage patterns, with many students finding LLMs beneficial for efficiency and problem-solving, though over-reliance and poor-quality outputs were noted concerns. The usage patterns can be linked to course performance and time spent on the project."
Experiential Co-Learning of Software-Developing Agents,"Qian, C; Dang, YF; Li, JH; Liu, W; Xie, ZH; Wang, YF; Chen, WZ; Yang, C; Cong, X; Che, XY; Liu, ZY; Sun, MS",,2024,"Recent advancements in large language models (LLMs) have brought significant changes to various domains, especially through LLM-driven autonomous agents. A representative scenario is in software development, where LLM agents demonstrate efficient collaboration, task division, and assurance of software quality, markedly reducing the need for manual involvement. However, these agents frequently perform a variety of tasks independently, without benefiting from past experiences, which leads to repeated mistakes and inefficient attempts in multi-step task execution. To this end, we introduce Experiential Co-Learning, a novel LLM-agent learning framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for future task execution. The extensive experiments demonstrate that the framework enables agents to tackle unseen software-developing tasks more effectively. We anticipate that our insights will guide LLM agents towards enhanced autonomy and contribute to their evolutionary growth in cooperative learning. The code and data are available at https://github.com/OpenBMB/ChatDev."
"A3-CodGen: A Repository-Level Code Generation Framework for Code Reuse With Local-Aware, Global-Aware, and Third-Party-Library-Aware","Liao, DS; Pan, SD; Sun, XY; Ren, XX; Huang, Q; Xing, ZC; Jin, H; Li, QY",10.1109/TSE.2024.3486195,2024,"LLM-based code generation tools are essential to help developers in the software development process. Existing tools often disconnect with the working context, i.e., the code repository, causing the generated code to be not similar to human developers. In this paper, we propose a novel code generation framework, dubbed A(3)-CodGen, to harness information within the code repository to generate code with fewer potential logical errors, code redundancy, and library-induced compatibility issues. We identify three types of representative information for the code repository: local-aware information from the current code file, global-aware information from other code files, and third-party-library information. Results demonstrate that by adopting the A(3)-CodGen framework, we successfully extract, fuse, and feed code repository information into the LLM, generating more accurate, efficient, and highly reusable code. The effectiveness of our framework is further underscored by generating code with a higher reuse rate, compared to human developers. This research contributes significantly to the field of code generation, providing developers with a more powerful tool to address the evolving demands in software development in practice."
"CoEdPilot: Recommending Code Edits with Learned Prior Edit Relevance, Project-wise Awareness, and Interactive Nature","Liu, CY; Cai, YF; Lin, Y; Huang, YH; Pei, YR; Jiang, B; Yang, P; Dong, JS; Hong, M",10.1145/3650212.3652142,2024,"Recent years have seen the development of LLM-based code generation. Compared to generating code in a software project, incremental code edits are empirically observed to be more frequent. The emerging code editing approaches usually formulate the problem as generating an edit based on known relevant prior edits and context. However, practical code edits can be more complicated. First, an editing session can include multiple (ir)relevant edits to the code under edit. Second, the inference of the subsequent edits is non-trivial as the scope of its ripple effect can be the whole project. In this work, we propose CoEdPilot, an LLM-driven solution to recommend code edits by discriminating the relevant edits, exploring their interactive natures, and estimating its ripple effect in the project. Specifically, CoEdPilot orchestrates multiple neural transformers to identify what and how to edit in the project regarding both edit location and edit content. When a user accomplishes an edit with an optional editing description, an Subsequent Edit Analysis first reports the most relevant files in the project with what types of edits (e.g., keep, insert, and replace) can happen for each line of their code. Next, an Edit-content Generator generates concrete edit options for the lines of code, regarding its relevant prior changes reported by an Edit-dependency Analyzer. Last, both the Subsequent Edit Analysis and the Edit-content Generator capture relevant prior edits as feedback to readjust their recommendations. We train our models by collecting over 180K commits from 471 open-source projects in 5 programming languages. Our extensive experiments show that (1) CoEdPilot can well predict the edits (i.e., predicting edit location with accuracy of 70.8%-85.3%, and the edit content with exact match rate of 41.8% and BLEU4 score of 60.7); (2) CoEdPilot can well boost existing edit generators such as GRACE and CCT5 on exact match rate by 8.57% points and BLEU4 score by 18.08. Last, our user study on 18 participants with 3 editing tasks (1) shows that CoEdPilot can be effective in assisting users to edit code in comparison with Copilot, and (2) sheds light on the future improvement of the tool design. The video demonstration of our tool is available at https://sites.google.com/view/coedpilot/home."
Frankincense preparation promotes formation of inflammation-resolving lipid mediators by manipulating lipoxygenases in human innate immune cells,"Nischang, V; Witt, FM; BÃ¶rner, F; Gomez, M; Jordan, PM; Werz, O",10.3389/fphar.2023.1332628,2024,"Introduction: Frankincense preparations are frequently used as traditional anti-inflammatory remedies in folk medicine with increasing popularity. Boswellic acids (BAs), especially 3-O-acetyl-11-keto-beta BA (AKBA), are unique anti-inflammatory principles of frankincense, with multiple pharmacological actions and target proteins. We recently showed that AKBA favorably impacts lipid mediator (LM) networks in innate immune cells, by modulation of lipoxygenase (LOX) activities. Thus, AKBA binds to allosteric sites in 5-LOX, shifting the regiospecificity to a 12/15-lipoxygnating enzyme, and to an analogous site in 15-LOX-1, leading to enzyme activation, which favors specialized pro-resolving mediator (SPM) formation at the expense of leukotriene production.Methods: Here, we investigated Boswellin super (R) (BSR), a commercially available frankincense extract with >= 30% AKBA, used as remedy that approved efficacy in osteoarthritis trials, for its ability to modulate LM pathways in human monocyte-derived macrophage (MDM) phenotypes, neutrophils, and neutrophil/platelet co-incubations. LM profiling was performed by using targeted ultraperformance liquid chromatography-tandem mass spectrometry (UPLC-MS-MS).Results: BSR concentration-dependently (10-100 mu g/ml) suppressed formation of pro-inflammatory 5-LOX products including LTB4 in exotoxin-stimulated M1-MDM and neutrophils, and strongly elevated 12/15-LOX products and SPM in activated M2-MDM and neutrophil/platelet cocultures, starting at 10 mu g/mL. Also, BSR (>= 10 mu g/mL) induced robust 12/15-LOX product and SPM generation in resting M2-MDM, which was further markedly elevated when exogenous docosahexaenoic acid (DHA) and eicosahexaenoic acid (EPA) were supplied, and induced translocation of 15-LOX from a soluble to a particulate locale in M2 MDM.Discussion: We conclude that BSR especially when co-added with DHA and EPA, promotes the LM class switch in innate immune cells from pro-inflammatory to pro-resolving mediators, which might be a plausible mechanism underlying the anti-inflammatory actions of BSR."
Exploring ChatGPT for Identifying Sexism in the Communication of Software Developers,"Sultana, S; Kali, MB",10.1145/3652037.3663918,2024,"The software development community, largely dominated by male developers, frequently exhibits instances of sexism and misogyny. While efforts have been made to automatically detect toxic and misogynistic language within this community, existing tools have yet to reach a satisfactory level of performance for practical implementation. However, there is growing interest in leveraging large language models (LLM) like ChatGPT for various tasks among developers. In this study, we aim to assess the effectiveness of ChatGPT for identifying particular forms of sexist remarks, such as maternal insult and stereotyping in software development communication channels. We experiment with various prompts, gradually scaffolding to fine-tune the task of detecting sexist remarks. Our preliminary analysis shows promising results in terms of identifying forms of sexist remarks such as maternal insult and stereotyping. Further research might consider investigating the dialogue as a unit of analysis to capture the nuances and context to better identify the remarks."
A Comparative Study of Large Language Models for Goal Model Extraction,"Siddeshwar, V; Alwidian, S; Makrehchi, M",10.1145/3652620.3686246,2024,"User stories, expressed in snippets of natural language text, are commonly used to elicit stakeholder's needs in agile software development. Requirement engineers model user stories to interpret the relations among goals and requirements. Manual transformation of goal models has challenges such as, difficulty of converting lower-abstraction user stories into higher-level goals, and extraction of goals embedded in user stories depends on the skill of requirements engineers. In this paper we introduce a technique that leverages Large Language Models (LLMs) to automatically generate goal models from user stories. The approach uses Iterative Prompt Engineering that guides LLM to extract intentional elements and generate its XML-compatible representation in Goal-oriented Requirements Language (GRL). The generated models can be visualized using jUCMNav tool. We evaluated our approach using three LLMs: GPT-4, Llama and Cohere. Our qualitative evaluation indicates that GPT-4 or Llama can be used to assist requirements engineers in modeling as they can produce GRL goal models that are understandable. Additionally, these LLMs are capable of exposing soft goals that are not apparent to stakeholders who are new to the domain."
Effective Integration and Use of Non-Development LLMs in Software Development,"Meem, FN",10.1109/VL/HCC60511.2024.00053,2024,"There is an increasing utilization of non-development large language models (LLMs), such as ChatGPT, in software development despite the availability of dedicated software development tools. I propose research that investigates the reasons behind use of non-development LLM tools and the impact they have on productivity and well-being of software practitioners, particularly those with diagnosed mental health disorders. I will use insights from these efforts to analyze the advantages and disadvantages of non-development LLMs in order to generate recommendations and interventions that will facilitate the effective integration and use of non-development LLM tools in software development."
iRisk: A Scalable Microservice for Classifying Issue Risks Based on Crowdsourced App Reviews,"de Lima, VM; Barbosa, JR; Marcacini, RM",10.1109/ICSME58944.2024.00091,2024,"Analyzing mobile app reviews is essential for identifying trends and issue patterns that affect user experience and app reputation in app stores. A risk matrix provides a straightforward, intuitive method to prioritize software maintenance actions to mitigate negative ratings. However, manually constructing a risk matrix is time-consuming, and stakeholders often struggle to understand the context of risks due to varied descriptions and the sheer volume of reviews. Therefore, machine learning-based methods are needed to extract risks and classify their priority effectively. While existing studies have automated risk matrix generation in software development, they have not explored app reviews or utilized Large Language Models (LLMs) in a scalable architecture. To address this gap, we present iRisk (scalable microservice for classifying issue Risks), a tool for generating a risk matrix based on crowdsourced app reviews using LLM. We present i-LLAMA, a fine-tuned version of LLaMA 3, optimized to detect and prioritize app-related issues using a risk analysis dataset of reviews categorized by severity and likelihood of occurrence. This dataset is also publicly available. Our contributions include the open-source resources to support the software maintenance and evolution industry, fine-tuning of LLaMA 3, and a scalable microservice architecture to handle large volumes of data. The iRisk can manage app issues and risks and provide an automated dashboard and visualizations for decision-making, monitoring, and risk mitigation. The tool is available on GitHub(1), and a presentation about the tool can be found in this video(2)."
Towards Trustworthy AI Software Development Assistance,"Maninger, D; Narasimhan, K; Mezini, M",10.1145/3639476.3639770,2024,"It is expected that in the near future, AI software development assistants will play an important role in the software industry. However, current software development assistants tend to be unreliable, often producing incorrect, unsafe, or low-quality code. We seek to resolve these issues by introducing a holistic architecture for constructing, training, and using trustworthy AI software development assistants. In the center of the architecture, there is a foundational LLM trained on datasets representative of real-world coding scenarios and complex software architectures, and fine-tuned on code quality criteria beyond correctness. The LLM will make use of graph-based code representations for advanced semantic comprehension. We envision a knowledge graph integrated into the system to provide up-to-date background knowledge and to enable the assistant to provide appropriate explanations. Finally, a modular framework for constrained decoding will ensure that certain guarantees (e.g., for correctness and security) hold for the generated code."
Vulnerability Handling of AI-Generated Code - Existing Solutions and Open Challenges,"Kaniewski, S; Holstein, D; Schmidt, F; Heer, T",10.1109/AIxSET62544.2024.00026,2024,"The increasing use of generative Artificial Intelligence (AI) in modern software engineering, particularly Large Language Models (LLMs) for code generation, has transformed professional software development by boosting productivity and automating development processes. This adoption, however, has highlighted a significant issue: the introduction of security vulnerabilities into the code. These vulnerabilities result, e.g., from flaws in the training data that propagate into the generated code, creating challenges in tackling them in established ways. Traditional vulnerability handling processes often involve extensive manual review. Applying such traditional processes to AI-generated code is challenging. AI-generated code may include several similar vulnerabilities, possibly in slightly different forms as developers might not build on already implemented code, using functions or libraries, but prompt similar tasks. In this work, we explore the current state of LLM-based approaches for vulnerability handling, focusing on approaches for vulnerability detection, localization, and repair. We provide an overview of recent progress in this area and highlight open challenges that must be addressed to establish a reliable and scalable vulnerability handling process for AI-generated code."
Generating and Reviewing Programming Codes with Large Language Models A Systematic Mapping Study,"de Albuquerque, BVL; da Cunha, AFS; Souza, L; Siqueira, SWM; dos Santos, RP",10.1145/3658271.3658342,2024,"Context: The proliferation of technologies based on Large Language Models (LLM) is reshaping various domains, also impacting on programming code creation and review. Problem: The decision-making process in adopting LLM in software development demands an understanding of associated challenges and diverse application possibilities. Solution: This study addresses the identified challenges linked to LLM utilization in programming code processes. It explores models, utilization strategies, challenges, and coping mechanisms, focusing on the perspectives of researchers in software development. IS Theory: Drawing on Task-Technology Fit (TTF) theory, the research examines the alignment between task characteristics in code generation and review, and LLM technology attributes to discern performance impacts and utilization patterns. Method: Employing the Systematic Mapping of the Literature method, the research analyzes 19 selected studies from digital databases-IEEE Digital Library, Compendex Engineering Village, and Scopus-out of 1,257 retrieved results. Summary of Results: The research reveals 23 models, 13 utilization strategies, 15 challenges, and 14 coping mechanisms associated with LLM in programming code processes, offering a comprehensive understanding of the application landscape. Contributions to IS: Contributing to the Information Systems (IS) field, This study provides valuable insights into the utilization of LLM in programming code generation and review. The identified models, strategies, challenges, and coping mechanisms offer practical guidance for decision-making processes related to LLM technology adoption. The research aims to support the IS community in effectively navigating the complexities of integrating large language models into the dynamic software development lifecycle."
Goal Model Extraction from User Stories Using Large Language Models,"Siddeshwar, V; Alwidian, S; Makrehchi, M",10.1007/978-3-031-70245-7_19,2024,"In agile software development, goal modeling is vital for understanding the relationships among user stories, commonly used to capture stakeholders' needs. Manual construction of goal models faces challenges, such as transforming lower-level user stories into higher-level models and capturing implicit goals. This paper presents early research proposing a technique using Large Language Models (LLMs), like GPT4, to automatically generate goal models from user stories. The approach employs Iterative Prompt Engineering to guide the LLM in extracting intentional elements and generating XML representations using the Goal-oriented Requirements Language (GRL), visualized with the jUCMNav tool. Our primitive qualitative evaluation indicates that GPT-4 can produce GRL models that are acceptable and understandable. Despite the generic nature of LLM-generated models, there is a potential for their use in requirements modeling, particularly in exposing soft goals not immediately apparent to stakeholders new to the domain."
Poster: Benchmarking of Code Generative LLMs,"Rahman, MM; Kundu, A; Bertino, E",10.1109/ICDCS60910.2024.00145,2024,"Generative LLMs have proven to be valuable code generators, thus enabling code copilots and meeting several requirements in software engineering. However, several questions arise: How good an LLM is as a software engineer? How secure is the code generated or fixed by an LLM? These are complex questions to address; however, addressing them is critical for enabling trustworthy software development ecosystems. Addressing those questions requires a designing rigorous benchmark to evaluate: (i) the code that is generated/completed, and (ii) the generative LLMs themselves. In this paper, we propose an automated benchmarking system covering the different aspects of the generated code and the LLMs that generate the code. We also propose the concept of a benchmark dependency graph, coupled with an automated benchmark scoring process that provides a vector of scores on how good or how bad an LLM is, or the code generated/modified by it is, additionally the artifacts generated or modified around the code."
From Human-to-Human to Human-to-Bot Conversations in Software Engineering,"Khojah, R; Neto, FGD; Leitner, P",10.1145/3664646.3664761,2024,"Software developers use natural language to interact not only with other humans, but increasingly also with chatbots. These interactions have different properties and flow differently based on what goal the developer wants to achieve and who they interact with. In this paper, we aim to understand the dynamics of conversations that occur during modern software development after the integration of AI and chatbots, enabling a deeper recognition of the advantages and disadvantages of including chatbot interactions in addition to human conversations in collaborative work. We compile existing conversation attributes with humans and NLU-based chatbots and adapt them to the context of software development. Then, we extend the comparison to include LLM-powered chatbots based on an observational study. We present similarities and differences between human-to-human and human-to-bot conversations, also distinguishing between NLU- and LLM-based chatbots. Furthermore, we discuss how understanding the differences among the conversation styles guides the developer on how to shape their expectations from a conversation and consequently support the communication within a software team. We conclude that the recent conversation styles that we observe with LLM-chatbots can not replace conversations with humans due to certain attributes regarding social aspects despite their ability to support productivity and decrease the developers' mental load."
Learning from Failures: Translation of Natural Language Requirements into Linear Temporal Logic with Large Language Models,"Xu, YF; Feng, JC; Miao, WK",10.1109/QRS62785.2024.00029,2024,"Formalization of intended requirements is indispensable when using formal methods in software development. However, translating Natural Language (NL) requirements into formal specifications, such as Linear Temporal Logic (LTL), is error-prone. Although Large Language Models (LLMs) offer the potential for automatically translating unstructured NL requirements to LTL formulas, general-purpose LLMs face two major problems: First, low accuracy in translation. Second, high cost of model training and tuning. To tackle these challenges, we propose a new approach that combines dynamic prompt generation with human-computer interaction to leverage LLM for an accurate and efficient translation of unstructured NL requirements to LTL formulas. Our approach consists of two techniques: 1) Dynamic Prompt Generation, which automatically generates the most appropriate prompts for translating the inquired NL requirements. 2) Interactive Prompt Evolution, which helps LLMs to learn from previous translation errors, i.e., erroneous formalizations are amended by users and added as new prompt fragments. Our approach achieves remarkable performance in publicly available datasets from two distinct domains, comprising 36 and 255,000 NL-LTL pairs, respectively. Without human interaction, our method achieves up to 94.4% accuracy. When our approach is extended to another domain, the accuracy improves from an initial 27% to 78% under interactive prompt evolution."
Costless Expert Systems Development and Reengineering,"Alsharidi, M; Ali, AH",,2024,"Symbolic AI is indispensable for the current LLM agents that are used for example to reason the context of the questions. An expert system is a symbolic AI that can explain the reasoning it reached to, which typically is a rule-based system has been attractive for different domains such as medicine, agriculture, and operations. On average, these systems involve hundreds of rules that are instable; moreover, they are coded at low levels of abstraction. Therefore, designing and reengineering an expert system is still costly and needs technical knowledge because of the manual process and maintaining of a low-level abstraction. On the other hand, model-driven architecture (MDA) has proven to be a successful technology that raised the abstraction level and formalized it to automate software development. It specifies business aspects in the platformindependent model (PIM) and implementation aspects in a platform-specific model (PSM). It then automates mapping between them using a standard mapping language called Query- View- Transform QVT. This paper argues that utilizing MDA principles such as the automation and abstractions represented by the descriptor PIM and PSM and mappings metamodels will not only overcome the instability of rules of expert systems, but also provides new insights for its usage. Therefore, this work proposes an MDA-compliant methodology that adopts a UML sequence diagram, a class diagram for the PIM descriptor, and a generic PSM) based on production rules. Moreover, a UML profile to support lacking features in the sequence model has been developed. However, the paper argues for a new kind of process-oriented expert system. Therefore, it not only allows domain experts to develop or participate in expert systems but also reduces the cost of developing new systems and reengineering or maintenance of the critical and large-scale legacy expert systems."
Mitigating Backdoor Threats to Large Language Models: Advancement and Challenges,"Liu, Q; Mo, WJ; Tong, T; Xu, JS; Wang, F; Xiao, CW; Chen, MH",10.1109/ALLERTON63246.2024.10735305,2024,"The advancement of Large Language Models (LLMs) has significantly impacted various domains, including Web search, healthcare, and software development. However, as these models scale, they become more vulnerable to cybersecurity risks, particularly backdoor attacks. By exploiting the potent memorization capacity of LLMs, adversaries can easily inject backdoors into LLMs by manipulating a small portion of training data, leading to malicious behaviors in downstream applications whenever the hidden backdoor is activated by the pre-defined triggers. Moreover, emerging learning paradigms like instruction tuning and reinforcement learning from human feedback (RLHF) exacerbate these risks as they rely heavily on crowdsourced data and human feedback, which are not fully controlled. In this paper, we present a comprehensive survey of emerging backdoor threats to LLMs that appear during LLM development or inference, and cover recent advancement in both defense and detection strategies for mitigating backdoor threats to LLMs. We also outline key challenges in addressing these threats, highlighting areas for future research."
PACGBI: A Pipeline for Automated Code Generation from Backlog Items,"Sarschar, M; Zhang, G; Nowak, A",10.1145/3691620.3695346,2024,"While there exist several tools to leverage Large Language Models (LLMs) for code generation, their capabilities are limited to the source code editor and are disconnected from the overall software development process. These tools typically generate standalone code snippets that still require manual integration into the codebase. There is still a lack of integrated solutions that seamlessly automate the entire development cycle, from backlog items to code generation and merge requests. We present the Pipeline for Automated Code Generation from Backlog Items (PACGBI), an LLM-assisted pipeline integrated into GitLab CI. PACGBI reads backlog items in the code repository, automatically generates the corresponding code, and creates merge requests for the generated changes. Our case study demonstrates the potential of PACGBI in automating agile software development processes, allowing parallelization of development and reduction of development costs. PACGBI can be utilized by software developers and enables non-technical stakeholders and designers by providing a holistic solution for using LLMs in software development. A screencast of this tool is available at https://youtu.be/TI53m-fIoyc, its source code at https://github.com/Masa-99/pacgbi."
PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs),"Nazzal, M; Khalil, I; Khreishah, A; Phan, N",10.1145/3658644.3690298,2024,"The capability of generating high-quality source code using large language models (LLMs) reduces software development time and costs. However, recent literature and our empirical investigation in this work show that while LLMs can generate functioning code, they inherently tend to introduce security vulnerabilities, limiting their potential. This problem is mainly due to their training on massive open-source corpora exhibiting insecure and inefficient programming practices. Therefore, automatic optimization of LLM prompts for generating secure and functioning code is a demanding need. This paper introduces PromSec, an algorithm for prompt optimization for secure and functioning code generation using LLMs. In PromSec, we combine 1) code vulnerability clearing using a generative adversarial graph neural network, dubbed as gGAN, to fix and reduce security vulnerabilities in generated codes and 2) code generation using an LLM into an interactive loop, such that the outcome of the gGAN drives the LLM with enhanced prompts to generate secure codes while preserving their functionality. Introducing a new contrastive learning approach in gGAN, we formulate the code-clearing and generation loop as a dual-objective optimization problem, enabling PromSec to notably reduce the number of LLM inferences. As a result, PromSec becomes a cost-effective and practical solution for generating secure and functioning codes. Extensive experiments conducted on Python and Java code datasets confirm that PromSec effectively enhances code security while upholding its intended functionality. Our experiments show that despite the comprehensive application of a state-of-the-art approach, it falls short in addressing all vulnerabilities within the code, whereas PromSec effectively resolves each of them. Moreover, PromSec achieves more than an order-of-magnitude reduction in operational time, number of LLM queries, and security analysis costs. Furthermore, prompts optimized with PromSec for a certain LLM are transferable to other LLMs across programming languages and generalizable to unseen vulnerabilities in training. This study presents an essential step towards improving the trustworthiness of LLMs for secure and functioning code generation, significantly enhancing their large-scale integration in real-world software code development practices."
RFLP Approach to DTT Divertor Fixation System Design Using the 3DExperience Platform,"Lanzotti, FG; Marzullo, D; Di Gironimo, G",10.1007/978-3-031-58094-9_32,2024,"The Product Lifecycle Management (PLM) systems lead the development of a product or a system from the early concept design until the end of the life allowing the knowledge management step by step. Among these systems, the 3DExperience platform adopts a Model Based Systems Engineering (MBSE) approach and allows to implement each phase of the design in a single co-simulation environment. This platform has been chosen for the development of the Divertor Tokamak Test (DTT) facility. The DTT tokamak, under construction at ENEA site in Frascati, has the main aim to contribute to the development of a reliable solution for power exhaust in a reactor, one of the major issues in the roadmap towards the realization of a nuclear fusion power plant. The divertor is one of the most challenging systems whose requirements coming from different physics and interfaces shall be balanced. The fixation system is the interface between the vacuum vessel and the divertor cassette body whose concept design has been carried out in a PLM system. This paper deals specifically with the application of the Requirement, Functional, Logical, Physical (RFLP) approach to the DTT divertor fixation system from the requirement elicitation and definition until the preliminary physical design, implementing each phase in the 3DExperience platform."
Sex hormone deprivation abolishes sex-specific differences in murine colon inflammation and related lipid mediator production,"Pace, S; Meyer, KPL; Troisi, F; Bilancia, R; D'Avino, D; Parisi, O; Rizza, R; Stiuso, P; Gerstmeier, J; Schaedel, P; Ialenti, A; Sautebin, L; Serhan, CN; Rossi, A; Borrelli, F; Werz, O",10.1096/fj.202400320R,2024,"Unresolved inflammation, due to unfavorable imbalances between pro-inflammatory and pro-resolving mediators, leads to chronic inflammatory pathologies that are often sex-biased and regulated by sex hormones, including inflammatory bowel disease. Lipid mediators (LM) produced from polyunsaturated fatty acids by various lipoxygenases (LOX) and cyclooxygenases govern all stages of inflammation, i.e., the initiation and progression by pro-inflammatory eicosanoids and its resolution by specialized pro-resolving mediators (SPM). Here, we reveal sex-specific differences in murine experimental colitis with male preponderance, which was abolished by sex hormone deprivation using gonadectomy, and this correlated to the levels of inflammation-relevant mediators in the colon. Oral dextran sodium sulfate administration caused more severe colon inflammation in male CD-1 mice than in female counterparts during the acute phase. Colitis in males yielded higher colonic cytokine/chemokine levels but lower 12-/15-LOX-derived LM including SPM compared to female animals in the resolving phase. Sex hormone deprivation in male mice by orchidectomy ameliorated colitis and impaired pro-inflammatory cytokine/chemokine levels but elevated 12-/15-LOX products including SPM, thus abolishing the observed sex differences. Conversely, ovariectomy impaired the levels of those LM that dominated in females and that were increased in males after gonadectomy. Our findings suggest that male sex hormones promote the development of colitis connected to the biosynthesis of inflammatory cytokines, chemokines, and certain LM, especially pro-resolving 12-/15-LOX products that appear to be suppressed in the male colon due to androgens. Female mice exhibit a lower susceptibility to develop colitis compared to male mice, which coincides with higher colonic levels of pro-resolving EPA and DHA metabolomes, especially 12-/15-lipoxygenase products in females during the resolving phase and that this is influenced by sex hormones. While in male mice androgens seem to worsen colitis along with low levels of pro-resolving 12-/15-lipoxygenase products, in females sex hormones might be colitis-protective and associated with elevated 12-/15-lipoxygenase-derived LM.image"
Dynamic changes in proresolving lipid mediators and their receptors following acute vascular injury in male rats,"Kagaya, H; Kim, AS; Chen, M; Lin, PY; Yin, XZ; Spite, M; Conte, MS",10.14814/phy2.16178,2024,"Acute vascular injury provokes an inflammatory response, resulting in neointimal hyperplasia (NIH) and downstream pathologies. The resolution of inflammation is an active process in which specialized proresolving lipid mediators (SPM) and their receptors play a central role. We sought to examine the acute phase response of SPM and their receptors in both circulating blood and the arterial wall in a rat angioplasty model. We found that the ratio of proresolving to pro-inflammatory lipid mediators (LM) in plasma decreased sharply 1 day after vascular injury, then increased slightly by day 7, while that in arteries remained depressed. Granulocyte expression of SPM receptors ALX/FPR2 and DRV2/GPR18, and a leukotriene B4 receptor BLT1 increased postinjury, while ERV1/ChemR23 expression was reduced early and then recovered by day 7. Importantly, we show unique arterial expression patterns of SPM receptors in the acute setting, with generally low levels through day 7 that contrasted sharply with that of the pro-inflammatory CCR2 receptor. Overall, these data document acute, time-dependent changes of LM biosynthesis and SPM receptor expression in plasma, leukocytes, and artery walls following acute vascular injury. A biochemical imbalance between inflammation and resolution LM pathways appears persistent 7 days after angioplasty in this model. These findings may help guide therapeutic approaches to accelerate vascular healing and improve the outcomes of vascular interventions for patients with advanced atherosclerosis."
Local large language models to simplify requirement engineering documents in the automotive industry,"Uygun, Y; Momodu, V",10.1080/21693277.2024.2375296,2024,"In automotive engineering, requirements management is crucial for determining the functional and technical qualities of a vehicle and ensuring reproducibility and uniformity throughout the development process. This paper presents a novel and innovative Local GPT Q&A retrieval solution for requirement engineering in the automotive industry. The study demonstrates that leveraging massive language models can significantly simplify the requirements analysis process, providing a more efficient and effective approach to handle complex requirement documents. The evaluation of various language models reveal their exceptional performance in answering evaluation questions, showcasing their potential for automating and enhancing requirement engineering tasks."
A Comparative Study of Contemporary Learning Paradigms in Bug Report Priority Detection,"Yilmaz, EH; Toroslu, IH; KÃ¶ksal,Ã",10.1109/ACCESS.2024.3451125,2024,"The increasing complexity of software development demands efficient automated bug report priority classification, and recent advancements in deep learning hold promise. This paper presents a comparative study of contemporary learning paradigms, including BERT, vector databases, large language models (LLMs), and a simple novel learning paradigm, contrastive learning for BERT. Utilizing datasets from bug reports, movie reviews, and app reviews, we evaluate and compare the performance of each approach. We find that transformer encoder-only models outperform in classification tasks measured by the precision, recall, and F1 score transformer decoder-only models despite an order of magnitude gap between the number of parameters. The novel use of contrastive learning for BERT demonstrates promising results in capturing subtle nuances in text data. This work highlights the potential of advanced NLP techniques for automated bug report priority classification and underscores the importance of considering multiple factors when developing models for this task. The paper's main contributions are a comprehensive evaluation of various learning paradigms, such as vector databases and LLMs, an introduction of contrastive learning for BERT, an exploration of applicability to other text classification tasks, and a contrastive learning procedure that exploits ordinal information between classes."
Strategic Digital Product Management in the Age of AI,"Olsson, HH; Bosch, J",10.1007/978-3-031-53227-6_24,2024,"The role of software product management is key for building, implementing and managing software products. However, although there is prominent research on software product management (SPM) there are few studies that explore how this role is rapidly changing due to digitalization and digital transformation of the software-intensive industry. In this paper, we study how key trends such as DevOps, data and artificial intelligence (AI), and the emergence of digital ecosystems are rapidly changing current SPM practices. Whereas earlier, product management was concerned with predicting the outcome of development efforts and prioritizing requirements based on these predictions, digital technologies require a shift towards experimental ways-of-working and hypotheses to be tested. To support this change, and to provide guidelines for future SPM practices, we first identify the key challenges that software-intensive embedded systems companies experience with regards to current SPM practices. Second, we present an empirically derived framework for strategic digital product management (SPM4AI) in which we outline what we believe are key practices for SPM in the age of AI."
Understanding the development of emerging complex intelligent systems,"Balachandran, A; Holmberg, G; Lakemond, N",10.1016/j.jengtecman.2024.101815,2024,"This paper explores the intricate emergence of complex and increasingly intelligent systems (CoIS) in the wake of new possibilities created by integrating artificial intelligence (AI) solutions, building on an analysis of the emergence of CoIS using perspectives of development and change. The findings, based on rich qualitative data collected through key informant interviews with reflective practitioners from aviation, automotive and naval system domains, indicate that firms facing the emergence of CoIS, need to build capabilities allowing several logics to co-exist in a newly evolving hybrid CoIS management logic."
Practical Software Development: Leveraging AI for Precise Cost Estimation in Lump-Sum EPC Projects,"Dzhusupova, R; Ya-alimadad, M; Shteriyanov, V; Bosch, J; Olsson, HH",10.1109/SANER60148.2024.00110,2024,"In the Engineering, Procurement, and Construction (EPC) sector, accurate cost estimations during the tendering phase are crucial for maintaining competitiveness, especially with constrained project schedules and rising labor expenses. Typically, these estimations are labor-intensive, relying heavily on manual evaluations of engineering drawings, which are often shared in PDF format due to intellectual property concerns. This study introduces an innovative solution tailored for the energy industry, utilizing Artificial Intelligence (AI) - primarily deep learning (DL) and machine learning (ML) techniques - to streamline material quantity estimation, thereby saving engineering time and costs. Built on empirical data from a large EPC company operating in the energy sector, AI-based product development experiences, and academic research, our approach aims to enhance the efficiency and accuracy of engineering work, promoting better decision-making and resource distribution. While our focus is on enhancing a particular activity within the case company using AI, the method's broader applicability in the EPC sector potentially benefits both industry professionals and researchers. This study not only advances a practical application but also provides valuable insights for those seeking to develop AI-driven solutions across various engineering disciplines."
Enhancing Agile Project Management Education with AI: ChatGPT-4's Role in Evaluating Student Contributions,"Miranda, D; Palma, D; FernÃ¡ndez, A; Noel, R; Cechinel, C; Munoz, R",10.1109/SCCC63879.2024.10767612,2024,"The planning poker estimation technique encourages all team members to participate equally, which is essential in the training of future software engineers. By proposing a coordination scheme based on the experience and knowledge of the team members, it enforces the common ownership of effort estimation. Thus, it is crucial that all members contribute to the process [10]. However, given the personal factors that could affect team interaction dynamics, the contributions of team members could not be equally distributed, hindering the goal of the technique. Ensuring the equal participation of team members sets a challenge not only in the professional context but also for training future software developers and team managers [18] that must facilitate team collaboration. Hence, it is vital to detect team members' contributions in order to value collaboration in a development team. In this article, we present the analyses of the interventions of 13 groups of students from the Computer Engineering course at the University of Valparaiso during a user story estimation activity using planning poker. The experimental setup involved computer science undergraduate students, performing a learning activity regarding the Planning Poker estimation technique. The students' interventions were classified according to a human expert following a collaboration framework. Subsequently, they were classified using ChatGPT 4 using the Zero Shot technique in order to compare the automatically generated labels with those provided by human experts. The type of classification used was binary to determine whether or not the intervention analysed was a contribution. The analysis focused on evaluating the accuracy and consistency of ChatGPT in the contribution classification task, considering the model's ability to correctly identify the different types of interventions. The results of this comparison demonstrate the effectiveness of ChatGPT and its potential to assist in realtime evaluation and analysis tasks. This study enhances the understanding of how artificial intelligence tools can complement the work of human experts, improving efficiency and accuracy in educational and agile project management activities."
Enhancing Commit Message Categorization in Open-Source Repositories Using Structured Taxonomy and Large Language Models,"Al-razgan, M; Alaqil, M; Almuwayshir, R; Alhijji, Z",10.54364/AAIML.2024.44171,2024,"Version Control Systems (VCS) manage source code changes by storing modifications in a database. A key feature of VCS is the commit function, which saves the project's current state and summarizes changes through Commit Message (CM). These messages are vital for collaboration, particularly in open-source artificial intelligence (AI) projects on platforms, where contributors work on rapidly evolving codebases. This paper presents an empirical analysis of CM within open-source AI repositories on GitHub, focusing on their content, the effectiveness of categorization by Large Language Models (LLMs), and the impact of message quality on categorization accuracy. A sample of 384 CMs from 34 repositories was manually categorized to establish a taxonomy. Python was then used for automated keyword extraction, refined with regex patterns. Also, an experiment involved assessing the performance of ChatGPT-4 in categorizing CMs, first without guidance and later using our developed taxonomy. Our findings indicate that the quality of CMs varies greatly, which has a clear impact on how efficiently they can be categorized. This study contributes to the field by providing a structured taxonomy of CMs and exploring how tools like ChatGPT-4 can be used to analyze them. The insights from this research are intended to benefit both academic studies and real-world software development, particularly by helping teams better understand and automate the handling of CM in AI projects."
Software Development Practices and Tools for University-Industry R&D projects,"Cruz, DT; Almeida, EP; Santos, JP ; Paixao, FDS; de Santana, EG ; Souza, RRGE; Iwamoto, HM; Durao, FA; Prazeres, CVS; Machado, IDC; Figueiredo, GB; Peixoto, MLM; de Almeida, ES",10.1145/3701625.3701627,2024,"Research and development (R&D) projects involving universities and industry drive innovation by bringing scientific knowledge closer to practical problems. Nevertheless, the partnership comes with challenges, such as high developer turnover, team members with diverse backgrounds and experience levels, and part-time contributors. In this paper, we report experiences in a large R&D project in the smart home field, in which we proposed software development practices and tools with the goal of promoting short feedback cycles and dissemination of best practices. By surveying the developers of this project, we discovered that the practices and tools were generally well accepted and also determined specific areas that need improvement. The insights collected in this study can be used by other teams conducting R&D projects."
Modelling of Carbon Monoxide and Suspended Particulate Matter Concentrations in a Rural Area Using Artificial Neural Networks,"Al-Sager, SM; Almady, SS; Al-Janobi, AA; Bukhari, AM; Abdel-Sattar, M; Al-Hamed, SA; Aboukarima, AM",10.3390/su16229909,2024,"Air pollution is a growing concern in rural areas where agricultural production can be reduced by it. This article analyses data obtained as part of a research project. The aim of this study is to understand the influence of atmospheric pressure, air temperature, air relative humidity, longitude and latitude of the location, and indoor and outdoor environment on local rural workplace diversity of air pollutants such as carbon monoxide (CO) and suspended particulate matter (SPM), as well as the contribution of these variables to changes in such air pollutants. The focus is on four topics: motivation, innovation and creativity, leadership, and social responsibility. Furthermore, this study developed an artificial neural network (ANN) model to predict CO and SPM concentrations in the air based on data collected from the mentioned inputs. The related sensors were assembled on an Arduino Mega 2560 board to form a field-portable device to detect air pollutants and meteorological parameters. The sensors included an MQ7 sensor for CO concentration measurement, a Sharp GP2Y1010AU0F dust sensor for SPM concentration measurement, a DHT11 sensor for air temperature and air relative humidity measurement, and a BMP180 sensor for air pressure measurements. The longitude and latitude of the location were measured using a smartphone. Measurements were conducted from 20 December 2021 to 16 July 2022. Results showed that the overall average outdoor CO and SPM concentrations were 10.97 ppm and 231.14 mu g/m3 air, respectively. The overall average indoor concentrations were 12.21 ppm and 233.91 mu g/m3 air for CO and SPM, respectively. Results showed that the ANN model demonstrated acceptable performance in predicting CO and SPM in both the training and testing phases, exhibiting a coefficient of determination (R2) of 0.575, a root mean square error (RMSE) of 1.490 ppm, and a mean absolute error (MAE) of 0.994 ppm for CO concentrations when applying the testing dataset. For SPM concentrations, the R2, RMSE, and MAE using the test dataset were 0.497, 30.301 mu g/m3 air, and 23.889 mu g/m3 air, respectively. The most influential input variable was air pressure, with contribution rates of 22.88% and 22.82% in predicting CO and SPM concentrations, respectively. The acceptable performance of the developed ANN model provides potential advances in air quality management and agricultural planning, enabling a more accurate and informed decision-making process regarding air pollution. The results of short-term estimation of CO and SPM concentrations suggest that the accuracy of the ANN model needs to be improved through more comprehensive data collection or advanced machine learning algorithms to improve the prediction results of these two air pollutants. Moreover, as even lower cost devices can predict CO and SPM concentrations, this study could lead to the development some kind of virtual sensor, as other air pollutants can be estimated from measurements of particulate matters."
Prompt Patterns for Agile Software Project Managers: First Results,"Sainio, K; Abrahamsson, P; Ahtee, T",10.1007/978-3-031-53227-6_14,2024,"In the evolving field of Agile Project Management (APM), the role of the project manager is in transition. This paper identifies common 'pain points' in APM through a literature review and constructs a theoretical model to address them. The study introduces 'Prompt Engineering' as a novel approach to leverage artificial intelligence (AI), specifically ChatGPT, for mitigating these challenges. Empirical research evaluates ChatGPT's capabilities and reliability in managing various project tasks using engineered prompts. The findings suggest that while ChatGPT cannot fully replace human project managers, it excels in assisting, guiding, and automating specific tasks when guided by well-crafted prompts. As an outcome, prompt engineering patterns for project managers is proposed to facilitate the application of AI in agile settings. In this paper, we introduce patterns for requirements management, stakeholder and management teams and role clarification. The paper concludes that ChatGPT's knowledge is generally reliable but emphasizes the need for expert evaluation in critical areas."
Towards supporting Software Engineering using Deep Learning: A case of Software Requirements Classification,"Navarro-Almanza, R; JuÃ¡rez-RamÃ­rez, R; Licea, G",10.1109/CONISOFT.2017.00021,2017,"Software Requirements are the basis of high-quality software development process, each step is related to SR, these represent the needs and expectations of the software in a very detailed form. The software requirement classification (SRC) task requires a lot of human effort, specially when there are huge of requirements, therefore, the automation of SRC have been addressed using Natural Language Processing (NLP) and Information Retrieval (IR) techniques, however, generally requires human effort to analyze and create features from corpus (set of requirements). In this work, we propose to use Deep Learning (DL) to classify software requirements without labor intensive feature engineering. The model that we propose is based on Convolutional Neural Network (CNN) that has been state of art in other natural language related tasks. To evaluate our proposed model, PROMISE corpus was used, contains a set of labeled requirements in functional and 11 different categories of nonfunctional requirements. We achieve promising results on SRC using CNN even without handcrafted features."
Increased 15-PGDH expression leads to dysregulated resolution responses in stromal cells from patients with chronic tendinopathy,"Dakin, SG; Ly, L; Colas, RA; Oppermann, U; Wheway, K; Watkins, B; Dalli, J; Carr, AJ",10.1038/s41598-017-11188-y,2017,"The mechanisms underpinning the failure of inflammation to resolve in diseased musculoskeletal soft tissues are unknown. Herein, we studied bioactive lipid mediator (LM) profiles of tendon-derived stromal cells isolated from healthy donors and patients with chronic tendinopathy. Interleukin(IL)-1 beta treatment markedly induced prostaglandin biosynthesis in diseased compared to healthy tendon cells, and up regulated the formation of several pro-resolving mediators including 15-epi-LXA4 and MaR1. Incubation of IL-1 beta stimulated healthy tendon cells with 15-epi-LXA4 or MaR1 down-regulated PGE2 and PGD2 production. When these mediators were incubated with diseased cells, we only found a modest down regulation in prostanoid concentrations, whereas it led to significant decreases in IL-6 and Podoplanin expression. In diseased tendon cells, we also found increased 15-Prostaglandin Dehydrogenase (15-PGDH) expression as well as increased concentrations of both 15-epi-LXA4 and MaR1 further metabolites, 15-oxo-LXA4 and 14-oxo-MaR1. Inhibition of 15-PGDH using either indomethacin or SW033291 significantly reduced the further conversion of 15-epi-LXA4 and MaR1 and regulated expression of IL-6, PDPN and STAT-1. Taken together these results suggest that chronic inflammation in musculoskeletal soft tissues may result from dysregulated LM-SPM production, and that inhibition of 15-PGDH activity together with promoting resolution using SPM represents a novel therapeutic strategy to resolve chronic tendon inflammation."
Natural Language Processing and Text Mining to Identify Knowledge Profiles for Software Engineering Positions,"Valdez-Almada, R; Rodriguez-Elias, OM; Rose-GÃ³mez, CE; VelÃ¡zquez-Mendoza, MD; GonzÃ¡lez-LÃ³pez, S",10.1109/CONISOFT.2017.00019,2017,"Organizations frequently report problems finding skillful people to cover their most knowledge intensive vacancies. Being software engineering positions some of the such kind of jobs, there is a considerable gap between job postings and hiring skillful engineers in many software engineering organizations. In this paper, we will introduce the prototype of a web application that helps identifying Technical Knowledge (TK) in software development, to serve as a tool in the hiring process of software engineering positions, and in talent management. The purpose of this tool is to do an initial screening when opening a job position. All this is accomplished using Natural Language Processing (NLP) and Text Mining (TM) to analyze unstructured text in resumes and curriculum. We propose a way to use NLP and TM to identify knowledge profiles for Software Engineering Positions."
Comparative Analysis of Solving Traveling Salesman Problem using Artificial Intelligence Algorithms,"Brucal, SGE; Dadios, EP",,2017,"This paper aims to provide a comparative study of the different artificial intelligence (AI) algorithms applied to solve the traveling salesman problem (TSP). Four (4) AI algorithms such as genetic algorithm, nearest neighbor, ant colony optimization, and neuro-fuzzy are executed in MatLab software to determine which among these techniques will provide the least execution time to solve a TSP. The objective of comparing and analyzing each AI algorithm -as applied to a single problem with the different program execution -is to identify if significant difference in execution time could lead to significant saving in energy consumption. The simulations using MatLab resulted to strong correlation at an R-2 of 0.95 in the average execution time with the number of code lines, but do not give a significant execution time variance as when ANOVA and ttest measures were performed. The result of this paper could be used as a basis in the design phase of software development life cycle to arrive into an energy efficient software application with respect to time needed to execute a program."
Analyzing the Potency of Pretrained Transformer Models for Automated Program Repair,"Leiwig, M; Swierzy, B; Bungartz, C; Meier, M",10.1109/SEAA64295.2024.00020,2024,"Manually finding and fixing bugs is cumbersome work, which consumes valuable resources in the software development cycle. In this work, we examine the capability of pretrained transformer models to tackle the task of automated program repair. Previous research has been focused on inherently different machine learning architectures for solving this use case. Our contributions include a novel dataset for fine-tuning the models, the introduction of a windowing technique augmenting the pretrained model and the evaluation on the commonly used Defects4J benchmark along with an ablation study. The findings demonstrate that leveraging our dataset leads to enhanced model performance surpassing Bugs2Fix. Our model enhancements significantly boost overall performance, enabling resulting models to achieve parity with the current state of the art by fixing 30 bugs in 27 minutes on Defects4J. This shows that pretrained transformers are promising for the task of automated bug fixing and should be considered by future research. However, similar to the existing state-of-the-art solutions, the performance still needs be improved to provide practical benefits to end users."
ChatDev: Communicative Agents for Software Development,"Qian, C; Liu, W; Liu, HZ; Chen, N; Dang, YF; Li, JH; Yang, C; Chen, WZ; Su, YS; Cong, X; Xu, JY; Li, DH; Liu, ZY; Sun, MS",,2024,"Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev."
"Generative Artificial Intelligence Assistants in Software Development Education: A Vision for Integrating Generative Artificial Intelligence Into Educational Practice, Not Instinctively Defending Against It","Bull, C; Kharrufa, A",10.1109/MS.2023.3300574,2024,The use of Generative AI in software development is gaining traction. But what are the potentials and implications on software development education? We gathered insights on the use of Generative AI from professional software developers and make some pedagogical recommendations.
Effective test generation using pre-trained Large Language Models and mutation testing,"Dakhel, AM; Nikanjam, A; Majdinasab, V; Khomh, F; Desmarais, MC",10.1016/j.infsof.2024.107468,2024,"Context: One of the critical phases in the software development life cycle is software testing. Testing helps with identifying potential bugs and reducing maintenance costs. The goal of automated test generation tools is to ease the development of tests by suggesting efficient bug-revealing tests. Recently, researchers have leveraged Large Language Models (LLMs) of code to generate unit tests. While the code coverage of generated tests was usually assessed, the literature has acknowledged that the coverage is weakly correlated with the efficiency of tests in bug detection. Objective: To improve over this limitation, in this paper, we introduce MuTAP (Mutation Mu tation T est case generation using A ugmented P rompt) for improving the effectiveness of test cases generated by LLMs in terms of revealing bugs by leveraging mutation testing. Methods: Our goal is achieved by augmenting prompts with surviving mutants, as those mutants highlight the limitations of test cases in detecting bugs. MuTAP is capable of generating effective test cases in the absence of natural language descriptions of the Program Under Test (PUTs). We employ different LLMs within MuTAP and evaluate their performance on different benchmarks. Results: Our results show that our proposed method is able to detect up to 28% more faulty human-written code snippets. Among these, 17% remained undetected by both the current state-of-the-art fully-automated test generation tool (i.e., Pynguin) and zero-shot/few-shot learning approaches on LLMs. Furthermore, MuTAP achieves a Mutation Score (MS) of 93.57% on synthetic buggy code, outperforming all other approaches in our evaluation. Conclusion: Our findings suggest that although LLMs can serve as a useful tool to generate test cases, they require specific post-processing steps to enhance the effectiveness of the generated test cases which may suffer from syntactic or functional errors and may be ineffective in detecting certain types of bugs and testing corner cases in PUT s."
Prompt-Enhanced Software Vulnerability Detection Using ChatGPT,"Zhang, CY; Liu, H; Zeng, JT; Yang, KJ; Li, YH; Li, H",10.1145/3639478.3643065,2024,"With the increase in software vulnerabilities that cause significant economic and social losses, automatic vulnerability detection has become essential in software development and maintenance. Recently, large language models (LLMs) have received considerable attention due to their stunning intelligence, and some studies consider using ChatGPT for vulnerability detection. However, they do not fully consider the characteristics of LLMs, since their designed questions to ChatGPT are simple without a prompt design tailored for vulnerability detection. This paper launches a study on the performance of software vulnerability detection using ChatGPT with different prompt designs. Firstly, we complement previous work by applying various improvements to the basic prompt. Moreover, we incorporate structural and sequential auxiliary information to improve the prompt design. Moreover, we leverage ChatGPT's ability of memorizing multi-round dialogue to design suitable prompts for vulnerability detection. We conduct extensive experiments on two vulnerability datasets to demonstrate the effectiveness of prompt-enhanced vulnerability detection using ChatGPT."
Accelerating Software Development Using Generative AI: ChatGPT Case Study,"Rajbhoj, A; Somase, A; Kulkarni, P; Kulkarni, V",10.1145/3641399.3641403,2024,"The Software Development Life Cycle (SDLC) comprises multiple phases, each requiring Subject Matter Experts (SMEs) with phase-specific skills. The efficacy and quality of deliverables of each phase are skill dependent. In recent times, Generative AI techniques, including Large-scale Language Models (LLMs) like GPT, have become significant players in software engineering. These models, trained on extensive text data, can offer valuable contributions to software development. Interacting with LLMs involves feeding prompts with the context information and guiding the generation of textual responses. The quality of the response is dependent on the quality of the prompt given. This paper proposes a systematic prompting approach based on meta-model concepts for SDLC phases. The approach is validated using ChatGPT for small but complex business application development. We share the approach and our experience, learnings, benefits obtained, and the challenges encountered while applying the approach using ChatGPT. Our experience indicates that Generative AI techniques, such as ChatGPT, have the potential to reduce the skills barrier and accelerate software development substantially."
Artificial Intelligence Applied to Software Testing: A Tertiary Study,"Amalfitano, D; Faralli, S; Hauck, JCR; Matalonga, S; Distante, D",10.1145/3616372,2024,"Context: Artificial intelligence (AI) methods and models have extensively been applied to support different phases of the software development lifecycle, including software testing (ST). Several secondary studies investigated the interplay between AI and ST but restricted the scope of the research to specific domains or sub-domains within either area. Objective: This research aims to explore the overall contribution of AI to ST, while identifying the most popular applications and potential paths for future research directions. Method: We executed a tertiary study following well-established guidelines for conducting systematic literature mappings in software engineering and for answering nine research questions. Results: We identified and analyzed 20 relevant secondary studies. The analysis was performed by drawing from well-recognized AI and ST taxonomies and mapping the selected studies according to them. The resulting mapping and discussions provide extensive and detailed information on the interplay between AI and ST. Conclusion: The application of AI to support ST is a well-consolidated and growing interest research topic. The mapping resulting from our study can be used by researchers to identify opportunities for future research, and by practitioners looking for evidence-based information on which AI-supported technology to possibly adopt in their testing processes."
Fairness-aware machine learning engineering: how far are we?,"Ferrara, C; Sellitto, G; Ferrucci, F; Palomba, F; De Lucia, A",10.1007/s10664-023-10402-y,2024,"Machine learning is part of the daily life of people and companies worldwide. Unfortunately, bias in machine learning algorithms risks unfairly influencing the decision-making process and reiterating possible discrimination. While the interest of the software engineering community in software fairness is rapidly increasing, there is still a lack of understanding of various aspects connected to fair machine learning engineering, i.e., the software engineering process involved in developing fairness-critical machine learning systems. Questions connected to the practitioners' awareness and maturity about fairness, the skills required to deal with the matter, and the best development phase(s) where fairness should be faced more are just some examples of the knowledge gaps currently open. In this paper, we provide insights into how fairness is perceived and managed in practice, to shed light on the instruments and approaches that practitioners might employ to properly handle fairness. We conducted a survey with 117 professionals who shared their knowledge and experience highlighting the relevance of fairness in practice, and the skills and tools required to handle it. The key results of our study show that fairness is still considered a second-class quality aspect in the development of artificial intelligence systems. The building of specific methods and development environments, other than automated validation tools, might help developers to treat fairness throughout the software lifecycle and revert this trend."
Leveraging Generative AI Tools to Support the Development of Digital Solutions in Health Care Research: Case Study,"Rodriguez, DV; Lawrence, K; Gonzalez, J; Brandfield-Harvey, B; Xu, L; Tasneem, S; Levine, DL; Mann, D",10.2196/52885,2024,"Background: Generative artificial intelligence has the potential to revolutionize health technology product development by improving coding quality, efficiency, documentation, quality assessment and review, and troubleshooting. Objective: This paper explores the application of a commercially available generative artificial intelligence tool (ChatGPT) to the development of a digital health behavior change intervention designed to support patient engagement in a commercial digital diabetes prevention program. Methods: We examined the capacity, advantages, and limitations of ChatGPT to support digital product idea conceptualization, intervention content development, and the software engineering process, including software requirement generation, software design, and code production. In total, 11 evaluators, each with at least 10 years of experience in fields of study ranging from medicine and implementation science to computer science, participated in the output review process (ChatGPT vs human -generated output). All had familiarity or prior exposure to the original personalized automatic messaging system intervention. The evaluators rated the ChatGPT-produced outputs in terms of understandability, usability, novelty, relevance, completeness, and efficiency. Results: Most metrics received positive scores. We identified that ChatGPT can (1) support developers to achieve high -quality products faster and (2) facilitate nontechnical communication and system understanding between technical and nontechnical team members around the development goal of rapid and easy -to -build computational solutions for medical technologies. Conclusions: ChatGPT can serve as a usable facilitator for researchers engaging in the software development life cycle, from product conceptualization to feature identification and user story development to code generation. Trial Registration: ClinicalTrials.gov NCT04049500; https://clinicaltrials.gov/ct2/show/NCT04049500"
How artificial intelligence will transform project management in the age of digitization: a systematic literature review,"Nenni, ME; De Felice, F; De Luca, C; Forcina, A",10.1007/s11301-024-00418-z,2025,"Among the causes of the low success rate of the projects (around 35% of the total) is the low level of maturity of the technologies available for the management of the projects themselves. However, today many researchers, startups and innovative companies are starting to apply artificial intelligence (AI), machine learning and other advanced technologies to the field of project management. By 2030 the industry will undergo significant changes. By using the Preferred Reporting Items for Systematic Review and Meta-Analyses (PRISMA) protocol this paper explores the intersection of project risk management and AI. The study highlights how AI-driven methodologies and tools can revolutionize the way project risks are managed throughout the project lifecycle. Specifically, 215 papers have been analysed to explore how the scientific community has been moving so far on the topic. Besides, a cross-sectional investigation of the PM processes and AI categories/tools was carried out to identify any path that is prevalent, where the prevalence comes from, and for which PM process or sector it is most successful. Finally, from this study several gaps emerged that scientific research would have to fill to effectively implement AI in PM and that have been turned into opportunities for future research in the form of a research agenda."
Strategies and Outcomes of Building a Successful University Research and Innovation Ecosystem,"Haidegger, TP; Galambos, P; Tar, JK; KovÃ¡cs, LA; Kozlovszky, M; Zrubka, Z; Eigner, G; Drexler, DA; SzakÃ¡l, A; Reicher, V; ArendÃ¡s, C; Tarsoly, S; GaramvÃ¶lgyi, T; Rudas, IJ",,2024,"& Oacute;buda University has created a regionally unique innovation ecosystem around the traditional university structure and organization. This includes the 7 faculties, the University Research and Innovation Center (EKIK), an independent academic organizational unit, 3 Science and Innovation Parks, Obuda Uni Venture Capital investment company and the Initium Venture Labs technology transfer company. Estalbished by Professor Imre Rudas, EKIK's main task remains the structured implementation of strategic research and innovation tasks at the University. It was established by the funding Rector in 2012, and has been growing dynamically eversince, becoming an integral part of the main campus. EKIK's activities include basic research focusing on the PhD students, applied research involving the regional industrial partners, while developing and co-creating the university innovation strategy and innovation ecosystem. EKIK operates under the direct supervision of the rector and cooperates with the Faculties in the implementation of R&D+I activities. The purpose of the key EKIK projects is to undertake and manage interdisciplinarity along the strategic scientific topic areas of the University, thus providing priority and optimal implementation conditions for the research community, in accordance with the University's Institutional Development Plan. Beyond the research centers, the functional units of EKIK include the Innovation Office, and back in the day, it had incorporated the Science Organization Office and the Electronic and Digital Learning Materials Office. More recently, Obuda University decided to enhance its innovation capabilities through establishing a technology transfer company, named Initium LLC, completing the investment capabilities of the university."
"Evaluation of Rhinoplasty Information from ChatGPT, Gemini, and Claude for Readability and Accuracy","Meyer, MKR; Kandathil, CK; Davis, SJ; Durairaj, KK; Patel, PN; Pepper, JP; Spataro, EA; Most, SP",10.1007/s00266-024-04343-0,2025,"ObjectiveAssessment of the readability, accuracy, quality, and completeness of ChatGPT (Open AI, San Francisco, CA), Gemini (Google, Mountain View, CA), and Claude (Anthropic, San Francisco, CA) responses to common questions about rhinoplasty.MethodsTen questions commonly encountered in the senior author's (SPM) rhinoplasty practice were presented to ChatGPT-4, Gemini and Claude. Seven Facial Plastic and Reconstructive Surgeons with experience in rhinoplasty were asked to evaluate these responses for accuracy, quality, completeness, relevance, and use of medical jargon on a Likert scale. The responses were also evaluated using several readability indices.ResultsChatGPT achieved significantly higher evaluator scores for accuracy, and overall quality but scored significantly lower on completeness compared to Gemini and Claude. All three chatbot responses to the ten questions were rated as neutral to incomplete. All three chatbots were found to use medical jargon and scored at a college reading level for readability scores.ConclusionsRhinoplasty surgeons should be aware that the medical information found on chatbot platforms is incomplete and still needs to be scrutinized for accuracy. However, the technology does have potential for use in healthcare education by training it on evidence-based recommendations and improving readability.Level of Evidence VThis journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266."
Exploiting Blockchain to Make AI Trustworthy: A Software Development Lifecycle View,"Zhang, PY; Ding, S; Zhao, QL",10.1145/3614424,2024,"Artificial intelligence (AI) is a very powerful technology and can be a potential disrupter and essential enabler. As AI expands into almost every aspect of our lives, people raise serious concerns about AI misbehaving and misuse. To address this concern, international organizations have put forward ethics guidelines for constructing trustworthy AI (TAI), including privacy, transparency, fairness, robustness, accountability, and so on. However, because of the black-box characteristics and complex models of AI systems, it is challenging to translate these guiding principles and aspirations into AI systems. Blockchain, an important decentralized technology, can provide the capabilities of transparency, traceability, immutability, and secure sharing and hence can be used to make AI trustworthy. In this paper, we survey studies on blockchain-based TAI (BTAI) from a software development lifecycle view. We classify the lifecycle of BTAI into four stages: Planning, data collection, model development, and system deployment/use. Particularly, we investigate and summarize the trustworthy issues that blockchain can achieve in the latter three stages, including (1) data transparency, privacy, and accountability; (2) model transparency, privacy, robustness, and fairness; and (3) robustness, privacy, transparency, and fairness of system deployment/use. Finally, we present essential open research issues and future work on developing BTAI systems."
Quality Assessment of ChatGPT Generated Code and their Use by Developers,"Siddiq, ML; Roney, L; Zhang, JH; Santos, JCS",10.1145/3643991.3645071,2024,"The release of large language models (LLMs) like ChatGPT has revolutionized software development. Prior works explored ChatGPT's generated response quality, the effectiveness of different prompting techniques, its performance in programming contests, etc. However, there is limited information regarding the practical usage of ChatGPT by software developers. This data mining challenge focuses on DevGPT, a curated dataset of developer-ChatGPT conversations encompassing prompts with ChatGPT's responses, including code snippets. Our paper leverages this dataset to investigate (RQ1) whether ChatGPT generates Python & Java code with quality issues; (RQ2) whether ChatGPT-generated code is merged into a repository, and, if it does, to what extent developers change them; and (RQ3) what are the main use cases for ChatGPT besides code generation. We found that ChatGPT-generated code suffers from using undefined/unused variables and improper documentation. They also have security issues related to improper resources and exception management. Our results show that ChatGPT-generated codes are hardly merged, and they are significantly modified before merging. Based on an analysis of developers' discussions and the developer-ChatGPT chats, we found that developers use ChatGPT for every stage of software development and leverage it to learn about new frameworks and development kits."
How AI shapes greener futures: Comparative insights from equity vs debt investment responses in renewable energy,"Wen, J; Yin, HT; Chang, CP; Tang, K",10.1016/j.eneco.2024.107700,2024,"This paper offers insights regarding the potential of AI software development to narrow the financing gap in renewables. By employing a panel of 49 economies covering 2011 -2020, we estimate a two-way fixed effects model and reveal that AI software development significantly promotes equity investments in renewables while imposing no substantial effect on debt investments in the same field. Such results are robust to extra controls, outlier consideration, and the endogeneity concern. Moreover, it is found that AI software development 's enhancing effect on equity investments in renewables manifests when the stringency of environmental policies, especially the intensity of public funding support for environmental-related R &D, is sufficiently high. Furthermore, AI software development has a more profound positive impact on equity investments in renewables in economies with more equal business opportunities and lower age dependency."
Significant Productivity Gains through Programming with Large Language Models,"Weber, T; Brandmaier, M; Schmidt, A; Mayer, S",10.1145/3661145,2024,"Large language models like GPT and Codex drastically alter many daily tasks, including programming, where they can rapidly generate code from natural language or informal specifications. Thus, they will change what it means to be a programmer and how programmers act during software development. This work explores how AI assistance for code generation impacts productivity. In our user study (N=24), we asked programmers to complete Python programming tasks supported by a) an auto-complete interface using GitHub Copilot, b) a conversational system using GPT-3, and c) traditionally with just the web browser. Aside from significantly increasing productivity metrics, participants displayed distinctive usage patterns and strategies, highlighting that the form of presentation and interaction affects how users engage with these systems. Our findings emphasize the benefits of AI-assisted coding and highlight the different design challenges for these systems."
Statistical Process Monitoring from Industry 2.0 to Industry 4.0: Insights into Research and Practice,"Colosimo, BM; Jones-Farmer, LA; Megahed, FM; Paynabar, K; Ranjan, C; Woodall, WH",10.1080/00401706.2024.2327341,2024,"Industry 4.0 has emerged as an important era for process monitoring and improvement. Our expository paper provides a historical perspective on research and practice of statistical process monitoring (SPM) from the 1920s to the present to bring a high-level view of current practice and research directions. We focus on the Industry 4.0 era, which began around 2011 with the introduction of cyber-physical systems and the growth of the Internet of Things. These technological changes have brought tremendous challenges and opportunities to SPM that can only be met with new paradigms for the problems we aim to solve and the approaches we use to evaluate SPM methodology. We provide our perspective on these challenges, primarily focusing on industrial applications. We give recommendations on the evaluation and comparison of monitoring methods to improve the usefulness of research in this area."
Voltammetry and Related Electrochemical Methods Based on Low-Cost Instrumentation: a Review from Basic to Advanced,"VilasÃ³-Cadre, JE; Reyes-DomÃ­nguez, IA; GonzÃ¡lez-Fontanet, JG; Hidalgo-Viteri, J; GonzÃ¡lez-FernÃ¡ndez, LA; Arada-PÃ©rez, MD; Turdean, GL",10.1134/S1061934824050150,2024,"This paper presents a review of voltammetric and related methods implemented with low-cost, mainly lab-made instrumentation. In recent years, there has been a growing interest in developing low-cost electrochemical instrumentation. Advances in electronics and software have made it easier to access high-performance components for the construction of potentiostats. Currently, lab-made instrumentation for electrochemical research can be very simple, with minimal components to perturb the system and record the response signal. It can also be very advanced, using high-performance electronic components and software development. Several authors have presented the feasibility of designing and manufacturing electrochemical instruments for voltammetry, chronoamperometry, chronopotentiometry, and electrodeposition. This review presents some available options, ranging from basic to advanced. The aim is to provide readers with the full range of possibilities for using their own instruments, especially in cases where access to commercial apparatus is difficult or where specific applications are needed, for example in the Internet of Things or artificial intelligence."
ChatGPT for Software Development: Opportunities and Challenges,"Rahmaniar, W",10.1109/MITP.2024.3379831,2024,"Rapid natural language processing advances, such as OpenAI's ChatGPT, promise profound transformations across multiple domains, including software development. This article discusses ChatGPT's role in software engineering, including an investigation of implications and applications highlighting ChatGPT's code-assistance capabilities. Through a series of analyses, we discuss the real impact of ChatGPT on open source software development. However, apart from offering efficiency and innovation, ChatGPT mandates a careful and well-informed approach to integration into software development paradigms."
Navigating autonomy and control in human-AI delegation: User responses to technology- versus user-invoked task allocation,"Adam, M; Diebel, C; Goutier, M; Benlian, A",10.1016/j.dss.2024.114193,2024,"Users can increasingly delegate to information systems (IS) - that is transferring rights and responsibilities regarding certain tasks - even to the degree that IS can act autonomously (i.e., without the intervention or supervision of users). What is more, IS increasingly offer to assume the rights and responsibilities for a task not only in response to user prompts (i.e., user-invoked delegation) but also without user prompts (i.e., IS-invoked delegation). Yet, little is known about whether, how, and why users agree to delegation when they are asked by the IS in contrast to when they self-initiate the delegation. Drawing on self-affirmation theory, we investigate user acceptance of IS- versus user-invoked delegation in two complementary online experiments in software development. Our core findings reveal that IS-invoked (vs. user-invoked) delegation increases users' perceived self-threat and thus decreases their willingness to accept delegation. This threatening effect is larger the less (vs. more) the user perceives control after the potential delegation. Taken together, we uncover defensive user responses to IS-invoked delegation. Furthermore, we shed light on the underlying and moderating mechanisms representing the reasons and contextual features that explain and mitigate these defensive measures. These findings have significant implications for IS designers seeking to improve user-IS collaboration and outcomes by employing IS-invoked delegation."
The Role of Generative AI in Software Development Productivity: A Pilot Case Study,"Coutinho, M; Marques, L; Santos, A; Dahia, M; FranÃ§a, C; Santos, RD",10.1145/3664646.3664773,2024,"With software development increasingly reliant on innovative technologies, there is a growing interest in exploring the potential of generative AI tools to streamline processes and enhance productivity. In this scenario, this paper investigates the integration of generative AI tools within software development, focusing on understanding their uses, benefits, and challenges to software professionals, in particular, looking at aspects of productivity. Through a pilot case study involving software practitioners working in different roles, we gathered valuable experiences on the integration of generative AI tools into their daily work routines. Our findings reveal a generally positive perception of these tools in individual productivity while also highlighting the need to address identified limitations. Overall, our research sets the stage for further exploration into the evolving landscape of software development practices with the integration of generative AI tools."
ChatGPT in Action: Analyzing Its Use in Software Development,"Champa, AI; Rabbi, MF; Nachuma, C; Zibran, MF",10.1145/3643991.3645077,2024,"The emergence of AI tools such as ChatGPT is being used to assist with software development, but little is known of how developers utilize these tools as well as the capabilities of these tools in software engineering tasks. Using the DevGPT dataset, we conduct quantitative analyses of the tasks developers seek assistance from ChatGPT and how effectively ChatGPT addresses them. We also examine the impact of initial prompt quality on conversation length. The findings reveal where ChatGPT is most and least suited to assist in the identified 12 software development tasks. The insights from this research would guide the software developers, researchers, and AI tool providers in optimizing these tools for more effective programming aid."
Facial emotion recognition through artificial intelligence,"Ballesteros, JA; Ramirez, GM; Moreira, F; Solano, A; Pelaez, CA",10.3389/fcomp.2024.1359471,2024,"This paper introduces a study employing artificial intelligence (AI) to utilize computer vision algorithms for detecting human emotions in video content during user interactions with diverse visual stimuli. The research aims to unveil the creation of software capable of emotion detection by leveraging AI algorithms and image processing pipelines to identify users' facial expressions. The process involves assessing users through images and facilitating the implementation of computer vision algorithms aligned with psychological theories defining emotions and their recognizable features. The study demonstrates the feasibility of emotion recognition through convolutional neural networks (CNN) and software development and training based on facial expressions. The results highlight successful emotion identification; however, precision improvement necessitates further training for contexts with more diverse images and additional algorithms to distinguish closely related emotional patterns. The discussion and conclusions emphasize the potential of A.I. and computer vision algorithms in emotion detection, providing insights into software development, ongoing training, and the evolving landscape of emotion recognition technology. Further training is necessary for contexts with more diverse images, alongside additional algorithms that can effectively distinguish between facial expressions depicting closely related emotional patterns, enhancing certainty and accuracy."
An Empirical Study of the Code Generation of Safety-Critical Software Using LLMs,"Liu, MX; Wang, JF; Lin, T; Ma, Q; Fang, ZY; Wu, YQ",10.3390/app14031046,2024,"In the digital era of increasing software complexity, improving the development efficiency of safety-critical software is a challenging task faced by academia and industry in domains such as nuclear energy, aviation, the automotive industry, and rail transportation. Recently, people have been excited about using pre-trained large language models (LLMs) such as ChatGPT and GPT-4 to generate code. Professionals in the safety-critical software field are intrigued by the code generation capabilities of LLMs. However, there is currently a lack of systematic case studies in this area. Aiming at the need for automated code generation in safety-critical domains such as nuclear energy and the automotive industry, this paper conducts a case study on generating safety-critical software code using GPT-4 as the tool. Practical engineering cases from the industrial domain are employed. We explore different approaches, including code generation based on overall requirements, specific requirements, and augmented prompts. We propose a novel prompt engineering method called Prompt-FDC that integrates basic functional requirements, domain feature generalization, and domain constraints. This method improves code completeness from achieving 30% functions to 100% functions, increases the code comment rate to 26.3%, and yields better results in terms of code compliance, readability, and maintainability. The code generation approach based on LLMs also introduces a new software development process and V-model lifecycle for safety-critical software. Through systematic case studies, we demonstrate that, with appropriate prompt methods, LLMs can auto-generate safety-critical software code that meets practical engineering application requirements. It is foreseeable that LLMs can be applied to various engineering domains to improve software safety and development efficiency."
Automated Generation and Analysis of Molecular Images Using Generative Artificial Intelligence Models,"Zhu, ZW; Lu, JY; Yuan, SX; He, Y; Zheng, FR; Jiang, H; Yan, YY; Sun, Q",10.1021/acs.jpclett.3c03504,2024,"The development of scanning probe microscopy (SPM) has enabled unprecedented scientific discoveries through high-resolution imaging. Simulations and theoretical analysis of SPM images are equally important as obtaining experimental images since their comparisons provide fruitful understandings of the structures and physical properties of the investigated systems. So far, SPM image simulations are conventionally based on quantum mechanical theories, which can take several days in tasks of large-scale systems. Here, we have developed a scanning tunneling microscopy (STM) molecular image simulation and analysis framework based on a generative adversarial model, CycleGAN. It allows efficient translations between STM data and molecular models. Our CycleGAN-based framework introduces an approach for high-fidelity STM image simulation, outperforming traditional quantum mechanical methods in efficiency and accuracy. We envision that the integration of generative networks and high-resolution molecular imaging opens avenues in materials discovery relying on SPM technologies."
SOH prediction of lithium-ion batteries using a hybrid model approach integrating single particle model and neural networks,"Zhou, D; Liang, JL; Li, FX; Cui, YX; Shan, YX; Zhang, YH; Chen, MH; Li, S",10.1016/j.est.2024.114579,2024,"The prediction of battery state of health (SOH) plays a vital role in battery management systems. A fusion model framework was proposed by integrating an improved single-particle model (SPM) with data-driven deep learning algorithms to enhance predictive accuracy and further elucidate the intrinsic mechanisms of battery aging. First, seven electrochemical features were extracted by the improved SPM, which exhibits a significant reduction in computational complexity compared to conventional electrochemical models. The validity of the extracted features was further verified through the utilization of differential voltage analysis (DVA). Second, a hybrid model was constructed which combines temporal convolutional network (TCN) and bidirectional long short-term memory network (BiLSTM). The effectiveness and superiority of the proposed model was demonstrated, with the full electrochemical features, on Oxford University dataset. Finally, experimental measurements were conducted on five different batteries with two different electrode materials combinations to further study SOH estimation across battery types. To address the forecasting challenges arising from data scarcity for a new type of battery, transfer learning was introduced. The results highlight the potential of this fusion framework to achieve more efficient and accurate SOH prediction."
Super SDKs: Tracking personal data and platform monopolies in the mobile,"Pybus, J; CotÃ©, M",10.1177/20539517241231270,2024,"In this article we address the question 'what is tracking in the mobile ecosystem' through a comprehensive overview of the Software Development Kit (SDK). Our research reveals a complex infrastructural role for these technical objects connecting end-user data with app developers, third parties and dominant advertising platforms like Google and Facebook. We present an innovative theoretical framework which we call a data monadology to foreground this interrelationship, predicated on an economic model that exchanges personal data for the infrastructural services used to build applications. Our main contribution is an SDK taxonomy, which renders them more transparent and observable. We categorise SDK services into three main categories: (i) Programmatic AdTech for monetisation; (ii) App Development, for building, maintaining and offering additional artificial intelligence features and (iii) App Extensions which more visibly embed third parties into apps like maps, wallets or other payment services. A major finding of our analysis is the special category of the Super SDK, reserved for platforms like Google and Facebook. Not only do they offer a vast array of services across all three categories, making them indispensable to developers, they are super conduits for personal data and the primary technical means for the expansion of platform monopolisation across the mobile ecosystem."
Advances in automated support for requirements engineering: a systematic literature review,"Umar, MA; Lano, K",10.1007/s00766-023-00411-0,2024,"Requirements Engineering (RE) has undergone several transitions over the years, from traditional methods to agile approaches emphasising increased automation. In many software development projects, requirements are expressed in natural language and embedded within large volumes of text documents. At the same time, RE activities aim to define software systems' functionalities and constraints. However, manually executing these tasks is time-consuming and prone to errors. Numerous research efforts have proposed tools and technologies for automating RE activities to address this challenge, which are documented in published works. This review aims to examine empirical evidence on automated RE and analyse its impact on the RE sub-domain and software development. To achieve our goal, we conducted a Systematic Literature Review (SLR) following established guidelines for conducting SLRs. We aimed to identify, aggregate, and analyse papers on automated RE published between 1996 and 2022. We outlined the output of the support tool, the RE phase covered, levels of automation, development approach, and evaluation approaches. We identified 85 papers that discussed automated RE from various perspectives and methodologies. The results of this review demonstrate the significance of automated RE for the software development community, which has the potential to shorten development cycles and reduce associated costs. The support tools primarily assist in generating UML models (44.7%) and other activities such as omission of steps, consistency checking, and requirement validation. The analysis phase of RE is the most widely automated phase, with 49.53% of automated tools developed for this purpose. Natural language processing technologies, particularly POS tagging and Parser, are widely employed in developing these support tools. Controlled experimental methods are the most frequently used (48.2%) for evaluating automated RE tools, while user studies are the least employed evaluation method (8.2%). This paper contributes to the existing body of knowledge by providing an updated overview of the research literature, enabling a better understanding of trends and state-of-the-art practices in automated RE for researchers and practitioners. It also paves the way for future research directions in automated requirements engineering."
Evaluating Success Factors of Software Project Management in Global Software Development,"Alqahtani, J; Siddique, A; Aseere, AM; Alasiry, A; Naveed, QN",10.1109/ACCESS.2024.3360415,2024,"At present, global software development (GSD) is gaining considerable attention in the realm of software engineering. The project management of global software projects presents substantial complexity owing to several inherent challenges of GSD. The software project management practices employed for in-house development appear inadequate to address the unique challenges posed by global software projects, making their management a formidable task. Software organizations rely on traditional software project management practices to manage global projects, often resulting in impairments or failures. This paper explores the critical success factors (CSFs) in software project management for global projects by developing a framework for effective project management within the context of GSD. The study focuses on identifying and prioritizing CSFs in software project management within a GSD setting utilizing multi-criteria decision-making (MCDM) analysis methods. Therefore, the present research provides an extensive literature review of CSFs in software project management within GSD. Additionally, the research applies the combinatorial approach to assess the various dimensions and CSFs of software project management in GSD. The proposed approach aids in measuring and comparing the effects of several dimensions and CSFs of software project management in GSD. Five dimensions and twenty factors have been determined through a literature review and further evaluated for prioritization using the combinatorial approach. The identified dimensions and factors will be valuable in devising strategies to effectively manage global software projects."
Toward an AI Knowledge Assistant for Context-Aware Learning Experiences in Software Capstone Project Development,"Neyem, A; GonzÃ¡lez, LA; Mendoza, M; Alcocer, JPS; Centellas, L; Paredes, C",10.1109/TLT.2024.3396735,2024,"Software assistants have significantly impacted software development for both practitioners and students, particularly in capstone projects. The effectiveness of these tools varies based on their knowledge sources; assistants with localized domain-specific knowledge may have limitations, while tools, such as ChatGPT, using broad datasets, might offer recommendations that do not always match the specific objectives of a capstone course. Addressing a gap in current educational technology, this article introduces an AI Knowledge Assistant specifically designed to overcome the limitations of the existing tools by enhancing the quality and relevance of large language models (LLMs). It achieves this through the innovative integration of contextual knowledge from a local lessons learned database tailored to the capstone course. We conducted a study with 150 students using the assistant during their capstone course. Integrated into the Kanban project tracking system, the assistant offered recommendations using different strategies: direct searches in the lessons learned database, direct queries to a generative pretrained transformers (GPT) model, query enrichment with lessons learned before submission to GPT and large language model meta AI (LLaMa) models, and query enhancement with Stack Overflow data before GPT processing. Survey results underscored a strong preference among students for direct LLM queries and those enriched with local repository insights, highlighting the assistant's practical value. Furthermore, our linguistic analysis conclusively demonstrated that texts generated by the LLM closely mirrored the linguistic standards and topical relevance of university course requirements. This alignment not only fosters a deeper understanding of course content but also significantly enhances the material's applicability to real-world scenarios."
Unit Test Generation using Generative AI : A Comparative Performance Analysis of Autogeneration Tools,"Bhatia, S; Gandhi, T; Kumar, D; Jalote, P",10.1145/3643795.3648396,2024,"Generating unit tests is a crucial task in software development, demanding substantial time and effort from programmers. The advent of Large Language Models (LLMs) introduces a novel avenue for unit test script generation. This research aims to experimentally investigate the effectiveness of LLMs, specifically exemplified by ChatGPT, for generating unit test scripts for Python programs, and how the generated test cases compare with those generated by an existing unit test generator (Pynguin). For experiments, we consider three types of code units: 1) Procedural scripts, 2) Function-based modular code, and 3) Class-based code. The generated test cases are evaluated based on criteria such as coverage, correctness, and readability. Our results show that ChatGPT's performance is comparable with Pynguin in terms of coverage, though for some cases its performance is superior to Pynguin. We also find that about a third of assertions generated by ChatGPT for some categories were incorrect. Our results also show that there is minimal overlap in missed statements between ChatGPT and Pynguin, thus, suggesting that a combination of both tools may enhance unit test generation performance. Finally, in our experiments, prompt engineering improved ChatGPT's performance, achieving a much higher coverage.*These authors contributed equally."
Leveraging NLP Techniques for Privacy Requirements Engineering in User Stories,"Herwanto, GB; Quirchmayr, G; Tjoa, AM",10.1109/ACCESS.2024.3364533,2024,"Privacy requirements engineering acts as a role to systematically elicit privacy requirements from system requirements and legal requirements such as the GDPR. Many methodologies have been proposed, but the majority of them are focused on the waterfall approach, making adopting privacy engineering in agile software development difficult. The other major issue is that the process currently is to a high degree manual. This paper focuses on closing these gaps through the development of a machine learning-based approach for identifying privacy requirements in an agile software development environment, employing natural language processing (NLP) techniques. Our method aims to allow agile teams to focus on functional requirements while NLP tools assist them in generating privacy requirements. The main input for our method is a collection of user stories, which are typically used to identify functional requirements in agile software development. The NLP approach is then used to automate some human-intensive tasks such as identifying personal data and creating data flow diagrams from user stories. The data flow diagram forms the basis for the automatic creation of privacy requirements. Our evaluation shows that our NLP method achieves a fairly good performance in terms of F-Measure. We are also demonstrate the feasibility of our NLP approach in CamperPlus project. Lastly, we are developing a tool to integrate our NLP approach into the privacy requirements engineering pipeline, allowing for manual editing of results so that agile teams can maintain control over the automated approach."
"A Systematic Review of Adversarial Machine Learning Attacks, Defensive Controls, and Technologies","Malik, J; Muthalagu, R; Pawar, PM",10.1109/ACCESS.2024.3423323,2024,"Adversarial machine learning (AML) attacks have become a major concern for organizations in recent years, as AI has become the industry's focal point and GenAI applications have grown in popularity around the world. Organizations are eager to invest in GenAI applications and develop their own large language models, but they face numerous security and data privacy issues, particularly AML attacks. AML attacks have jeopardized numerous large-scale machine learning models. If carried out successfully, AML attacks can significantly reduce the efficiency and precision of machine learning models. They have far-reaching negative consequences in the context of critical healthcare and autonomous transportation systems. In this paper, AML attacks are identified, analyzed, and classified using adversarial tactics and techniques. This research also recommends open-source tools for testing AI and ML models against AML attacks. Furthermore, this research suggests specific mitigating measures against each attack. It aims to serve as a guidance for organizations to defend against AML attacks and gain assurance in the security of ML models."
Generative AI in Responsible Conversational Agent Integration: Guidelines for Service Managers,"Sidaoui, K; Mahr, D; Odekerken-SchrÃ¶der, G",10.1016/j.orgdyn.2024.101045,2024,"Responsible integration of conversational agents (CAs) like chatbots is crucial for service firms to mitigate risks and foster positive outcomes. This article provides managerial guidelines through a Corporate Digital Responsibility (CDR) lens, focusing on CDR Culture, Management Structure, and Digital Governance across the service firm, software provider, and customers/society. It examines how organizational sensemaking processes of creation, interpretation, and enactment are triggered by CA-related issues and events. The research highlights the role of generative AI (GenAI) in implementing CDR factors and responsible CA software development lifecycle phases during development and integration. Guidelines are provided for leveraging GenAI to enhance CDR Culture, incorporate ethical considerations into CDR Management Structure, and enable robust Digital Governance mechanisms to prioritize customer/societal well-being. A multilevel framework illustrates reinforcing the guidelines through organizational sensemaking processes, and fostering responsible CA integration aligned with ethical principles and societal values."
Navigating software development in the ChatGPT and GitHub Copilot era,"France, SL",10.1016/j.bushor.2024.05.009,2024,"Generative artificial intelligence (GenAI) technologies using LLMs (large language models), such as ChatGPT and GitHub Copilot, with the ability to create code, have the potential to change the software-development landscape. Will this process be incremental, with software developers learning GenAI skills to supplement their existing skills, or will the process be more destructive, with the loss of large numbers of development jobs and a radical change in the responsibilities of the remaining developers? Given the rapid growth of AI capabilities, it is impossible to provide a crystal ball, but this article aims to give insight into the adoption of GenAI with LLMs in software development. The article gives an overview of the software-development industry and of the job functions of software developers. A literature review, combined with a content analysis of online comments from developers, gives insight into how GenAI implemented with LLMs is changing software development and how developers are responding to these changes. The article ties the academic and developer insights together into recommendations for software developers, and it describes a CMM (capability maturity model) framework for assessing and improving LLM development usage. (c) 2024 Kelley School of Business, Indiana University. Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/ licenses/by/4.0/)."
Enhancing biomechanical machine learning with limited data: generating realistic synthetic posture data using generative artificial intelligence,"Dindorf, C; Dully, J; Konradi, J; Wolf, C; Becker, S; Simon, S; Huthwelker, J; Werthmann, F; Kniepert, J; Drees, P; Betz, U; FrÃ¶hlich, M",10.3389/fbioe.2024.1350135,2024,"Objective: Biomechanical Machine Learning (ML) models, particularly deep-learning models, demonstrate the best performance when trained using extensive datasets. However, biomechanical data are frequently limited due to diverse challenges. Effective methods for augmenting data in developing ML models, specifically in the human posture domain, are scarce. Therefore, this study explored the feasibility of leveraging generative artificial intelligence (AI) to produce realistic synthetic posture data by utilizing three-dimensional posture data.Methods: Data were collected from 338 subjects through surface topography. A Variational Autoencoder (VAE) architecture was employed to generate and evaluate synthetic posture data, examining its distinguishability from real data by domain experts, ML classifiers, and Statistical Parametric Mapping (SPM). The benefits of incorporating augmented posture data into the learning process were exemplified by a deep autoencoder (AE) for automated feature representation.Results: Our findings highlight the challenge of differentiating synthetic data from real data for both experts and ML classifiers, underscoring the quality of synthetic data. This observation was also confirmed by SPM. By integrating synthetic data into AE training, the reconstruction error can be reduced compared to using only real data samples. Moreover, this study demonstrates the potential for reduced latent dimensions, while maintaining a reconstruction accuracy comparable to AEs trained exclusively on real data samples.Conclusion: This study emphasizes the prospects of harnessing generative AI to enhance ML tasks in the biomechanics domain."
CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation for Evaluating LLMs in Cybersecurity Knowledge,"Tihanyi, N; Ferrag, MA; Jain, R; Bisztray, T; Debbah, M",10.1109/CSR61664.2024.10679494,2024,"Large Language Models (LLMs) are increasingly used across various domains, from software development to cyber threat intelligence. Understanding all the different cybersecurity fields, including topics such as cryptography, reverse engineering, and risk assessment, poses a challenge even for human experts. The research community needs a diverse, accurate, and up-to-date dataset to test the general knowledge of LLMs in cybersecurity. To address this gap, we present CyberMetric-80, CyberMetric-500, CyberMetric-2000, and CyberMetric-10000, which are multiple-choice Q&A benchmark datasets comprising 80, 500, 2000, and 10,000 questions, respectively. By utilizing GPT-3.5 and Retrieval-Augmented Generation (RAG), we collected documents, including NIST standards, research papers, publicly accessible books, RFCs, and other publications in the cybersecurity domain, to generate questions, each with four possible answers. The results underwent several rounds of error checking and refinement. Human experts invested over 200 hours validating the questions and solutions to ensure their accuracy and relevance and to filter out any questions unrelated to cybersecurity. We have evaluated and compared 25 state-of-the-art LLM models on the CyberMetric datasets. In addition to our primary goal of evaluating LLMs, we involved 30 human participants to solve CyberMetric-80 in a closed-book scenario. The results can serve as a reference for comparing the general cybersecurity knowledge of humans and LLMs. The findings revealed that GPT-4o, GPT-4-turbo, Mixtral-8x7B-Instruct, Falcon-180B-Chat, and GEMINI-pro 1.0 were the best-performing LLMs. Additionally, the top LLMs were more accurate than humans on CyberMetric-80, although highly experienced human experts still outperformed small models such as Llama-3-8B, Phi-2 or Gemma-7b. The CyberMetric dataset is publicly available for the research community and can be downloaded from the projects' website: https://github.com/CyberMetric."
Using LLMs in Software Requirements Specifications: An Empirical Evaluation,"Krishna, M; Gaur, B; Verma, A; Jalote, P",10.1109/RE59067.2024.00056,2024,"The creation of a Software Requirements Specification (SRS) document is important for any software development project. Given the recent prowess of Large Language Models (LLMs) in answering natural language queries and generating sophisticated textual outputs, our study explores their capability to produce accurate, coherent, and structured drafts of these documents to accelerate the software development lifecycle. We assess the performance of GPT-4 and CodeLlama in drafting an SRS for a university club management system and compare it against human benchmarks using eight distinct criteria. Our results suggest that LLMs can match the output quality of an entry-level software engineer to generate an SRS, delivering complete and consistent drafts. We also evaluate the capabilities of LLMs to identify and rectify problems in a given requirements document. Our experiments indicate that GPT-4 is capable of identifying issues and giving constructive feedback for rectifying them, while CodeLlama's results for validation were not as encouraging. We repeated the generation exercise for four distinct use cases to study the time saved by employing LLMs for SRS generation. The experiment demonstrates that LLMs may facilitate a significant reduction in development time for entrylevel software engineers. Hence, we conclude that the LLMs can be gainfully used by software engineers to increase productivity by saving time and effort in generating, validating and rectifying software requirements."
Objectivity by design: The impact of AI-driven approach on employees' soft skills evaluation,"Gafni, R; Aviv, I; Kantsepolsky, B; Sherman, S; Rika, H; Itzkovich, Y; Barger, A",10.1016/j.infsof.2024.107430,2024,"Engineers' team collaboration skills are among software development's most important success factors. Existing Artificial Intelligence practices for the engineers' soft skills assessment mainly rely on evaluations of subjective data gathered through surveys, interviews, or observations. As a result, the insights gained by these methods are biased because of the subjective data people report. To overcome the challenge of subjectivity, we offer a novel objectivity-by-design approach for continuous AI-driven team collaboration skills analytics. The method analyzes the data from workstreams gathered from data repositories like Jira. Based on the study results, we conclude that this approach enables a continuous assessment of employees' team collaboration skills, provides more accurate insights, eliminates subjective biases, and helps uncover trends and deficits on individual and team levels. Understanding and recognizing employees' strengths and weaknesses can foster an organizational culture of growth and development. An improved organizational climate is expected to result in work satisfaction, engagement, and motivation, thus positively impacting employees, businesses, and society."
An Approach for Rapid Source Code Development Based on ChatGPT and Prompt Engineering,"Li, YJ; Shi, JJ; Zhang, Z",10.1109/ACCESS.2024.3385682,2024,"Code generation stands as a powerful technique in modern software development, improving development efficiency, reducing errors, and fostering standardization and consistency. Recently, ChatGPT has exhibited immense potential in automatic code generation. However, existing researches on code generation lack guidance for practical software development process. In this study, we utilized ChatGPT to develop a web-based code generation platform consisting of key components: User Interface, Prompt Builder, and Backend Service. Specifically, Prompt Builder dynamically generated comprehensive prompts to enhance model generation performance. We conducted experiments on 2 datasets to evaluate the performance of code generation in our approach. through 8 widely used metrics. The results demonstrate that (1) our Prompt Builder is effective, resulting in a 65.06% improvement in the exact match (EM), a 38.45% improvement in Bilingual Evaluation Understudy (BLEU), a 15.70% improvement in CodeBLEU, and a 50.64% improvement in Pass@1. (2) In real development scenarios, 98.5% of test cases can be validated through manual validation, highlighting the genuine assistance provided by the ChatGPT-based code generation approach."
Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers' Coding Practices with Insecure Suggestions from Poisoned AI Models,"Oh, S; Lee, K; Park, S; Kim, D; Kim, H",10.1109/SP54263.2024.00046,2024,"AI-powered coding assistant tools (e.g., ChatGPT, Copilot, and IntelliCode) have revolutionized the software engineering ecosystem. However, prior work has demonstrated that these tools are vulnerable to poisoning attacks. In a poisoning attack, an attacker intentionally injects maliciously crafted insecure code snippets into training datasets to manipulate these tools. The poisoned tools can suggest insecure code to developers, resulting in vulnerabilities in their products that attackers can exploit. However, it is still little understood whether such poisoning attacks against the tools would be practical in real-world settings and how developers address the poisoning attacks during software development. To understand the real-world impact of poisoning attacks on developers who rely on AI-powered coding assistants, we conducted two user studies: an online survey and an in-lab study. The online survey involved 238 participants, including software developers and computer science students. The survey results revealed widespread adoption of these tools among participants, primarily to enhance coding speed, eliminate repetition, and gain boilerplate code. However, the survey also found that developers may misplace trust in these tools because they overlooked the risk of poisoning attacks. The in-lab study was conducted with 30 professional developers. The developers were asked to complete three programming tasks with a representative type of AI-powered coding assistant tool (e.g., ChatGPT or IntelliCode), running on Visual Studio Code. The in-lab study results showed that developers using a poisoned ChatGPT-like tool were more prone to including insecure code than those using an IntelliCode-like tool or no tool. This demonstrates the strong influence of these tools on the security of generated code. Our study results highlight the need for education and improved coding practices to address new security issues introduced by AI-powered coding assistant tools."
The AI community building the future? A quantitative analysis of development activity on Hugging Face Hub,"Osborrne, C; Ding, J; Kirk, HR",10.1007/s42001-024-00300-8,2024,"Open model developers have emerged as key actors in the political economy of artificial intelligence (AI), but we still have a limited understanding of collaborative practices in the open AI ecosystem. This paper responds to this gap with a three-part quantitative analysis of development activity on the Hugging Face (HF) Hub, a popular platform for building, sharing, and demonstrating models. First, various types of activity across 348,181 model, 65,761 dataset, and 156,642 space repositories exhibit right-skewed distributions. Activity is extremely imbalanced between repositories; for example, over 70% of models have 0 downloads, while 1% account for 99% of downloads. Furthermore, licenses matter: there are statistically significant differences in collaboration patterns in model repositories with permissive, restrictive, and no licenses. Second, we analyse a snapshot of the social network structure of collaboration in model repositories, finding that the community has a core-periphery structure, with a core of prolific developers and a majority of isolate developers (89%). Upon removing these isolates from the network, collaboration is characterised by high reciprocity regardless of developers' network positions. Third, we examine model adoption through the lens of model usage in spaces, finding that a minority of models, developed by a handful of companies, are widely used on the HF Hub. Overall, the findings show that various types of activity across the HF Hub are characterised by Pareto distributions, congruent with open source software development patterns on platforms like GitHub. We conclude with recommendations for researchers, and practitioners to advance our understanding of open AI development."
On the Evaluation of Large Language Models in Unit Test Generation,"Yang, L; Yang, C; Gao, ST; Wang, WJ; Wang, B; Zhu, QH; Chu, X; Zhou, JY; Liang, GT; Wang, QX; Chen, JJ",10.1145/3691620.3695529,2024,"Unit testing is an essential activity in software development for verifying the correctness of software components. However, manually writing unit tests is challenging and time-consuming. The emergence of Large Language Models (LLMs) offers a new direction for automating unit test generation. Existing research primarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed prompting strategies, leaving the capabilities of advanced open-source LLMs with various prompting settings unexplored. Particularly, open-source LLMs offer advantages in data privacy protection and have demonstrated superior performance in some tasks. Moreover, effective prompting is crucial for maximizing LLMs' capabilities. In this paper, we conduct the first empirical study to fill this gap, based on 17 Java projects, five widely-used open-source LLMs with different structures and parameter sizes, and comprehensive evaluation metrics. Our findings highlight the significant influence of various prompt factors, show the performance of open-source LLMs compared to the commercial GPT-4 and the traditional Evosuite, and identify limitations in LLM-based unit test generation. We then derive a series of implications from our study to guide future research and practical use of LLM-based unit test generation."
CS1 with a Side of AI: Teaching Software Verification for Secure Code in the Era of Generative AI,"Fernandez, AS; Cornell, KA",10.1145/3626252.3630817,2024,"As AI-generated code promises to become an increasingly relied upon tool for software developers, there is a temptation to call for significant changes to early computer science curricula. A move from syntax-focused topics in CS1 toward abstraction and high-level application design seems motivated by the new large language models (LLMs) recently made available. In this position paper however, we advocate for an approach more informed by the AI itself teaching early CS learners not only how to use the tools but also how to better understand them. Novice programmers leveraging AI-code-generation without proper understanding of syntax or logic can create black box code with significant security vulnerabilities. We outline methods for integrating basic AI knowledge and traditional software verification steps into CS1 along with LLMs, which will better prepare students for software development in professional settings."
X-Lifecycle Learning for Cloud Incident Management using LLMs,"Goel, D; Husain, F; Singh, A; Ghosh, S; Parayil, A; Bansal, C; Zhang, XC; Rajmohan, S",10.1145/3663529.3663861,2024,"Incident management for large cloud services is a complex and tedious process that requires a significant amount of manual effort from on-call engineers (OCEs). OCEs typically leverage data from different stages of the software development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service properties, service dependencies, trouble-shooting documents, etc.) to generate insights for detection, root cause analysis and mitigation of incidents. Recent advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini) have created opportunities to automatically generate contextual recommendations for the OCEs, assisting them in quickly identifying and mitigating critical issues. However, existing research typically takes a silo-ed view of solving a certain task in incident management by leveraging data from a single stage of the SDLC. In this paper, we demonstrate that augmenting additional contextual data from different stages of the SDLC improves the performance of two critically important and practically challenging tasks: (1) automatically generating root cause recommendations for dependency failure related incidents, and (2) identifying the ontology of service monitors used for automatically detecting incidents. By leveraging a dataset of 353 incidents and 260 monitors from Microsoft, we demonstrate that augmenting contextual information from different stages of the SDLC improves the performance over state-of-the-art methods."
Comparing Large Language Models and Human Programmers for Generating Programming Code,"Hou, WP; Ji, ZC",10.1002/advs.202412279,2025,"The performance of seven large language models (LLMs) in generating programming code using various prompt strategies, programming languages, and task difficulties is systematically evaluated. GPT-4 substantially outperforms other LLMs, including Gemini Ultra and Claude 2. The coding performance of GPT-4 varies considerably with different prompt strategies. In most LeetCode and GeeksforGeeks coding contests evaluated in this study, GPT-4, employing the optimal prompt strategy, outperforms 85 percent of human participants in a competitive environment, many of whom are students and professionals with moderate programming experience. GPT-4 demonstrates strong capabilities in translating code between different programming languages and in learning from past errors. The computational efficiency of the code generated by GPT-4 is comparable to that of human programmers. GPT-4 is also capable of handling broader programming tasks, including front-end design and database operations. These results suggest that GPT-4 has the potential to serve as a reliable assistant in programming code generation and software development. A programming assistant is designed based on an optimal prompt strategy to facilitate the practical use of LLMs for programming."
Generating domain models from natural language text using NLP: a benchmark dataset and experimental comparison of tools,"Bozyigit, F; Bardakci, T; Khalilipour, A; Challenger, M; Ramackers, G; Babur,Ã; Chaudron, MRV",10.1007/s10270-024-01176-y,2024,"Software requirements specification describes users' needs and expectations on some target system. Requirements documents are typically represented by unstructured natural language text. Such texts are the basis for the various subsequent activities in software development, such as software analysis and design. As part of software analysis, domain models are made that describe the key concepts and relations between them. Since the analysis process is performed manually by business analysts, it is time-consuming and may introduce mistakes. Recently, researchers have worked toward automating the synthesis of domain models from textual software requirements. Current studies on this topic have limitations in terms of the volume and heterogeneity of experimental datasets. To remedy this, we provide a curated dataset of software requirements to be utilized as a benchmark by algorithms that transform textual requirements documents into domain models. We present a detailed evaluation of two text-to-model approaches: one based on a large-language model (ChatGPT) and one building on grammatical rules (txt2Model). Our evaluation reveals that both tools yield promising results with relatively high F-scores for modeling the classes, attributes, methods, and relationships, with txt2Model performing better than ChatGPT on average. Both tools have relatively lower performance and high variance when it comes to the relation types. We believe our dataset and experimental evaluation pave to way to advance the field of automated model generation from requirements."
The Potential of AI-Driven Assistants in Scaled Agile Software Development,"Saklamaeva, V; Pavlic, L",10.3390/app14010319,2024,"Scaled agile development approaches are now used widely in modern software engineering, allowing businesses to improve teamwork, productivity, and product quality. The incorporation of artificial intelligence (AI) into scaled agile development methods (SADMs) has emerged as a potential strategy in response to the ongoing demand for simplified procedures and the increasing complexity of software projects. This paper explores the intersection of AI-driven assistants within the context of the scaled agile framework (SAFe) for large-scale software development, as it stands out as the most widely adopted framework. Our paper pursues three principal objectives: (1) an evaluation of the challenges and impediments encountered by organizations during the implementation of SADMs, (2) an assessment of the potential advantages stemming from the incorporation of AI in large-scale contexts, and (3) the compilation of aspects of SADMs that AI-driven assistants enhance. Through a comprehensive systematic literature review, we identified and described 18 distinct challenges that organizations confront. In the course of our research, we pinpointed seven benefits and five challenges associated with the implementation of AI in SADMs. These findings were systematically categorized based on their occurrence either within the development phase or the phases encompassing planning and control. Furthermore, we compiled a list of 15 different AI-driven assistants and tools, subjecting them to a more detailed examination, and employing them to address the challenges we uncovered during our research. One of the key takeaways from this paper is the exceptional versatility and effectiveness of AI-driven assistants, demonstrating their capability to tackle a broader spectrum of problems. In conclusion, this paper not only sheds light on the transformative potential of AI, but also provides invaluable insights for organizations aiming to enhance their agility and management capabilities."
Clinical Performance Evaluation of an Artificial Intelligence-Powered Amyloid Brain PET Quantification Method,"Kang, SK; Heo, M; Chung, JY; Kim, D; Shin, SA; Choi, H; Chung, A; Ha, JM; Kim, H; Lee, JS",10.1007/s13139-024-00861-6,2024,"Purpose This study assesses the clinical performance of BTXBrain-Amyloid, an artificial intelligence-powered software for quantifying amyloid uptake in brain PET images.Methods 150 amyloid brain PET images were visually assessed by experts and categorized as negative and positive. Standardized uptake value ratio (SUVR) was calculated with cerebellum grey matter as the reference region, and receiver operating characteristic (ROC) and precision-recall (PR) analysis for BTXBrain-Amyloid were conducted. For comparison, same image processing and analysis was performed using Statistical Parametric Mapping (SPM) program. In addition, to evaluate the spatial normalization (SN) performance, mutual information (MI) between MRI template and spatially normalized PET images was calculated and SPM group analysis was conducted.Results Both BTXBrain and SPM methods discriminated between negative and positive groups. However, BTXBrain exhibited lower SUVR standard deviation (0.06 and 0.21 for negative and positive, respectively) than SPM method (0.11 and 0.25). In ROC analysis, BTXBrain had an AUC of 0.979, compared to 0.959 for SPM, while PR curves showed an AUC of 0.983 for BTXBrain and 0.949 for SPM. At the optimal cut-off, the sensitivity and specificity were 0.983 and 0.921 for BTXBrain and 0.917 and 0.921 for SPM12, respectively. MI evaluation also favored BTXBrain (0.848 vs. 0.823), indicating improved SN. In SPM group analysis, BTXBrain exhibited higher sensitivity in detecting basal ganglia differences between negative and positive groups.Conclusion BTXBrain-Amyloid outperformed SPM in clinical performance evaluation, also demonstrating superior SN and improved detection of deep brain differences. These results suggest the potential of BTXBrain-Amyloid as a valuable tool for clinical amyloid PET image evaluation."
The Future of Electronic Commerce in the IoT Environment,"Lazic, A; Milic, S; Vukmirovic, D",10.3390/jtaer19010010,2024,"The Internet of Things (IoT) was born from the fusion of virtual and physical space and became the initiator of many scientific fields. Economic sustainability is the key to further development and progress. To keep up with the changes, it is necessary to adapt economic models and concepts to meet the requirements of future smart environments. Today, the need for electronic commerce (e-commerce) has become an economic priority during the transition between Industry 4.0 and Industry 5.0. Unlike mass production in Industry 4.0, customized production in Industry 5.0 should gain additional benefits in vertical management and decision-making concepts. The authors' research is focused on e-commerce in a three-layer vertical IoT environment. The vertical IoT concept is composed of edge, fog, and cloud layers. Given the ubiquity of artificial intelligence in data processing, economic analysis, and predictions, this paper presents a few state-of-the-art machine learning (ML) algorithms facilitating the transition from a flat to a vertical e-commerce concept. The authors also propose hands-on ML algorithms for a few e-commerce types: consumer-consumer and consumer-company-consumer relationships. These algorithms are mainly composed of convolutional neural networks (CNNs), natural language understanding (NLU), sequential pattern mining (SPM), reinforcement learning (RL for agent training), algorithms for clicking on the item prediction, consumer behavior learning, etc. All presented concepts, algorithms, and models are described in detail."
"Using AI-based coding assistants in practice: State of affairs, perceptions, and ways forward","Sergeyuk, A; Golubev, Y; Bryksin, T; Ahmed, I",10.1016/j.infsof.2024.107610,2025,"Context: The last several years saw the emergence of AI assistants for code - multi-purpose AI-based helpers in software engineering. As they become omnipresent in all aspects of software development, it becomes critical to understand their usage patterns. Objective: We aim to better understand how specifically developers are using AI assistants, why they are not using them in certain parts of their development workflow, and what needs to be improved in the future. Methods: In this work, we carried out a large-scale survey aimed at how AI assistants are used, focusing on specific software development activities and stages. We collected opinions of 481 programmers on five broad activities: (a) implementing new features, (b) writing tests, (c) bug triaging, (d) refactoring, and (e) writing natural-language artifacts, as well as their individual stages. Results: Our results provide a novel comparison of different stages where AI assistants are used that is both comprehensive and detailed. It highlights specific activities that developers find less enjoyable and want to delegate to an AI assistant, e.g., writing tests and natural-language artifacts. We also determine more granular stages where AI assistants are used, such as generating tests and generating docstrings, as well as less studied parts of the workflow, such as generating test data. Among the reasons for not using assistants, there are general aspects like trust and company policies, as well as more concrete issues like the lack of project-size context, which can be the focus of the future research. Conclusion: The provided analysis highlights stages of software development that developers want to delegate and that are already popular for using AI assistants, which can be a good focus for features aimed to help developers right now. The main reasons for not using AI assistants can serve as a guideline for future work."
Integrating Generative AI for Advancing Agile Software Development and Mitigating Project Management Challenges,"Bahi, A; Gharib, J; Gahi, Y",,2024,"Agile software development emphasizes iterative progress, adaptability, and stakeholder collaboration. It champions flexible planning, continuous improvement, and rapid delivery, aiming to respond swiftly to change and deliver value efficiently. Integrating Generative Artificial Intelligence (AI) into Agile software development processes presents a promising avenue for overcoming project management challenges and enhancing the efficiency and effectiveness of software development endeavors. This paper explores the potential benefits of leveraging Generative AI in Agile methodologies, aiming to streamline development workflows, foster innovation, and mitigate common project management challenges. By harnessing the capabilities of Generative AI for tasks such as code generation, automated testing, and predictive analytics, Agile teams can augment their productivity, accelerate delivery cycles, and improve the quality of software products. Additionally, Generative AI offers opportunities for enhancing collaboration, facilitating decision-making, and addressing uncertainties inherent in Agile project management. Through an in-depth analysis of the integration of Generative AI within Agile frameworks, this paper provides insights into how organizations can harness the transformative potential of AI to advance Agile software development practices and navigate the complexities of modern software projects more effectively."
ChatGPT as a Tool for User Story Quality Evaluation: Trustworthy Out of the Box?,"Ronanki, K; Cabrero-Daniel, B; Berger, C",10.1007/978-3-031-48550-3_17,2024,"In Agile software development, user stories play a vital role in capturing and conveying end-user needs, prioritizing features, and facilitating communication and collaboration within development teams. However, automated methods for evaluating user stories require training in NLP tools and can be time-consuming to develop and integrate. This study explores using ChatGPT for user story quality evaluation and compares its performance with an existing benchmark. Our study shows that ChatGPT's evaluation aligns well with human evaluation, and we propose a best of three strategy to improve its output stability. We also discuss the concept of trustworthiness in AI and its implications for non-experts using ChatGPT's unprocessed outputs. Our research contributes to understanding the reliability and applicability of Generative AI in user story evaluation and offers recommendations for future research."
AI-Equipped Scanning Probe Microscopy for Autonomous Site-Specific Atomic-Level Characterization at Room Temperature,"Diao, Z; Ueda, K; Hou, LF; Li, FX; Yamashita, H; Abe, M",10.1002/smtd.202400813,2025,"An advanced scanning probe microscopy system enhanced with artificial intelligence (AI-SPM) designed for self-driving atomic-scale measurements is presented. This system expertly identifies and manipulates atomic positions with high precision, autonomously performing tasks such as spectroscopic data acquisition and atomic adjustment. An outstanding feature of AI-SPM is its ability to detect and adapt to surface defects, targeting or avoiding them as necessary. It is also designed to overcome typical challenges such as positional drift and tip apex atomic variations due to the thermal effects, ensuring accurate, site-specific surface analysis. The tests under the demanding conditions of room temperature have demonstrated the robustness of the system, successfully navigating thermal drift and tip fluctuations. During these tests on the Si(111)-(7 x 7) surface, AI-SPM autonomously identified defect-free regions and performed a large number of current-voltage spectroscopy measurements at different adatom sites, while autonomously compensating for thermal drift and monitoring probe health. These experiments produce extensive data sets that are critical for reliable materials characterization and demonstrate the potential of AI-SPM to significantly improve data acquisition. The integration of AI into SPM technologies represents a step toward more effective, precise and reliable atomic-level surface analysis, revolutionizing materials characterization methods. The advanced AI-enhanced scanning probe microscopy (AI-SPM) system autonomously performs atomic-scale measurements at room temperature, identifying and manipulating atomic positions with high precision. It adapts to surface defects, compensates for positional drift and thermal effects, and ensures accurate surface analysis. Tests on the Si(111)-(7 x 7) surface confirmed its robustness and provided comprehensive, reliable data for materials characterization, demonstrating the potential of AI-SPM to revolutionize atomic-level analysis. image"
A systematic literature review on the impact of AI models on the security of code generation,"Negri-Ribalta, C; Geraud-Stewart, R; Sergeeva, A; Lenzini, G",10.3389/fdata.2024.1386720,2024,"Introduction Artificial Intelligence (AI) is increasingly used as a helper to develop computing programs. While it can boost software development and improve coding proficiency, this practice offers no guarantee of security. On the contrary, recent research shows that some AI models produce software with vulnerabilities. This situation leads to the question: How serious and widespread are the security flaws in code generated using AI models?Methods Through a systematic literature review, this work reviews the state of the art on how AI models impact software security. It systematizes the knowledge about the risks of using AI in coding security-critical software.Results It reviews what security flaws of well-known vulnerabilities (e.g., the MITRE CWE Top 25 Most Dangerous Software Weaknesses) are commonly hidden in AI-generated code. It also reviews works that discuss how vulnerabilities in AI-generated code can be exploited to compromise security and lists the attempts to improve the security of such AI-generated code.Discussion Overall, this work provides a comprehensive and systematic overview of the impact of AI in secure coding. This topic has sparked interest and concern within the software security engineering community. It highlights the importance of setting up security measures and processes, such as code verification, and that such practices could be customized for AI-aided code production."
AI-Analyst: An AI-Assisted SDLC Analysis Framework for Business Cost Optimization,"Faruqui, N; Thatoi, P; Choudhary, R; Roncevic, I; Alqahtani, H; Sarker, IH; Khanam, S",10.1109/ACCESS.2024.3519423,2024,"Managing the System Development Lifecycle (SDLC) is a complex task because of its involvement in coordinating diverse activities, stakeholders, and resources while ensuring project goals are met efficiently. The complex nature of the SDLC process leaves plenty of scope for human error, which impacts the overall business cost. This paper introduces AI-Analyst, an AI-assisted framework developed using the transformer-based model with more than 150 million parameters to assist with SDLC management. It minimizes manual effort errors, optimizes resource allocation, and improves decision-making processes, resulting in substantial cost savings. The statistical analysis shows that it saves around 53.33% of costs in an experimental project. The transformer model has been trained with a uniquely prepared dataset tailored for SDLC through transfer learning. It achieved impressive results, with an accuracy of 91.5%, precision of 91.9%, recall of 91.3%, and an F1-score of 91.5%, demonstrating its high reliability and performance. The perplexity score of 15 further indicates the model's strong language understanding capabilities to retrieve relations from complex characteristics of Natural Language Processing (NLP). The AI-Analyst framework represents a significant advancement in integrating Large Language Models (LLMs) into SDLC, offering a scalable and cost-effective solution for optimizing business processes."
An Industry Case Study on Adoption of AI-based Programming Assistants,"Davila, N; Wiese, I; Steinmacher, I; da Silva, LL; Kawamoto, A; Favaro, GJP; Nunes, I",10.1145/3639477.3643648,2024,"Programming assistants based on artificial intelligence (AI), such as ChatGPT and GitHub Copilot, have gained worldwide popularity recently. Studies in software development have explored the adoption of these tools, investigating their characteristics and impacts and how practitioners interact and perceive them. To contribute to this growing body of knowledge, in this study, we aim to explore the adoption of AI-based programming assistants in the Brazilian industry. More specifically, we aim to understand how practitioners of a particular Brazilian agroindustry-related company perceive and use AI-based tools to develop software. Using an online survey, we collected and analyzed 72 responses from employees of the studied company. Our findings suggest that practitioners mainly adopt ChatGPT and GitHub Copilot, interacting with these tools to accelerate online searching, typing, and syntax recall. A recurrent difficulty is the lack of context in the suggestions provided by these tools, but participants work on detailed descriptions to contextualize and cope with this challenge. Among the reasons for not using AI-based tools, the most influential is that participants use a commercial programming language, i.e., Uniface, which these tools lack examples. Our results provide insights into the state of the practice related to AI-based programming assistants and discuss implications for practitioners and researchers."
Leveraging ChatGPT for Enhanced Logical Analysis in the Theory of Constraints Thinking Process,"Aljaz, T",10.2478/orga-2024-0014,2024,"Background/Purpose Artificial intelligence (AI) has traditionally been used for quantitative analysis using explicit data. However, much of the information critical to decision making remains undocumented and is not stored in a structured way. This study explores the integration of AI, specifically ChatGPT, into Theory of Constraints (TOC) Thinking Process (TP) tools. Method In this study, we applied ChatGPT to a real-world IT project management case using a variety of research methods, including international literature analysis, observation, and personal experience. The use of the TOC TP allowed us to understand the decision-making process of ChatGPT and to systematically explore its advantages and limitations in creating logical trees of TOC TP. Results ChatGPT significantly enhanced efficiency and depth in TOC TP data collection and analysis, effectively addressing logical leaps for more coherent structures. It also promoted deeper analytical thinking and aided root cause identification. The integration of ChatGPT into the TOC TP process led to faster decision-making, reduced bias, and clearer analysis. Challenges of ChatGPT including the need for human oversight, specific TOC TP training, and ethical considerations were noted. Conclusion This study provides an initial investigation into the use of ChatGPT in TOC TP tools. The results suggest that ChatGPT has the potential to be a valuable tool for organizations seeking to improve their decision making and performance. However, further research is needed to validate these findings and explore the full potential of AI in TOC TP."
AI-Powered VR for Enhanced Learning Compared to Traditional Methods,"Cinar, OE; Rafferty, K; Cutting, D; Wang, H",10.3390/electronics13234787,2024,"This paper evaluates a VR (Virtual Reality) application aimed at enhancing the learning of Python collection data types and structures for electrical and electronic engineering students. By incorporating gamification and personalisation features, the application provides an immersive environment where students can interact with virtual representations of complex programming concepts. To further enhance interactivity and engagement, the application integrates a virtual assistant and example generator, developed using Meta Voice SDK (Software Development Kit) and wit.ai. These AI (Artificial Intelligence)-NLP (Natural Language Processing) tools create personalised learning paths and generate dynamic examples based on individual learning progress. A user study was conducted with a total of 48 participants. During the user study, participants were divided into two equal groups of 24, both wearing EEG (Electroencephalography) headsets: one group engaged with the VR application, while the other read the traditional booklet, allowing for the recording and analysis of attention and engagement levels. These measures of engagement and attention were then compared to those extracted from a benchmark cohort of students whose learning experience was through more traditional booklets. The results indicated a statistically significant improvement in understanding Python collections among VR users compared to their baseline scores, highlighting the benefits of interactive and tailored learning environments. Additionally, EEG data analysis showed that VR users exhibited higher average levels of attention and engagement compared to those using the paper-based method, demonstrating the effectiveness of immersive technologies in sustaining learner interest and focus, particularly in enhancing learning for software development."
Chatting with AI: Deciphering Developer Conversations with ChatGPT,"Mohamed, S; Parvin, A; Parra, E",10.1145/3643991.3645078,2024,"Large Language Models (LLMs) have been widely adopted and are becoming ubiquitous and integral to software development. However, we have little knowledge as to how these tools are being used by software developers beyond anecdotal evidence and word-of-mouth reports. In this work, we present a study toward understanding how developers engage with and utilize LLMs by reporting the results of an empirical study identifying patterns in the conversation that developers have with LLMs. We identified a total of 19 topics describing the purpose of the developers in their conversations with LLMs. Our findings reveal that developers use LLMs to facilitate various aspects of their software development processes (e.g., information-seeking about programming languages and frameworks and soliciting high-level design recommendations) to a similar extent to which they use them for non-development purposes such as writing assistance, general purpose queries, and conducting Turing tests to assess the intrinsic capabilities of the models. This work not only sheds light on the diverse applications of LLMs in software development but also underscores their emerging role as critical tools in enhancing developer productivity and creativity as we move closer to widespread AI-assisted software development."
"Exploring the problems, their causes and solutions of AI pair programming: A study on GitHub and Stack Overflow","Zhou, XY; Liang, P; Zhang, BQ; Li, ZY; Ahmad, A; Shahin, M; Waseem, M",10.1016/j.jss.2024.112204,2025,"With the recent advancement of Artificial Intelligence (AI) and Large Language Models (LLMs), AI-based code generation tools become a practical solution for software development. GitHub Copilot, the AI pair programmer, utilizes machine learning models trained on a large corpus of code snippets to generate code suggestions using natural language processing. Despite its popularity in software development, there is limited empirical evidence on the actual experiences of practitioners who work with Copilot. To this end, we conducted an empirical study to understand the problems that practitioners face when using Copilot, as well as their underlying causes and potential solutions. We collected data from 473 GitHub issues, 706 GitHub discussions, and 142 Stack Overflow posts. Our results reveal that (1) Operation Issue and Compatibility Issue are the most common problems faced by Copilot users, (2) Copilot Internal Error, Network Connection Error, and Editor/IDE Compatibility Issue are identified as the most frequent causes, and (3) Bug Fixed by Copilot, Modify Configuration/Setting, and Use Suitable Version are the predominant solutions. Based on the results, we discuss the potential areas of Copilot for enhancement, and provide the implications for the Copilot users, the Copilot team, and researchers."
A Sugeno ANFIS Model Based on Fuzzy Factor Analysis for IS/IT Project Portfolio Risk Prediction,"Zaidouni, A; Idrissi, MAJ; Bellabdaoui, A",10.32890/jict2024.23.2.1,2024,"Risk inherence jeopardises Information System (IS) and Information Technology (IT) Project Portfolio Management (PPM) to realise the strategic objectives. Previous studies have mainly provided Artificial Intelligence (AI) and statistical models to predict the overall risk of IS/IT project portfolio, whereas neuro-fuzzy models were rarely used. This paper proposes a Sugeno Adaptive Neuro-Fuzzy Inference System (ANFIS) model based on Fuzzy Factor Analysis (FFA) named ANFIS-OPR to predict the overall risk of IS/IT project portfolio from historical IS/IT project risk data. The ANFIS-OPR inputs are the relevant factor loadings resulting from the FFA application on the IS/ IT projects risks set to cope with the curse of dimensionality. Then, the Sugeno ANFIS model is adopted to give strategic interpretability to the predicted IS/IT project portfolio overall risk by implementing the IS/IT Project Management Office (PMO) expert knowledge, represented by fuzzy rules, on the relationship between IS/IT project portfolio strategic alignment and the IS/IT projects risks. The ANFISOPR outputs are the predicted Overall Portfolio Risk (OPR) and Root Mean Square Error (RMSE). The paper also presents an IS/ IT PMO case study that shows the proposed ANFIS-OPR efficacy, which predicted the OPR values closely to the OPR estimates with an accepted RMSE of 0.108. The proposed ANFIS-OPR is a novel intelligent decision -making tool that enables the IS/IT PMO to monitor the OPR, considering its linkage with strategic alignment; thus, contingency plans can be carried out appropriately while ensuring that the IS/IT project portfolio is strategically aligned."
From Requirements to Architecture: An AI-Based Journey to Semi-Automatically Generate Software Architectures,"Eisenreich, T; Speth, S; Wagner, S",10.1145/3643660.3643942,2024,"Designing domain models and software architectures represents a significant challenge in software development, as the resulting architectures play a vital role in fulfilling the system's quality of service. Due to time pressure, architects often model only one architecture based on their known limited domain understanding, patterns, and experience instead of thoroughly analyzing the domain and evaluating multiple candidates, selecting the best fitting. Existing approaches try to generate domain models based on requirements, but still require time-consuming manual effort to achieve good results. Therefore, in this vision paper, we propose a method to generate software architecture candidates semi-automatically based on requirements using artificial intelligence techniques. We further envision an automatic evaluation and trade-off analysis of the generated architecture candidates using, e.g., the architecture trade-off analysis method combined with large language models and quantitative analyses. To evaluate this approach, we aim to analyze the quality of the generated architecture models and the efficiency and effectiveness of our proposed process by conducting qualitative studies."
Analyzing Developer-ChatGPT Conversations for Software Refactoring: An Exploratory Study,"Deo, S; Hinge, D; Chavan, OS; Wang, YX; Mkaouer, MW",10.1145/3643991.3645082,2024,"In recent years, Large Language Models (LLMs) have witnessed a remarkable ascent, with OpenAI's ChatGPT, introduced in 2022, garnering substantial attention. ChatGPT's rapid adoption in the software development community has opened up new avenues for exploring its qualitative and quantitative impact on Developer-ChatGPT conversations. In this paper, we delve into a rich dataset from GitHub and Hacker News to perform a thorough analysis. Our objectives include characterizing the nature of these interactions and evaluating the use of ChatGPT in refactoring. To achieve these goals, we employ a combination of exploratory data analysis and data annotation, utilizing relevant keyword filters to extract pertinent information. Our examination encompasses the identification and analysis of code refactorings facilitated by ChatGPT. Through a meticulous exploration of these conversations, our goal is to illuminate the potential of ChatGPT to enhance software development practices. This research promises to provide valuable insights into the evolving role of ChatGPT in the world of software development."
A trustworthy hybrid model for transparent software defect prediction: SPAM-XAI,"Mustaqeem, M; Mustajab, S; Alam, M; Jeribi, F; Alam, S; Shuaib, M",10.1371/journal.pone.0307112,2024,"Maintaining quality in software development projects is becoming very difficult because the complexity of modules in the software is growing exponentially. Software defects are the primary concern, and software defect prediction (SDP) plays a crucial role in detecting faulty modules early and planning effective testing to reduce maintenance costs. However, SDP faces challenges like imbalanced data, high-dimensional features, model overfitting, and outliers. Moreover, traditional SDP models lack transparency and interpretability, which impacts stakeholder confidence in the Software Development Life Cycle (SDLC). We propose SPAM-XAI, a hybrid model integrating novel sampling, feature selection, and eXplainable-AI (XAI) algorithms to address these challenges. The SPAM-XAI model reduces features, optimizes the model, and reduces time and space complexity, enhancing its robustness. The SPAM-XAI model exhibited improved performance after experimenting with the NASA PROMISE repository's datasets. It achieved an accuracy of 98.13% on CM1, 96.00% on PC1, and 98.65% on PC2, surpassing previous state-of-the-art and baseline models with other evaluation matrices enhancement compared to existing methods. The SPAM-XAI model increases transparency and facilitates understanding of the interaction between features and error status, enabling coherent and comprehensible predictions. This enhancement optimizes the decision-making process and enhances the model's trustworthiness in the SDLC."
An Investigation into Misuse of Java Security APIs by Large Language Models,"Mousavi, Z; Islam, C; Moore, K; Abuadbba, A; Babar, MA",10.1145/3634737.3661134,2024,"The increasing trend of using Large Language Models (LLMs) for code generation raises the question of their capability to generate trustworthy code. While many researchers are exploring the utility of code generation for uncovering software vulnerabilities, one crucial but often overlooked aspect is the security Application Programming Interfaces (APIs). APIs play an integral role in upholding software security, yet effectively integrating security APIs presents substantial challenges. This leads to inadvertent misuse by developers, thereby exposing software to vulnerabilities. To overcome these challenges, developers may seek assistance from LLMs. In this paper, we systematically assess ChatGPT's trustworthiness in code generation for security API use cases in Java. To conduct a thorough evaluation, we compile an extensive collection of 48 programming tasks for 5 widely used security APIs. We employ both automated and manual approaches to effectively detect security API misuse in the code generated by ChatGPT for these tasks. Our findings are concerning: around 70% of the code instances across 30 attempts per task contain security API misuse, with 20 distinct misuse types identified. Moreover, for roughly half of the tasks, this rate reaches 100%, indicating that there is a long way to go before developers can rely on ChatGPT to securely implement security API code."
Dear ChatGPT - can you teach me how to program an app for laboratory medicine?,"Meyer, A; Ruthard, J; Streichert, T",10.1515/labmed-2024-0034,2024,"Objectives The multifaceted potential of ChatGPT in the medical domain remains underexplored, particularly regarding its application in software development by individuals with a medical background but limited information technology expertise.Case presentation This study investigates ChatGPT's utility in creating a laboratory medicine application. Despite minimal programming skills, the authors successfully developed an automated intra-assay, inter-device precision test for immunophenotyping with a shiny user interface, facilitated by ChatGPT. While the coding process was expedited, meticulous oversight and error correction by the authors were imperative.Conclusions These findings highlight the value of large language models such as ChatGPT in code-based application development for automating work processes in a medical context. Particularly noteworthy is the facilitation of these tasks for non-technically trained medical professionals and its potential for digital medical education."
Enhancing Disease Classification with Deep Learning: a Two-Stage Optimization Approach for Monkeypox and Similar Skin Lesion Diseases,"Savas, S",10.1007/s10278-023-00941-7,2024,"Monkeypox (MPox) is an infectious disease caused by the monkeypox virus, presenting challenges in accurate identification due to its resemblance to other diseases. This study introduces a deep learning-based method to distinguish visually similar diseases, specifically MPox, chickenpox, and measles, addressing the 2022 global MPox outbreak. A two-stage optimization approach was presented in the study. By analyzing pre-trained deep neural networks including 71 models, this study optimizes accuracy through transfer learning, fine-tuning, and ensemble learning techniques. ConvNeXtBase, Large, and XLarge models were identified achieving 97.5% accuracy in the first stage. Afterwards, some selection criteria were followed for the models identified in the first stage for use in ensemble learning technique within the optimization approach. The top-performing ensemble model, EM3 (composed of RegNetX160, ResNetRS101, and ResNet101), attains an AUC of 0.9971 in the second stage. Evaluation on unseen data ensures model robustness and enhances the study's overall validity and reliability. The design and implementation of the study have been optimized to address the limitations identified in the literature. This approach offers a rapid and highly accurate decision support system for timely MPox diagnosis, reducing human error, manual processes, and enhancing clinic efficiency. It aids in early MPox detection, addresses diverse disease challenges, and informs imaging device software development. The study's broad implications support global health efforts and showcase artificial intelligence potential in medical informatics for disease identification and diagnosis."
APPT: Boosting Automated Patch Correctness Prediction via Fine-Tuning Pre-Trained Models,"Zhang, QJ; Fang, CR; Sun, WS; Liu, Y; He, TK; Hao, XD; Chen, ZY",10.1109/TSE.2024.3354969,2024,"Automated program repair (APR) aims to fix software bugs automatically without human debugging efforts and plays a crucial role in software development and maintenance. Despite the recent significant progress in the number of fixed bugs, APR is still challenged by a long-standing overfitting problem (i.e., the generated patch is plausible but overfitting). Various techniques have thus been proposed to address the overfitting problem. Recently, researchers have employed BERT to extract code features, which are then used to train a classifier for patch correctness prediction, indicating the potential of such pre-trained models in reasoning about patch correctness. However, BERT is restricted to feature extraction for classifier training without benefiting from the training process, potentially generating sub-optimal vector representations for patched code snippets. In this paper, we propose APPT, a pre-trained model-based automated patch correctness assessment technique by both pre-training and fine-tuning. APPT adopts a pre-trained model as the encoder stack, followed by an LSTM stack and a deep learning classifier. More importantly, the pre-trained model is fine-tuned in conjunction with other components as a whole pipeline to fully adapt it specifically for reasoning about patch correctness. Although our idea is general and can be built on various existing pre-trained models, we have implemented APPT based on the BERT model. We conduct an extensive experiment on 1,183 Defects4J patches and the experimental results show that APPT achieves prediction accuracy of 79.7% and recall of 83.2%, outperforming the state-of-the-art technique CACHE by 4.3% and 6.7%. Our additional investigation on 49,694 real-world patches shows that APPT achieves the optimum performance (exceeding 99% in five common metrics for assessing patch classification techniques) compared with existing representation learning techniques. We further investigate the impact of each component and find that they all positively contribute to APPT, e.g., the fine-tuning process and the LSTM stack increase F1-score by 10.22% and 4.11%, respectively. We also prove that adopting advanced pre-trained models can further provide substantial advancement (e.g., GraphCodeBERT-based APPT improves BERT-based APPT by 2.8% and 3.3% in precision and AUC, respectively), highlighting the generalizability of APPT. Overall, our study highlights the promising future of fine-tuning pre-trained models to assess patch correctness and reduce the manual inspection effort of debugging experts when deploying APR tools in practice."
Comprehensive Evaluation and Insights Into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation,"Karpurapu, S; Myneni, S; Nettur, U; Gajja, LS; Burke, D; Stiehm, T; Payne, J",10.1109/ACCESS.2024.3391815,2024,"Behavior-driven development (BDD) is an Agile testing methodology fostering collaboration among developers, QA analysts, and stakeholders. In this manuscript, we propose a novel approach to enhance BDD practices using large language models (LLMs) to automate acceptance test generation. Our study uses zero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B, and PaLM-2. The paper presents a detailed methodology that includes the dataset, prompt techniques, LLMs, and the evaluation process. The results demonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests with better performance. The few-shot prompt technique highlights its ability to provide higher accuracy by incorporating examples for in-context learning. Furthermore, the study examines syntax errors, validation accuracy, and comparative analysis of LLMs, revealing their effectiveness in enhancing BDD practices. However, our study acknowledges that there are limitations to the proposed approach. We emphasize that this approach can support collaborative BDD processes and create opportunities for future research into automated BDD acceptance test generation using LLMs."
Exploring the Impact of Generative AI for StandUp Report Recommendations in Software Capstone Project Development,"Neyem, A; Alcocer, JPS; Mendoza, M; Centellas-Claros, L; Gonzalez, LA; Paredes-Robles, C",10.1145/3626252.3630854,2024,"StandUp Reports play an important role in capstone software engineering courses, facilitating progress tracking, obstacle identification, and team collaboration. However, despite their significance, students often grapple with the challenge of creating StandUp Reports that are clear, concise, and actionable. This paper investigates the impact of the use of generative AI in producing StandUp report recommendations, aiming to assist students in enhancing the quality and effectiveness of their reports. In a semester-long capstone course, 179 students participated in 16 real-world software development projects. They submitted weekly StandUp Reports with the assistance of an AI-powered Slack, which analyzed their initial reports and provided suggestions for enhancing them using both GPT-3.5 and the early access GPT-4 API. After each submitted report, students voluntarily answered a survey about usability and suggestion preference. Furthermore, we conducted a linguistic analysis of the recommendations made by the algorithms to gauge reading ease and comprehension complexity. Our findings indicate that the AI-based recommendation system helped students improve the overall quality of their StandUp Reports throughout the semester. Students expressed a high level of satisfaction with the tool and exhibited a strong willingness to continue using it in the future. The survey reveals that students perceived a slight improvement when using GPT-4 compared to GPT-3.5. Finally, a computational linguistic analysis performed on the recommendations demonstrates that both algorithms significantly improve the alignment between the generated texts and the students' educational level, thereby improving the quality of the original texts."
Developing Critical Thinking Practices Interwoven with Generative AI usage in an Introductory Programming Course,"Styve, A; Virkki, OT; Naeem, U",10.1109/EDUCON60312.2024.10578746,2024,"Software development has evolved significantly. In the past, developers were required to have comprehensive understanding of programming languages, algorithms, and computer architecture. However, with the emergence of the Internet, software libraries, frameworks, and forums became widely available, which utilize reusable software components that can reduce development time and costs. The advent of Generative Artificial Intelligence (AI) tools, such as ChatGPT, GitHub Copilot, and Amazon CodeWhisperer, has further enhanced the developer's toolkit, as these tools can be used for a wide variety of tasks such as code generation, documentation, commenting and reviewing. As programming is often slow and requires trial and error, novice programmers can be tempted to apply the first solution found on the Internet or proposed by an AI tool without much critical reflection or notion of responsibility. Hence, the advances of AI have raised both excitement and concerns among Information Technology (IT)/Computer Science (CS) students and educators. Yet, AI tools are here to stay, and students must learn to use them responsibly. The aim of this paper is to investigate how to design learning activities that introduce Generative AI tools (GitHub Copilot and ChatGPT) for programming while promoting critical thinking practices among students in an introductory programming course in the first semester. Students' opinions and customs were surveyed before and after the AI-based programming assignment. The results indicate that students' awareness of the possibilities and limitations of AI, as well as practices of critical thinking in programming increased. This is encouraging as critical thinking is an integral part of best programming practices."
Using AI Assistants in Software Development: A Qualitative Study on Security Practices and Concerns,"Klemmer, JH; Horstmann, SA; Patnaik, N; Ludden, C; Burton, C ; Powers, C; Massacci, F; Rahman, A; Votipka, D; Lipford, HR; Rashid, A; Naiakshina, A; Fahl, S",10.1145/3658644.3690283,2024,"Following the recent release of AI assistants, such as OpenAI's ChatGPT and GitHub Copilot, the software industry quickly utilized these tools for software development tasks, e.g., generating code or consulting AI for advice. While recent research has demonstrated that AI-generated code can contain security issues, how software professionals balance AI assistant usage and security remains unclear. This paper investigates how software professionals use AI assistants in secure software development, what security implications and considerations arise, and what impact they foresee on security in software development. We conducted 27 semi-structured interviews with software professionals, including software engineers, team leads, and security testers. We also reviewed 190 relevant Reddit posts and comments to gain insights into the current discourse surrounding AI assistants for software development. Our analysis of the interviews and Reddit posts finds that, despite many security and quality concerns, participants widely use AI assistants for security-critical tasks, e.g., code generation, threat modeling, and vulnerability detection. Participants' overall mistrust leads to checking AI suggestions in similar ways to human code. However, they expect improvements and, therefore, a heavier use of AI for security tasks in the future. We conclude with recommendations for software professionals to critically check AI suggestions, for AI creators to improve suggestion security and capabilities for ethical security tasks, and for academic researchers to consider generalpurpose AI in software development."
Exploring the adoption of the metaverse and chat generative pre-trained transformer: A single-valued neutrosophic Dombi Bonferroni-based method for the selection of software development strategies,"Ãnden, A; Kara, K; Ãnden, I; YalÃ§in, GC; Simic, V; Pamucar, D",10.1016/j.engappai.2024.108378,2024,"The contemporary era has witnessed remarkable developments that seek to transform and reshape traditional software development methodologies. Notably, artificial intelligence (AI) supported software development as well as software development in virtual reality environments have gained considerable prominence. This article introduces software development strategies to examine how software developers and companies respond to this transformation. Also, an advanced decision model is developed using the alternative ranking order method accounting for two-step normalization (AROMAN) method and further analyzed with the single-valued neutrosophic set-based AROMAN technique. The single-valued neutrosophic weighted Dombi Bonferroni operator is employed in the analysis process. This research offers two case studies investigating the preferences of developers and managers in software development strategies. The first case study examines the preferences of developers, while the second focuses on the preferences of managers. In both case studies, three fundamental software development methods are presented. These include the traditional developers approach, AI-supported developers approach, and mixed reality and AI-supported developers approach. These methods are ranked based on expert opinions concerning 10 criteria that influence the software development process. In both case studies, output quality is identified as the most influential criterion. From the perspective of software development methods, in both case studies, the mixed reality and AI-supported developers approach is identified as the most effective. Recommendations are provided for developers and managers. The findings also have significant implications for guiding developers and managers in making informed decisions and optimizing software development practices to align with the evolving AI and virtual reality landscape."
Test Suite Optimization Using Machine Learning Techniques: A Comprehensive Study,"Mehmood, A; Ilyas, QM; Ahmad, M; Shi, ZL",10.1109/ACCESS.2024.3490453,2024,"Software testing is an essential yet costly phase of the software development lifecycle. While machine learning-based test suite optimization techniques have shown promise in reducing testing costs and improving fault detection, a comprehensive evaluation of their effectiveness across different environments is still lacking. This paper reviews 43 studies published between 2018 and 2023, covering various test case selection, prioritization, and reduction techniques using machine learning. The findings reveal that conventional machine learning techniques, particularly supervised learning methods, have been widely adopted for test case prioritization and selection. Recent advancements, such as deep learning and hybrid models, show potential in improving fault detection rates and scalability, though challenges remain in adapting these techniques to large-scale and dynamic environments. Additionally, Generative AI and large language models (LLMs) are emerging as promising tools for automating aspects of test case generation and prioritization, offering new avenues for future research in enhancing test suite optimization. The study identifies recent trends, challenges, and opportunities for further research, with a focus on both conventional and emerging methods, including deep learning, hybrid approaches, and Generative AI models. By systematically analyzing these techniques, this work contributes to the understanding of how machine learning and Generative AI can enhance test suite optimization and highlights future directions for improving the scalability and real-world applicability of these methods."
Enhancing educational efficiency: Generative AI chatbots and DevOps in Education 4.0,"Mekic, ES; Jovanovic, MN; Kuk, KV; Prlincevic, BP; Savic, AM",10.1002/cae.22804,2024,"The emergence of new technologies has developed a new industrial need for engineering graduate students. This modern industry needs a modern educational framework to support further development. This research revolves around developing the learning methodological approach for university courses that aligns with Education 4.0 and can fulfill the essential knowledge requirements for future engineers in Industry 4.0. We used a constructive approach as a baseline methodology already proven in the field of engineering education. On the other hand, we modified this approach by implementing Industry 4.0 standards, such as Agile methodology, DevOps tools, and Artificial intelligence chatbots. In this way, we managed to establish one more step in the digital transformation of education as one of the paramount cornerstones of Industry 4.0. As a result, during the course, Agile methodology and DevOps tools created a learning environment very similar to the real software development environment in companies. The efficiency of the delivery of the learning material also increased by implementation and integration of AI chatbots. This was tracked by the number of projects developed during course implementation. The study showed that in the field of education, the implementation of novel approaches developed in the industry to increase efficiency can be implemented in the educational environment. It also showed that the implementation of those methodologies does not hinder but improves the efficiency of the educational cycle."
Regulatory Frameworks for AI-Enabled Medical Device Software in China: Comparative Analysis and Review of Implications for Global Manufacturer,"Han, Y; Ceross, A; Bergmann, J",10.2196/46871,2024,"The China State Council released the new generation artificial intelligence (AI) development plan, outlining China's ambitious aspiration to assume global leadership in AI by the year 2030. This initiative underscores the extensive applicability of AI across diverse domains, including manufacturing, law, and medicine. With China establishing itself as a major producer and consumer of medical devices, there has been a notable increase in software registrations. This study aims to study the proliferation of health care-related software development within China. This work presents an overview of the Chinese regulatory framework for medical device software. The analysis covers both software as a medical device and software in a medical device. A comparative approach is employed to examine the regulations governing medical devices with AI and machine learning in China, the United States, and Europe. The study highlights the significant proliferation of health care-related software development within China, which has led to an increased demand for comprehensive regulatory guidance, particularly for international manufacturers. The comparative analysis reveals distinct regulatory frameworks and requirements across the three regions. This paper provides a useful outline of the current state of regulations for medical software in China and identifies the regulatory challenges posed by the rapid advancements in AI and machine learning technologies. Understanding these challenges is crucial for international manufacturers and stakeholders aiming to navigate the complex regulatory landscape."
AI in software programming: understanding emotional responses to GitHub Copilot,"Eshraghian, F; Hafezieh, N; Farivar, F; de Cesare, S",10.1108/ITP-01-2023-0084,2025,"PurposeThe applications of Artificial Intelligence (AI) in various areas of professional and knowledge work are growing. Emotions play an important role in how users incorporate a technology into their work practices. The current study draws on work in the areas of AI-powered technologies adaptation, emotions, and the future of work, to investigate how knowledge workers feel about adopting AI in their work.Design/methodology/approachWe gathered 107,111 tweets about the new AI programmer, GitHub Copilot, launched by GitHub and analysed the data in three stages. First, after cleaning and filtering the data, we applied the topic modelling method to analyse 16,130 tweets posted by 10,301 software programmers to identify the emotions they expressed. Then, we analysed the outcome topics qualitatively to understand the stimulus characteristics driving those emotions. Finally, we analysed a sample of tweets to explore how emotional responses changed over time.FindingsWe found six categories of emotions among software programmers: challenge, achievement, loss, deterrence, scepticism, and apathy. In addition, we found these emotions were driven by four stimulus characteristics: AI development, AI functionality, identity work, and AI engagement. We also examined the change in emotions over time. The results indicate that negative emotions changed to more positive emotions once software programmers redirected their attention to the AI programmer's capabilities and functionalities, and related that to their identity work.Practical implicationsOverall, as organisations start adopting AI-powered technologies in their software development practices, our research offers practical guidance to managers by identifying factors that can change negative emotions to positive emotions.Originality/valueOur study makes a timely contribution to the discussions on AI and the future of work through the lens of emotions. In contrast to nascent discussions on the role of AI in high-skilled jobs that show knowledge workers' general ambivalence towards AI, we find knowledge workers show more positive emotions over time and as they engage more with AI. In addition, this study unveils the role of professional identity in leading to more positive emotions towards AI, as knowledge workers view such technology as a means of expanding their identity rather than as a threat to it."
"AI-smartphone markerless motion capturing of hip, knee, and ankle joint kinematics during countermovement jumps","Barzyk, P; Zimmermann, P; Stein, M; Keim, D; Gruber, M",10.1002/ejsc.12186,2024,"Recently, AI-driven skeleton reconstruction tools that use multistage computer vision pipelines were designed to estimate 3D kinematics from 2D video sequences. In the present study, we validated a novel markerless, smartphone video-based artificial intelligence (AI) motion capture system for hip, knee, and ankle angles during countermovement jumps (CMJs). Eleven participants performed six CMJs. We used 2D videos created by a smartphone (Apple iPhone X, 4K, 60 fps) to create 24 different keypoints, which together built a full skeleton including joints and their connections. Body parts and skeletal keypoints were localized by calculating confidence maps using a multilevel convolutional neural network that integrated both spatial and temporal features. We calculated hip, knee, and ankle angles in the sagittal plane and compared it with the angles measured by a VICON system. We calculated the correlation between both method's angular progressions, mean squared error (MSE), mean average error (MAE), and the maximum and minimum angular error and run statistical parametric mapping (SPM) analysis. Pearson correlation coefficients (r) for hip, knee, and ankle angular progressions in the sagittal plane during the entire movement were 0.96, 0.99, and 0.87, respectively. SPM group-analysis revealed some significant differences only for ankle angular progression. MSE was below 5.7 degrees, MAE was below 4.5 degrees, and error for maximum amplitudes was below 3.2 degrees. The smartphone AI motion capture system with the trained multistage computer vision pipeline was able to detect, especially hip and knee angles in the sagittal plane during CMJs with high precision from a frontal view only. Smartphone-based movement analysis can accurately estimate joint angles during countermovement jumps. AI-controlled algorithms calculate joint angles in the sagittal plane from a smartphone video in a frontal view. The study demonstrates the potential of AI-controlled movement analysis with one camera for 3D kinematics. Future research should focus on expanding the training data to enable the analysis of more complex movements and improve temporal modeling."
Harnessing robotics for environmental economics and energy strategy: Advancing global economic policies towards green growth,"Zhao, D; Gao, YY; Wu, ZI; Shabaz, M",10.1016/j.esr.2024.101504,2024,"The composition of energy consumption is shifting from fossil fuels to renewable energy sources. Determining if reducing energy use may increase green factor production (GFP) is thus crucial. This study, which examines data from 2013 to 2019, indicates that GFP is impacted by both the Robotics and natural resource markets. It conducts a comprehensive analysis of Robotics's impact on the development of green economies using China as a case study and a suitable mathematical model, namely a static panel model (SPM). By applying robotics analytical powers and predictive modelling, the research sheds light on how economic growth and environmental preservation might be balanced, or green growth. The magazine aims to find synergies that support the incorporation of AI-driven solutions into policy frameworks, promoting a shift toward more environmentally sustainable practices globally by carefully analyzing global economic policies. It has been observed that different industries and development phases have other impacts of AI on carbon intensity. Furthermore, this investigation dissects GFP into its component elements, green efficiency improvement effects, and technological advancement to elucidate how AI influences green economic growth."
Tales From the Trenches: Expectations and Challenges From Practice for Code Review in the Generative AI Era,"Davila, N; Melegati, J; Wiese, I",10.1109/MS.2024.3428439,2024,"In this study, we investigate what has been discussed about generative AI in the code review context by performing a gray literature review. We analyzed 42 documents and found insights from practice and proposals of solutions using generative AI models."
Enhancing Agile Software Development: A Novel Approach to Automated Requirements Prioritization,"Izhar, R; Cosh, K; Bhatti, SN",10.1109/JCSSE61278.2024.10613648,2024,"In Agile Software Development (ASD), accurately comprehending and prioritizing project requirements is crucial for aligning project results with stakeholder expectations and achieving success. The Rule-Based Automated Requirements Prioritization (RAR-P) paradigm is novel in using Natural Language Processing (NLP) and Machine Learning (ML) to improve requirements prioritization (RP). The RAR-P model uses advanced text preprocessing, tokenization, TF-IDF vectorization, cosine similarity measures, log likelihood calculations, and K-Means clustering to systematically analyze and priorities user requirements based on their semantic importance. This innovative approach surpasses the constraints of conventional prioritization methods by automating the decision-making process, guaranteeing a more profound and impartial alignment of project outcomes with the subtle requirements of stakeholders. This article explores the creation and use of the RAR-P model, demonstrating its ability to greatly improve the efficiency and efficacy of requirement prioritization in ASD. The RAR-P model has been identified as a crucial tool for transforming software development projects, offering the potential for improved success rates and more efficient resource allocation in project management. This study introduces an innovative solution to a persistent problem and also creates opportunities for future developments in agile project management approaches."
POLARIS: A framework to guide the development of Trustworthy AI systems,"Baldassarre, MT; Gigante, D; Kalinowski, M; Ragone, A",10.1145/3644815.3644947,2024,"In the ever-expanding landscape of Artificial Intelligence (AI), where innovation thrives and new products and services are continuously being delivered, ensuring that AI systems are designed and developed responsibly throughout their entire lifecycle is crucial. To this end, several AI ethics principles and guidelines have been issued to which AI systems should conform. Nevertheless, relying solely on high-level AI ethics principles is far from sufficient to ensure the responsible engineering of AI systems. In this field, AI professionals often navigate by sight. Indeed, while recommendations promoting Trustworthy AI (TAI) exist, they are often high-level statements difficult to translate into concrete implementation strategies. Currently, there is a significant gap between high-level AI ethics principles and low-level concrete practices for AI professionals. To address this challenge, our work presents an experience report where we develop a novel holistic framework for Trustworthy AI - designed to bridge the gap between theory and practice - and report insights from its application in an industrial case study. The framework builds up from the results of a systematic review of the state of the practice as well as a survey and think-aloud interviews with 34 AI practitioners. The framework, unlike most of the ones in literature, is designed to provide actionable guidelines and tools to support different types of stakeholders throughout the entire Software Development Life Cycle (SDLC). Our goal is to empower AI professionals to confidently navigate the ethical dimensions of TAI through practical insights, ensuring that the vast potential of AI is exploited responsibly for the benefit of society as a whole."
In-IDE Human-AI Experience in the Era of Large Language Models; A Literature Review,"Sergeyuk, A; Titov, S; Izadi, M",10.1145/3643796.3648463,2024,"Integrated Development Environments (IDEs) have become central to modern software development, especially with the integration of Artificial Intelligence (AI) to enhance programming efficiency and decision-making. The study of in-IDE Human-AI Experience is critical in understanding how these AI tools are transforming the software development process, impacting programmer productivity, and influencing code quality. We conducted a literature review to study the current state of in-IDE Human-AI Experience research, bridging a gap in understanding the nuanced interactions between programmers and AI assistants within IDEs. By analyzing 36 selected papers, our study illustrates three primary research branches: Design, Impact, and Quality of Interaction. The trends, challenges, and opportunities identified in this paper emphasize the evolving landscape of software development and inform future directions for research, and development in this dynamic field. Specifically, we invite the community to investigate three aspects of these interactions: designing task-specific user interface, building trust, and improving readability."
A Comparison of the Effectiveness of ChatGPT and Co-Pilot for Generating Quality Python Code Solutions,"Nikolaidis, N; Flamos, K; Gulati, K; Feitosa, D; Ampatzoglou, A; Chatzigeorgiou, A",10.1109/SANER-C62648.2024.00018,2024,"Artificial intelligence (AI) has become increasingly popular in software development to automate tasks and improve efficiency. AI has the potential to help while developing or maintaining software, in the sense that it can produce solutions out of a textual requirement specification, and understand code to provide suggestion on how a new requirement could be implemented. In this paper, we focus on the first scenario. Two AI-powered tools that have the potential to revolutionize the way software is developed are OpenAI's ChatGPT and GitHub's Copilot. In this paper, we used LeetCode, a popular platform for technical interview preparation and personal upskilling (self-learning), to evaluate the effectiveness of ChatGPT and Copilot on a set of coding problems, along with ChatGPT's ability to correct itself when provided with feedback. The analysis of the effectiveness can lead to various conclusions, such as on if these solutions are ready to take over coding roles, and to what extent several parameters (difficulty and quality requirements) influence this result. Solutions have been generated for 60 problems using ChatGPT and Copilot, for the Python programming language. We investigated the performance of the models, the recurrent kinds of errors, and the resulting code quality. The evaluation revealed that ChatGPT and Copilot can be effective tools for generating code solutions for easy problems while both models are prone to syntax and semantic errors. Small improvements are observed for ode quality metrics across iterations, although the improvement pattern is not consistently monotonic, questioning ChatGPT's awareness of the quality of its own solutions. Nevertheless, the improvement that was found along iterations, highlights the potential of AI and humans, acting as partners, in providing the optimal combination. The two models demonstrate a limited capacity for understanding context. Although AI-powered coding tools driven by large language models have the potential to assist developers in their coding tasks, they should be used with caution and in conjunction with human coding expertise. Developer intervention is necessary not only to debug errors but also to ensure high-quality and optimized code."
The State of Generative AI Adoption from Software Practitioners' Perspective: An Empirical Study,"Simaremare, M; Edison, H",10.1109/SEAA64295.2024.00024,2024,"Context: Generative AI (GenAI) brings new opportunities to the software industry and the digital economy in a broader context. Objective: This study aimed to explore and capture the practitioners' perception of GenAI adoption in the fast-paced software industry in the context of developing countries. Method: We conducted online focus group discussions with 18 practitioners from various roles to collect qualitative data. The practitioners have an average of 7.8 years of working experience and have used GenAI for over a year. We employed thematic analysis and the Human-AI Collaboration and Adaptation Framework (HACAF) to identify the influencing factors of GenAI adoption, such as awareness, use cases, and challenges. Results: The adoption of GenAI technology is evident from practitioners. We identified 22 practical use cases, three of which were novel, i.e., contextualizing solutions, assisting the internal audit process, and benchmarking the internal software development process. We also discovered seven key challenges associated with the GenAI adoption, two of which were novel, namely, no matching use cases and unforeseen benefits. These challenges slow GenAI adoption and potentially hinder developing countries from entering a high-skill industry. Conclusion: While the adoption of GenAI technology is promising, industry-academia collaboration is needed to find solutions and strategies to address the challenges and maximize its potential benefits."
Identifying the Factors That Influence Trust in AI Code Completion,"Brown, A; D'Angelo, S; Murillo, A; Jaspan, C; Green, C",10.1145/3664646.3664757,2024,"AI-powered software development tooling is changing the way that developers interact with tools and write code. However, the ability for AI to truly transform software development may depend on developers' levels of trust in these tools, which has consequences for tool adoption and repeated usage. In this work, we take a mixed-methods approach to measure the factors that influence developers' trust in AI-powered code completion. We found that characteristics about the AI suggestion itself (e.g., the quality of the suggestion), the developer interacting with the suggestion (e.g., their expertise in a language), and the context of the development work (e.g., was the suggestion in a test file) all influenced acceptance rates of AI-powered code suggestions. Based on these findings we propose a number of recommendations for the design of AI-powered development tools to improve trust."
Success Prediction of Crowdsourced Projects for Competitive Crowdsourced Software Development,"Rashid, T; Anwar, S; Jaffar, MA; Hakami, H; Baashirah, R; Umer, Q",10.3390/app14020489,2024,"Competitive Crowdsourcing Software Development (CCSD) is popular among academics and industries because of its cost-effectiveness, reliability, and quality. However, CCSD is in its early stages and does not resolve major issues, including having a low solution submission rate and high project failure risk. Software development wastes stakeholders' time and effort as they cannot find a suitable solution in a highly dynamic and competitive marketplace. It is, therefore, crucial to automatically predict the success of an upcoming software project before crowdsourcing it. This will save stakeholders' and co-pilots' time and effort. To this end, this paper proposes a well-known deep learning model called Bidirectional Encoder Representations from Transformers (BERT) for the success prediction of Crowdsourced Software Projects (CSPs). The proposed model is trained and tested using the history data of CSPs collected from TopCoder using its REST API. The outcomes of hold-out validation indicate a notable enhancement in the proposed approach compared to existing methods, with increases of 13.46%, 8.83%, and 11.13% in precision, recall, and F1 score, respectively."
Use of ChatGPT as an Assistant in the End-to-End Test Script Generation for Android Apps,"GarcÃ­a, B; Leotta, M; Ricca, F; Whitehead, J",10.1145/3678719.3685691,2024,"Automated testing is crucial in software development to ensure that applications perform as intended. However, generating automated End-to-End (E2E) tests can be time-consuming and challenging, especially for junior developers. This study investigates the use of ChatGPT, a popular Generative Artificial Intelligence (GenAI) model, as an assistant in developing automated E2E test scripts for Android apps. We present an empirical study that compares the effort required to create E2E test scripts and the resulting reliability of these tests using two treatments: manually and assisted by ChatGPT. We used Gherkin, a domain-specific language that allows non-technical practitioners to define test scenarios using a human-readable syntax. Our findings indicate that using ChatGPT significantly reduces the time required to develop automated test scripts without compromising the reliability of the scripts. Statistical analysis shows a notable reduction in development time for the ChatGPT-assisted group compared to the manual group, with a large effect size. While the reliability of the tests did not show a significant difference between the two groups, the results suggest practical benefits in terms of efficiency."
"May the Source Be with You: On ChatGPT, Cybersecurity, and Secure Coding","Gasiba, TE; Iosif, AC; Kessba, I; Amburi, S; Lechner, U; Pinto-Albuquerque, M",10.3390/info15090572,2024,"Software security is an important topic that is gaining more and more attention due to the rising number of publicly known cybersecurity incidents. Previous research has shown that one way to address software security is by means of a serious game, the CyberSecurity Challenges, which are designed to raise awareness of software developers of secure coding guidelines. This game, proven to be very successful in the industry, makes use of an artificial intelligence technique (laddering technique) to implement a chatbot for human-machine interaction. Recent advances in machine learning have led to a breakthrough, with the implementation and release of large language models, now freely available to the public. Such models are trained on a large amount of data and are capable of analyzing and interpreting not only natural language but also source code in different programming languages. With the advent of ChatGPT, and previous state-of-the-art research in secure software development, a natural question arises: to what extent can ChatGPT aid software developers in writing secure software? In this work, we draw on our experience in the industry, and also on extensive previous work to analyze and reflect on how to use ChatGPT to aid secure software development. Towards this, we conduct two experiments with large language models. Our engagements with ChatGPT and our experience in the field allow us to draw conclusions on the advantages, disadvantages, and limitations of the usage of this new technology."
TransformCode: A Contrastive Learning Framework for Code Embedding via Subtree Transformation,"Xian, ZX; Huang, RB; Towey, D; Fang, CR; Chen, ZY",10.1109/TSE.2024.3393419,2024,"Artificial intelligence (AI) has revolutionized software engineering (SE) by enhancing software development efficiency. The advent of pre-trained models (PTMs) leveraging transfer learning has significantly advanced AI for SE. However, existing PTMs that operate on individual code tokens suffer from several limitations: They are costly to train and fine-tune; and they rely heavily on labeled data for fine-tuning on task-specific datasets. In this paper, we present TransformCode, a novel framework that learns code embeddings in a contrastive learning manner. Our framework is encoder-agnostic and language-agnostic, which means that it can leverage any encoder model and handle any programming language. We also propose a novel data-augmentation technique called abstract syntax tree (AST) transformation, which applies syntactic and semantic transformations to the original code snippets, to generate more diverse and robust samples for contrastive learning. Our framework has several advantages over existing methods: (1) It is flexible and adaptable, because it can easily be extended to other downstream tasks that require code representation (such as code-clone detection and classification); (2) it is efficient and scalable, because it does not require a large model or a large amount of training data, and it can support any programming language; (3) it is not limited to unsupervised learning, but can also be applied to some supervised learning tasks by incorporating task-specific labels or objectives; and (4) it can also adjust the number of encoder parameters based on computing resources. We evaluate our framework on several code-related tasks, and demonstrate its effectiveness and superiority over the state-of-the-art methods such as SourcererCC, Code2vec, and InferCode."
A Disruptive Research Playbook for Studying Disruptive Innovations,"Storey, MA; Russo, D; Novielli, N; Kobayashi, T; Wang, D",10.1145/3678172,2024,"As researchers today, we are witnessing a fundamental change in our technologically-enabled world due to the advent and diffusion of highly disruptive technologies such as generative Artificial Intelligence (AI), Augmented Reality (AR) and Virtual Reality (VR). In particular, software engineering has been profoundly affected by the transformative power of disruptive innovations for decades, with a significant impact of technical advancements on social dynamics due to its socio-technical nature. In this article, we reflect on the importance of formulating and addressing research problems in software engineering through a socio-technical lens, thus ensuring a holistic understanding of the complex phenomena in this field. We propose a research playbook with the aim of providing a guide to formulate compelling and socially relevant research questions and to identify the appropriate research strategies for empirical investigations, with an eye on the long-term implications of technologies or their use. We showcase how to apply the research playbook. Firstly, we show how it can be used retrospectively to reflect on a prior disruptive technology, Stack Overflow, and its impact on software development. Secondly, we show how it can be used to question the impact of two current disruptive technologies: AI and AR/VR. Finally, we introduce a specialized GPT model to support the researcher in framing future investigations. We conclude by discussing the broader implications of adopting the playbook for both researchers and practitioners in software engineering and beyond."
Generative Models for Source Code: Fine-Tuning Techniques for Structured Pattern Learning,"Franzoni, V; Tagliente, S; Milani, A",10.3390/technologies12110219,2024,"This study addresses the problem of how to automatically generate source code that is not only functional, but also well-structured, readable, and maintainable. Existing generative models for source code often produce functional code, but they lack consistency in structure and adherence to coding standards, essential for integration into existing application development projects and long-term software maintenance. By training the model on specific code structures, including a dataset with Italian annotations, the proposed methodology ensures that the generated code is compliant with both the functional requirements and the pre-defined coding standards. The methodology proposed in this study applies transfer learning techniques on the DeepSeek Coder model, to refine pre-trained models to generate code that integrates additional structuring constraints. By training the model on specific code structures, including a dataset with Italian comments, the proposed methodology ensures that the generated code meets both functional requirements and coding structure. Experimental results, evaluated using the perplexity metric, demonstrate the effectiveness of the proposed approach, which impacts the goals of reducing errors, and ultimately improves software development quality."
Identification of Software Bugs by Analyzing Natural Language-BasedRequirements Using Optimized Deep Learning Features,"ul Haq, QM; Arif, F; Aurangzeb, K; ul Ain, N; Khan, JA; Rubab, S; Anwar, MS",10.32604/cmc.2024.047172,2024,"Software project outcomes heavily depend on natural language requirements, often causing diverse interpretationsand issues like ambiguities and incomplete or faulty requirements. Researchers are exploring machine learningto predict software bugs, but a more precise and general approach is needed. Accurate bug prediction is crucialfor software evolution and user training, prompting an investigation into deep and ensemble learning methods.However, these studies are not generalized and efficient when extended to other datasets. Therefore, this paperproposed a hybrid approach combining multiple techniques to explore their effectiveness on bug identificationproblems. The methods involved feature selection, which is used to reduce the dimensionality and redundancyof features and select only the relevant ones; transfer learning is used to train and test the model on differentdatasets to analyze how much of the learning is passed to other datasets, and ensemble method is utilized toexplore the increase in performance upon combining multiple classifiers in a model. Four National Aeronauticsand Space Administration (NASA) and four Promise datasets are used in the study, showing an increase in themodel's performance by providing better Area Under the Receiver Operating Characteristic Curve (AUC-ROC)values when different classifiers were combined. It reveals that using an amalgam of techniques such as those usedin this study, feature selection, transfer learning, and ensemble methods prove helpful in optimizing the softwarebug prediction models and providing high-performing, useful end mode"
Cognitive Complexity Analysis and Optimization Tool for Java,"De Silva, DI; Wijesundara, DAR; Perera, VHP; Rathnayake, MRTN; Udumulla, CJ; Godapitiya, V",10.1007/978-981-97-3562-4_6,2024,"Cognitive complexity, often neglected initially, becomes a significant challenge as software systems expand. This article introduces the Cognitive Complexity Analysis and Optimization Tool to address this issue comprehensively. Cognitive Complexity Analysis and Optimization Tool incorporates various metrics, graphical visualizations, and suggestions for complexity reduction, enhancing code quality and maintainability. The study outlines specific objectives: reducing cognitive complexity, enhancing code reliability, streamlining software maintenance, providing reporting and visualization capabilities, and offering an interface for automatic code complexity reduction. This research focuses on modern software development challenges, emphasizing the importance of lower cognitive complexity and streamlined maintenance processes. The article reviews extensive research in cognitive complexity metrics, highlighting the limitations of existing methods and introducing the Improved CB measure as the proposed tool's primary metric. The methodology adheres to the Institute of Electrical and Electronics Engineers Standard for Software Quality Metric Methodology, ensuring rigorous quality standards. In conclusion, the proposed tool automates the computation of cognitive complexity, code size, and code maintainability, offering enhanced insights into Java-based system development. It streamlines code refactoring and provides optimized code suggestions, contributing to cost and time savings in software development and maintenance."
A Catalog of Transformations to Remove Smells From Natural Language Tests,"Aranda, M; Oliveira, N; Soares, E; Ribeiro, M; Romao, D; Patriota, U; Gheyi, R; Souza, E; Machado, I",10.1145/3661167.3661225,2024,"Test smells can pose difficulties during testing activities, such as poor maintainability, non-deterministic behavior, and incomplete verification. Existing research has extensively addressed test smells in automated software tests but little attention has been given to smells in natural language tests. While some research has identified and catalogued such smells, there is a lack of systematic approaches for their removal. Consequently, there is also a lack of tools to automatically identify and remove natural language test smells. This paper introduces a catalog of transformations designed to remove seven natural language test smells and a companion tool implemented using Natural Language Processing (NLP) techniques. Our work aims to enhance the quality and reliability of natural language tests during software development. The research employs a two-fold empirical strategy to evaluate its contributions. First, a survey involving 15 software testing professionals assesses the acceptance and usefulness of the catalog's transformations. Second, an empirical study evaluates our tool to remove natural language test smells by analyzing a sample of real-practice tests from the Ubuntu OS. The results indicate that software testing professionals find the transformations valuable. Additionally, the automated tool demonstrates a good level of precision, as evidenced by a F-Measure rate of 83.70%."
Write me this Code: An Analysis of ChatGPT Quality for Producing Source Code,"Moratis, K; Diamantopoulos, T; Nastos, DN; Symeonidis, A",10.1145/3643991.3645070,2024,"Developers nowadays are increasingly turning to large language models (LLMs) like ChatGPT to assist them with coding tasks, inspired by the promise of efficiency and the advanced capabilities they offer. However, this raises important questions about the ease of integration and the safety of incorporating these tools into the development process. To investigate these questions, this paper examines a set of ChatGPT conversations. Upon annotating the conversations according to the intent of the developer, we focus on two critical aspects: firstly, the ease with which developers can produce suitable source code using ChatGPT, and, secondly, the quality aspects of the generated source code, determined by the compliance to standards and best practices. We research both the quality of the generated code itself and its impact on the project of the developer. Our results indicate that ChatGPT can be a useful tool for software development when used with discretion."
An assessment of large language models for OpenMP-based code parallelization: a user perspective,"Misic, M; Dodovic, M",10.1186/s40537-024-01019-z,2024,"Large language models have sparked a lot of attention in the research community in recent days, especially with the introduction of practical tools such as ChatGPT and Github Copilot. Their ability to solve complex programming tasks was also shown in several studies and commercial solutions increasing the interest in using them for software development in different fields. High performance computing is one of such fields, where parallel programming techniques have been extensively used to utilize raw computing power available in contemporary multicore and manycore processors. In this paper, we perform an evaluation of the ChatGPT and Github Copilot tools for OpenMP-based code parallelization using a proposed methodology. We used nine different benchmark applications which represent typical parallel programming workloads and compared their OpenMP-based parallel solutions produced manually and using ChatGPT and Github Copilot in terms of obtained speedup, applied optimizations, and quality of the solution. ChatGPT 3.5 and Github Copilot installed with Visual Studio Code 1.88 were used. We concluded that both tools can produce correct parallel code in most cases. However, performance-wise, ChatGPT can match manually produced and optimized parallel code only in simpler cases, as it lacks a deeper understanding of the code and the context. The results are much better with Github Copilot, where much less effort is needed to obtain correct and performant parallel solution."
TCP: A Tensor Contraction Processor for AI Workloads,"Kim, H; Choi, Y; Park, J; Bae, B; Jeong, H; Lee, SM; Yeon, J; Kim, M; Park, C; Gu, B; Lee, C; Bae, J; Bae, S; Cha, Y; Choe, W; Choi, J; Ha, J; Han, H; Hwang, N; Hwang, S; Jang, K; Je, H; Jeon, H; Jeon, J; Jeong, H; Jung, Y; Kang, D; Kim, H; Kim, M; Kim, M; Kim, S; Kim, S; Kim, W; Kim, Y; Kim, Y; Ku, Y; Lee, JK; Lee, J; Lee, K; Lee, S; Noh, M; Oh, H; Park, G; Park, S; Seo, J; Seong, J; Paik, J; Lopes, NP; Yoo, S",10.1109/ISCA59077.2024.00069,2024,"We introduce a novel tensor contraction processor (TCP) architecture that offers a paradigm shift from traditional architectures that rely on fixed-size matrix multiplications. TCP aims at exploiting the rich parallelism and data locality inherent in tensor contractions, thereby enhancing both efficiency and performance of AI workloads. TCP is composed of coarse-grained processing elements (PEs) to simplify software development. In order to efficiently process operations with diverse tensor shapes, the PEs are designed to be flexible enough to be utilized as a large-scale single unit or a set of small independent compute units. We aim at maximizing data reuse on both levels of inter and intra compute units. To do that, we propose a circuit switch-based fetch network to flexibly connect compute units to enable inter-compute unit data reuse. We also exploit input broadcast to multiple contraction engines and input buffer based reuse to further exploit reuse behavior in tensor contraction. Our compiler explores the design space of tensor contractions considering tensor shapes and the order of their associated loop operations as well as the underlying accelerator architecture. A TCP chip was designed and fabricated in 5nm technology as the second-generation product of Furiosa AI, offering 256/512/1024 TOPS (BF16/FP8 or INT8/INT4) with 256 MB SRAM and 1.5 TB/s 48 GB HBM3 under 150 W TDP. Commercialization will start in August 2024. We performed an extensive case study of running the LLaMA-2 7B model and evaluated its performance and power efficiency on various configurations of sequence length and batch size. For this model, TCP is 2.7x and 4.1x better than H100 and L40s, respectively, in terms of performance per watt."
Leveraging Artificial Intelligence and Provenance Blockchain Framework to Mitigate Risks in Cloud Manufacturing in Industry 4.0,"Umer, MA; Belay, EG; Gouveia, LB",10.3390/electronics13030660,2024,"Cloud manufacturing is an evolving networked framework that enables multiple manufacturers to collaborate in providing a range of services, including design, development, production, and post-sales support. The framework operates on an integrated platform encompassing a range of Industry 4.0 technologies, such as Industrial Internet of Things (IIoT) devices, cloud computing, Internet communication, big data analytics, artificial intelligence, and blockchains. The connectivity of industrial equipment and robots to the Internet opens cloud manufacturing to the massive attack risk of cybersecurity and cyber crime threats caused by external and internal attackers. The impacts can be severe because the physical infrastructure of industries is at stake. One potential method to deter such attacks involves utilizing blockchain and artificial intelligence to track the provenance of IIoT devices. This research explores a practical approach to achieve this by gathering provenance data associated with operational constraints defined in smart contracts and identifying deviations from these constraints through predictive auditing using artificial intelligence. A software architecture comprising IIoT communications to machine learning for comparing the latest data with predictive auditing outcomes and logging appropriate risks was designed, developed, and tested. The state changes in the smart ledger of smart contracts were linked with the risks so that the blockchain peers can detect high deviations and take actions in a timely manner. The research defined the constraints related to physical boundaries and weightlifting limits allocated to three forklifts and showcased the mechanisms of detecting risks of breaking these constraints with the help of artificial intelligence. It also demonstrated state change rejections by blockchains at medium and high-risk levels. This study followed software development in Java 8 using JDK 8, CORDA blockchain framework, and Weka package for random forest machine learning. As a result of this, the model, along with its design and implementation, has the potential to enhance efficiency and productivity, foster greater trust and transparency in the manufacturing process, boost risk management, strengthen cybersecurity, and advance sustainability efforts."
Project-specific code summarization with in-context learning,"Yun, SB; Lin, SH; Gu, XD; Shen, BJ",10.1016/j.jss.2024.112149,2024,"Automatically generating summaries for source code has emerged as a valuable task in software development. While state-of-the-art (SOTA) approaches have demonstrated significant efficacy in summarizing general code, they seldom concern code summarization for a specific project. Project-specific code summarization (PCS) poses special challenges due to the scarce availability of training data and the unique styles of different projects. In this paper, we empirically analyze the performance of Large Language Models (LLMs) on PCS tasks. Our study reveals that using appropriate prompts is an effective way to solicit LLMs for generating project-specific code summaries. Based on these findings, we propose a novel project-specific code summarization approach called P-CodeSum. P-CodeSum gathers a repository-level pool of (code, summary) examples to characterize the project-specific features. Then, it trains a neural prompt selector on a high-quality dataset crafted by LLMs using the example pool. The prompt selector offers relevant and high-quality prompts for LLMs to generate project- specific summaries. We evaluate against a variety of baseline approaches on six PCS datasets. Experimental results show that the P-CodeSum improves the performance by 5.9% (RLPG) to 101.51% (CodeBERT) on BLEU-4 compared to the state-of-the-art approaches in project-specific code summarization."
Optimizing OCR Performance for Programming Videos: The Role of Image Super-Resolution and Large Language Models,"Alahmadi, MD; Alshangiti, M",10.3390/math12071036,2024,"The rapid evolution of video programming tutorials as a key educational resource has highlighted the need for effective code extraction methods. These tutorials, varying widely in video quality, present a challenge for accurately transcribing the embedded source code, crucial for learning and software development. This study investigates the impact of video quality on the performance of optical character recognition (OCR) engines and the potential of large language models (LLMs) to enhance code extraction accuracy. Our comprehensive empirical analysis utilizes a rich dataset of programming screencasts, involving manual transcription of source code and the application of both traditional OCR engines, like Tesseract and Google Vision, and advanced LLMs, including GPT-4V and Gemini. We investigate the efficacy of image super-resolution (SR) techniques, namely, enhanced deep super-resolution (EDSR) and multi-scale deep super-resolution (MDSR), in improving the quality of low-resolution video frames. The findings reveal significant improvements in OCR accuracy with the use of SR, particularly at lower resolutions such as 360p. LLMs demonstrate superior performance across all video qualities, indicating their robustness and advanced capabilities in diverse scenarios. This research contributes to the field of software engineering by offering a benchmark for code extraction from video tutorials and demonstrating the substantial impact of SR techniques and LLMs in enhancing the readability and reusability of code from these educational resources."
Easing Adoption of Model Based System Engineering With Application of Generative AI,"Patel, A; Maheshwaran, Y; Santhya, P",10.1109/SPACE63117.2024.10667868,2024,"Though Model based System Engineering (MBSE) for complex products has manifold advantages, there is a need to find techniques to facilitate its adoption. Here, we discuss ML technique based on Generative AI particularly Natural Language Processing (NLP) and propose its integration with existing MBSE tools. The conceptual framework of generating requirements from unstructured documents using NLP is presented. The same technique to be utilized for generating Systems Modeling Language (SysML) model entities by training NLP model with labelled data. Once the model is trained and integrated with SysML tool, it eases applying MBSE for multi-domain multidisciplinary complex systems."
Impacts of the Usage of Generative Artificial Intelligence on Software Development Process,"Santos, PD; Chamon, A; Moura, PND; Diirr, B; Alvim, ACD; dos Santos, RP",10.1145/3658271.3658337,2024,"Context: Over the years, tools have been created to improve the execution of development process activities. The emergence of generative Artificial Intelligence (AI) and, more recently, the launch and dissemination of Copilot, ChatGPT-3 and other generative tools, have broadened the discussion about the possibility of using conversational generative AI tools in diverse development tasks. Problem: There is still a lack of secondary studies to map the literature about how software development process activities can be affected by the usage of generative AI tools. Solution: This study aims to identify in which activities of the software development process Natural Language (NL) generative AI tools have been used and how they can impact requirements specification, design/architecture, development and testing activities. IS Theory: The study was developed under the aegis of the Task Technology Fit theory. Method: This work presents the results of a Systematic Mapping Review (SMR) carried out to collect research results that investigate the application of generative AI tools in the software development process. Results: Results indicate that the main activities affected are development and testing and that, although there are still some issues to be addressed, there are benefits in using AI generative tools compared to using more traditional methods like human-human pair programming and code testing made by software engineering professionals. Contribution: It was possible to collect studies to identify in which activities of the software development process generative AI tools can be applied and what are the impacts of using this technology."
Using LLMs for Use Case Modelling of IoT Systems: An Experience Report,"Tabassum, MR; Ritchie, MJ; Mustafiz, S; Kienzle, J",10.1145/3652620.3687810,2024,"Requirements engineering (RE) plays an essential role in the success of system and software development. Textual use case models are valuable for capturing diverse scenarios describing the interactions between the system and its actors, but their development, particularly for the Internet of Things (IoT), can be tedious and error-prone due to the added complexities and heterogeneous nature of such systems. Automating requirements elicitation and specification tasks with the use of generative AI is highly desirable. This paper explores the potential of large language models (LLMs) for generating interaction models for IoT systems from informal problem descriptions. We investigate the capabilities of OpenAI's GPT-4 and Google's Gemini for generating standard and UCM4IoT textual use cases by carrying out a comparative study using four IoT applications. While both of these LLMs show promise as supporting tools, our findings indicate a need for further refinement and domain-specific training to enhance their precision and reliability in requirements development for the IoT domain."
ChatGPT Chats Decoded: Uncovering Prompt Patterns for Superior Solutions in Software Development Lifecycle,"Wu, LX; Zhao, YJ; Hou, XY; Liu, TM; Wang, HY",10.1145/3643991.3645069,2024,"The advent of Large Language Models (LLMs) like ChatGPT has markedly transformed software development, aiding tasks from code generation to issue resolution with their human-like text generation. Nevertheless, the effectiveness of these models greatly depends on the nature of the prompts given by developers. Therefore, this study delves into the DevGPT dataset, a rich collection of developer-ChatGPT dialogues, to unearth the patterns in prompts that lead to effective problem resolutions. The underlying motivation for this research is to enhance the collaboration between human developers and AI tools, thereby improving productivity and problem-solving efficacy in software development. Utilizing a combination of textual analysis and data-driven approaches, this paper seeks to identify the attributes of prompts that are associated with successful interactions, providing crucial insights for the strategic employment of ChatGPT in software engineering environments."
Design Principles for Collaborative Generative AI Systems in Software Development,"Chen, J; Zacharias, J",10.1007/978-3-031-61175-9_23,2024,"Generative artificial intelligence (GAI) has the potential to transform software development practices with prior research indicating significant overall enhancements in developers' productivity. However, there exists a lack of design knowledge for organization-specific GAI systems to assist software development. To bridge this research gap, we derive a design framework for collaborative GAI systems in software development following design science research. Specifically, we conducted eight interviews with practitioners and reviewed extant literature to formulate design requirements and design principles. In our analysis of the literature and our qualitative data, we identify problems surrounding usability, data privacy, hallucination and transparency. To address these problems, we propose GAI system designs that enable user-centricity, data protection, quality control and communication. Our findings contribute valuable design knowledge to the field of generative AI and organizational software development practices."
AI-Integrated Cyber Security Risk Management Framework for IT Projects,"Jabbar, H; Al-Janabi, S; Syms, F",10.1109/IJCC64742.2024.10847294,2024,"Amid the rising complexity and frequency of cyber threats, integrating emerging technologies such as artificial intelligence (AI) within IT project management frameworks is crucial for fostering cyber resilience. This paper proposes an AI-enhanced Cyber-Resilient IT Project Management Framework that integrates predictive analytics and machine learning across the cybersecurity process. The proposed framework emphasizes governance and risk management through proactive risk assessment, real-time threat detection, and automated incident response, enhancing resilience against evolving threats. The framework's adaptability and effectiveness are illustrated through its potential applications in diverse domains such as forensics, healthcare, and other sectors requiring robust data protection and cybersecurity strategies."
Adding External Artificial Intelligence (AI) into Internal Firm-Wide Smart Dynamic Warehousing Solutions,"Hamilton, JR; Maxwell, SJ; Ali, SA; Tee, S",10.3390/su16103908,2024,"This study advances knowledge in the AI field. It provides deep insight into current industry generative AI inclusion systems. It shows both literature and practical leading industry operations can link, overlap, and complement each other when it comes to AI and understanding its complexities. It shows how to structurally model and link AI inclusions towards delivering a suitable sustainability positioning. It shows approaches to integrate external AI contributions from one firm into another firm's intelligences developments. It shows how to track, and maybe benchmark, the progress of such AI inclusions from either an external or an integrated internal software developer perspective. It shows how to understand and create a more sustainable, AI-integrated business positioning. This study considers firm artificial intelligence (AI) and the inclusion of additional external software developer engineering as another AI related pathway to future firm or industry advancement. Several substantive industrial warehousing throughput areas are discussed. Amazon's 'smart dynamic warehousing' necessitates both digital and generative ongoing AI system prowess. Amazon and other substantive, digitally focused industry warehousing operations also likely benefit from astute ongoing external software developer firm inclusions. This study causally, and stagewise, models significant global software development firms involved in generative AI systems developments-specifically ones designed to beneficially enhance both warehouse operational productivity and its ongoing sustainability. A structural equation model (SEM) approach offers unique perspectives through which substantive firms already using AI can now model and track/benchmark the relevance of their prospective or existing external software developer firms, and so create rapid internal 'net-AI' competencies incorporations and AI capabilities developments through to sustainable operational and performance outcomes solutions."
Leveraging Large Language Models for Python Unit Test,"Jiri, M; Emese, B; Medlen, P",10.1109/AITest62860.2024.00020,2024,"Generative Artificial Intelligence is becoming an integral and enduring part of our lives, growing more powerful with each passing day. This paper explores Large Language Models and their application in text generation, specifically examining their potential to assist software quality assurance engineers in their daily tasks. Our focus is on the generation of unit tests as a critical component of software development. The research question is simple: Can Generative AI generate comprehensive unit tests? We started with Python and a very simple use case, and if Gen AI is successful, we will continue with complex tasks. Current literature focuses on success, but we are interested in failures as well. How many test cases are missing?"
ChatGPT as a Fullstack Web Developer Early Results,"Abrahamsson, P; Anttila, T; Hakala, J; Ketola, J; Knappe, A; Lahtinen, D; Liukko, V; Poranen, T; Ritala, TM; SetÃ¤lÃ¤, M",10.1007/978-3-031-48550-3_20,2024,"The arrival of ChatGPT has caused a lot of turbulence also in the field of software engineering in the past few months. Little is empirically known about the capabilities of ChatGPT to actually implement a complete system rather than a few code snippets. This paper reports the first-hand experiences from a graduate level student project where a real-life software platform for financial sector was implemented from the scratch by using ChatGPT for all possible software engineering tasks. The main conclusions drawn are as follows: 1) these findings demonstrate the potential for ChatGPT to be integrated into the software engineering workflow, 2) it can be used for creating a base for new components and for dividing coding tasks into smaller pieces, and 3) noticeable enhancements in ChatGPT-4, compared to ChatGPT-3.5, indicate superior working memory and the ability to continue incomplete responses, thereby leading to more coherent and less repetitive dialogues."
Pair Programming With Generative AI,"Spinellis, D",10.1109/MS.2024.3363848,2024,"Generative AI based on large-language models is significantly impacting software development through IDE assistants, cloud-based APIs, and interactive chatbots for coding assistance. It excels in generating and translating code and data, navigating APIs, and creating boilerplate content, thereby enhancing productivity. However, it is prone to generating inaccurate information (hallucinations), erroneous code, and potentially introducing security vulnerabilities. To counter these risks, employing automated analysis tools, conducting rigorous testing, and maintaining a deep understanding of computer science concepts are essential. While generative AI can substantially aid development tasks it is not a replacement for human expertise, especially in understanding complex software, its requirements, and architecture."
The Impact of ChatGPT on Students' Learning Programming Languages,"Aviv, I; Leiba, M; Rika, H; Shani, Y",10.1007/978-3-031-61691-4_14,2024,"This study addresses the gap in understanding the impact of ChatGPT, on Java programming language education. We examined ChatGPT's afinity on undergraduate Information Systems students learning Java through a mixed-methods approach. Quantitatively, we assessed constructs like ChatGPT Prompting Skills, Trust, Objective Values, and their relationship with student satisfaction, revealing mixed effectiveness. Qualitatively, we explored students' perspectives, uncovering insights into ChatGPT's role in coding support and the nuances of its educational impact. Our findings indicate that while ChatGPT can enhance certain aspects of learning, its effectiveness varies with context and task complexity. Key positive findings from the regression analysis indicated that ChatGPT's prompting skills positively impacted both Objective and Subjective Values, suggesting a significant role in enhancing students' understanding and engagement with programming concepts. This positive influence extends to the relationship between Subjective Value and Student Satisfaction, highlighting the importance of students' subjective experiences in their overall satisfaction with learning programming languages. The study contributes to the evolving discourse on AI in education, highlighting the need to integrate LLMs carefully in educational settings. It underscores the importance of aligning AI tools with specific learning objectives and outlines implications for educators and AI developers in optimizing these tools for educational purposes."
Leveraging Artificial Intelligence and Reverse Tip Sample Configuration for Automation of Data Processing in Quantitative Scanning Spreading Resistance Microscopy,"Wouters, L; Peters, K; Lagrain, P; Drees, R; Peric, N; Hantschel, T",10.1002/pssa.202400688,2025,"Scanning probe microscopy (SPM) has become a vital metrology tool for characterizing nanoscale devices with exceptional spatial resolution, driving advances in various fields. However, its low overall throughput remains a major limitation. To address this, the high-efficiency data acquisition capabilities of reverse tip sample (RTS) SPM are combined with automated data processing via artificial intelligence (AI)-based computer vision algorithms. The effectiveness of this approach is demonstrated through a case study of scanning spreading resistance microscopy (SSRM). YOLO (You Only Look Once) models are trained to detect each layer in SSRM resistance maps of a calibration sample, serving as a key step in automating the quantitative SSRM data processing workflow. Models trained on mixed datasets of standard and RTS SPM images (ratio 1:4) achieve an excellent accuracy of 97.8%, while reducing the data collection time fivefold compared to using solely standard calibration datasets. Additionally, the model's strong ability to effectively recognize and exclude measurement artifacts during layer selection further enhances its suitability for real-world applications. This work significantly accelerates the SSRM data analysis by automating the workflow and highlights the potential of RTS SPM as a high-throughput solution for generating AI training data, facilitating faster AI model deployment in SPM applications."
"Public Opinions on ChatGPT : An Analysis of Reddit Discussions by Using Sentiment Analysis, Topic Modeling, and SWOT Analysis","Naing, SZS; Udomwong, P",10.1162/dint_a_00250,2024,"The sudden arrival of AI (Artificial Intelligence) into people's daily lives all around the world was marked by the introduction of ChatGPT, which was officially released on November 30, 2022. This AI invasion in our lives drew the attention of not only tech enthusiasts but also scholars from diverse fields, as its capacity extends across various fields. Consequently, numerous articles and journals have been discussing ChatGPT, making it a headline for several topics. However, it does not reflect most public opinion about the product. Therefore, this paper investigated the public's opinions on ChatGPT through topic modelling, Vader-based sentiment analysis and SWOT analysis. To gather data for this study, 202905 comments from the Reddit platform were collected between December 2022 and December 2023. The findings reveal that the Reddit community engaged in discussions related to ChatGPT, covering a range of topics including comparisons with traditional search engines, the impacts on software development, job market, and education industry, exploring ChatGPT's responses on entertainment and politics, the responses from Dan, the alter ego of ChatGPT, the ethical usage of user data as well as queries related to the AI-generated images. The sentiment analysis indicates that most people hold positive views towards this innovative technology across these several aspects. However, concerns also arise regarding the potential negative impacts associated with this product. The SWOT analysis of these results highlights both the strengths and pain points, market opportunities and threats associated with ChatGPT. This analysis also serves as a foundation for providing recommendations aimed at the product development and policy implementation in this paper."
Enhancing Security in Industrial Application Development: Case Study on Self-Generating Artificial Intelligence Tools,"Sanguino, TDM",10.3390/app14093780,2024,"The emergence of security vulnerabilities and risks in software development assisted by self-generated tools, particularly with regard to the generation of code that lacks due consideration of security measures, could have significant consequences for industry and its organizations. This manuscript aims to demonstrate how such self-generative vulnerabilities manifest in software programming, through a case study. To this end, this work undertakes a methodology that illustrates a practical example of vulnerability existing in the code generated using an AI model such as ChatGPT, showcasing the creation of a web application database, SQL queries, and PHP server-side. At the same time, the experimentation details a step-by-step SQL injection attack process, highlighting the hacker's actions to exploit the vulnerability in the website's database structure, through iterative testing and executing SQL commands to gain access to sensitive data. Recommendations on effective prevention strategies include training programs, error analysis, responsible attitude, integration of tools and audits in software development, and collaboration with third parties. As a result, this manuscript discusses compliance with regulatory frameworks such as GDPR and HIPAA, along with the adoption of standards such as ISO/IEC 27002 or ISA/IEC 62443, for industrial applications. Such measures lead to the conclusion that incorporating secure coding standards and guideline-from organizations such as OWASP and CERT training programs-further strengthens defenses against vulnerabilities introduced by AI-generated code and novice programming errors, ultimately improving overall security and regulatory compliance."
The Fine Balance Between Helping With Your Job and Taking It: AI Code Assistants Come to the Fore,"de Souza, CRB; Rodriguez-Perez, G; Basha, M; Yoon, D; Beschastnikh, I",10.1109/MS.2024.3357787,2024,"AI code generation tools are reshaping the software engineering landscape. We provide recommendations for practitioners interested in these tools based on narratives we have collected regarding two AI code generation tools, GitHub Copilot and Tabnine."
Human-AI interaction: Augmenting decision-making for IT leader's project selection,"Judkins, JT; Hwang, Y; Kim, S",10.1177/02666669241249744,2025,"This research seeks to determine whether IT leaders are willing to leverage Artificial Intelligence (AI) recommendations to augment their decision-making when considering which IT projects to select based on the projects' contribution towards organizational goals and objectives and the IT leaders' level of knowledge and trust in AI. The research design utilized a quantitative survey of 113 IT leaders to understand their willingness to accept AI recommendations that augment their own decision-making while considering organizational criteria for selecting the best IT projects to invest in. AI recommendations were measured based on factors contained within a conceptual AI-driven online recommendation system, while decision-making was measured based on factors such as uncertainty, consequences of decisions, information & goals, motivation, and self-regulation that were designed and validated to examine certain aspects that influence decision-making. IT project contributions were measured based on relevance, risk, reasonableness, basis research return, and business return associated with the project. The research results showed the relationship between IT Project Contributions and IT leader decision was significant in that the leader's perception of IT project contributions towards organizational goals and objectives has a positive influence on the IT project selection decision. Unfortunately, the remaining hypothesized relationships showed no significant outcomes. The research results did not show that IT Leader Decisions may be influenced by AI recommendations, nor the greater knowledge of AI an IT Leader has, the stronger the relationship will be in support of or against AI recommendations, nor that the greater trust in AI an IT Leader has, the stronger the relationship will be in support of or against AI recommendations. This research will provide important insights to understand IT project selection and human-AI collaboration by IT leaders in organizations."
AI Over-Hype: A Dangerous Threat (and How to Fix It),"Johnson, B; Menzies, T",10.1109/MS.2024.3439138,2024,"An ethical approach to AI need not be revolutionary or exceptional. We argue that it is the ethical duty of software professionals to rally against AI over-hype. As shown here, this is not hard to do. If we apply just simple empirical methods, we can better monitor the true progress in AI for software engineering."
Exploring potential implications of intelligent tools for human aspects of software engineering,"Melegati, J; Nascimento, N; Chanin, R; Sales, A; Wiese, I",10.1145/3641822.3641877,2024,"Background. The emergence of tools based on artificial intelligence (AI) to support software development suggests an overhaul on how developers program and interact among themselves. This disruption might bring challenges regarding human and social aspects of the software development process. Objective. This paper is a first exploration of the consequences of AI-based tools for software development teams and their members. Method. We conducted a social science fiction exercise, a sort of thought experiment, narrating two fictional stories about a futuristic software company employing AI-based tools. Then, we evaluated the plausibility of one of the scenarios through a qualitative experiment with 38 students to observe their perception regarding the use of AI-based tools. Results. The stories suggest potential challenges related to the adoption of these tools: a change on how developers perceive themselves, a clash between quantitative and qualitative worker contribution assessment, and the training of future developers to handle the imminent changes on their profession. In the qualitative experiment, we collected evidence supporting negative feelings, such as lack of trust and control and fear of being replaced. We also identified other attitudes and perceptions of developers, such as positive feelings towards AI-based tools. Conclusion. We identified several aspects that might influence the adoption of AI-based tools and their implications for individuals involved. They should be further investigated and represent a challenge for the research on human aspects of software engineering. We also demonstrated the use of social science fiction to explore novel research problems."
The Smart Product Backlog: A Classification Model of User Stories,"Gaona-Cuevas, M; Guerrero, VB; Vera-Rivera, FH",10.1109/ACCESS.2024.3478833,2024,"In agile software development processes, user stories (US) had been used to specify application functionalities from the users' perspective. For intelligent applications leveraging artificial intelligence (AI), the Smart Product Backlog (SPB) has included both AI-implementable and non-AI functionalities. This paper had proposed a model employing supervised machine learning techniques to classify US descriptions based on their technical feasibility for AI implementation. This model had aimed to assist in constructing smart product backlogs (SPB). Classifying US in agile development, particularly with AI, had been a labor-intensive process demanding significant time and expert involvement. Additionally, the lack of a dedicated dataset for this task had limited the applicability of automated methods. This study addressed this challenge by having experts classify the Mendeley US dataset using binary classification (AI and non-AI). The proposal had involved developing an automatic classification model to process US descriptions and distinguish those suitable for AI implementation. This model had leveraged advanced text processing techniques to refine the textual features of the US. Additionally, it had employed three binary classification techniques: logistic regression, k-nearest neighbors (k-NN), and support vector machines (SVM). The model's performance had been evaluated using metrics such as accuracy, loss (Log-Likelihood Loss), precision, recall, F1 score, area under the ROC curve, and specificity to identify the best-performing technique in binary classification. Logistic regression and SVM models had demonstrated high accuracy, with scores of 0.748 and 0.740, respectively. These results had highlighted the potential of an automated tool for recommending US feasible for AI development, thereby supporting decision-making in agile software projects."
A Tool for Automatically Identifying Semantic Conflicts in User Stories by Combining NLP and BERT Model,"Xuan, Z; Wang, TC; Wang, CH; Li, T",10.1109/RE59067.2024.00057,2024,"In agile software development, user stories are provided by different stakeholders, and there may be semantic conflicts between them. It takes a lot of time and manpower to identify semantic conflicts from a large number of user stories, and the identification effect is not ideal. This paper proposes a semantic conflict identifying tool. The tool mainly includes two parts: one is to identify whether there are syntax problems or ambiguity problems in each user story, and the other is to identify whether there are semantic problems between user stories. The tool combines the NLP and BERT model for improving the syntactic and semantic quality of user stories through feedback and interaction. Our preliminary results on 9060 user stories show an F1-Score of 92%. A demo video of this tool is available at https://youtu.be/EGYyI9RclRc."
Deriving Domain Models from User Stories: Human vs. Machines,"Bragilovski, M; van Can, AT; Dalpiaz, F; Sturm, A",10.1109/RE59067.2024.00014,2024,"Domain models play a crucial role in software development, as they provide means for communication among stakeholders, for eliciting requirements, and for representing the information structure behind a database scheme or at the basis of model-driven development. However, creating such models is a tedious activity and automated support may assist in obtaining an initial domain model that can later be enriched by human analysts. In this paper, we propose an experimental comparison of the effectiveness of various approaches for deriving domain models from a given set of user stories. We contrast human derivation with machine derivation; for the latter, we compare (i) the Visual Narrator: an existing rule-based NLP approach; (ii) a machine-learning classifier that we feature engineered; and (iii) a generative AI approach that we constructed via prompt engineering. Based on a benchmark dataset that consists of nine collections of user stories and corresponding domain models, the evaluation indicates that no approach matches human performance, although a tuned version of the machine learning approach comes close. To better understand the results, we qualitatively analyze them and identify differences in the types of false positives as well as other factors that affect performance."
Evaluating Impact of Conventional Code Analysis Against Large Language Models in API Vulnerability Detection,"Yildirim, R; Aydin, K; Cetin, O",10.1145/3655693.3655701,2024,"In the rapidly changing world of digital technologies, application programming interfaces (APIs) have become extremely important to allow different software applications to communicate with each other. This communication has greatly enhanced the capabilities and functionality of web applications. This shift towards using more APIs in software development marks a major change in how digital services connect with each other. However, this progress also brings certain security concerns. The increasing reliance on APIs underscores the importance of employing tools that allow early detection and remediation of security vulnerabilities. In this paper, we detail a study that engaged 10 static code analysers and four popular Large Language Models (LLMs), each queried with two unique prompts. Our focus was on assessing their ability to detect a compilation of 40 API vulnerabilities in the source code, specifically selected to represent each category within the OWASP Top 10 API Security Risks. Our results revealed significant variations in the performance of these tools. ChatGPT 4 emerged as the most effective LLM, with a detection rate of 62.5% for the first prompt and 42.5% for the second prompt. In contrast, LLaMA 2 showed the lowest effectiveness in both prompts. Meanwhile, static code analyser results showed a generally low detection rate of API vulnerabilities. Snyk led the group with a 25% detection rate, while several analysers such as pylint, Pyre, and Trivy did not detect any vulnerabilities. These findings indicate that while static code analysers are valuable in certain contexts, their effectiveness remains lower than LLMs when appropriately prompted."
Goodbye Hello World - Research Questions for a Future CS1 Curriculum,"Keuning, H; Luxton-Reilly, A; Ott, C; Petersen, A; Kiesler, N",10.1145/3699538.3699591,2024,"Generative AI (GenAI) is currently capable of generating correct code for introductory level programming problems, and its performance is improving. We believe that this capability can be leveraged to improve student motivation, broaden students' understanding of software development, and engage them in more authentic learning. We defined a set of assumptions about GenAI's future capabilities (e.g., the ability to generate small pieces of code and to compose these pieces of code via user prompts) and engaged in a backcasting exercise to identify what else is needed to develop a CS1 course that places GenAI in a central role. Undertaking this thought experiment immediately revealed that aspects of the software development process usually reserved for later in the curriculum, such as requirements elicitation and design, could be introduced earlier in the process. With GenAI tools bearing the load of generating correct code snippets, students could focus on higher-level software design and construction skills and practice them in an authentic environment. Our thought experiment identified a set of questions that need to be addressed for such a course to actually exist, including questions about student preparation, and the ability of students to decompose problems effectively and to resolve problems that arise when integrating pieces of code. We also identified questions related to the design of a GenAI centered course, such as the impact on student motivation of using GenAI instead of engaging directly with code, the extent to which social learning theories apply to interactions with GenAI, and how existing pedagogies can integrate GenAI tools."
Commit-Level Software Change Intent Classification Using a Pre-Trained Transformer-Based Code Model,"Hericko, T; Sumak, B; Karakatic, S",10.3390/math12071012,2024,"Software evolution is driven by changes made during software development and maintenance. While source control systems effectively manage these changes at the commit level, the intent behind them are often inadequately documented, making understanding their rationale challenging. Existing commit intent classification approaches, largely reliant on commit messages, only partially capture the underlying intent, predominantly due to the messages' inadequate content and neglect of the semantic nuances in code changes. This paper presents a novel method for extracting semantic features from commits based on modifications in the source code, where each commit is represented by one or more fine-grained conjoint code changes, e.g., file-level or hunk-level changes. To address the unstructured nature of code, the method leverages a pre-trained transformer-based code model, further trained through task-adaptive pre-training and fine-tuning on the downstream task of intent classification. This fine-tuned task-adapted pre-trained code model is then utilized to embed fine-grained conjoint changes in a commit, which are aggregated into a unified commit-level vector representation. The proposed method was evaluated using two BERT-based code models, i.e., CodeBERT and GraphCodeBERT, and various aggregation techniques on data from open-source Java software projects. The results show that the proposed method can be used to effectively extract commit embeddings as features for commit intent classification and outperform current state-of-the-art methods of code commit representation for intent categorization in terms of software maintenance activities undertaken by commits."
Ethical Design of Computers: From Semiconductors to IoT and Artificial Intelligence,"Pasricha, S; Wolf, M",10.1109/MDAT.2023.3277815,2024,This article proposes a new approach to estimate peak memory usage of software that is synthesized from a dataflow-model-based software development framework.
Cost Adjustment for Software Crowdsourcing Tasks Using Ensemble Effort Estimation and Topic Modeling,"Yasmin, A",10.1007/s13369-024-08746-8,2024,"Crowdsourced software development (CSSD) is a fast-growing field among software practitioners and researchers from the last two decades. Despite being a favorable environment, no intelligent mechanism exists to assign price to CSSD tasks. Software development effort estimation (SDEE) on the other hand is already an established field in traditional software engineering. SDEE is largely facilitated by machine learning (ML), particularly, ML-based ensemble effort estimation (EEE) which targets accurate estimate by avoiding biases of single ML model. This accuracy of EEE can be exploited for CSSD platforms to establish intelligent cost assignment mechanism. This study aims to integrate EEE with CSSD platform to provide justified costing solution for crowdsourced tasks. Effort-based cost estimation model is proposed, implementing EEE to predict task's effort along with natural language processing (NLP) analysis on task's textual description to assign effort-based cost. TopCoder is selected as targeted CSSD platform, and the proposed scheme is implemented on TopCoder QA category comprising software testing tasks. Ensemble prediction is incorporated via random forest, support vector machine and neural network as base learners. LDA topic modeling is utilized for NLP analysis on the textual aspects of CSSD task, with a specific emphasis on the testing and technology factors. Effort estimation results confirm that EEE models, particularly stacking and weighted ensemble, surpass their base learners with 50% overall increased accuracy. Moreover, R2, log-likelihood and topic quality measures confirm considerable LDA model significance. Findings confirmed that cost adjustment achieved from EEE and NLP defines acceptable price range, covering major testing aspects."
Bridging the Theory-Practice Gap in a Maintenance Programming Course: An Experience Report,"Ouhbi, S",10.1145/3639474.3640062,2024,"This paper presents our experience in teaching a maintenance programming course with the aim of bridging the gap between theory and practice, a recurring issue in previous course offerings. To achieve this goal, we implemented active learning strategies within an active learning classroom setting and redesigned the project work. Our approach involves peer learning and teamwork activities to cover various aspects of legacy code maintenance. For the project work, we adopted an open-ended approach that allowed students to choose their legacy code projects, which could be open source software or a previous software project they had worked on. Analysis of students' feedback and project reports highlighted the effectiveness of our approach in bridging the gap between theory and practice. We believe that our approach had the potential to enhance students' engagement and critical thinking abilities, as well as improve practical maintenance skills relevant to their future careers."
CAIS-DMA: A Decision-Making Assistant for Collaborative AI Systems,"Rimawi, D; Liotta, A; Todescato, M; Russo, B",10.1007/978-3-031-49266-2_13,2024,"A Collaborative Artificial Intelligence System (CAIS) is a cyber-physical system that learns actions in collaboration with humans in a shared environment to achieve a common goal. In particular, a CAIS is equipped with an AI model to support the decision-making process of this collaboration. When an event degrades the performance of CAIS (i.e., a disruptive event), this decision-making process may be hampered or even stopped. Thus, it is of paramount importance to monitor the learning of the AI model, and eventually support its decision-making process in such circumstances. This paper introduces a new methodology to automatically support the decision-making process in CAIS when the system experiences performance degradation after a disruptive event. To this aim, we develop a framework that consists of three components: one manages or simulates CAIS's environment and disruptive events, the second automates the decision-making process, and the third provides a visual analysis of CAIS behavior. Overall, our framework automatically monitors the decision-making process, intervenes whenever a performance degradation occurs, and recommends the next action. We demonstrate our framework by implementing an example with a real-world collaborative robot, where the framework recommends the next action that balances between minimizing the recovery time (i.e., resilience), and minimizing the energy adverse effects (i.e., greenness)."
Multilayered Fault Detection and Localization With Transformer for Microservice Systems,"Wang, JY; Li, YW; Qi, Q; Lu, Y; Wu, B",10.1109/TR.2024.3356717,2024,"Software architecture is undergoing a transition from monolithic architecture to microservices to achieve resilience, agility, and scalability in the software life cycle. The complex dependability of these microservices may lead to unexpected failures, which becomes a major concern on reliability for application providers. The existing fault detection and localization algorithms for microservice systems only focus on the relationship within microservices and cannot achieve finer granularity from a layered system perspective, including microservices, containers, physical machines, and networks. To tackle this problem, we propose a multilayered method that deconstructs cloud-based microservices and connects the information from various layers to enhance the precision of fault detection and localization. The proposed Transformer encoder model can detect anomalies of containers in the resource layer, and by decomposing and analyzing invocation latency, anomalies in the service layer can be detected. After determining the faulty area of the resource layer based on the above anomalies, a multifactor root cause score is used to sort root cause metrics in the faulty area for localization. Evaluations were performed on three datasets: the Sock-Shop dataset we collected from an actual microservice system, the AIOps2020 preliminary dataset, and the SMD. Empirical investigations conducted on these datasets show that our models enhance the F1 score by approximately 0.25 for anomaly detection and improve the mean average precision by up to 0.54 for root cause localization, which underscores the utility of our models in effectively managing microservice systems in practical scenarios."
"LLMs for Code: The Potential, Prospects, and Problems","Sharma, T",10.1109/ICSA-C63560.2024.00067,2024,"With the introduction of Large Language Models (LLMs) and their integration with software development tasks, the software development landscape has changed drastically in the last couple of years. In this session, we delve into the intricate world of large language models for code ( LLMs4Code) and explore their benefits, challenges, and threats. On one hand, these models have revolutionized code completion, bug detection, and even generated entire sections of code with remarkable accuracy. However, on the other side, several concerns have emerged surrounding inaccurate, buggy, and vulnerable code generation, biases, implications for climate, and the potential for unintended consequences. Together, we'll dissect real-world examples, discussing the transformative power of large language models while exploring the gray side of LLMs4Code that developers tread. The talk will discuss strategies for effectively leveraging these tools, mitigating risks, and contributing to the ongoing dialogue about responsible AI in the coding ecosystem. The talk promises an exploratory take that not only seeks to harness the potential of LLMs4Code but also ensures a conscientious and mindful approach toward their integration into our coding practices."
Tactics to mitigate the negative impact of introducing advanced technology on employees: Evidence from large listed companies in Japan,"Kato, T; Koizumi, M",10.1016/j.chbr.2024.100423,2024,"This study examines how the introduction of advanced technology such as artificial intelligence adversely affects employees and the tactics to mitigate these negative effects. We surveyed 5438 employees working for large companies listed on the Tokyo Stock Exchange in nine industries in Japan. The questionnaire considered the five drivers of employee attitudes (skill acquisition, fair wages, work/life balance, relationship with supervisor, and diversity of human resources) as well as the introduction of advanced technology in workplaces. Using structural equation modeling, we revealed that the introduction of advanced technology increases turnover intention. While the effect is not observed for sales and marketing, it is evident among back-office and software development personnel. Further, this effect is more pronounced among younger workers and those who change jobs more frequently. Managers should consider not only the positive effect of cutting-edge technology, but also its adverse impact on employees."
Effective Vulnerable Function Identification based on CVE Description Empowered by Large Language Models,"Wu, YL; Wen, M; Yu, ZL; Guo, XC; Jin, H",10.1145/3691620.3695013,2024,"Open-source software (OSS) has profoundly transformed the software development paradigm by facilitating effortless code reuse. However, in recent years, there has been an alarming increase in disclosed vulnerabilities within OSS, posing significant security risks to downstream users. Therefore, analyzing existing vulnerabilities and precisely assessing their threats to downstream applications become pivotal. Plenty of efforts have been made recently towards this problem, such as vulnerability reachability analysis and vulnerability reproduction. The key to these tasks is identifying the vulnerable function (i.e., the function where the root cause of a vulnerability resides). However, public vulnerability datasets (e.g., NVD) rarely include this information as pinpointing the exact vulnerable functions remains to be a longstanding challenge. Existing methods mainly detect vulnerable functions based on vulnerability patches or Proof-of-Concept (PoC). However, such methods face significant limitations due to data availability and the requirement for extensive manual efforts, thus hindering scalability. To address this issue, we propose a novel approach VFFinder that localizes vulnerable functions based on Common Vulnerabilities and Exposures (CVE) descriptions and the corresponding source code utilizing Large Language Models (LLMs). Specifically, VFFinder adopts a customized in-context learning (ICL) approach based on CVE description patterns to enable LLM to extract key entities. It then performs priority matching with the source code to localize vulnerable functions. We assess the performance of VFFinder on 75 large open-source projects. The results demonstrate that VFFinder surpasses existing baselines significantly. Notably, the Top-1 and MRR metrics have been improved substantially, averaging 4.25X and 2.37X respectively. We also integrate VFFinder with Software Composition Analysis (SCA) tools, and the results show that our tool can reduce the false positive rates of existing SCA tools significantly."
DeVAIC: : A tool for security assessment of AI-generated code,"Cotroneo, D; De Luca, R; Liguori, P",10.1016/j.infsof.2024.107572,2025,"Context: AI code generators are revolutionizing code writing and software development, but their training on large datasets, including potentially untrusted source code, raises security concerns. Furthermore, these generators can produce incomplete code snippets that are challenging to evaluate using current solutions. Objective: This research work introduces DeVAIC (Detection of Vulnerabilities in AI-generated Code), a tool to evaluate the security of AI-generated Python code, which overcomes the challenge of examining incomplete code. Methods: We followed a methodological approach that involved gathering vulnerable samples, extracting implementation patterns, and creating regular expressions to develop the proposed tool. The implementation of DeVAIC includes a set of detection rules based on regular expressions that cover 35 Common Weakness Enumerations (CWEs) falling under the OWASP Top 10 vulnerability categories. Results: We utilized four popular AI models to generate Python code, which we then used as a foundation to evaluate the effectiveness of our tool. DeVAIC demonstrated a statistically significant difference in its ability to detect security vulnerabilities compared to the state-of-the-art solutions, showing an F 1 Score and Accuracy of 94% while maintaining a low computational cost of 0.14 s per code snippet, on average. Conclusions: The proposed tool provides a lightweight and efficient solution for vulnerability detection even on incomplete code."
FINANCIAL ADMINISTRATION INFORMATION SYSTEMS (FMIS) IN SMART PUBLIC GOVERNANCE: AN EXPLORATION OF THE COLOMBIAN CASE,"DÃ­az, MRO; Osorio, SPV; Ospina, KJZ",10.18601/16578651.n34.03,2024,"Financial Management Information Systems (FMIS) are essential for efficient and transparent management of financial resources in the public sector. Due to the scarcity of publications on this topic in Colombia, this article aims to provide a contribution to academia and professionals who work in public finance by examining the importance of systems such as SIIF, CHIP, SIRECI, ePICO, SIA Observa, the DNP Investment Map, and the new PIIP platform, among others. Topics such as Smart Public Management (SPM), digital government, the fight against corruption, interoperability, cybersecurity, artificial intelligence (AI) and public marketing are also addressed. The research used an interpretative perspective, based on a theoretical, documentary, descriptive and purposeful type. It also used a qualitative and quantitative approach, which included the application of a survey as an instrument of data collection. At the end of the article, the challenge of improving the interoperability and security of FMIS in the Colombian State is raised, highlighting the importance of training officials in information and communication technologies (ICT), as well as promoting websites and platforms related to state public finances to strengthen transparency and encourage citizen participation."
Making Federated Learning Accessible to Scientists: The AI4EOSC Approach,"DÃ­az, JSP; Canales, AH; Cacha, IH; Tran, V; Nguyen, G; Alibabaei, K; Ruiz, MO; Ruiz, SR; GarcÃ­a, AL",10.1145/3658664.3659642,2024,"Access to computing resources is a critical requirement for researchers in a wide diversity of areas. This has become even more important with the rise of artificial intelligence techniques through the training of machine learning and deep learning models. In this sense, the AI4EOSC project aims to respond to this need by delivering an enhanced set of advanced services and tools for the development of artificial intelligence, machine and deep models, such as federated learning, in the European Open Science Cloud (EOSC). Federated learning is a technology in the field of privacy-preserving machine learning techniques that has revolutionized the current state of the art, evolving from classical centralized approaches to allow training models in a decentralized way, without sharing raw data. In this work, we present the production implementation of a federated learning system based on the Flower framework that allows users, without a technological background, to exploit this technique, performing federated learning training within the AI4EOSC platform. The objective is to be able to train this type of architecture in an intuitive way; for this purpose, a userfriendly dashboard has been implemented, whose development will be reviewed. The frameworks and technologies used for this implementation will be exposed together with an example of use from scratch, in order to demonstrate the use of this functionality of the platform. Finally, two scenarios concerning client availability are analyzed."
RMCBENCH: Benchmarking Large Language Models' Resistance to Malicious Code,"Chen, JC; Zhong, QY; Wang, YL; Ning, KW; Liu, YK; Xu, ZN; Zhao, Z; Chen, T; Zheng, ZB",10.1145/3691620.3695480,2024,"The emergence of Large Language Models (LLMs) has significantly influenced various aspects of software development activities. Despite their benefits, LLMs also pose notable risks, including the potential to generate harmful content and being abused by malicious developers to create malicious code. Several previous studies have focused on the ability of LLMs to resist the generation of harmful content that violates human ethical standards, such as biased or offensive content. However, there is no research evaluating the ability of LLMs to resist malicious code generation. To fill this gap, we propose RMCBench, the first benchmark comprising 473 prompts designed to assess the ability of LLMs to resist malicious code generation. This benchmark employs two scenarios: a text-to-code scenario, where LLMs are prompted with descriptions to generate code, and a code-to-code scenario, where LLMs translate or complete existing malicious code. Based on RMCBench, we conduct an empirical study on the 11 representative LLMs to assess their ability to resist malicious code generation. Our findings indicate that current LLMs have a limited ability to resist malicious code generation with an average refusal rate of 40.36% in text-to-code scenario and 11.52% in code-to-code scenario. The average refusal rate of all LLMs in RMCBench is only 28.71%; ChatGPT-4 has a refusal rate of only 35.73%. We also analyze the factors that affect LLM's ability to resist malicious code generation and provide implications for developers to enhance model robustness."
A Practical Failure Prediction Model based on Code Smells and Software Development Metrics,"SchÃ¼tz, M; PlÃ¶sch, R",10.1145/3651640.3651644,2024,"Making errors during software development is unavoidable. Developers inevitably make errors that take additional time to fix later. Consequently, efforts for bug fixing compete with implementing new features. Typically, the later bugs are found, the higher the cost for remediation. To address this concern, software testing should start as early as possible in software development lifecycle. For this purpose, static analysis is proposed, but typically shows too many findings and hence do not support development teams appropriately. So, it would be a benefit to premature detect those findings in static analysis that will result in failures to reduce subsequent efforts notably. The purpose of the paper is to analyze failure data from issue tracking systems that are correlated to findings from static analysis. Thereupon an artificial intelligence-based approach is used to train practicable models for business environment that enables effective prediction of software faults. The results from static analysis show that predefined complexity measures encompassed the most defects. While there are commonalities in relevant defect findings in static analysis reports, meaningful prediction models cannot be expected based solely on this data. In addition to the findings of the static analysis, metrics like code changes in a time period or number of authors involved in code changes were considered for building the prediction models. Two of the developed prediction models have a high accuracy and excellent utility rate. These resulting prediction models are currently used at Raiffeisen Software GmbH for a long-term study on failure prediction based on code smells."
Problematic Workplace Behaviours in the Software Development Profession: Using Transactional Analysis to Diagnose Toxicity and Improve Relationships at Work,"Tassabehji, R; Lee, HG; Harding, N",10.1177/10596011241276586,2024,"The growing and unmet demand for coding skills is becoming critical in a world that is ever-more driven by digital technologies, embedded algorithms and artificial intelligence systems. However, sustainability of the profession is threatened because of the failure to attract and retain women developers, which has been an ongoing and corrosive problem for decades and remains unresolved. While many previous studies attribute 'toxic' workplace cultures in the software development industry, as a major contributing factor, few examine their root causes and almost none offer practical solutions. To address this lack of both knowledge and effective response, we propose a novel approach building on psychoanalytical Transactional Analysis (TA) theory that is little used in the field of management and organisation studies. TA theory provides a framework using common and simplified language to better understand why communications in the workplace fail, and how occupying incompatible ego states might lead to 'negative', 'problematic' and, in the worst cases, 'toxic' behaviours and workplaces. We propose a TA-based model (OCTAPos) that helps explain how crossed communications at work can result in a lack of acceptance of women in the workplace and attrition among female software developers and the resultant dearth of diverse coders. We further propose a theoretically informed HRM Structured TA Response (STAR) to help increase self-awareness, emotional intelligence, empathy and mutual understanding, with the ultimate aim of positively impacting prevailing attitudes, behaviours and organisational culture to achieve more inclusive and sustainable recruitment and retention in the longer term."
DHG-BiGRU: Dual-attention based hierarchical gated BiGRU for software defect prediction,"Malhotra, R; Singh, P",10.1016/j.infsof.2024.107646,2025,"Context: Software defect prediction (SDP) is a prominent research area focussed on anticipating defects early in the software lifecycle. Traditional machine learning models are based on static features, which are not enough to capture contextual information in the source code. In recent years, researchers have also developed deep learning models that extract semantic information from source code using the abstract syntax tree (AST). These approaches often combine static and semantic features by a simple merger operation. Objective: The article aims to address the limitations of the existing models by utilizing advanced feature extraction and integration techniques. It develops a deep learning model that can effectively prioritize the crucial features and intelligently combine the static and semantic features to provide robust predictions Method: The article proposes a novel model namely, dual-attention-based hierarchical gated BiGRU (DHGBiGRU). The model first employs a static feature extractor (StatFE) and a semantic feature extractor (SemFE) to capture static and semantic features, respectively. Next, the outputs from StatFE and SemFE are passed to individual BiGRUs. The BiGRU output associated with the semantic features is subsequently processed by a dual attention mechanism (DAM), that captures the complex semantic information with emphasis on the most crucial features. Afterward, the hierarchical gated fusion (HGF) meticulously merges the static and semantic features. Finally, these integrated features are passed through a sigmoid function to predict defects. Results: The extensive experiments on extensively utilized datasets from the PROMISE repository reveal that DHG-BiGRU performs significantly better than the most advanced models and consistently achieves higher precision, recall and f-measure, demonstrating a reliable prediction capability. Conclusion: The results of the study underscore the potential advanced feature extraction and integration techniques for SDP. By achieving considerable improvements over state-of-the-art techniques, the proposed approach paves the way for sophisticated defect prediction models to improve software quality and reliability."
Requirements Engineering for Trustworthy Human-AI Synergy in Software Engineering 2.0,"Lo, D",10.1109/RE59067.2024.00011,2024,"Software Engineering 2.0 envisions trustworthy and synergistic collaborations between humans and AI agents that are diverse, responsible, and autonomous, aiming to build the software of tomorrow - a vision that has garnered significant attention recently. Despite this growing interest, we are only beginning to unravel the complexities of fostering this synergy to develop trusted software that benefits society. This keynote will (i) highlight existing efforts to engineer the requirements for this new paradigm of software development, and (ii) chart the road ahead where Requirements Engineering can play a crucial role in defining the sine qua nons - the indispensable elements that ensure Software Engineering 2.0 can meet the diverse needs of various stakeholders."
Systematic Literature Review of Project Management Maturity Models,"Ruiz-Lopez, F; Ortiz-Hernandez, J; Bonjour, E; Micaelli, JP; HernÃ¡ndez, Y",10.1134/S0361768824700750,2024,"Project management is a field that has been applied in various areas of knowledge, particularly in engineering and software development. For organizations, projects are a central element for generating value. They allow to reach the organizational goal by using specific methodologies, tools and software. One of the most recognized tools, even in other fields of knowledge, for its impact on process improvement is maturity models. These models have already begun to be implemented in project management. Project management maturity models are useful tools to evaluate the management process using a process reference (e.g., PMBOK). This process reference describes the best practices to achieve success in projects. The purpose of this paper is the identification of research papers that present maturity models specifically for project management. A useful classification for project managers using maturity models in a project management context is generated from the results of the review. The proposed classification considers 4 categories: (1) project management, (2) organizational performance of project management, (3) project management for specific areas, and (4) project management process in organizations. Each category classifies the project management maturity models according to their characteristics."
Comparing emotions in ChatGPT answers and human answers to the coding questions on Stack Overflow,"Fatahi, S; Vassileva, J; Roy, CK",10.3389/frai.2024.1393903,2024,"Introduction Recent advances in generative Artificial Intelligence (AI) and Natural Language Processing (NLP) have led to the development of Large Language Models (LLMs) and AI-powered chatbots like ChatGPT, which have numerous practical applications. Notably, these models assist programmers with coding queries, debugging, solution suggestions, and providing guidance on software development tasks. Despite known issues with the accuracy of ChatGPT's responses, its comprehensive and articulate language continues to attract frequent use. This indicates potential for ChatGPT to support educators and serve as a virtual tutor for students.Methods To explore this potential, we conducted a comprehensive analysis comparing the emotional content in responses from ChatGPT and human answers to 2000 questions sourced from Stack Overflow (SO). The emotional aspects of the answers were examined to understand how the emotional tone of AI responses compares to that of human responses.Results Our analysis revealed that ChatGPT's answers are generally more positive compared to human responses. In contrast, human answers often exhibit emotions such as anger and disgust. Significant differences were observed in emotional expressions between ChatGPT and human responses, particularly in the emotions of anger, disgust, and joy. Human responses displayed a broader emotional spectrum compared to ChatGPT, suggesting greater emotional variability among humans.Discussion The findings highlight a distinct emotional divergence between ChatGPT and human responses, with ChatGPT exhibiting a more uniformly positive tone and humans displaying a wider range of emotions. This variance underscores the need for further research into the role of emotional content in AI and human interactions, particularly in educational contexts where emotional nuances can impact learning and communication."
Generative AI in the Software Development Lifecycle,"Bannon, T; Laplante, P",10.1109/MC.2024.3474789,2024,"We conducted a virtual roundtable on generative AI (GenAI), human/machine teaming, and the future of the software development lifecycle (SDLC). Experts from industry, academia, and government explored how GenAI is transforming the SDLC across mission and business domains."
Cautious Optimism: The Influence of Generative AI Tools in Software Development Projects,"Mbizo, T; Oosterwyk, G; Tsibolane, P; Kautondokwa, P",10.1007/978-3-031-64881-6_21,2024,"Generative artificial intelligence has emerged as a disruptive technology with the potential to transform traditional software development practices and methodologies. This study examines the implications of integrating AI tools in software development projects, focusing on potential benefits, challenges, and perceptions of the broader software development community. The study employs a qualitative methodology that captures the sentiments and personal adaptive measures from a diverse group of industry professionals who integrate generative AI tools such as ChatGPT and GitHub's Copilot in their software development projects. Findings suggest that generative AI tools aid developers in automating repetitive tasks, improve their workflow efficiency, reduce the coding learning curve, and complement traditional coding practices and project management techniques. However, generative AI tools also present ethical limitations, including privacy and security issues. The study also raises concerns regarding the long-term potential for job elimination (insecurity), over-reliance on generative AI assistance by developers, generativeAI lack of contextual understanding, and technical skills erosion. While developers are optimistic about the positive benefits of generative AI use within project environments in the short term, they also hold a pessimistic view in the longer term. There is a need for the software development projects community to critically assess the use of generative AI in software development projects while exploring how to retain the critical aspect of human oversight and judgment in the software development process in the long term."
Predicting Software Functional Size Using Natural Language Processing: An Exploratory Case Study,"ÃnlÃ¼, H; Tenekeci, S; ÃiftÃ§i, C; Oral, IB; Atalay, T; Hacaloglu, T; Musaoglu, B; DemirÃ¶rs, O",10.1109/SEAA64295.2024.00036,2024,"Software Size Measurement (SSM) plays an essential role in software project management as it enables the acquisition of software size, which is the primary input for development effort and schedule estimation. However, many small and medium-sized companies cannot perform objective SSM and Software Effort Estimation (SEE) due to the lack of resources and an expert workforce. This results in inadequate estimates and projects exceeding the planned time and budget. Therefore, organizations need to perform objective SSM and SEE using minimal resources without an expert workforce. In this research, we conducted an exploratory case study to predict the functional size of software project requirements using state-of-the-art large language models (LLMs). For this aim, we fine-tuned BERT and BERT_SE with a set of user stories and their respective functional size in COSMIC Function Points (CFP). We gathered the user stories included in different project requirement documents. In total size prediction, we achieved 72.8% accuracy with BERT and 74.4% accuracy with BERT_SE. In data movement-based size prediction, we achieved 87.5% average accuracy with BERT and 88.1% average accuracy with BERT_SE. Although we use relatively small datasets in model training, these results are promising and hold significant value as they demonstrate the practical utility of language models in SSM."
Providing a Natural Language Processing App for Language Teachers,"Posekany, A; Dolezal, D",10.1007/978-3-031-51979-6_48,2024,"Natural Language Processing (NLP) is a common application for Artificial Intelligence. The goal is to provide language teachers with a simple to apply tool for topic model analyses to integrate into their classroom. The project also involves project based learning for students programming the actual AI web application. The original notion is to provide language teacher with AI methodology without requiring any technical knowledge in AI or any programming skills. Natural Language Processing provides various tools for word frequencies, but also topic modelling, allowing to track the relevance of topics over time in the media or in the literature. In collaboration with University of Technology linguistics, we intend to provide a corpus of classical English and German literature, as well as the option of uploading your own corpus which can be obtained from webscraping or other sources. A team of students of the vocational high school TGM Wien specialised in IT and Software Development is working on the design of the interactive GUI for this NLP application, learning in this way the methods of Natural Language Processing and Artrificial Intelligence in a project based setting. For this the statistical programming language R is utilized which already provides packages with implementation for Natural Language Processing and in addition the shiny package which allows to develop interactie web apps without additional web and app programming. A team of teachers supervises and supports the students during the development process, providing expertise in AI and NLP, in web and app programming, as well as server management. Two intended outcomes exist. Ont the one hand, we want our students to learn Natural Language Processing first hand through development of this application. On the other hand, we intend to obtain an interactive AI tool which can assist language teachers and their students on the long term in the classroom. In times of GPT3 and GPT4 dominating the media and perception of Artificial Intelligence, we intend to teach students about the methodology involved first hand by having them participate in the development of such an interactive application for Natural Language Processing. Through a project based learning approach, we involve them in all steps of the development over a times of one and a half years, two school years. The final product of this project is an interactive web application for topic modelling and visualising word frequency counts in various ways."
Enhancing commit message quality in software capstone projects with generative AI,"Neyem, A; Rios-Letelier, A; CÃ©spedes-Arancibia, K; Alcocer, JPS; Mendoza, M",10.1016/j.softx.2024.101947,2024,"Software Capstone Projects provide valuable hands-on experience for students in software development, and creating effective commit messages is an essential, though often challenging, part of this process. These messages playa key role in managing repositories, facilitating collaboration, and offering insights into the project's progression for mentors and managers. However, creating high-quality commit messages can be challenging, especially for novice developers. We introduce LetsCommit, a tool designed to improve the traditional Git commit command line interface. The tool utilizes three state-of-the-art Large Language Models (LLMs): GPT-3.5, GPT-4, and LLaMa-2, to provide commit message suggestions to students. Results from a user experience survey showed high satisfaction, indicating strong potential for incorporating LetsCommit into future projects. Beyond its technical applications, LetsCommit possesses transformative potential in the field of education. The iterative learning process it supports, coupled with real-time insights, reinforces good software development practices and enhances the overall learning experience. These findings highlight LetsCommit's substantial impact on software engineering education, setting the stage for further advancements."
Software development in the age of LLMs and XR,"Gonzalez-Barahona, JM",10.1145/3643796.3648457,2024,"Let's imagine that in a few years generative AI has changed software development dramatically, taking charge of most of the programming tasks. Let's also assume that extended reality devices became ubiquitous, being the preferred interface for interacting with computers. This paper proposes how this situation would impact IDEs, by exploring how the development process would be affected, and analyzing which tools would be needed for supporting developers."
Using ChatGPT in Software Development Education,"Anderson, N; McGowan, A; Hanna, P; Cutting, D; Galway, L; Collins, M",10.1109/EDUCON60312.2024.10578808,2024,"Generative Artificial Intelligence (AI) and Large Language Models (LLMs) such as ChatGPT are revolutionizing the landscape of learning and teaching. They excel in understanding and creating natural language texts, thereby captivating students with their quick and well-crafted responses. While some perceive AI simply as a tool to reduce workload, our study appreciates these technologies for their ability to beautifully augment human capabilities. In this study, we tasked ChatGPT with designing a relational database for an online food delivery system, similar to an early university computer science assignment. This paper explains the attention mechanism, which is a crucial component in LLMs, enabling them to focus on specific parts of the presented input (prompt) and enhances their ability to 'understand' context. Through a series of iterative prompt refinements, we evaluate ChatGPT's effectiveness in developing this database, with a goal to enhance the accuracy and relevance of its responses. Our findings reveal both the benefits and limitations of using LLMs in education, highlighting their potential to significantly enrich the learning experience."
Automating Test Case Generation from Class Diagram Using Generative AI,"Naimi, L; Bouziane, E; Jakimi, A",10.1007/978-3-031-66850-0_15,2024,"In the realm of software engineering, the automation of test case generation represents a significant advancement towards improving efficiency and reliability. This paper introduces a novel approach to automate the generation of test cases from class diagrams using generative artificial intelligence (AI). By extracting class and attribute information from the XML representation of class diagrams, we can formulate structured prompts that are then fed into a generative AI model. The model is designed to interpret these prompts and produce comprehensive test cases corresponding to each class. Our methodology not only streamlines the test case creation process but also leverages the advanced capabilities of AI to ensure thorough coverage and accuracy. The implications of this approach extend to enhancing the quality assurance phase of software development, thereby contributing to the development of robust and error-resistant software systems."
Predicting the Duration of User Stories in Agile Project Management,"Raza, A; Espinosa-Leal, L",10.1007/978-3-031-61905-2_31,2024,"Effective effort estimation in agile project planning is vital because it helps organizations build product plans that they can stick to, have shorter turn-around time, and have better cost discipline. Machine learning can play an essential role in planning and estimating the project schedule. In this paper, a series of supervised machine learning models were studied, analyzed, and implemented to solve the problem of predicting effort estimates in Agile Scrum. The obtained results are compared with similar previous studies. We performed experiments using different Natural Language Processing (NLP) methods such as Term Frequency-inverse document frequency (TF-IDF), fastText, and different Neural Networks, including Recurrent Neural Networks (RNN), Long Short Term Memory (LSTM), and Bidirectional Encoder Representations from Transformers (BERT). The trained models were fitted with three publicly available datasets. Our findings show that fastText (with a pre-trained model) significantly performed better in predicting the story-points of user-stories. The second-best performing model was bidirectional LSTM. Moreover, distilBERT performs poorly among all the models analyzed. This study can pave the way for organizations to benefit from these machine learning models and accurately predict project deadlines and schedules."
"Developer Experiences with a Contextualized AI Coding Assistant: Usability, Expectations, and Outcomes","Pinto, G; de Souza, C; Rocha, T; Steinmacher, I; de Souza, A; Monteiro, E",10.1145/3644815.3644949,2024,"In the rapidly advancing field of artificial intelligence, software development has emerged as a key area of innovation. Despite the plethora of general-purpose AI assistants available, their effectiveness diminishes in complex, domain-specific scenarios. Noting this limitation, both the academic community and industry players are relying on contextualized coding AI assistants. These assistants surpass general-purpose AI tools by integrating proprietary, domain-specific knowledge, offering precise and relevant solutions. Our study focuses on the initial experiences of 62 participants who used a contextualized coding AI assistant - named StackSpot AI- in a controlled setting. According to the participants, the assistants' use resulted in significant time savings, easier access to documentation, and the generation of accurate codes for internal APIs. However, challenges associated with the knowledge sources necessary to make the coding assistant access more contextual information as well as variable responses and limitations in handling complex codes were observed. The study's findings, detailing both the benefits and challenges of contextualized AI assistants, underscore their potential to revolutionize software development practices, while also highlighting areas for further refinement."
Requirements Verification Through the Analysis of Source Code by Large Language Models,"Couder, JO; Gomez, D; Ochoa, O",10.1109/SOUTHEASTCON52093.2024.10500073,2024,"In the most recent years, Large Language Models (LLMs) have gained popularity and have been accepted and used in different domains due to their ability to understand and generate written language. LLMs allow us to analyze large amounts of data in a few moments, yet they are also extremely simple to use, making them a very powerful assistive tool that can aid in a wide range of tasks; from planning a family trip, to aid during the development process of a huge system. For software developers, LLMs have been mostly used for code generation, explanation, or optimization. Software verification is a crucial part of software development as it is the process of ensuring that a system meets specific requirements. Requirements specifications play a pivotal role in software verification as they define what a system should do. In this paper we propose the use of LLMs for code verification through the analysis of requirements specifications. We prove that LLMs, such as GPT-3.5, can verify a list of requirements through a given code and evaluate why the requirements have or have not been met."
Automating Fault Test Cases Generation and Execution for Automotive Safety Validation via NLP and HIL Simulation,"Amyan, A; Abboush, M; Knieke, C; Rausch, A",10.3390/s24103145,2024,"The complexity and the criticality of automotive electronic implanted systems are steadily advancing and that is especially the case for automotive software development. ISO 26262 describes requirements for the development process to confirm the safety of such complex systems. Among these requirements, fault injection is a reliable technique to assess the effectiveness of safety mechanisms and verify the correct implementation of the safety requirements. However, the method of injecting the fault in the system under test in many cases is still manual and depends on an expert, requiring a high level of knowledge of the system. In complex systems, it consumes time, is difficult to execute, and takes effort, because the testers limit the fault injection experiments and inject the minimum number of possible test cases. Fault injection enables testers to identify and address potential issues with a system under test before they become actual problems. In the automotive industry, failures can have serious hazards. In these systems, it is essential to ensure that the system can operate safely even in the presence of faults. We propose an approach using natural language processing (NLP) technologies to automatically derive the fault test cases from the functional safety requirements (FSRs) and execute them automatically by hardware-in-the-loop (HIL) in real time according to the black-box concept and the ISO 26262 standard. The approach demonstrates effectiveness in automatically identifying fault injection locations and conditions, simplifying the testing process, and providing a scalable solution for various safety-critical systems."
Limitations and Benefits of the ChatGPT for Python Programmers and Its Tools for Evaluation,"Arias, R; Martinez, G; CÃ¡ceres, D; Garces, E",10.1007/978-3-031-70300-3_12,2024,"The artificial intelligence called ChatGPT exhibits an outstanding feature, which is its ability to automatically generate code; in this case, the scope is with programming in Python. This paper focuses on evaluating the quality of the code generated and its limitations by ChatGPT compared to the code created by an advanced human programmer. To achieve this purpose, a systematic research is carried out with an exhaustive analysis of the artificial intelligence with a new tool and survey for the evaluation, in order to clarify the capabilities and constraints with its context in the field of artificial intelligence. Through a rigorous systematic review process, following PRISMA guidelines, a total of 6879 relevant publications are initially identified. After applying inclusion and exclusion criteria, the sample was reduced to 165 publications, and after eliminating irrelevant publications, a set of 15 quality articles that fit the study objectives was finally selected. The results of the articles selected in the research reveal an in-depth evaluation of ChatGPT, the understanding of the integration of Artificial Intelligence in education, the analysis of the motivations behind the use of generative chatbots, the challenges and paradigms that arise from the use of AIs in programming, the perception of AI with human attributes, the evaluation of metrics for software quality, the detection of irregularities in the code, the categorization of clones and the exploration of software quality and errors in the code. The PICOC methodology included the use of software related to quality testing, which allows achieving an astonishing 95.66% accuracy in the evaluation of the code generated, both by the programmer and by ChatGPT. This resulted in a 30.28% decrease in human errors and 96.2% effectiveness in evaluating the quality of the generated code, the evaluation considered 3 surveys for Python programmers in 1000 of advanced programmers from October to December 2023. In summary, this paper provides a complete view of the success factors in the comparison of both codes, which in turn leads to greater efficiency in software development, significantly reducing the time required by programmers with its limitation with ChatGPT."
Enhancing Security Assurance in Software Development: AI-Based Vulnerable Code Detection with Static Analysis,"Rajapaksha, S; Senanayake, J; Kalutarage, H; Al-Kadri, MO",10.1007/978-3-031-54129-2_20,2024,"The presence of vulnerable source code in software applications is causing significant reliability and security issues, which can be mitigated by integrating and assuring software security principles during the early stages of the development lifecycle. One promising approach to identifying vulnerabilities in source code is the use of Artificial Intelligence (AI). This research proposes an AI-based method for detecting source code vulnerabilities and leverages Explainable AI to help developers identify and understand vulnerable source code tokens. To train the model, a web crawler was used to collect a real-world dataset of 600,000 source code samples, which were annotated using static analysers. Several ML classifiers were tested on a feature vector generated using Natural Language Processing techniques. The Random Forest and Extreme Gradient Boosting classifiers were found to perform well in binary and multi-class approaches, respectively. The proposed model achieved a 0.96 F1-Score in binary classification and a 0.85 F1-Score in multi-class classification based on Common Weakness Enumeration (CWE) IDs. The model, trained on a dataset of actual source codes, is highly generalisable and has been integrated into a live web portal to validate its performance on real-world code vulnerabilities."
VSEST 29110 Tool: Using ChatGPT to Evaluate the Implementation of the ISO/IEC 29110 Work Products,"MejÃ­a, J; TerrÃ³n-Macias, V; MuÃ±oz, M; TerrÃ³n-HernÃ¡ndez, M; Canseco-PÃ©rez, M",10.1109/ACCESS.2024.3449252,2024,"The global software industry is predominantly composed of micro, small, and medium-sized enterprises (MSMEs), highlighting the need for software quality management to ensure the proper functioning and quality of the software. This research focuses on the evaluation of the implementation of the ISO/IEC 29110 standard work products, which is a standard tailored by the ISO/IEC specifically for MSMEs, which improves the software development process by implementing two processes in its basic profile: Project Management (PM) and Software Implementation (SI). Despite this standard being tailored specifically for this type of enterprise, implementing ISO/IEC 29110 faces several challenges, such as a lack of knowledge and difficulties in adequately implementing the work products regarding the compliance of standard criteria, among others. To address these challenges, we introduce VSEST 29110, a web tool designed to evaluate the ISO/IEC 29110 standard implementation work products by leveraging Artificial Intelligence (AI) technologies, specifically the ChatGPT model, provide detailed feedback on compliance with standard criteria, offer suggestions for improvement based on ChatGPT analysis and streamline the implementation process for MSMEs. To achieve this, our research incorporates a systematic literature review and validation through a case study by document analysis, demonstrating VSEST 29110's effectiveness in enhancing compliance and providing comprehensive feedback compared to auditor recommendations, which impacts 69.33% on average."
Ethical Considerations Toward Protestware,"Cheong, MR; Kula, RG; Treude, C",10.1109/MS.2023.3344778,2024,"This article looks into possible scenarios where developers might consider turning their free and open source software into protestware. Using different frameworks commonly used in artificial intelligence (AI) ethics, we extend the applications of AI ethics to the study of protestware."
A search-and-fill strategy to code generation for complex software requirements,"Dong, YK; Kong, LJ; Zhang, LL; Wang, SQ; Liu, XS; Liu, S; Chen, MC",10.1016/j.infsof.2024.107584,2025,"Context: The realm of software development has seen significant transformations with the rise of Low-Code Development (LCD) and the integration of Artificial Intelligence (AI), particularly large language models, into coding practices. The proliferation of open-source software also offers vast resources for developers. Objective: We aim to combine the benefits of modifying retrieved code with the use of an extensive code repository to tackle the challenges of complex control structures and multifunctional requirements in software development. Method: Our study introduces a Search-and-Fill strategy that utilizes natural language processing (NLP) to dissect complex software requirements. It extracts control structures and identifies atomic function points. By leveraging large-scale pre-trained models, the strategy searches for these elements to fill in the automatically transformed program structures derived from descriptions of control structures. This process generates a code snippet that includes program control structures and the implementations of various function points, thereby facilitating both code reuse and efficient development. Results: We have validated the effectiveness of our strategy in generating code snippets. For natural language requirements involving multifunctional complex structures, we constructed two datasets: the Basic Complex Requirements Dataset (BCRD) and the Advanced Complex Requirements Dataset (ACRD). These datasets are based on natural language descriptions and Python code that were randomly extracted and combined. For the code snippets to be generated, we achieved the best results with the ACRD dataset, with BLEU-4 scores reaching up to 0.6326 and TEDS scores peaking at 0.7807. Conclusion: The Search-and-Fill strategy successfully generates a comprehensive code snippets, integrating essential control structures and functions to streamline the development process. Experimental results substantiate our strategy's efficacy in optimizing code reuse by effectively integrating preprocessing and selection optimization approach. Future research will focus on enhancing the recognition of complex software requirements and further refining the code snippets."
Determinants of ChatGPT Adoption in Academe & Other Fields - A Review on Theoretical Perspective,"Elinzano, GBO; Ching, MR",,2024,"ChatGPT has been showing promising advantages including its capability to optimize work and converse like human being. In the academe, ChatGPT was seen to have the capability to answer formative assessments, aid in research, and act as virtual tutor. However, ChatGPT is also being criticized for its misleading and inaccurate responses. This led the scientific community to further study its adoption factors. This review discussed and analyzed 53 empirical studies that aimed to determine the factors influencing ChatGPT adoption and use in the academe and other fields. Performance expectancy, personal innovativeness, trust, attitude, and self-efficacy were identified as common determinants of ChatGPT adoption in various fields. To add, experience and presence of Generative AI policy also determine ChatGPT adoption. Technology Acceptance Model (TAM) and Unified Theory of Acceptance and Use of Technology (UTAUT/UTUAT2) are the most widely used frameworks found in this review. Practically, this review recommends that ChatGPT adoption and use be further studied in educational sector focusing on the contrasting results of significant factors found. Policy on how academic institutions will adopt and use ChatGPT is also highly recommended. With respect to other areas, studies on ChatGPT adoption and use in other economic institutions (healthcare, business, law, software development, dentistry, etc.) are recommended. Theoretically, this review recommends use of TAM and UTUAT/UTUAT2 in future studies of ChatGPT adoption considering personal innovativeness, trust, and self-efficacy as extension constructs and focusing on experience and policy as moderating constructs."
Navigating (Dis)agreement: AI Assistance to Uncover Peer Feedback Discrepancies,"Rashid, MP; Gehringer, E; Khosravi, H",10.1145/3636555.3636931,2024,"Engaging students in the peer review process has been recognized as a valuable educational tool. It not only nurtures a collaborative learning environment where reviewees receive timely and rich feedback but also enhances the reviewer ' s critical thinking skills and encourages reflective self-evaluation. However, a common concern arises when students encounter misaligned or conflicting feedback. Not only can such feedback confuse students; but it can also make it difficult for the instructor to rely on the reviews when assigning a score to the work. Addressing this pressing issue, our paper introduces an innovative, AI-assisted approach that is designed to detect and highlight disagreements within formative feedback. We ' ve harnessed extensive data from 170 students, analyzing 15,500 instances of peer feedback from a software development course. By utilizing clustering techniques coupled with sophisticated natural language processing (NLP) models, we transform feedback into distinct feature vectors to pinpoint disagreements. The findings from our study underscore the effectiveness of our approach in enhancing text representations to significantly boost the capability of clustering algorithms in discerning disagreements in feedback. These insights bear implications for educators and software development courses, offering a promising route to streamline and refine the peer review process for the betterment of student learning outcomes."
Automatic dataset generation for automated program repair of bugs and vulnerabilities through SonarQube,"del-Hoyo-Gabaldon, JA; Moreno-Cediel, A; Garcia-Lopez, E; Garcia-Cabot, A; de-Fitero-Dominguez, D",10.1016/j.softx.2024.101664,2024,"Software maintenance is an important and expensive stage during software development. Most of these tasks are done manually with static code analyzers, but this might change if new Artificial Intelligence approaches were used. For this purpose, huge amounts of data are extremely necessary to achieve a good performance by using traditional Data Science and Deep Learning techniques. Accordingly, this paper presents a software capable of creating, automatically, customizable coding error datasets in JSON format by using the SonarQube static analyzer. Consequently, coding error datasets could be easily created, encouraging new maintenance approaches (e.g., automated program repair through Deep Learning Models)."
Learn to Code Sustainably: An Empirical Study on Green Code Generation,"Vartziotis, T; Dellatolas, I; Dasoulas, G; Schmidt, M; Schneider, F; Hoffmann, T; Kotsopoulos, S; Keckeisen, M",10.1145/3643795.3648394,2024,"The increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers. These contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (AI) models. The need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of AI models can lead to energy efficiency gains. Here, we provide an empirical study on green code and an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of AI models. In this framework, we evaluate the sustainability of auto-generated code. The auto-generated code considered in this study is produced by generative commercial AI language models, GitHub Copilot, OpenAI ChatGPT-3, and Amazon CodeWhisperer. Within our methodology, in order to quantify the sustainability awareness of these AI models, we propose a definition of the code's green capacity, based on certain sustainability metrics. We compare the performance and green capacity of human-generated code and code generated by the three AI language models in response to easy-to-hard problem statements. Our findings shed light on the current capacity of AI models to contribute to sustainable software development."
Researchers' Concerns on Artificial Intelligence Ethics: Results from a Scenario-Based Survey,"Jantunen, M; Meyes, R; Kurchyna, V; Meisen, T; Abrahamsson, P; Mohanani, R",10.1145/3643690.3648238,2024,"The ethical impacts of Artificial Intelligence (AI) are causing concern in many areas of AI research and development. The implementation of AI ethics is still, in many ways, a work in progress, but various initiatives are tackling the issues by creating guidelines and implementation methods. This study investigates concerns about the negative impacts of AI systems posed by researchers working with AI. The study was conducted as a scenario-based survey, in which participants answered the question, What could go wrong? regarding five scenarios depicting fictional AI systems. The study concludes with the results from 33 survey participants who gave 161 responses to the scenarios. The results suggest that researchers can identify threats posed by AI systems, particularly regarding their social and ethical consequences. This is even though half of the participants reported limited involvement with AI ethics in their work. The widespread understanding of ethics among researchers could positively impact AI software development due to increased capabilities to bring theoretical AI ethics to practice."
"Enhancing Agile Story Point Estimation: Integrating Deep Learning, Machine Learning, and Natural Language Processing with SBERT and Gradient Boosted Trees","YalÃ§iner, B; DinÃ§er, K; KaraÃ§or, AG; Efe, MO",10.3390/app14167305,2024,"Advances in software engineering, particularly in Agile software development (ASD), demand innovative approaches to effort estimation due to the volatility in Agile environments. Recent trends have made the automation of story point (SP) estimation increasingly relevant, with significant potential for enhancing accuracy. This study introduces a novel model for software effort estimation (SEE) utilizing a deep learning (DL)-based sentence-BERT (SBERT) model for feature extraction combined with advanced gradient-boosted tree (GBT) algorithms. A comprehensive evaluation shows that the proposed model outperforms standard SEE and state-of-the-art models, demonstrating a mean absolute error (MAE) of 2.15 and a median absolute error (MdAE) of 1.85, representing a 12% improvement over the baseline model and an 18% improvement over the best-performing state-of-the-art model. The standardized accuracy (SA) is 93%, which is 7% higher than the next best model, across a dataset of 31,960 issues from 26 open-source Agile projects. This study contributes to software engineering by offering a more accurate and reliable decision support system for estimating project efforts."
Decoding and Answering Developers' Questions about Web Services Managed by Marketplaces,"Baravkar, S; Zhang, C; Hassan, F; Cheng, L; Song, Z",10.1109/SSE62657.2024.00037,2024,"Service registry, a key component of the service-oriented architecture (SOA), aids software developers in discovering services that meet specific functionality requirements. Recent years have witnessed the transition from the traditional service registries to its successor, the Service Marketplaces, which involves deeper engagement in the SOA software lifecycle and offers additional features, such as service request delegation and monitoring of services' Quality of Service (QoS). However, by analyzing developers' questions posted on online Q&A forums, we found that many developers struggle with such transition, leading to development inefficiencies and even security vulnerabilities. This paper presents the first empirical study aimed at uncovering the issues developers face with marketplaces, particularly those arising from the transition. Through a meticulous process of manually labeling and analyzing developers' questions, we develop a taxonomy of these issues, summarize the impacts caused by the transition, and provide actionable suggestions to App developers, service providers, and marketplaces. Utilizing the labeled questions and our insights, we fine-tune a Large Language Model (LLM) for providing answers to similar questions raised by developers and helping service providers and marketplaces extract useful information from these questions, such as service outages and key leakages. Our evaluation of the model's performance in answering and extracting pertinent information from a set of real-world questions demonstrates its effectiveness: it accurately classified 85% of the queries and successfully identified 88% of service names and 77% of key leakages. As the first empirical study in this domain, this work not only aids developers in navigating the transition more effectively but also sheds light on the under explored issue of service registry evolution, offering valuable insights for researchers."
Assessing Code Review Quality with ChatGPT: A Survey of Automated Reviewer Assignment Methods and Experimental Outcomes,"Zydron, P; Protasiewicz, J",10.1007/978-3-031-66594-3_6,2024,"In software development, the efficiency of the code review processes is paramount and necessitates intelligent reviewer recommendations. This article explores various methodologies-including machine learning, heuristic-based algorithms, and social network analysisto automate the process of reviewer suggestion. We examined the RevFinder, TIE, WhoReview, and RSTrace+ approaches, considering file paths, commit messages, reviewer expertise, workload, and response time. We conducted an experiment to assess the review quality annotation abilities of gpt-3.5-turbo and gpt-4 compared to domain expert annotations. The gpt-4 model produced promising results in automating the evaluation of code review quality, aligning with expert opinions 69% of the time. The gpt-3.5-turbo model failed to exhibit similar capabilities. Future research should focus on optimising the practical application of large language models such as gpt-4, concentrating on the reliable handling of linguistic and technical nuances, and the development of industry-specific private models. These models must ensure secure and effective reviewer recommendation in software development, as well as catering to the unique needs and challenges of the industry."
A Software Bug Fixing Approach Based on Knowledge-Enhanced Large Language Models,"Bo, LL; He, YT; Sun, XB; Ji, WJ; Wu, XH",10.1109/QRS62785.2024.00026,2024,"Software Bug Fixing is a time-consuming task in software development and maintenance. Despite the success of Large Language Models (LLMs) using in Automatic Program Repair (APR), they still have the limitations of generating patches with low accuracy and explainability. In this paper, we propose a software bug-fixing approach based on knowledge-enhanced large language models. First, we collect bugs as well as their fix information from bug tracking systems, such as Github and Stack Overflow. Then, we extract bug entities and inter-entity relationships using Named Entity Recognition (NER) to construct a Bug Knowledge Graph (BKG). Finally, we utilize LLMs (e.g., GPT-4) which is enhanced by the knowledge of the similar historical bugs as well as fix information from BKG to generate patches for new bugs. The experimental results show that the our approach can fix 28.52% (85/298) bugs correctly, which is significantly better than the state-of-the-art approaches. Furthermore, the generated patches are explainable and more credible."
ReqCompletion: Domain-Enhanced Automatic Completion for Software Requirements,"Lian, XL; Ma, JP; Lv, HY; Zhang, L",10.1109/RE59067.2024.00023,2024,"Software requirements are the driving force behind software development. As the cornerstone of the entire software lifecycle, the efficiency of crafting requirement specifications and the quality of these requirements significantly influence the duration of software development. Despite massive research on requirements elicitation, the reality is that requirements are often painstakingly crafted manually, word by word. This manual process is not only time-consuming but also prone to issues such as the misuse of terminology. To address these challenges, we introduce ReqCompletion, an approach designed to recommend the next token in real-time for given prefix of requirements description. ReqCompletion comprises two primary components. First, we have devised and integrated a knowledge-injection module into GPT-2-which stands as the largest available GPT model that allows for fine-tuning on specialized downstream tasks. This injection imbues GPT-2 with richer domain-specific knowledge, thus improving the relevance of the suggested tokens. Additionally, we employ a pointer network to optimize the recommendation quality by utilizing completed requirements as contextual support. Empirical evaluations using two public datasets demonstrate that ReqCompletion surpasses all baselines in performance (Recall@7 gains up to 65.87% than the second-best model). Furthermore, the effectiveness of its two pivotal design elements has been substantiated through rigorous ablation studies. The utility of our work has been evaluated preliminarily through a small user study."
Glottis Recognition Software Development Using Artificial Intelligence,"Masumori, Y; Inoue, S; Seino, Y; Konishi, M; Nishikawa, H",10.7759/cureus.61464,2024,"The use of video laryngoscopes has enhanced the visualization of the vocal cords, thereby improving the accessibility of tracheal intubation. Employing artificial intelligence (AI) to recognize images obtained through video laryngoscopy, particularly when marking the epiglottis and vocal cords, may elucidate anatomical structures and enhance anatomical comprehension of anatomy. This study investigates the ability of an AI model to accurately identify the glottis in video laryngoscope images captured from a manikin. Tracheal intubation was conducted on a manikin using a bronchoscope with recording capabilities, and image data of the glottis was gathered for creating an AI model. Data preprocessing and annotation of the vocal cords, epiglottis, and glottis were performed, and human annotation of the vocal cords, epiglottis, and glottis was carried out. Based on the AI's determinations, anatomical structures were color -coded for identification. The recognition accuracy of the epiglottis and vocal cords recognized by the AI model was 0.9516, which was over 95%. The AI successfully marked the glottis, epiglottis, and vocal cords during the tracheal intubation process. These markings significantly aided in the visual identification of the respective structures with an accuracy of more than 95%. The AI demonstrated the ability to recognize the epiglottis, vocal cords, and glottis using an image recognition model of a manikin."
Automated Quality Concerns Extraction from User Stories and Acceptance Criteria for Early Architectural Decisions,"Alam, KA; Asif, H; Inayat, I; Khan, SUR",10.1007/978-3-031-70797-1_24,2024,"User stories serve as a fundamental tool in agile software development methodologies, articulating the functional requirements of a system from an end-user perspective. However, while user stories are crucial for capturing the desired features and functionalities, they frequently overlook the non-functional aspects critical to the system's success. Despite their paramount importance, these quality concerns often remain implicit or underrepresented in user stories, necessitating a deliberate effort to extract them during the elicitation and architectural analysis phases. Failure to address these quality concerns upfront may lead to poor architectural decisions. Consequently, this oversight may result in sub-optimal system designs, increased development costs, delayed time-to-market, diminished user satisfaction, and increased operational risks. This paper presents an ISO-25010 compliant Transfer Learning approach for automated quality concerns extraction from user stories and corresponding acceptance criteria. The proposed solution is constructed upon the Transformer-based RoBERTa-Large model, leveraging and extending its pre-trained capabilities. This approach proficiently classifies user stories and acceptance criteria into 5 most critical user quality concerns including Usability, Performance, Reliability, Security, and Compatibility. This process involves cleaning and preprocessing the dataset followed by fine-tuning the pre-trained models on the refined data set. A comparative analysis of the Three mainstream BERT variants including RoBERTa-base, DistilBERT and XLNET is also provided. Considering the non-availability of public data sets in this scope, a dataset of approximately 1000 user stories with acceptance criteria was compiled from diverse sources and real-world projects. This dataset was subsequently labeled through an extensive labeling activity. The findings suggest that the RoBERTa-Large fine-tuned variant achieves an impressive level of performance in terms of accuracy, precision, recall and Avg F1 score."
A Transformer-based Model for Assisting Dockerfile Revising,"Wu, YW; Zhang, Y; Wang, T; Wang, HM",10.1145/3639478.3643083,2024,"Dockerfile plays an important role in the containerized software development process since it specifies the structure and functionality of the built Docker image. Currently, Dockerfile writing and modification still rely on manual operations which can be time-consuming. Thus, there is a need for automation tools to support the Dockerfile revising process. In this study, we focus on utilizing pre-training techniques for the tasks in the Dockerfile revising scenario. We propose a Transformer-based model and pre-train it with an instruction-aware objective. Furthermore, we fine-tune our model in two downstream tasks, including revision opportunity estimation and revision activity prediction. The experimental results show that our model outperforms the baseline models."
Enhancing User Story Generation in Agile Software Development through Open AI and Prompt Engineering,"Ramasamy, V; Ramamoorthy, S; Walia, GS; Kulpinski, E; Antreassian, A",10.1109/FIE61694.2024.10893343,2024,"This innovative practice full paper explores the use of AI technologies in user story generation. With the emergence of agile software development, generating comprehensive user stories that capture all necessary functionalities and perspectives has become crucial for software development. Every computing program in the United States requires a semester- or year-long senior capstone project, which requires student teams to gather and document technical requirements. Effective user story generation is crucial for successfully implementing software projects. However, user stories written in natural language can be prone to inherent defects such as incompleteness and incorrectness, which may creep in during the downstream development activities like software designs, construction, and testing. One of the challenges faced by software engineering educators is to teach students how to elicit and document requirements, which serve as a blueprint for software development. Advanced AI technologies have increased the popularity of large language models (LLMs) trained on large multimodal datasets. Therefore, utilizing LLM-based techniques can assist educators in helping students discover aspects of user stories that may have been overlooked or missed during the manual analysis of requirements from various stakeholders. The main goal of this research study is to investigate the potential application of OpenAI techniques in software development courses at two academic institutions to enhance software design and development processes, aiming to improve innovation and efficiency in team project-based educational settings. The data used for the study constitute student teams generating user stories by traditional methods (control) vs. student teams using OpenAI agents (treatment) such as gpt-4-turbo for generating user stories. The overarching research questions include: RQ-1) What aspects of user stories generated using OpenAI prompt engineering differ significantly from those generated using the traditional method? RQ-2) Can the prompt engineering data provide insights into the efficacy of the questions/prompts that affect the quality and comprehensiveness of user stories created by software development teams? Industry experts evaluated the user stories created and analyzed how prompt engineering affects the overall effectiveness and innovation of user story creation, which provided guidelines for incorporating AI-driven approaches into software development practices. Overall, this research seeks to contribute to the growing body of knowledge on the application of AI in software engineering education, specifically in user story generation. Investigating the use of AI technologies in user story generation could further enhance the usability of prompt engineering in agile software development environments. We plan to expand the study to investigate the long-term effects of prompt engineering on all phases of software development."
F-CodeLLM: A Federated Learning Framework for Adapting Large Language Models to Practical Software Development,"Cai, ZJ; Chen, JG; Chen, WQ; Wang, WC; Zhu, XY; Ouyang, AJ",10.1145/3639478.3643533,2024,"Large Language Models (LLMs) have revolutionized code intelligence tasks, but their performance in specific software development tasks often requires fine-tuning with task-specific data. However, acquiring such data is challenging due to privacy concerns. We introduce F-CodeLLM, a novel federated learning framework for adapting LLMs to software development tasks while preserving code data privacy. Leveraging federated learning and LoRA-based efficient fine-tuning, F-CodeLLM allows organizations to collaboratively improve LLMs without sharing sensitive data. Our experiments demonstrate that F-CodeLLM achieves comparable results to centralized fine-tuning methods and excels in multi-language environments, marking a significant advancement in the application of LLMs for software engineering."
Investigating the Utility of ChatGPT in the Issue Tracking System: An Exploratory Study,"Das, JK; Mondal, S; Roy, CK",10.1145/3643991.3645083,2024,"Issue tracking systems serve as the primary tool for incorporating external users and customizing a software project to meet the users' requirements. However, the limited number of contributors and the challenge of identifying the best approach for each issue often impede effective resolution. Recently, an increasing number of developers are turning to AI tools like ChatGPT to enhance problem-solving efficiency. While previous studies have demonstrated the potential of ChatGPT in areas such as automatic program repair, debugging, and code generation, there is a lack of study on how developers explicitly utilize ChatGPT to resolve issues in their tracking system. Hence, this study aims to examine the interaction between ChatGPT and developers to analyze their prevalent activities and provide a resolution. In addition, we assess the code reliability by confirming if the code produced by ChatGPT was integrated into the project's codebase using the clone detection tool NiCad. Our investigation reveals that developers mainly use ChatGPT for brainstorming solutions but often opt to write their code instead of using ChatGPT-generated code, possibly due to concerns over the generation of hallucinated code, as highlighted in the literature."
"Diverging assessments: What, Why, and Experiences","Sakzad, A; Paul, D; Sheard, J; Brankovic, L; Skerritt, MP; Li, N; Minagar, S; Simon; Billingsly, W",10.1145/3626252.3630832,2024,"In this experience paper, we introduce the concept of 'diverging assessments', process-based assessments designed so that they become unique for each student while all students see a common skeleton. We present experiences with diverging assessments in the contexts of computer networks, operating systems, ethical hacking, and software development. All the given examples allow the use of generative-AI-based tools, are authentic, and are designed to generate learning opportunities that foster students' meta-cognition. Finally, we reflect upon these experiences in five different courses across four universities, showing how diverging assessments enhance students' learning while respecting academic integrity."
The use of artificial intelligence for automatic analysis and reporting of software defects,"Esposito, M; Sarbazvatan, S; Tse, T; Silva-Atencio, G",10.3389/frai.2024.1443956,2024,"The COVID-19 pandemic marked a before and after in the business world, causing a growing demand for applications that streamline operations, reduce delivery times and costs, and improve the quality of products. In this context, artificial intelligence (AI) has taken a relevant role in improving these processes, since it incorporates mathematical models that allow analyzing the logical structure of the systems to detect and reduce errors or failures in real-time. This study aimed to determine the most relevant aspects to be considered for detecting software defects using AI. The methodology used was qualitative, with an exploratory, descriptive, and non-experimental approach. The technique involved a documentary review of 79 bibliometric references. The most relevant finding was the use of regression testing techniques and automated log files, in machine learning (ML) and robotic process automation (RPA) environments. These techniques help reduce the time required to identify failures, thereby enhancing efficiency and effectiveness in the lifecycle of applications. In conclusion, companies that incorporate AI algorithms will be able to include an agile model in their lifecycle, as they will reduce the rate of failures, errors, and breakdowns allowing cost savings, and ensuring quality."
Introduction of a Remote Lab for Indoor Object Position Control based on Computer Vision Sensors and AI-enabled Embedded Systems,"Beneder, R; Schmitt, P",10.1109/EDUCON60312.2024.10578864,2024,"Recent strides in the efficacy of Artificial Intelligence (AI), the adoption of software frameworks for AI and the power of Artificial Intelligence and Machine Learning (AIML) Embedded Platforms have come together to unlock the power of AI for embedded devices. The integration of these topics into degree program courses is mandatory for electronic engineers of the future. The University of Applied Science (UAS) Technikum Wien offers various bachelor and master degree programs dedicated to electronic engineering particularly with a focus on embedded systems software development based on compact AI inference embedded platforms. These degree programs are available in the form of full-time (accompanied with remote classrooms) and part-time. Moreover, the university promotes these degree programs to be adaptable for handicapped students as well. Due to the fact, that part-time and handicapped students do not attend the university on a regular basis and even full-time students have to attend remote classroom sessions, it is necessary that they have the opportunity to complete their hands-on exercises and assignments at home. These tasks often require the implementation of computer vision software modules based on various sensor technologies, utilization of (pre-)trained neural networks for embedded devices, implementation of embedded control algorithms and evaluation of their applications. To address this need of access outside the university, a suitable remote lab which provides convenient hardware-acceleration for AI-enabled embedded computer vision is necessary. This paper introduces a remote lab for indoor object position control based on various computer vision sensor technologies accessed via an AI-enabled embedded platform which has the potential to increase the availability of expensive AIML embedded platform hardware and closes the lack of access for students. Hence, this paper gives an overview of the actual indoor object position control application, the utilized sensor technologies and the graphical user interface."
Testing Learning-Enabled Cyber-Physical Systems with Large-Language Models: A Formal Approach,"Zheng, X; Mok, AK; Piskac, R; Lee, YJ; Krishnamachari, B; Zhu, DK; Sokolsky, O; Lee, I",10.1145/3663529.3663779,2024,"The integration of machine learning into cyber-physical systems (CPS) promises enhanced efficiency and autonomous capabilities, revolutionizing fields like autonomous vehicles and telemedicine. This evolution necessitates a shift in the software development life cycle, where data and learning are pivotal. Traditional verification and validation methods are inadequate for these AI-driven systems. This study focuses on the challenges in ensuring safety in learning-enabled CPS. It emphasizes the role of testing as a primary method for verification and validation, critiques current methodologies, and advocates for a more rigorous approach to assure formal safety."
Leveraging Historical Data to Support User Story Estimation,"Duszkiewicz, AG; Sorensen, JG; Johansen, N; Edison, H; Silva, TR",10.1007/978-3-031-49266-2_20,2024,"Accurate and reliable effort and cost estimation are still challenging for agile teams in the industry. It is argued that leveraging historical data regarding the actual time spent on similar past projects could be very helpful to support such an activity before companies embark upon a new project. In this paper, we investigate to what extent user story information retrieved from past projects can help developers estimate the effort needed to develop new similar projects. In close collaboration with a software development company, we applied design science and action research principles to develop and evaluate a tool that employs Natural Language Processing (NLP) algorithms to find past similar user stories and retrieve the actual time spent on them. The tool was then used to estimate a real project that was about to start in the company. A focus group with a team of six developers was conducted to evaluate the tool's efficacy in estimating similar projects. The results of the focus group with the developers revealed that the tool has the potential to complement the existing estimation process and help different interested parties in the company. Our results contribute both towards a new tool-supported approach to help user story estimation based on historical data and with our lessons learned on why, when, and where such a tool and the estimations provided may play a role in agile projects in the industry."
Anew approach for competency frameworks mapping using large language models,"Jemal, I; Armand, NSW; Chikhaoui, B",10.1016/j.eswa.2024.125648,2025,"Competency frameworks are essential for organizations to align their workforce with strategic goals and for individuals to assess and develop their skills. However, the absence of a universal or unified competency framework presents a challenge, as each framework is subject to distinct guidelines and standards. Moreover, within a single framework, continuous updates to align with evolving standards can lead to equivalent competencies being expressed indifferent ways. Despite the fundamental similarity of the competencies, this divergence across frameworks can impede interoperability and complicate the aggregation of data from multiple frameworks. This paper addresses this issue by proposing an approach that leverages large language models (LLMs) for mapping competency frameworks to enhance interoperability among frameworks. We investigated various pre-trained LLMs to encode competency names from each framework. Subsequently, we employed cosine similarity to measure semantic similarity scores, which facilitated the identification of equivalent or closely related competencies across different frameworks. We evaluated our approach using three competency frameworks for project management, each aligned with different editions of the Project Management Body of Knowledge (PMBOK) standards. The experimental results demonstrate the effectiveness of the proposed approach in ameliorating frameworks interoperability."
"Assessing GitHub Copilot in Solidity Development: Capabilities, Testing, and Bug Fixing","Baralla, G; Ibba, G; Tonelli, R",10.1109/ACCESS.2024.3486365,2024,"In the rapidly evolving landscape of blockchain technology, the development of reliable and secure smart contracts represents one of several crucial challenges. GitHub Copilot, an AI-powered code assistant, aims to enhance developer productivity by generating code snippets, facilitating testing, and assisting in program repair. This research examines Copilot's proficiency in generating functional and secure smart contracts, including token creation adhering to standards such as ERC20, ERC721, and ERC1155 with various optional features. Additionally, the study assesses its effectiveness in common development tasks, including the implementation of widely employed libraries such as SafeMath. Through controlled experiments, the accuracy, efficiency, and security of the code generated by Copilot are evaluated. This evaluation identifies both its strengths in expediting the development process and its limitations in managing complex blockchain-specific logic and security considerations. The findings contribute to an expanded understanding of the role of AI-assisted programming in blockchain development, offering insights into how developers can best leverage such tools in creating and testing smart contracts. This research aims to guide both practitioners and researchers in the blockchain domain, advancing the discussion on integrating AI into software development workflows in the context of Solidity and smart contract development, underscoring the need for further research to address the challenges and opportunities presented by AI in blockchain technology."
Fight Fire With Fire: How Much Can We Trust ChatGPT on Source Code-Related Tasks?,"Yu, X; Liu, L; Hu, X; Keung, JW; Liu, J; Xia, X",10.1109/TSE.2024.3492204,2024,"With the increasing utilization of large language models such as ChatGPT during software development, it has become crucial to verify the quality of code content it generates. Recent studies proposed utilizing ChatGPT as both a developer and tester for multi-agent collaborative software development. The multi-agent collaboration empowers ChatGPT to produce test reports for its generated code, enabling it to self-verify the code content and fix bugs based on these reports. However, these studies did not assess the effectiveness of the generated test reports in validating the code. Therefore, we conduct a comprehensive empirical investigation to evaluate ChatGPT's self-verification capability in code generation, code completion, and program repair. We request ChatGPT to (1) generate correct code and then self-verify its correctness; (2) complete code without vulnerabilities and then self-verify for the presence of vulnerabilities; and (3) repair buggy code and then self-verify whether the bugs are resolved. Our findings on two code generation datasets, one code completion dataset, and two program repair datasets reveal the following observations: (1) ChatGPT often erroneously predicts its generated incorrect code as correct, its vulnerable completed code as non-vulnerable, and its failed program repairs as successful during its self-verification. (2) The self-contradictory hallucinations in ChatGPT's behavior arise: (a) ChatGPT initially generates code that it believes to be correct but later predicts it to be incorrect; (b) ChatGPT initially generates code completions that it deems secure but later predicts them to be vulnerable; (c) ChatGPT initially outputs code that it considers successfully repaired but later predicts it to be buggy during its self-verification. (3) The self-verification capability of ChatGPT can be enhanced by asking the guiding question, which queries whether ChatGPT agrees with assertions about incorrectly generated or repaired code and vulnerabilities in completed code. (4) Using test reports generated by ChatGPT can identify more vulnerabilities in completed code, but the explanations for incorrectly generated code and failed repairs are mostly inaccurate in the test reports. Based on these findings, we provide implications for further research or development using ChatGPT."
AGONETEST: Automated creation and assessment of Unit tests leveraging Large Language Models,"Lops, A; Narducci, F; Ragone, A; Trizio, M",10.1145/3691620.3695318,2024,"Software correctness is crucial, with unit testing playing an indispensable role in the software development lifecycle. However, creating unit tests is time-consuming and costly, underlining the need for automation. Leveraging Large Language Models (LLMs) for unit test generation is a promising solution, but existing studies focus on simple, small-scale scenarios, leaving a gap in understanding LLMs' performance in real-world applications, particularly regarding integration and assessment efficacy at scale. Here, we present AgoneTest, a system focused on automatically generating and evaluating complex class-level test suites. Our contributions include a scalable automated system, a newly developed dataset for rigorous evaluation, and a detailed methodology for test quality assessment."
Assessing the Use of GitHub Copilot on Students of Engineering of Information Systems,"Cirett-GalÃ¡n, F; Torres-Peralta, R; Navarro-HernÃ¡ndez, R; Ochoa-HernÃ¡ndez, JL; Contreras-Rivera, S; Estrada-RÃ­os, LA; Machado-Encinas, G",10.1142/S0218194024500335,2024,"This study examines the impact of AI programming assistants like GitHub Copilot and ChatGPT on software engineering efficiency, an area that has seen limited empirical research. We experimentally evaluated the performance of programmers (n=16) in Python coding tasks with and without AI assistance, measuring time-to-completion and feature implementation. Results indicate that participants utilizing AI assistance completed tasks significantly faster (p = 0.033) and implemented more required features (p = 0.012) compared to those relying solely on unaided coding. These findings offer empirical insights into the integration of AI tools in software development workflows, highlighting their potential to enhance efficiency without compromising code quality or completeness, with implications for organizational pipelines and practitioner skills. Responses to exit surveys suggest that participants without IA tools assistance encountered frustrations related to code recall, time constraints, and problem-solving, while assisted participants reported no negative experiences, focusing instead on successful completion of tasks within the allotted time."
Comparison of Multi-Agent Platform Usability for Industrial-Grade Applications,"Wrona, Z; Ganzha, M; Paprzycki, M; Pawlowski, W; Ferrando, A; Cabri, G; Badica, C",10.3390/app142210124,2024,"Modern systems often employ decentralised and distributed approaches. This can be attributed, among others, to the increasing complexity of system processes, which go beyond the capabilities of singular components. Additionally, with the growth in demand for system automation and high-level coordination, solutions belonging to the decentralised Artificial Intelligence and collaborative decision-making are often applied. It can be observed that these concerns fall within the domain of multi-agent systems. However, even though MAS concepts emerged more than 40 years ago, despite their obvious advantages and continuous efforts of the scientific community, agents remain rarely used in industrial-grade applications. In this context, the goal of this contribution is to analyse the reasons for the lack of adoption of agent solutions in the real world. During the analysis, all pertinent aspects of the modern software development life cycle are examined and compared to what is currently available in the agent system domain. Specifically, the study focuses on identifying gaps that are often overlooked when it comes to scientific applications of MAS, but are critical in terms of potential for large-scale system development in practice."
New approach methodologies for risk assessment using deep learning,"Junquera, E; DÃ­az, I; Montes, S; Febbraio, F",10.2903/j.efsa.2024.e221105,2024,"The advancement of technologies and the development of more efficient artificial intelligence (AI) enable the processing of large amounts of data in a very short time. Concurrently, the increase in information within biological databases, such as 3D molecular structures or networks of functional macromolecule associations, will facilitate the creation of new methods for risk assessment that can serve as alternatives to animal testing. Specifically, the predictive capabilities of AI as new approach methodologies (NAMs) are poised to revolutionise risk assessment approaches. Our previous studies on molecular docking predictions, using the software Autodock Vina, indicated high-affinity binding of certain toxic chemicals to the 3D structures of human proteins associated with nervous and reproductive functions. Similar approaches revealed potential sublethal interactions of neonicotinoids with proteins linked to the bees' immune system. Building on these findings, we plan to develop an AI-based decision tool that exploits the data available on the toxicity of the most know chemical, such as LD50, and the data obtainable by their interaction with the human proteins to support risk assessment studies for multiple stressors still not characterised. Our focus will be on utilising these new bioinformatics methodologies to develop specific experimental designs that allow for confident and predictable study of the toxic and sublethal effects of pesticides on humans. We will also validate the developed NAMs by integrating existing in vivo information from scientific literature and technical reports. These approaches will significantly impact toxicity studies, guiding researchers' experiments and greatly reducing the need for animal testing."
Modular Learning: Agile Development of Robust Traffic Sign Recognition,"Lin, YH; Wang, YS",10.1109/TIV.2023.3322407,2024,"Autonomous driving is an important research domain with great impact for the future traffic communication. The trend of self-driving car will affect the design of vehicles that incorporates with numerous intelligent modules (e.g., traffic sign recognition). In contrast to a typical AI task (e.g., digit recognition), self-driving car will have countless situations in the real world. Therefore, the self-driving car system requires a long-term development process or even endless maintenance activities. We have to address the newly discovered image corruption rapidly for a robust recognition system. Hence, we proposed our agile development framework for AI system, which is inspired from agile software development paradigm. Our agile framework of AI system aims to speed up the development cycle for each newly discovered image corruption. We denote the training time of each incremental development cycle as the marginal cost of AI system development. We proposed an agile development paradigm called modular learning that incorporates with knowledge distillation to reduce the marginal cost. The acceleration of our method is up to 49x while the recognition accuracy degradation is around 1%. We found that the studies for long-term AI system development are rarely addressed in the literature. We expect our preliminary and promising results can inspire more efforts in this direction."
Toward a New Era of Rapid Development: Assessing GPT-4-Vision's Capabilities in UML-Based Code Generation,"Antal, G; VozÃ¡r, R; Ferenc, R",10.1145/3643795.3648391,2024,"The emergence of advanced neural networks has opened up new ways in automated code generation from conceptual models, promising to enhance software development processes. This paper presents a preliminary evaluation of GPT-4-Vision, a state-of-the-art deep learning model, and its capabilities in transforming Unified Modeling Language (UML) class diagrams into fully operating Java class files. In our study, we used exported images of 18 class diagrams comprising 10 single-class and 8 multi-class diagrams. We used 3 different prompts for each input, and we manually evaluated the results. We created a scoring system in which we scored the occurrence of elements found in the diagram within the source code. On average, the model was able to generate source code for 88% of the elements shown in the diagrams. Our results indicate that GPT-4-Vision exhibits proficiency in handling single-class UML diagrams, successfully transforming them into syntactically correct class files. However, for multi-class UML diagrams, the model's performance is weaker compared to single-class diagrams. In summary, further investigations are necessary to exploit the model's potential completely."
Chirality Detection in Scanning Tunneling Microscopy Data Using Artificial Intelligence,"Seifert, TJ; Stritzke, M; Kasten, P; MÃ¶ller, B; Fingscheidt, T; Etzkorn, M; de Wolff, T; Schlickum, U",10.1002/smtd.202400549,2024,"Enantiospecific effects play an uprising role in chemistry and technical applications. Chiral molecular networks formed by self-assembly processes at surfaces can be imaged by scanning probe microscopy (SPM). Low contrast and high noise in the topography map often interfere with the automatic image analysis using classical methods. The long SPM image acquisition times restrain Artificial Intelligence-based methods requiring large training sets, leaving only tedious manual work, inducing human-dependent errors and biased labeling. By generating realistic looking synthetic images, the acquisition of real datasets is avoided. Two state-of-the-art object detection architectures are trained to localize and classify chiral unit-cells in a regular molecular chiral network formed by self-assembly of linear molecular bricks. The comparison of different architectures and datasets demonstrates that the training on purely synthetic data outperforms models trained using augmented datasets. A Faster R-CNN model trained solely on synthetic data achieved an excellent mean average precision of 99% on real data. Hence this approach and the transfer to real data show high success, also highlighting the high robustness against experimental noise and different zoom levels across the full experimentally reasonable parameter range. The generalizability of this idea is demonstrated by achieving equally high performance on a different structure, too. The lack of available annotated Scanning Probe Microscopy (SPM) data limits the applicability of Machine Learning to evaluate images on the nanoscale, requiring laborious manual work. The use of synthetic data to train state-of-the-art object detection models can overcome this limitation, enabling a fast, accurate, and reliable analysis of SPM images, as demonstrated on two self-assembled molecular chiral networks. image"
Development of an Artificial Intelligence-Based Application to Predict Air Quality and Environmental Comfort from Design Parameters in New Buildings,"Moreno, S; Villarreal, R; Saavedra, J; Vega-Sampayo, Y; Chamorro-Solano, S; Espejo, CA; Rivera, J; Daza, Y",10.1007/978-981-97-3302-6_48,2024,"In this paper, the development of an AI-based application to predict levels of environmental comfort and air quality in new buildings from the design stage is presented. The objective of this approach is to improve construction quality and reduce their environmental impact. This project uses as knowledge base the experience of the enterprise Bioteckta, a bioclimatics consulting firm, and is funded and supported by the Colombian Ministry of Science (MINCIENCIAS) and the National Learning Service (SENA). The research is divided into three key stages: predictive model optimization, software development, and validation. As a result, a predictive model and a Web application were obtained. A model was generated using 18 design parameters as input variables. In the initial model assessment, an accuracy of 91% was obtained, which then was improved to 100% with machine learning techniques (decision trees). This model was then integrated into a practical and user-friendly Web platform, and a coherence with the base model of 99.7% was obtained, with only one case getting a different result from the optimized base model. These results indicate that the model and Web platform are reliable and can be used to predict environmental comfort in new buildings in a simple and practical way."
Addressing Ethics and Sustainability in Ubiquitous Computing and Ambient Intelligence,"McCullagh, P; Moore, S",10.1007/978-3-031-77571-0_82,2024,"Ubiquitous computing and ambient intelligence have raised ethical concerns of privacy and autonomy. Increasingly ethics also addresses sustainability and green computing, indeed the 'first do not harm' principle is a significant driver. Two of the UN sustainability goals are used to provide a structure for evaluation in this context: good health and wellbeing, quality education. Good health and wellbeing are areas in which ubiquitous computing and ambient intelligence can provide significant positive societal impact. However ethical considerations must be part of the design process and software life cycle. Quality education in computing courses should provide knowledge to advance the discipline from a technical standpoint but also to equip students to critically appraise hardware and software implementations from both ethical and sustainability perspectives."
Exploring the Impact of Vocabulary Techniques on Code Completion: A Comparative Approach,"Hussain, Y; Huang, ZQ; Zhou, Y; Khan, IA",10.1142/S0218194023500687,2024,"Integrated Development Environments (IDEs) are pivotal in enhancing productivity with features like code completion in modern software development. Recent advancements in Natural Language Processing (NLP) have empowered neural language models for code completion. In this study, we present an extensive investigation of the impact of open and closed vocabulary systems on the task of code completion. Specifically, we compare open and closed vocabulary systems with various vocabulary sizes to observe their impact on code completion performance. We experiment with three different open vocabulary systems: byte pair encoding (BPE), WordPiece and Unigram to compare them with closed-vocabulary systems to analyze their modeling performance. We also conduct experiments with different context sizes to study their impact on code completion performance. We have experimented using various prominent language models, including one from recurrent neural networks and five from transformers. Our results indicate that vocabulary size significantly impacts modeling performance and can artificially boost the accuracy of code completion models, especially in the case of a closed-vocabulary system. Moreover, we find that different vocabulary systems have varying impacts on token coverage, whereas open-vocabulary systems exhibit better token coverage. Our findings offer valuable insights for building effective code completion models, aiding researchers and practitioners in this field."
An Ensemble Method for Bug Triaging using Large Language Models,"Dipongkor, AK",10.1145/3639478.3641228,2024,"This study delves into the automation of bug triaging - the process of assigning bug reports to appropriate developers and components in software development. At the core of our investigation are six transformer-based Large Language Models (LLMs), which we fine-tuned using a sequence classification method tailored for bug triaging tasks. Our results demonstrate a noteworthy performance of the DeBERTa model, which significantly outperforms its counterparts CodeBERT, DistilBERT, RoBERTa, ALBERT, and BERT in terms of effectiveness. However, it is crucial to note that despite the varying performance of each model, each model exhibits a unique degree of orthogonality, indicating distinct strengths in their bug triaging capabilities. Leveraging these orthogonal characteristics, we propose an ensemble method combining these LLMs through voting and stacking techniques. Remarkably, our findings reveal that the voting-based ensemble method surpasses all individual baselines in performance."
Exploring the Effectiveness of LLMs in Automated Logging Statement Generation: An Empirical Study,"Li, YC; Huo, YT; Jiang, ZH; Zhong, RY; He, PJ; Su, YX; Briand, LC; Lyu, MR",10.1109/TSE.2024.3475375,2024,"Automated logging statement generation supports developers in documenting critical software runtime behavior. While substantial recent research has focused on retrieval-based and learning-based methods, results suggest they fail to provide appropriate logging statements in real-world complex software. Given the great success in natural language generation and programming language comprehension, large language models (LLMs) might help developers generate logging statements, but this has not yet been investigated. To fill the gap, this paper performs the first study on exploring LLMs for logging statement generation. We first build a logging statement generation dataset, LogBench, with two parts: (1) LogBench-O: 3,870 methods with 6,849 logging statements collected from GitHub repositories, and (2) LogBench-T: the transformed unseen code from LogBench-O. Then, we leverage LogBench to evaluate the effectiveness and generalization capabilities (using LogBench-T) of 13 top-performing LLMs, from 60M to 405B parameters. In addition, we examine the performance of these LLMs against classical retrieval-based and machine learning-based logging methods from the era preceding LLMs. Specifically, we evaluate the logging effectiveness of LLMs by studying their ability to determine logging ingredients and the impact of prompts and external program information. We further evaluate LLM's logging generalization capabilities using unseen data (LogBench-T) derived from code transformation techniques. While existing LLMs deliver decent predictions on logging levels and logging variables, our study indicates that they only achieve a maximum BLEU score of 0.249, thus calling for improvements. The paper also highlights the importance of prompt constructions and external factors (e.g., programming contexts and code comments) for LLMs' logging performance. In addition, we observed that existing LLMs show a significant performance drop (8.2%-16.2% decrease) when dealing with logging unseen code, revealing their unsatisfactory generalization capabilities. Based on these findings, we identify five implications and provide practical advice for future logging research. Our empirical analysis discloses the limitations of current logging approaches while showcasing the potential of LLM-based logging tools, and provides actionable guidance for building more practical models."
Transformer bridge-based metrological unit for scanning thermal microscopy with resistive nanoprobes,"Pruchnik, B; Smagowski, P; Badura, D; Piasecki, T; Polacik, W; Putek, P; Gotszalk, T",10.1088/1361-6501/ad3f38,2024,"Scanning probe microscopy (SPM) is a broad family of diagnostic methods. Common restraint of SPM is only surficial interaction with specimen, especially troublesome in case of complex volumetric systems, e.g. microbial or microelectronic. Scanning thermal microscopy (SThM) overcomes that constraint, since thermal information is collected from broader space. We present transformer bridge-based setup for resistive nanoprobe-based microscopy. With low-frequency (LF) (approx. 1 kHz) detection signal bridge resolution becomes independent on parasitic capacitances present in the measurement setup. We present characterisation of the setup and metrological description-with resolution of the system 0.6 mK with sensitivity as low as 5 mV K-1. Transformer bridge setup brings galvanic separation, enabling measurements in various environments, pursued for purposes of molecular biology. We present results SThM measurement results of high-thermal contrast sample of carbon fibres in an epoxy resin. Finally, we analyse influence of thermal imaging on topography imaging in terms of information channel capacity. We state that transformer bridge-based SThM system is a fully functional design along with low driving frequencies and resistive thermal nanoprobes by Kelvin Nanotechnology."
Component-based Approach to Software Engineering of Machine Learning-enabled Systems,"Indykov, V",10.1145/3644815.3644976,2024,"Machine Learning (ML) - enabled systems capture new frontiers of industrial use. The development of such systems is becoming a priority course for many vendors due to the unique capabilities of Artificial Intelligence (AI) techniques. The current trend today is to integrate ML functionality into complex systems as architectural components. There are a lot of relevant challenges associated with this strategy in terms of the overall system architecture and in the context of development workflow (MLOps). The probabilistic nature, crucial dependency on data, and work in an environment of high uncertainty do not allow software engineers to apply traditional software development methodologies. As a result, there is a community request to systematize the most relevant experience in building software architectures with ML components, to create new approaches to organizing the process of developing ML-enabled systems, and to build new models for assessing the system quality. Our research contributes to all mentioned directions and aims to create a methodology for the efficient implementation of ML-enabled software and AI components. The results of the research can be used in the design and development in industrial settings, as well as a basis for further studies in the research field, which is of both practical and scientific value."
Robustness evaluation of code generation systems via concretizing instructions,"Yan, M; Chen, JJ; Zhang, JM; Cao, XJ; Yang, C; Harman, M",10.1016/j.infsof.2024.107645,2025,"Context: Code generation systems have been extensively developed in recent years to generate source code based on natural language instructions. However, despite their advancements, these systems still robustness issues where even slightly different instructions can result in significantly different code semantics. Robustness is critical for code generation systems, as it can have significant impacts on software development, software quality, and trust in the generated code. Although existing testing techniques for general text text software can detect some robustness issues, they can produce many false positives and are limited effectiveness due to ignoring the characteristics of this kind of systems. Objective: To better evaluate (and further enhance) the robustness of code generation systems, in this we conducted the first exploration by carefully considering the characteristics of code generation systems. Specifically, we propose such a novel technique (called COCO) and perform an extensive study to evaluate robustness of code generation systems with COCO. Method: COCO exploits the usage scenario of code generation systems to make the original programming instruction more concrete by incorporating features known to be present in the original code. A robust system should maintain code semantics for the concretized instruction, and COCO detects robustness inconsistencies when it does not. In the extensive study, we evaluated the robustness of eight advanced code generation systems (including commercial tools Copilot and ChatGPT) with COCO, using two widely-used datasets. Results: Our results demonstrate the effectiveness of COCO. It does not produce any false positive, ensuring the accuracy of robustness evaluation. Additionally, it outperforms the two baselines adopted from general text-to-text software testing, detecting 440.31% and 95.81% more inconsistencies, respectively. Concretized instructions generated by COCO can further help reduce robustness inconsistencies by 21.90% to 60.18% fine-tuning. Conclusions: COCO is effective in detecting robust inconsistencies in code generation systems and significantly outperforms baselines. Additionally, fine-tuning code generation systems with the concretized instructions provided by COCO can largely enhance their robustness."
Cross-lingual Code Clone Detection: When LLMs Fail Short Against Embedding-based Classifier,"Moumoula, MB; Kabore, AK; Klein, J; Bissyande, TF",10.1145/3691620.3695335,2024,"Cross-lingual code clone detection has gained attention in software development due to the use of multiple programming languages. Recent advances in machine learning, particularly Large Language Models (LLMs), have motivated a reexamination of this problem. This paper evaluates the performance of four LLMs and eight prompts for detecting cross-lingual code clones, as well as a pre-trained embedding model for classifying clone pairs. Both approaches are tested on the XLCoST and CodeNet datasets. Our findings show that while LLMs achieve high F1 scores (up to 0.98) on straightforward programming examples, they struggle with complex cases and cross-lingual understanding. In contrast, embedding models, which map code fragments from different languages into a common representation space, allow for the training of a basic classifier that outperforms LLMs by approximately 2 and 24 percentage points on the XLCoST and CodeNet datasets, respectively. This suggests that embedding models provide more robust representations, enabling state-of-the-art performance in cross-lingual code clone detection."
Predicting Tags for Learner Questions on Stack Overflow,"Olatinwo, SO; Epp, CD",10.1007/s40593-024-00441-x,2025,"Online question answering sites, such as Stack Overflow (SO), have become an important learning and support platform for computer-science learners and practitioners who are seeking help. Learners on SO are currently faced with the problem of unanswered questions, inhibiting their lifelong-learning efforts and contributing to delays in their software development process. The major reason for this problem is that most of the technical problems posted on SO are not seen by those who have the required expertise and knowledge to answer a specific question. This issue is often attributed to the use of inappropriate tags when posting questions. We developed a new method, BERT-CBA, to predict tags for answering user questions. BERT-CBA combines a convolutional network, BILSTM, and attention layers with BERT. In BERT-CBA, the convolutional layer extracts the local semantic features of an SO post, the BILSTM layer fuses the local semantic features and the word embeddings (contextual features) of an SO post, and the attention layer selects the important words from a post to identify the most appropriate tag labels. BERT-CBA outperformed four existing tag recommendation approaches by 2-73% as measured by F1@K=1-5. These findings suggest that BERT-CBA could be used to recommend appropriate tags to learners before they post their question which would increase their chances of getting answers."
Enhancing Collaborative Software Development: A Deep Learning Approach for Bot Recommendation,"Zhang, J; Wu, XJ; Zhang, Y; Xu, S; Peng, MQ",10.1109/COMPSAC61105.2024.00180,2024,"In collaborative software development, bots have become increasingly prevalent, making effective bot recommendation a key factor in enhancing development efficiency. This study aims to explore the application and efficacy of deep learning models in bot recommendation. Focusing on the Code-BERT model, we conduct a comprehensive evaluation through comparison with baseline models, parameter tuning (including batch size and learning rate), and the incorporation of language data. Our findings demonstrate that under specific conditions, the CodeBERT model exhibits superior performance in bot recommendation tasks, with parameter adjustments and the inclusion of language data significantly impacting the model's effectiveness. These insights offer new perspectives and strategies for the effective recommendation of bots in open-source software platforms."
Best Practices Implementing AIOps in Large Organizations,"Potts, DWC; Carver, C",10.1109/SMARTNETS61466.2024.10577643,2024,"This paper is a Qualitative Research Narrative Case study in which the researchers were total participants, and federal contractors, focused on an essential but underrepresented area, the Artificial Intelligence for IT Operations (AIOps) space, including the need for understanding interdependencies, technology adoption curve, and consolidation of technical silos. The research question is, Do the observations and recommendations surrounding AIOps from commercial enterprise literature and use cases apply to AIOps executed in the government space by federal contractors? This paper will focus on first a literature review from commercial sources on AIOps, understanding the interdependencies and what technology leaders need to go through for the adoption curve, and then examine merging data science, IT operations, and software development. Then, the paper will provide Veterans Engineering's (VE) related work experience and our observations. Finally, in this paper, we will discuss future work, our recommendations for best practices, and a conclusion to the initial question we asked in this research paper. As researchers and practitioners in the AIOps space, Veterans Engineering (VE) works with federal agencies. It has provided observations and lessons learned from our recent work and research to foster best practices in the AIOPs space. VE's top observations appropriately estimate the interdependencies and the resulting level of effort, including competencies in soft skills, process engineering, political capital, communication, and authority to make the initiative a company or agency-wide effort. VE recommends focusing on the skill domains rather than the tool selection. AIOps connects business processes and values to automated actions; focusing on business objectives first and tool selection second is critical to overall program success. Both the federal government and private sectors face similar challenges and success factors when implementing AIOp tools, such as the need for skilled engineers, dealing with large-scale data, and interoperability issues among tools. Success often hinges on having a senior leader champion the effort and a dedicated team. The federal government could learn from successful private sector practices to navigate these challenges more effectively."
Automated Extraction of Compliance Elements in Software Engineering Contracts Using Natural Language Generation,"Rejithkumar, G; Anish, PR; Sonar, P; Ghaisas, S",10.1145/3643787.3648030,2024,"Software Engineering (SE) contracts are legally binding agreements governing software development, usage, related responsibilities, and rights between the parties involved. SE contracts contain obligatory clauses which encompass compliance elements such as Regulations, Standards, and Policies. Identifying these elements is essential for ensuring compliance by aiding in contract negotiation, assignment of the obligatory clauses to relevant departments within the organization and for identification of high-level software requirements. Non-compliance may lead to penalties, damaging an organization's reputation and trust. In this paper, we automate the extraction of compliance elements from SE contracts. We employed a text-to-text generation approach and conducted experiments using the T5 model for the automated extraction of compliance elements from obligatory contractual clauses. The text-to-text generation approach yielded a mean F1-score of 0.92 for ROUGE-L."
Nigerian Software Engineer or American Data Scientist? GitHub Profile Recruitment Bias in Large Language Models,"Nakano, T; Shimari, K; Kula, RG; Treude, C; Cheong, M; Matsumoto, K",10.1109/ICSME58944.2024.00063,2024,"Large Language Models (LLMs) have taken the world by storm, demonstrating their ability not only to automate tedious tasks, but also to show some degree of proficiency in completing software engineering tasks. A key concern with LLMs is their black-box nature, which obscures their internal workings and could lead to societal biases in their outputs. In the software engineering context, in this early results paper, we empirically explore how well LLMs can automate recruitment tasks for a geographically diverse software team. We use OpenAI's ChatGPT to conduct an initial set of experiments using GitHub User Profiles from four regions to recruit a six-person software development team, analyzing a total of 3,657 profiles over a five-year period (2019-2023). Results indicate that ChatGPT shows preference for some regions over others, even when swapping the location strings of two profiles (counterfactuals). Furthermore, ChatGPT was more likely to assign certain developer roles to users from a specific country, revealing an implicit bias. Overall, this study reveals insights into the inner workings of LLMs and has implications for mitigating such societal biases in these models."
Enhancing Software Effort Estimation with Pre-Trained Word Embeddings: A Small-Dataset Solution for Accurate Story Point Prediction,"Atoum, I; Otoom, AA",10.3390/electronics13234843,2024,"Traditional software effort estimation methods, such as term frequency-inverse document frequency (TF-IDF), are widely used due to their simplicity and interpretability. However, they struggle with limited datasets, fail to capture intricate semantics, and suffer from dimensionality, sparsity, and computational inefficiency. This study used pre-trained word embeddings, including FastText and GPT-2, to improve estimation accuracy in such cases. Seven pre-trained models were evaluated for their ability to effectively represent textual data, addressing the fundamental limitations of TF-IDF through contextualized embeddings. The results show that combining FastText embeddings with support vector machines (SVMs) consistently outperforms traditional approaches, reducing the mean absolute error (MAE) by 5-18% while achieving accuracy comparable to deep learning models like GPT-2. This approach demonstrated the adaptability of pre-trained embeddings for small datasets, balancing semantic richness with computational efficiency. The proposed method optimized project planning and resource allocation while enhancing software development through accurate story point prediction while safeguarding privacy and security through data anonymization. Future research will explore task-specific embeddings tailored to software engineering domains and investigate how dataset characteristics, such as cultural variations, influence model performance, ensuring the development of adaptable, robust, and secure machine learning models for diverse contexts."
Enhancing integration testing efficiency through AI-driven combined structural and textual class coupling metric,"Alazzam, I; AlSobeh, AMR; Melhem, BB",10.30935/ojcmt/15524,2024,"Integration testing, a critical and resource-intensive phase in the software development lifecycle, can account for up to a high percentage of the total testing cost. Identifying classes with high coupling is crucial for efficient integration testing, as these classes are more susceptible to the impact of maintenance-related changes. This research introduces a novel metric called combined structural and textual class coupling (CSTCC), which harnesses the power of artificial intelligence (AI) techniques to predict and rank the most critical classes in an object-oriented software system. CSTCC integrates structural coupling metrics with latent semantic indexing (LSI)-based textual coupling, providing a comprehensive measure of class coupling. LSI, an information retrieval technique, analyses the semantic relationships between classes based on their textual content, enabling CSTCC to capture both structural and conceptual dependencies, resulting in a more accurate identification of high-risk classes. The effectiveness of the proposed approach is rigorously evaluated using mutation testing on four Java open-source projects, and the results demonstrate that test cases developed based on CSTCC achieve high mutation scores, indicating their ability to detect a significant percentage of integration faults. By focusing testing efforts on high-coupling classes identified by CSTCC, developers can potentially save time and cost during integration testing. The results demonstrate that test cases developed based on CSTCC achieve high mutation scores, ranging from 98% to 100%, indicating their ability to detect a significant percentage of integration faults. Additionally, the approach results in substantial efficiency gains, with a notable reduction in the number of test cases needed, saving up to 33.3% of the testing effort in some cases. By focusing testing efforts on high-coupling classes identified by CSTCC, developers can potentially save time and cost during integration testing. The CSTCC metric provides a novel and effective approach to prioritize testing resources and improve the efficiency of integration testing in object-oriented software systems."
A Secure Authentication Scheme with Local Differential Privacy in Edge Intelligence-Enabled VANET,"Kwon, D; Son, S; Park, K; Park, Y",10.3390/math12152383,2024,"Edge intelligence is a technology that integrates edge computing and artificial intelligence to achieve real-time and localized model generation. Thus, users can receive more precise and personalized services in vehicular ad hoc networks (VANETs) using edge intelligence. However, privacy and security challenges still exist, because sensitive data of the vehicle user is necessary for generating a high-accuracy AI model. In this paper, we propose an authentication scheme to preserve the privacy of user data in edge intelligence-enabled VANETs. The proposed scheme can establish a secure communication channel using fuzzy extractor, elliptic curve cryptography (ECC), and physical unclonable function (PUF) technology. The proposed data upload process can provide privacy of the data using local differential privacy and symmetric key encryption. We validate the security robustness of the proposed scheme using informal analysis, the Real-Or-Random (ROR) model, and the Scyther tool. Moreover, we evaluate the computation and communication efficiency of the proposed and related schemes using Multiprecision Integer and Rational Arithmetic Cryptographic Library (MIRACL) software development kit (SDK). We simulate the practical deployment of the proposed scheme using network simulator 3 (NS-3). Our results show that the proposed scheme has a performance improvement of 10 similar to 48% compared to the state-of-the-art research. Thus, we can demonstrate that the proposed scheme provides comprehensive and secure communication for data management in edge intelligence-enabled VANET environments."
Transfer Learning for Caladium bicolor Classification: Proof of Concept to Application Development,"Visutsak, P; Liu, XB; Ryu, KH; Bussabong, N; Sirikong, N; Intamong, P; Sonnui, W; Boonkerd, S; Thongpiem, J; Poonpanit, M; Homwiseswongsa, A; Hirunwannapong, K; Suksomsong, C; Budrit, R",10.3837/tiis.2024.01.008,2024,"Caladium bicolor is one of the most popular plants in Thailand. The original species of Caladium bicolor was found a hundred years ago. Until now, there are more than 500 species through multiplication. The classification of Caladium bicolor can be done by using its color and shape. This study aims to develop a model to classify Caladium bicolor using a transfer learning technique. This work also presents a proof of concept, GUI design, and web application deployment using the user-design-center method. We also evaluated the performance of the following pre-trained models in this work, and the results are as follow: 87.29% for AlexNet, 90.68% for GoogleNet, 93.59% for XceptionNet, 93.22% for MobileNetV2, 89.83% for RestNet18, 88.98% for RestNet50, 97.46% for RestNet101, and 94.92% for InceptionResNetV2. This work was implemented using MATLAB R2023a."
Evaluating Fine-tuned BERT-based Language Models for Web API Recommendation,"Alam, KA; Haroon, M",10.1109/CloudCom62794.2024.00032,2024,"The increasing availability of Web APIs has brought about a revolution in software development. Developers can now create innovative web applications by combining existing services. However, with so many APIs available, it can take time to identify the most suitable ones for a particular task. Many existing recommendation systems rely on keyword matching and historical data, which can limit their effectiveness when dealing with complex functional requirements and new mashup creation scenarios. This paper presents a new method for recommending web APIs to developers for mashup composition. Our goal is to improve the accuracy of recommendations, particularly when developers need more domain knowledge or encounter ambiguous functional descriptions. To achieve this, we propose a solution driven by natural text descriptions, which utilizes advanced techniques such as semantic enrichment and deep learning. The approach to recommendation methods combines content-based and quality-of-service (QoS) techniques with the advanced capabilities of BERT (Bidirectional Encoder Representations from Transformers) Variants and Graph Generative Adversarial Networks (Graph GAN). BERT's Variant's contextual understanding of text allows us to capture more comprehensive functional descriptions, overcoming the limitations of traditional keyword matching. Meanwhile, Graph GAN helps us learn from existing mashup-service invocation records, leading to more accurate and relevant service recommendations. Our framework consists of a robust data and semantic enrichment component that employs paraphrase mining to extend the vocabulary and enhance semantic similarity measures. As a result, our recommendation system can handle various natural language queries and identify subtle contextual nuances in service descriptions."
From Literature to Practice: Exploring Fairness Testing Tools for the Software Industry Adoption,"Nguyen, T; Baldassarre, MT; de Lima, LF; Santos, RD",10.1145/3674805.3695404,2024,"Context: The increasing integration of artificial intelligence and machine learning into software systems has highlighted the critical importance of ensuring fairness in these technologies. Bias in software can lead to inequitable outcomes, making fairness testing essential. However, the current landscape of fairness testing tools remains underexplored, particularly regarding their practical applicability and usability for software development practitioners. Goal: This study aimed to evaluate the practical applicability of existing fairness testing tools for software development practitioners, assessing their usability, documentation, and overall effectiveness in real-world industry settings. Method: We identified 41 fairness testing tools from the literature and conducted a heuristic evaluation and documentary analysis of their installation processes, user interfaces, supporting documentation, and update frequencies. Technical analysis included assessing configurability for diverse datasets. The analysis focused on identifying strengths and deficiencies to determine their suitability for industry use. Findings: Our findings revealed that most fairness testing tools show significant deficiencies, particularly in user-friendliness, detailed documentation, and configurability. These limitations restrict their practical use in industry settings. The tools also lack regular updates and possess a narrow focus on specific datasets, which constrains their versatility and scalability. Despite some strengths, such as cost-effectiveness and compatibility with several environments, the overall landscape of fairness testing tools requires substantial improvements to meet industry needs. Conclusion: There is a pressing need to develop fairness testing tools that align more closely with industry requirements, offering enhanced usability, comprehensive documentation, and greater configurability to effectively support software development practitioners. In today's world, we need to ensure that AI systems are fair and unbiased. Our study looked at tools designed to test the fairness of software to see if they are practical and easy for software developers to use. We found that while some tools are cost-effective and compatible with various programming environments, many are hard to use and lack detailed instructions. They also tend to focus on specific types of data, which limits their usefulness in real-world situations. Overall, current fairness testing tools need significant improvements to better support software developers in creating fair and equitable technology. We suggest that new tools should be user-friendly, well-documented, and flexible enough to handle different kinds of data, helping developers identify and fix biases early in the development process. This will lead to more trustworthy and fair software for everyone."
An adaptive synthetic method for long sequence radar mode recognition,"Chen, XZ; Hu, MZ; Wang, XB; Liu, XZ; Lu, XY",10.1049/rsn2.12643,2024,"Radar work mode recognition is crucial to analyse radar behaviour and intention. There are some challenges limiting the recognition of long sequences with multiple mode classes. First, the performance of recognition method relies on precise segregation of intercepted sequence, which is often unfeasible in reality. Second, the states at the boundaries of adjacent modes may create extraneous mode samples that intervenes the recognition. Third, current methods fail to deal with the scenarios where multiple modes share the same state sequence. To address these problems, a novel forward matching method (FMM) is proposed, comprising a shortest path method (SPM) for intra-mode recognition, a matching strategy, and an adjustment mechanism. SPM is to provide potential recognition for short fragments of the given long sequence. The matching strategy is to assess the availability of current recognition. The adjustment mechanism tunes the segregation and improves the subsequent recognition. FMM offers several distinct advantages. First, the model can explicitly characterise the mode transition probability and is totally interpretable. Second, FMM can distinguish intentional ambiguities, alleviate mosaic ambiguity and probability deviation associated with inter-mode recognition. Third, FMM is extendable to integrate with other intro-mode recognition methods to cater to various scenarios. A synthetic approach forward matching method (FMM) is proposed to recognise the mode of given state sequence. FMM begins with a tentative segregation from the long sequence, then utilises an intro-mode recognition to the segments. After judging whether the results are 'matched' , FMM adjusts the segregation and provides more precise recognition. image"
A Systematic Literature Review of 10 years of Research on Program Synthesis and Natural Language Processing,"RamÃ­rez-Rueda, R; BenÃ­tez-Guerrero, E; Mezura-Godoy, C; BÃ¡rcenas, E",10.1134/S0361768824700737,2024,"Program synthesis is the process of automatically generating software from a requirement specification. This paper presents a systematic literature review focused on program synthesis from specifications expressed in natural language. The research problem centers on the complexity of automatically generating accurate and robust code from high-level, ambiguous natural language descriptions - a barrier that limits the broader adoption of automatic code generation in software development. To address this issue, the study systematically examines research published between 2014 and 2024, focusing on works that explore various approaches to program synthesis from natural language inputs. The review follows a rigorous methodology, incorporating search strings tailored to capture relevant studies from five major data sources: IEEE, ACM, Springer, Elsevier, and MDPI. The selection process applied strict inclusion and exclusion criteria, resulting in a final set of 20 high-quality studies. The findings reveal significant advancements in the field, particularly in the integration of large language models (LLMs) with program synthesis techniques. The review also highlights the challenges and concludes by outlining key trends and proposing future research directions aimed at overcoming these challenges and expanding the applicability of program synthesis across various domains."
MatsVD: Boosting Statement-Level Vulnerability Detection via Dependency-Based Attention,"Weng, C; Qin, YH; Lin, B; Liu, P; Chen, LQ",10.1145/3671016.3674807,2024,"Software vulnerabilities inevitably arise during software development and may leave behind huge security risks. In order to detect and mitigate vulnerabilities before they can be exploited, various fine-grained deep learning (DL)-based vulnerablity detection (VD) approaches have been proposed to locate vulnerable statements, among which the Transformer-based methods have shown promising performances. However, existing Transformer-based statement-level approaches still suffer from a crucial limitation: they ignore the intrinsic data/control dependency relations between the statements. In this work, we propose a novel Transformer-based model MatsVD, which aims to address the above challenge from two aspects: Firstly, inspired by the hierarchical structure of code (i.e., tokens, statements, and functions), MatsVD comprises three different Transformer-based layers (i.e., statement embedding layer, statement representation layer, and function representation layer) to gradually aggregate the basic code tokens into meaningful statement/function representations; Secondly, to further exploit the data/control dependencies between statements, we replace the original attention mechanism of the Transformer with a novel dependency-based attention by masking irrelevant attention scores according to the program dependency graph. We comprehensively evaluate MatsVD on the widely used C/C++ vulnerability dataset Big-Vul. The results show that MatsVD significantly outperforms 6 other statement-level methods on both binary classification and ranking metrics. In particular, MatsVD obtains an F1 score of 86% and a Top-1 Accuracy of 93% on statement-le, which improves by respectively 22.97% and 7.76% compared to the state-of-the-art method VELVET."
"FeaMod: Enhancing Modularity, Adaptability and Code Reuse in Embedded Software Development","Al Maruf, M; Azim, A; Auluck, N; Sahit, M",10.1109/IRI62200.2024.00058,2024,"The increasing prevalence of embedded systems in Cyber-Physical Systems (CPS) and the Internet of Things (IoT) has amplified the necessity for effective and adaptable software development practices. The challenges encountered in designing and developing these systems stem from the requirement to efficiently integrate advanced computational paradigms like machine learning and fog computing. Their inherent complexity and rigidity often limit the systems' adaptability to evolving requirements and complicate the effective management of feature dependencies, versioning, customization, and configuration in distributed environments. To address these challenges, we propose the FeaMod framework, integrating feature-based modularity with adaptive feature modeling for enhanced efficiency in embedded software design. Using the Bidirectional Encoder Representations from Transformers (BERT) model, FeaMod employs automated feature extraction through advanced static code analysis, facilitating the identification of computational features and requirements from existing codebases. These features are encapsulated in an adaptive feature model (AFM) that encourages code reuse and allows for dynamic configuration and system integration. By introducing a set of rules governing feature relationships, our approach ensures the adaptive nature of the model, enhancing its flexibility in response to changing system requirements, user preferences, and varying environmental conditions."
"Generative AI for software architecture. Applications, challenges, and future directionsâ","Esposito, M; Li, XZ; Moreschini, S; Ahmad, N; Cerny, T; Vaidhyanathan, K; Lenarduzzi, V; Taibi, D",10.1016/j.jss.2025.112607,2026,"Context: Generative Artificial Intelligence (GenAI) is transforming much of software development, yet its application in software architecture is still in its infancy. Aim: Systematically synthesize the use, rationale, contexts, usability, and challenges of GenAI in software architecture. Method: Multivocal literature review (MLR), analyzing peer-reviewed and gray literature, identifying current practices, models, adoption contexts, reported challenges, and extracting themes via open coding. Results: This review identifies a significant adoption of GenAI for architectural decision support and architectural reconstruction. OpenAI GPT models are predominantly applied, and there is consistent use of techniques such as few-shot prompting and retrieval-augmented generation (RAG). GenAI has been applied mostly to the initial stages of the Software Architecture Life Cycle (SALC), such as Requirements-to-Architecture and Architecture-to-Code. Monolithic and microservice architectures were the main targets. However, rigorous testing of GenAI outputs was typically missing from the studies. Among the most frequent challenges are model precision, hallucinations, ethical aspects, privacy issues, lack of architecture-specific datasets, and the absence of sound evaluation frameworks. Conclusions: GenAI shows significant potential in software design, but there are several challenges on its way towards greater adoption. Research efforts should target designing general evaluation methodologies, handling ethics and precision, increasing transparency and explainability, and promoting architecture-specific datasets and benchmarks to overcome the gap between theoretical possibility and practical use."
Designing a CURE for CS1,"Buffardi, K; Brooks, J; Alexander, D",10.1145/3649217.3653573,2024,"Course-based Undergraduate Research Experience (CURE) is a pedagogy for engaging an entire class in the scientific exploration of real research problems with unknown solutions that have impact beyond the classroom. Since CUREs originated in biological sciences, there are unique challenges to adapting the CURE model for computer science. We designed a CURE by aligning its principles with entrepreneurial mindset (EM) and software development practices by applying them in a programming project. This experience report introduces the design and adoption of a CURE project for an introductory programming class (CS1). The CURE project aims to innovate color selection for digital visualizations by facilitating easier interpretation and improved accessibility for people with color vision deficiencies. We designed the color vision project to engage students in developing novel algorithms for issues in color accessibility, which replaced traditional, auto-graded programming projects. The project includes multiple deliverables for students to demonstrate authentic software development practices while incrementally adapting to new discoveries. Over three semesters, we gathered surveys (n=62) to gauge students' connections with scientific practices and entrepreneurial mindsets. Survey responses indicate that students identified with both CURE and EM. We also offer reflections on continuing to improve CURE implementations in computer science and give recommendations on how to mitigate the misuse of artificial intelligence in CURE projects."
Navigating the AI technology landscape from GitHub data,"Choi, J; Lee, S; Lee, H",10.1016/j.techsoc.2025.103090,2026,"As artificial intelligence (AI) is considered a pivotal technology determining competitiveness, understanding the current and future state of AI technology has become crucial. Conventional approaches to mapping the technology landscape have relied heavily on patent data, but patents cannot adequately capture the state of the art in rapidly changing technologies like AI, due to significant time lags from development to registration. Given that much of the AI technology is developed through open source projects on GitHub, the largest and most popular code host and social coding platform, GitHub emerges as a promising data source for navigating the AI technology landscape. This study aims to explore and predict the AI landscape based on GitHub data. We propose a new bibliometric-like measure, called library coupling, which leverages the unique aspect of code reuse in open source software development to capture the relationships between GitHub repositories. A total of 2879 AI-related repositories with Python-based libraries were collected from GitHub. An AI repository network is constructed based on library coupling relationships among these repositories. Using the attributed graph clustering technique, the AI repositories within the network are grouped into 20 AI technology clusters. Subsequently, we employ graph convolutional network-based link prediction to predict the changes in the AI technology landscape. The proposed GitHub-based technology landscaping approach can be effectively utilized to grasp the current state of rapidly evolving AI technologies and predict their future trends, thereby supporting informed decision making in national AI policy formulation and corporate AI strategy."
Leveraging Generative AI for Accelerating Enterprise Application Development: Insights from ChatGPT,"Rajbhoj, A; Sant, T; Somase, A; Kulkarni, V",10.1109/APSEC65559.2024.00052,2024,"Enterprise application development faces significant challenges, with each phase of the software development life cycle (SDLC) requiring experts with specific skills. The expertise of the individuals involved, greatly affects the quality and speed of work in each phase. The large size and complexity of modern software systems further exacerbates these problems. Recently, there has been a growing interest in using Generative AI (GenAI) techniques for software engineering tasks. GenAI can help Subject Matter Experts (SMEs) work more efficiently and can help in overcoming skill barriers. By leveraging GenAI, SMEs can save significant time and effort. This paper introduces meta-model based prompting approach to generate enterprise application code leveraging large language models (LLMs). Prompts help in the refinement of input requirements into refined requirements and design specifications using LLMs, ultimately generating code from these specifications. We share our approach and results of applying approach to generate small yet complex applications."
Analysis of Natural Language Processing Techniques and Tools for Requirements Elicitation: A Systematic Literature Review,"Torres-Igartua, MJ; SÃ¡nchez-GarcÃ­a, AJ; OcharÃ¡n-HernÃ¡ndez, JO; VÃ¡zquez-Reyes, S",10.1109/CONISOFT63288.2024.00017,2024,"Requirements Engineering (RE) encompasses activities such as requirements elicitation, analysis, specification, and validation, which are essential in software development for defining and aligning stakeholder needs and expectations. However, these processes are often laborious and prone to errors and misunderstandings, resulting in systems that fail to meet expectations and require costly revisions. This study specifically focuses on requirements elicitation, as accurate and comprehensive gathering of requirements is vital for the success of subsequent activities. The aim is to explore how Natural Language Processing (NLP) techniques and tools can enhance requirements elicitation, emphasizing their adaptability to diverse linguistic contexts. A Systematic Literature Review (SLR) identified the most commonly used Artificial Intelligence (AI) techniques in NLP, such as Support Vector Machine (SVM), Random Forest (RF), and Transformer-based approaches like Bidirectional Encoder Representations from Transformers (BERT). The Natural Language Toolkit (NLTK) is recognized as a prominent tool in the broader NLP domain, while the ELICitation Aid (ELICA) tool stands out for its application in requirement elicitation. Despite the advantages offered by these approaches, it is crucial to acknowledge and address their limitations. Further research is recommended to enhance their effectiveness, particularly in adapting to languages other than English."
Applications of Generative Artificial Intelligence in the Software Industry,"Damyanov, I; Tsankov, N; Nedyalkov, I",10.18421/TEM134-10,2024,"- The increasing demands on software development are putting serious pressure on its pace. To assist software developers, an increasing number of tools powered by generative artificial intelligence are being introduced. This paper aims to investigate how the use and integration of generative AI have evolved among professionals in the software industry, based on a study involving 104 individuals working in Bulgarian software companies. Data was collected in April 2024 through an online questionnaire with four separate groups of questions related to the use of generative AI at work. The study found that 2/3 of the respondents use generative AI actively in their daily work. They highly value the practical benefits of this type of technology, which most often consist of automating routine activities, accessing information quickly, generating initial code, and writing documentation. As a result of these benefits, developers are increasingly moving towards using generative AI at the expense of professional support platforms. The main benefits they cite include faster solutions, more specific and relevant answers, and significantly shorter time to reach the desired outcome."
Development and Validation of AI-Driven NLP Algorithms for Chatbots in Requirement Engineering,"Seidel, P; SpÃ¤the, S",10.1007/978-3-031-60433-1_9,2024,"The present research focused on the use of artificial intelligence (AI) and natural language processing (NLP) techniques in the field of requirements engineering within software development. The primary challenge is the prevention of miscommunication between the customer and the development team. In the worst-case scenario, it might lead to the premature termination of the project. The aim of this project is to develop a prototype of a chatbot able to evaluate consumer needs and suggest potential requests. The first step comprised a thorough evaluation of the chatbot's requirements, followed by the development of a prototype. Two transformer models have been developed to classify customer input, and an additional model has been established to generate suitable requests. The classification was obtained by assessing the level of detail of the provided user input using a classification model, as well as classifying them based on ISO 25010 (quality criteria for software). Both versions utilized the DistilBERT models as their foundation. A GPT-2 model was trained to generate the inquiry. This approach utilized ambiguous user inputs and generated inquiries to get further information. To determine the user's intention, it was decided to use RASA software to train an intention module. This module will be able to differentiate between a user's question and their intention to proceed with the acceptance procedure. The initial classification model achieved an accuracy of 0.7033, whereas the second model had an accuracy of 0.2784. Moreover, the output generated by the GPT model varies only to a limited degree. The quality of the model is directly influenced by the quality of the training data. Increasing the number of data points and balancing the classes can help enhance the model quality. Nevertheless, this scientific work presents a fundamental basis for the possible utilization of transformer models in the field of requirement engineering. Further exploration of the application of NLP approaches using transformer models to understand customer requirements has the potential to reduce the failure rate of software development projects."
Sustainability Integration of Artificial Intelligence into the Software Development Life Cycle,"Trinh, E; Funke, M; Lago, P; Bogner, J",10.1109/ICSA-C63560.2024.00044,2024,"The onslaught of artificial intelligence (AI) in the global scientific and industrial landscape has brought with it far-reaching implications into how the software development process can be transformed. This article presents a systematic literature review focused on the integration of AI into the software development life cycle (SDLC) with a specific emphasis on sustainability. The research explores the application of AI in all facets of the SDLC. To this end, we structure 34 primary studies into the different stages of the SDLC, including requirements elicitation, analysis/design, development, testing, and deployment, while considering multiple dimensions of sustainability. Our findings present a synthesis of commonly used AI approaches under various aspects. These encompass guidelines for (i) automating requirements formulation, (ii) designing sustainable software, (iii) enhancing energy efficiency and code reuse, and (iv) effectively testing software. Environmental sustainability was found to be the most common dimension in the literature, primarily addressing energy efficiency and electronic waste. Additionally, we identify gaps in the literature, particularly the absence of addressing AI in SDLC from the angle of social sustainability and the lack of integration into developer toolkits."
Generative AI in Higher Education: Educators' Perspectives on Academic Learning and Integrity,"VrÃ¥gÃ¥rd, J; Brorsson, F; Aghaee, N",,2024,"Generative Artificial Intelligence (Gen AI), exemplified by models such as ChatGPT, has rapidly advanced, becoming a significant force in various sectors, including higher education. ChatGPT, a leading application of Gen AI, utilizes large language models (LLMs) to generate human-like text, providing capabilities that range from answering complex questions to facilitating software development. As these tools become increasingly integrated into academic environments, their impact on teaching and learning processes has become more under focus. However, there are not many studies focusing on the impact of such systems on students' performance and the use of such systems. This study therefore examines the impact of GPT tools on higher education, aiming to address the following research questions: How does the use of GPTs influence the teaching and learning process in higher education? What are the perceived impacts of GPTs on educational practices from university educators' perspective? The research employs a qualitative methodology, incorporating semi-structured interviews with educators from Swedish universities who have integrated GPT into their curricula. Data were transcribed and analysed using OpenAI's Whisper to extract key themes and insights. Data analysis was done through thematic analysis and categorizing the data using codes. The study uncovers a dual impact of GPT on education, while it offers substantial opportunities for enhancing productivity and personalized learning, it also raises significant concerns about academic integrity, over-reliance on AI, and the potential influences on students' soft skills. These findings contribute to the discourse on digital learning by highlighting the need for instructional and constructive integration of AI technologies such as GenAI, in educational settings. In addition, the rise and integration of GPT technology is irreversible, and we must adapt to it rather than return to old ways. Embracing AI's potential while addressing its challenges is essential for progress and innovation in this new era. The study emphasizes the importance of developing clear policies and guidelines to ensure that the benefits of GPT are realized without compromising the integrity of the educational process. As such, this research provides valuable insights for educators, policymakers, and scholars interested in the ethical and effective implementation of AI in higher education."
Comparative Analysis of Large Language Models in Source Code Analysis,"Erdogan, H; Turan, NT; Onan, A",10.1007/978-3-031-70018-7_21,2024,"This article is a summary of a study focusing on Artificial Intelligence (AI) based source code analysis amidst the complexity of software development and rapidly evolving technological needs. The study evaluates analyses conducted to improve code quality, detect errors, and perform code optimization by examining the potential impacts of AI in software development processes. The time spent on research and experiments for detecting and resolving errors in the software development process has been a constant source of concern. In this context, the results of using unoptimized source code often lead to outputs that directly affect complex and maintenance costs. The topic has been extensively addressed in the literature as a comprehensive subject known as AI, Code Intelligence (CI), and Programming Language Processing (PLP) and has been the focus of various surveys and application studies. The article suggests that the use of AI could be a potential solution to increase efficiency and minimize errors in software development processes. In the study, two different AI tools, namely ChatGPT and Gemini, were used to address problem resolution. Two different models, GPT4 and Gemini, were included in the analysis process. JavaScript was the preferred language for obtaining source code, which was sourced from the GitHub platform."
CODING WITH AI S AN ASSISTANT : CAN AI GENERATE CONCISE COMPUTER CODE ?,"Millam, A; Bakke, C",10.28945/5362,2024,"Aim/Purpose This paper is part of a multi-case study that aims to test whether generative AI makes an effective coding assistant. Particularly, this work evaluates the ability of two AI chatbots (ChatGPT and Bing Chat) to generate concise computer code, considers ethical issues related to generative AI, and offers suggestions for how to improve the technology. Background Since the release of ChatGPT in 2022, generative artificial intelligence has steadily gained wide use in software development. However, there is conflicting information on the extent to which AI helps developers be more productive in the long term. Also, whether using generated code violates copyright restrictions is a matter of debate. Methodology ChatGPT and Bing Chat were asked the same question, their responses were recorded, and the percentage of each chatbot's code that was extraneous was calculated. Also examined were qualitative factors, such as how often the generated code required modifications before it would run. Contribution This paper adds to the limited body of research on how effective generative AI is at aiding software developers and how to practically address its shortcomings. Findings Results of AI testing observed that 0.7% of lines and 1.4% of characters in ChatGPT's responses were extraneous, while 0.7% of lines and 1.1% of characters in Bing Chat's responses were extraneous. This was well below the 2% threshold, meaning both chatbots can generate concise code. However, code from both chatbots frequently had to be modified before it would work; ChatGPT's code needed major modifications 30% of the time and minor ones 50% of the time, while Bing Chat's code needed major modifications 10% of the time and minor ones 70% of the time. Recommendations for Practitioners Recommendations for Researchers Companies building generative AI solutions are encouraged to use this study's findings to improve their models, specifically by decreasing error rates, adding more training data for programming languages with less public documentation, and implementing a mechanism that checks code for syntactical errors. Devel- opers can use the findings to increase their productivity, learning how to reap generative AI's full potential while being aware of its limitations. Researchers are encouraged to continue where this paper left off, exploring more programming languages and prompting styles than the scope of this study allowed. Impact on Society As artificial intelligence touches more areas of society than ever, it is crucial to make AI models as accurate and dependable as possible. If practitioners and re- searchers use the findings of this paper to improve coders' experience with gen- erative AI, it will make millions of developers more productive, saving their companies money and time. Future Research The results of this study can be strengthened (or refuted) by a future study with a large, diverse dataset that more fully represents the programming languages and prompting styles developers tend to use. Moreover, further research can ex- amine the reasons generative AI fails to deliver working code, which will yield valuable insights into improving these models."
Enhancing Software Design and Developer Experience Via LLMs,"Sun, SM",10.1145/3691620.3695606,2024,"This research explores the transformative potential of generative AI in software development. Generative AI is revolutionizing the field by offering capabilities to automatically generate, refactor, and test code. Through the use of action research, new methods and tools based on generative AI models are studied and developed. The initial focus is on the models' ability to comprehend high-level design concepts. Subsequently, the research moves into the augmented generation of software artifacts. Finally, organization-specific or task-specific methods are introduced to enhance software developers' productivity and experience."
Innovative Approach to Agile Education: Generative AI-Supported Planning Poker Simulation,"Nafil, K; Lefdaoui, Y",10.1109/ITHET61869.2024.10837671,2024,"This study investigates the integration of generative AI into the Scrum planning poker process within an educational environment. Through the Character.ai platform, Students interacted with AI agents, including a Scrum Master and three Developers, using their mobile devices to conduct Planning Poker sessions. Quantitative criteria were developed to assess Agent interactions, Student participation, Collaboration, Communication, and Decision-making Patterns. Initial findings reveal encouraging levels of Student engagement and effective utilization of AI-supported techniques during the sessions. These results highlight the potential of generative AI to enhance educational experiences, particularly within the context of Agile project management methodologies."
Integrating Generative AI in Software Engineering Education: Practical Strategies,"Li, YS; Keung, J; Ma, XX",10.1109/ISET61814.2024.00019,2024,"The transformative influence of generative artificial intelligence (AI), notably large language models (LLMs), has significantly reshaped the software engineering (SE) landscape, impacting various aspects of software development within industry and academia. The imperative to integrate generative AI into educational programs arises from the necessity to furnish graduates with contemporary methodologies that enhance software quality and streamline development processes. Nevertheless, a research gap exists concerning the systematic integration of established SE education guidelines with specific course contexts to strengthen SE education through incorporating generative AI. In response to this gap, our study presents a vision for integrating generative AI into SE education, with a particular emphasis on practical integration strategies aimed at endowing students with essential competencies tailored for contemporary software development. Aligning our vision with the knowledge domains within SE education, we delineate its application across specific areas such as code generation, auto test case completion, and others. The overall objective of these proposed initiatives is to furnish students in SE with an updated and immersive learning experience, thereby addressing the evolving demands of the field."
Fostering Agile IT Project Management and Interpersonal Skills Using AI-Enhanced Game-Based Learning,"Dolezal, D; Velaj, Y; Spreitzer, L; Plant, C",10.1109/FIE61694.2024.10893026,2024,"This research-to-practice full paper showcases the integration of AI-enhanced game-based learning in an IT project management course, aimed at improving students' Agile project management skills. We provide a qualitative analysis of students' reflections on teamwork in the IT project management course, alongside standardized course feedback. Students collaborate in teams of 5-7 on self-selected topics, creating coarse prototypes. The analysis revealed key success factors in teamwork, including effective communication, apportionment of work, regular meetings, and high motivation. Barriers included poor time management, unproductive meetings, external obligations, and absenteeism. Course feedback was overwhelmingly positive, with students valuing the course structure, atmosphere, and lecturer. Suggestions for improvement focused on workload reduction. Then, from these 48 teamwork reflections, we identify requirements for developing an AI-enhanced game-based project simulation platform. The main goal of the platform is to enhance the Agile skills of the students. The course and prototype serve as valuable resources for educators and curriculum designers, with future work focusing on student evaluation and further AI integration."
Agile Project Management Using Large Language Models,"Dhruva, G; Shettigar, I; Parthasarthy, S; Sapna, VM",10.1109/ICITIIT61487.2024.10580873,2024,"Agile data-driven methodology encourages engineering leaders to measure their teams' performance by leveraging metrics for improving visibility, identifying potential roadblocks, and increasing delivery velocity. The work presented here introduces a cutting-edge paradigm of a data-driven approach to Agile project management, contributing to the evolving research in project management methodologies. As organizations strive to consolidate the competitive market, their success is often measured by their agility and resilience. Such agility stems from the underlying management practices that an organization embraces and is crucial for the effective planning and delivery of large-scale projects. While management philosophies have continued to evolve, organizations specializing in software development have increasingly adopted Agile project management practices to keep up with a disruptive landscape inundated with rapidly emerging technological advancements. As organizations have continued to expand globally, the existing Agile practices have been laggard and sluggish, unable to keep up with the demands of a modern market. In this context, the authors introduce an Agile project management framework driven by Large Language Models (LLMs) to facilitate efficient management of large projects."
DevCoach: Supporting Students in Learning the Software Development Life Cycle at Scale with Generative Agents,"Wang, TJ; Ramanujan, R; Lu, Y; Mao, CY; Chen, Y; Brown, C",10.1145/3657604.3664663,2024,"Supporting novice computer science students in learning the software development life cycle (SDLC) at scale is vital for ensuring the quality of future software systems. However, this presents unique challenges, including the need for effective interactive collaboration and access to diverse skill sets of members in the software development team. To address these problems, we present DevCoach, an online system designed to support students learning the SDLC at scale by interacting with generative agents powered by large language models simulating members with different roles in a software development team. Our preliminary user study results reveal that DevCoach improves the experiences and outcomes for students, with regard to learning concepts in SDLC's Plan and Design and Develop phases. We aim to use our findings to enhance DevCoach to support the entire SDLC workflow by incorporating additional simulated roles and enabling students to choose their project topics. Future studies will be conducted in an online Software Engineering class at our institution, aiming to explore and inspire the development of intelligent systems that provide comprehensive SDLC learning experiences to students at scale."
AI-Augmented Software Engineering: Revolutionizing or Challenging Software Quality and Testing?,"Ramos, T; Dean, A; Mcgregor, D",10.1002/smr.2741,2025,"With organizations seeking faster, cheaper, and smarter ways of delivering higher quality software, many are looking towards generative artificial intelligence (AI) to drive efficiencies and innovation throughout the software development lifecycle. However, generative AI can suffer from several fundamental issues, including a lack of traceability in concept generation and decision-making, the potential for making incorrect inferences (hallucinations), shortcomings in response quality, and bias. Quality engineering (QE) has long been utilized to enable more efficient and effective delivery of higher quality software. A core aspect of QE is adopting quality models to support various lifecycle practices, including requirements definition, quality risk assessments, and testing. In this position paper, we introduce the application of QE to AI systems, consider shortcomings in existing AI quality models from the International Organization for Standardization (ISO), and propose extensions to ISO models based on the results of a survey. We also reflect on skills that IT graduates may need in the future, to support delivery of better-quality AI."
Transforming Software Development: A Study on the Integration of Multi-Agent Systems and Large Language Models for Automatic Code Generation,"RamÃ­rez-Rueda, R; BenÃ­tez-Guerrero, E; Mezura-Godoy, C; BÃ¡rcenas, E",10.1109/CONISOFT63288.2024.00013,2024,"This paper explores the integration of Multi-Agent Systems (MAS) and Large Language Models (LLMs) for automatic code generation, addressing the limitations of traditional manual coding. By conducting a comprehensive review of existing literature and analyzing a practical case study, we demonstrate how MAS and LLMs can collaboratively enhance software development processes. The research focuses on the technical and theoretical challenges of this integration, highlighting the potential for improved productivity, adaptability, and quality in code generation. The findings contribute to AI-based software engineering by revealing new research directions in collective intelligence and automated programming."
Research on Computer Intelligent ChatGPT Natural Language Processing System Based on Scientific Knowledge Graph,"Zhan, LC; Huang, CJ",10.1145/3662739.3664740,2024,A synonym mining method is proposed by combining the character vector graph and noise robust learning method. The model uses paired word vectors pre-trained by ChatGPT to enhance entity semantic representation. Classify marks with noise. Then the cross optimal processing is carried out to identify the true and false marks. The two-layer construction system of knowledge extraction and knowledge fusion is constructed to realize the independent construction and answer of software engineering questions. The system effectively improves the efficiency of software project understanding and software reuse.
Can Large-Language Models Replace Humans in Agile Effort Estimation? Lessons from a Controlled Experiment,"Pavlic, L; Saklamaeva, V; Beranic, T",10.3390/app142412006,2024,"Effort estimation is critical in software engineering to assess the resources needed for development tasks and to enable realistic commitments in agile iterations. This study investigates whether generative AI tools, which are transforming various aspects of software development, can improve effort estimation efficiency. A controlled experiment was conducted in which development teams upgraded an existing information system, with the experimental group using the generative-AI-based tool GitLab Duo for estimation and the control group using conventional methods (e.g., planning poker or analogy-based planning). Results show that while generative-AI-based estimation tools achieved only 16% accuracy-currently insufficient for industry standards-they offered valuable support for task breakdown and iteration planning. Participants noted that a combination of conventional methods and AI-based tools could offer enhanced accuracy and efficiency in future planning."
Microservice-Based Interface to ChatGPT,"Ivanov, R; Velkova, V",10.1109/AQTR61889.2024.10554146,2024,"In today's digitally connected world, the emergence of conversational artificial intelligence powered by generative language models has ushered in a new era of human-computer interaction. Chatbots using these technologies are increasingly being used in a variety of scientific as well as social domains. These intelligent conversational agents, powered by advances in generative language models, offer a wide range of applications from customer support and healthcare to software development and education. This paper discusses the development of a microservice that works as an interface to ChatGPT through the GPT API. The goal is to facilitate the integration of next generation chatbots to distributed architecture services. Access to the microservice is implemented using an Advanced Message Queuing Protocol (AMQP) message broker. To conduct the experiments, a microservice was developed that provides a REST interface to the proposed microservice for clients that do not support the AMQP protocoL"
An Empirical Study to Evaluate AIGC Detectors on Code Content,"Wang, J; Liu, SQ; Xie, XF; Li, Y",10.1145/3691620.3695468,2024,"Artificial Intelligence Generated Content (AIGC) has garnered considerable attention for its impressive performance, with Large Language Models (LLMs), like ChatGPT, emerging as a leading AIGC model that produces high-quality responses across various applications, including software development and maintenance. Despite its potential, the misuse of LLMs, especially in security and safety-critical domains, such as academic integrity and answering questions on Stack Overflow, poses significant concerns. Numerous AIGC detectors have been developed and evaluated on natural language data. However, their performance on code-related content generated by LLMs remains unexplored. To fill this gap, in this paper, we present an empirical study evaluating existing AIGC detectors in the software domain. We select three state-of-the-art LLMs, i.e., GPT-3.5, WizardCoder and CodeLlama, for machine-content generation. We further created a comprehensive dataset including 2.23M samples comprising code-related content for each model, encompassing popular software activities like Q&A (150K), code summarization (1M), and code generation (1.1M). We evaluated thirteen AIGC detectors, comprising six commercial and seven open-source solutions, assessing their performance on this dataset. Our results indicate that AIGC detectors perform less on code-related data than natural language data. Fine-tuning can enhance detector performance, especially for content within the same domain; but generalization remains a challenge."
AI Ethical Framework: A Government-Centric Tool Using Generative AI,"KonÃ©, LA; Leonteva, AO; Diallo, MT; Haouba, A; Collet, P",,2024,"Artificial Intelligence (AI) is transforming industries and societies globally. To fully harness this advancement, it is crucial for countries to integrate AI across different domains. Moral relativism in AI ethics suggests that as ethical norms vary significantly across societies, frameworks guiding AI development should be context-specific, reflecting the values, norms, and beliefs of the cultures where these technologies are deployed. To address this challenge, we introduce an intuitive, generative AI based solution that could help governments establish local ethical principles for AI software and ensure adherence to these standards. We propose two web applications: one for government use and another for software developers. The government-centric application dynamically calibrates ethical weights across domains such as the economy, education, and healthcare according to sociocultural context. By using LLMs, this application enables the creation of a tailored ethical blueprint for each domain or context, helping each country or region better define its core values. For developers, we propose a diagnostic application that actively checks software, assessing its alignment with the ethical principles established by the government. This feedback allows developers to recalibrate their AI applications, ensuring they are both efficient and ethically suitable for the intended area of use. In summary, this paper presents a tool utilizing LLMs to adapt software development to the ethical and cultural principles of a specific society."
Interactive AI for Software Development Learning: Shifting Focus to Requirement Specification,"Otsuka, A; Sasaki, A; Ito, A",10.1109/CogInfoCom63007.2024.10894718,2024,"We propose a new approach to programming learning. Specifically, it is a learning method centered on creating requirement specifications using OpenAI's interactive AI, ChatGPT, which enables learners to gain a deeper understanding of the overall program design and structure and aims at learning through the experience of creating and publishing their software. To this end, we clarify the current programming learning status and problems, propose new learning methods, and evaluate their effectiveness. Furthermore, we will develop a vision for the future based on them. We then provide insights into how programming education should evolve and how advanced technologies such as interactive AI should be used."
Integrating Generative AI into Knowledge Management: The Case of Software Development,"Kirchner, K; Bolisani, E; Kassaneh, TC; Scarso, E; Taraghi, N",,2024,"The recent significant developments in Generative Artificial Intelligence (GAI) have contributed to renewing the interest of knowledge management (KM) scholars in Artificial Intelligence. In fact, the latest GAI tools, like ChatGPT or CoPilot, can process extremely large and varied sets of unstructured data and perform different tasks, such as classifying data, reading and writing texts, composing music, and developing software code. These tools already play a role in knowledge management by automating the generation of content and solutions. By harnessing GAI, knowledge management processes might become more efficient, scalable, and adaptive to the needs of organizations and users. In spite of this, studies on the use of GAI as a KM tool are still scarce, and there is a need to collect more empirical evidence about this phenomenon. This study intends to improve our understanding of GAI's use in KM by investigating how software developers use such tools in their knowledge generation process, i.e., to create new software code. The case of software developers was chosen because they are currently among the main adopters of this technology, and the first studies about the use of GAI in software development are already available. Furthermore, while software developers used to reach out to fellow developers on forums like StackOverflow, they now rely more on GAI for help with coding. The study, which, given the novelty of the topic, followed an exploratory approach, is based on interviews with 11 developers coming from 8 different countries and having different backgrounds and experiences with GAI. We found that the interviewees liked the benefits of solving simpler programming tasks efficiently and rapidly. However, GAI requires expertise to review and modify the code and write the right prompts, as well as to evaluate the reliability of the output provided. The study also revealed that knowledge exchange with fellow programmers is partly, but not fully, exchanged with GAI, as it is more efficient. Furthermore, interviewees think that AI could become in the future a new member of the development team. Nevertheless, they also underline that continuous learning, adaptation, and ethical consideration are needed to fully realize the benefits of these powerful tools in software development."
Java coding using artificial intelligence,"Uandykova, M; Baitenova, L; Mukhamejanova, G; Yeleukulova, A; Mirkassimova, T",10.3389/fcomp.2024.1473870,2024,"This study explores the potential of chatbots, specifically ChatGPT, in Java software development. The aim is to classify tasks for effective use of industrial code and develop recommendations for applying chatbot assistance, identifying boundaries where human intervention remains essential. The methodology included analyzing scientific literature and empirically testing ChatGPT-3.5 on various Java development tasks. The tasks were divided into simple (working with XML, JSON, multithreading, and data input/output) and complex (writing MVC applications, REST services, and GUI). The results showed that ChatGPT successfully handles simple tasks but struggles with complex problems. The study identified scenarios where the chatbot can effectively use existing codebases and design patterns to accelerate development. The conclusions highlight ChatGPT's potential in improving developer productivity, optimizing certain development tasks, and more efficiently allocating human resources in projects. However, the study also points out the need for human intervention to verify, correct, and improve generated code. The study contributes to understanding the practical usefulness of chatbots in real development scenarios and offers recommendations for integrating AI tools into the software development process."
Optimizing LLMs for Code Generation: Which Hyperparameter Settings Yield the Best Results?,"Arora, C; Ibn Sayeed, A; Licorish, S; Wang, FY; Treude, C",10.1109/APSEC65559.2024.00039,2024,"Large Language Models (LLMs), such as GPT models, are increasingly used in software engineering for various tasks, such as code generation, requirements management, and debugging. While automating these tasks has garnered significant attention, a systematic study on the impact of varying hyperparameters on code generation outcomes remains unexplored. This study aims to assess LLMs' code generation performance by exhaustively exploring the impact of various hyperparameters. Hyperparameters for LLMs are adjustable settings that affect the model's behaviour and performance. Specifically, we investigated how changes to the hyperparameters-temperature, top probability (top p), frequency penalty, and presence penalty-affect code generation outcomes. We systematically adjusted all hyperparameters together, exploring every possible combination by making small increments to each hyperparameter at a time. This exhaustive approach was applied to 13 Python code generation tasks, yielding one of four outcomes for each hyperparameter combination: no output from the LLM, non-executable code, code that fails unit tests, or correct and functional code. We analysed these outcomes for a total of 14,742 generated Python code segments, focusing on correctness, to determine how the hyperparameters influence the LLM to arrive at each outcome. Using correlation coefficient and regression tree analyses, we ascertained which hyperparameters influence which aspect of the LLM. Our results indicate that optimal performance is achieved with a temperature below 0.5, top probability below 0.75, frequency penalty above -1 and below 1.5, and presence penalty above -1. We make our dataset and results available to facilitate replication."
Exploring AI Integration in Software Development: Case Studies and Insights,"Udoidiok, I; Reza, H; Zhang, JL",10.1109/NAECON61878.2024.10670347,2024,"The integration of Artificial Intelligence in software development has emerged as a promising solution to address the rising costs and scarcity of skilled developers. In this paper, we present a comprehensive evaluation of the impact of AI on software development. Specifically, we examine several AI tools, including GitHub Copilot, Vercel's v0, and Meta AI, through detailed case studies to demonstrate their effectiveness in automating tasks and reducing human resource dependency. Our analysis highlights the significant benefits of AI-driven tools in enhancing developer productivity by automating repetitive and mundane tasks, thereby allowing developers to focus on more complex and creative aspects of software development. Additionally, we discuss the challenges and limitations of integrating AI into the software development lifecycle, focusing on issues such as data quality and model interpretability. Overall, this paper aims to provide practical insights into the real-world applications and implications of AI tools in software development."
Zero-Shot Learning for Accurate Project Duration Prediction in Crowdsourcing Software Development,"Rashid, T; Illahi, I; Umer, Q; Jaffar, MA; Ramay, WY; Hakami, H",10.3390/computers13100266,2024,"Crowdsourcing Software Development (CSD) platforms, i.e., TopCoder, function as intermediaries connecting clients with developers. Despite employing systematic methodologies, these platforms frequently encounter high task abandonment rates, with approximately 19% of projects failing to meet satisfactory outcomes. Although existing research has focused on task scheduling, developer recommendations, and reward mechanisms, there has been insufficient attention to the support of platform moderators, or copilots, who are essential to project success. A critical responsibility of copilots is estimating project duration; however, manual predictions often lead to inconsistencies and delays. This paper introduces an innovative machine learning approach designed to automate the prediction of project duration on CSD platforms. Utilizing historical data from TopCoder, the proposed method extracts pertinent project attributes and preprocesses textual data through Natural Language Processing (NLP). Bidirectional Encoder Representations from Transformers (BERT) are employed to convert textual information into vectors, which are then analyzed using various machine learning algorithms. Zero-shot learning algorithms exhibit superior performance, with an average accuracy of 92.76%, precision of 92.76%, recall of 99.33%, and an f-measure of 95.93%. The implementation of the proposed automated duration prediction model is crucial for enhancing the success rate of crowdsourcing projects, optimizing resource allocation, managing budgets effectively, and improving stakeholder satisfaction."
AI-Integrated Weather Forecasting System: Empowering Agriculture through Intelligent Insights and Accessibility,"Shetty, S; Shetty, M; Shetty, AP; Kumari, KS; Shettigar, M",10.1109/MPCIT62449.2024.10892774,2024,"The increasing unpredictability of weather due to climate change poses significant challenges to agricultural productivity, affecting crop management and food security. To mitigate these challenges, this Artificial Intelligence (AI)-integrated weather forecasting system aimed at enhancing decision making in agriculture practices and offers a novel solution by combining real-time weather data with advanced Artificial Intelligence-driven insights to deliver accurate crop specific recommendations, support farmers in making informed management decisions and optimizing resource utilization for improved yields. Utilizing technologies such as the OpenWeatherMap Application Programming Interface (API) for real-time weather information and the Alan AI Software Development Kit for natural language processing, the system provides personalized, actionable recommendations tailored based on specific growth stages of crops. The application's user-friendly interface, developed with React and Tailwind Cascading Style Sheets (CSS), ensures accessibility and ease of use across various devices, enhancing the farming community's ability to adapt to weather fluctuations effectively. This system's robust back-end infrastructure, built with Node.js and MongoDB, provides efficient data management and seamless integration, facilitating a responsive and reliable service. By incorporating user feedback and continuous technological advancements, this system emphasizes scalability, real-time responsiveness and user centered design. This research makes a novel contribution by combining powerful AI technology with real-time environmental data to assist adaptive and sustainable farming operations, ultimately fostering a sustainable farming future. Future enhancements will focus on integrating more advanced AI algorithms, expanding data sources, and improving user interaction."
Industrial Experience Report on AI-Assisted Coding in Professional Software Development,"Ramler, R; Moser, M; Fischer, L; Nissl, M; Heinzl, R",10.1145/3643795.3648377,2024,"AI-based tools for software development are widely discussed in academic literature. They promise to boost software development performance, especially in code creation. This paper collects insights from practitioners about the use and implications of AI assistance in industrial software development, with a focus on SMEs. Through interviews with five developers from three software development organization, we gathered and analyzed the experiences made in industrial practice, and we identified lessons learned and open challenges. ChatGPT and Copilot are used in industry projects. While they are considered useful for many code-related development activities, their integration in the development workflow remains mostly shallow. Contradicting observations about speed-ups due to AI support in development are reported. Legal issues are of minor concern although awareness exists."
Keynote on Augmented Agile: Human-centred AI-assisted Software Project Management at FinanSE Workshop (ICSE 2024),"Hoda, R",10.1145/3643665.3648567,2024,"Software engineering teams face a number of challenges on a regular basis. Software practitioners have employed agile methods for over two decades to work around challenges such as inadequate customer collaboration, poor management practices, teamwork issues, and coordination challenges in large-scale teams. While they have served us well for over two decades, agile methods are not without limitations. Transitioning into agile methods as well as adapting agile practices to suit different domains such as Finance, can be challenging for software teams, management, and firms at large. This keynote shares experiences from industrial agile research to shed light on what's been working, what's missing, and what can be done better, including a vision of a new future of software project management augmented agile that combines a deeply human-centric approach (the heart') with AI-assisted techniques (the mind') to augment and boost current agile practice."
"DevOps 2.0: Embracing AI/ML, Cloud-Native Development, and a Culture of Continuous Transformation","Chowdary, VHD; Shanmukh, A; Nikhil, TP; Kumar, BS; Khan, F",10.1109/ICPCSN62568.2024.00112,2024,"This research study investigates the emerging domain of DevOps, assessing its capacity to fundamentally transform the processes of software development and delivery. A notable advancement in the field of DevOps is the amalgamation of AI/ML and artificial intelligence. These technologies possess significant potential to revolutionize the DevOps domain through the automation of tedious duties, including provisioning infrastructure, managing configurations, and identifying anomalies. This study highlights the increasing need for streamlined solutions. By harnessing the capabilities of AI and ML-driven tools, DevOps teams will have the ability to streamline their processes, enabling them to allocate resources to more strategic approaches such as nurturing innovation, addressing complex challenges, and making decisions based on data. This study will investigate the precise methods by which AI/ ML can enhance DevOps practices, resulting in a software development environment that is more secure, efficient, and agile."
NLP in Requirements Processing: A Content Analysis Based Systematic Literature Mapping,"Manrique-Losada, B; Moreira, F; Cadavid, EJ",10.1007/978-3-031-60328-0_24,2024,"As a result of the evolution of agile methodologies in the software development industry, there are currently various applications of Natural Language Processing techniques, models, and tools to classify, extract, and analyze documents within the stages of the software development process. However, their utility has been relatively unexplored in relation to the processing of user stories, such as the most widely used technique for capturing and specifying requirements in the last decade. This article presents a content analysis based on a systematic literature mapping on the application of natural language processing in user stories, following Petersen's methodological proposal. The search methodology is based on obtaining relevant articles from Dimensions, ScienceDirect, IEEE, and Scopus. Initially, 483 articles published between 2018 and 2022 were identified, and inclusion and exclusion criteria were applied, filtering down to 125 articles for review. Finally, a quality assessment was conducted, resulting in 57 articles relevant. Analyzing these primary studies, findings are identified, and current/future lines of work are proposed as contributions to this field of knowledge."
Generative Artificial Intelligence to Improve the Learning of Test-Driven Development,"Cassieri, P; Romano, S; Scanniello, G",,2024,"Test-Driven Development (TDD) can be beneficial not only in traditional software development contexts but also in other development contexts such as in the implementation of Embedded Systems (ESs). However, learning TDD, as well as correctly applying it, is not easy, especially in the implementation of ESs. To ease the learning process of TDD in such a context, we envisage using generative Artificial Intelligence (AI)."
Incorporating AI in the Teaching of Requirements Tracing Within Software Engineering,"Couder, JO; Pate, WC; Machado, DA; Ochoa, O",10.1109/FIE61694.2024.10892858,2024,"During the Software Development Lifecycle (SDLC), the first stage entails the Requirement Engineering phase. In this phase, engineers gather, analyze, and specify the requirements for a software system. Requirements play a crucial role in the SDLC as they establish the foundation for the entire system by defining the expected behaviors of the software system to be built. The resulting specifications are captured in a Software Requirement Specification (SRS) document. As part of the validation process, requirement specifications are traced. Requirement tracing involves linking the requirement to the artifacts where the customer requested the high-level requirement. Teaching proper requirements tracing can be challenging in a traditional classroom setting. It is essential to educate future software engineers on the proper process of developing an SRS document and of tracing requirements back to the originating artifact, which is also challenging due to the complexity and large scope of applying the complete requirements engineering process. Understanding how changes in customer needs can impact requirements is an imperative learning opportunity. In this work, we aim to incorporate the use of AI in the teaching of requirements tracing using Large Language Models. In this experiment, both GPT-3.5 and GPT-4 are provided the transcript of an interview between the customer and the engineering team, as well as the subsequent requirements elicited from that meeting and other customer provided artifacts. The GPTs are then instructed to determine which requirements can be traced back to the interview transcript. At the same time, the students (the requirements engineering team) conduct their own effort to trace requirements back to the original interview. The experiment was taken one step further to assess students' and the GPTs abilities to address requirements modifications. After another interview with the customer, where some needs were changed, some requirements were modified, and students, and GPTs were asked to trace the modified requirements to the new interview. The results proved that students are better than both GPT versions at tracing modified requirements, yet GPTs again identified requirements that students didn't trace back. The findings, illustrate that AI can help in the teaching of requirement tracing; these results suggest that while no AI model is currently capable of replacing real requirement engineers as they don't outperform students, it can be used as a tool to test the completeness of the requirement tracing process. We posit that GPT can be a tool for students to self-assess the degree to which their own requirements tracing is exhaustive."
Integration of a NLP-Based Industrial Robot Programming System,"Makrylakis, P; Benardos, P",10.1007/978-3-031-74482-2_35,2024,"This paper presents the integration of a Natural Language Processing (NLP) approach with an industrial robot to enable voice control for online programming. This approach initially involves real-time conversion of voice commands into text, utilizing the Microsoft Speech Software Development Kit (SDK). The generated text is processed to extract numerical data corresponding to desired positions and orientations. Subsequently, a machine learning binary classification model is trained to distinguish between robot related and unrelated commands. The recognized commands undergo further keyword-based classification based on a preselected vocabulary of supported robot operations, so that finally the inverse kinematics problem can be solved and the relevant robot program can be generated. A custom graphical user interface (GUI) facilitates user interaction. Two modes are supported, the first for issuing multiple individual commands and the second for programming typical pick-and-place industrial tasks. The obtained results using a 6-DOF Staubli RX90L robot verify the approach's feasibility."
AACEM: Automatic Annotation and Classification of Emotions for mixed-codes,"Samreen, A; Ali, SA; Shakir, H",10.1016/j.simpa.2024.100626,2024,"This paper presents a framework for automatic creation of an emotions-labeled dataset specifically designed for short texts written in a blend of Roman Urdu and English, and addresses the inherent absence of distinct structure in Roman Urdu language. The software development is carried out in two key phases. During the first phase, cleaning and automatic annotation of raw text is performed and in the second phase, classification of emotions along with prediction is carried out. The developed software significantly simplifies the process of dataset creation by employing natural language processing (NLP) techniques, tailored for the mixed-codes."
NL2Code-Reasoning and Planning with LLMs for Code Development,"Xing, Y; Huan, J; Tok, WH; Shen, C; Gehrke, J; Lin, K; Guha, A; Tripp, O; Ramanathan, MK",10.1145/3637528.3671505,2024,"There is huge value in making software development more productive with AI. An important component of this vision is the capability to translate natural language to a programming language (NL2Code) and thus to significantly accelerate the speed at which code is written. This workshop gathers researchers, practitioners, and users from industry and academia that are working on NL2Code, specifically on the problem of using large language models to convert statements posed in a human language to a formal programming language."
An Intelligent Hybrid GenAI Model for Software Testing,"Maharana, T; Agrawal, N; Sharma, V; Alkhayyat, A",10.1109/ICEC59683.2024.10837186,2024,"Software Testing, a well-defined process and a major component of software development. It covers various methods such as execution of a software program, predicting errors, to meet user needs, and ensure that the system is working correctly. Due to the advanced development in technology around the world, new methods are introduced to test the software. This summary talk discusses the current state of the art of AI(Artificial Intelligence) for software testing. The purpose of software testing is validation, verifying and detection of errors. For the study, we explored around 30 articles from different research surveys. 12-15 articles were selected for the final review. Artificial intelligence(AI) which has emerged in a promising approach to software testing in recent years. The main aim of this paper was to review how Artificial Intelligence(AI) works in Software testing. This paper had also proposed many key offerings of AI in the upcoming future for the area of software testing."
Machine Learning-Based Software Development challenges focusing on using best practices of Software Engineering Standards,"Tablada-Dominguez, A; MuÃ±oz, M; OcharÃ¡n-HernÃ¡ndez, JO; SÃ¡nchez-Garcia, AJ",10.1109/ENC60556.2023.10508685,2024,"The AI era established significant challenges for software developers, especially those working on Machine Learning (ML)-based software. This article presents the findings of a systematic literature review (SLR) focused on identifying software engineering practices for ML-based software development. We identified 16 primary studies highlighting the challenges scientists face in lacking software engineering training when developing ML-based software. The results emphasize the importance of documentation, standardized processes, and skills acquisition to overcome these challenges effectively in the AI era."
Investigating Developers' Preferences for Learning and Issue Resolution Resources in the ChatGPT Era,"Tayeb, A; Alahmadi, M; Tajik, E; Haiduc, S",10.1109/ICSME58944.2024.00045,2024,"The landscape of software developer learning resources has continuously evolved, with recent trends favoring engaging formats like video tutorials. The emergence of Large Language Models (LLMs) like ChatGPT presents a new learning paradigm. While existing research explores the potential of LLMs in software development and education, their impact on developers' learning and solution-seeking behavior remains unexplored. To address this gap, we conducted a survey targeting software developers and computer science students, gathering 341 responses, of which 268 were completed and analyzed. This study investigates how AI chatbots like ChatGPT have influenced developers' learning preferences when acquiring new skills, exploring technologies, and resolving programming issues. Through quantitative and qualitative analysis, we explore whether AI tools supplement or replace traditional learning resources such as video tutorials, written tutorials, and Q&A forums. Our findings reveal a nuanced view: while video tutorials continue to be highly preferred for their comprehensive coverage, a significant number of respondents view AI chatbots as potential replacements for written tutorials, underscoring a shift towards more interactive and personalized learning experiences. Additionally, AI chatbots are increasingly considered valuable supplements to video tutorials, indicating their growing role in the developers' learning resources. These insights offer valuable directions for educators and the software development community by shedding light on the evolving preferences toward learning resources in the era of ChatGPT."
Augmenting Human Teams with Robots in Knowledge Work Settings: Insights from the Literature,"Ren, YQ; Clement, J",10.1145/3649884,2024,"Recent developments in large language models open doors for Artificial Intelligence and robots to augment knowledge workers and teams in a variety of domains, such as customer service, data science, legal work, and software development. In this article, we review 317 articles from multiple disciplines and summarize the insights in a theoretical framework linking key robot attributes to human perceptions and behaviors. The robot attributes include embodiment, nonverbal and verbal communication, perceived gender and race, emotions, perceived personality, and competence. The outcomes include human perceptions, acceptance, engagement, compliance, trust, and willingness to help. We identify four differences between one human and one robot settings and team settings and use them as the springboard to generalize insights from the literature review to the design and impact of a robot in assisting humans in knowledge work teams. We report two high-level observations around the interplay among robot attributes and context dependent designs and discuss their implications."
Boosting self-repair workflow with brainstorming for code generation,"Jin, ZM; Shi, ZX",10.1109/ATS64447.2024.10915518,2024,"Utilizing large language models (LLMs) for code generation has greatly enhanced software development. However, despite the advances in automated code generation frameworks that incorporate self-repair strategies, successful outcomes are not always guaranteed. This led us to explore a different approach to improving code quality. So we implemented a brainstorm-select-repair framework. For a natural language query, our framework first generates multiple foundational code snippets. Then these snippets are tested on test cases, and a text-similarity-based algorithm is used to identify the most accurate code. If none of the generated code snippets successfully fulfills the test cases, our proposed self-repair strategy is used to rectify any flawed code snippets until a code snippet that can pass the test case is identified and then provided to the user. Based on ChatGPT3, our framework reached 89% Pass@1 on HumanEval dataset (at least 3% higher than ChatGPT-4 the best model to date on this dataset). Our framework also surpasses state-of-the-art methods and delivers superior performance on the HumanEval-ET, MBPP, and MBPP-ET datasets. To further validate our design rationale, we conducted comprehensive experiments and analyzed the impact of each component."
Shaping the Future: A Cross-Disciplinary Journey in Design and Technology Integration,"Socher, G; Weisser, T",10.1109/EDUCON60312.2024.10578655,2024,"In today's world, where digital technologies and artificial intelligence (AI) are everywhere, the education sector is challenged with updating its course formats to stay relevant and future-focused. This paper discusses an innovative educational approach in the Informatics and Design program, combining computer science elements with design education. The program differs from traditional computer science curriculums. It caters to students with heterogeneous levels of tech skills and emphasizes a balanced approach to teach both, design and software development. Central to this program is a unique project co-taught by a computer scientist and a service designer. The project centers on creating a chatbot, incorporating the latest in Natural Language Processing and Large Language Models. This initiative not only keeps pace with the latest tech developments but also stresses the importance of a human-centered design approach in merging design with technology. The outcomes of this educational approach demonstrate that when design and technology are taught in tandem, it leads to a more comprehensive understanding and skill set among students. This approach not only prepares them for the current landscape of digital technology but also instills a mindset of continuous adaptation and learning, which is crucial in the ever-evolving field of informatics and design."
"Developing with Compliance in Mind: Addressing Data Protection Law, Cybersecurity Regulation, and AI Regulation During Software Development","Juliussen, BA; Rui, JP; Johansen, D",10.1007/978-3-031-57978-3_6,2024,"This paper explores the concept of complying with relevant legal requirements when developing software systems. Specifically, it focuses on data protection law, cybersecurity regulation, and Artificial Intelligence (AI) regulation requirements in the software system development processes. The paper analyses the impact of three key regulatory frameworks in the European Union: the General Data Protection Regulation (GDPR), the Network and Information Security (NIS) 2 Directive, and the proposed Artificial Intelligence Act (AIA). The article examines the interplay and potential conflicts between different requirements in these rule sets. Towards the end of the paper, some suggestions are made for achieving alignment with these regulations in software systems, enabling concurrent compliance with the GDPR, the NIS 2 Directive, and the AIA, in situations where all the regulations enter into effect simultaneously."
AI-SPRINT: Design and Runtime Framework for Accelerating the Development of AI Applications in the Computing Continuum,"Lattari, F; Matteucci, M; Ardagna, D",10.1007/978-3-031-57931-8_17,2024,"Artificial Intelligence (AI) and edge computing have recently emerged as major trends in the ICT industry. Enterprise applications increasingly make intensive use of AI technologies and are often based on multiple components running across a computing continuum. However, the heterogeneity of the technologies and software development solutions in use are evolving quickly and are still a challenge for researchers and practitioners. Indeed, lack of solutions tailored for AI applications is observed in the areas of applications placement and design space exploration with performance guarantees, both under-developed. The aim of the AI-SPRINT Artificial Intelligence in Secure PRIvacy-preserving computing coNTinuum project is to develop a framework composed of design and runtime management tools to seamlessly design, partition and operate Artificial Intelligence (AI) applications among the current plethora of cloud-based solutions and AI-based sensor devices (i.e., devices with intelligence and data processing capabilities), providing resource efficiency, performance, data privacy, and security guarantees. AI-SPRINT is intended to accelerate the development of AI applications, whose components are spread across the edge-cloud computing continuum, while allowing trading-off application performance and AI models accuracy. This is accomplished by the thorough suite of design tools provided by the AI-SPRINT framework, which exposes a set of programming abstractions with the goal of hiding as much as possible the computing continuum complexity, while further providing a simple interface to define desired constraints upon which the application design is guided."
New Approaches to Automated Software Testing Based on Artificial Intelligence,"Zhang, YL",10.1109/ICAICE63571.2024.10863866,2024,"With the increasing complexity of software development, traditional automated testing methods face significant challenges in efficiency and coverage. This paper proposes a novel AI-driven automated testing method aimed at optimizing test case generation and selection through machine learning and deep learning techniques. Experimental results demonstrate that this approach significantly enhances test coverage and accuracy, addressing limitations inherent in existing methods. This research provides new insights for automation and intelligence in the field of software testing, advancing the application of artificial intelligence in software engineering."
Creating UML Class Diagrams with General-Purpose LLMs,"Shehata, M; Lepore, B; Cummings, H; Parra, E",10.1109/VISSOFT64034.2024.00031,2024,"General-purpose large language models (LLMs) have become a versatile tool in software development and maintenance. These models offer support in tasks such as understanding, writing, and summarizing code. While these LLMs can generate code quickly their use in software modeling is relatively under-explored. This research aims to study ChatGPT's ability to generate class diagrams from source code. To do this, we engineered a prompt that takes in source code to create a UML class diagram for that system. We used this prompt to create class diagrams for 40 systems and assessed the diagrams on their correctness and structure. Our results show that ChatGPT creates class diagrams that accurately capture 90% of the classes and their attributes and 66% of the associations. While ChatGPT performed nearly flawlessly for smaller projects, the diagrams for larger projects had more issues. We conclude that ChatGPT is best utilized as a complementary tool rather than the sole resource for software modeling and maintenance."
Ensemble Balanced Nested Dichotomy Fuzzy Models for Software Requirement Risk Prediction,"Kumar, G; Imam, AA; Basri, S; Hashim, AS; Naim, AGH; Capretz, LF; Balogun, AO; Mamman, H",10.1109/ACCESS.2024.3473942,2024,"Modern software systems are becoming more intricate, making identification of risks in the software requirement phase- a fundamental aspect of the software development life cycle (SDLC)-complex. Inadequate risk assessment may result in the malfunction of a software system, either in the development or production phase. Therefore, risk prediction plays a crucial role in software requirements, serving as the first step in any software project. Hence, developing adaptive predictive models that can offer consistent and explainable insights for handling risk prediction is imperative. This study proposes novel ensemble class balanced nested dichotomy (EBND) fuzzy induction models for risk prediction in software requirement. Specifically, the proposed EBND models employ a hierarchical structure consisting of binary trees featuring distinct nested dichotomies that are generated randomly for each tree. Thereafter, we use an ensemble principle to refine rules generated from the resulting binary tree. The predictive efficacy of the suggested EBND models is further extended by introducing a data sampling method into their prediction process. The inclusion of the data sampling method acts to mitigate the underlying disparity in the class labels that may affect its prediction processes. The efficacy of the EBND models is then evaluated and compared to current solutions using the open-source software risk dataset. The observed findings revealed that the EBND models demonstrated superior predictive capabilities when compared to the conventional models and state-of-the-art methodologies. Specifically, the EBND models achieved an average accuracy threshold value of 98%, as well as high values for the f-measure metric."
METHOD OF IMPERATIVE VARIABLES FOR SEARCH AUTOMATION OF TEXTUAL CONTENT IN UNSTRUCTURED DOCUMENTS,"Boiko, VO",10.15588/1607-3274-2024-2-12,2024,"Context. Currently, there are a lot of approaches that are used for textual search. Nowadays, methods such as pattern-matching and optical character recognition are highly used for retrieving preferred information from documents with proven effectiveness. However, they work with a common or predictive document structure, while unstructured documents are neglected. The problem - is automating the textual search in documents with unstructured content. The object of the study was to develop a method and implement it into an efficient model for searching the content in unstructured textual information. Objective. The goal of the work is the implementation of a rule-based textual search method and a model for seeking and retrieving information from documents with unstructured text content. Method. To achieve the purpose of the research, the method of rule-based textual search in heterogenous content was developed and applied in the appropriately designed model. It is based on natural language processing that has been improved in recent years along with a new generative artificial intelligence becoming more available. Results. The method has been implemented in a designed model that represents a pattern or a framework of unstructured textual search for software engineers. The application programming interface has been implemented. Conclusions. The conducted experiments have confirmed the proposed software's operability and allow recommendations for use in practice for solving the problems of textual search in unstructured documents. The prospects for further research may include the improvement of the performance using multithreading or parallelization for large textual documents along with the optimization approaches to minimize the impact of OpenAI application programming interface content processing limitations. Furthermore, additional investigation might incorporate extending the area of imperative variables usage in programming and software development."
Bridging the Gap between Source Code and Requirements Using GPT (Student Abstract),"Xu, RY; Xu, ZY; Li, GX; Sheng, VS",,2024,"Reverse engineering involves analyzing the design, architecture, and functionality of systems, and is crucial for legacy systems. Legacy systems are outdated software systems that are still in use and often lack proper documentation, which makes their maintenance and evolution challenging. To address this, we introduce SC2Req, utilizing the Generative Pre-trained Transformer (GPT) for automated code analysis and requirement generation. This approach aims to convert source code into understandable requirements and bridge the gap between those two. Through experiments on diverse software projects, SC2Req shows the potential to enhance the accuracy and efficiency of the translation process. This approach not only facilitates faster software development and easier maintenance of legacy systems but also lays a strong foundation for future research, promoting better understanding and communication in software development."
FRINGE: context-aware FaiRness engineerING in complex software systEms,"Palomba, F; Di Sorbo, A; Di Ruscio, D; Ferrucci, F; Catolino, G; Giordano, G; Di Dario, D; Voria, G; Pentangelo, V; Tortorella, M; Sgueglia, A; Di Sipio, C; D'Aloisio, G; Di Marco, A",10.1145/3674805.3695394,2024,"Machine learning (ML) is essential in modern technology, driving complex data-driven decisions. By 2025, daily data generation will exceed 463 exabytes, increasing ML's influence and ethical risks of data exploitation and discrimination. The European Union's Artificial Intelligence Act highlights the need for ethical AI solutions. Project FRINGE (context-aware FaiRness engineerING in complex software systEms) addresses software fairness in ML-intensive systems that collect data through interconnected devices. Fringe aims to provide software engineers, data scientists, and ML experts with methodologies and software engineering solutions to improve fairness in ML systems. The goals of the project include developing a metamodel for ML fairness, a fairness-aware monitoring infrastructure, contextual solutions for identifying fairness issues, and automated recommendation systems to design fairness properties throughout the software development lifecycle."
Investigating BERT Layer Performance and SMOTE Through MLP-Driven Ablation on Gittercom,"Akash, BS; Singh, V; Krishna, A; Murthy, LB; Kumar, L",,2024,"For software development teams, communication is necessary to preserve growth consciousness, streamline the management of projects, and avoid misunderstandings. Among the features that chat rooms offer to help and satisfy the interaction requirements of software-development groups are instant personal messaging, group conversations, and code knowledge exchange. This is all capable of occurring in real time. As a result, developers are increasingly using chat rooms. One of these prominent forums is Gitter, and the chats it includes might be a goldmine of information for researchers studying open-source software systems. The GitterCom dataset, the biggest repository of Gitter developer messages that have been meticulously labeled and organized, was used in this study to conduct a multi-label categorization for the dataset's Purpose Category. 9 MLP machine learning classifiers, six feature selection methods, and the layered architecture of the BERT transformer are all subjected to thorough empirical research and evaluation. As a consequence, our research process shows competent results with a maximum AUC score of 0.97 with MLP variants using Adam optimizer(MLP2 and MLP3). Additionally, the research process might be used to text data from software development forums for general multi-label text categorization. The insights of the research, which give a holistic understanding of the Machine learning pipeline driven by BERT, shall serve the research community for preferential selection of Feature selection techniques, BERT layers, and classification model selection, among others for text classification."
Text-To-Text Generation for Issue Report Classification,"Rejithkumar, G; Anish, PR; Ghaisas, S",10.1145/3643787.3648042,2024,"We present our participation in the issue report classification tool competition at the 3rd International Workshop on Natural Language-based Software Engineering. Given the substantial influx of issue reports in large scale software development each day, there is a pronounced need for a system capable of automatically classifying issue reports into categories such as bugs, enhancements, and features. We propose a text-to-text generation-based supervised approach for issue report classification. We fine-tuned and evaluated our approach on the provided dataset of 3000 labeled issue reports (as bugs, enhancements, and questions). Our approach yielded an average cross-repo F1-score of 0.8297 across all classes, which is comparable to the SetFit baseline of 0.8270."
Analyzing the Correlation Between Toxic Comments and Code Quality,"Sayago-Heredia, J; Sailema, GC; PÃ©rez-Castillo, R; Piattini, M",10.1002/smr.2739,2025,"Software development has a relevant human side, and this could, for example, imply that developers' feelings have an impact on certain aspects of software development such as quality, productivity, or performance. This paper explores the effects of toxic emotions on code quality and presents the SentiQ tool, which gathers and analyzes sentiments from commit messages (obtained from GitHub) and code quality measures (obtained from SonarQube). The SentiQ tool we proposed performs a sentiment analysis (based on natural language processing techniques) and relates the results to the code quality measures. The datasets extracted are then used as the basis on which to conduct a preliminary case study, which demonstrates that there is a relationship between toxic comments and code quality that may affect the quality of the whole software project. This has resulted in the drafting of a predictive model to validate the correlation of the impact of toxic comments on code quality. The main implication of this work is that these results could, in the future, make it possible to estimate code quality as a function of developers' toxic comments."
sAIfe: Towards a Lightweight Threat Modeling Approach to Support Machine Learning Application Development,"Messas, GE; Miani, RS; Zarpelao, BB",10.1145/3701625.3701640,2024,"With the growing popularization of the Artificial Intelligence (AI) field, the development of systems that rely on, at least, one of its subareas has also experienced a great increase. The recent adoption of AI techniques in common systems - such as mobile apps and household appliances - requires a higher level of attention, to ensure their safety and proper operation. In this scenario, assuring the adequate functioning of these solutions culminates, in most cases, in ensuring the security of the application and its data throughout the software development life cycle. Software developers, however, often find security-related tasks challenging to learn and execute, and frequently put them aside. Additionally, currently available threat modeling frameworks are difficult to integrate into software development life cycles, which prioritize agility and automation over extensive analysis and documentation. This work, therefore, proposes sAIfe, a new threat modeling method for security analysis of AI applications under development. By providing prescriptive steps, a graphical reference element and ready-made remediation suggestions, this approach aims at simplifying the risk assessment process for the programmer, unveiling possible weaknesses and suggesting respective solutions in a practical way. sAIfe is tested on a real-world AI application, revealing positive results, with many potential issues and mitigation options detected by the method, which are registered in the form of a case study. Finally, a validation with developers in academia is also carried out, returning great feedback on sAIfe's ease of use and speed of application."
Research on the Application of Clustering Algorithm for Online Education Students Based on Artificial Intelligence,"Liu, BZ",10.1145/3677182.3677199,2024,"The aim of this study is to explore the application of artificial intelligence in clustering algorithms for student data in the field of online education. Clustering analysis, as a data mining technique, effectively supports the discovery of student behavior patterns. Firstly, the principle of clustering analysis is elaborated, and by listing the applicable data types, the implementation principles and characteristics of K-means clustering algorithm and grid clustering algorithm are thoroughly analyzed. To enhance the accuracy of clustering, the GBKM clustering algorithm is introduced, which combines the advantages of Bayesian theory to improve the efficiency and accuracy of clustering. Subsequently, a design of an AI-based online education platform is described, outlining the software development model and system functionality design of the platform. Through time complexity analysis and experimental result comparison, the performance differences between the GBKM algorithm and traditional grid clustering and K-means clustering algorithms are tested, further confirming the superiority of GBKM algorithm in clustering students for online education and revealing the potential application value of AI technology in personalized teaching resource recommendation systems."
A Transformer-based Approach for Augmenting Software Engineering Chatbots Datasets,"Abdellatif, A; Badran, K; Costa, DE; Shihab, E",10.1145/3674805.3686695,2024,"Background: The adoption of chatbots into software development tasks has become increasingly popular among practitioners, driven by the advantages of cost reduction and acceleration of the software development process. Chatbots understand users' queries through the Natural Language Understanding component (NLU). To yield reasonable performance, NLUs have to be trained with extensive, high-quality datasets, that express a multitude of ways users may interact with chatbots. However, previous studies show that creating a high-quality training dataset for software engineering chatbots is expensive in terms of both resources and time. Aims: Therefore, in this paper, we present an automated transformer-based approach to augment software engineering chatbot datasets. Method: Our approach combines traditional natural language processing techniques with the BART transformer to augment a dataset by generating queries through synonym replacement and paraphrasing. We evaluate the impact of using the augmentation approach on the Rasa NLU's performance using three software engineering datasets. Results: Overall, the augmentation approach shows promising results in improving the Rasa's performance, augmenting queries with varying sentence structures while preserving their original semantics. Furthermore, it increases Rasa's confidence in its intent classification for the correctly classified intents. Conclusions: We believe that our study helps practitioners improve the performance of their chatbots and guides future research to propose augmentation techniques for SE chatbots."
Enabling AI-Driven Customer Experiences in Fashion E-Commerce through an End-to-End ML Software Development Framework,"Bahuleyan, HP; Puzikov, Y; Koriagin, E; Lasserre, J; Weffer, R; Shirvany, R",10.1145/3708635.3708636,2024,"Artificial Intelligence (AI) and Machine Learning (ML) solutions are transforming fashion e-commerce by enhancing various aspects of the customer journey, such as product recommendations, virtual try-ons, and size and fit assistance. While there are many innovative AI-driven solutions being developed, not all of them successfully make it to the market as customer-facing products. Key underlying reasons for this are the uncertainty around the infrastructure needs, customer experience requirements, and a lack of systematic alignment between algorithmic and business metrics. These challenges are significant in the context of fashion e-commerce due to the dynamic nature of the industry and the importance of personalized customer experiences. To address these challenges in a scalable and repeatable manner, it is critical to establish ML software development processes and frameworks that can be applied consistently across projects. In this work, we explore these challenges in detail and provide insights and recommendations based on real-world experiences. We propose a comprehensive framework that outlines a structured approach to taking AI/ML models from initial experimental projects to live customer-facing software products at a scale of millions of customers, where multidisciplinary teams with expertise in data science, software engineering, business strategy, and product development collaborate."
User Story Automation in Software Engineering: insights from the literature,"DomÃ­nguez, FAM; Quintana, MA; Cinco, RRP; Borrego, G; GonzÃ¡lez-LÃ³pez, S",10.1109/CONISOFT63288.2024.00044,2024,"User stories are widely regarded as an effective method for articulating software requirements, especially within agile development. However, the brevity often encouraged in user stories can lead to a lack of detail, resulting in miscommunication and inefficiencies during development. A significant challenge is the manual extraction of user stories from unstructured text, which is labor-intensive and prone to errors. Automated extraction using Natural Language Processing (NLP) and Machine Learning offers a promising solution. These technologies can identify and extract user roles, actions, and benefits from unstructured text, reducing the reliance on manual intervention and ensuring consistent quality. We conducted a literature review spanning the past 20 years, from 2004 to 2024, on which we identified 77 relevant articles, with only 10 focusing on the extraction of user stories using NLP techniques. These studies emphasize the need for automation in user story extraction to improve the efficiency and accuracy of requirements gathering. The findings highlight a gap in current research regarding automated extraction methods and the importance of addressing this through advanced NLP techniques. Implementing such solutions can transform the handling of user stories, making the requirements elicitation process more efficient and less error-prone, ultimately contributing to the success of agile software development projects."
Software Requirement Smells and Detection Techniques: A Systematic Literature Review,"Alemneh, E; Berhanu, F",10.2478/cait-2024-0037,2024,"One of the major reasons for software project failure is poor requirements, so numerous requirement smells detection solutions are proposed. Critical appraisal of the proposed requirement fault detection methods is crucial for refining knowledge of requirement smells and developing new research ideas. The objective of this paper was to systematically review studies that focused on detecting requirement discrepancies in textual requirements. After applying inclusion and exclusion criteria and forward and backward snowball sampling techniques using database-specific search queries, 19 primary studies were selected. A deep analysis of the studies shows that classical NLP-based requirement smells detection techniques are the most commonly used ones and ambiguity is the requirement smell that has the utmost attention. Further investigation depicts the scarcity of open-access datasets, and tools employed to detect requirement faults. The review has also revealed there is no comprehensive definition and classification of requirement smells."
How I Learned to StopWorrying and Love ChatGPT,"Przymus, P; Fejzer, M; Narebski, J; Stencel, K",10.1145/3643991.3645073,2024,"In the dynamic landscape of software engineering, the emergence of ChatGPT-generated code signifies a distinctive and evolving paradigm in development practices. We delve into the impact of interactions with ChatGPT on the software development process, specifically analysing its influence on source code changes. Our emphasis lies in aligning code with ChatGPT conversations, separately analysing the user-provided context of the code and the extent to which the resulting code has been influenced by ChatGPT. Additionally, employing survival analysis techniques, we examine the longevity of ChatGPT-generated code segments in comparison to lines written traditionally. The goal is to provide valuable insights into the transformative role of ChatGPT in software development, illuminating its implications for code evolution and sustainability within the ecosystem."
Generative Artificial Intelligence for Test-Driven Development: GAI4-TDD,"Cassieri, P; Romano, S; Scanniello, G",10.1109/SANER60148.2024.00098,2024,"Test-Driven Development (TDD) is an agile software development approach. It promotes short cycles, composed of three phases each, to incrementally implement software functionality: Red, Green, and Refactor. In the Red Phase, the TDD developer writes a unit test for a small chunk of functionality not yet implemented and watches the test fail. In the Green Phase, the TDD developer writes production code to make the test pass as quickly as possible. Finally, in the refactor phase, the TDD developer cleans written code. In this demo paper, we present GAI4-TDD (Generative Artificial Intelligence for Test-Driven Development), a PyCharm plugin to support the Green Phase of TDD. Specifically, this plugin generates production code from the tests the TDD developer had written in the Red Phase so that the generated code makes these tests pass. Previous studies have shown that TDD can be beneficial not only in the traditional software development context but also in the Embedded Systems (ESs) development context. We preliminary assessed GAI4-TDD in the latter context through a laboratory empirical study on three ESs. We observed that in the greater part of the cases, GAI4-TDD generates production code that successfully implements the chunk of functionality required. A video showing GAI4-TDD in action is available at: https://youtu.be/3oIDb-6NgS4."
Quality 4.0 in Software Engineering: Incorporating Scaled Agile Insights to a Framework Proposal,"Bankoff, KP; MuÃ±oz, R; Pasini, A; Pesado, P",10.1007/978-3-031-62245-8_13,2024,"Software Engineering is a constantly evolving discipline that is present in all aspects of daily life. The quality of software has been addressed for a long time, emerging from the same principles of quality in the manufacturing industry but adapted to the characteristics of the development of intangible products. The industry has evolved towards a digital revolution that implies the use of emerging technologies such as the Internet of Things (IoT), artificial intelligence, and machine learning. Quality 4.0 adopts the principles of Industry 4.0 and applies them to quality management with an approach based on big data analysis, realtime monitoring, and a greater focus on the customer through the use of communication and collaboration tools. A framework is proposed that integrates different processes, tools, and techniques of Software Engineering with the objective of enabling the implementation of Quality 4.0 principles in software development, taking DevOps as an enabler as it is an approach that promotes automation and the integration of tools. Additionally, we have expanded our analysis to include the Scaled Agile Framework (SAFe), examining how our proposed framework aligns with SAFe's requirements, thus providing a holistic approach to implement Quality 4.0 in software development projects."
Prototype of Domain-Specific Language for Uniform Access to Vector Databases,"Akik, E; Vjestica, M; Dimitrieski, V; Celikovic, M; Ristic, S",10.1109/Informatics62280.2024.10900871,2024,"Vector databases efficiently gather, retrieve, and evaluate data in vector form, complementing and enhancing Artificial Intelligence (AI) applications. The utilization of programming languages is typically necessitated for accessing these databases, requiring users to possess programming skills, thus hindering the user experience. Additionally, challenges are encountered due to differences in the syntax used among vector databases of different vendors, making querying and migration between diverse vector databases difficult. The main goal of the research presented in this paper is to provide users with straightforward use and uniform access to vector databases of various vendors. To achieve such a goal, a prototype of Domain-Specific Language (DSL) is developed, with the aim to overcome challenges in accessing vector databases and to offer users the benefits of simplified interaction with vector databases."
Engaging with AI: An Exploratory Study on Developers' Sharing and Reactions to ChatGPT in GitHub Pull Requests,"Hao, H; Tian, Y",10.1145/3691621.3694946,2024,"ChatGPT, as a representative Foundation Model (FM)-powered tool, has demonstrated significant potential in assisting developers with various software engineering tasks, such as code generation, program repair, and test creation. However, the timing of developers seeking assistance from ChatGPT and their perceptions of ChatGPT-generated content remain underexplored. In this paper, we analyze a dataset comprising 211 developers' shared conversations with ChatGPT within GitHub Pull Requests (PRs). Our study investigates the events in the GitHub PR timeline that precede these shared conversations, the sentiments expressed by developers when sharing these conversations, and the reactions from other developers to PR comments and descriptions that include shared conversations with ChatGPT. Our key findings are: (1) Shared conversations with ChatGPT are posted after seven distinct types of pull request timeline events, with the most frequent being comments added, PR creation, and review requests. (2) Positive sentiment is the most prevalent among developers when sharing these conversations, followed by neutral and negative sentiments. Developer reactions to comments and PR descriptions containing shared conversations are generally sparse; when they do occur, the most common reactions are (thumbs up), (sic) (heart), and (sic) (eyes). These findings provide new insights into how developers incorporate FM-powered tools into their collaborative software development workflows."
A Survey on Data Quality Dimensions and Tools for Machine Learning,"Zhou, YH; Tu, FJ; Sha, KW; Ding, JH; Chen, HH",10.1109/AITest62860.2024.00023,2024,"Machine learning (ML) technologies have become substantial in practically all aspects of our society, and data quality (DQ) is critical for the performance, fairness, robustness, safety, and scalability of ML models. With the large and complex data in data-centric AI, traditional methods like exploratory data analysis (EDA) and cross-validation (CV) face challenges, highlighting the importance of mastering DQ tools. In this survey, we review 17 DQ evaluation and improvement tools in the last 5 years. By introducing the DQ dimensions, metrics, and main functions embedded in these tools, we compare their strengths and limitations and propose a roadmap for developing open-source DQ tools for ML. Based on the discussions on the challenges and emerging trends, we further highlight the potential applications of large language models (LLMs) and generative AI in DQ evaluation and improvement for ML. We believe this comprehensive survey can enhance understanding of DQ in ML and could drive progress in data-centric AI. A complete list of the literature investigated in this survey is available on GitHub at: https://github.com/haihua0913/awesome-dq4ml."
Empirical Analysis of Multi-label Classification on GitterCom Using BERT and ML Classifiers,"Akash, BS; Kumar, L; Singh, V; Patel, AK; Krishna, A",10.1007/978-981-99-8073-4_19,2024,"To maintain development consciousness, simplify project coordination, and prevent misinterpretation, communication is essential for software development teams. Instant private messaging, group chats, and sharing code are just a few of the capabilities that chat rooms provide to assist and meet the communication demands of software development teams. All of this is capacitated to happen in real-time. Consequently, chat rooms have gained popularity among developers. Gitter is one of these platforms that has gained popularity, and the conversations it contains may be a treasure trove of data for academics researching open-source software systems. This research made use of the GitterCom dataset, The largest collection of Gitter developer messages that have been carefully labelled and curated and perform multi-label classification for the 'Purpose' category in the dataset. An extensive empirical analysis is performed on 6 feature selection techniques, 14 machine learning classifiers, and BERT transformer layer architecture with layer-by-layer comparison. Consequently, we achieve proficient results through our research pipeline involving Extra Trees Classifier and Random Forest classifiers with AUC (OvR) median performance of 0.94 and 0.92 respectively. Furthermore, The research proposed research pipeline could be utilized for generic multi-label text classification on software developer forum text data."
Natural Language-Oriented Programming (NLOP): Towards Democratizing Software Creation,"Beheshti, A",10.1109/SSE62657.2024.00047,2024,"As generative Artificial Intelligence (AI) technologies evolve, they offer unprecedented potential to automate and enhance various tasks, including coding. Natural Language-Oriented Programming (NLOP), a vision introduced in this paper, harnesses this potential by allowing developers to articulate software requirements and logic in their natural language, thereby democratizing software creation. This approach streamlines the development process and significantly lowers the barrier to entry for software engineering, making it feasible for non-experts to contribute effectively to software projects. By simplifying the transition from concept to code, NLOP can accelerate development cycles, enhance collaborative efforts, and reduce misunderstandings in requirement specifications. This paper reviews various programming models, assesses their contributions and limitations, and highlights that natural language will be the new programming language. Through this comparison, we illustrate how NLOP stands to transform the landscape of software engineering by fostering greater inclusivity and innovation."
Twisted Games: A First Experience of Inclusion of AI tools in First Year Programming Classes,"Wightman, P",10.1109/LA-CCI62337.2024.10814750,2024,"The recent rapid advancements in Large Language Models (LLMs) and the increasing availability of AI-powered tools have underscored the need for the current generation of programmers to learn how to effectively collaborate with AI from the early stages of their university education. This paper explores the integration of AI tools into a first-year programming course through the implementation of modified classic games (4 in a row with L-shapes, 3-player Battleship, etc.). The primary objective of this study was to assess the impact of AI assistance on students' ability to define and adapt requirements for novel software applications, while also fostering an understanding of the power and limitations of AI in the classroom. The results reveal a positive student experience, with participants reporting increased confidence in utilizing AI tools for requirements elicitation and recognizing the potential benefits for their future careers. In addition, it highlighted the need to train students in developing skills for requirement identification, prompt creation, and testing and debugging the code."
DNA Virus Detection System Based on RPA-CRISPR/ Cas12a-SPM and Deep Learning,"Liu, CY; Lei, ZY; Lian, LJ; Zhang, LK; Du, ZC; Qin, PW",10.3791/64833,2024,"We report a fast, easy-to-implement, highly sensitive, sequence-specific, and pointof-care (POC) DNA virus detection system, which combines recombinase polymerase amplification (RPA) and CRISPR/Cas12a system for trace detection of DNA viruses. Target DNA is amplified and recognized by RPA and CRISPR/Cas12a separately, which triggers the collateral cleavage activity of Cas12a that cleaves a fluorophorequencher labeled DNA reporter and generalizes fluorescence. For POC detection, portable smartphone microscopy is built to take fluorescent images. Besides, deep learning models for binary classification of positive or negative samples, achieving high accuracy, are deployed within the system. Frog virus 3 (FV3, genera Ranavirus, family Iridoviridae) was tested as an example for this DNA virus POC detection system, and the limits of detection (LoD) can achieve 10 aM within 40 min. Without skilled operators and bulky instruments, the portable and miniature RPA-CRISPR/Cas12a-SPM with artificial intelligence (AI) assisted classification shows great potential for POC DNA virus detection and can help prevent the spread of such viruses."
Clinical application of convolutional neural network lung nodule detection software: An Australian quaternary hospital experience,"Mark, P; Papalia, I; Lai, JKC; Pascoe, DM",10.1111/1754-9485.13734,2024,"Introduction: Early-stage lung cancer diagnosis through detection of nodules on computed tomography (CT) remains integral to patient survivorship, promoting national screening programmes and diagnostic tools using artificial intelligence (AI) convolutional neural networks (CNN); the software of AI-Rad Companion (TM) (AIRC), capable of self-optimising feature recognition. This study aims to demonstrate the practical value of AI-based lung nodule detection in a clinical setting; a limited body of research. Methods: One hundred and eighty-three non-contrast CT chest studies from a single centre were assessed for AIRC software analysis. Prospectively collected data from AIRC detection and characterisation of lung nodules (size: >= 3 mm) were assessed against the reference standard; reported findings of a blinded consultant radiologist. Results: One hundred and sixty-seven CT chest studies were included; 52% indicated for nodule or lung cancer surveillance. Of 289 lung nodules, 219 (75.8%) nodules (mean size: 10.1 mm) were detected by both modalities, 28 (9.7%) were detected by AIRC alone and 42 (14.5%) by radiologist alone. Solid nodules missed by AIRC were larger than those missed by radiologist (11.5 mm vs 4.7 mm, P < 0.001). AIRC software sensitivity was 87.3%, with significant false positive and negative rates demonstrating 12.5% specificity (PPV 0.6, NPV 0.4). Conclusion: In a population of high nodule prevalence, AIRC lung nodule detection software demonstrates sensitivity comparable to that of consultant radiologist. The clinical significance of larger sized nodules missed by AIRC software presents a barrier to current integration in practice. We consider this research highly relevant in providing focus for ongoing software development, potentiating the future success of AI-based tools within diagnostic radiology."
A MULTI-FIDELITY APPROACH TO TESTING AND EVALUATION OF AI-ENABLED SYSTEMS,"Seif, RJ; Yang, ZC; Wang, ZR; Freeman, L; Panchal, JH",,2024,"As Artificial Intelligence (AI) and Machine Learning (ML) technologies advance, the need for methods to test them also increases. Several methods for testing ML and AI systems have recently been developed by the ML/AI and software development communities. These methods assume that the team that is developing the system is also responsible for testing the system, therefore, they have access to the datasets on which the ML models were trained, and the knowledge of the environment in which the system is expected to operate. However, this is not true in certain situations such as in the Department of Defense (DoD) acquisition where the systems are developed by other organizations, due to which, existing methods for Test and Evaluation (T&E) of AI-enabled systems are not adequate for DoD acquisition. To address this gap we propose a multi-fidelity approach for testing and evaluation that consists of (i) a representation of the model space with dimensions along which different fidelities of models can be developed, and (ii) a method to integrate multiple fidelities for continuous T&E of AI-enabled systems. The approach is illustrated using an example of a visual perception system in an autonomous vehicle (AV) use case, where a simulation space across different fidelities is constructed to test how well the system meets the listed requirements. A model space is first identified, in which models are characterized for their cost and performance. A method to generate test plans is then devised to maximize the utility across the span of given system requirements. We illustrate how the proposed approach can be used to develop test combinations that minimize the cost and maximize the utility under a set of system requirements to be tested."
Revolutionizing Software Project Development: A CNN-LSTM Hybrid Model for Effective Defect Prediction,"Jose, GS; Charles, J",,2024,"Within the domain of software development, the practice of software defect prediction (SDP) holds a central and critical position, significantly contributing to the efficiency and ultimate success of projects. It embodies a proactive approach that harnesses data-driven techniques and analytics to preemptively identify potential defects or vulnerabilities within software systems, thereby enhancing overall quality and reliability while significantly impacting project timelines and resource allocation. The efficiency of software development projects hinges on their ability to adhere to deadlines, budget constraints, and deliver high-quality products. SDP contributes to these objectives through various means. This paper introduces a novel SDP model that harnesses the combined capabilities of Convolutional Neural Networks (CNNs) and Long Short Term Memory (LSTMs) unit. CNNs excel at extracting features from structured data, enabling them to discern patterns and dependencies within code repositories and change histories. LSTMs, conversely, excel in handling sequential data, which is pivotal for capturing the temporal aspects of software development and tracking the evolution of defects over time. The outcomes of the proposed CNN-LSTM hybrid model showcase its superior predictive performance. Simulation results affirm the substantial potential of this model to bolster the efficiency and reliability of software development processes. As technology advances and data-driven methodologies become increasingly prevalent in the software industry, the integration of such hybrid models presents a promising avenue for continually elevating software quality and ensuring the triumph of software projects. In summary, the utilization of this innovative SDP model offers a transformative approach to efficient software development, positioning it as a vital tool for project success and quality assurance."
A hybrid-ensemble model for software defect prediction for balanced and imbalanced datasets using AI-based techniques with feature preservation: SMERKP-XGB,"Mustaqeem, M; Siddiqui, T; Mustajab, S",10.1002/smr.2731,2025,"Maintaining software quality is a significant challenge as the complexity of software is increasing with the rise of the software industry. Software defects are a primary concern in complex modules, and predicting them in the early stages of the software development life cycle (SDLC) is difficult. Previous techniques to address this issue have not been very promising. We have proposed A hybrid ensemble model for software defect prediction using AI-based techniques with feature preservation to overcome this problem. We have used the National Aeronautics and Space Administration (NASA) dataset from the PROMISE repository for testing and validation. By applying exploratory data analysis (EDA), feature engineering, scaling, and standardization, we found that the dataset is imbalanced, which can negatively affect the model's performance. To address this, we have used the Synthetic Minority Oversampling (SMOTE) technique and the edited nearest neighbor (ENN) (SMOTE-ENN). We have also used recursive feature elimination cross-validation (RFE-CV) with a pipeline to prevent data leaking in CV and kernel-based principal component analysis (K-PCA) to minimize dimensionality and selectively relevant features. The reduced dimensional data is then given to the eXtreme Gradient Boosting (XGBoost) for classification, resulting in the hybrid-ensemble (SMERKP-XGB) model. The proposed SMERKP-XGB model is better than previously developed models in terms of accuracy (CM1: 97.53%, PC1: 92.05%, and PC2: 97.45%, KC1:95.65%), and area under the receiver operating characteristic curve values (CM1:96.30%, PC1:98.30%, and PC2:99.30%: KC1: 93.54) and other evaluation criteria mentioned in the literature. SMERKP-XGB represents a hybrid-ensemble model for software defect prediction for balanced and imbalanced datasets using artificial intelligence -based techniques with feature preservation.image"
Automated Commit Intelligence by Pre-training,"Liu, SQ; Li, YZ; Xie, XF; Ma, W; Meng, GZ; Li, Y",10.1145/3674731,2024,"GitHub commits, which record the code changes with natural language messages for description, play a critical role in software developers' comprehension of software evolution. Due to their importance in software development, several learning-based works are conducted for GitHub commits, such as commit message generation and security patch identification. However, most existing works focus on customizing specialized neural networks for different tasks. Inspired by the superiority of code pre-trained models, which has confirmed their effectiveness across different downstream tasks, to promote the development of open-source software community, we first collect a large-scale commit benchmark including over 7.99 million commits across 7 programming languages. Based on this benchmark, we present CommitBART, a pre-trained encoder- decoder Transformer model for GitHub commits. The model is pre-trained by three categories (i.e., denoizing objectives, cross-modal generation, and contrastive learning) for six pre-training tasks to learn commit fragment representations. Our model is evaluated on one understanding task and three generation tasks for commits. The comprehensive experiments on these tasks demonstrate that CommitBART significantly outperforms previous pre-trained works for code. Further analysis also reveals that each pre-training task enhances the model performance."
A text mining-based method for domains using AI generative text-to-text,"CanelÃ³n, R; Cruz, F; Barrios, I",10.1109/LA-CCI62337.2024.10814840,2024,"Software reuse is an important principle within a software development process model, that efficiently and effectively manages the different variations that may exist between products of the same family, centered on a domain. However most existing software systems are developed without a process model that clearly defines the disciplines, activities, artifacts, and the actors involved, without considering software development as an engineering discipline with predictable times, cost estimates and obtaining a quality product. An agile model is proposed and evaluated that enables a domain engineering process with the disciplines of analysis, design and implementation using generative artificial intelligence and domain engineering with software production lines to build families of natural language text analysis applications. The study has succeeded in demonstrating the feasibility and effectiveness of software reuse in the text mining domain. By applying domain engineering principles and software production lines, it has been possible to create families of text analysis applications more efficiently and effectively. The results obtained provide new evidence to support the usefulness of this approach in software development."
Enhancing Source Code Comment Generation via Retrieval-Augmented Generation with Design Document Term Dictionary,"Nishikawa, K; Koreki, G; Kanuka, H",10.1109/APSEC65559.2024.00061,2024,"Effective software development depends on clear code comments for better understanding. We introduce a method for generating automated source-code comments using retrieval-augmented generation (RAG) with a design document term dictionary. This method aligns terms and their meanings from design documents with lines of source code, producing comments that clearly reflect the source-code's intent and functionality. Our evaluations on the open-source software iDempiere show significant improvements: context precision increased by 22% and faithfulness by 17% compared with conventional RAG methods These results confirm our method's validity. Therefore, we plan to explore its application to different software contexts in future work."
Requirements Elicitation in the Age of AI: A Tool's Multi-system Journey,"Ahmad, K; Arora, C; Abdelrazek, M; Grundy, J; Vasa, R",10.1007/978-3-031-64182-4_4,2024,"Traditional Requirements Engineering (RE) practices have introduced new tools to elicit and model requirements. Applying these tools to building AI software solutions has raised new issues and challenges. Also, most AIbased software solutions ignore human-centred values and focus on technical aspects. Existing tools and RE methods must extend to consider building more human-centred AI solutions. Recognizing this, we present a novel tool to support requirements elicitation and modelling for human-centred AI software. This paper details the tool's multi-system journey as it was applied across three diverse real-world projects: a mHealth app, a Virtual Reality 360 video enhancer, and a supermarket compliance app. The first two projects were in the later stages of software development, and the third was conducted during the early stages of building the software solution. The tool was used to elicit and model requirements for the three case studies in collaboration with eight experts. The tool helped to understand what requirements must be captured at the initial stages vs later stages in RE for AI (RE4AI)."
PTB-FLA Development Paradigm Adaptation for ChatGPT,"Popovic, M; Popovic, M; Kastelan, I; Djukic, M; Basicevic, I",10.2298/CSIS231224036P,2024,"The Python Testbed for Federated Learning Algorithms (PTB-FLA) is a simple Python FL framework that is targeting edge systems and is by its design easy to use by human ML&AI developers. The original PTB-FLA development paradigm intended for humans consists of the four phases (producing the sequential code, the federated code, the federated code with callbacks, and the PTB-FLA code, respectively), and hence dubbed the four-phases (development) paradigm, was validated in the case study on the logistic regression. In this paper, we adapted the original paradigm into the two new paradigms for ChatGPT, named the adapted four-phases paradigm and the adapted two-phases paradigm, respectively. In tune with its name, the latter consists of two phases (producing the sequential and the PTB-FLA code, respectively). We successfully validated both new paradigms using the same case study on logistic regression that was used for the original paradigm. The results are positive and encouraging as the resulting program codes are of better quality than the codes solely made by humans using the original paradigm."
Towards AI-centric Requirements Engineering for Industrial Systems,"Bashir, S",10.1145/3639478.3639811,2024,"Engineering large-scale industrial systems mandate an effective Requirements Engineering (RE) process. Such systems necessitate RE process optimization to align with standards, infrastructure specifications, and customer expectations. Recently, artificial intelligence (AI) based solutions have been proposed, aiming to enhance the efficiency of requirements management within the RE process. Despite their advanced capabilities, generic AI solutions exhibit limited adaptability within real-world contexts, mainly because of the complexity and specificity inherent to industrial domains. This limitation notably leads to the continued prevalence of manual practices that not only cause the RE process to be heavily dependent on practitioners' experience, making it prone to errors, but also often contributes to project delays and inefficient resource utilization. To address these challenges, this Ph.D. dissertation focuses on two primary directions: i) conduct a comprehensive focus group study with a large-scale industry to determine the requirements evolution process and their inherent challenges and ii) propose AI solutions tailored for industrial case studies to automate and streamline their RE process and optimize the development of large-scale systems. We anticipate that our research will significantly contribute to the RE domain by providing empirically validated insights in the industrial context."
Analysis of cognitive processes applying virtual reality for the teaching-learning of Alternative Dispute Resolution (ADR) in Mexico,"Delgado, MOC; PÃ©rez, MAM; LandÃ­n, CJ; Ibarra, JRSG",10.4995/redu.2024.21804,2024,"Actually, Information and Communication Technologies, the Internet of Things and Artificial Intelligence (AI) are positioned as innovative educational technological tools in the face of new challenges, globalization and a new educational society. However, there is very little implementation in the area of law. The main objective of this article is to analysis of the cognitive processes when a simulator with virtual reality and AI support is implemented in teaching- learning process in a branch of law, which is Alternative Dispute Resolution (ADR) in M & eacute;xico. The methodology used in the statistical analysis is random, documentary theoretical and empirical. The simulator was elaborated under the ADDIE model software development methodology and its programming in UNITY. OCULUS QUEST 2 glasses were used for immersion. The methodology used in the teaching-learning was the Local Theoretical Models (LTM) and the data collection was through questionnaires, interviews, case studies, observation, videos, etc. As a result, it was found that the simulator is useful for presenting scenarios, real ADR cases, a video game whit evaluation and feedback, etc. Likewise, the students of law or ADR can interact and be immersed with the simulator and in this way emotions, cognitive processes, situated, individualized, collaborative and meaningful learning will be stimulated."
Semantic-aware Source Code Modeling,"Ding, YRB",10.1145/3691620.3695605,2024,"Source code modeling represents a promising avenue for automating software development, such as code generation, bug repair, and program analysis. This research direction aims to train deep neural nets to learn the statistical predictability inherent in human-written programs to enhance developer productivity, code quality, and the overall software development life cycle. Although existing code modeling approaches, particularly those underpinned by Transformer-based language models, have demonstrated effectiveness across various software engineering tasks, most of them have directly adopted learning schemes from natural language processing (e.g., data collection and processing, training objectives) to source code, primarily focusing on learning code text and syntax. However, such a direct transplant limits the models' capability to capture deep program semantics, such as code functionality, dependencies, and program states during execution. In this research proposal, we highlight the critical role of program semantics in source code modeling. We propose a range of innovative methodologies to bridge the gap between the text-based language models for large-scale code training and the requirement of deep semantic understanding to assist with software engineering tasks effectively. Furthermore, we showcase the efficacy of the proposed semantic-aware code modeling through a handful of published papers and preliminary results, with motivations to delve deeper into this avenue during doctoral research."
The Origin and Opportunities of Developers' Perceived Code Accountability in Open Source AI Software Development,"Bartsch, SC; Lother, M; Schmidt, JH; Adam, M; Benlian, A",,2024,"Open source (OS) software projects in artificial intelligence (AI), such as TensorFlow and scikit-learn, depend on developers' continuous, voluntary code contributions. However, recent security incidents highlighted substantial risks in such software, requiring examinations of factors motivating developers to continuously contribute high-quality code (i.e., providing secure and reliable code fulfilling its functions). Prior research suggests code accountability (i.e., requirements to explain and justify contributed code) to improve code quality, enforced through external accountability mechanisms such as sanctions and rewards. However, the OS domain often lacks such mechanisms, questioning whether and how code accountability arises in this domain and how it affects code contributions. To address these questions, we conducted 26 semi-structured interviews with developers contributing to OS AI software projects. Our findings reveal that despite the absence of external accountability mechanisms, system-, project-, and individual-related factors evoke developers' perceived code accountability. Notably, we discovered a trade-off as high perceived code accountability is associated with higher code quality but discourages developers from participating in OS AI software projects. Overall, this study contributes to understanding the nuanced roles of perceived code accountability in continuously contributing high-quality code without external accountability mechanisms and highlights the complex trade-offs developers face in OS AI software projects."
An empirical study on developers' shared conversations with ChatGPT in GitHub pull requests and issues,"Hao, HZ; Hasan, KA; Qin, H; Macedo, M; Tian, Y; Ding, SHH; Hassan, AE",10.1007/s10664-024-10540-x,2024,"ChatGPT has significantly impacted software development practices, providing substantial assistance to developers in various tasks, including coding, testing, and debugging. Despite its widespread adoption, the impact of ChatGPT as an assistant in collaborative coding remains largely unexplored. In this paper, we analyze a dataset of 210 and 370 developers' shared conversations with ChatGPT in GitHub pull requests (PRs) and issues. We manually examined the content of the conversations and characterized the dynamics of the sharing behavior, i.e., understanding the rationale behind the sharing, identifying the locations where the conversations were shared, and determining the roles of the developers who shared them. Our main observations are: (1) Developers seek ChatGPT's assistance across 16 types of software engineering inquiries. In both conversations shared in PRs and issues, the most frequently encountered inquiry categories include code generation, conceptual questions, how-to guides, issue resolution, and code review. (2) Developers frequently engage with ChatGPT via multi-turn conversations where each prompt can fulfill various roles, such as unveiling initial or new tasks, iterative follow-up, and prompt refinement. Multi-turn conversations account for 33.2% of the conversations shared in PRs and 36.9% in issues. (3) In collaborative coding, developers leverage shared conversations with ChatGPT to facilitate their role-specific contributions, whether as authors of PRs or issues, code reviewers, or collaborators on issues. Our work serves as the first step towards understanding the dynamics between developers and ChatGPT in collaborative software development and opens up new directions for future research on the topic."
A Brain-Computer Interface Application Based on P300 Evoked EEG Potentials for Enabling the Communication Between Users and Chat GPT,"Rusanu, OA",10.1007/978-3-031-56075-0_22,2024,"The brain-computer interface or the BCI is a high-technology provided by the breakthroughs from the biomedical engineering research field. The BCI is aimed at supporting people with neuromotor disabilities by enabling the achievement of movement and communication tasks using only the mental decoded intentions. This paper presents the development and the experimentation of BCI application integrated with the Chat GPT assistant. Considering that this AI modern instrument is really engaging triggering the feeling of communicating with a real human being, then it will prove its usefulness even for the disabled users who need to be treated as normal persons still having unaffected emotions and cognitive abilities. The EEG signals are acquired from the GTEC Unicorn headset embedding eight sensors placed to the frontal, parietal, temporal, and occipital cerebral lobes. The current work also focuses on the implementation of a LabVIEW instrument providing the solution of calling specific Python functions able to achieve the data transfer between computer and the Chat GPT API. Therefore, the originality is determined by solving the challenge underlying the software development of a functional brain-computer interface by combining LabVIEW graphical programming environment, Python language and P300 Speller Unicorn platform. This way, the users firstly need to focus their attention and eyesight to the alphanumeric symbols displayed by the Speller board. The target is to obtain simultaneous real-time data transfer starting with the questions addressed by the P300 Speller board and ending with the answers provided by the Chat GPT."
When XAI Meets CAPTCHA: A Case Study,"Udoidiok, I; Zhang, JL",10.1109/CARS61786.2024.10778641,2024,"In the rapidly evolving landscape of cybersecurity, the need for robust and transparent defenses against automated attacks is paramount. This paper examines the current landscape of human-centric cybersecurity, focusing on understanding human behavior, cognition, and emotions in relation to cybersecurity practices. We explore how Artificial Intelligence (AI) and machine learning (ML) technologies are being integrated into this framework to create more adaptive and responsive security systems, with a focal point on Completely Automated Public Turing tests to tell Computers and Humans Apart (CAPTCHA). Specifically, we investigate the integration of the eXplainable AI (XAI), using the Local Interpretable Model-Agnostic Explanations (LIME) technique, into CAPTCHA solvers to enhance the transparency of the model performance. By progressively refining a deep learning model designed to solve text-based CAPTCHAs, we demonstrate the impact of increasing training epochs and the number of samples explained by LIME on model accuracy and interpretability. The final results underscore the potential of XAI to improve the transparency and efficacy of ML models in cybersecurity applications, paving the way for more secure and trustworthy digital environments."
Investigating Software Development Teams Members' Perceptions of Data Privacy in the Use of Large Language Models (LLMs),"Falcao, FDS; Canedo, ED",10.1145/3701625.3701675,2024,"Context: Large Language Models (LLMs) have revolutionized natural language generation and understanding. However, they raise significant data privacy concerns, especially when sensitive data is processed and stored by third parties. Goal: This paper investigates the perception of software development teams members regarding data privacy when using LLMs in their professional activities. Additionally, we examine the challenges faced and the practices adopted by these practitioners. Method: We conducted a survey with 78 ICT practitioners from five regions of the country. Results: Software development teams members have basic knowledge about data privacy and LGPD, but most have never received formal training on LLMs and possess only basic knowledge about them. Their main concerns include the leakage of sensitive data and the misuse of personal data. To mitigate risks, they avoid using sensitive data and implement anonymization techniques. The primary challenges practitioners face are ensuring transparency in the use of LLMs and minimizing data collection. Software development teams members consider current legislation inadequate for protecting data privacy in the context of LLM use. Conclusions: The results reveal a need to improve knowledge and practices related to data privacy in the context of LLM use. According to software development teams members, organizations need to invest in training, develop new tools, and adopt more robust policies to protect user data privacy. They advocate for a multifaceted approach that combines education, technology, and regulation to ensure the safe and responsible use of LLMs."
Iterative Development of a Python-Driven Software for Detector Control Replacement at Gemini Observatory,"Stecher, HK; White, J; Stephens, AW; Tapia, E; D'Amato, J; Jacobson, S; Adamson, A; Figura, C; Miller, J; Cardenes, R; Boucher, L; Cooper, B; Quiroz, C; Kurz, E; Ramos, F",10.1117/12.3019553,2024,"Gemini Observatory commissioned a ARC (SDSU) detector controller (DC) replacement for the aging GNAAC DC for the Gemini Near Infrared Spectrograph (GNIRS). The focus of this paper is on the iterative development approach that led to a unique Python-based DC. We leveraged the stability and modern technology of the Gemini Data System (GDS) and Gemini Instrument API (GIAPI) to facilitate communication between the DC and the Gemini telescope systems. Another core innovation was to implement a Python version of the Gemini specific CAD/CAR EPICS records which allowed us to switch from an EPICS Input Output Controller (IOC) to a Caproto Python IOC. These innovations allow the Python based DC to communicate with all of the many Gemini systems required to process GNIRS observations. The use of a Python based DC enhances the system's functionality but also simplifies future updates and maintenance. Our paper delves into the team-centric iterative development process, the software engineering challenges, and the initial operational performance, emphasizing the software's role in modernizing the observatory's infrastructure."
Large Language Models for Cybersecurity: New Opportunities,"Divakaran, DM; Peddinti, ST",10.1109/MSEC.2024.3504512,2025,"With the emergence of powerful, versatile, and beneficial large language models (LLMs), we take a fresh look at cybersecurity, specifically exploring and summarizing the potential of LLMs in addressing challenging problems in the security and safety domains."
Integrating Feedback From Application Reviews Into Software Development,"Abdelaziz, O; Codabux, Z; Schneider, K",10.1109/APSEC65559.2024.00034,2024,"In application (app) development, effectively harnessing user feedback is crucial for enhancing app quality and user feedback. However, the vast and unstructured nature of user reviews often complicates these efforts, posing challenges in accurately capturing and integrating this feedback into the development processes. We automate the classification of issues in app reviews and examine how these issues correlate with code quality metrics (code smells and bug reports) and development activities (additions, deletions, and time to merge in pull requests). We aim to provide evidence-based guidance for effectively prioritizing and addressing user feedback. Employing a Mining Software Repositories (MSR) approach, we gathered and analyzed reviews from seven open-source Android apps. We evaluated the efficacy of three machine learning models-Support Vector Machines (SVM), BERT, and a fine-tuned GPT-3.5-for classifying issues in app reviews. The GPT-3.5 model achieved the highest accuracy at 95.0%. We found statistically significant correlations between the classified issues, code quality metrics, and development activities. However, these relationships varied across applications, highlighting the complex relationship between user feedback and the development process. Our study highlights the effectiveness of automated tools in identifying and classifying feedback within app reviews. Our automated approach enhances developers' ability to manage feedback effectively and supports optimal resource allocation to improve app quality and user feedback."
Developers' Perspective on Today's and Tomorrow's Programming Tool Assistance: A Survey,"Kuang, P; SÃ¶derberg, E; HÃ¶st, M",10.1145/3660829.3660848,2024,"Software development is a complex activity that needs a lot of tool assistance. Over the years there has been a lot of effort put into development of automated assistance to help with activities such as detection of issues via program analysis, or refactoring of code. Recently, the landscape of developer tool assistance is being disrupted with the entry of AI tools, such as Copilot and ChatGPT, powered via Large Language Models. Other kinds of tool assistance, for instance, gaze-driven assistance, is around the corner. What are programmers' perceptions on tool assistance today? What do they see as good directions for the future? In this paper, we present the results of a survey where we asked developers about their programming practices, experience with program analysis, and attitudes and views on enabling technologies, like AI and eye-tracking. We received 68 replies from a diverse group of developers from 12 countries. We found that 50% of the participants use program analysis and that many participants (N=28) already use AI-enabled tools for programming. We found that our participants were positive toward AI-powered tools, neutral toward eye-tracking, and negative toward gamification. We discuss these and other findings and point out directions for future work."
On the Effectiveness of Large Language Models for GitHub Workflows,"Zhang, XY; Muralee, S; Cherupattamoolayil, S; Machiry, A",10.1145/3664476.3664497,2024,"GitHub workflows or GitHub CI is a popular continuous integration platform that enables developers to automate various software engineering tasks by specifying them as workflows, i.e., YAML files with a list of jobs. However, engineering valid workflows is tedious. They are also prone to severe security issues, which can result in supply chain vulnerabilities. Recent advancements in Large Language Models (LLMs) have demonstrated their effectiveness in various software development tasks. However, GitHub workflows differ from regular programs in both structure and semantics. We perform the first comprehensive study to understand the effectiveness of LLMs on five workflow-related tasks with different levels of prompts. We curated a set of similar to 400K workflows and generated prompts with varying detail. We also fine-tuned LLMs on GitHub workflow tasks. Our evaluation of three state-of-the-art LLMs and their fine-tuned variants revealed various interesting findings on the current effectiveness and drawbacks of LLMs."
An Approach to Cluster Scenarios According to Their Similarity Using Natural Language Processing,"Delle Ville, J; Torres, D; FernÃ¡ndez, A; Antonelli, L",10.1007/978-3-031-57982-0_5,2024,"Scenarios are ideal to capture knowledge in human computer interface software engineering. Requirements engineering is a fundamental part of software development. If errors appear in this stage, it will be expensive to correct them in further stages. The domain experts and the developer team belong to different worlds. This generates a gap in communication between them. Because of it, it is important to use artifacts in natural language to communicate both sides. One simpler approach to specify requirements is Scenarios. They are widely used artifacts that generally describe the dynamics (tasks, activities) to be carried out in some specific situation. Generally, scenarios promote communication and participation from both sides. This can cause some problems. One of these problems is redundancy, that occurs when two stakeholders describe the same situation in different artifacts. This paper proposes an approach to analyze a set of scenarios by grouping them according to their similarity. The similarity is calculated through a series of comparisons of the different attributes of the scenario. This paper also describes a prototype implementing this method. Finally, the paper shows the result of a preliminary evaluation with results about the applicability of the approach."
Conscious Technologies Projects as a Hub for Real Life Challenges in Engineering Education,"Candela-Leal, MO; Aguilar-Herrera, AJ; RamÃ­rez-Moreno, MA; Lozoya-Santos, JDJ; FÃ©lix-HerrÃ¡n, LC; MartÃ­nez, JCT",10.1109/EDUCON60312.2024.10578738,2024,"Traditionally, research and scientific endeavors have predominantly involved graduate students, often sidelining participation of undergraduate students. However, integrating undergraduate students into leading cutting-edge projects significantly enhances their academic experience and professional growth. This manuscript delves into the case study of Conscious Technologies, an innovative research group comprising the Campus City initiative and the IUCRC International BRAIN Affiliate Site at Tecnologico de Monterrey. The group's focus encompasses smart city solutions, computer vision, robotic operating systems, neuroengineering, biometry, and biomechanics, all powered by artificial intelligence. The group efficiently manages a diverse array of projects by employing methodologies such as agile project management and a progressive-phased research approach, which leads to beneficial outcomes for all stakeholders. Undergraduate students gain hands-on experience, increasing their prospects for admission into prestigious graduate programs abroad, while at the same time, research professors expand their international presence through impactful publications. High research output gradually fosters collaboration, including exchanges and stays abroad, further catalyzing the centre's growth. By tackling real-world challenges within a well-structured framework, supplemented with intrinsic and extrinsic rewards and regular meetings, students enjoy autonomy within guided projects, culminating in the publication of 26 scientific works over a span of three years."
A Systematic Approach to Enhancing ISO 26262 With Machine Learning-Specific Life Cycle Phases and Testing Methods,"Iyenghar, P; Gracic, E; Pawelke, G",10.1109/ACCESS.2024.3506333,2024,"This paper presents a systematic approach to enhancing ISO 26262, a widely adopted standard for automotive functional safety, by integrating Machine Learning (ML)-specific life cycle phases and testing methods for Automotive Safety Integrity Level (ASIL) A/B. With the increasing incorporation of ML techniques in automotive systems, the current ISO 26262 framework reveals significant gaps in addressing ML-specific safety requirements. While ISO/DPAS 8800 provides an approach for developing AI systems that meet some safety properties, it does not provide a mapping concept for ASIL classification of ML systems. Furthermore, given the complexity of ML techniques in automotive systems, issues such as interpretability-critical for transparency and accountability-along with robustness and uncertainty handling, pose significant challenges that are not fully addressed by ISO 26262 and ISO/DPAS 8800. This study identifies and addresses these gaps by defining three additional life cycle phases: prepare data, train ML model, and deploy ML model. For each life cycle phase, we establish desired properties such as robustness, uncertainty handling, and interpretability, and propose suitable methods to achieve these properties. We adopt a rigorous evaluation framework inspired by IEC 61508 to assess the effectiveness of these methods. Since the method recommendations of ISO 26262 for ML-based products are incomplete, the approach presented in this paper provides critical guidance and room for expert assessment and independent certification, ensuring solid and reliable recommendations. This systematic, clear, uniform development procedure not only supports product teams in achieving their safety goals but also facilitates the certification process, reducing ambiguity and enhancing the overall safety and reliability of ML-based automotive systems."
Towards Generating a Dataset for Failure Prediction in Microservices Applications,"Tarhri, I; Allaki, D; Idrissi, HK",10.1007/978-3-031-67321-4_15,2024,"Microservices architecture has gained significant traction in modern software development due to its agility and scalability. However, using artificial intelligence models to effectively predict the failure of microservices workloads poses challenges, as there is a lack of comprehensive datasets that provide dedicated data to train these models. This paper presents a methodology for generating a dataset specifically designed for failure prediction tasks in microservices applications. We detail our approach for capturing relevant metrics, including the chosen workload generation method and data collection techniques. The paper then describes the characteristics of the generated dataset, including its size, granularity, and captured workload failure scenarios and patterns. Evaluation results demonstrate the utility of the obtained dataset in gaining insights into microservices performance. The aim of this work is to offer a new dataset with useful workload metrics, serving as a valuable resource for researchers and practitioners in the field."
Framework Architecture for AI/ML Data Management for Safety-Critical Applications,"Davison, JCDC; Tostes, PI; Carneiro, CAG",10.1109/DASC62030.2024.10748782,2024,"Artificial Intelligence introduces a new paradigm to software development. While traditional algorithms are explicitly programmed using instructions, in the machine learning approach the software is trained while exposed to data. The assurance of the result becomes directly dependent on the quality of the data. Data needs to be collected, processed, analyzed, validated, stored and protected. This study aims to establish the architecture for a framework where data management activities can be performed easily and efficiently. Regulation and standards about AI development for safety-critical applications are under development, but initial guidance already suggests a set of objectives that need to be satisfied during data management. A new concept of learning assurance is proposed to provide the adequate level of confidence. The concept proposed for data management can be applied with benefits in the quality of any safety-critical application based on machine learning, doesn't matter if it's in the aeronautical domain or others. The main contribution of this article, in addition to discussing the standards that will be applied in the aviation domain, is to bring in a practical way the tools that will assist in the development of machine learning systems and some ways to demonstrate compliance with the objectives of the standards in accordance with the roadmap of EASA for level 1 and level 2 machine learning safety-critical applications."
GraphPyRec: A novel graph-based approach for fine-grained Python code recommendation,"Zong, X; Zheng, S; Zou, HT; Yu, HL; Gao, S",10.1016/j.scico.2024.103166,2024,"Artificial intelligence has been widely applied in software engineering areas such as code recommendation. Significant progress has been made in code recommendation for static languages in recent years, but it remains challenging for dynamic languages like Python as accurately determining data flows before runtime is difficult. This limitation hinders data flow analysis, affecting the performance of code recommendation methods that rely on code analysis. In this study, a graph-based Python recommendation approach (GraphPyRec) is proposed by converting source code into a graph representation that captures both semantic and dynamic information. Nodes represent semantic information, with unique rules defined for various code statements. Edges depict control flow and data flow, utilizing a child-sibling-like process and a dedicated algorithm for data transfer extraction. Alongside the graph, a bag of words is created to include essential names, and a pre-trained BERT model transforms it into vectors. These vectors are integrated into a Gated Graph Neural Network (GGNN) process of the code recommendation model, enhancing its effectiveness and accuracy. To validate the proposed method, we crawled over a million lines of code from GitHub. Experimental results show that GraphPyRec outperforms existing mainstream Python code recommendation methods, achieving Top-1, 5, and 10 accuracy rates of 68.52%, 88.92%, and 94.05%, respectively, along with a Mean Reciprocal Rank (MRR) of 0.772."
Advancing Software Security and Reliability in Cloud Platforms through AI-based Anomaly Detection,"Saleh, SM; Sayem, IM; Madhavji, N; Steinbacher, J",10.1145/3689938.3694779,2024,"Continuous Integration/Continuous Deployment (CI/CD) is fundamental for advanced software development, supporting faster and more efficient delivery of code changes into cloud environments. However, security issues in the CI/CD pipeline remain challenging, and incidents (e.g., DDoS, Bot, Log4j, etc.) are happening over the cloud environments. While plenty of literature discusses static security testing and CI/CD practices, only a few deal with network traffic pattern analysis to detect different cyberattacks. This research aims to enhance CI/CD pipeline security by implementing anomaly detection through AI (Artificial Intelligence) support. The goal is to identify unusual behaviour or variations from network traffic patterns in pipeline and cloud platforms. The system shall integrate into the workflow to continuously monitor pipeline activities and cloud infrastructure. Additionally, it aims to explore adaptive response mechanisms to mitigate the detected anomalies or security threats. This research employed two popular network traffic datasets, CSE-CIC-IDS2018 and CSE-CIC-IDS2017. We implemented a combination of Convolution Neural Network (CNN) and Long Short-Term Memory (LSTM) to detect unusual traffic patterns. We achieved an accuracy of 98.69% and 98.30% and generated log files in different CI/CD pipeline stages that resemble the network anomalies affected to address security challenges in modern DevOps practices, contributing to advancing software security and reliability."
Smart Health Software to Support Rescue Personnel in Emergency Situations,"Ahammed, A; Obermaisser, R",10.1109/SST61991.2024.10755467,2024,"Rescue stations around the world receive millions of emergency rescue calls each year, most of which are due to health complications. Due to the high frequency and necessity of rescue services, there is always an increasing demand for quick, accurate, and coordinated responses from rescue personnel to save lives and mitigate damage. This paper introduces a rescue health management software solution designed to improve the efficiency and effectiveness of rescue situational awareness by rapidly assessing the health status of emergency patients using AI-driven decision support systems. The novelty in this software approach is it's user-centered design principles to ensure that its solutions are specifically tailored to meet the unique requirements of emergency responders. It used pre-trained machine learning models with rescue data and accepted new patient's input data to provide a probability of the major health complications so that rescue personnel can expedite treatment plan following the outcome. The paper focuses primarily on the software development and implementation steps with three use cases, while also providing a short overview of the previous machine learning-based development phases."
Context-based Python Code Recommendation,"Zong, X; Zhang, Q; Li, Q; Xia, S",10.1109/ICCEA62105.2024.10603551,2024,"Code recommendation plays a crucial role in programming, assisting programmers in improving code quality, efficiency, and maintainability, thereby better addressing complex software development tasks. In recent years, artificial intelligence has been widely applied in software engineering domains such as code recommendation, particularly in the field of static languages, where significant advancements have been made in intelligent code recommendation. However, for dynamic languages like Python, intelligent code recommendation faces numerous challenges, such as the difficulties in dynamic analysis and code data imbalance. In light of this, building upon previous work, this paper achieves more accurate Python code recommendation by representing code context with semantically richer graphs and employing conditional generative adversarial networks for data augmentation of code semantic graphs, mitigating the impact of data imbalance. Experimental results demonstrate that the proposed code recommendation method, GraphPyRec, can effectively recommend Python code, and the data augmentation method, GraphGAN, can significantly optimize the recommendation model."
TESTING METHODOLOGY FOR DEVS MODELS IN CADMIUM,"Winstanley, C; Wainer, G",10.1109/WSC63780.2024.10838817,2024,"The practice of testing in modeling and simulation software development can be a very lengthy and tedious process but is arguably one of the most important phases in the software development lifecycle. As the complexity of a simulation model increases, so does the amount of testing to thoroughly verify and validate and to achieve adequate quality assurance of the software. This paper introduces a testing framework that is used to assist in proving the validity of DEVS atomic models in the open-source simulation tool Cadmium. Furthermore, this framework utilizes the ChatGPT Application Programming Interface (API) to help lighten the workload involved in testing those DEVS atomic models. We show how to use the framework using the Cadmium simulator to show the effectiveness of the framework."
AthenaLLM: Supporting Experiments with Large Language Models in Software Development,"de Oliveira, B; Castor, F",10.1145/3643916.3644437,2024,"Existing studies on the use of Large Language Models (LLMs) in software development leverage methodologies that limit their scalability and require intensive manual data collection and analysis, for example, due to the use of video data or think-aloud protocols. We propose the use of a specialized tool capable of automatically collecting fine-grained, relevant data during experiments and case studies. It enables researchers to understand for example how often participants accept or reject suggestions made by LLMs and what kinds of prompts are more likely to trigger accepted suggestions, even in studies targeting a large number of participants. We implement this idea as a Visual Studio Code plugin named AthenaLLM(1). It mimics the functionalities of GitHub Copilot and offers seamless integration with OpenAI API models like GPT-4 and GPT-3.5, and compatibility with other models providing an OpenAI-compatible API, e.g., Vicuna [6]. It automatically collects data at a fine level of granularity and covers both the interactions of developers with their IDE, e.g., all changes made in the code, and the products of such interactions, e.g., the generated code, when accepted. Thus, the proposed approach also reduces bias that the experimental process itself may introduce, e.g., due to the need for participants to verbalize their thoughts. In this paper we discuss how AthenaLLM could enable researchers to go both broader (in terms of number of participants) and deeper (in terms of the kinds of research questions that can be tackled)."
Risk management in large-scale information system projects,"Castillo-Ãopo, DA; Loyola-Blanco, KF; Castro-Marca, R; Rosa-Gavino, GDL; AragÃ³n-Retuerto, JG; Rafael-Sifuentes, HA; MarÃ­n-RodrÃ­guez, WJ",10.4108/eetsis.4608,2024,"This article deals with project management in information systems, whose relevance lies in the vital importance of these systems in modern companies. Information systems are essential for decision making and data management in today's interconnected world. Project management, on the other hand, coordinates elements such as scope, resources, costs, schedules and risks to achieve defined objectives. The systems development life cycle (SDLC) structures the process, encompassing phases such as scope definition, planning, execution, monitoring and closure. These phases are integrated with risk management, which identifies, evaluates and mitigates threats and opportunities. Mitigation strategies act before adversity, while contingency planning prepares for the unforeseen. That is why risk management is integrated throughout the project life cycle to anticipate and address challenges. The combination of both aspects is critical in a constantly evolving technology environment. In addition, organizational culture and communication play a critical role. A culture of awareness and accountability, transparency in communication and active stakeholder participation are essential. Training and continuous adaptation allow learning from past experiences and improving practices."
Low-Modeling of Software Systems,"Cabot, J",10.1007/978-3-031-61753-9_2,2024,"There is a growing need for better development methods and tools to keep up with the increasing complexity of new software systems. New types of user interfaces, the need for intelligent components, sustainability concerns,... bring new challenges that we need to handle. In the last years, model-driven engineering has been key to improving the quality and productivity of software development, but models themselves are becoming increasingly complex to specify and manage. In this paper, we present the concept of low-modeling as a solution to enhance current model-driven engineering techniques and get them ready for this new generation of software systems."
Open-Source Tools for the Fabrication and Characterization of Organic Electronics,"Butscher, JF; Kwon, S; Popczyk, A; Gather, MC",10.1002/aelm.202400460,2025,"By promoting collaborative sharing of knowledge, the open-source movement has catalyzed substantial progress across diverse fields, including software development and artificial intelligence. Similarly, the concept of openly shared hardware has gained attention, due to its cost-effectiveness and the prospect of improved reproducibility. A major motivation for the development of organic electronics is its promise to deliver substantial advantages in price and manufacturability relative to its inorganic counterpart. Here, two open-source tools for organic electronics are introduced: a dip-coating device designed for thin film fabrication and a four-point probe for precisely measuring the resistance of thin films. These tools only cost a fraction of comparable commercial devices and run with open-source software to ensure a user-friendly experience. A case study demonstrates the optimization of simple fluorescent organic light-emitting diodes (OLEDs) using these open-source tools achieving 4% external quantum efficiency (EQE). To characterize these OLEDs, a previously reported open-source setup for accurate efficiency measurements is used. A substantial software upgrade to this setup, which speeds up the characterization of electroluminescence, is also repor. This work contributes open-source hardware and software to the field of organic electronics, thereby lowering the entrance barrier to the field and fostering the involvement of scientists with diverse scientific backgrounds. The open-source movement has catalyzed progress in many areas of research such as software development, artificial intelligence, and hardware development. This work introduces new cost-effective, open-source hardware projects for the organic electronics community: a dip-coating device for thin film fabrication, a four-point probe for resistance measurement, and improvements to the previously reported setup for efficiency measurements of light-emitting devices. image"
Transformer DGA Fault Diagnosis Method Based on Data Reconstruction Combined with IDBO-LSSVM Model,"Huang, HL; Hu, Y; Deng, Y; Li, L; Zeng, QZ; Yan, KL",10.1109/ICPES63746.2024.10856607,2024,"Currently, the combination of big data mining techniques and dissolved gas analysis (DGA) monitoring can be used to diagnose transformer faults. However, due to the high dimensionality and imbalanccd fault samples in DGA data in practical engineering, existing diagnostic methods generally suffer from low accuracy and poor fault tolerance. Therefore, this paper proposes a transformer DGA fault diagnosis method based on data reconstruction combined with the Improved Dung Beetle Optimizer-Least Squares Support Vector Machine (IDBO-LSSVM) model. This method first employs the Adaptive Synthetic Sampling (ADASYN) method to synthesize minority class samples and utilizes t-Distributed Stochastic Neighbor Embedding (t-SNE) for feature extraction to reconstruct the DGA raw data samples. Subsequently, a transformer fault diagnosis model is constructed based on Least Squares Support Vector Machine (LSSVM), and the Improved Dung Beetle Optimizer (IDBO) is used to optimize the model parameters. Efficient acquisition of the global optimal solution is achieved by introducing the Sine Power Mapping (SPM) chaos map, adaptive weights, and Cauchy mutation strategy. Finally, the accuracy and superiority of the proposed transformer fault diagnosis method are verified through algorithm case analysis and comparative calculations with multiple methods using measured data."
Advancing Software Efficiency: A Novel Dynamic Code Optimizer Empowered by Blackbox AI Integration,"Das, D; Maity, A; Raj, A; Manna, D; Bandyopadhyay, A; Chakraborty, P",10.1109/TENSYMP61132.2024.10752220,2024,"This study aims to address the persistent challenge of optimizing code for efficiency by leveraging AI-driven solutions. We propose a novel approach that integrates Dynamic Code Optimization (DCO) with Blackbox AI to create intelligent and adaptive optimization strategies. Our methodology involves using Blackbox AI to automatically analyze code and generate optimization suggestions, ensuring consistent and efficient application across the codebase. The major findings from our experimental evaluation reveal significant improvements in code efficiency and performance, demonstrating the effectiveness of this integrated approach. Our results suggest that AI-driven optimization can transform traditional code optimization workflows, making them more efficient and reliable. These findings have important implications for software development, highlighting the potential of AI to enhance the quality and efficiency of code optimization processes."
Software Engineering and Gender: A Tutorial,"Jaccheri, L; Duc, AN",10.1145/3663529.3663818,2024,"Software runs the world and should provide equal rights and opportunities to all genders. However, the gender gap exists in the software engineering workforce and many software products are still gender biased. Recently, AI systems, including modern large language models are shown to be related to gender bias issues. Many efforts have been devoted to understanding the problem and investigating solutions. The tutorial aims to present a set of scientific studies based on qualitative and quantitative research methods. The authors have a long record of research leadership in interdisciplinary projects with a focus on gender and software engineering. The issues with team diversity in software development and AI engineering will be presented to highlight the importance of fostering inclusive and diverse software development teams."
Intelligent Analysis and Software Development of Converter Transformer Failure Cases Based on Pearson Correlation Coefficient,"He, KH; Zhang, T; Cao, YM; Wu, JZ; Liu, BL",10.1109/ACFPE63443.2024.10800759,2024,"The occurrence of converter transformer failures frequently results in considerable damage and substantial financial losses. While failure case analysis may assist in reducing these problems, the current analytical methods have yet to fully investigate the manufacturing stage. In order to conduct an in-depth study of converter transformer failure cases and to improve the manufacturing process, we have designed and developed a software platform for processing these cases. This platform enables the identification, extraction and statistical analysis of feature in the failure cases. Furthermore, we put forward a method for constructing a multidimensional feature correlation matrix based on the Pearson correlation coefficient. This provides a transparent visual representation of the relationships between the multidimensional features. Finally, a statistical and computational analysis was conducted to determine the proportion of failure problems and the correlations between features. The results validated the effectiveness of the proposed method in the comprehensive analysis of converter transformer failure cases."
Design and Implementation of Security Enhancement Mechanism for Smart Distribution Transformer Combine Terminal Based on WAPI,"Fan, C; Qian, J; Du, MJ; Wang, XJ; Xiang, X",10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics60724.2023.00050,2024,"Smart grid has high security requirements for communication technology. Smart distribution transformer (SDT) combine terminals are installed in the intelligent collection and control terminal in the low-voltage station area to support marketing, power distribution and emerging business development needs. In this paper, we propose an SDT combine terminal design scheme based on Wireless LAN Authentication and Privacy Infrastructure (WAPI) communication technology. A security enhancement mechanism is introduced for designed terminal scheme. We also analyzed application scenarios in smart grids for our scheme. Our proposed solution is verified on the real SDT combine terminal by integrating software development kit into WAPI communication module."
Assessing the Performance of AI-Generated Code: A Case Study on GitHub Copilot,"Li, S; Cheng, YT; Chen, JF; Xuan, JF; He, S; Shang, WY",10.1109/ISSRE62328.2024.00030,2024,"The integration of Large Language Models (LLMs) into software development tools like GitHub Copilot holds the promise of transforming code generation processes. While AI-driven code generation presents numerous advantages for software development, code generated by large language models may introduce challenges related to security, privacy, and copyright issues. However, the performance implications of AI-generated code remain insufficiently explored. This study conducts an empirical analysis focusing on the performance regressions of code generated by GitHub Copilot across three distinct datasets: HumanEval, AixBench, and MBPP. We adopt a comprehensive methodology encompassing static and dynamic performance analyses to assess the effectiveness of the generated code. Our findings reveal that although the generated code is functionally correct, it frequently exhibits performance regressions compared to code solutions crafted by humans. We further investigate the code-level root causes responsible for these performance regressions. We identify four major root causes, i.e., inefficient function calls, inefficient looping, inefficient algorithm, and inefficient use of language features. We further identify a total of ten sub-categories of root causes attributed to the performance regressions of generated code. Additionally, we explore prompt engineering as a potential strategy for optimizing performance. The outcomes suggest that meticulous prompt designs can enhance the performance of AI-generated code. This research offers valuable insights contributing to a more comprehensive understanding of AI-assisted code generation."
Leveraging Large Language Models for Efficient Failure Analysis in Game Development,"Marini, L; Gisslen, L; Sestini, A",10.1109/CoG60054.2024.10645540,2024,"In games, and more generally in the field of software development, early detection of bugs is vital to maintain a high quality of the final product. Automated tests are a powerful tool that can catch a problem earlier in development by executing periodically. As an example, when new code is submitted to the code base, a new automated test verifies these changes. However, identifying the specific change responsible for a test failure becomes harder when dealing with batches of changes especially in the case of a large-scale project such as a AAA game, where thousands of people contribute to a single code base. This paper proposes a new approach to automatically identify which change in the code caused a test to fail. The method leverages Large Language Models (LLMs) to associate error messages with the corresponding code changes causing the failure. We investigate the effectiveness of our approach with quantitative and qualitative evaluations. Our approach reaches an accuracy of 71% in our newly created dataset, which comprises issues reported by developers at EA over a period of one year. We further evaluated our model through a user study to assess the utility and usability of the tool from a developer perspective, resulting in a significant reduction in time - up to 60% - spent investigating issues."
Ensemble-SMOTE Model to Evaluate Air Quality in the Industrial Area in Chavara,"Johnson, S; Perumalsamy, D",,2024,"Air quality is a critical environmental concern, particularly in industrial areas where emissions from factories can significantly impact the health of nearby populations. This study focuses on evaluating the air quality on pollutants like SO2, NO2, PM10, and SPM in the Kerala Minerals and Metals Limited (KMML) industrial area in Chavara, Kerala, India. To predict air quality indicators accurately, the researchers used a combination of artificial intelligence techniques. By comparing error metrics across different approaches, they identified the optimal method for accurate predictions. The study employed machine learning algorithms and SMOTE to predict Air Quality Index (AQI) levels. The ensemble SMOTE method outperformed individual classifiers like KNN, SVM, DT, RF, and GaussianNB, achieving higher accuracy, precision, recall, and F1-score, indicating its effectiveness in predicting AQI levels. The study also highlighted the importance of data preprocessing and balancing for improved prediction accuracy."
A System-in-package for Smart Microphone with Ultra-low Power AI co-processor,"Yang, XF; Jiang, Z; Zhao, XM; Chen, K; Lu, JP; Huang, Y; Zhang, J; Xiao, FX; Pang, S; Liu, HJ",10.1109/ICICDT63592.2024.10717725,2024,"This paper presents a compact and ultra-low power System in Package (SiP) for AI processing of voice signal. The proposed SiP is composed of a MEMS microphone and AI SoC, where the MEMS microphone and AI SoC are connected through wire bonding. The AI SoC integrates analog front- end, power management, clock generation, analog signal processor (ASP), OTP, SRAM and configurable NPU. The SiP smart microphone achieve ultra-low system power by replacing audio ADC and DSP with ASP. The high integration of the SiP requires only voltage supply to accomplish the Keyword Spotting (KWS) and Voice Activity Detector (VAD). The Software Development Kit (SDK) can configure the AI SoC for various tasks. The compact AI SoC designed with size of 1.7 mm x 1.5 mm and pads locate only in two edges. The operating power consumption is 100- 290uA and KWS accuracy reach 95%."
Quality of AI-Generated vs. Human-Generated Code,"Eltabakh, TM; Soudi, NN; Shawky, D",10.1109/ICCTA64612.2024.10974782,2024,"This study examines the quality differences between AI-generated and human-generated code through an evaluation of multiple software quality metrics, including maintainability, complexity, and documentation. Using a dataset of 5,312 code samples-2,700 human-generated and 2,612 AI-generated-we applied machine learning techniques to classify and analyze the code based on these metrics. The results revealed that AI-generated code tends to excel in maintainability and documentation, demonstrating higher maintainability index scores and a higher ratio of comments. Additionally, AI-generated code often features simpler control structures, reflected in its lower cyclomatic complexity. In contrast, human-generated code showcased greater adaptability and flexibility, particularly in addressing complex problem statements. A neural network classifier achieved 88.05% accuracy in distinguishing between the two code origins, with comments ratio, maintainability index, and cyclomatic complexity being the most significant differentiators. These findings highlight the complementary roles of AI and human contributions in software development, suggesting strategic integration of both for enhanced efficiency and quality."
Stacked Ensemble Deep Learning for the Classification of Nonfunctional Requirements,"Alqurashi, A; Alawneh, L",10.1109/TR.2024.3513834,2025,"Requirements engineering is the foundation for software quality. Defining the correct software requirements in the initial phases of the software development life cycle minimizes project costs and efforts. While functional requirements (FRs) define the software features, nonfunctional requirements (NFRs), such as availability, performance, security, and reliability are essential for the acceptance and deployment of the software. Understanding software requirements from different stakeholders is a tedious task. Manual investigation of the stakeholder needs may skip important NFRs. Thus, the need for automatic requirements classification techniques arose to eliminate the misinterpretation of stakeholder needs and to speed up the development process. Several machine learning approaches targeted the classification of NFRs. We explore the recurrent neural network, long short-term memory, and gated recurrent unit deep learning (DL) methods. We apply the random search technique for hyperparameter optimization. Further, we use stacked ensemble learning to enhance the classification by combining the strengths of the base models using support vector machine as a meta-learner. We use grid search to optimize the hyperparameters of the meta-learner. Further, we compare the stacked ensemble approach with the BERT language model. The proposed approach is evaluated on 914 NFRs gathered from two datasets. Our ensemble model achieved a weighted average precision, recall, and F1-Score of 0.91, 0.90, 0.90, respectively."
Method of Searching for Clones of the Program Code in Binary Executive Files,"Zavadskii, EV; Bulat, AV; Gribkov, NA",10.3103/S0146411624700913,2024,"The modern trend of increasing labor productivity and business process efficiency entails the optimization of software development processes through the use of generative artificial intelligence models trained on various code bases and manual copying of code fragments. Given the growing number of registered vulnerabilities, methods for detecting clones of the software code are needed. A method for assessing the similarity of fragments of the program code of binary executable files, which is based on the representation of the code in the form of an FA-AAST tree and the apparatus of graph neural networks, is proposed. The results obtained during testing on open and closed source software demonstrate the correctness of the proposed method and higher accuracy compared to the solutions considered."
The Impact of the Digital Economy on the Strategic Management of Enterprise Logistics,"Hurzhyi, N; Klymenko, Y; Mieniyailova, H; Andrushkevych, Z; Kharsun, L",,2024,"While many industries are rapidly adopting digital technologies to adapt to rapidly changing economic conditions and increasing customer demands, logistics and supply chain management companies have historically been slow to adopt logistics software development services. And while many logistics companies are already beginning to actively implement digital technologies, the level of adoption among large and small companies is still relatively low. Nevertheless, logistics offers a lot of opportunities for the use of digital technologies. Given the relevance of the topic, the purpose of the study is to substantiate the prospects and opportunities for using the digital economy to stimulate the strategic management of enterprise logistics. The study found that modern logistics companies and companies engaged in logistics have many opportunities for active development using innovative technologies. It has been established that digital transformation in logistics is a practical approach to implementing digitalisation at all levels of a company's business processes: from optimising the logistics of physical flows and optimising data exchange to customer service and much more. Overall, it is only natural for companies that want to maintain their competitive edge in the market to think about the next steps in their digital transformation. That's why they have many opportunities, including the introduction of artificial intelligence, self-driving vehicles, and machine optimisation of business processes to improve the efficiency of enterprise management."
Power Transformer Fault Diagnosis Based on Random Forest and Improved Particle Swarm Optimization-Backpropagation-AdaBoost,"Zhou, L; Fu, ZJ; Li, KY; Wang, YH; Rao, H",10.3390/electronics13214149,2024,"This paper proposes a novel fault diagnosis methodology for oil-immersed transformers to improve the diagnostic accuracy influenced by gas components in power transformer oil. Firstly, the Random Forest (RF) algorithm is utilized to evaluate and filter the raw data features, solving the problem of determining significant features in the dataset. Secondly, a multi-strategy Improved Particle Swarm Optimization (IPSO) is applied to optimize a double-hidden layer backpropagation neural network (BPNN), which overcomes the challenge of determining hyperparameters in the model. Four enhancement strategies, including SPM chaos mapping based on opposition-based learning, adaptive weight, spiral flight search, and crisscross strategies, are introduced based on traditional Particle Swarm Optimization (PSO) to enhance the model's optimization capabilities. Lastly, AdaBoost is integrated to fortify the resilience of the IPSO-BP network. Ablation experiments demonstrate an enhanced convergence rate and model accuracy of IPSO. Case analysis using Dissolved Gas Analysis (DGA) samples compares the proposed IPSO-BP-AdaBoost model with other swarm intelligence optimization algorithms integrated with BPNN. The experimental findings highlight the superior diagnostic accuracy and classification performance of the IPSO-BP-AdaBoost model."
Code Ownership in Open-Source AI Software Security,"Wen, JW; Yuari, D; Ma, L; Chen, HM",10.1145/3643691.3648586,2024,"As open-source AI software projects become an integral component in the AI software development, it is critical to develop a novel measurement method to ensure the security of the open-source AI projects for developers. Code ownership, pivotal in the evolution of such projects, offers insights into developer engagement and potential vulnerabilities. In this paper, we leverage the code ownership metrics to empirically investigate the correlation with the latent vulnerabilities across five prominent open-source AI software projects. The findings from the large-scale empirical study suggest a positive relationship between high-level ownership (characterised by a limited number of minor contributors) and a decrease in vulnerabilities. Furthermore, we innovatively introduce the time metrics, anchored on the project's duration, individual source code file timelines, and the count of impacted releases. These metrics adeptly categorise distinct phases of open-source AI software projects and their respective vulnerability intensities. With these novel code ownership metrics, we have implemented a Python-based command-line application to aid project curators and quality assurance professionals in evaluating and benchmarking their on-site projects. We anticipate this work will embark a continuous research development for securing and measuring open-source AI project security."
Image resolution enhancement for advanced semiconductor nodes,"Rencker, L; Tajalizadehkhoob, O; Elsayed, K; Khachaturiants, A; Pahlavani, H; Guo, Y; van de Laar, J; Simons, E; Saikumar, N; Sadeghian, H",10.1117/12.3011239,2024,"Advanced semiconductor nodes are pushing the limits of feature sizes and require metrology with sub-nm resolution without compromising on the throughput as needed for in-line process control. Recently, high-throughput scanning probe microscopy (SPM) based metrology and inspection tools capable of meeting these needs have been introduced to the market and qualified for use in HVM. While innovative measurement methods and tool architecture have allowed for a leap of improvement in throughput, the next step in further reducing imaging time can be obtained through the application of machine learning for enhancing the resolution of measured images for extraction of relevant parameters. In this work, we provide the general framework under which a neural network-based resolution enhancer is designed and used for SPM images. We showcase the effectiveness of this framework using measurements performed on Line/Space structures with a pitch of 200 nm. For the reusability of a pre-developed pre-trained model, we additionally leverage transfer learning and show that a new model for slightly differing structures can be re-trained and calibrated with a smaller data set of measurements performed on Line/Space structures with a pitch of 100 nm."
ConCPDP: A Cross-Project Defect Prediction Method Integrating Contrastive Pretraining and Category Boundary Adjustment,"Song, HJ; Pan, YF; Guo, F; Zhang, X; Ma, L; Jiang, SY",10.1049/2024/5102699,2024,"Software defect prediction (SDP) is a crucial phase preceding the launch of software products. Cross-project defect prediction (CPDP) is introduced for the anticipation of defects in novel projects lacking defect labels. CPDP can use defect information of mature projects to speed up defect prediction for new projects. So that developers can quickly get the defect information of the new project, so that they can test the software project pertinently. At present, the predominant approaches in CPDP rely on deep learning, and the performance of the ultimate model is notably affected by the quality of the training dataset. However, the dataset of CPDP not only has few samples but also has almost no label information in new projects, which makes the general deep-learning-based CPDP model not ideal. In addition, most of the current CPDP models do not fully consider the enrichment of classification boundary samples after cross-domain, leading to suboptimal predictive capabilities of the model. To overcome these obstacles, we present contrastive learning pretraining for CPDP (ConCPDP), a CPDP method integrating contrastive pretraining and category boundary adjustment. We first perform data augmentation on the source and target domain code files and then extract the enhanced data as an abstract syntax tree (AST). The AST is then transformed into an integer sequence using specific mapping rules, serving as input for the subsequent neural network. A neural network based on bidirectional long short-term memory (Bi-LSTM) will receive an integer sequence and output a feature vector. Then, the feature vectors are input into the contrastive module to optimise the feature extraction network. The pretrained feature extractor can be fine-tuned by the maximum mean discrepancy (MMD) between the feature distribution of the source domain and the target domain and the binary classification loss on the source domain. This paper conducts a large number of experiments on the PROMISE dataset, which is commonly used for CPDP, to validate ConCPDP's efficacy, achieving superior results in terms of F1 measure, area under curve (AUC), and Matthew's correlation coefficient (MCC)."
Feature Creation to Enhance Explainability and Predictability of ML Models Using XAI,"Ahmed, W",10.14569/IJACSA.2024.01510101,2024,"Bringing more transparency to the decision making process in fields deploying ML tools is important in various fields. ML tools need to be designed in such a way that they are more understandable and explainable to end users while bringing trust. The field of XAI, although a mature area of research, is increasingly being seen as a solution to address these missing aspects of ML systems. In this paper, we focus on transparency issues when using ML tools in the decision making process in general, and specifically while recruiting candidates to high-profile positions. In the field of software development, it is important to correctly identify and differentiate highly skilled developers from developers who are adept at only performing regular and mundane programming jobs. If AI is used in the decision process, HR recruiting agents need to justify to their managers why certain candidates were selected and why some were rejected. Online Judges (OJ) are increasingly being used for developer recruitment across various levels attracting thousands of candidates. Automating this decision-making process using ML tools can bring speed while mitigating bias in the selection process. However, the raw and huge dataset available on the OJs need to be well curated and enhanced to make the decision process accurate and explainable. To address this, we built and subsequently enhanced a ML regressor model and the underlying dataset using XAI tools. We evaluated the model to show how XAI can be actively and iteratively used during pre-deployment stage to improve the quality of the dataset and to improve the prediction accuracy of the regression model. We show how these iterative changes helped improve the r2-score of the GradientRegressor model used in our experiments from 0.3507 to 0.9834 (an improvement of 63.27%). We also show how the explainability of LIME and SHAP tools were increased using these steps. A unique contribution of this work is the application of XAI to a very niche area in recruitment, i.e. in the evaluation of performance of users on OJs in software developer recruitment."
Automated Identification of Security Issues from Commit Messages and Bug Reports,"Zhou, YQ; Sharma, A",10.1145/3106237.3117771,2017,"The number of vulnerabilities in open source libraries is increasing rapidly. However, the majority of them do not go through public disclosure. These unidentified vulnerabilities put developers' products at risk of being hacked since they are increasingly relying on open source libraries to assemble and build software quickly. To find unidentified vulnerabilities in open source libraries and secure modern software development, we describe an effcient automatic vulnerability identification system geared towards tracking large-scale projects in real time using natural language processing and machine learning techniques. Built upon the latent information underlying commit messages and bug reports in open source projects using GitHub, JIRA, and Bugzilla, our K-fold stacking classifier achieves promising results on vulnerability identification. Compared to the state of the art SVM-based classifier in prior work on vulnerability identification in commit messages, we improve precision by 54.55% while maintaining the same recall rate. For bug reports, we achieve a much higher precision of 0.70 and recall rate of 0.71 compared to existing work. Moreover, observations from running the trained model at SourceClear in production for over 3 months has shown 0.83 precision, 0.74 recall rate, and detected 349 hidden vulnerabilities, proving the effectiveness and generality of the proposed approach."
A cluster of immunoresolvents links coagulation to innate host defense in human blood,"Norris, PC; Libreros, S; Chiang, N; Serhan, CN",10.1126/scisignal.aan1471,2017,"Blood coagulation is a protective response that prevents excessive bleeding upon blood vessel injury. We investigated the relationship between coagulation and the resolution of inflammation and infection by lipid mediators (LMs) through metabololipidomics-based profiling of human whole blood (WB) during coagulation. We identified temporal clusters of endogenously produced prothrombotic and proinflammatory LMs (eicosanoids), as well as specialized proresolving mediators (SPMs). In addition to eicosanoids, a specific SPM cluster was identified that consisted of resolvin E1 (RvE1), RvD1, RvD5, lipoxin B-4, and maresin 1, each of which was present at bioactive concentrations (0.1 to 1 nM). Removal of adenosine from the coagulating blood markedly enhanced the amounts of SPMs produced and further increased the biosynthesis of RvD3, RvD4, and RvD6. The cyclooxygenase inhibitors celecoxib and indomethacin, which block the production of thromboxanes and prostanoids, did not block the production of clot-driven SPMs. Unbiased mass cytometry analysis demonstrated that the SPM cluster produced in human blood targeted leukocytes at the single-cell level, directly activating ERK and CREB signaling in neutrophils and CD14(+) monocytes. Treatment of human WB with the components of this SPM cluster enhanced both the phagocytosis and killing of Escherichia coli by leukocytes. Together, these data identify a proresolving LM circuit, including endogenous molecular brakes and accelerators, which promoted host defense. These temporal LM-SPM clusters can provide accessible metabolomic profiles for precision and personalized medicine."
"Investigating the relationship between price, rating, and popularity in the Blackberry World App Store","Finkelstein, A; Harman, M; Jia, Y; Martin, W; Sarro, F; Zhang, YY",10.1016/j.infsof.2017.03.002,2017,"Context: App stores provide a software development space and a market place that are both different from those to which we have become accustomed for traditional software development: The granularity is finer and there is a far greater source of information available for research and analysis. Information is available on price, customer rating and, through the data mining approach presented in this paper, the features claimed by app developers. These attributes make app stores ideal for empirical software engineering analysis. Objective: This paper(1) exploits App Store Analysis to understand the rich interplay between app customers and their developers. Method: We use data mining to extract app descriptions, price, rating, and popularity information from the Blackberry World App Store, and natural language processing to elicit each apps' claimed features from its description. Results: The findings reveal that there are strong correlations between customer rating and popularity (rank of app downloads). We found evidence for a mild correlation between app price and the number of features claimed for the app and also found that higher priced features tended to be lower rated by their users. We also found that free apps have significantly (p-value < 0.001) higher ratings than non free apps, with a moderately high effect size (<(A)over cap>(12) = 0.68). All data from our experiments and analysis are made available on-line to support further investigations. (C) 2017 The Authors. Published by Elsevier B.V."
Ontology Based Multiagent Effort Estimation System for Scrum Agile Method,"Adnan, M; Afzal, M",10.1109/ACCESS.2017.2771257,2017,"This paper emphasizes on software effort estimation and knowledge management of practicing Scrum methodology that are challenging tasks in agile context. Proposed approach improves software effort estimation and knowledge management of software projects by focusing on Scrum process and practices using ontology model in a multiagent estimation system. It also motivates project key stakeholders to regularly save significant tacit knowledge of unique situations in the form of lessons learnt during the project development. Various agents of the estimation system access the existing knowledge base and autonomously perform their inferencing activities using description logic as per requirements specified by the scrum master and respond with suitable estimate to him/her in the form of time, resources, and lessons learnt for the success of future projects. To validate our approach, an experiment, based on twelve web projects, was conducted using proposed approach, delphi and planning poker estimation methods. The obtained results by applying MMRE, PRED(x) evaluation measures reveals that proposed approach delivers more accurate estimates as compared with delphi and planning poker methods."
Core-periphery communication and the success of free/libre open source software projects,"Crowston, K; Shamshurin, I",10.1186/s13174-017-0061-4,2017,"We examine the relationship between communications by core and peripheral members and Free/Libre Open Source Software project success. The study uses data from 74 projects in the Apache Software Foundation Incubator. We conceptualize project success in terms of success building a community, as assessed by graduation from the Incubator. We compare successful and unsuccessful projects on volume of communication and on use of inclusive pronouns as an indication of efforts to create intimacy among team members. An innovation of the paper is that use of inclusive pronouns is measured using natural language processing techniques. We also compare the volume and content of communication produced by core (committer) and peripheral members and by those peripheral members who are later elected to be core members. We find that volume of communication is related to project success but use of inclusive pronouns does not distinguish successful projects. Core members exhibit more contribution and use of inclusive pronouns than peripheral members."
Integration of artificial intelligence activities in software development processes and measuring effectiveness of integration,"Kulkarni, RH; Padmanabham, P",10.1049/iet-sen.2016.0095,2017,"Recently, the modelling of whole process of software (SW) development is performed using extended waterfall and agile models. The further advancement of extended waterfall and agile models in the main phases like communication, planning, modelling, construction and deployment can improve the overall quality of the product. Accordingly, in this study, artificial intelligence (AI) activities are integrated into SW development processes. The important AI activities like intelligent agents, machine learning (ML), knowledge representation, statistical model, probabilistic methods, and fuzzy are integrated into the extended waterfall model. Again, AI activities like intelligent decision making, ML, Turing test, search and optimisation are integrated into the agile model. Two metrics such as, Usability Goals Achievement Metric and Index of Integration are evaluated in five independent SW projects. Once SW projects are developed using these models, feedback queries have been collected formally and the collected data are extensively analysed to identify the individual characteristics of products, identifying correlation behaviour of products with respect to model and metrics."
Software requirements as an application domain for natural language processing,"Diamantopoulos, T; Roth, M; Symeonidis, A; Klein, E",10.1007/s10579-017-9381-z,2017,"Mapping functional requirements first to specifications and then to code is one of the most challenging tasks in software development. Since requirements are commonly written in natural language, they can be prone to ambiguity, incompleteness and inconsistency. Structured semantic representations allow requirements to be translated to formal models, which can be used to detect problems at an early stage of the development process through validation. Storing and querying such models can also facilitate software reuse. Several approaches constrain the input format of requirements to produce specifications, however they usually require considerable human effort in order to adopt domain-specific heuristics and/or controlled languages. We propose a mechanism that automates the mapping of requirements to formal representations using semantic role labeling. We describe the first publicly available dataset for this task, employ a hierarchical framework that allows requirements concepts to be annotated, and discuss how semantic role labeling can be adapted for parsing software requirements."
Understanding Feature Requests by Leveraging Fuzzy Method and Linguistic Analysis,"Shi, L; Chen, C; Wang, Q; Li, SB; Boehm, B",,2017,"In open software development environment, a large number of feature requests with mixed quality are often posted by stakeholders and usually managed in issue tracking systems. Thoroughly understanding and analyzing the real intents that feature requests imply is a labor-intensive and challenging task. In this paper, we introduce an approach to understand feature requests automatically. We generate a set of fuzzy rules based on natural language processing techniques that classify each sentence in feature requests into a set of categories: Intent, Explanation, Benefit, Drawback, Example and Trivia. Consequently, the feature requests can be automatically structured based on the classification results. We conduct experiments on 2,112 sentences taken from 602 feature requests of nine popular open source projects. The results show that our method can reach a high performance on classifying sentences from feature requests. Moreover, when applying fuzzy rules on machine learning methods, the performance can be improved significantly."
A Comprehensive Investigation of Natural Language Processing Techniques and Tools to Generate Automated Test Cases,"Ahsan, I; Butt, WH; Ahmed, MA; Anwar, MW",10.1145/3018896.3036375,2017,"Natural Language Processing (NLP) techniques show promising results to organize and identify desired information from the bulky raw data. As a result, NLP techniques are continuously getting researcher's attention to automate various software development activities like test cases generation. However, selection of right NLP techniques and tools to generate automated test cases is always challenging. Therefore, in this paper, we investigate the application of NLP techniques to generate test cases from preliminary requirements document. A Systematic Literature Review (SLR) has been conducted to identify 16 research works published during 2005-2014. Consequently, 6 NLP techniques and 18 tools have been identified. Furthermore, 4 test case generation approaches and 9 NLP algorithms have also been presented. The identified NLP techniques and tools are highly beneficial for the researchers and practitioners of the domain."
A Conceptual Framework for Clone Detection using Machine Learning,"Ghofrani, J; Mohseni, M; Bozorgmehr, A",,2017,"Code clones can happen in any software project. One of the challenges is that, code clones come in various forms which makes them hard to detect using standard templates. Due to this variety in structure and form of semantically similar clones, machine learning techniques are required to detect them. Recently in many domains, e.g., natural language processing, deep neural networks drew a lot of attention due to their accuracy. In this paper, we exploit the results of some convolutional neural networks for code summarization to find the code clones. We use the generated descriptions for two code snippets as a metric to measure the similarities between them. We propose a vector similarity measure to calculate a similarity indicator between these measures which can decide which code snippets are clones."
Dragonfly Estimator: A Hybrid Software Projects' Efforts Estimation Model using Artificial Neural Network and Dragonfly Algorithm,"Yousef, QM; Alshaer, YA; Alhammad, NK",,2017,"The estimation of software development efforts has become a crucial activity in software project management. Due to this significance, a few models have been proposed so far to build a connection between the required efforts to be employed, and the software size, time schedule, budget and similar requirements. However, various holes and slips can still be noticed in software effort's estimation processes due to the lack of enough data available in the initial stage of project's lifecycle. In order to improve the accuracy of time estimation in the software industry, this work used NASA projects dataset to train and validate the proposed model, which is based on Feedforward Artificial Neural Network. Moreover, Dragonfly Algorithm was used to provide optimal training, which in consequence offered more enhanced and accurate software estimation model. Randomly selected project datasets were used to test the proposed model, which resulted in clear enhanced results in comparison to similar estimation models. Different performance criteria were used to validate and accept the hypothesis suggested by this paper that the proposed model could be used in predicting the efforts required for various types of software projects."
Natural Language Processing and Machine Learning Methods for Software Development Effort Estimation,"Ionescu, VS; Demian, H; Czibula, IG",10.24846/v26i2y201710,2017,"The growing complexity and number of software projects requires both increasingly more experienced developers, testers and other specialists as well as a larger number of persons to fill these roles. This leads to increased personnel and management costs and also makes effort and cost estimation at task and activity levels more difficult for software development companies. An automated solution for software development effort estimation based on text descriptions of tasks and activities, combined with available metrics, is introduced. A real world case study consisting of data from a software company whose activity spans a rich development spectrum is conducted. The results obtained are very encouraging and surpass the few similar approaches available in research literature."
Summarizing Software Engineering Communication Artifacts from Different Sources,"KÃ¤fer, V",10.1145/3106237.3119877,2017,"During software development, developers communicate a lot and with many different people. Communication is an important factor, to the point that communication failures are seen as the causes of productivity losses or even project failures. To communicate with each other, software developers use many different tools like mailing lists, forums, issue trackers or chats. Even in a short time span, a lot of information artifacts can arise through these channels, which can be very time consuming to get through after a long vacation or for new members of the team. This paper describes a research plan for an approach which can summarize different communication sources into one big summary using and improving existing text summarization approaches. The resulting tool would have the potential to decrease the effort needed for sense-making and comprehension of communication, as well as the time needed for locating and using information from the communication sources. This reduction in effort will result in a significant increase in the productivity of software development companies."
Let's hear it from RETTA: A Requirements Elicitation Tool for TrAffic management systems,"Noaeen, M; Abad, ZSH; Far, BH",10.1109/RE.2017.78,2017,"The area of Traffic Management (TM) is characterized by uncertainty, complexity, and imprecision. The complexity of software systems in the TM domain which contributes to a more challenging Requirements Engineering (RE) job mainly stems from the diversity of stakeholders and complexity of requirements elicitation in this domain. This work brings an interactive solution for exploring functional and non-functional requirements of software-reliant systems in the area of traffic management. We prototyped the RETTA tool which leverages the wisdom of the crowd and combines it with machine learning approaches such as Natural Language Processing and Naive Bayes to help with the requirements elicitation and classification task in the TM domain. This bridges the gap among stakeholders from both areas of software development and transportation engineering. The RETTA prototype is mainly designed for requirements engineers and software developers in the area of TM and can be used on Android-based devices."
Review of Development and Construction of Uyghur Knowledge Graph,"Qiu, LR; Zhang, HL",10.1109/CSE-EUC.2017.181,2017,"Knowledge graph technology belongs to the field of artificial intelligence. It is widely used in semantic search and intelligent question answering. Construction of Uyghur's knowledge graph has the great value of Uyghur information processing and Uyghur application software development. Firstly, this paper describes the definition and structure of the knowledge graph, then it reviews the related research and construction process of the Uyghur's knowledge graph, and analyses the characteristics of word formation of Uighur entities in depth, focuses on the related concepts and methods of knowledge extraction, knowledge fusion, and knowledge update, Introduced the Uyghur semantic disambiguation technology in detail, Finally, looking forward to the future works."
From User Demand to Software Service: Using Machine Learning to Automate the Requirements Specification Process,"van Rooijen, L; BÃ¤umer, FS; Platenius, MC; Geierhos, M; Hamann, H; Engels, G",10.1109/REW.2017.26,2017,"Bridging the gap between informal, imprecise, and vague user requirements descriptions and precise formalized specifications is the main task of requirements engineering. Techniques such as interviews or story telling are used when requirements engineers try to identify a user's needs. The requirements specification process is typically done in a dialogue between users, domain experts, and requirements engineers. In our research, we aim at automating the specification of requirements. The idea is to distinguish between untrained users and trained users, and to exploit domain knowledge learned from previous runs of our system. We let untrained users provide unstructured natural language descriptions, while we allow trained users to provide examples of behavioral descriptions. In both cases, our goal is to synthesize formal requirements models similar to statecharts. From requirements specification processes with trained users, behavioral ontologies are learned which are later used to support the requirements specification process for untrained users. Our research method is original in combining natural language processing and search-based techniques for the synthesis of requirements specifications. Our work is embedded in a larger project that aims at automating the whole software development and deployment process in envisioned future software service markets."
Learning to predict characteristics for engineering service projects,"Shi, L; Newnes, L; Culley, S; Allen, B",10.1017/S0890060416000470,2017,"An engineering service project can be highly interactive, collaborative, and distributed. The implementation of such projects needs to generate, utilize, and share large amounts of data and heterogeneous digital objects. The information overload prevents the effective reuse of project data and knowledge, and makes the understanding of project characteristics difficult. Toward solving these issues, this paper emphasized the using of data mining and machine learning techniques to improve the project characteristic understanding process. The work presented in this paper proposed an automatic model and some analytical approaches for learning and predicting the characteristics of engineering service projects. To evaluate the model and demonstrate its functionalities, an industrial data set from the aerospace sector is considered as a the case study. This work shows that the proposed model could enable the project members to gain comprehensive understanding of project characteristics from a multidimensional perspective, and it has the potential to support them in implementing evidence-based design and decision making."
A Novel Natural Language Processing (NLP) Approach to Automatically Generate Conceptual Class Model from Initial Software Requirements,"Ahmed, MA; Butt, WH; Ahsan, I; Anwar, MW; Latif, M; Azam, F",10.1007/978-981-10-4154-9_55,2017,"Conceptual class model is an essential design artifact of Software Development Life Cycle (SDLC). The involvement of several resources and additional time is required to generate the class model from early software requirements. On the other hand, Natural Language Processing (NLP) is a knowledge discovery approach to automatically extract elements of concern from initial plain text documents. Consequently, it is frequently utilized to generate various SDLC artifacts like class model from the early software requirements. However, it is usually required to perform few manual processing on textual requirements before applying NLP techniques that makes the whole process semi-automatic. This article presents a novel fully automated NLP approach to generate conceptual class model from initial software requirements. As a part of research, Automated Requirements 2 Design Transformation (AR2DT) tool is developed. The validation is performed through three benchmark case studies. The experimental results prove that the proposed NLP approach is fully automated and considerably improved as compared to the other state-of-the-art approaches."
Leveraging Business Process Improvement with Natural Language Processing and Organizational Semantic Knowledge,"Iren, D; Reijers, HA",10.1145/3084100.3084112,2017,"Contemporary organizations need to adapt their business processes swiftly to cope with ever-changing requirements. Requirement changes originate from a wide variety of sources. Business analysts gather these requirements, resolve conflicts, analyze impacts, and prepare actionable improvement plans. These tasks require a comprehensive knowledge of business processes and other entities within the organization. Business process model repositories, which may contain hundreds of models, are important sources of such cross-functional information. In this study, we introduce an approach which facilitates business process improvement by utilizing the comprehensive information covered by process models. Specifically, we associate requirements with other organizational entities based on their transitive relations with process models. To infer these associations, our approach makes use of natural language processing techniques and enterprise semantics. A quantitative evaluation of our approach, which took place within a major telecommunication company, displayed that it accurately detects associations between requirements and process models. Furthermore, semi-structured interviews with business analysts revealed that their expectations are high on efficiency increases due to the usage of this approach."
Understanding Task Interruptions in Service Oriented Software Development Projects: An Exploratory Study,"Abad, ZSH; Ruhe, G; Bauer, M",10.1109/SER-IP.2017..5,2017,"The Service Oriented Software Development (SOSD) approach is a common software development paradigm. Previous qualitative and quantitative studies looked at the main reasons for the delay in software development so as to help project's stakeholders to take appropriate actions for improvement in their planning. In SOSD projects, due to the high level of user involvement in new service and product development, service providers need to make dynamic trade-offs to address their clients' demands. In this paper, we look at interruptions and their impact on tasks duration. We used text classification, Natural Language Processing (NLP), and quantitative time series analysis techniques to analyze 7, 770 development tasks of five real SOSD projects at Arcurve Inc. Our results show that fixing an issue, addressing changes, and adding new features are the most frequently perceived causes of interruption in SOSD projects. Furthermore, we have found that requirements engineering and project management tasks experience less delay time over a project's life-cycle. We also visualized the association between the interruption length and the extra tasks' duration resulted from these interruptions within various development task types, which shows there is no strong association between task type with regard to these two parameters."
Extracting UML Class Diagrams from Software Requirements in Thai using NLP,"Jaiwai, M; Sammapun, U",,2017,"In software development, requirements, normally written in natural language, are documents that specify what users want in software products. Software developers then analyze these requirements to create domain models represented in UML diagrams in an attempt to comprehend what users need in the software products. These domain models are usually converted into design models and finally carried over into classes in source code. Thus, domain models have an impact on the final software products. However, creating correct domain models can be difficult when software developers are not skilled. Moreover, even for skilled developers, when requirements are large, wading through all requirements to create domain models can take times and might result in errors. Therefore, researchers have studied various approaches to apply natural language processing techniques to transform requirements written in natural language into UML diagrams. Those researches focus on requirements written in English. This paper proposes an approach to process requirements written in Thai to extract UML class diagrams using natural language processing techniques. The UML class diagram extraction is based on transformation rules that identify classes and attributes from requirements. The results are evaluated with recall and precision using truth values created by humans. Future works include identifying operations and relationships from requirements to complete class diagram extraction. Our research should benefit Thai software developers by reducing time in requirement analysis and also helping novice software developers to create correct domain models represented in UML class diagram."
JSEA: A Program Comprehension Tool Adopting LDA-based Topic Modeling,"Wang, TX; Liu, Y",,2017,"Understanding a large number of source code is a big challenge for software development teams in software maintenance process. Using topic models is a promising way to automatically discover feature and structure from textual software assets, and thus support developers comprehending programs on software maintenance. To explore the application of applying topic modeling to software engineering practice, we proposed JSEA (Java Software Engineers Assistant), an interactive program comprehension tool adopting LDA-based topic modeling, to support developers during performing software maintenance tasks. JSEA utilizes essential information automatically generated from Java source code to establish a project overview and to bring search capability for software engineers. The results of our preliminary experimentation suggest the practicality of JSEA."
Process Model Proposal for Requirements Engineering in Information Mining Projects,"Pollo-Cattaneo, MF; Pesado, P; Britos, P; GarcÃ­a-MartÃ­nez, R",10.1007/978-3-319-66562-7_10,2017,"Information Mining Projects provide synthesis and analysis tools which allow the available data of an organization to be transformed into useful knowledge for the decision-making process. It is for this reason that requirements of this type of project are different from requirements of traditional projects for software development. Consequently, processes associated with requirements engineering for this type of project cannot be reused in Information Mining projects. Likewise, available methodologies for these last projects leave aside activities associated with the Requirements Management of stakeholders and customers. In this context, a model giving solution to the necessities of managing project Information Mining requirements is offered."
A Miniature Smart Home Testbed for Research and Education,"Nguyen, T; Lakshunanan, B; Lin, CJJ; Sheng, WH; Gu, Y; Liu, MQ; Zhang, SL",,2017,"In this paper, we present a cloud-based smart home testbed using a miniature doll house. The testbed consists of a smart home server, a home controller, a smart home assistant, appliances, and clients communicating with each other using web sockets and existing social network software development kits (SDKs). Particularly, we use this testbed to demonstrate energy conservation with the interconnected network of devices. The proposed testbed allows research and education in areas such as smart grid, wireless sensor network, machine learning, pattern recognition, embedded programming, natural language processing, social media, etc."
HOW SCRUM IMPROVES PROJECT-BASED COURSEWORK AT UNIVERSITY,"Babic, F",,2017,"In this paper, we describe our experience and knowledge from applying the Scrum methodology into the learning process of a study program called Business informatics at Department of cybernetics and artificial intelligence. It combines informatics like software development or data analytics with economical knowledge, such as financial markets trading, enterprise economic analysis or e-commerce. This combination produces valuable graduates for the IT market consisting of cooperating companies from a cluster named Kosice IT Valley. Since some of them started to use Scrum in their development processes, they welcomed our effort to familiarize students with this approach. From our point of view, Scrum helps our students to improve their study results and various personal skills."
NATURAL LANGUAGE PROCESSING TECHNIQUES USED FOR AN AUTOMATIZED TEST GENERATION PROCESS FOR TURKISH,"Sari, ÃC; Aktas, Ã",,2017,"This paper is about a software project which includes the processing of text-based Turkish lecture notes of secondary education students and automatic test generation. Main objective here is to provide a computer mediated self-study opportunity while simplifying an examination process by isolating students from the question preparation. Besides the educational software developed, this research draws conclusions about some of the major NLP (Natural Language Processing) tasks for Turkish; like document classification, sentence boundary detection, morphological analysis, POS Part-of-Speech) tagging, classification of verbs as positive or negative, finding phrases of sentences."
Design and Development of Certification Compliance Tool for Airborne Systems,"Anusha, BK; Nanda, M; Jayanthi, J",,2017,"Certification compliance check for airborne software is very critical as it aids in the certification of the software. Since this compliance check is performed manually which is time-consuming and erroneous, an in-house developed Certification Compliance Tool (CCT) helps in checking the compliance as per RTCA DO-178B/C and generate artifacts depicting the magnitude of compliance. In order to generate the magnitude of compliance for the artifacts with respect to the Civil Aerospace Certification standard, RTCA DO-178B/C, an effective parsing technique is required to be incorporated to parse the artifact/s and generate compliance metric for the artifact/s. In this paper we propose a novel approach used in the design and development of an effective and efficient parsing technique incorporated in the indigenous software tool CCT used for compliance check. The tool checks the ratio of compliance of the artifacts generated across various phases of Software Development Life Cycle (SDLC) process involved in the development of Safety Critical software as per RTCA DO-178B/C. The indigenous tool accepts these artifacts as inputs and based on the software criticality level, it analyzes the compliance of these artifacts with the guidelines provided and recommended by RTCA DO-178B/C. The output of the tool provides the percentage of compliance of the artifacts that helps in accessing the Certification capabilities of the developed software. The percentage of compliance predicts the acceptance or rejection probabilities of the software being certified by the Certification Agency. The certification parser is developed using Python modules like Pywin32, Pypdf parsers and different approaches for Natural language processing using Python Natural Language Toolkit (NLTK). The in-house tool replaces the manual effort by an individual/s which may be erroneous and impact the time-schedule, which compromises the software safety. The integration of the tool with commercial tools will help in analyzing the report/documentation content with respect to the certification."
CoMSS: Context based Measure for Semantic Similarity between Conceptual models,"Kaundal, A; Kaur, A",,2017,"Artifacts are defined as the tools which act as building block for developing a software or project. Artifacts are of several kinds like UML and BPMN. The overall connotation of the document in SDLC is combination of sensible or significant words represented in the form of different artifacts. In this approach main focus is on the precision of the documents or artifacts developed by requirement analysts. This phase is purely personal opinion oriented, later on which may cause rejection of the software during acceptance testing. Our aim is to preserve the overall meaning of the document that's why we compare the class diagram (conceptual model) with BPMN so that the meaning of the requirements provided by the stakeholders should be preserved in both forms of artifacts. The word specificity and word semantics plays vital role in assessing the semantic similarity between the two artifacts. In this paper the frequency (word specificity) is calculated between the vectors with the help of TF-IDF and the semantic similarity between the vectors is calculated with the help of WordNet algorithms. The proposed similarity measures are evaluated in divergent context, the benchmark dataset i.e SemEval (Semantic Evaluation) 2012 Semantic Textual Similarity test set, competition organized in 2012. Three standard case studies are used to evaluate the semantic similarity between the artifacts named as Rambaugh's ATM Model (Rambaugh et al. 1991), EFP (Kurt, 1995), Course Registration (IBM Corp, 2004)"
Automatic Grading of Short Answers Using Large Language Models in Software Engineering Courses,"Duong, TNB; Meng, CY",10.1109/EDUCON60312.2024.10578839,2024,"Short-answer based questions have been used widely due to their effectiveness in assessing whether the desired learning outcomes have been attained by students. However, due to their open-ended nature, many different answers could be considered entirely or partially correct for the same question. In the context of computer science and software engineering courses where the enrolment has been increasing recently, manual grading of short-answer questions is a time-consuming and tedious process for instructors. In software engineering courses, assessments concern not just coding but many other aspects of software development such as system analysis, architecture design, software processes and operation methodologies such as Agile and DevOps. However, existing work in automatic grading/scoring of text-based answers in computing courses have been focusing more on coding-oriented questions. In this work, we consider the problem of autograding a broader range of short answers in software engineering courses. We propose an automated grading system incorporating both text embedding and completion approaches based on recently introduced pre-trained large language models (LLMs) such as GPT-3.5/4. We design and implement a web-based system so that students and instructors can easily leverage autograding for learning and teaching. Finally, we conduct an extensive evaluation of our automated grading approaches. We use a popular public dataset in the computing education domain and a new software engineering dataset of our own. The results demonstrate the effectiveness of our approach, and provide useful insights for further research in this area of AI-enabled education."
Glucose Metabolic Profile by Visual Assessment Combined with Statistical Parametric Mapping Analysis in Pediatric Patients with Epilepsy,"Zhu, YK; Feng, JH; Wu, S; Hou, HF; Ji, JF; Zhang, K; Chen, Q; Chen, L; Cheng, HY; Gao, LY; Chen, ZX; Zhang, H; Tian, M",10.2967/jnumed.116.187492,2017,"PET with F-18-FDG has been used for presurgical localization of epileptogenic foci; however, in nonsurgical patients, the correlation between cerebral glucose metabolism and clinical severity has not been fully understood. The aim of this study was to evaluate the glucose metabolic profile using F-18-FDG PET/CT imaging in patients with epilepsy. Methods: One hundred pediatric epilepsy patients who underwent F-18-FDG PET/CT, MRI, and electroencephalography examinations were included. Fifteen age-matched controls were also included. F-18-FDG PET images were analyzed by visual assessment combined with statistical parametric mapping (SPM) analysis. The absolute asymmetry index (jAIj) was calculated in patients with regional abnormal glucose metabolism. Results: Visual assessment combined with SPM analysis of F-18-FDG PET images detected more patients with abnormal glucose metabolism than visual assessment only. The jAIj significantly positively correlated with seizure frequency (P < 0.01) but negatively correlated with the time since last seizure (P < 0.01) in patients with abnormal glucose metabolism. The only significant contributing variable to the vertical bar AI vertical bar was the time since last seizure, in patients both with hypometabolism (P = 0.001) and with hypermetabolism (P = 0.005). For patients with either hypometabolism (P < 0.01) or hypermetabolism (P = 0.209), higher jAIj values were found in those with drug resistance than with seizure remission. In the post-1-y follow-up PET studies, a significant change of jAIj (%) was found in patients with clinical improvement compared with those with persistence or progression (P < 0.01). Conclusion: F-18-FDG PET imaging with visual assessment combined with SPM analysis could provide cerebral glucose metabolic profiles in nonsurgical epilepsy patients. jAIj might be used for evaluation of clinical severity and progress in these patients. Patients with a prolonged period of seizure freedom may have more subtle (or no) metabolic abnormalities on PET. The clinical value of PET might be enhanced by timing the scan closer to clinical seizures."
"Uncharted dimensions, gaps, and future trends of serious games in software engineering","Kharbouch, M; Vizcaino, A; GarcÃ­a-BernÃ¡ , J; GarcÃ­a, F; Toval, A; Pedreira, O; Idri, A; FernÃ¡ndez-AlemÃ¡n, J",10.1016/j.csi.2024.103915,2025,"Objective: Serious Games (SG) are a rising trend in Software Engineering (SE) education, for this reason, and since this topic is still immature and further research was encouraged, it is important to investigate how SGs are integrated into SE education. In this line, this study explores the landscape of SGs in SE) education, focusing on their categorization according to their addressed SWEBOK areas and Bloom's levels, extracted their key elements, mechanics and dynamics, exploring in depth their most portrayed player profiles, finding what makes them successful SGs in this field, and last addressing their resulting challenges in the realm of SE education. Methodology: A systematic search was conducted across prominent databases: Science Direct, IEEE Xplore, ACM, Scopus, and Wiley. Initially, 125 papers met our initial inclusion criteria, from which 46 remained after rigorous full-text review. Utilizing snowball sampling, we added 28 additional studies, resulting in a total of 74 selected papers for comprehensive analysis. Results: Among the selected papers, which spanned from the early 2000s to May 2021, a notable increase in publications on SGs in SE was observed, particularly since 2010. The majority of these studies focused on validation research (60 %), followed by solution proposals (17.56 %) and evaluation research (13.51 %). Publication channels predominantly included conferences (79.73 %), underscoring the emerging nature of SGs in SE research, with a smaller proportion appearing in journal articles (20.27 %). Specific focus areas within SE, such as Software Engineering Management (33.78 %) and SE Professional Practice (13.51 %), received significant attention, while others, like SE Models and Methods, showed minimal representation. Furthermore, SGs were found to effectively target higher-order cognitive skills based on Bloom's Taxonomy, with notable implementations of game dynamics such as Teams and Realism to enhance learning experiences. Despite these advancements, there remains a predominant focus on player profiles like Achievers (48.64 %) and Players (47.30 %), suggesting potential gaps in addressing a broader spectrum of learner types within SGs designed for SE education. Conclusion: This study underscores the evolving role of SGs in SE education, emphasizing the need for diverse approaches to enhance engagement and educational outcomes. Future research should focus on optimizing SG potential across educational and industrial settings by expanding publication visibility, integrating artificial intelligence (AI), and conducting comprehensive evaluations of SGs tailored to SE contexts."
Open Source Code Contributions to Global Health: The Case of Antimalarial Drug Discovery,"Turon, G; Tse, E; Qiu, X; Todd, M; Duran-Frigola, M",10.1021/acsmedchemlett.4c00131,2024,"The discovery of treatments for infectious diseases that affect the poorest countries has been stagnant for decades. As long as expected returns on investment remain low, pharmaceutical companies' lack of interest in this disease area must be compensated for with collaborative efforts from the public sector. New approaches to drug discovery, inspired by the open source philosophy prevalent in software development, offer a platform for experts from diverse backgrounds to contribute their skills, enhancing reproducibility, progress tracking, and public discussion. Here, we present the first efforts of Ersilia, an initiative focused on attracting data scientists into contributing to global health, toward meeting the goals of Open Source Malaria, a consortium of medicinal chemists investigating antimalarial compounds using a purely open science approach. We showcase the chemical space exploration of a set of triazolopyrazine compounds with potent antiplasmodial activity and discuss how open source practices can serve as a common ground to make drug discovery more inclusive and participative."
KubePlaybook: A Repository of Ansible Playbooks for Kubernetes Auto-Remediation with LLMs,"Sarda, K; Namrud, Z; Litoiu, M; Shwartz, L; Watts, I",10.1145/3629527.3653665,2024,"In the evolving landscape of software development and system operations, the demand for automating traditionally manual tasks has surged. Continuous operation and minimal downtimes highlight the need for automated detection and remediation of runtime anomalies. Ansible, known for its scalable features, including high-level abstraction and modularity, stands out as a reliable solution for managing complex systems securely. The challenge lies in creating an on-the-spot Ansible solution for dynamic auto-remediation, requiring a substantial dataset for in-context tuning of large language models (LLMs). Our research introduces KubePlaybook, a curated dataset with 130 natural language prompts for generating automation-focused remediation code scripts. After rigorous manual testing, the generated code achieved an impressive 98.86% accuracy rate, affirming the solution's reliability and performance in addressing dynamic auto-remediation complexities."
Natural Language Processing For Requirement Elicitation In University Using Kmeans And Meanshift Algorithm,"Bernanda, DY; Jawawi, DNA; Abd Halim, S; Adikara, F",10.21123/bsj.2024.9675,2024,"Data Driven Requirement Engineering (DDRE) represents a vision for a shift from the static traditional methods of doing requirements engineering to dynamic data-driven user-centered methods. Data available and the increasingly complex requirements of system software whose functions can adapt to changing needs to gain the trust of its users, an approach is needed in a continuous software engineering process. This need drives the emergence of new challenges in the discipline of requirements engineering to meet the required changes. The problem in this study was the method in data discrepancies which resulted in the needs elicitation process being hampered and in the end software development found discrepancies and could not meet the needs of stakeholders and the goals of the organization. The research objectives in this research to the process collected and integrating data from multiple sources and ensuring interoperability. Conclusion in this research is determining is the clustering algorithm help the collection data and elicitation process has a somewhat greater impact on the ratings provided by professionals for pairs that belong to the same cluster. However, the influence of POS tagging on the ratings given by professionals is relatively consistent for pairs within the same cluster and pairs in different clusters."
Mobile-Optimized Real-Time Vessel Detection for Ultrasound-Guided Surgical Procedures,"Wolak, M; Amin, F; DeLosa, N; Telfer, B; Roop, B; Gjesteby, L",10.1109/HPEC62836.2024.10938431,2024,"Non-compressible torso hemorrhage is the leading cause of potentially survivable fatalities in civilian and battlefield trauma. An insufficient number of trauma surgeons are expected to be available in future large-scale combat operations and natural disasters, creating a need for assistive technology to enable fast and accurate vascular access in pre-hospital environments. AI-GUIDE is a handheld surgical tool designed for emergency medical operations that combines an ultrasound probe with real-time image processing software, which controls robotic needle insertion components. Currently, AI-GUIDE relies on an external display/computer to perform the required computations. To reduce its size and weight, we investigate optimizations for mobile inference of the AI algorithms used in the AI-GUIDE prototype. Key optimizations include the use of mobile-optimized neural network models, quantization techniques, and leveraging a software development kit to harness portable hardware acceleration. We compare the trade-offs between speed and accuracy for different runtime configurations, quantization methods, and model sizes for two resource-conscious neural networks. We run our experiments on a smartphone as a reliable proxy for performance on the AI-GUIDE."
A Systematic Evaluation of Code-generating Chatbots for Use in Undergraduate Computer Science Education,"Torek, A; Sorensen, E; Hahle, N; Kennington, C",10.1109/FIE61694.2024.10893165,2024,"This research paper focuses on evaluating code-generating chatbots. Chatbots like ChatGPT released in the past three years have proven capable of a wide variety of tasks within a conversational interaction, including writing code and answering code-related questions. With these recent advances, chatbots have many potential uses in education, including computer science education. However, before these chatbots are used in CS curricula, their capabilities and limitations must be systematically tested and understood. In this work, we evaluate the capabilities and limitations of four known, open-source, code-based chatbots in programming tasks by performing a standardized study in which different chatbots are tasked with providing answers for a variety of assignments from Boise State University's computer science program. We found that while all of the chatbots can write code and provide explanations, some do better than others, and each of them work differently in conversations. Moreover, all of them suffered similar and important limitations, which has implications for adoption in curriculum. As a second experiment, we used the Llama chatbot to perform a human evaluation by enabling student novice and experienced programmers to use it as a coding assistant to complete specific tasks in a common software development environment. We found that the coding assistant can help novice programmers accomplish simple tasks in comparable time and code efficacy as more experienced programmers. Given these experiments, and given feedback from participants in our studies, we see a clear picture emerge: new programmers should learn important concepts about programming without the help of code assistants so students can (1) demonstrate their understanding of important concepts and (2) have enough experience to assess code assistant output as useful or erroneous. Then, once intermediate skills are mastered (e.g., object oriented programming and data structures), it seems appropriate to introduce students systematically to coding assistants to help with specific assignments throughout the undergraduate computer science curriculum. We conclude by addressing ethical considerations for the use of code-based chatbots in computer science education and future directions of research."
SNN eXpress: Streamlining Low-Power AI-SoC Development With Unsigned Weight Accumulation Spiking Neural Network,"Jang, H; Han, K; Oh, KI; Lee, S; Lee, JJ; Lee, W",10.4218/etrij.2024-0114,2024,"SoCs with analog-circuit-based unsigned weight-accumulating spiking neural networks (UWA-SNNs) are a highly promising solution for achieving low-power AI-SoCs. This paper addresses the challenges that must be overcome to realize the potential of UWA-SNNs in low-power AI-SoCs: (i) the absence of UWA-SNN learning methods and the lack of an environment for developing applications based on trained SNN models and (ii) the inherent issue of testing and validating applications on the system being nearly impractical until the final chip is fabricated owing to the mixed-signal circuit implementation of UWA-SNN-based SoCs. This paper argues that, by integrating the proposed solutions, the development of an EDA tool that enables the easy and rapid development of UWA-SNN-based SoCs is feasible, and demonstrates this through the development of the SNN eXpress (SNX) tool. The developed SNX automates the generation of RTL code, FPGA prototypes, and a software development kit tailored for UWA-SNN-based application development. Comprehensive details of SNX development and the performance evaluation and verification results of two AI-SoCs developed using SNX are also presented."
INTEGRATED PROCESS AND DATA MODEL FOR APPLYING SCENARIO-TECHNIQUE IN REQUIREMENTS ENGINEERING,"Graessler, I; Scholle, P; Pottebaum, J",,2017,"Originating from strategic management, scenario-technique yields potentials for requirements engineering. In this paper an integrated process model for such an application of scenario-technique is proposed. Flanked by an Integrated Scenario Data Model (ISDM), efficient prognosis of changes in complex requirements models is facilitated. The ISDM support this process by interlinking scenario data, requirements management data and additional data sources such as PDM/PLM systems. Scenarios are interpreted as results of interrelations among requirements. Combined with consistency assessment, the anticipation of potential future changes in requirements is facilitated, reducing potential risks for the product development process. Alongside the product development process, developers can develop reaction strategies for changes of requirements. The ISDM reduces the required effort for scenario derivation significantly by integrating data analytics and semantic modelling. In addition, the combination of process and data model allows efficient adaptations of scenarios to depict the dynamics of requirements. Intuitivism of derived scenarios is enhanced by the proposed approach."
Multi-Fidelity Aerodynamic Methods for the Analysis of Propeller Wing Interaction,"Li, ZN; Maltsev, V; Modarres, AH; Da Ronch, A",10.2514/6.2024-3521,2024,"The propeller-wing aerodynamic interaction is investigated with multi-fidelity methods including Lifting Line Method (LLM), Surface Panel Method (SPM), and Vortex Particle Method (VPM). High-fidelity results from in-house unsteady RANS computations and experiments are used as a reference. Two open-source codes FlowUnsteady and DUST are used to carry out low to mid fidelity simulations. For the computations performed with FlowUnsteady, the lifting surfaces are modeled using actuator line model or actuator surface model, while the propeller wake and wing wake are modeled using a reformulated VPM. With DUST, the lifting surfaces are modelled using surface panels and the wake is modelled by classic VPM. A clean wing, an isolated beaver propeller and a wing-mounted propeller are used as benchmarks for comparing the performance of the faster methods with uRANS and experiments. For simpler test cases such as the clean wing, a comparable accuracy is found for the prescribed wake and VPM wake, while the simulation time of the former is orders of magnitude lower than the latter. For the isolated propeller, the LLM that couples with VPM wake can predict thrust coefficient in the order of 10 minutes with less than 5% deviation compared to experiment data, while with high fidelity uRANS, more than 20 hours are needed. When the wing wake and propeller wake interaction is expected to have an impact on lift, drag and pressure distribution, the middle-fidelity VPM recovers much of the physics lost with lower fidelity methods, yet providing a good compromise between computational costs and accuracy."
Energy Efficiency and Classification Locality: Pareto-optimal trade-offs in multi-class sensor-based Human Activity Recognition,"Hoof, T; Buchwitz, B",,2024,"Human Activity Recognition (HAR) is one of the central analytical workloads on wearable devices and effective solutions often require methods from machine learning or artificial intelligence to perform activity classification. Contrary to the hardware in wearable devices, these algorithms have mainly been developed without strict constraints on processing power, memory footprint, or storage space. Wearable applications, therefore, need to simultaneously balance at least the energy consumption of the device and the predictive performance of the algorithms. A plethora of software development kits and frameworks aim to bring those algorithms to smaller ultra-low power Systems-on-Chip (SoCs) and promise efficient execution of analytical workloads. In this study, we provide a holistic view of hardware, algorithms, and software that is useful to build smart wearable devices and provide guidance to researchers and practitioners for the selection of algorithms, configurations, and toolsets that, in combination, provide a Pareto-optimal trade-off between energy consumption and classification performance."
CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios,"Zeng, ZR; Wang, YD; Xie, R; Ye, W; Zhang, SK",10.1145/3650212.3652115,2024,"In the evolving landscape of large language models (LLMs) tailored for software engineering, the need for benchmarks that accurately reflect real-world development scenarios is paramount. Current benchmarks are either too simplistic or fail to capture the multi-tasking nature of software development. To address this, we introduce CoderUJB, a new benchmark designed to evaluate LLMs across diverse Java programming tasks that are executable and reflective of actual development scenarios, acknowledging Java's prevalence in real-world software production. CoderUJB comprises 2,239 programming questions derived from 17 real open-source Java projects and spans five practical programming tasks. Our empirical study on this benchmark investigates the coding abilities of various open-source and closed-source LLMs, examining the effects of continued pre-training in specific programming languages code and instruction fine-tuning on their performance. The findings indicate that while LLMs exhibit strong potential, challenges remain, particularly in non-functional code generation (e.g., test generation and defect detection). Importantly, our results advise caution in the specific programming languages continued pre-training and instruction fine-tuning, as these techniques could hinder model performance on certain tasks, suggesting the need for more nuanced strategies. CoderUJB thus marks a significant step towards more realistic evaluations of programming capabilities in LLMs, and our study provides valuable insights for the future development of these models in software engineering."
An Intelligent Tutoring System Proposal Based on Chatbot and Learning Styles to the Project Management Study,"Silva, EFF; Maia, DM; dos Santos, SC; Santana, AF",10.1109/FIE61694.2024.10893385,2024,"This research full paper proposes an intelligent tutoring system (ITS) to support the study of software project management, considering the students' learning styles. The learning process is a complex and multifaceted phenomenon that varies significantly from one person to another. Everyone absorbs knowledge differently because each has learning characteristics that directly impact the learning process. Ignoring these differences can result in inefficient and demotivating teaching. Therefore, teachers must invest time in understanding each student's learning style, strengths, and weaknesses, which can be overwhelming, especially in classrooms with a high student-teacher ratio. In this scenario, AI-based technology can be a strong ally. For example, chatbot technology offers an opportunity to adapt to these individual differences and to enable a more engaging and friendly approach for students, contributing to an active and interactive learning environment when exercising the role of a study companion and, why not, a bit of study guide. In this context, this paper proposes developing and applying a chatbot-based ITS called Juh to identify each student's learning style and, based on this understanding, present study activities appropriate to their learning style. The ITS uses meaningful learning theory to personalize the learning journey, detecting five profiles: active, constructive, intentional, authentic, and cooperative. This study chose project management (PM) as its knowledge area, particularly project management methodologies, aiming to evaluate the benefits of this research proposal. In this context, this research proposes to answer the following research question: RQ) How can the study of project management be supported actively and interactively, considering the students' learning styles? As its main contribution, the solution aims to offer support in the introduction to the study of project management and improve the learning experience according to the individual characteristics of each student, maximizing the assimilation of knowledge. Therefore, the difference lies in exploring how and what to learn. The solution was developed using the Design Science Research method and the low code Blip platform. After prototyping and evaluating the ITS with PM and education experts, promising points about the approach were observed, with users highlighting improvements in productivity and ease of use of the tutoring system."
Securing Code With Context: Enhancing Vulnerability Detection Through Contextualized Graph Representations,"Rozi, MF; Ban, T; Ozawa, S; Yamada, A; Takahashi, T; Inoue, D",10.1109/ACCESS.2024.3467180,2024,"Detecting source code vulnerabilities is a critical challenge in secure software development. Early identification of vulnerabilities ensures that software performance and security remain uncompromised. However, existing vulnerability detection methods often struggle to capture the semantic meaning of source code, particularly for vulnerability types that require a deeper understanding of code flow and context. This work addresses this challenge by introducing ContextCPG, a novel enhancement of the code property graph (CPG) representation. ContextCPG augments the CPG by incorporating additional information about variable names and data types within the source code. Our approach combines natural language processing analysis with graph-based analysis to capture a richer context surrounding the source code, relying on both the structural features of the graph representation and the naming, type, and value of nodes as natural language analysis. These additional features enhance the capability of graph neural network models to capture the semantic meaning of the source code and better detect vulnerabilities. We evaluate ContextCPG by applying it to three selected C/C++ vulnerabilities (buffer overflow, invalid input, and use-after-free) and comparing its performance against CPGs. The evaluation results reveal that ContextCPG consistently outperforms the CPG on all vulnerability types, demonstrating an average accuracy increase of 8%. ContextCPG showcases the value of providing supplementary information within the graph representation, consistently enhancing vulnerability detection efficacy."
Evaluating Text Generation Model Performance by Combining Semantic Meaning and Word Order,"Novak, E; Bizjak, L; Mladenic, D; Grobelnik, M",10.1109/ACCESS.2024.3426082,2024,"Modern text generation metrics use semantic representations of words to assess the quality of a text generation model without considering the fluency of the generated text. This paper proposes a novel text generation metric that combines adequacy and fluency to measure the quality of the generated text. When computing the final score using optimal transport, the metric considers semantic meaning and word order. We evaluate the metric on text translation data sets consisting of 20 language pairs from various language families and scripts. Using a novel statistic for measuring word order sensitivity, we analyze its adequacy-based performance using Pearson's r and Kendall's $\tau $ correlation coefficients and their sensitivity to fluency-related modifications. Results show that the proposed metric is the most sensitive to fluency-related changes among all top-performing embedding-based metrics, which were found to be relatively invariant to variations in word order. The proposed metric's overall adequacy-based performance is lower than the best embedding-based metric but higher than the n-gram matching metrics. Our code is publicly available on GitHub (https://github.com/eriknovak/metric-OPWScore) under the BSD-2-Clause license."
Debugging with Open-Source Large Language Models: An Evaluation,"Majdoub, Y; Ben Charrada, E",10.1145/3674805.3690758,2024,"Large language models have shown good potential in supporting software development tasks. This is why more and more developers turn to LLMs (e.g. ChatGPT) to support them in fixing their buggy code. While this can save time and effort, many companies prohibit it due to strict code sharing policies. To address this, companies can run open-source LLMs locally. But until now there is not much research evaluating the performance of open-source large language models in debugging. This work is a preliminary evaluation of the capabilities of open-source LLMs in fixing buggy code. The evaluation covers five open-source large language models and uses the benchmark DebugBench which includes more than 4000 buggy code instances written in Python, Java and C++. Open-source LLMs achieved scores ranging from 43.9% to 66.6% with DeepSeek-Coder achieving the best score for all three programming languages."
ROOT: Requirements Organization and Optimization Tool,"Dearstyne, KR; Rodriguez, AD; Cleland-Huang, J",10.1109/ICSME58944.2024.00096,2024,"Software engineering practices such as constructing requirements and establishing traceability help ensure systems are safe, reliable, and maintainable. However, they can be resource-intensive and are frequently underutilized. To alleviate the burden of these essential processes, we developed the Requirements Organization and Optimization Tool (ROOT). ROOT centralizes project information and offers project visualizations and AI-based tools designed to streamline engineering processes. With ROOT's assistance, engineers benefit from improved oversight and early error detection, leading to the successful development of software systems. Link to screen cast: https://youtu.be/3rtMYRnsu24"
Imaging of Cervical Spine Trauma,"Warstadt, M; Winegar, B; Shah, LM",10.1097/BSD.0000000000001677,2024,"Imaging of cervical spine trauma most commonly begins with computed tomography (CT) for initial osseous and basic soft tissue evaluation, followed by magnetic resonance imaging (MRI) for complementary evaluation of the neural structures (i.e., spinal cord, nerves) and soft tissues (i.e., ligaments). Although CT and conventional MRI sequences have been the mainstay of trauma imaging for decades, there have been significant advances in CT processing, imaging sequences and techniques made possible by hardware and software development, and artificial intelligence. These advancements may provide advantages in increasing sensitivity for detection of pathology as well as in decreasing imaging and interpretation time. Unquestionably, the most important role of imaging is to provide information to help direct patient care, including diagnosis, next steps in treatment plan, and prognosis. As such, there has been a growing body of research investigating the clinical relevance of imaging findings to clinical outcomes in the setting of spinal cord injury. This article will focus on these recent advances in imaging of cervical spinal trauma."
A dance movement quality evaluation model using transformer encoder and convolutional neural network,"Qu, JP",10.1038/s41598-024-83608-9,2024,"The purpose of this study is to put forward a new evaluation model of dance movement quality to deal with the subjectivity and inconsistency in traditional evaluation methods. In view of the complexity and diversity of dance art and the widespread popularity of dance videos on social media, it is particularly urgent to develop an automatic and efficient tool for evaluating the quality of dance movements. Therefore, this study puts forward the Transformer Convolutional Neural Network with Dynamic and Static Streams (TransCNN-DSSS) model, which combines the analysis of dynamic flow and static flow, and makes use of the advantages of Transformer and Convolutional Neural Network (CNN) to deeply analyze and evaluate the dance movements. The core of the model is Quality Score Decoupling (QSD), which decouples and weights different quality dimensions of dance movements through attention mechanism, such as accuracy, fluency and expressiveness. Score Prediction module (SPM) uses Transformer network to further process the fused features, and outputs the final evaluation score through the full connection layer. In the experimental part, the TransCNN-DSSS model is trained and tested on the marked dance movement dataset. The performance of the model is evaluated by accuracy, recall and F1 score. The results show that the model has achieved 90% accuracy, 89% recall and F1 score of 0.90 in the task of evaluating the quality of dance movements. These results prove the effectiveness and reliability of the model. In addition, the adaptability test of the model in different dance styles also shows good generalization ability. The research contribution of this study is to put forward a new evaluation model of dance movement quality, which provides an objective and automatic evaluation tool for dance teaching, competition scoring and fans."
Trust in Software Supply Chains: Blockchain-Enabled SBOM and the AIBOM Future,"Xia, BM; Zhang, DW; Liu, Y; Lu, QH; Xing, ZC; Zhu, LM",10.1145/3643662.3643957,2024,"The robustness of critical infrastructure systems is contingent upon the integrity and transparency of their software supply chains. A Software Bill of Materials (SBOM) is pivotal in this regard, offering an exhaustive inventory of components and dependencies crucial to software development. However, prevalent challenges in SBOM sharing, such as data tampering risks and vendors' reluctance to fully disclose sensitive information, significantly hinder its effective implementation. These challenges pose a notable threat to the security of critical infrastructure and systems where transparency and trust are paramount, underscoring the need for a more secure and flexible mechanism for SBOM sharing. To bridge the gap, this study introduces a blockchain-empowered architecture for SBOM sharing, leveraging verifiable credentials to allow for selective disclosure. This strategy not only heightens security but also offers flexibility. Furthermore, this paper broadens the remit of SBOM to encompass AI systems, thereby coining the term AI Bill of Materials (AIBOM). The advent of AI and its application in critical infrastructure necessitates a nuanced understanding of AI software components, including their origins and interdependencies. The evaluation of our solution indicates the feasibility and flexibility of the proposed SBOM sharing mechanism, positing a solution for safeguarding (AI) software supply chains, which is essential for the resilience and reliability of modern critical infrastructure systems."
The Environmental Cost of Engineering Machine Learning-Enabled Systems: A Mapping Study,"Chadli, K; Botterweck, G; Saber, T",10.1145/3642970.3655828,2024,"The integration of Machine Learning (ML) across public and industrial sectors has become widespread, posing unique challenges in comparison to conventional software development methods throughout the lifecycle of ML-Enabled Systems. Particularly, with the rising importance of ML platforms in software operations and the computational power associated with their frequent training, testing, and retraining, there is a growing concern about the sustainability of DevOps practices in the context of AI-enabled software. Despite the increasing interest in this domain, a comprehensive overview that offers a holistic perspective on research related to sustainable AI is currently lacking. This paper addresses this gap by presenting a Systematic Mapping Study that thoroughly examines techniques, tools, and lessons learned to assess and promote environmental sustainability in MLOps practices for ML-Enabled Systems."
"Machine learning and deep learning in project analytics: methods, applications and research trends","Uddin, S; Yan, SR; Lu, HH",10.1080/09537287.2024.2320790,2025,"Project analytics refers to applying analytical techniques and methods to past and present data to gain insights into how the underlying project is performing. Machine learning (ML) and Deep learning (DL) have acquired extensive usage in various disciplines due to their analytical strength and the availability of high-speed computational devices. This article comprehensively surveys commonly used ML and DL algorithms for addressing project-related research problems. This study used author-selected keywords from article metadata to construct, analyse and visualise keyword co-occurrence networks to explore research trends. It has several notable observations: (a) Support vector machine and Random forest are the most used ML algorithms in project analytics; (b) although Artificial neural network remains a frequently used DL algorithm, its project-related applications have recently experienced a substantial decrease; (c) genetic algorithm and Fuzzy logic are the other advanced analytical methods frequently coined with ML and DL algorithms for addressing project-related problems; (d) there is a sharp increase of ML and DL applications in various project contexts; and (e) researchers used ML and DL algorithms for studying cost and time performance in construction and software project contexts. This article details these observations further and discusses their novelty and implications for research and practice."
MLR-predictor: a versatile and efficient computational framework for multi-label requirements classification,"Saleem, S; Asim, MN; Van Elst, L; Junker, M; Dengel, A",10.3389/frai.2024.1481581,2024,"Introduction Requirements classification is an essential task for development of a successful software by incorporating all relevant aspects of users' needs. Additionally, it aids in the identification of project failure risks and facilitates to achieve project milestones in more comprehensive way. Several machine learning predictors are developed for binary or multi-class requirements classification. However, a few predictors are designed for multi-label classification and they are not practically useful due to less predictive performance.Method MLR-Predictor makes use of innovative OkapiBM25 model to transforms requirements text into statistical vectors by computing words informative patterns. Moreover, predictor transforms multi-label requirements classification data into multi-class classification problem and utilize logistic regression classifier for categorization of requirements. The performance of the proposed predictor is evaluated and compared with 123 machine learning and 9 deep learning-based predictive pipelines across three public benchmark requirements classification datasets using eight different evaluation measures.Results The large-scale experimental results demonstrate that proposed MLR-Predictor outperforms 123 adopted machine learning and 9 deep learning predictive pipelines, as well as the state-of-the-art requirements classification predictor. Specifically, in comparison to state-of-the-art predictor, it achieves a 13% improvement in macro F1-measure on the PROMISE dataset, a 1% improvement on the EHR-binary dataset, and a 2.5% improvement on the EHR-multiclass dataset.Discussion As a case study, the generalizability of proposed predictor is evaluated on softwares customer reviews classification data. In this context, the proposed predictor outperformed the state-of-the-art BERT language model by F-1 score of 1.4%. These findings underscore the robustness and effectiveness of the proposed MLR-Predictor in various contexts, establishing its utility as a promising solution for requirements classification task."
A New Generation of Intelligent Development Environments,"Marron, M",10.1145/3643796.3648452,2024,"The practice of programming is undergoing a revolution with the introduction of AI assisted development (copilots) and the creation of new programming languages that are designed explicitly for tooling, analysis, and automation. Integrated Development Environments (IDEs) as they are currently conceptualized have not yet responded to these changes. They are still designed around the idea of a human programmer typing textual code into an editor window with the IDE providing assistance via the integration of various tools for syntax highlighting, compilation, debugging, and (maybe) code version control. This paper presents a vision for transforming the IDE from an Integrated Development Environment to an Intelligent Development Environment. The new IDE will be designed around the idea of a human programmer as the manager or curator of a software project who, rather than manually typing in code to implement a solution, will instead use the IDE to direct AI programming agents and/or automated tools to combine existing APIs, packages, and new code to implement the needed features. In this new model, the fundamental roles of the IDE are to 1) facilitate the communication between the human programmer and the AI agents and automated tools and 2) organize the workflow tasks needed to go from requirements gathering to the final tested and validated deployed feature. This paper presents a vision for the new Intelligent Development Environment based on a range of proof-of-concept high-value scenarios we have experimented with and discusses the challenges that remain to realizing these in a cohesive intelligent development experience."
Qiskit HumanEval: An evaluation benchmark for Quantum Code Generative Models,"Vishwakarma, S; Harkins, F; Golecha, S; Bajpe, VS; Dupuis, N; Buratti, L; Kremer, D; Faro, I; Puri, R; Cruz-Benito, J",10.1109/QCE60285.2024.00137,2024,"Quantum programs are typically developed using quantum Software Development Kits (SDKs). The rapid advancement of quantum computing necessitates new tools to streamline this development process, and one such tool could be Generative Artificial intelligence (GenAI). In this study, we introduce and use the Qiskit HumanEval dataset, a hand-curated collection of tasks designed to benchmark the ability of Large Language Models (LLMs) to produce quantum code using Qiskit - a quantum SDK. This dataset consists of more than 100 quantum computing tasks, each accompanied by a prompt, a canonical solution, a comprehensive test case, and a difficulty scale to evaluate the correctness of the generated solutions. We systematically assess the performance of a set of LLMs against the Qiskit HumanEval dataset's tasks and focus on the models ability in producing executable quantum code. Our findings not only demonstrate the feasibility of using LLMs for generating quantum code but also establish a new benchmark for ongoing advancements in the field and encourage further exploration and development of GenAI-driven tools for quantum code generation."
Refining software defect prediction through attentive neural models for code understanding,"Nashaat, M; Miller, J",10.1016/j.jss.2024.112266,2025,"Identifying defects through manual software testing is a resource-intensive task in software development. To alleviate this, software defect prediction identifies code segments likely to contain faults using data-driven methods. Traditional techniques rely on static code metrics, which often fail to reflect the deeper syntactic and semantic features of the code. This paper introduces a novel framework that utilizes transformer-based networks with attention mechanisms to predict software defects. The framework encodes input vectors to develop meaningful representations of software modules. A bidirectional transformer encoder is employed to model programming languages, followed by fine-tuning with labeled data to detect defects. The performance of the framework is assessed through experiments across various software projects and compared against baseline techniques. Additionally, statistical hypothesis testing and an ablation study are performed to assess the impact of different parameter choices. The empirical findings indicate that the proposed approach can increase classification accuracy by an average of 15.93% and improve the F1 score by up to 44.26% compared to traditional methods."
Model-Based Requirements Management in Gear Systems Design Based On Graph-Based Design Languages,"Holder, K; Zech, A; Ramsaier, M; Stetter, R; Niedermeier, HP; Rudolph, S; Till, M",10.3390/app7111112,2017,"For several decades, a wide-spread consensus concerning the enormous importance of an in-depth clarification of the specifications of a product has been observed. A weak clarification of specifications is repeatedly listed as a main cause for the failure of product development projects. Requirements, which can be defined as the purpose, goals, constraints, and criteria associated with a product development project, play a central role in the clarification of specifications. The collection of activities which ensure that requirements are identified, documented, maintained, communicated, and traced throughout the life cycle of a system, product, or service can be referred to as requirements engineering. These activities can be supported by a collection and combination of strategies, methods, and tools which are appropriate for the clarification of specifications. Numerous publications describe the strategy and the components of requirements management. Furthermore, recent research investigates its industrial application. Simultaneously, promising developments of graph-based design languages for a holistic digital representation of the product life cycle are presented. Current developments realize graph-based languages by the diagrams of the Unified Modelling Language (UML), and allow the automatic generation and evaluation of multiple product variants. The research presented in this paper seeks to present a method in order to combine the advantages of a conscious requirements management process and graph-based design languages. Consequently, the main objective of this paper is the investigation of a model-based integration of requirements in a product development process by means of graph-based design languages. The research method is based on an in-depth analysis of an exemplary industrial product development, a gear system for so-called Electrical Multiple Units (EMU). Important requirements were abstracted from a gear system specification list and were analyzed in detail. As a second basis, the research method uses a conscious expansion of graph-based design languages towards their applicability for requirements management. This expansion allows the handling of requirements through a graph-based design language model. The first two results of the presented research consist of a model of the gear system and a detailed model of requirements, both modelled in a graph-based design language. Further results are generated by a combination of the two models into one holistic model."
A Multi-Method Approach to Assess the Adoption of Precision Agriculture Technology in Brazil,"Ivale, AH; Naas, ID; Jani, MD",10.7160/aol.2024.160304,2024,"Precision Agriculture (PA) application aims to increase crop productivity while minimizing environmental impacts. We analyzed the topics most studied in the advancement of crop production in Brazil by applying the concepts of PA using the systematic literature review (SLR). A multi-method approach combined an SLR applying the PRISMA method and secondary data analysis. We found five clusters of technologies using the PA concept related to hardware development and four clusters related to applying technologies to software development in the PA concept. Most topics focused on using sensors to control water (soil and environment), soil electrical conductivity, and data communication. The focus on sustainability led researchers to reduce chemical products related to fertilizers and pesticides using Variable Rate Fertilizers (VRT) and reducing the environmental loading. According to the research results, it was evident that PA technology might help farmers make more accurate decisions about cultivation, production, harvest, and soil management. The availability of decision support systems powered by big data and artificial intelligence to select the best crop for a given season and soil might assist Brazil's sustainable growth of food production."
Transfer learning for cross-platform software crowdsourcing recommendation,"Yan, SH; Shen, BJ; Mo, WK; Li, N",10.1109/APSEC.2017.33,2017,"Recently, with the development of software crowd sourcing industry, an increasing number of users joined the software crowdsourcing platforms to publish software project tasks or to seek proper work opportunities. One of competitive functions of these platforms is to recommend proficient projects to developers. However, in such recommender system, there exists a serious platform cold-start problem, especially for new software crowdsourcing platforms, as they usually have too little cumulative data to support accurate model training and prediction. This paper focuses on solving the platform cold-start problem in software crowdsourcing recommendation system by transfer learning technologies. We proposed a novel cross-platform recommendation method for new software crowdsourcing platforms, whose idea is trying to transfer data and knowledge from other mature software crowdsourcing platforms (source domains) to solve the insufficient recommendation model training problem in a new platform (target domain). The proposed method maps different kinds of features both in the source domain and the target domain after a certain transformation and combination to a latent space by learning the correspondences between features. Specifically, our method is an instance of content-based recommendation, which uses tags and keywords extracted from project description in crowdsourcing platforms as features, and then set weights for each feature to reflect its importance. Then, Weight-SCL is proposed to merge and distinguish tag features and keyword features before doing feature mapping and data migration to implement knowledge transformation. Finally, we use the data from two famous software crowdsourcing platform as dataset, and a series of experiments are conducted to evaluate the performance of the multi-source recommendation system in comparison with the baseline methods, and get 1.2X performance promotion."
TinyCount: an efficient crowd counting network for intelligent surveillance,"Lee, H; Lee, J",10.1007/s11554-024-01531-8,2024,"Crowd counting, the task of estimating the total number of people in an image, is essential for intelligent surveillance. Integrating a well-trained crowd counting network into edge devices, such as intelligent CCTV systems, enables its application across various domains, including the prevention of crowd collapses and urban planning. For a model to be embedded in edge devices, it requires robust performance, reduced parameter count, and faster response times. This study proposes a lightweight and powerful model called TinyCount, which has only 60k parameters. The proposed TinyCount is a fully convolutional network consisting of a feature extract module (FEM) for robust and rapid feature extraction, a scale perception module (SPM) for scale variation perception and an upsampling module (UM) that adjusts the feature map to the same size as the original image. TinyCount demonstrated competitive performance across three representative crowd counting datasets, despite utilizing approximately 3.33 to 271 times fewer parameters than other crowd counting approaches. The proposed model achieved relatively fast inference times by leveraging the MobileNetV2 architecture with dilated and transposed convolutions. The application of SEblock and findings from existing studies further proved its effectiveness. Finally, we evaluated the proposed TinyCount on multiple edge devices, including the Raspberry Pi 4, NVIDIA Jetson Nano, and NVIDIA Jetson AGX Xavier, to demonstrate its potential for practical applications."
Natural Language Insights from Code Reviews that Missed a Vulnerability A Large Scale Study of Chromium,"Munaiah, N; Meyers, BS; Alm, CO; Meneely, A; Murukannaiah, PK; Prud'hommeaux, E; Wolff, J; Yu, Y",10.1007/978-3-319-62105-0_5,2017,"Engineering secure software is challenging. Software development organizations leverage a host of processes and tools to enable developers to prevent vulnerabilities in software. Code reviewing is one such approach which has been instrumental in improving the overall quality of a software system. In a typical code review, developers critique a proposed change to uncover potential vulnerabilities. Despite best efforts by developers, some vulnerabilities inevitably slip through the reviews. In this study, we characterized linguistic features-inquisitiveness, sentiment and syntactic complexity-of conversations between developers in a code review, to identify factors that could explain developers missing a vulnerability. We used natural language processing to collect these linguistic features from 3,994,976 messages in 788,437 code reviews from the Chromium project. We collected 1,462 Chromium vulnerabilities to empirically analyze the linguistic features. We found that code reviews with lower inquisitiveness, higher sentiment, and lower complexity were more likely to miss a vulnerability. We used a Naive Bayes classifier to assess if the words (or lemmas) in the code reviews could differentiate reviews that are likely to miss vulnerabilities. The classifier used a subset of all lemmas (over 2 million) as features and their corresponding TF-IDF scores as values. The average precision, recall, and F-measure of the classifier were 14%, 73%, and 23%, respectively. We believe that our linguistic characterization will help developers identify problematic code reviews before they result in a vulnerability being missed."
Towards Higher Abstraction Levels in Quantum Computing,"Furntratt, H; Schnabl, P; Krebs, F; Unterberger, R; Zeiner, H",10.1007/978-981-97-0989-2_13,2024,"This work is a survey and a position paper towards a higher abstraction in quantum computing (QC) programming frameworks and software development kits (SDKs). Since in 2003, Peter Shor complained about the limited increase in the number of QC algorithms [19], we see an urgent need to bridge the gap between well-established classical physics and quantum physics so that approaches become more intuitive, and - hopefully - more quantum algorithms can be discovered. In service-based hybrid QC frameworks, where algorithms need to be partitioned into quantum and classical tasks, we look at the methods available and the abstractions used. For this paper we have investigated the various levels of abstraction in Silq, Qrisp, OpenQl, Qiskit, Cirq, IonQ, and Ocean, which are originated in the QC domain, as well as CUDA Quantum, rooted in the classical software domain. With the rise of Large Language Models (LLMs), we have also explored the capabilities of LLM-powered tools like GitHub Copilot, which currently represents the top level of abstraction."
Optimization of the Use of Cloud Computing Resources Using Exploratory Data Analysis and Machine Learning,"Nawrocki, P; Smendowski, M",10.2478/jaiscr-2024-0016,2024,"Rapid growth in the popularity of cloud computing has been largely caused by increasing demand for scalable IT solutions, which could provide a cost-effective way to manage the software development process and meet business objectives. Optimization of cloud resource usage remains a key issue given its potential to significantly increase efficiency and flexibility, minimize costs, ensure security, and maintain high availability of services. This paper presents a novel concept of a Cloud Computing Resource Prediction and Optimization System, which is based on exploratory data analysis that acknowledges, among others, the information value of outliers and dynamic feature selection. The optimization of cloud resource usage relies on long-term forecasting, which is considered a dynamic and proactive optimization category. The analysis presented here focuses on the applicability of classical statistical models, XGBoost, neural networks and Transformer. Experimental results reveal that machine learning methods are highly effective in long-term forecasting. Particularly promising results - in the context of potential prediction-based dynamic resource reservations - have been yielded by prediction methods based on the BiGRU neural network and the Temporal Fusion Transformer."
Crowd behavior detection: leveraging video swin transformer for crowd size and violence level analysis,"Qaraqe, M; Yang, YD; Varghese, EB; Basaran, E; Elzein, A",10.1007/s10489-024-05775-6,2024,"In recent years, crowd behavior detection has posed significant challenges in the realm of public safety and security, even with the advancements in surveillance technologies. The ability to perform real-time surveillance and accurately identify crowd behavior by considering factors such as crowd size and violence levels can avert potential crowd-related disasters and hazards to a considerable extent. However, most existing approaches are not viable to deal with the complexities of crowd dynamics and fail to distinguish different violence levels within crowds. Moreover, the prevailing approach to crowd behavior recognition, which solely relies on the analysis of closed-circuit television (CCTV) footage and overlooks the integration of online social media video content, leads to a primarily reactive methodology. This paper proposes a crowd behavior detection framework based on the swin transformer architecture, which leverages crowd counting maps and optical flow maps to detect crowd behavior across various sizes and violence levels. To support this framework, we created a dataset comprising videos capable of recognizing crowd behaviors based on size and violence levels sourced from CCTV camera footage and online videos. Experimental analysis conducted on benchmark datasets and our proposed dataset substantiates the superiority of our proposed approach over existing state-of-the-art methods, showcasing its ability to effectively distinguish crowd behaviors concerning size and violence level. Our method's validation through Nvidia's DeepStream Software Development Kit (SDK) highlights its competitive performance and potential for real-time intelligent surveillance applications."
Repetition between stakeholder (user) and system requirements,"Ellis-Braithwaite, R; Lock, R; Dawson, R; King, T",10.1007/s00766-015-0239-x,2017,"Stakeholder requirements (also known as user requirements) are defined at an early stage of a software project to describe the problem(s) to be solved. At a later stage, abstract solutions to those problems are prescribed in system requirements. The quality of these requirements has long been linked to the quality of the software system and its development or procurement process. However, little is known about the quality defect of redundancy between these two sets of requirements. Previous literature is anecdotal rather than exploratory, and so this paper empirically investigates its occurrence and consequences with a case study from a UK defense contractor. We report on a survey of sixteen consultants to understand their perception of the problem, and on an analysis of real-world software requirements documents using natural language processing techniques. We found that three quarters of the consultants had seen repetition in at least half of their projects. Additionally, we found that on average, a third of the requirement pairs' (comprised of a system and its related stakeholder requirement) description fields were repeated such that one requirement in the pair added only trivial information. That is, solutions were described twice while their respective problems were not described, which ultimately lead to suboptimal decisions later in the development process, as well as reduced motivation to read the requirements set. Furthermore, the requirement fields considered to be secondary to the primary description field, such as the rationale or fit criterion fields, had considerably more repetition within UR-SysR pairs. Finally, given that the UR-SysR repetition phenomena received most of its discussion in the literature over a decade ago, it is interesting that the survey participants did not consider its occurrence to have declined since then. We provide recommendations on preventing the defect, and describe the freely available tool developed to automatically detect its occurrence and alleviate its consequences."
Software Vulnerability and Functionality Assessment using LLMs,"Jensen, RIT; Tawosi, V; Alamir, S",10.1145/3643787.3648036,2024,"While code review is central to the software development process, it can be tedious and expensive to carry out. In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews. Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality. To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final approve or reject recommendations. As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE). Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs. We find that the former outperforms the latter by a large margin. Motivated by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities. Results show that 36.7% of LLM-generated descriptions can be associated with true CWE vulnerabilities."
Code Parameter Summarization Based on Transformer and Fusion Strategy,"Zhang, FL; Fan, JC; Li, WQ; Khoo, SC",10.1049/sfw2/3706673,2024,"Context: As more time has been spent on code comprehension activities during software development, automatic code summarization has received much attention in software engineering research, with the goal of enhancing software comprehensibility. In the meantime, it is prevalently known that a good knowledge about the declaration and the use of method parameters can effectively enhance the understanding of the associated methods. A traditional approach used in software development is to declare the types of method parameters.Objective: In this work, we advocate parameter-level code summarization and propose a novel approach to automatically generate parameter summaries of a given method. Parameter summarization is considerably challenging, as neither do we know the kind of information of the parameters that can be employed for summarization nor do we know the methods for retrieving such information.Method: We present paramTrans, which is a novel approach for parameter summarization. paramTrans characterizes the semantic features from parameter-related information based on transformer; it also explores three fusion strategies for absorbing the method-level information to enhance the performance. Moreover, to retrieve parameter-related information, a parameter slicing algorithm (named paramSlice) is proposed, which slices the parameter-related node from the abstract syntax tree (AST) at the statement level.Results: We conducted experiments to verify the effectiveness of our approach. Experimental results show that our approach possesses an effective ability in summarizing parameters; such ability can be further enhanced by understanding the available summaries about individual methods, through the introduction of three fusion strategies.Conclusion: We recommend developers employ our approach as well as the fusion strategies to produce parameter summaries to enhance the comprehensibility of code."
A survey analysis of quantum computing adoption and the paradigm of privacy engineering,"Mousa, N; Shirazi, F",10.1002/spy2.419,2024,"This study investigates the adoption of quantum computing (QC) technology using the diffusion of innovation (DOI) theory and provides an extensive literature review. We deployed structural equation modeling to analyze data from a survey conducted among 96 top managers in various industries from Canada, the US, and Europe, including IT-based small and medium-sized enterprises (SMEs) dealing with QC software development. Our survey analysis indicates that the complexity of QC systems and software is the main barrier to the future adoption of quantum computing. This research offers insights into how future quantum computers can impact the security and privacy of information, emphasizing the importance of privacy protection. In this context, the study contributes to the notion of privacy engineering in the complex context of QC. The study established important outlines and tools for shaping future QCs. Our study, backed by empirical evidence, underscores the significant impact of new technology on citizens', organizations', firms', and government-private data. The results provide a clear message to policymakers, industry leaders, and developers: privacy engineering should be an integral part of technical development, and it's crucial to act before costs escalate. In this context, our study stands out as one of the few that use NLP and structural equation modeling to address privacy challenges in QC research through experimental research, offering practical solutions to real-world problems."
Automated Scriptless GUI Testing Aligned with Requirements and User Stories,"Karimi, M",10.1007/978-3-031-59468-7_15,2024,"Testing is an essential phase of software development to evaluate the quality of the product. Scriptless testing is a prominent technique that makes this phase efficient. However, there is a research gap in automating the testing process from the requirements. In this research we want to propose an innovative approach: Automated Scriptless GUI Testing Aligned with Requirements and User Stories. Using the open-source GUI testing tool, TESTAR, we want to propose an AI-powered tool that enables TESTAR to test software against specified requirements and user stories."
Understanding the landscape of software modelling assistants for MDSE tools: A systematic mapping,"Mosquera, D; Ruiz, M; Pastor, O; Spielberger, J",10.1016/j.infsof.2024.107492,2024,"Context: Model Driven Software Engineering (MDSE) and low-code/no-code software development tools promise to increase quality and productivity by modelling instead of coding software. One of the major advantages of modelling software is the increased possibility of involving diverse stakeholders since it removes the barrier of being IT experts to actively participate in software production processes. From an academic and industry point of view, the main question remains: What has been proposed to assist humans in software modelling tasks? Objective: In this paper, we systematically elucidate the state of the art in assistants for software modelling and their use in MDSE and low-code/no-code tools. Method: We conducted a systematic mapping to review the state of the art and answer the following research questions: i) how is software modelling assisted? ii) what goals and limitations do existing modelling assistance proposals report? iii) which evaluation metrics and target users do existing modelling assistance proposals consider? For this purpose, we selected 58 proposals from 3.176 screened records and reviewed 17 MDSE and low-code/no-code tools from main market players published by the Gartner Magic Quadrant. Result: We clustered existing proposals regarding their modelling assistance strategies, goals, limitations, evaluation metrics, and target users, both in research and practice. Conclusions: We found that both academic and industry proposals recognise the value of assisting software modelling. However, documentation about MDSE assistants' limitations, evaluation metrics, and target users is scarce or non-existent. With the advent of artificial intelligence, we expect more assistants for MDSE and lowcode/no-code software development will emerge, making imperative the need for well-founded frameworks for designing modelling assistants focused on addressing target users' needs and advancing the state of the art."
Automating modern code review processes with code similarity measurement,"Kartal, Y; Akdeniz, EK; Ãzkan, K",10.1016/j.infsof.2024.107490,2024,"Context: Modern code review is a critical component in software development processes, as it ensures security, detects errors early and improves code quality. However, manual reviews can be time-consuming and unreliable. Automated code review can address these issues. Although deep -learning methods have been used to recommend code review comments, they are expensive to train and employ. Instead, information retrieval (IR) -based methods for automatic code review are showing promising results in efficiency, effectiveness, and flexibility. Objective: Our main objective is to determine the optimal combination of the vectorization method and similarity to measure what gives the best results in an automatic code review, thereby improving the performance of IR -based methods. Method: Specifically, we investigate different vectorization methods (Word2Vec, Doc2Vec, Code2Vec, and Transformer) that differ from previous research (TF-IDF and Bag -of -Words), and similarity measures (Cosine, Euclidean, and Manhattan) to capture the semantic similarities between code texts. We evaluate the performance of these methods using standard metrics, such as Blue, Meteor, and Rouge -L, and include the run-time of the models in our results. Results: Our results demonstrate that the Transformer model outperforms the state-of-the-art method in all standard metrics and similarity measurements, achieving a 19.1% improvement in providing exact matches and a 6.2% improvement in recommending reviews closer to human reviews. Conclusion: Our findings suggest that the Transformer model is a highly effective and efficient approach for recommending code review comments that closely resemble those written by humans, providing valuable insight for developing more efficient and effective automated code review systems."
Journalism (Ethics) in the Loop: Software Development as a Cultural Competency for News Organizations,"Kiesow, D",10.1080/21670811.2024.2396068,2024,"As news organizations continue to adapt to the digital economy, they must integrate technological solutions aligned with journalistic ethics and values. Too often, the uncritical adoption of digital tools and services imposes potential risks to journalistic norms or community well-being. A proactive and digitally-native approach is required whereby journalistic values are actively embedded in purchased or locally-produced technological solutions. With the integration of software developers and other technologists into the organizational culture, news organizations can assess, avoid, or mitigate ethical risks to better achieve their business goals and their mission to their communities."
On the Use of Deep Learning Models for Semantic Clone Detection,"Pinku, SN; Mondal, D; Roy, CK",10.1109/ICSME58944.2024.00053,2024,"Detecting and tracking code clones can ease various software development and maintenance tasks when changes in a code fragment should be propagated over all its copies. Several deep learning based clone detection models have appeared in the literature for detecting syntactic and semantic clones, and these models have widely been evaluated with the BigCloneBench dataset. However, the class imbalance and small number of semantic clones make BigCloneBench less ideal when interpreting the model performances. Sometimes researchers use a few other semantic clone datasets such as GoogleCodeJam, OJClone and SemanticCloneBench to understand a model's generalizability. To overcome the limitations of the existing datasets, recently a GPT-assisted large semantic and cross-language clone dataset GPTCloneBench has been released, but it is not clear how all these models would compare and contrast in terms of these datasets. In this paper, we propose a multi-step evaluation approach for five state-of-the-art clone detection models leveraging existing benchmark datasets including the recently proposed GPTCloneBench and exploiting the mutation operators to study the extent of these clone detection models' ability. More specifically, we examined the performance of three highly-performing single-language clone detection models (ASTNN, GMN, CodeBERT) that use various code representations (e.g., AST, flow augmented AST with graph matching network, and bidirectional encoder representation) for detecting semantic clones. In addition to using BigCloneBench, we tested them on SemanticCloneBench and GPTCloneBench, investigated their robustness under mutation operations, and examined them against cutting-edge cross-language clone detection tools (C4, CLCDSA) that are also known to learn semantic clones. While all single-language models showed high F1 scores for BigCloneBench, their performances varied quite differently (sometimes over 20%) when tested on SemanticCloneBench. Interestingly, the cross-language model (C4) consistently showed superior performance (around 7%) on SemanticCloneBench over other models and performed similarly for BigCloneBench and GPTCloneBench. On mutation-based datasets, C4 appeared to have a more robust performance (less than 1% difference) whereas single-language models showed high variability."
Transforming the field of Vulnerability Prediction: Are Large Language Models the key?,"Siavvas, M; Kalouptsoglou, I; Gelenbe, E; Kehagias, D; Tzovaras, D",10.1109/MASCOTS64422.2024.10786575,2024,"Vulnerability prediction is an important mechanism for secure software development, as it enables the early identification and mitigation of software vulnerabilities. Vulnerability Prediction Models (VPMs) are Machine Learning (ML) models able to detect potentially vulnerable software components based on information retrieved from their source code. Despite the notable advancements in the field of vulnerability prediction, especially with the utilization of Deep Learning (DL) and text mining techniques, current literature still lacks a highly accurate, reliable, and practical VPM. Recently, the Large Language Models (LLMs), which have demonstrated remarkable capabilities in text understanding and processing, have started being utilized for vulnerability prediction, demonstrating highly promising results. The purpose of the present paper is to explore the utilization of LLMs in the field of vulnerability detection, identify challenges and open issues that still need to be addressed, and potentially propose directions for future research. Our analysis suggests that while LLM-based VPMs have outperformed traditional DL approaches in vulnerability prediction, significant challenges still need to be addressed to be considered sufficiently accurate, reliable, and practical."
Enhancing Operational Effectiveness Evaluation of Aeronautical Telecommunication System Through Quality Function Deployment and the Stage Probability Method,"Jiang, GF; Yuan, PL",10.1109/ACCESS.2024.3442302,2024,"In response to the challenges encountered in evaluating the operational effectiveness of aeronautical telecommunication system (ATS), including discrepancies between evaluation results and mission completion, incomplete index selection, and hierarchical index decomposition, this study aimed to address these issues comprehensively. Based on the characteristics of mission demands for ATS, primary indices such as spatial coverage, timeliness, reliability, security, and the capabilities of voice and data were determined. The signal flow of ATS and principles of radio reception guided the application of quality function deployment (QFD) and the analytic hierarchy process (AHP) to establish a comprehensive evaluation index system linking effectiveness to technical performance, application environment, and operational mode. Considering varying mission demands across different flight phases in aeronautical telecommunication, including the significance of phase missions within the overall mission, requirements for voice or data, and needs for transmission or reception, an enhanced stage probability method (SPD) based on weighted sum was proposed to formulate an aggregation model for system effectiveness. The paper discusses the application of the proposed method with an illustrative example. As indicated by the results, this model comprehensively describes the impact of aeronautical telecommunication demand, equipment reliability, functional disparities, operation modes, and technical characteristics on mission completion rates. In summary, this paper presents valuable insights and guidance for enhancing performance measures, refining parameter adjustments, and optimizing utilization strategies for ATS."
Edge Device Verification Techniques for Updated Object Detection AI via Target Object Existence,"Kitayama, A; Ono, G; Ito, H",10.1587/transfun.2023EAP1131,2024,"Edge devices with strict safety and reliability requirements, such as autonomous driving cars, industrial robots, and drones, necessitate software verification on such devices before operation. The human cost and time required for this analysis constitute a barrier in the cycle of software development and updating. In particular, the final verification at the edge device should at least strictly confirm that the updated software is not degraded from the current it. Since the edge device does not have the correct data, it is necessary for a human to judge whether the difference between the updated software and the operating it is due to degradation or improvement. Therefore, this verification is very costly. This paper proposes a novel automated method for efficient verification on edge devices of an object detection AI, which has found practical use in various applications. In the proposed method, a target object existence detector (TOED) (a simple binary classifier) judges whether an object in the recognition target class exists in the region of a prediction difference between the AI's operating and updated versions. Using the results of this TOED judgement and the predicted difference, an automated verification system for the updated AI was constructed. TOED was designed as a simple binary classifier with four convolutional layers, and the accuracy of object existence judgment was evaluated for the difference between the predictions of the YOLOv5 L and X models using the Cityscapes dataset. The results showed judgement with more than 99.5% accuracy and 8.6% over detection, thus indicating that a verification system adopting this method would be more efficient than simple analysis of the prediction differences."
LLM-based Vulnerability Sourcing from Unstructured Data,"Ashiwal, V; Finster, S; Dawoud, A",10.1109/EuroSPW61312.2024.00077,2024,"While open source software (OSS) components are integral to modern software development, they can introduce security risks due to potential unknown vulnerabilities. The shift left approach from DevSecOps recommends to integrate security measures from the early stages of development. However, the existing Common Vulnerabilities and Exposures (CVE) program has limitations in covering all vulnerabilities, leading to delays in their publication. Additionally, vulnerabilities are increasingly reported through unconventional channels, such as blog posts, newsletters, and emails, in unstructured formats. This poses challenges in processing and integrating vulnerability information into existing vulnerability management systems. To address this issue, we propose leveraging Large Language Models (LLMs) for extracting vulnerability information from unstructured sources and converting them into a machine-readable CSAF VEX format. In this research paper, we present a prototype implementation that extracts information from supplier communication emails and generates it in CSAF VEX format. It can be incorporated into the corresponding DevSecOps workflow, supporting a shift left approach. Our approach can help organizations to improve their vulnerability management practices by enabling the early detection and remediation of vulnerabilities in software supply chains."
Self-Transcendent Values and Neural Responses to Threatening Health Messages,"Kang, Y; O'Donnell, MB; Strecher, VJ; Taylor, SE; Lieberman, MD; Falk, EB",10.1097/PSY.0000000000000445,2017,"Objective Prioritizing self-transcendent values such as family and friends more than nontranscendent values such as wealth and privilege is associated with lower stress response. In this study, we tested whether having self-transcendent values can reduce specific responses in the brain in the context of potentially threatening health communications. Methods Sedentary adults (N = 67) who would likely feel threatened by health messages that highlight the risk of sedentary behavior were recruited. Participants indicated the degree to which they prioritize self-transcendent values more than nontranscendent values. Using functional magnetic resonance imaging, participants' neural responses to health messages were assessed within neural regions implicated in threat responses, including bilateral amygdala and anterior insula (AI). Results A tendency to prioritize self-transcendent more than nontranscendent values was associated with lower reactivity during exposure to health messages within anatomically defined regions of left amygdala (t(55) = -2.66, p = .010, 95% confidence interval [CI] = -0.08 to -0.01), right amygdala (t(55) = -2.22, p = .031, 95% CI = -0.06 to 0.0), and left AI (t(55) = -2.17, p = .034, 95% CI = -0.04 to 0.0), as well as a mask functionally defined to be associated with threat using an automated meta-analysis (t(55) = -2.04, p = .046, 95% CI = -0.05 to 0.0). No significant effect was obtained within the right AI (t(55) = -1.38, p = .17, 95% CI = -0.04 to .01). These effects were partially enhanced by reinforcing important values through self-affirmation, remained significant after accounting for self-reported social connection, and were specific to health message processing (versus generic self-related information). Conclusions Attenuated neural reactivity to potentially threatening health messages may be a novel way that prioritizing self-transcendent values could lead to positive health behaviors."
Social Robots for Service Environments in Academia,"Zapata, M; Alvarez-Tello, J",10.1007/978-3-031-61932-8_48,2024,"The increase of social robots in services has generated academic interest in their design and development to enhance human-robot interaction across various contexts, from home assistance to educational and medical environments. Design criteria increasingly explore the software development aspect, but interdisciplinary focus on sustainability also pursues hardware improvement for human-robot interaction in contexts ranging from home assistance to medical environments. With this background, the research question arises: What key features should be considered for the development of a social robot in academia? A systematic metadata review according to PRISMA of 698 studies from the SCOPUS database was conducted, selecting 10 relevant ones to explore functions and applications that allow characterizing hardware and software in the design of social robots for development in academia. Preliminary results indicate a focus on creating smarter, safer, and more adaptable robots, utilizing advances in hardware and software such as flexible electronics and artificial intelligence. It is concluded that the balance between hardware and software proposes the prototyping of a robotic platform with LiDAR/ROS technology, flexible electronics, and ultralightweight material structure, for sustainable, low-power consumption, and low-cost developments. Collaboration among researchers, educators, and developers is crucial to create educational social robots that are both useful and accepted."
Innovative Approach to Teaching Distributed Systems inEducation 4.0,"Purkovic, S; Fetahovic, I; Mekic, E; Katipoglu, G; Utku, S",,2024,"his paper presents an innovative pedagogical approach for teaching university-level computer science courses,specifically distributed systems. Integrating concepts from computer science and educational sciences, the approachcombines large language models (LLMs), tools integrating and automating software development (Dev) and IToperations (Ops), i.e. DevOps tools, and the educational framework based on agile approach in which students takeresponsibility for organizing their own learning process (eduScrum) within the vision of aligning education with thedemands of the 21st century (Education 4.0). Over four academic years, the new approach was compared to traditionalteaching methods. The course was structured into three sprints, each encompassing theoretical and practical tasks.ChatGPT was utilized in example solutions for code generation and debugging, while Git repositories supported practicalprogramming exercises. Efficiency was measured through qualitative and quantitative analysis, indicating a markedimprovement with the new methodology. The study demonstrates the potential of modern technologies to create dynamic,effective learning environments and suggests a pathway for updating computer science and software engineeringeducation to keep pace with technological advancemen"
Analysing the Analysers: An Investigation of Source Code Analysis Tools,"Bhutani, V; Toosi, FG; Buckley, J",10.2478/acss-2024-0013,2024,"Context The primary expectation from a software system revolves around its functionality. However, as the software development process advances, equal emphasis is placed on the quality of the software system for non-functional attributes like maintainability and performance. Tools are available to aid in this endeavour, assessing the quality of a software system from multiple perspectives.Objective This study aims to perform a comprehensive analysis of a particular set of source code analytical tools by examining diverse perspectives found in the literature and documentations. Given the vast array of programming languages available today, selecting appropriate source-code analytical tools presents a significant challenge. Therefore, this analysis aims to provide general insights to aid in selecting a more suitable analytical tool tailored to specific requirements.Method Seven prominent static analysis tools, namely SonarQube, Coverty, CodeSonar, Snyk Code, ESLint, Klocwork, and PMD, were chosen based on their prevalence in the literature and recognition in the software development community. To systematically categorise and organise their distinctive features and capabilities, a taxonomy was developed. This taxonomy covers crucial dimensions, including input support, technology employed, extensibility, user experience, rules, configurability, and supported languages.Results The comparative analysis highlights the distinctive strengths of each tool. SonarQube stands out as a comprehensive solution with a hybrid approach supporting static and dynamic code evaluations, accommodating multiple languages and integrating with popular Integrated Development Environments (IDEs). Coverity excels in identifying security vulnerabilities and defects, making it an excellent choice for security -focused development. CodeSonar prioritises code security and safety, offering a robust analysis. Snyk Code and ESLint, focusing on JavaScript, emphasise code quality and standards adherence. Klocwork is exceptional in defect detection and security analysis for C, C++, and Java. Lastly, PMD specialises in Java, emphasising code style and best practices."
Predicting students' performance at higher education institutions using a machine learning approach,"Zaki, SM; Razali, S; Kader, MARAK; Laton, MZ; Ishak, M; Burhan, NM",10.1108/K-12-2023-2742,2024,"PurposeMany studies have examined pre-diploma students' backgrounds and academic performance with results showing that some did not achieve the expected level of competence. This study aims to examine the relationship between students' demographic characteristics and their academic achievement at the pre-diploma level using machine learning.Design/methodology/approachSecondary data analysis was used in this study, which involved collecting information about 1,052 pre-diploma students enrolled at Universiti Teknologi MARA (UiTM) Pahang Branch between 2017 and 2021. The research procedure was divided into two parts: data collecting and pre-processing, and building the machine learning algorithm, pre-training and testing.FindingsGender, family income, region and achievement in the national secondary school examination (Sijil Pelajaran Malaysia [SPM]) predict academic performance. Female students were 1.2 times more likely to succeed academically. Central region students performed better with a value of 1.26. M40-income students were more likely to excel with an odds ratio of 2.809. Students who excelled in SPM English and Mathematics had a better likelihood of succeeding in higher education.Research limitations/implicationsThis research was limited to pre-diploma students from UiTM Pahang Branch. For better generalizability of the results, future research should include pre-diploma students from other UiTM branches that offer this programme.Practical implicationsThis study is expected to offer insights for policymakers, particularly, the Ministry of Higher Education, in developing a comprehensive policy to improve the tertiary education system by focusing on the fourth Sustainable Development Goal.Social implicationsThese pre-diploma students were found to originate mainly from low- or middle-income families; hence, the programme may help them acquire better jobs and improve their standard of living. Most students enrolling on the pre-diploma performed below excellent at the secondary school level and were therefore given the opportunity to continue studying at a higher level.Originality/valueThis predictive model contributes to guidelines on the minimum requirements for pre-diploma students to gain admission into higher education institutions by ensuring the efficient distribution of resources and equal access to higher education among all communities."
Agent-Driven Automatic Software Improvement,"Ruiz, FV",10.1145/3661167.3661171,2024,"With software maintenance accounting for 50% of the cost of developing software, enhancing code quality and reliability has become more critical than ever. In response to this challenge, this doctoral research proposal aims to explore innovative solutions by focusing on the deployment of agents powered by Large Language Models (LLMs) to perform software maintenance tasks. The iterative nature of agents, which allows for continuous learning and adaptation, can help surpass common challenges in code generation. One distinct challenge is the last-mile problems, errors at the final stage of producing functionally and contextually relevant code. Furthermore, this project aims to surpass the inherent limitations of current LLMs in source code through a collaborative framework where agents can correct and learn from each other's errors. We aim to use the iterative feedback in these systems to further fine-tune the LLMs underlying the agents, becoming better aligned to the task of automated software improvement. Our main goal is to achieve a leap forward in the field of automatic software improvement by developing new tools and frameworks that can enhance the efficiency and reliability of software development."
Machine Learning System Development in Information Systems Development Praxis,"Laato, S; Minkkinen, M; MÃ¤ntymÃ¤ki, M; Dennehy, D",10.17705/1CAIS.05406,2024,"Advancements in hardware and software have propelled machine learning (ML) solutions to become vital components of numerous information systems. This calls for research on the integration and evaluation of ML development practices within software companies. To investigate these issues, we conducted expert interviews with software and ML professionals. We structured the interviews around information systems development (ISD) models, which serve as conceptual frameworks that guide stakeholders throughout software projects. Using practice theory, we analyzed how software professionals perceive ML development within the context of ISD models and identified themes that characterize the transformative impact of ML development on these conceptual models. Our findings show that developer -driven conceptual models, such as DevOps and MLOps, have been embraced as common frameworks for developers and management to understand and guide the ML development processes. We observed ongoing shifts in predefined developer roles, wherein developers are increasingly adopting ML techniques and tools in their professional work. Overall, our findings underscore that ML technologies are becoming increasingly prominent in software projects across industries, and that the incorporation of ML development in ISD models is an ongoing, largely practice -driven, process."
Beyond Functional Correctness: An Exploratory Study on the Time Efficiency of Programming Assignments,"Tao, YD; Chen, WY; Ye, QY; Zhao, Y",10.1145/3639474.3640065,2024,"Practical programming assignments are critical parts of programming courses in Computer Science education. Students are expected to translate programming concepts learned from lectures into executable implementations that solve the tasks outlined in the assignments. These implementations are primarily assessed based on their functional correctness, ensuring that students' code produces the expected output when provided with specific inputs. However, functional correctness is not the only metric that evaluates the quality of programs. Runtime efficiency is a metric that is less frequently evaluated in programming courses, yet it holds significant importance in the context of professional software development. To investigate this gap and its potential ramifications, we conducted a large-scale empirical study on the time efficiency of 250 programming assignments that are evaluated solely on functional correctness. The results demonstrate that students' programming assignments exhibit significant variance in terms of execution time. We further identified 27 recurring inefficient code patterns from these assignments, and observed that most of the inefficient patterns can be optimized by automated tools such as PMD, IntelliJ IDEA and ChatGPT. Our findings provide actionable guidelines for educators to enhance the organization and integration of code performance topics throughout the programming course curriculum."
Service-oriented Extension of IEC 61850 for Model-driven Smart Grid Automation Design,"Yang, CW; Vyatkin, V; Pang, C",,2017,This paper proposes a design pattern based on the service-oriented architecture for the design of flexible distributed automation control system for substation automation systems. The design pattern is intended to be applied to control systems based on the leading standards for software development of smart grid: IEC 61850 and IEC 61499. The proposed design pattern models system requirements as orchestrators and IEC 61850 logical node specification is used as the basis for the design of smart grid services. The proposed design pattern is illustrated on a case study of a distribution system which consists of one transformer and five feeder sections. The design pattern is used to design the control system for the feeder section of the case study distribution system which includes the development of the orchestrator from the system requirement to the resultant IEC 61499 control system.
Using GitHub Copilot for Test Generation in Python: An Empirical Study,"El Haji, K; Brandt, C; Zaidman, A",10.1145/3644032.3644443,2024,"Writing unit tests is a crucial task in software development, but it is also recognized as a time-consuming and tedious task. As such, numerous test generation approaches have been proposed and investigated. However, most of these test generation tools produce tests that are typically difficult to understand. Recently, Large Language Models (LLMs) have shown promising results in generating source code and supporting software engineering tasks. As such, we investigate the usability of tests generated by GitHub Copilot, a proprietary closed-source code generation tool that uses an LLM. We evaluate GitHub Copilot's test generation abilities both within and without an existing test suite, and we study the impact of different code commenting strategies on test generations. Our investigation evaluates the usability of 290 tests generated by GitHub Copilot for 53 sampled tests from open source projects. Our findings highlight that within an existing test suite, approximately 45.28% of the tests generated by Copilot are passing tests; 54.72% of generated tests are failing, broken, or empty tests. Furthermore, if we generate tests using Copilot without an existing test suite in place, we observe that 92.45% of the tests are failing, broken, or empty tests. Additionally, we study how test method comments influence the usability of test generations."
Parameter-Efficient Multi-classification Software Defect Detection Method Based on Pre-trained LLMs,"Wang, XY; Lu, L; Yang, ZY; Tian, QY; Lin, HS",10.1007/s44196-024-00551-3,2024,"Software Defect Detection (SDD) has always been critical to the development life cycle. A stable defect detection system can not only alleviate the workload of software testers but also enhance the overall efficiency of software development. Researchers have recently proposed various artificial intelligence-based SDD methods and achieved significant advancements. However, these methods still exhibit limitations in terms of reliability and usability. Therefore, we introduce MSDD-(IA)(3), a novel framework leveraging the pre-trained CodeT5+ and (IA)(3) for parameter-efficient multi-classification SDD. This framework constructs a detection model based on pre-trained CodeT5+ to generate code representations while capturing defect-prone features. Considering the high overhead of pre-trained LLMs, we injects (IA)(3) vectors into specific layers, where only these injected parameters are updated to reduce the training cost. Furthermore, leveraging the properties of the pre-trained CodeT5+, we design a novel feature sequence that enriches the input data through the combination of source code with Natural Language (NL)-based expert metrics. Our experimental results on 64K real-world Python snippets show that MSDD-(IA)(3) demonstrates superior performance compared to state-of-the-art SDD methods, including PM2-CNN, in terms of F1-weighted, Recall-weighted, Precision-weighted, and Matthews Correlation Coefficient. Notably, the training parameters of MSDD-(IA)(3) are only 0.04% of those of the original CodeT5+. Our experimental data and code can be available at (https://gitee.com/wxyzjp123/msdd-ia3/)."
Stochastic Optimization of Program Obfuscation,"Liu, H; Sun, CN; Su, ZD; Jiang, Y; Gu, M; Sun, JG",10.1109/ICSE.2017.28,2017,"Program obfuscation is a common practice in software development to obscure source code or binary code, in order to prevent humans from understanding the purpose or logic of software. It protects intellectual property and deters malicious attacks. While tremendous efforts have been devoted to the development of various obfuscation techniques, we have relatively little knowledge on how to most effectively use them together. The biggest challenge lies in identifying the most effective combination of obfuscation techniques. This paper presents a unified framework to optimize program obfuscation. Given an input program P and a set T of obfuscation transformations, our technique can automatically identify a sequence seq = < t(1), t(2), . . . , t(n)> (for all i is an element of [1, n]. t(i) is an element of T), such that applying t(i) in order on P yields the optimal obfuscation performance. We model the process of searching for seq as a mathematical optimization problem. The key technical contributions of this paper are: (1) an obscurity language model to assess obfuscation effectiveness/optimality, and (2) a guided stochastic algorithm based on Markov chain Monte Carlo methods to search for the optimal solution seq. We have realized the framework in a tool Closure(star) for JavaScript, and evaluated it on 25 most starred JavaScript projects on GitHub (19K lines of code). Our machinery study shows that Closure(star) outperforms the well-known Google Closure Compiler by defending 26% of the attacks initiated by JSNice. Our human study also reveals that Closure(star) is practical and can reduce the human attack success rate by 30%."
iiPCS: Intent-Based In-Context Learning for Project-Specific Code Summarization,"Wang, Y; Liu, X; Lu, XS; Zhou, AY",10.1109/IJCNN60899.2024.10650776,2024,"Recent years have witnessed growing research interest in automatic source code summarization due to its beneficial potential in software development and maintenance tasks. In the past few years, various deep learning models have been developed to leverage structural and textual features in the code for generating meaningful and succinct summaries. However, the summaries generated by traditional deep learning models often have syntax errors or are meaningless. The emergence of large language models provides an opportunity to overcome the problem. However, the quality of the summaries largely depends on the in-context learning examples of code-summary pairs. In this work, we develop iiPCS, an LLM-based method for code summarization. We retrieve relevant code-summary pairs as incontext learning examples from the same project of the target code, which ensures to generate more project-specific summaries, and use the predicted intent of the target code to pick few-shot examples, which ensures to generate summaries with the correct intent. Experimental results show that iiPCS can generate code summaries with higher quality compared to traditional methods using deep learning and recent methods using LLMs."
AIS-Based Vessel Trajectory Compression: A Systematic Review and Software Development,"Liu, RW; Zhou, SQ; Yin, SK; Shu, YQ; Liang, MH",10.1109/OJVT.2024.3443675,2024,"With the advancement of satellite and 5G communication technologies, vehicles can transmit and exchange data from anywhere in the world. It has resulted in the generation of massive spatial trajectories, particularly from the Automatic Identification System (AIS) for surface vehicles. The massive AIS data lead to high storage requirements and computing costs, as well as low data transmission efficiency. These challenges highlight the critical importance of vessel trajectory compression for surface vehicles. However, the complexity and diversity of vessel trajectories and behaviors make trajectory compression imperative and challenging in maritime applications. Therefore, trajectory compression has been one of the hot spots in research on trajectory data mining. The major purpose of this work is to provide a comprehensive reference source for beginners involved in vessel trajectory compression. The current trajectory compression methods could be broadly divided into two types, batch (offline) and online modes. The principles and pseudo-codes of these methods will be provided and discussed in detail. In addition, compressive experiments on several publicly available data sets have been implemented to evaluate the batch and online compression methods in terms of computation time, compression ratio, trajectory similarity, and trajectory length loss rate. Finally, we develop a flexible and open software, called AISCompress, for AIS-based batch and online vessel trajectory compression. The conclusions and associated future works are also given to inspire future applications in vessel trajectory compression."
Enhancing Sustainability of Complex Epidemiological Models through a Generic Multilevel Agent-based Approach,"Picault, S; Huang, YL; Sicard, V; Ezanno, P",,2017,"The development of computational sciences has fostered major advances in life sciences, but also led to reproducibility and reliability issues, which become a crucial stake when simulations are aimed at assessing control measures, as in epidemiology. A broad use of software development methods is a useful remediation to reduce those problems, but preventive approaches, targeting not only implementation but also model design, are essential to sustainable enhancements. Among them, AI techniques, based on the separation between declarative and procedural concerns, and on knowledge engineering, offer promising solutions. Especially, multilevel multi-agent systems, deeply rooted in that culture, provide a generic way to integrate several epidemiological modeling paradigms within a homogeneous interface. We explain in this paper how this approach is used for building more generic, reliable and sustainable simulations, illustrated by real-case applications in cattle epidemiology."
Research on real-time software defect prediction system based on deep neural network,"Lv, PL",10.1177/18724981241307452,2025,"In response to the problems of poor generalization ability and difficulty in feature selection of traditional software defect prediction models, this paper introduces deep neural networks to build an automated, real-time updated, and efficient software defect prediction system that processes large-scale data. Data, code features, historical defect records, and developer activities can be collected from the version control system Git and defect tracking system JIRA. The quartile range method can be used to handle outliers, the mean interpolation method and forward filling method can be used to fill in missing values, and these raw data have been cleaned and feature extracted. In terms of the model, the Deep Neural Networks (DNN) algorithm is used for model architecture and training. In terms of real-time prediction, Apache Kafka and Spark Streaming are used to achieve real-time acquisition, processing, and analysis of software data, achieving real-time software defect prediction. After multiple experiments on several open-source projects, the model achieves a prediction accuracy of 92.5%, a recall of 88.3%, a precision of 90.2%, and an F1 score of 89.2%. The results show that the system has high prediction performance when dealing with large-scale data in complex environments, and can help improve the efficiency and quality of software development."
Human-Centered Digital Sustainability: Handling Enumerated Lists in Digital Texts,"Csernoch, M; Nagy, T; Nagy, K; Csernoch, J; Hannusch, C",10.1109/ACCESS.2024.3369587,2024,"In advance to the present study, the authors introduced a method which makes it possible to calculate the entropy of natural language digital texts, focusing on word-processed texts, presentations, and webpages. This entropy reveals that the more underdeveloped documents are, the more demanding their content-related modification becomes. It was also found that the time and data required to complete a modification task in an erroneous document is several times more than in its correct counterpart. This finding leads to the end-user paradox: the less trained end-users are, the more errors they make, and the modification of their documents requires more resources. To resolve these discrepancies, the present study defines the sustainability rate of natural language digital texts which calculates the losses - the waste of human resources, time, workspace, computers, energy, frustration, working in bees, losing data - generated by negligent text management. Furthermore, we present examples of how manual and enumerated lists behave to modifications in a 213-page long document and conclude from our investigations that while the waste of human and machine resources occurs repeatedly in erroneous documents, the sustainability rate remains low. To prove the necessity of correction, we cleared the sample document, which took approximately 67 hours of two experts of our research group ( $2\times67$ hours). With this method, we found that the correction of errors can be extremely demanding, but uses resources only once, and further modifications in the now correct document need only the content-required amount of time, activities, entropy, and resources, in accordance with the expectations of the person intended to update the document. To correct documents, we present the Error Recognition Model, which is proved effective and efficient in digital education. All our findings indicate that both education and industry should adapt the presented approach (1) to develop students' and end-users' computational thinking skills, (2) to manage and take advantage of errors, (3) to recognize connections between the structure of the text and the complex word processing tools, (4) to pay attention to digital sustainability - beyond hardware and software development and recycling - with a focus on the human factor. Recently, the Error Recognition Model is a reactive problem-solving approach, whose effectiveness is justified. However, the near future is to run parallel the reactive and proactive uses of this approach, while if we look far into the future, the proactive use to digital born natural language texts should dominate."
Designing NLP-Based Solutions for Requirements Variability Management: Experiences from a Design Science Study at Visma,"Elahidoost, P; Unterkalmsteiner, M; Fucci, D; Liljenberg, P; Fischbach, J",10.1007/978-3-031-57327-9_12,2024,"Context andmotivation: In this industry-academia collaborative project, a team of researchers, supported by a software architect, business analyst, and test engineer explored the challenges of requirement variability in a large business software development company. Question/ problem: Following the design science paradigm, we studied the problem of requirements analysis and tracing in the context of contractual documents, with a specific focus on managing requirements variability. This paper reports on the lessons learned from that experience, highlighting the strategies and insights gained in the realm of requirements variability management. Principal ideas/results: This experience report outlines the insights gained from applying design science in requirements engineering research in industry. We show and evaluate various strategies to tackle the issue of requirement variability. Contribution: We report on the iterations and how the solution development evolved in parallel with problem understanding. From this process, we derive five key lessons learned to highlight the effectiveness of design science in exploring solutions for requirement variability in contract-based environments."
CODEAGENT: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges,"Zhang, KC; Li, J; Li, G; Shi, XJ; Jint, Z",,2024,"Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. However, real-world software development often involves complex code repositories with complex dependencies and extensive documentation. To enable LLMs to handle these realworld repo-level code generation, we present CODEAGENT, a novel LLM-based agent framework that employs external tools for effective repo-level code generation. CODEAGENT integrates five programming tools, enabling interaction with software artifacts for information retrieval, code implementation, and code testing. We implement four agent strategies to optimize these tools' usage. To the best of our knowledge, CODEAGENT is the first agent framework specifically for repolevel code generation. In order to measure the effectiveness of our method at the repository level, we design a repo-level benchmark CODEAGENTBENCH. The performance on this benchmark shows a significant improvement brought by our method, with improvements in pass rate ranging from 2.0 to 15.8. Further tests on the HumanEval benchmark confirm CODEAGENT's adaptability and efficacy across various code generation tasks. Notably, CODEAGENT outperforms commercial products like GitHub Copilot, showcasing superior accuracy and efficiency. These results demonstrate CODEAGENT's robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges."
A New Model for Predicting Surface Pump Pressure of Drilling Rig Using Artificial Neural Network,"Mohammed, SE; Al-Bayati, D; Tawfeeq, YJ",10.1134/S0965544124050141,2024,"Machine learning and artificial intelligence are recently used in many engineering sectors. Artificial neural network (ANN) has been widely used in oil and gas to predict many important parameters. This work uses ANN to predict the required surface pump pressure at the surface, considering the impact of different drilling parameters. These parameters are: depth, rate of penetration (ROP), weight on bit (WOB), rotation per minute (RPM), stroke per minute (SPM), mud weight, and mud flow rate. ANN models were built using two layers, and both hyperbolic Tanh and Log sigmoid transfer functions were used to predict the model's validity. Around 2020 data values were used to test, train and validate model prediction. Sensitivity analysis used 2, 4, 8, and 10 neurons for each transfer function (Log sigmoid and hyperbolic Tanh). Results indicated that the prediction for the eight nodes Tanh model best matches the overall data available for the test. For instance, a 99.67% R for training, 99.45% test, 98.57% validation, and 99.47% overall data set were obtained. On the other hand, using a Log model with ten nodes offered the best data set matching for the same data tested above. Results show that test data converged 99.58 with the model prediction method, while 99.52 and 98.95 were obtained for training and validation, respectively. Therefore, we suggest a new model based on the Log model to predict surface pump pressure. This model would be beneficial for predicting the required number and size of pumps at any drilling site."
ASKDetector: An AST-Semantic and Key Features Fusion based Code Comment Mismatch Detector,"Yang, HY; Chen, H; Kuai, ZR; Tu, SY; Kuang, L",10.1145/3643916.3644405,2024,"Code comments are essential for programming comprehension. Nevertheless, developers often neglect to update comments after modifying the source code. Wrong code comments may lead to bugs in the maintenance process, thus affecting the reliability of the software. So, timely comment mismatch detection is crucial for software development and maintenance. However, existing works have the following two limitations: 1) the lack of use of code structural and sequential information, and 2) the ignorance of existing associations between code and comments. In this paper, we propose a new model called ASKDetector (AST-Semantic and Key features fusion based mismatch Detector). For the first limitation, we encode code with an attention-based preorder traversal abstract syntax tree sequence to obtain both order and structural information. And Code-BERT is utilized to capture contextual semantic features further. For the second one, we encode extracted association information between the code snippets and comments to reduce the semantic gap. The correlations between the encoders are learned through a fusion layer and a multi-layer perceptron. The experimental results prove that our detector outperforms the state-of-the-art model in evaluation metrics, where our F1 and accuracy exceed an average of 3.4%."
ANN-based software cost estimation with input from COCOMO: CANN model,"Rashid, CH; Shafi, I; Khattak, BHA; Safran, M; Alfarhood, S; Ashraf, I",10.1016/j.aej.2024.11.042,2025,"Different project management processes have been used in software engineering to support managers in keeping project costs manageable. One of the essential processes in software engineering is to accurately and reliably estimate the required effort and cost to complete the projects. The domain of software cost estimation has witnessed a prominent surge in research activities in recent years and being an evolving process, it keeps opening new avenues, each with advantages and disadvantages, making it important to work out better options. This research aims to identify the factors that influence the software effort estimation using the constructive cost model (COCOMO), and artificial neural networks (ANN) model by introducing a novel cost estimation approach, COCOMO-ANN (CANN), utilizing a partially connected neural network (PCNN) with inputs derived from calibrated values of the COCOMO model. A publicly available dataset (COCOMONASA 2), various combinations of activation functions, and layer densities have been systematically explored, employing multiple evaluation metrics such as MAE, MRE, and MMRE. In the PCNN model, the ReLU activation function and a 1000-dense layer have demonstrated better performance. While layer density generally correlates with better outcomes, this correlation is not universally applicable for all activation functions and outcomes vary across different combinations. The use of the relationships between 26 key parameters of COCOMO in PCNN produced better results than FCNN by 0.59%, achieving an MRE of 6.55 and an MMRE of 7.04. The results indicated that the CANN model (COCOMO & ANN) presented better results than existing models."
CCMT: A Code Complexity Measuring Tool,"De Silva, DI; Godapitiya, MVN; Kodithuwakku, YS; Dewmin, TY; Dayananda, IHMBL; Fernando, KRAW",10.1007/978-981-97-3559-4_8,2024,"As the complexity of software systems continues to grow, effectively managing this intricacy becomes paramount. In response, this study introduces the Code Complexity Measuring Tool (CCMT), a comprehensive solution designed to address the growing challenges in assessing code complexity. CCMT redefines complexity measurement by incorporating nine fundamental factors: size, type of control structures, nesting level of control structures, inheritance level of statements, try-catch blocks, recursion, array declarations, compound conditional statements, and input-output statements. This sophisticated instrument, tailored to these factors, enables precise complexity estimation by strategically assigning distinct weightings, yielding a holistic code complexity evaluation. Its adaptability and flexibility make it not just a tool for the present but a catalyst for the future. CCMT has the potential to integrate seamlessly into cutting-edge domains like AI-driven software development, ensuring its continued relevance and significance in the ever-changing landscape of software engineering."
"ModelHamiltonian: A Python-scriptable library for generating 0-, 1-, and 2-electron integrals","Chuiko, V; Richards, ADS; SÃ¡nchez-DÃ­az, G; MartÃ­nez-GonzÃ¡lez, M; Sanchez, W; Da Rosa, GB; Richer, M; Zhao, YL; Adams, W; Johnson, PA; Heidar-Zadeh, F; Ayers, PW",10.1063/5.0219015,2024,"ModelHamiltonian is a free, open source, and cross-platform Python library designed to express model Hamiltonians, including spin-based Hamiltonians (Heisenberg and Ising models) and occupation-based Hamiltonians (Pariser-Parr-Pople, Hubbard, and H & uuml;ckel models) in terms of 1- and 2-electron integrals, so that these systems can be easily treated by traditional quantum chemistry software programs. ModelHamiltonian was originally intended to facilitate the testing of new electronic structure methods using HORTON but emerged as a stand-alone research tool that we recognize has wide utility, even in an educational context. ModelHamiltonian is written in Python and adheres to modern principles of software development, including comprehensive documentation, extensive testing, continuous integration/delivery protocols, and package management. While we anticipate that most users will use ModelHamiltonian as a Python library, we include a graphical user interface so that models can be built without programming, based on connectivity/parameters inferred from, for example, a SMILES string. We also include an interface to ChatGPT so that users can specify a Hamiltonian in plain language (without learning ModelHamiltonian's vocabulary and syntax). This article marks the official release of the ModelHamiltonian library, showcasing its functionality and scope."
Efficient and Accurate Calculation of Equivalent Thermal Conductivity of Wiring Structures in Mesoscopic Scale and Intelligent Prediction,"Zheng, H; Zhao, ZY; Wan, G; Cheng, MX; Qiao, W; Zheng, RQ; Jia, YX",10.1109/ICEPT63120.2024.10668724,2024,"With the development of artificial intelligence and autonomous driving technologies, the complexity of the conductive layers (CDLs) in the integrated circuit (IC) is increasing, leading to severe thermomechanical reliability issues. To ensure the reliability, temperature distribution must be accurately simulated. It is impractical to capture all wiring structure features during modeling, so partition homogenization methods have been proposed. However, most current equivalence methods still have limitations when dealing with complex microstructures. Thus, the equivalent thermal conductivity (ETC) calculation method based on mesoscopic finite element analysis (MFEA) was proposed, which fully takes into account the detailed structural characteristics of each partition to batch calculate the their ETCs through automated scripts, and ultimately to achieve efficient and accurate simulation of the macroscopic temperature field. The superiority of the proposed method in predicting the temperature distribution of IC products with complex wiring structures was verified by comparing the results of the proposed method in this paper with those obtained from linear interpolation model (LIM) and series-parallel model (SPM) methods. To further improve the computational efficiency of ETC, the study also introduced Global Context Vision Transformers (GC ViT) for fast predicting ETC, which provides the possibility that this equivalent method can meet the industrial requirements for efficiency. In conclusion, the scheme in this study not only provides an efficient simulation strategy for the thermal management design of IC products, but also opens up new possibilities for the future application of deep learning techniques in material property prediction and complex system modeling in a wider range of engineering fields."
Reusable and generic design decisions for developing UML-based domain-specific languages,"Hoisl, B; Sobernig, S; Strembeck, M",10.1016/j.infsof.2017.07.008,2017,"Context: In recent years, UML-based domain-specific model languages (DSMLs) have become a popular option in model-driven development projects. However, making informed design decisions for such DSMLs involves a large number of non-trivial and inter-related options. These options concern the language-model specification, UML extension techniques, concrete-syntax language design, and modeling tool support. Objective: In order to make the corresponding knowledge on design decisions reusable, proven design rationale from existing DSML projects must be collected, systematized, and documented using an agreed upon documentation format. Method: We applied a sequential multi-method approach to identify and to document reusable design decisions for UML-based DSMLs. The approach included a Web-based survey with 80 participants. Moreover, 80 DSML projects(1), which have been identified through a prior systematic literature review, were analyzed in detail in order to identify reusable design decisions for such DSMLs. Results: We present insights on the current state of practice in documenting UML-based DSMLs (e.g., perceived barriers, documentation techniques, reuse potential) and a publicly available collection of reusable design decisions, including 35 decision options on different DSML development concerns (especially concerning the language model, concrete-syntax language design, and modeling tools). The reusable design decisions are documented using a structured documentation format (decision record). Conclusion: Our results are both, scientifically relevant (e.g. for design-space analyses or for creating classification schemas for further research on UML-based DSML development) and important for actual software engineering projects (e.g. by providing best-practice guidelines and pointers to common pitfalls). (C) 2017 Elsevier B.V. All rights reserved."
Multi-Level AI-Driven Analysis of Software Repository Similarities,"Zhang, HL; Zhang, LY; Fang, L; Filgueira, R",10.1109/e-Science62913.2024.10678701,2024,"This paper introduces significant enhancements to RepoSim4Py and RepoSnipy, advanced semantic tools for deep analysis of software repositories. RepoSim4Py command-line toolbox now supports multi-level embedding, encompassing code, documentation, requirements, README, and comprehensive repository analysis, which enable the understanding of repository dynamics. Concurrently, RepoSnipy web-based search engine facilitates sophisticated repository similarity searches and introduces clustering based on both repository tags (topic_cluster) and code embeddings (code_cluster). We also introduce SimilarityCal, a novel binary classification model trained on these clusters, to predict and quantify repository similarities with high accuracy. These developments provide researchers and developers with powerful tools to navigate the complex landscape of software repositories, improving efficiency in software development and fostering innovation through better reuse of existing resources."
Semantic-Enhanced Indirect Call Analysis with Large Language Models,"Cheng, BJ; Zhang, C; Wang, KL; Shi, L; Liu, Y; Wang, HY; Guo, Y; Li, D; Chen, XQ",10.1145/3691620.3695016,2024,"In contemporary software development, the widespread use of indirect calls to achieve dynamic features poses challenges in constructing precise control flow graphs (CFGs), which further impacts the performance of downstream static analysis tasks. To tackle this issue, various types of indirect call analyzers have been proposed. However, they do not fully leverage the semantic information of the program, limiting their effectiveness in real-world scenarios. To address these issues, this paper proposes Semantic-Enhanced Analysis (SEA), a new approach to enhance the effectiveness of indirect call analysis. Our fundamental insight is that for common programming practices, indirect calls often exhibit semantic similarity with their invoked targets. This semantic alignment serves as a supportive mechanism for static analysis techniques in filtering out false targets. Notably, contemporary large language models (LLMs) are trained on extensive code corpora, encompassing tasks such as code summarization, making them well-suited for semantic analysis. Specifically, SEA leverages LLMs to generate natural language summaries of both indirect calls and target functions from multiple perspectives. Through further analysis of these summaries, SEA can determine their suitability as caller-callee pairs. Experimental results demonstrate that SEA can significantly enhance existing static analysis methods by producing more precise target sets for indirect calls."
What Data Scientists (Care To) Recall,"Saeed, S; Sheikholeslami, S; KrÃ¼ger, J; Hebig, R",10.1007/978-3-031-49266-2_15,2024,"To maintain and evolve a software system, developers need to gain new or recover lost knowledge about that system. Thus, program comprehension is a crucial activity in software development and maintenance processes. We know from previous work that developers prioritize what information they want to remember about a system based on the perceived importance of that information. However, AI-based software systems as a special case are not developed by software developers alone, but also by data scientists who deal with other concepts and have a different educational background than most developers. In this paper, we study what information data scientists (aim to) recall about their systems. For this purpose, we replicated our previous work by interviewing 11 data scientists, investigating the knowledge they consider important to remember, and whether they can remember parts of their systems correctly. Our results suggest that data scientists consider knowledge about the AI-project settings to be the most important to remember and that they perform best when remembering knowledge they consider important. Contrary to software developers, data scientists' self-assessments increase when reflecting on their systems. Our findings indicate similarities and differences between developers and data scientists that are important for managing the processes surrounding a system."
Series-Parallel Multiple Integrated Modular Multilevel DC-DC Converter for All-DC Offshore Wind Power System,"Zhao, PQ; Meng, YQ; Ge, MW; Duan, ZY; Wang, XL; Wang, JX",10.1109/TPWRD.2024.3419087,2024,"All-dc offshore wind power system is a promising solution for long distance and high power transmission. This paper proposes a series-parallel multiple integrated modular multilevel dc-dc converter (SPMI-MMC) for all-dc offshore wind power systems to meet its demand for high voltage, high power, and large-step ratio dc voltage transmission. Different from the traditional isolated dc-dc converter, the submodules (SMs) of the SPMI-MMC adopt the input-parallel-output-series (IPOS) arrangement to achieve a large step ratio. The symmetrical parallel multiple submodule (SPM-SM) is proposed to realize parallel multiple of SMs through coupling reactors (CRs), which not only can enhance the large current operation ability but also can cancel out the nonzero-sequence high frequency harmonics. Considering the cost factor, the simplified full-bridge double submodule (S-FBDSM) is first introduced in dc-dc converter. It is economical and has good bidirectional dc-fault blocking ability. Digital simulation and real time hard-in-loop (HIL) simulation tests are conducted to verify the effectiveness and superiority of the proposed SPMI-MMC."
Automated Machine Learning for Enhanced Software Reliability Growth Modeling : A Comparative Analysis with Traditional SRGMs,"Kim, T; Ryu, D; Baik, J",10.1109/QRS62785.2024.00055,2024,"Traditional Software Reliability Growth Models (SRGMs) depend on unrealistic assumptions, which makes it difficult to capture the complexities of modern software development. Recent advancements in artificial intelligence have introduced new modeling techniques, but these methods are complex and require careful algorithm selection and hyperparameter tuning. Automated Machine Learning (AutoML) has emerged as a promising solution to streamline this process. However, its application in the field of software reliability growth modeling remains unexplored. In this study, we explore the effectiveness of AutoML in enhancing software reliability growth modeling and compare its performance with traditional SRGMs. We employ two prominent AutoML packages, Auto-sklearn and H2O AutoML, and leverage twelve project datasets to answer three research questions: (1) the impact of various AutoML package options on software reliability growth modeling, (2) the identification of the optimal AutoML approach for modeling software reliability growth, and (3) the overall effectiveness of AutoML in software reliability growth modeling. We found that using ensemble options enhanced predictive performance across multiple projects. Auto-sklearn with the ensemble option emerged as the most effective approach when evaluated based on End-point Prediction values, while H2O AutoML with the ensemble option demonstrated superior performance based on Mean Squared Error values. Additionally, AutoML packages with ensemble options demonstrated more accurate predictive performance compared to traditional SRGMs across the majority of datasets. Our study highlights the potential of AutoML to enhance software reliability growth modeling and provides insights for future research and practical applications in software engineering."
CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation,"Yan, WX; Liu, HT; Wang, YK; Li, YZ; Chen, Q; Wang, W; Lin, TY; Zhao, WS; Zhu, L; Sundaram, H; Deng, SG",,2024,"Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce CodeScope, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers 43 programming languages and eight coding tasks. It evaluates the coding performance of LLMs from three dimensions (perspectives): length, difficulty, and efficiency. To facilitate execution-based evaluations of code generation, we develop MultiCodeEngine, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at https://github.com/WeixiangYAN/CodeScope. [GRAPHICS] ."
Design of Intelligent On-load Tap Changer Controlled by Permanent Magnetic Actuator,"Deng, JQ; Zhang, GG; Geng, YS; Wang, JH",,2017,"Traditional mechanical OLTC has some disadvantages while changing the taps, such as causing arc, switching slowly, having high malfunction rate. This paper proposed a kind of intelligent OLTC which equips with vacuum interrupters controlled by permanent magnetic actuators (PMA) as its switching parts. In this paper, the overall structure and electrical circuit scheme of the OLTC were designed on the basis of the simulation results under different circuit conditions. The type of vacuum interrupter and the transition resistance was theoretically analyzed and calculated. Furthermore, the hardware design and software development of intelligent OLTC controller were completed based on digital signal processor (DSP). An experimental prototype of intelligent OLTC has been developed, in which the controller takes power from the secondary side of transformer and the rectifier circuit has been used to recharge the capacitor array which acts as the power source of PMA. The capacitor-charging circuit, voltage and current measurement circuit, PMA-driving circuit and overvoltage absorption circuit were designed. The power supply module of the controller had been allocated systematically, so that the supply side of the controller and the main circuit of OLTC were electrical isolated. Finally, the switching experiments have been carried out for verifying the functions of the OLTC prototype. The results indicated that the intelligent OLTC prototype can switch off the circuit current precisely around the current zero point, and adjust the taps' positions of transformer automatically to complete the voltage regulating task. Moreover, it has the advantages of simple structure, satisfies the requests of miniaturization and intellectualization."
[18F]FDG PET for mapping the cerebral glucose metabolic characteristics of drug-sensitive and drug-resistant epilepsy in pediatric patients,"Hu, DY; Yu, CC; Zhang, XH; Zhong, Y; Zhu, YK; Tian, M; Zhang, H",10.1007/s00259-024-06933-1,2025,"Objective This study aimed to investigate [F-18]fluorodeoxyglucose positron emission tomography ([F-18]FDG PET) mapping for cerebral glucose metabolism in drug-sensitive and drug-resistant pediatric epilepsy patients. Methods This retrospective study enrolled 40 patients and 25 controls. Patients were categorized into drug-sensitive epilepsy (n = 22) and drug-resistant epilepsy (n = 18) according to the seizure frequency at follow-up. All patients underwent two [F-18]FDG PET scans separated by a minimum of one year. Absolute asymmetry index (|AI|) was calculated for assessing metabolic differences and changes in epileptic foci. Statistical Parametric Mapping (SPM) was utilized to reveal voxel-wise metabolic characteristics and alterations throughout the brain. Network analysis based on graph theory was used to investigate network-level differences between the two patient groups. Results The drug-sensitive group showed a lower |AI| at both baseline (P = 0.038) and follow-up (P = 0.003) PET scans than the drug-resistant group. |AI| decreased in the drug-sensitive group and increased in the drug-resistant group across scans, but these trends were not statistically significant (P = 0.240 and P = 0.450, respectively). Both groups exhibited hypometabolism at baseline. The drug-sensitive group showed less hypometabolic brain regions than the drug-resistant group. The drug-sensitive maintained stable level of hypometabolism between the two scans, whereas the drug-resistant group showed an increasing hypometabolism. Network analysis demonstrated that the drug-sensitive group had a higher global efficiency, average degree, and clustering, along with a shorter characteristic path length compared to the drug-resistant group. Conclusions For the first time, this study revealed in vivo cerebral glucose metabolic pattern of nonsurgical pediatric epilepsy patients treated by antiepileptic drugs. Especially, drug-resistant epilepsy patients represented significantly extensive and progressive hypometabolism with inefficient brain network connectivity compared with drug-sensitive epilepsy. [F-18]FDG PET imaging may be a potential visual approach for theranostics of epilepsy patients."
Hosting Capacity Estimation for Behind-the-Meter Distributed Generation,"Pereira, O; Parajeles, M; Zuniga, B; Quiros-Tortos, J; Valverde, G",10.1109/TPWRS.2023.3326859,2024,"Power utilities worldwide are facing a growing number of customers' requests to authorize the interconnection of behind-the-meter distributed generation. This paper presents a new practical methodology for power utilities to estimate the amount of small-scale distributed generation they can accommodate in the low-voltage level of distribution feeders without potential harm to the latter. It considers both medium-voltage and low-voltage levels limiting criteria to determine the locational hosting capacities. The proposed method uses detailed models of distribution feeders extracted from the geographical information system of power utilities and the location of existing customers. A new tool based on the proposed methodology is also described in the paper to show how the methodology can be easily integrated into existing planning tools of power utilities. It reports the circuits' total hosting capacity and provides hosting capacity maps with results per medium-voltage feeder section, distribution transformer, and low-voltage system. Results for real large-scale distribution feeder models demonstrate the practicality and potential of this methodology."
Predicting Line-Level Defects by Capturing Code Contexts with Hierarchical Transformers,"Mahbub, P; Rahman, MM",10.1109/SANER60148.2024.00038,2024,"Software defects consume 40% of the total budget in software development and cost the global economy billions of dollars every year. Unfortunately, despite the use of many software quality assurance (SQA) practices in software development (e.g., code review, continuous integration), defects may still exist in the official release of a software product. Therefore, prioritizing SQA efforts for the vulnerable areas of the codebase is essential to ensure the high quality of a software release. Predicting software defects at the line level could help prioritize the SQA effort but is a highly challenging task given that only similar to 3% lines of a codebase could be defective. Existing works on line-level defect prediction often fall short and cannot fully leverage the line-level defect information. In this paper, we propose - Bugsplorer a novel deep-learning technique for line-level defect prediction. It leverages a hierarchical structure of transformer models to represent two types of code elements: code tokens and code lines. Unlike the existing techniques that are optimized for file-level defect prediction, Bugsplorer is optimized for a line-level defect prediction objective. Our evaluation with five performance metrics shows that Bugsplorer has a promising capability of predicting defective lines with 26-72% better accuracy than that of the state-of-the-art technique. It can rank the first 20% defective lines within the top 1-3% suspicious lines. Thus, Bugsplorer has the potential to significantly reduce SQA costs by ranking defective lines higher."
Measuring Impacts of Poisoning on Model Parameters and Embeddings for Large Language Models of Code,"Hussain, A; Rabin, MRI; Alipour, MA",10.1145/3664646.3664764,2024,"Large language models (LLMs) have revolutionized software development practices, yet concerns about their safety have arisen, particularly regarding hidden backdoors, aka trojans. Backdoor attacks involve the insertion of triggers into training data, allowing attackers to manipulate the behavior of the model maliciously. In this paper, we focus on analyzing the model parameters to detect potential backdoor signals in code models. Specifically, we examine attention weights and biases, and context embeddings of the clean and poisoned CodeBERT and CodeT5 models. Our results suggest noticeable patterns in context embeddings of poisoned samples for both the poisoned models; however, attention weights and biases do not show any significant differences. This work contributes to ongoing efforts in white-box detection of backdoor signals in LLMs of code through the analysis of parameters and embeddings."
When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention,"Guo, LH; Wang, YL; Shi, ES; Zhong, WJ; Zhang, HY; Chen, JC; Zhang, RK; Ma, YC; Zheng, ZB",10.1145/3650212.3680343,2024,"Code generation aims to automatically generate code snippets that meet given natural language requirements and plays an important role in software development. Although Code LLMs have shown excellent performance in this domain, their long generation time poses a signification limitation in practice use. In this paper, we first conduct an in-depth preliminary study with different Code LLMs on code generation tasks and identify a significant efficiency issue, i.e., continual generation of excess tokens. It harms the developer productivity and leads to huge computational wastes. To address it, we introduce CodeFast, an inference acceleration approach for Code LLMs on code generation. The key idea of CodeFast is to terminate the inference process in time when unnecessary excess tokens are detected. First, we propose an automatic data construction framework to obtain training data. Then, we train a unified lightweight model GenGuard applicable to multiple programming languages to predict whether to terminate inference at the current step. Finally, we enhance Code LLM with GenGuard to accelerate its inference in code generation tasks. We conduct extensive experiments with CodeFast on five representative Code LLMs across four widely used code generation datasets. Experimental results show that (1) CodeFast can significantly improve the inference speed of various Code LLMs in code generation, ranging form 34% to 452%, without compromising the quality of generated code. (2) CodeFast is stable across different parameter settings and can generalize to untrained datasets. Our code and data are available at https://github.com/DeepSoftwareAnalytics/CodeFast."
MicroHFRCL: A History Faults Based Root Cause Localization Framework in Microservice Systems,"Zhang, LY; Shi, YL; Qi, KY; Wu, D; Wang, XJ; Yan, ZM; Chen, ZY",10.1109/IJCNN60899.2024.10650929,2024,"At present, the microservice architecture is widely used in modern software development for its flexibility and scalability. However, the huge data scale and complex invocation relationships between services make the root cause localization of faults in microservice systems extremely difficult. Some of the current root cause localization methods based on metrics data use history fault data to effectively improve the localization results, but there are challenges such as not representing history faults effectively and limiting the localization results to repetitive faults that have occurred in history. In this paper, we propose an automatic root cause localization framework MicroHFRCL based on the history fault library to address the above issues. MicroHFRCL constructs an instance causal graph based on metric data for causal analysis. The instance causal graph is weighted by encoding the anomalous subgraph and calculating the similarity of history faults. The PageRank algorithm is used to locate the root cause of faults. Among them, MicroHFRCL learns the structure and feature information of fault anomalous subgraphs through GCN and Transformer models, achieving effective representation of history faults and fast calculation of similarity in history fault codes, improving the efficiency of repetitive fault localization, and effectively solving the problem of the limitation of using history faults for root cause localization results. We implemented MicroHFRCL and tested it on the fault dataset collected by a benchmark microservice test system. Compared with the latest baseline models, MicroHFRCL has significantly improved the localization accuracy, and can also achieve good results in the case of small-scale history faults."
Implementing and Evaluating Automated Bug Triage in Industrial Projects,"Hong, HT; Wang, DS; Kim, SJ; Sung, H; Park, CW; Park, HH; Lee, CG",10.1109/ACCESS.2024.3519418,2024,"Resolving bugs on time is essential for software development and is critical in industrial projects because it directly affects businesses. Automatic bug triage has been investigated to increase software productivity, and research has become more active as machine learning techniques have improved. However, most research has focused on open-source projects, whereas studies on industrial projects remain limited. The research gap in previous studies is that the research has directly triaged developers, reducing accuracy in industrial projects where organizational structures frequently change. Moreover, developers often move between teams, making this approach less effective. The research in this article applies automatic bug triage to industrial projects by adapting the characteristics of industrial projects. Addressing these limitations establishes an approach that is better suited to industrial projects and has enhanced accuracy. Based on this background, we propose a novel approach to triage developers associated with component-based developer lists. Each component has an associated list of developers, and the triage results of the model are limited to selecting from among the listed developers, enhancing triage accuracy. The proposed approach reflects the characteristics of industrial projects and addresses the dynamic workload adjustments in a component-based team structure. The proposed approach improves the results by 6.2 percentage points over human triage for top-1 results, suggesting that this approach could be further expanded for broader application in industrial contexts. Future research should focus on refining the proposed method with real-time feedback and experiment with a broader dataset for generalizability and scalability."
The Software Genome Project: Unraveling Software Through Genetic Principles,"Wu, YM; Liu, CW; Xu, ZZ; Zhang, LY; Zhang, YR; Zhu, ZL; Liu, Y",10.1145/3691620.3695307,2024,"Open-source software is crucial to modern development, but its complexity creates challenges in quality, security, and management. Current governance approaches excel at collaboration but struggle with decentralized management and security. With the rise of large language models (LLM)-based software engineering, the need for a finer-grained understanding of software composition is more urgent than ever. To address these challenges, inspired by the Human Genome Project, we treat the software source code as software DNA and propose the Software Genome Project (SGP), which is geared towards the secure monitoring and exploitation of open-source software. By identifying and labeling integrated and classified code features at a fine-grained level, and effectively identifying safeguards for functional implementations and non-functional requirements at different levels of granularity, the SGP could build a comprehensive set of software genome maps to help developers and managers gain a deeper understanding of software complexity and diversity. By dissecting and summarizing functional and undesirable genes, SGP could help facilitate targeted software optimization, provide valuable insight and understanding of the entire software ecosystem, and support critical development tasks such as open source governance. SGP could also serve as a comprehensive dataset with abundant semantic labeling to enhance the training of LLMs for code. Based on these, we expect SGP to drive the evolution of software development towards more efficient, reliable, and sustainable software solutions."
A framework to support educational gerontology based on agile culture,"De Bortoli, LA; De Marchi, ACB; Neves, BB",10.1007/s10639-024-13260-3,2025,"Older adults have unique learning needs that require distinct approaches, particularly in non-formal education settings, which are flexible, systematic, and less bureaucratic. Seeking methods to support these needs, this study explores agile culture-a people-centered, adaptable approach originally from software development that emphasizes collaboration, communication, and gradual planning without rigid rules. The objective is to present Agil-Idade, a non-formal education framework based on agile culture, aimed at supporting older adult education in a way that promotes lifelong learning and meaningful engagement. Agil-Idade was developed through bibliographic research, co-design, and literature mapping, then evaluated in a 14-week exploratory study with six participants aged 60 to 83. Data collection included questionnaires and semi-structured interviews, analyzed using a mixed approach that combined content analysis with descriptive statistics. The study employed both manual coding and qualitative analysis tools (ChatGPT and ATLAS.ti) to establish comprehensive categories, with a Likert scale used for self-assessment in areas such as collaboration, satisfaction, experience sharing, engagement, and communication. Results indicate that Agil-Idade effectively enhanced collaboration and communication, fostering greater satisfaction, engagement, and personal appreciation among participants. These findings suggest that agile culture holds potential for advancing educational practices with older adults, contributing to the literature on non-formal education and supporting personal and social development within this demographic."
Evaluation of Code LLMs on Geospatial Code Generation,"Gramacki, P; Martins, B; Szymanski, P",10.1145/3687123.3698286,2024,"Software development support tools have been studied for a long time, with recent approaches using Large Language Models (LLMs) for code generation. These models can generate Python code for data science and machine learning applications. LLMs are helpful for software engineers because they increase productivity in daily work. An LLM can also serve as a mentor for inexperienced software developers, and be a viable learning support. High-quality code generation with LLMs can also be beneficial in geospatial data science. However, this domain poses different challenges, and code generation LLMs are typically not evaluated on geospatial tasks. Here, we show how we constructed an evaluation benchmark for code generation models, based on a selection of geospatial tasks. We categorised geospatial tasks based on their complexity and required tools. Then, we created a dataset with tasks that test model capabilities in spatial reasoning, spatial data processing, and geospatial tools usage. The dataset consists of specific coding problems that were manually created for high quality. For every problem, we proposed a set of test scenarios that make it possible to automatically check the generated code for correctness. In addition, we tested a selection of existing code generation LLMs for code generation in the geospatial domain. We share our dataset and reproducible evaluation code on a public GitHub repository1, arguing that this can serve as an evaluation benchmark for new LLMs in the future. Our dataset will hopefully contribute to the development new models capable of solving geospatial coding tasks with high accuracy. These models will enable the creation of coding assistants tailored for geospatial applications."
3D printing traceability in healthcare using 3Diamond software,"Capek, L; Schwarz, D",10.1016/j.heliyon.2024.e32664,2024,"Background: 3D printing is one of the fastest-growing technologies in medicine, but it is essential to have a system for 3D printing documentation that is accessible for not only clinical engineers and surgeons, but also quality managers and data-privacy officers in hospitals. Dedicated software such as product lifecycle management (PLM) software could enable comprehensive management and traceability of all data relevant to 3D printing tasks in a hospital and would highly beneficial. Therefore, customizable software called 3Diamond was developed for 3D printing in medicine. Methods: The software development process involved several stages, including setting specifications based on end-user requirements, design, implementation, and testing. In order to ensure the software's long-term success and smooth operation, critical phases were also considered, such as deployment and maintenance. Results: The developed software provides immediate and complete traceability of all preparations and controls, as well as management of reports, orders, stock, and post-operative follow-up of tasks related to 3D printing in a hospital. Based on user requirements, software testing is provided automatically with each release. The software was implemented in a natural clinical environment with a developed 3D printing center. Conclusion: Although 3D printing has potential for innovation in the medical profession, it is nevertheless subject to regulations. Even though there are exemptions for patient-specific products, the effects of their local legal implementations related to 3D printing cannot be fully overseen. To this end, 3Diamond provides a robust system for 3D printing documentation that is accessible to different personnel in hospitals."
Exploring Software Quality Through Data-Driven Approaches and Knowledge Graphs,"Chand, R; Khan, SUR; Hussain, S; Wang, WL; Tang, MH; Ibrahim, N",10.1007/978-3-031-60328-0_37,2024,"Context: The quality of software systems has always been a crucial task and has led to the establishment of various reputable software quality models. However, the automation trends in Software Engineering have challenged the traditional notion of quality assurance, motivating the development of a new paradigm with advanced AI-based quality standards. Objective: The goal of this paper is to bridge the gap between theoretical frameworks and practical implementations on the aspects of software quality. Methodology: This study involved an extensive literature review of software quality models, including McCall, Boehm, Dromey, FURPS, and ISO/IEC 25010. The detailed information about quality attributes from each model was systematically synthesized and organized into datasets, data frames, and Python dictionaries. The resulting resources were then shared and made accessible through a public GitHub repository. Results: In brief, this research provides (i) a comprehensive dataset on software quality containing catalogs of quality models and attributes, (ii) a Python dictionary encapsulating the quality models and their associated characteristics for convenient empirical experimentation, (iii) the application of advanced knowledge graph techniques for the analysis and visualization of software quality parameters, and (iv) the complete construction steps and resources for download, ensuring easy integration and accessibility. Conclusion: This study builds a foundational step towards the standardization of automating software quality modeling to enhance not just quality but also efficiency for software development. For our future work, there will be a concentration on the practical utilization of the dataset in real-world software development contexts."
Bridging Incremental Programming and Complex Software Development Environments,"Boksem, M; van Binsbergen, LT",10.1145/3689488.3689991,2024,"In modern software development, programmers typically choose between two main types of coding environments: Incremental Programming Environments (IPEs), such as the Read-Eval-Print-Loop (REPL) interpreter IPython and the Jupyter Computational Notebook, and Integrated (text-based) Development Environments (IDEs), such as Visual Studio Code. IPEs excel in providing immediate feedback for iterative development, testing, and debugging, making them ideal for fields like data science and AI. However, their typically linear and isolated interface struggles with managing the complexity of larger software projects. Conversely, traditional IDEs support extensive project management and debugging tools suited for complex applications but lack the interactive and incremental nature of IPEs. This paper reports on an ongoing investigation and design of a hybrid environment that combines benefits of IPEs and IDEs and the programming styles they naturally support. Central to our design is a graph structure representing code fragments as nodes and code structure as edges. By considering various types of nodes and relationships (e.g. for representing class membership, execution order, documentation, and dependencies) we can facilitate aspects of both incremental programming (in IPEs) and complexity management (in IDEs). We demonstrate our approach with a prototype, called Incremental Graph Code (IGC), by presenting its architecture and a showcase. We demonstrate IGC's functionality and discuss its potential advantages over existing environments. Key features include advanced code visualization, modular and incremental development, and complexity management. IGC aims to provide a unified, extensible, and flexible development environment that bridges the gap between different styles of programming."
"An intelligent decision support system for on-demand fixture retrieval, adaptation and manufacture","Kasie, FM; Bright, G; Walker, A",10.1108/JMTM-08-2016-0116,2017,"Purpose - The purpose of this paper is to propose a decision support system (DSS) that stabilizes the flow of fixtures in manufacturing systems. The proposed DSS assists decision-makers to reuse or adapt the available fixtures or to manufacture new fixtures depending upon the similarity between the past and new cases. It considers the cost effectiveness of the proposed decision when an adaptation decision is passed. Design/methodology/approach - The research problem is addressed by integrating case-based reasoning, rule-based reasoning and fuzzy set theory. Cases are represented using an object-oriented (OO) approach to characterize them by their feature vectors. The fuzzy analytic hierarchy process (FAHP) and the inverse of weighted Euclidean distance measure are applied for case retrieval. A machining operation is illustrated as a computational example to demonstrate the applicability of the proposed DSS. Findings - The problems of fixture assignment and control have not been well-addressed in the past, although fixture management is one of the complex problems in manufacturing. The proposed DSS is a promising approach to address such kinds of problems using the three components of an artificial intelligence and FAHP. Research limitations/implications - Although the DSS is tested in a laboratory environment using a numerical example, it has not been validated in real industrial systems. Practical implications - The DSS is proposed in terms of simple rules and equations. This implies that it is not complex for software development and implementation. The illustrated numerical example indicates that the proposed DSS can be implemented in the real-world. Originality/value - Demand-driven fixture retrieval and manufacture to assign the right fixtures to planned part-orders using an intelligent DSS is the main contribution. It provides special consideration for the adaptation of the available fixtures in a system."
Integration of Systems Engineering Approach in Product-Lifecycle-Management by Means of a Mechatronic System,"Salehi, V; Burseg, L; Paetzold, K; Chahin, A; Taha, J; Rieger, T",10.1007/978-3-319-49103-5_18,2017,"To achieve the full potential of PLM in Systems Engineering tools especially in view of the system's complexity in industries such as the consumer industry a clear understanding of how best to use such systems is important to product development activities. Systems Engineering is an interdisciplinary field of engineering that focuses on how to design and manage complex engineering systems over their life cycles. Issues such as reliability, logistics, coordination of different teams (requirements management), evaluation measurements, and other disciplines become more difficult when dealing with large or complex projects. Systems Engineering deals with work-processes, optimization methods and tools in such projects. It overlaps technical and human-centered disciplines such as control engineering, industrial engineering, organizational studies, and project management. Systems Engineering ensures that all likely aspects of a project or system are considered, and integrated into a whole. After a short introduction, this paper, which is based on the results of the accomplished descriptive study and literature survey of the Design Research Methodology according to Blessing and Chakrabarti, presents a generic integrated approach of System Driven Product Development (SDPD) and demonstrates the general requirements of a generic integrated approach during the Engineering Design of Systems. The second section presents a new approach of Systems Engineering, which is based on SDPD and will explain the different phases and sub-phases of the developed approach. By means of designing an electric skateboard the different phases of the developed generic integrated approach will be demonstrated and presented. Section three will discuss the results of the Prescriptive Study and address the most important issues. In general, this paper presents the Prescriptive Phase of the Design Research Methodology."
Analysis of Software Developers' Programming Language Preferences and Community Behavior From Big5 Personality Traits,"Mukta, MSH; Antu, BN; Azad, N; Abedeen, I; Islam, N",10.1002/spe.3381,2025,"Many programming languages and technologies have appeared for the purpose of software development. When choosing a programming language, the developers' cognitive attributes, such as the Big5 personality traits (BPT), may play a role. The developers' personality traits can be reflected in their social media content (e.g., tweets, statuses, Q&A, reputation). In this article, we predict the developers' programming language preferences (i.e., the pattern of picking up a language) from their BPT derived from their content produced on social media. We randomly collected data from a total of 820 Twitter (currently X) and Stack Overflow (SO) users. Then, we collected user features (i.e., BPT, word embedding of tweets) from Twitter and programming preferences (i.e., programming tags, reputation, question, answer) from SO. We applied various machine learning (ML) and deep learning (DL) techniques to predict their programming language preferences from their BPT. We also investigated other interesting insights, such as how reputation and question-asking/replying are associated with the users' BPT. The findings suggest that developers with high openness, conscientiousness, and extraversion are inclined to mobile applications, object-oriented programming, and web programming, respectively. Furthermore, developers with high openness and conscientiousness traits have a high reputation in the SO community. Our ML and DL techniques classify the developers' programming language preferences using their BPT with an average accuracy of 78%."
Deep Multiple Assertions Generation,"Wang, HL; Xu, TT; Wang, B",10.1145/3650105.3652293,2024,"Software testing is one of the most crucial parts of the software development life cycle. Developers spend substantial amount of time and efforts on software testing. Recently, there has been a growing scholarly interest in the automation of software testing. However, recent studies have revealed significant limitations in the quality and efficacy of the generated assert statements. These limitations primarily arise due to: (i) the inherent complexity involved in generating assert statements that are both meaningful and effective; (ii) the challenge of capturing the relationship between multiple assertions in a single test case. In recent research, deep learning techniques have been employed to generate meaningful assertions. However, it is typical for a single assertion to be generated for each test case, which contradicts the current situation where over 40% of test cases contain multiple assertions. To address these open challenges, we propose a novel approach, called DeepAssert that exploits the pre-trained model GraphCode-BERT to automatically generate multiple assertions for test methods. It can recommend a sequence of assert statements effectively given a test method and a focal method (the method under test). To evaluate the effectiveness of our approach, we conduct extensive experiments on the dataset built on the top of Methods2Test dataset. Experimental results showthat DeepAssert achieves scores of 54.16%, 18.36%, and 15.38% in terms of CodeBLEU, accuracy and perfect prediction and substantially outperforms the state-of-the-art baselines by a large margin. Furthermore, we evaluate the effectiveness of DeepAssert on the task of bug detection and the result indicates that the assert sequences generated by DeepAssert can assist in exposing 42 real-world bugs extracting from Defects4J while only considering the first compiled assert sequence, outperforming the SOTA approaches by a large margin as well."
5G MEC+AI Pathology Anti-Cancer Guardian: Design of Intelligent Gastric Cancer Auxiliary Diagnosis and Warning Platform for Smart Hospital System,"Sun, X; Shu, X; Hu, JC; Mo, CB",10.1007/978-3-031-61063-9_21,2024,"Purpose: Investigate the technical means of intelligent Gastric Cancer (GC) auxiliary diagnosis and warning, providing treatment guidance for physicians. Methods: The product is developed from a technical perspective, involving algorithm design, software development, and application design. Based on the UNet3+ digital pathology slices auxiliary diagnosis method, it enables precise segmentation of breast cancer pathology slices across the entire field of view, addressing issues such as excessive network complexity, high false positives, and inadequate capture of multi-scale information in existing algorithms. Additionally, utilizing a convolutional neural network-based cancer cell image classification algorithm enables rapid generation of predictive results for pathology slices. Key algorithms in 5G Mobile Edge Computing (MEC) for smart healthcare applications facilitate specific applications of 5G technology in integrated data exchange and device integration within medical consortia, achieving offloading of computational tasks and storage content to MEC nodes. This implementation includes patient physiological indicator warning mechanisms and device management techniques based on MEC nodes. The platform assists physicians in diagnosing more efficiently, aiding patients in earlier recovery, and providing robust support for treatment. Results: The new algorithm accurately segments and predicts entire cancer pathology slices, identifying cancerous regions. Conclusion: The AI Pathology Anti-Cancer Guardian intelligent GC auxiliary diagnosis and warning platform can analyze gastroscopy results more quickly and accurately, aiding physicians in diagnosis and treatment."
Enhancing the Financial Sector with Quantum Computing: A Comprehensive Review of Current and Future Applications,"Claudiu, B; Cosmin, E; Otniel, D; Andrei, A",10.1007/978-981-99-6529-8_17,2024,"Quantum Computing is currently a disruptive and game-changing technology that challenges the development of various industries, and it is a new paradigm in business software development. In this paper, we explore the current and future applications and research of quantum computing in the financial sector and present a SWOT analysis to evaluate its potential impact. We begin by introducing the fundamental concepts of quantum computing. Next, we analyze existing use cases and research on quantum computing in the financial sector. Finally, the SWOT analysis highlights the strengths, weaknesses, opportunities, and threats associated with implementing quantum computing in the financial sector. Our study emphasizes the transformative potential of quantum computing to reshape the financial sector by enabling faster processing, faster communication, better security, new financial services, solving complex problems, and more accurate decision-making processes. Quantum computing can significantly impact the following financial areas: portfolio optimization, risk management, option pricing, high-frequency trading, blockchain and cryptocurrencies, and AI applications (such as fraud detection and credit scoring)."
KFCC: A differentiation-aware and keyword-guided fine-grain code comment generation model,"Zhang, R; Qiao, ZY; Zhang, CH; Yu, JJ",10.1016/j.eswa.2024.123946,2024,"An efficient and accurate understanding of the intent of code is an indispensable skill in computer technology, especially in collaborative engineering and experimental reproduction. AI-assisted automated code comment generator, with the goal of generating programmer-readable explanations, has been an emerging hot topic for software project comprehension. Despite promising performances, three critical issues emerged: 1) The summary comment is limited in understanding the fine-grain details of the code. 2) key-word level guidance in the model should be included for better comments generation. 3) performance of the generative model may be dampened by noises in the manual annotation. In response, we propose a novel fine-grain comment generation, a scenario of generating the statement-level comment with the assistance of method-level comment. We also propose KFCC, a differentiation-aware and keyword-guided fine-grain comment generation model. Specifically, the proposed KFCC model generates the statement-level comments by incorporating the key information extracted by the keyword extractor in a gate fusion way. To enhance the effectiveness and robustness of the proposed KFCC model, we propose a differentiation-aware enhancing encoder comprehension, letting the model distinguish significant knowledge via contrastive learning. Extensive experiments conducted on open-source projects demonstrate that the KFCC model achieves outstanding performance in six programming languages (including Ruby, Python, JavaScript, Java, etc.) on the CodeSearchNet benchmark."
Fusing Code Searchers,"Wang, SW; Geng, MY; Lin, B; Sun, ZS; Wen, M; Liu, YP; Li, L; Bissyande, TF; Mao, XG",10.1109/TSE.2024.3403042,2024,"Code search, which consists in retrieving relevant code snippets from a codebase based on a given query, provides developers with useful references during software development. Over the years, techniques alternatively adopting different mechanisms to compute the relevance score between a query and a code snippet have been proposed to advance the state of the art in this domain, including those relying on information retrieval, supervised learning, and pre-training. Despite that, the usefulness of existing techniques is still compromised since they cannot effectively handle all the diversified queries and code in practice. To tackle this challenge, we present Dancer, a data fusion based code searcher. Our intuition (also the basic hypothesis of this study) is that existing techniques may complement each other because of the intrinsic differences in their working mechanisms. We have validated this hypothesis via an exploratory study. Based on that, we propose to fuse the results generated by different code search techniques so that the advantage of each standalone technique can be fully leveraged. Specifically, we treat each technique as a retrieval system and leverage well-known data fusion approaches to aggregate the results from different systems. We evaluate six existing code search techniques on two large-scale datasets, and exploit eight classic data fusion approaches to incorporate their results. Our experiments show that the best fusion approach is able to outperform the standalone techniques by 35% - 550% and 65% - 825% in terms of MRR (mean reciprocal rank) on the two datasets, respectively."
A Semi-Automated Approach for Resolving Data-Driven Architecture Mismatches,"Karathanasis, C; Maikantis, T; Nikolaidis, N; Ampatzoglou, A; Chatzigeorgiou, A; Mittas, N",10.1109/ICSA-C63560.2024.00007,2024,"In contemporary software development, there is a need for delivering solutions that require the integration of multiple software systems, each one relying on different architectural decisions. For instance, e-shop solutions must communicate with the ERP solution that the company possesses to handle prices, products, and stock. However, such an integration is not always a trivial issue since interoperability problems might arise. A root cause for such interoperability issues is architecture mismatches: e.g., caused by heterogeneity on how data are stored and are expected to be exchanged in the two systems. Interoperability problems can cause delays to the development, require extended communication with different teams, and usually adds complexity to the system. In this paper, we propose a semi-automated AI-based approach and a middleware software solution (a connector) to aid software engineers in  connecting applications with heterogeneous data storing schemas. We have validated our approach and tool with a company that connects ERP systems with e-shops, through a qualitative study."
Semantic Prompt for Task-Adaptive Semantic Communication with Feedback,"He, JH; Wu, SH; Meng, SQ; Zhang, WH; Zhang, QY",10.1109/GLOBECOM52923.2024.10901569,2024,"Task-oriented semantic communications introduce a novel paradigm specifically designed to enhance task-specific performance. However, this paradigm may face limitations as it requires frequent updating with task changes or necessitates storing multiple distinct models for various tasks. To address these challenges, we propose a task-adaptive semantic communication system with feedback (TASC-f), which utilizes a single model capable of adapting to variable tasks. In particular, we formulate a conditional rate-distortion optimization problem, where task-specific prompts serve as dynamic side information to guide coding strategies and enhance task performance. Inspired by visual prompt tuning, we present a learnable semantic prompt model (SPM) coupled with a dynamic parameters network, aimed at effectively extracting task-specific features. A feedback mechanism is also integrated to capture real-time task information, facilitating timely adjustments of the coding policy. In our experiments, we employ the TASC-f system to evaluate its effectiveness across three AI tasks within two distinct scenarios: tasks that are newly introduced and those previously encountered during the training phase. Simulation results show that our proposed TASCf surpasses all data-oriented communication schemes in both scenarios and achieves performance comparable to single-task-oriented semantic systems with reduced communication overhead and fewer model parameters."
"Identification of A0 minimum ablative margins for colorectal liver metastases: multicentre, retrospective study using deformable CT registration and artificial intelligence-based autosegmentation","Paolucci, I; Silva, JAM; Lin, YM; Laimer, G; Cignini, V; Menchini, F; Meira, M; Shieh, A; O'Connor, C; Jones, KA; Gazzera, C; Fonio, P; Brock, KK; Calandri, M; Menezes, M; Bale, R; Odisio, BC",10.1093/bjs/znae165,2024,"Background: Several ablation confirmation software methods for minimum ablative margin assessment have recently been developed to improve local outcomes for patients undergoing thermal ablation of colorectal liver metastases. Previous assessments were limited to single institutions mostly at the place of development. The aim of this study was to validate the previously identified 5 mm minimum ablative margin (A0) using autosegmentation and biomechanical deformable image registration in a multi-institutional setting. Methods: This was a multicentre, retrospective study including patients with colorectal liver metastases undergoing CT- or ultrasound-guided microwave or radiofrequency ablation during 2009-2022, reporting 3-year local disease progression (residual unablated tumour or local tumour progression) rates by minimum ablative margin across all institutions and identifying an intraprocedural contrast-enhanced CT-based minimum ablative margin associated with a 3-year local disease progression rate of less than 1%. Results: A total of 400 ablated colorectal liver metastases (median diameter of 1.5 cm) in 243 patients (145 men; median age of 62 [interquartile range 54-70] years) were evaluated, with a median follow-up of 26 (interquartile range 17-40) months. A total of 119 (48.9%) patients with 186 (46.5%) colorectal liver metastases were from international institutions B, C, and D that were not involved in the software development. Three-year local disease progression rates for 0 mm, >0 and <5 mm, and 5 mm or larger minimum ablative margins were 79%, 15%, and 0% respectively for institution A (where the software was developed) and 34%, 19%, and 2% respectively for institutions B, C, and D combined. Local disease progression risk decreased to less than 1% with an intraprocedurally confirmed minimum ablative margin greater than 4.6 mm. Conclusion: A minimum ablative margin of 5 mm or larger demonstrates optimal local oncological outcomes. It is proposed that an intraprocedural minimum ablative margin of 5 mm or larger, confirmed using biomechanical deformable image registration, serves as the A0 for colorectal liver metastasis thermal ablation."
Open RAN: The Definitive Guide,"Mitra, RN; Rong, B",10.1109/MWC.2024.10495326,2024,"Apart from 3GPP specifications for 5G and beyond, the Open Radio Access Network (RAN) initiative has garnered tremendous excitement from the mobile industry. As a result, the O-RAN Alliance has emerged as the primary open RAN specification body, drawing contributions from major mobile operators and telco players. Leading mobile operators and equipment vendors are actively advocating for the deployment of O-RAN in the field, aiming to break free from vendor lock-in and unleash the full potential of AI-powered cloud-native RAN. This book is a collaborative effort by a group of industry experts, addressing the timely need for a comprehensive understanding of the Open RAN movement, its current status, and future roadmaps. Structured into 12 chapters, the book provides in-depth insights into various technical concepts related to Open RAN. It covers the overview of O-RAN architecture, explores various interfaces, highlights critical security aspects, discusses software development initiatives, and elucidates testing and integration activities. The content is presented in a clear and accessible manner, making it suitable for a broad audience of technical readers. This book explores O-RAN architectures, functions of O-RAN RIC/SMO and interfaces, key procedures, and deployment strategies in diverse cloud environments. The introduction of AI/ML-driven applications on O-RAN RICs accelerates the attainment of goals such as energy reduction in RAN, enhanced user experiences, and enablement of precise network resource management. This is achieved through the development of innovative xApps and rApps. Developers of xApps and rApps, as well as telecommunications professionals in general, can derive valuable insights from this book to better understand the intricacies of O-RAN specifications."
Embarking on Inclusive Voice User Interfaces: Initial Steps in Exploring Technology Integration Within the Seminar 'AI and Educational Sciences',"Busch, M; Ibs, R; Siegert, I",10.1007/978-3-031-60875-9_3,2024,"Voice assistants, widely integrated into daily life, demonstrate vast potential across various applications, primarily catering to typical users with occasional focus on children, the elderly (specifically those with dementia), but not yet extending to those with mental disabilities. This paper explores the underutilization of Voice User Interfaces (VUIs) among individuals with disabilities, particularly those with mental disabilities. Drawing from Smith and Smith's [40] emphasis on inclusive co-design, we advocate for involving individuals with disabilities as well as including domain experts in the development of VUIs from the outset. The AI and Educational Sciences Seminar at the Otto-von-Guericke-Universitat Magdeburg (OVGU) serves as a practical exploration ground, engaging educational sciences students in developing an inclusive Alexa Skill for a diverse user group. Establishing a true co-design process is a complex endeavour, and as Dirks argues in [6], overcoming social challenges is as crucial as addressing technical ones. This work presents an initial step towards 'Developing a Co-Design Process for digital Speech Assistant (DSA) Systems', focusing on the presentation of the concept and the introduction of a local focus group. Our approach aligns with existing literature on involving domain experts, technicians, and individuals with disabilities in complex software development. To meet the diverse expertise needed for effective VUI development, we advocate for a comprehensive curriculum covering technical skills, domain knowledge, and human-computer interaction. The paper concludes by introducing a prototype tool, a Dialog-Content Management Systems (Dialog-CMSs) for Alexa Skills, enabling non-programming educational sciences students to contribute to inclusive VUI development."
A Multidocument Summarization Technique for Informative Bug Summaries,"Mukhtar, S; Lee, S; Heo, J",10.1109/ACCESS.2024.3487443,2024,"To help developers grasp bug information, bug summaries should contain bug descriptions and information on the reproduction steps, environment, and solutions to be informative for developers. However, previously established bug report summarization techniques generate bug summaries mainly by identifying significant sentences, which often miss those bug report attributes. In this paper, we aim to generate informative summaries that cover these specific bug report attributes in a structured form. There are two challenges. First, the relevant information is sometimes scattered over multiple sources. Second, information on the reproduction steps and environment is often filtered out by previous techniques, which identify significant sentences on the basis of their relationships. Therefore, we propose a bug summarization technique that collects information from multiple sources, including duplicates and pull requests, and a classification technique for identifying sentences that provide relevant information on the reproduction steps and environment. Our proposed technique, ClaSum, consists of four steps: preprocessing, classification, sentence ranking, and summarization. We adopted RoBERTa for our classification step, Opinion and Topic association scores for the sentence ranking step, and BART for the summarization step. Our comparative experiments show that our technique outperforms the state-of-the-art technique BugSum in terms of the F1 score by 14%, 8%, and 35% on the SDS, ADS, and DDS datasets, respectively. Additionally, our qualitative investigation shows that our technique generates a more structural summary than two well-known LLMs, Gemini and Claude."
Qiskit Code Assistant: Training LLMs for generating Quantum Computing Code,"Dupuis, N; Buratti, L; Vishwakarma, S; Forrat, AV; Kremer, D; Faro, I; Puri, R; Cruz-Benito, J",10.1109/LAD62341.2024.10691762,2024,"Code Large Language Models (Code LLMs) have emerged as powerful tools, revolutionizing the software development landscape by automating the coding process and reducing time and effort required to build applications. This paper focuses on training Code LLMs to specialize in the field of quantum computing. We begin by discussing the unique needs of quantum computing programming, which differ significantly from classical programming approaches or languages. A Code LLM specializing in quantum computing requires a foundational understanding of quantum computing and quantum information theory. However, the scarcity of available quantum code examples and the rapidly evolving field, which necessitates continuous dataset updates, present significant challenges. Moreover, we discuss our work on training Code LLMs to produce high-quality quantum code using the Qiskit library. This work includes an examination of the various aspects of the LLMs used for training and the specific training conditions, as well as the results obtained with our current models. To evaluate our models, we have developed a custom benchmark, similar to HumanEval, which includes a set of tests specifically designed for the field of quantum computing programming using Qiskit. Our findings indicate that our model outperforms existing state-of-the-art models in quantum computing tasks. We also provide examples of code suggestions, comparing our model to other relevant code LLMs. Finally, we introduce a discussion on the potential benefits of Code LLMs for quantum computing computational scientists, researchers, and practitioners. We also explore various features and future work that could be relevant in this context."
MalwareBench: Malware samples are not enough,"Zahan, N; Burckhardt, P; Lysenko, M; Aboukhadijeh, F; Williams, L",10.1145/3643991.3644883,2024,"The prevalent use of third-party components in modern software development, rapid modernization, and digitization have significantly amplified the risk of software supply chain attacks. Popular large registries like npm and PyPI are highly targeted malware distribution channels for attackers due to heavy growth and dependence on third-party components. Industry and academia are working towards building tools to detect malware in the software supply chain. However, a lack of benchmark datasets containing both malicious and neutral packages hampers the evaluation of the performance of these malware detection tools. The goal of our study is to aid researchers and tool developers in evaluating and improving malware detection tools by contributing a benchmark dataset built by systematically collecting malicious and neutral packages from the npm and PyPI ecosystems. We present MalwareBench, a labeled dataset of 20,792 packages (of which 6,659 are malicious) from the npm and PyPI ecosystems. We constructed the benchmark dataset by incorporating pre-existing malware datasets with the Socket internal benchmark data and including popular and newly released npm and PyPI packages. The ground truth labels of these neutral packages were determined using the Socket AI Scanner and manual inspection."
Gotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in Code Models,"Yang, Z; Zhao, ZP; Wang, CY; Shi, JK; Kim, D; Han, D; Lo, D",10.1109/TSE.2024.3482719,2024,"Leveraging large-scale datasets from open-source projects and advances in large language models, recent progress has led to sophisticated code models for key software engineering tasks, such as program repair and code completion. These models are trained on data from various sources, including public open-source projects like GitHub and private, confidential code from companies, raising significant privacy concerns. This paper investigates a crucial but unexplored question: What is the risk of membership information leakage in code models? Membership leakage refers to the vulnerability where an attacker can infer whether a specific data point was part of the training dataset. We present Gotcha, a novel membership inference attack method designed for code models, and evaluate its effectiveness on Java-based datasets. Gotcha simultaneously considers three key factors: model input, model output, and ground truth. Our ablation study confirms that each factor significantly enhances attack performance. Our ablation study confirms that each factor significantly enhances attack performance. Our investigation reveals a troubling finding: membership leakage risk is significantly elevated. While previous methods had accuracy close to random guessing, Gotcha achieves high precision, with a true positive rate of 0.95 and a low false positive rate of 0.10. We also demonstrate that the attacker's knowledge of the victim model (e.g., model architecture and pre-training data) affects attack success. Additionally, modifying decoding strategies can help reduce membership leakage risks. This research highlights the urgent need to better understand the privacy vulnerabilities of code models and develop strong countermeasures against these threats."
ID3-driven insights: the proactive prosumer's role in technological innovation,"Abdelaziz, MAA; Ghonim, MA; Wu, JN; Almandooh, AMA",10.1108/TQM-05-2024-0200,2024,"PurposeThe study aims to reveal the relative importance of each characteristic of a proactive prosumer and determine the role of each characteristic in improving technological innovation.Design/methodology/approachOur data were collected via emails using a final sample of 280 technical managers from Egypt's information technology (IT) firms who have some software development expertise. The data were analyzed using the iterative dichotomiser 3 (ID3) algorithm.FindingsThe results indicate that proactive prosumer characteristics (initiative and creativity) positively affect technological innovation. Furthermore, there is no technological innovation without initiative.Research limitations/implicationsThis study uses ID3 decision tree analysis to analyze the role of proactive prosumers in technological innovation. It greatly advances the area by addressing the need for quantitative studies in administrative sciences. The study highlights initiative and creativity as essential innovation drivers, contributing to consumer and prosumption theories and the changing consumer-firm connection. The ID3 algorithm distinguishes the study from earlier statistical studies. The study's limitations advise increasing prosumer literature, studying psychological aspects and comparing cultures. Use various algorithms and moderator and mediator factors to improve methodology. Technical managers' opinions must also be understood.Originality/valueThis research contributes to the existing knowledge base by examining the intricate relationship between proactive prosumers and technological innovation. It offers a novel approach by employing the ID3 decision tree algorithm within the administrative sciences, a domain that needs to be explored in prosumer research. By bridging the methodological gap, this study addresses the need for more quantitative investigations into prosumers in the management field. Furthermore, it pioneers the application of artificial intelligence techniques to analyze the prosumer phenomenon quantitatively. The paper concludes by outlining potential avenues for future research."
PublicVision: A Secure Smart Surveillance System for Crowd Behavior Recognition,"Qaraqe, M; Elzein, A; Basaran, E; Yang, Y; Varghese, EB; Costandi, W; Rizk, J; Alam, N",10.1109/ACCESS.2024.3366693,2024,"Crowd behavior recognition plays a critical role in various domains, including public safety, event management, and urban planning. Understanding crowd dynamics and detecting behaviors based on violence levels are crucial for preventing incidents and maintaining order in crowded environments. However, traditional surveillance methods fall short of providing comprehensive and real-time insights into complex crowd behavior patterns and fail to distinguish different violence levels within crowds that affect proactive decision-making. Moreover, most of the current systems do not provide reliable secure data transmission and are not viable in protecting the privacy of individuals. This paper designs an end-to-end secure and smart surveillance system, namely PublicVision, that transmits CCTV data securely to a remote central hub where a deep learning (DL) model based on Swin Transformer is utilized to identify and analyze crowd behaviors. A novel video dataset was created to train the DL model that identifies crowds based on size and violence level. The proposed system incorporates end-to-end security by creating a Dynamic Multipoint Virtual Private Network (DMVPN) and leverages the property of IP Security (IPSec) and Firewall for confidentiality and integrity during transmission and storage. Experiment analysis and real-time inference using DeepStream Software Development Kit (SDK) proved that the proposed system has significant implications for public safety, security, and crowd management in various contexts, including public spaces, transportation hubs, and large-scale events."
CogCol: Code Graph-Based Contrastive Learning Model for Code Summarization,"Shi, YC; Yin, Y; Yu, MQ; Chu, LY",10.3390/electronics13101816,2024,"Summarizing source code by natural language aims to help developers better understand existing code, making software development more efficient. Since source code is highly structured, recent research uses code structure information like Abstract Semantic Tree (AST) to enhance the structure understanding rather than a normal translation task. However, AST can only represent the syntactic relationship of code snippets, which can not reflect high-level relationships like control and data dependency in the program dependency graph. Moreover, researchers treat the AST as the unique structure information of one code snippet corresponding to one summarization. It will be easily affected by simple perturbations as it lacks the understanding of code with similar structure. To handle the above problems, we build CogCol, a Code graph-based Contrastive learning model. CogCol is a Transformer-based model that converts code graphs into unique sequences to enhance the model's structure learning. In detail, CogCol uses supervised contrastive learning by building several kinds of code graphs as positive samples to enhance the structural representation of code snippets and generalizability. Moreover, experiments on the widely used open-source dataset show that CogCol can significantly improve the state-of-the-art code summarization models under Meteor, BLEU, and ROUGE."
Revisiting the Conflict-Resolving Problem from a Semantic Perspective,"Dong, JH; Sun, J; Lin, Y; Zhang, YD; Ma, MR; Dong, JS; Hao, D",10.1145/3691620.3694993,2024,"Collaborative software development significantly enhances development productivity by enabling multiple contributors to work concurrently on different branches. Despite these advantages, such collaboration often increases the likelihood of causing conflicts. Resolving these conflicts brings huge challenges, primarily due to the necessity of comprehending the differences between conflicting versions. Researchers have explored various automatic conflict resolution techniques, including unstructured, structured, and learning-based approaches. However, these techniques are mostly heuristic-based or black-box in nature, which means they do not attempt to solve the root cause of the conflicts, i.e., the existence of different program behaviors exhibited by the conflicting versions. In this work, we propose sMerge, a novel conflict resolution approach based on the semantics of program behaviors. We first give the formal definition of the merge conflict problem as well as the specific conditions under which conflicts happen and the criteria employed to select certain version as the resolution. Based on the definition, we propose to resolve the conflicts from the perspective of program behaviors. In particular, we argue that the key to resolving conflicts is identifying different program behaviors, and thus can be solved through targeted test generation. We conduct an extensive evaluation of sMerge using a comprehensive dataset of conflicts sourced from various projects. Our results show that sMerge can effectively solve the merge problem by employing different test generation techniques, including search-based, GPT-based, and manual testing. We remark that sMerge provides a way to understand the program behavior differences through testing, which not only allows us to solve the merge problem soundly but also enables the detection of incorrect ground truths provided by developers, thereby enhancing the reliability of the merge process."
Recent advances in Machine Learning based Advanced Driver Assistance System applications,"Tatar, G; Bayar, S; Cicek, I; Niar, S",10.1016/j.micpro.2024.105101,2024,"In recent years, the rise of traffic in modern cities has demanded novel technology to support the drivers and protect the passengers and other third parties involved in transportation. Thanks to rapid technological progress and innovations, many Advanced Driver Assistance Systems (A/DAS) based on Machine Learning (ML) algorithms have emerged to address the increasing demand for practical A/DAS applications. Fast and accurate execution of A/DAS algorithms is essential for preventing loss of life and property. High-speed hardware accelerators are vital for processing the high volume of data captured by increasingly sophisticated sensors and complex mathematical models' execution of modern deep learning (DL) algorithms. One of the fundamental challenges in this new era is to design energy-efficient and portable ML-enabled platforms for vehicles to provide driver assistance and safety. This article presents recent progress in ML-driven A/DAS technology to offer new insights for researchers. We covered standard ML models and optimization approaches based on widely accepted open-source frameworks extensively used in A/DAS applications. We have also highlighted related articles on ML and its sub-branches, neural networks (NNs), and DL. We have also reported the implementation issues, bench-marking problems, and potential challenges for future research. Popular embedded hardware platforms such as Field Programmable Gate Arrays (FPGAs), central processing units (CPUs), Graphical Processing Units (GPUs), and Application Specific Integrated Circuits (ASICs) used to implement A/DAS applications are also compared concerning their performance and resource utilization. We have examined the hardware and software development environments used in implementing A/DAS applications and reported their advantages and disadvantages. We provided performance comparisons of usual A/DAS tasks such as traffic sign recognition, road and lane detection, vehicle and pedestrian detection, driver behavior, and multiple tasking. Considering the current research dynamics, A/DAS will remain one of the most popular application fields for vehicular transportation shortly."
MicroBooNE Public Data Sets: a Collaborative Tool for LArTPC Software Development,"Cerati, G",10.1051/epjconf/202429508012,2024,"Among liquid argon time projection chamber (LArTPC) experiments MicroBooNE is the one that continually took physics data for the longest time (2015-2021), and represents the state of the art for reconstruction and analysis with this detector. Recently published analyses include oscillation physics results, searches for anomalies and other BSM signatures, and cross section measurements. LArTPC detectors are being used in current experiments such as ICARUS and SBND, and being planned for future experiments such as DUNE. MicroBooNE has recently released to the public two of its data sets, with the goal of enabling collaborative software developments with other LArTPC experiments and with AI or computing experts. These data sets simulate neutrino interactions on top of off-beam data, which include cosmic ray background and noise. The data sets are released in two formats: the native art/ROOT format used internally by the collaboration and familiar to other LArTPC experts, and the HDF5 format which contains reduced and simplified content and is suitable for usage by the broader community. This contribution presents the open data sets, discusses their motivation, the technical implementation, and the extensive documentation - all inspired by FAIR principles. Finally, opportunities for collaborations are discussed."
FastLog: An End-to-End Method to Efficiently Generate and Insert Logging Statements,"Xie, XY; Cai, ZP; Chen, SQ; Xuan, JF",10.1145/3650212.3652107,2024,"Logs play a crucial role in modern software systems, serving as a means for developers to record essential information for future software maintenance. As the performance of these log-based maintenance tasks heavily relies on the quality of logging statements, various works have been proposed to assist developers in writing appropriate logging statements. However, these works either only support developers in partial sub-tasks of this whole activity; or perform with a relatively high time cost and may introduce unwanted modifications. To address their limitations, we propose FastLog, which can support the complete logging statement generation and insertion activity, in a very speedy manner. Specifically, given a program method, FastLog first predicts the insertion position in the finest token level, and then generates a complete logging statement to insert. We further use text splitting for long input texts to improve the accuracy of predicting where to insert logging statements. A comprehensive empirical analysis shows that our method outperforms the state-of-the-art approach in both efficiency and output quality, which reveals its great potential and practicality in current real-time intelligent development environments."
Automatic Commit Message Generation: A Critical Review and Directions for Future Work,"Zhang, YX; Qiu, ZQ; Stol, KJ; Zhu, WH; Zhu, JX; Tian, YC; Liu, H",10.1109/TSE.2024.3364675,2024,"Commit messages are critical for code comprehension and software maintenance. Writing a high-quality message requires skill and effort. To support developers and reduce their effort on this task, several approaches have been proposed to automatically generate commit messages. Despite the promising performance reported, we have identified three significant and prevalent threats in these automated approaches: 1) the datasets used to train and evaluate these approaches contain a considerable amount of 'noise'; 2) current approaches only consider commits of a limited diff size; and 3) current approaches can only generate the subject of a commit message, not the message body. The first limitation may let the models 'learn' inappropriate messages in the training stage, and also lead to inflated performance results in their evaluation. The other two threats can considerably weaken the practical usability of these approaches. Further, with the rapid emergence of large language models (LLMs) that show superior performance in many software engineering tasks, it is worth asking: can LLMs address the challenge of long diffs and whole message generation? This article first reports the results of an empirical study to assess the impact of these three threats on the performance of the state-of-the-art auto generators of commit messages. We collected commit data of the Top 1,000 most-starred Java projects in GitHub and systematically removed noisy commits with bot-submitted and meaningless messages. We then compared the performance of four approaches representative of the state-of-the-art before and after the removal of noisy messages, or with different lengths of commit diffs. We also conducted a qualitative survey with developers to investigate their perspectives on simply generating message subjects. Finally, we evaluate the performance of two representative LLMs, namely UniXcoder and ChatGPT, in generating more practical commit messages. The results demonstrate that generating commit messages is of great practical value, considerable work is needed to mature the current state-of-the-art, and LLMs can be an avenue worth trying to address the current limitations. Our analyses provide insights for future work to achieve better performance in practice."
One Size Does Not Fit All: Multi-granularity Patch Generation for Better Automated Program Repair,"Lin, B; Wang, SW; Wen, M; Chen, LQ; Mao, XG",10.1145/3650212.3680381,2024,"Automated program repair aims to automate bug correction and alleviate the burden of manual debugging, which plays a crucial role in software development and maintenance. Recent studies reveal that learning-based approaches have outperformed conventional APR techniques (e.g., search-based APR). Existing learning-based APR techniques mainly center on treating program repair either as a translation task or a doze task. The former primarily emphasizes statement-level repair, while the latter concentrates on token-level repair, as per our observations. In practice, however, patches may manifest at various repair granularity, including statement, expression, or token levels. Consequently, merely generating patches from a single granularity would be ineffective to tackle real-world defects. Motivated by this observation, we propose Mulpor, a multigranularity patch generation approach designed to address the diverse nature of real-world bugs. Mulpor comprises three components: statement-level, expression-level, and token-level generator, each is pre-trained to generate correct patches at its respective granularity. The approach involves generating candidate patches from various granularities, followed by a re-ranking process based on a heuristic to prioritize patches. Experimental results on the Defects4J dataset demonstrate that Mulpor correctly repair 92 bugs on Defects4J-v1.2, which achieves 27.0% (20 bugs) and 12.2% (10 bugs) improvement over the previous state-of-the-art NMT-style Rap-Gen and Cloze-style GAMMA. We also investigated the generalizability of Mulpor in repairing vulnerabilities, revealing a notable 51% increase in the number of correctly-fixed patches compared with state-of-the-art vulnerability repair approaches. This paper underscores the importance of considering multiple granularities in program repair techniques for a comprehensive strategy to address the diverse nature of real-world software defects. Mulpor, as proposed herein, exhibits promising results in achieving effective and diverse bug fixes across various program repair scenarios."
A Survey of Learning-based Automated Program Repair,"Zhang, QJ; Fang, CR; Ma, YX; Sun, WS; Chen, ZY",10.1145/3631974,2024,"Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance. In this article, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our article can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at the repository: https://github.com/iSEngLab/AwesomeLearningAPR."
When simplicity meets effectiveness: Detecting code comments coherence with word embeddings and LSTM,"Igbomezie, MD; Nguyen, PT; Di Ruscio, D",10.1145/3661167.3661187,2024,"Code comments play a crucial role in software development, as they provide programmers with practical information, allowing them to understand better the intent and semantics of the underpinning code. Nevertheless, developers tend to leave comments unchanged after updating the code, resulting in a discrepancy between the two artifacts. Such a discrepancy may trigger misunderstanding and confusion among developers, impeding various activities, including code comprehension and maintenance. Thus, it is crucial to identify if, given a code snippet, its corresponding comment is coherent and reflects well the intent behind the code. Unfortunately, existing approaches to this problem, while obtaining an encouraging performance, either rely on heavily pre-trained models, or treat input data as text, neglecting the intrinsic features contained in comments and code, including word order and synonyms. We present Co3D as a practical approach to the detection of code comment coherence. We pay attention to internal meaning of words and sequential order of words in text while predicting coherence in code-comment pairs. We deployed a combination of Gensim word2vec encoding and a simple recurrent neural network, a combination of Gensim word2vec encoding and an LSTM model, and CodeBERT. The first two methods can be considered a manual version of the last two methods, with the last two methods involving the use of transfer learning from large pre-trained models. The experimental results show that Co3D obtains a promising prediction performance, thus outperforming well-established baselines. We conclude that depending on the context, using a simple architecture can introduce a satisfying prediction."
Integrating Observability with DevOps Practices in Financial Services Technologies: A Study on Enhancing Software Development and Operational Resilience,"Mahida, A",,2024,"The finance market closely depends on translation and high-quality software solutions when performing crucial transactions and processing important information and customer services. Thus, systems' reliability and good performance become crucial when these systems become complicated. This paper aims to focus on the implementation of the observability concept with the DevOps approach in financial services technologies, where its strengths, weaknesses, opportunities, and threats are also discussed with regard to the future. The concept of observability is intertwined with DevOps since, with its help, it is possible to gain deep insights into the system's inner state and further enhance status monitoring, detect problems in less time, and optimize performance constantly. When organized and analyzed properly, observability data can, therefore, play a critical role in increasing software quality in financial institutions, aligning with regulatory standards, and decreasing development and operations teams' silos. However, ver, the implementation of observability within an organization using DevOps best practices in the financial services industry has some challenges, which include The issue of security, especially when it comes to data, the Challenge of data overload, the challenging task of encouraging the right organizational culture for continuous and consistent observability. The article presents a guide that discusses how to incorporate observability with DevOps: the step-by-step process of defining observability needs, choosing the most suitable tools, integrating with other tools in the existing DevOps frameworks, laboratory of alarms, and constant enhancement. Furthermore, it considers examples of how some financial organizations have applied observability to reduce risks, improve efficacy, and enrich customers' interactions. In addition, the article also deliberates on the future perspectives of observability, for instance, artificial intelligence and machine learning are quickly emerging as means through which different tasks of observability can be automated, and there are increasing concerns with security when it comes to the implementation of observability in the financial services industry. By adopting observability and aligning it with DevOps, financial institutions can develop and sustain sound, reliable and high-quality infrastructure and maintain the industry's leadership."
Conquering the Robotic Software Development Cycle at Scale: Using KubeROS from Simulation to Real-world Deployment,"Zhang, YZ; Pasch, F; Minis, F; Scholl, KU; Wurll, C; Hein, B",10.1109/CASE59546.2024.10711591,2024,"Integrating state-of-the-art robotics algorithms into a working system remains a challenge, especially for largescale deployments. One promising way to address such issues is to apply to robotics the massive cloud and edge computing that already powers AI research from training to deployment. In this paper, we extend the KubeROS platform to serve as a one-stop solution aimed at facilitating robotic development cycles from research to deployment. The closed-loop workflow maintains a single software stack containing many microserviceoriented and containerized software modules across all stages. In addition to the architectural adaptation, we develop a new subsystem called BatchJob to leverage cloud/edge to efficiently run experiments at scale and evaluate the system with statistical significance. Finally, our approach enables a seamless transition to production and closes the cycle for continuous improvement. To demonstrate its applicability and scalability, we choose mobile robot navigation as an example and perform experiments on three levels: comparison of localization approaches with benchmark datasets, testing and evaluation with closedloop simulation, and evaluation on a real robot. Compared to sequential execution on a single machine, our approach accelerates experiments by at least a factor of 30 on a 9node edge cluster. Our platform and code to reproduce the experiments are available on Github."
Automatic Classification of Recurring Tasks in Software Development Projects,"Wysocki, W; Ochodek, M",10.1109/SEAA64295.2024.00076,2024,"Background: Information about project tasks stored in Issue tracking systems (ITS) can be used for project analytics or process simulation. Such tasks can be categorized as stateful or recurrent. Although automatic categorization of stateful tasks is relatively simple, performing the same task for recurrent tasks constitutes a challenge. Aims: The goal of this study is to investigate the possibility of employing machine-learning algorithms to automatically categorize recurrent tasks in software projects based on information stored in ITS. Method: We perform a study on a dataset from six industrial projects containing 9,589 tasks and augment it with an additional dataset of 91,145 task descriptions from other industrial projects to up-sample minority classes during training. We perform ten runs of 10-fold cross-validation for each project and evaluate classifiers using a set of state-of-the-art prediction quality metrics, i.e., Accuracy, Precision, Recall, F1-score, and MCC. Our machine-learning pipeline includes a Transformer-based sentence embedder ('mxbai-embed-large-v1') and XGBoost classifier. Results: The model automatically classifies software process tasks into 14 classes with MCC ranging between 0.69 and 0.88 (mean: 0.77). We observed higher prediction quality for the largest projects in the dataset and those managed according to traditional project management methodologies. Conclusions: We conclude that machine-learning algorithms can effectively categorize recurrent tasks. However, it requires collecting a large balanced dataset of ITS tasks or using a pre-trained model like the one provided in this study."
A Decision Support System for Crop Recommendation Using Machine Learning Classification Algorithms,"Senapaty, MK; Ray, A; Padhy, N",10.3390/agriculture14081256,2024,"Today, crop suggestions and necessary guidance have become a regular need for a farmer. Farmers generally depend on their local agriculture officers regarding this, and it may be difficult to obtain the right guidance at the right time. Nowadays, crop datasets are available on different websites in the agriculture sector, and they play a crucial role in suggesting suitable crops. So, a decision support system that analyzes the crop dataset using machine learning techniques can assist farmers in making better choices regarding crop selections. The main objective of this research is to provide quick guidance to farmers with more accurate and effective crop recommendations by utilizing machine learning methods, global positioning system coordinates, and crop cloud data. Here, the recommendation can be more personalized, which enables the farmers to predict crops in their specific geographical context, taking into account factors like climate, soil composition, water availability, and local conditions. In this regard, an existing historical crop dataset that contains the state, district, year, area-wise production rate, crop name, and season was collected for 246,091 sample records from the Dataworld website, which holds data on 37 different crops from different areas of India. Also, for better analysis, a dataset was collected from the agriculture offices of the Rayagada, Koraput, and Gajapati districts in Odisha state, India. Both of these datasets were combined and stored using a Firebase cloud service. Thirteen different machine learning algorithms have been applied to the dataset to identify dependencies within the data. To facilitate this process, an Android application was developed using Android Studio (Electric Eel | 2023.1.1) Emulator (Version 32.1.14), Software Development Kit (SDK, Android SDK 33), and Tools. A model has been proposed that implements the SMOTE (Synthetic Minority Oversampling Technique) to balance the dataset, and then it allows for the implementation of 13 different classifiers, such as logistic regression, decision tree (DT), K-Nearest Neighbor (KNN), SVC (Support Vector Classifier), random forest (RF), Gradient Boost (GB), Bagged Tree, extreme gradient boosting (XGB classifier), Ada Boost Classifier, Cat Boost, HGB (Histogram-based Gradient Boosting), SGDC (Stochastic Gradient Descent), and MNB (Multinomial Naive Bayes) on the cloud dataset. It is observed that the performance of the SGDC method is 1.00 in accuracy, precision, recall, F1-score, and ROC AUC (Receiver Operating Characteristics-Area Under the Curve) and is 0.91 in sensitivity and 0.54 in specificity after applying the SMOTE. Overall, SGDC has a better performance compared to all other classifiers implemented in the predictions."
Function-Level Compilation Provenance Identification with Multi-Faceted Neural Feature Distillation and Fusion,"Gao, Y; Liang, LJ; Li, YF; Li, R; Wang, Y",10.3390/electronics13091692,2024,"In the landscape of software development, the selection of compilation tools and settings plays a pivotal role in the creation of executable binaries. This diversity, while beneficial, introduces significant challenges for reverse engineers and security analysts in deciphering the compilation provenance of binary code. To this end, we present MulCPI, short for Multi-representation Fusion-based Compilation Provenance Identification, which integrates the features collected from multiple distinct intermediate representations of the binary code for better discernment of the fine-grained function-level compilation details. In particular, we devise a novel graph-oriented neural encoder improved upon the gated graph neural network by subtly introducing an attention mechanism into the neighborhood nodes' information aggregation computation, in order to better distill the more informative features from the attributed control flow graph. By further integrating the features collected from the normalized assembly sequence with an advanced Transformer encoder, MulCPI is capable of capturing a more comprehensive set of features manifesting the multi-faceted lexical, syntactic, and structural insights of the binary code. Extensive evaluation on a public dataset comprising 854,858 unique functions demonstrates that MulCPI exceeds the performance of current leading methods in identifying the compiler family, optimization level, compiler version, and the combination of compilation settings. It achieves average accuracy rates of 99.3%, 96.4%, 90.7%, and 85.3% on these tasks, respectively. Additionally, an ablation study highlights the significance of MulCPI's core designs, validating the efficiency of the proposed attention-enhanced gated graph neural network encoder and the advantages of incorporating multiple code representations."
A Novel Throughput Enhancement Method for Deep Learning Applications on Mobile Devices With Heterogeneous Processors,"Park, C; Ha, S",10.1109/ACCESS.2024.3375517,2024,"Contemporary smartphones integrate dedicated AI accelerators alongside CPUs and GPUs, in response to the growing demand for deep learning applications. While existing software development kits (SDKs) for these devices provide neural network optimization techniques, they often lack system-level optimizations, specifically in distributing layers across heterogeneous processors. This paper introduces a novel approach to enhance the throughput of deep learning applications through the utilization of quantization and pipelining techniques. The proposed technique employs different quantization schemes for activation data and filter weights to minimize accuracy drop. A genetic algorithm is employed to explore the extensive design space of layer-wise mapping and pipelining, aiming to find the best pipelining solution. To estimate performance of each solution candidate, the actual execution time of the application on the device is measured, accounting for unique smartphone characteristics, such as dynamic voltage and frequency scaling (DVFS) and OS scheduling. The impact of thermal throttling on throughput is also investigated by running benchmark applications continuously for 10 minutes. Our technique is validated through experiments conducted on Google Pixel 6 and Samsung Galaxy S22. Throughput enhancements, rangingfromx5.4 tox7.6 on Google Pixel 6 andx35.5 tox44.2 on Samsung Galaxy S22, are achieved, compared to single-processor mappings for networks with floating-point parameters. It confirms that significant performance improvements can be achieved through the proposed software optimization methodology on contemporary smartphones with diverse constraints at the user level."
"Oxygen vacancies-elusive but common catalytic key defects for thermal upgrading of CO2 to CH4, CO and CH3OH","Cisneros, S; Rabeah, J",10.1016/j.jechem.2024.10.044,2025,"Single carbon products (C1 compounds) are simple but important chemicals in the road towards energy transition. Catalytic conversion of CO2 with H2 (desirably renewable) can be performed over reducible oxides supporting transition metals to obtain products such as CH4, CO and MeOH. Oxygen vacancies (O-vacancies), which are inherent defects of reducible metal oxides, play an enormous role in driving the catalytic performance (activity, selectivity, stability) for the desired reactions. Yet, the assessment of O-defects at realistic conditions is often complex. Only few techniques can provide direct evidence for their existence and influence in CO2 activation. Among them, electron paramagnetic spectroscopy (EPR), Raman spectroscopy, scanning probe microscopies (SPM) and environmental transmission electron microscopy (ETEM) are nowadays the most informative. In most cases, however, the measurements require reaction conditions far away from CO2 valorization applications. Although great efforts have been fruitful in explaining and demonstrating the huge importance of O-vacancies in CO2 catalysis, still ambiguous or erroneous interpretations about structure-function correlations involving O-vacancies are found in literature, especially, when information is not properly gathered, e.g., by O 1s ex-situ X-ray photon spectroscopy (XPS). Moreover, despite the recognized importance of O-vacancies for CO2 valorization, critical literature compilations about their effects in thermal processes are scarce. Herein, we attempt to contribute in closing this gap by integrally encompassing representative investigations on the thermo-catalytic production of CH4, CO and MeOH. Particularly, we emphasize on the proper selection of assessment tools (direct/indirect) to unambiguously establish structure-function relationships to design optimized O-defective catalysts for the targeted compounds. (c) 2024 Science Press and Dalian Institute of Chemical Physics, Chinese Academy of Sciences. Published by Elsevier B.V. and Science Press. All rights are reserved, including those for text and data mining, AI training, and similar technologies."
Exploration of individual colorectal cancer cell responses to H2O2 eustress using hopping probe scanning ion conductance microscopy,"Wang, D; Woodcock, E; Yang, X; Nishikawa, H; Sviderskaya, E; Oshima, M; Edwards, C; Zhang, YJ; Korchev, Y",10.1016/j.scib.2024.04.004,2024,"Colorectal cancer (CRC), a widespread malignancy, is closely associated with tumor microenvironmental hydrogen peroxide (H 2 O 2 ) levels. Some clinical trials targeting H 2 O 2 for cancer treatment have revealed its paradoxical role as a promoter of cancer progression. Investigating the dynamics of cancer cell H 2 O 2 eustress at the single-cell level is crucial. In this study, non-contact hopping probe mode scanning ion conductance microscopy (HPICM) with high-sensitive Pt-functionalized nanoelectrodes was employed to measure dynamic extracellular to intracellular H 2 O 2 gradients in individual colorectal cancer Caco-2 cells. We explored the relationship between cellular mechanical properties and H 2 O 2 gradients. Exposure to 0.1 or 1 mmol/L H 2 O 2 eustress increased the extracellular to intracellular H 2 O 2 gradient from 0.3 to 1.91 or 3.04, respectively. Notably, cellular F-actin-dependent stiffness increased at 0.1 mmol/L but decreased at 1 mmol/L H 2 O 2 eustress. This H 2 O 2 -induced stiffness modulated AKT activation positively and glutathione peroxidase 2 (GPX2) expression negatively. Our findings unveil the failure of some H 2 O 2-targeted therapies due to their ineffectiveness in generating H 2 O 2 , which instead acts eustress to promote cancer cell survival. This research also reveals the complex interplay between physical properties and biochemical signaling in cancer cells' antioxidant defense, illuminating the exploitation of H 2 O 2 eustress for survival at the single-cell level. Inhibiting GPX and/or catalase (CAT) enhances the cytotoxic activity of H 2 O 2 eustress against CRC cells, which holds significant promise for developing innovative therapies targeting cancer and other H 2 O 2- related inflammatory diseases. (c) 2024 Science China Press. Published by Elsevier B.V. and Science China Press. All rights are reserved, including those for text and data mining, AI training, and similar technologies."
"Foundations of population-based SHM, Part V: Network, framework and database","Brennan, DS; Gosliga, J; Cross, EJ; Worden, K",10.1016/j.ymssp.2024.111602,2025,"This paper represents the final part of a sequence on the foundations of Population-Based Structural Health Monitoring (PBSHM). The previous papers presented detailed analyses of how PBSHM can extend the functionality of 'classical' single-structure SHM by leveraging information and data available across a population of structures. Two main concepts were critical in establishing the basic methodology. In the first place, it was assumed that the uplift from considering a population would only be truly present if a structure of interest could be shown to be 'similar' in some sense to at least one other structure in the population. This requirement of similarity demanded the development of a principled means of measuring the degree of said similarity; this was accomplished by the proposal and design of Irreducible Element (IE) and Attributed Graph (AG) models of structures, which naturally resided in a metric space. The second key concept in PBSHM was the idea of moving inferences between structures, and the natural methodology for this was established as transfer learning. This final paper is concerned with showing how the main ideas of PBSHM can be embedded in software; in fact, this proved to be non-trivial and not a simple matter of programming. In order to embed the concepts of PBSHM within a computational framework, a number of new concepts needed to be developed - the network, framework and database - and these ideas are introduced within the paper. Furthermore, the development of the computational architecture revealed a number of limitations inherent in the previously-proposed base PBSHM ideas, notably in the conception of the IE model. This paper therefore provides a radical reformulation of the IE-model concept, presenting it in a new form appropriate to embedding in software. The development of new theoretical constructs proved to be so entwined with the software development issues that they are not easily separable, so new concepts and software implementation are both presented here. To avoid interrupting the narrative flow of the conceptual material, the details of many of the software 'objects' are presented in a substantial appendix."
Evaluating an App-Based Intervention for Preventing Firearm Violence and Substance Use in Young Black Boys and Men: Usability Evaluation Study,"Emezue, C; Dan-Irabor, D; Froilan, A; Dunlap, A; Zamora, P; Negron, S; Simmons, J; Watkins, J; Julion, WA; Karnik, S",10.2196/60918,2024,"Background: Young Black male individuals are 24 timesmore likely to be impacted by firearm injuries and homicides but encounter significant barriers to care and service disengagement, even in program-rich cities across the United States, leaving them worryingly underserved. Existing community-based interventions focus on secondary and tertiary prevention after firearm violence has occurred and are typically deployed in emergency settings. To address these service and uptake issues, we developed BrotherlyACT-a nurse-led, culturally tailored, multicomponent app-to reduce the risk and effects of firearm injuries and homicides and to improve access to precrisis and mental health resources for young Black male individuals (aged 15-24 years) in low-resource and high-violence settings. Grounded in Acceptance and Commitment Therapy, the app provides life skills coaching, safety planning, artificial intelligence-powered talk therapy, and zip code-based service connections directly to young Black male individuals at risk for violence and substance use. Objective: The primary aim of this study is to evaluate the usability, engagement, and satisfaction of BrotherlyACT among target young Black male users and mobile health (mHealth) experts, using a combination of formative usability testing (UT) and heuristic evaluation (HE). Methods: Using a convergent mixed methods approach, we evaluated the BrotherlyACT app using HE by 8 mHealth specialists and conducted UT with 23 participants, comprising 15 young Black male users (aged 15-24 years), alongside4 adult internal team testers and 4 high school students who were part of our youth advisory board. UT included the System Usability Scale and thematic analysis of think-aloud interviews and cognitive walkthroughs. HE involved mHealth experts applying the Nielsen severity rating scale (score 0-3, with 3 indicating a major issue). All testing was conducted via REDCap (Research Electronic Data Capture) and Zoom or in person. Results: Qualitative usability issues were categorized into 8 thematic groups, revealing only minor usability concerns. The app achieved an average System Usability Scale score of 79, equivalent to an A-minus grade and placing it in the 85th percentile, indicating near-excellent usability. Similarly, the HE by testers identified minor and cosmetic usability issues, with a median severity score of 1 across various heuristics (on a scale of 0-3), indicating minimal impact on user experience. Overall, minor adjustments were recommended to enhance navigation, customization, and guidance for app users, while the app's visual and functional design was generally well received. Conclusions: BrotherlyACT was considered highly usable and acceptable. Testers in the UT stage gave the app a positive overall rating and emphasized that several key improvements were made. Findings from our UT prompted revisions to the app prototype. Moving forward, a pilot study with a pretest-posttest design will evaluate the app's efficacy in community health and emergency care settings."
Addressing critical barriers for sustainability of asthma stock inhaler policy implementation and resultant programming,"Adeleke, SA; Ongtengco, A; Youssef, C; Hardy, P; Pappalardo, AA",10.1016/j.anai.2024.06.023,2024,"Background: Asthma is a prevalent health concern among Illinois (IL) children, and management is significantly influenced by social determinants. There were 17 states who have adopted stock inhaler laws, but implementation varies widely. Objective: To assess critical barriers to implementation and address sustainability of stock inhaler programming in school-based asthma care in IL. Methods: Semistructured interviews were conducted with high asthma burden school districts in IL to assess barriers in implementing stock inhaler policies and resultant programming. Thematic analysis was performed using Atlas.ti (Scientific Software Development GmbH, Berlin, Germany) to identify and code threats to future sustainability. Data were synthesized and presented to stakeholders for barrier mitigation. A schematic flowchart outlining steps to support sustainability was created. Results: A total of 18 interviews were conducted with key community partners across 8 IL school districts, representing rural, urban, and suburban areas. Analysis revealed 25 barriers, with several identified as threats to future sustainability, including liability concerns, follow-up care assurance, funding/resources, pharmacy dispensing practices, district-level readiness to change, and nurse staffing. Stakeholders formed a statewide coalition to address these barriers, increase awareness, plan evaluations, and advise on state funding allocation. A national stock inhaler toolkit tailored to school administrative needs was developed to support sustainability efforts. Conclusion: Strategic stakeholder and community engagement are vital for establishing and sustaining stock inhaler programs that adhere to policy mandates. Many districts face challenges initiating and maintaining such programs without critical barrier mitigation and support. Collaborative solutions are necessary to ensure effective school-based asthma management and mitigate persistent pediatric asthma health disparities. (c) 2024 American College of Allergy, Asthma & Immunology. Published by Elsevier Inc. All rights are reserved, including those for text and data mining, AI training, and similar technologies."
Discovery of Floating-Point Differences Between NVIDIA and AMD GPUs,"Li, XY; Li, A; Fang, B; Swirydowicz, K; Laguna, I; Gopalakrishnan, G",10.1109/CCGrid59990.2024.00083,2024,"NVIDIA and AMD GPUs are fundamental components in contemporary high-performance systems, boosting computational capabilities in the HPC and AI fields. However, a clear understanding of the nuances in floatingpoint operations between these GPU variants is crucial to avoid introducing errors during software development or porting, and such clarity is currently insufficient. The complexity of this issue is amplified when considering the variety of floating-point precision options (such as FP16, FP32, etc.), floating-point formats (like standard floats, bfloats, etc.), and the different execution units (elementary units, matrix/tensor cores, etc.). As it stands, much of this information is either not well-known or is difficult to obtain. Our work aims to shed light on these areas through a pioneering testing-guided methodology that seeks to unravel many of these uncertainties. We are in the process of developing a series of tests that uncover the numerical discrepancies in elementary computing units, the built-in math libraries, and the numerical properties of matrix accelerators present in both NVIDIA (tensor cores) and AMD GPUs (matrix cores). The significance of this testing approach extends beyond current GPU models; it is designed to be forward-compatible with upcoming GPU technologies. We have already identified discrepancies as significant as 7 ulps for trigonometric functions at FP32 precision and 3 ulps at FP64 precision between NVIDIA and AMD GPUs. Additionally, our comprehensive examination has documented the behaviors of matrix cores (NVIDIA) and tensor cores (AMD), including their rounding modes (such as truncation and round-to-nearest), the extent of extra internal bits maintained (specifically, whether an additional 3 bits are retained), the handling of subnormal numbers in inputs and outputs and the FMA features in these units. This analysis spans four distinct floating-point formats and multiple GPU models, including NVIDIA's V100, A100, H100 and AMD's MI100 and MI250X. We believe that the information now being disclosed will reduce the risk of porting errors when codes are adapted across these different hardware platforms."
KeyTitle: towards better bug report title generation by keywords planning,"Meng, QS; Zou, WQ; Cai, BY; Zhang, JX",10.1007/s11219-024-09695-z,2024,"Bug reports play an important role in the software development and maintenance process. As the eye of a bug report, a concise and fluent title is always preferred and expected by developers as it could help them quickly seize the problem point and make better decisions in handling the bugs. However, in practice, not all titles filled by bug reporters are found to be of high quality; some may not carry essential bug-related information, and some may be hard to understand or contain extra noise. With the aim to reduce the burden of bug reporters and ease developers' life in handling bugs, we propose a deep learning-based technique named KeyTitle, to automatically generate a title for a given bug report. KeyTitle formulates the title generation problem as a one-sentence summarization task. It could be viewed as a Seq2Seq generation model (which generally directly generates target text based on source text) that incorporates keywords planning. Specifically, within KeyTitle, a transformer-based encoder-decoder model is enforced to generate a chain of keywords first from the detailed textual problem description, and then generate the target title by considering both these keywords and description content. Experiments over three large bug datasets collected from GitHub, Eclipse, and Apache shows that KeyTitle could outperform state-of-art title generation models relatively by up to 8.9-18.2%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\%$$\end{document}, 11.4-30.4%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\%$$\end{document}, and 13.0-18.0%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\%$$\end{document} in terms of ROUGE-1, ROUGE-2, and ROUGE-L F1-scores; the titles generated by KeyTitle are also found to be better in terms of Relevance, Accuracy, Conciseness, Fluency in human evaluation. Besides generating titles from textual descriptions, KeyTitle is also found to have great potential in generating titles based on just a few keywords, a task that also has much value in bug reporting/handling practice."
LLM-Driven Active Learning for Dependency Analysis of Mobile App Requirements Through Contextual Reasoning and Structural Relationships,"Almoqren, N; Alrashoud, M",10.3390/app15189891,2025,"In today's fast-paced release cycles, mobile app user reviews offer a valuable source for tracking the evolution of user needs. At the core of these needs lies a structure of interdependencies-some enhancements are only relevant in specific usage contexts, while others may conflict when implemented together. Identifying these relationships is essential for anticipating feature interactions, resolving contradictions, and enabling context-aware, user-driven planning. The present work introduces an ontology-enhanced AI framework for predicting whether the requirements mentioned in reviews are interdependent. The core component is a Bidirectional Encoder Representations from Transformers (BERT) classifier retrained within a large-language-model-driven active learning loop that focuses on instances with uncertainty. The framework integrates contextual and structural reasoning; contextual analysis captures the semantic intent and functional role of each requirement, enriching the understanding of user expectations. Structural reasoning relies on a domain-specific ontology that serves as both a knowledge base and an inference layer, guiding the grouping of requirements. The model achieved strong performance on annotated banking app reviews, with a validation F1-score of 0.9565 and an area under the ROC curve (AUC) exceeding 0.97. The study results contribute to supporting developers in prioritizing features based on dependencies and delivering more coherent, conflict-free releases."
Leveraging large language model and social network service for automation in scanning probe microscopy,"Diao, Z; Yamashita, H; Abe, M",10.1088/1361-6501/adbf3a,2025,"We present the development of an automated scanning probe microscopy (SPM) measurement system using an advanced large language model (LLM). This SPM system can receive instructions via social networking services (SNS), and the integration of SNS and LLMs enables real-time, language-agnostic, and portable control of SPM operations, thereby improving accessibility and efficiency. The integration of LLMs with scientific instruments brings the realization of automated solutions closer."
TransferFuzz-Pro: Large Language Model Driven Code Debugging Technology for Verifying Propagated Vulnerability,"Li, SY; Xie, KY; Li, YK; Li, H; Ren, YM; Sun, LM; Zhu, HS",10.1109/TSE.2025.3584774,2025,"Code reuse in software development frequently facilitates the spread of vulnerabilities, leading to imprecise scopes of affected software in CVE reports. Traditional methods focus primarily on detecting reused vulnerability code in target software but lack the ability to confirm whether these vulnerabilities can be triggered in new software contexts. In previous work, we introduced the TransferFuzz framework to address this gap by using historical trace-based fuzzing. However, its effectiveness is constrained by the need for manual intervention and reliance on source code instrumentation. To overcome these limitations, we propose TransferFuzz-Pro, a novel framework that integrates Large Language Model (LLM)-driven code debugging technology. By leveraging LLM for automated, human-like debugging and Proof-of-Concept (PoC) generation, combined with binary-level instrumentation, TransferFuzz-Pro extends verification capabilities to a wider range of targets. Our evaluation shows that TransferFuzz-Pro is significantly faster and can automatically validate vulnerabilities that were previously unverifiable using conventional methods. Notably, it expands the number of affected software instances for 15 CVE-listed vulnerabilities from 15 to 53 and successfully generates PoCs for various Linux distributions. These results demonstrate that TransferFuzz-Pro effectively verifies vulnerabilities introduced by code reuse in target software and automatically generation PoCs."
Leveraging LLM Enhanced Commit Messages to Improve Machine Learning Based Test Case Prioritization,"Mahmoud, Y; Azim, A; Liscano, R; Smith, K; Chang, YK; Seferi, G; Tauseef, Q",10.1145/3727582.3728681,2025,"In the rapidly evolving landscape of software development, software testing is critical for maintaining code quality and reducing defects. Effective test case prioritization employs techniques to identify defects early and ensure software quality. New avenues of research have explored using machine learning (ML) to automate the process, most current applications leverage a machine learning model using numerical features to prioritize the test cases. This study investigates the enhancement of this process by incorporating text-based features derived from git commit messages, which often include valuable information about code changes. Given that commit messages are often poorly written and inconsistent, we employ a large language model (LLM) to rewrite these messages based on code diffs, with the aim of improving the quality of their format and the information they contain. We then assess whether these refined commit messages, as an additional feature, contribute to better performance of the test case prioritization model. Our preliminary results indicate that the inclusion of LLM-enhanced commit messages leads to a noticeable improvement in prioritization effectiveness, suggesting a promising avenue for integrating natural language processing techniques in software testing workflows."
A Multi-Agent LLM Environment for Software Design and Refactoring: A Conceptual Framework,"Rajendran, V; Besiahgari, D; Patil, SC; Chandrashekaraiah, M; Challagulla, V",10.1109/SOUTHEASTCON56624.2025.10971563,2025,"Modern software systems demand continuous evolution to maintain performance, scalability, and security. Traditional single-agent AI-driven code refactoring approaches are often limited in addressing the multi-faceted constraints (e.g., performance, security, maintainability) that emerge during complex software design tasks. In this paper, we propose a novel Multi-Agent Large Language Model (LLM) Environment for automated software design and refactoring. Our conceptual framework comprises specialized LLM experts, each trained or fine-tuned on a different aspect of software engineering (performance optimization, security hardening, UI/UX, maintainability). These agents collaborate in a cooperative or competitive fashion-using coordination protocols akin to consensus or auction mechanisms-to synthesize design insights and refactoring recommendations. We present formal definitions of agent interactions (including mathematical notation for termination conditions), a sequence diagram demonstrating agent collaboration, a complexity analysis of the coordination mechanism, and an expanded reference list. Preliminary experimental design is outlined to demonstrate how multi-agent interactions may resolve conflicting design goals more effectively than a single-agent approach. Our aim is to provide a roadmap for integrating multi-agent LLMs into the software development lifecycle, thereby improving development efficiency, reducing technical debt, and enhancing software quality."
ALDExA: Automated LLM-Assisted Detection of CVE Exploitation Attempts in Host-Captured Data,"Ilg, N; Pfitzenmaier, M; Germek, D; Duplys, P; Menth, M",10.1109/ACCESS.2025.3575258,2025,"Currently, the detection of Common Vulnerabilities and Exposures (CVE) exploitation attempts heavily depends on rule sets manually written for the detection unit. As the number of published CVEs increases each year, there is a need to advance automation efforts for CVE detection. For this purpose, we introduce ALDExA, a framework that fetches CVE information and corresponding exploit codes to identify an exploit string supported by a Large Language Model (LLM). An exploit string is a characteristic element of the analyzed attack and can be monitored during an actual exploitation attempt. As the novelty in this framework lies in the extraction capabilities of the LLM, we furthermore evaluate eight different models towards their performance in identifying a correct exploit string. We evaluate contemporary models in two experiments and find that they are, in up to 81% of the evaluated cases, capable of extracting a correct exploit string from the attack code. In addition, we propose a promising approach to increase the accuracy further and to automatically detect false predictions. As ALDExA is the first approach to fully automate the CVE detection pipeline, we also discuss remaining limitations and worthwhile areas of future research."
Exploring ChatGPT's Potential in Java API Method Recommendation: An Empirical Study,"Wang, Y; Xue, WH; Huang, Q; Jiang, B; Zhang, H",10.1002/smr.2765,2025,"As software development grows increasingly complex, application programming interface (API) plays a significant role in enhancing development efficiency and code quality. However, the explosive growth in the number of APIs makes it impossible for developers to become familiar with all of them. In actual development scenarios, developers may spend a significant amount of time searching for suitable APIs, which could severely impact the development process. Recently, the OpenAI's large language model (LLM) based application-ChatGPT has shown exceptional performance across various software development tasks, responding swiftly to instructions and generating high-quality textual responses, suggesting its potential in API recommendation tasks. Thus, this paper presents an empirical study to investigate the performance of ChatGPT in query-based API recommendation tasks. Specifically, we utilized the existing benchmark APIBENCH-Q and the newly constructed dataset as evaluation datasets, selecting the state-of-the-art models BIKER and MULAREC for comparison with ChatGPT. Our research findings demonstrate that ChatGPT outperforms existing approaches in terms of success rate, mean reciprocal rank (MRR), and mean average precision (MAP). Through a manual examination of samples in which ChatGPT exceeds baseline performance and those where it provides incorrect answers, we further substantiate ChatGPT's advantages over the baselines and identify several issues contributing to its suboptimal performance. To address these issues and enhance ChatGPT's recommendation capabilities, we employed two strategies: (1) utilizing a more advanced LLM (GPT-4) and (2) exploring a new approach-MACAR, which is based on the Chain of Thought methodology. The results indicate that both strategies are effective."
"LLMs and Stack Overflow discussions: Reliability, impact, and challenges","Da Silva, L; Samhi, J; Khomh, F",10.1016/j.jss.2025.112541,2025,"Since its release in November 2022, ChatGPT has shaken up Stack Overflow, the premier platform for developers' queries on programming and software development. Demonstrating an ability to generate instant, human-like responses to technical questions, ChatGPT has ignited debates within the developer community about the evolving role of human-driven platforms in the age of generative AI. Two months after ChatGPT's release, Meta released its answer with its own Large Language Model (LLM) called LLaMA: the race was on. We conducted an empirical study analyzing questions from Stack Overflow and using these LLMs to address them. This way, we aim to Or quantify the reliability of LLMs' answers and their potential to replace Stack Overflow in the long term; OO identify and understand why LLMs fail; O0 measure users' activity evolution with Stack Overflow over time; and D compare LLMs together. Our empirical results are unequivocal: ChatGPT and LLaMA challenge human expertise, yet do not outperform it for some domains, while a significant decline in user posting activity has been observed. Furthermore, we also discuss the impact of our findings regarding the usage and development of new LLMs and provide guidelines for future challenges faced by users and researchers."
Assessing Large Language Models Effectiveness in Outdated Method Renaming,"Ben Mrad, A; Thiombiano, AMO; Mkaouer, MW; Hnich, B",10.1007/978-981-96-0805-8_18,2025,"Identifying effective methods for automatic method renaming after code modifications is crucial for maintaining developer productivity and enhancing the performance of source code analysis tools. In this study, we focus on benchmarking the effectiveness of the ChatGPT large language model (LLM) in predicting new method names after code modifications. Leveraging a dataset of method code snippets along with their original and modified names, we conducted experiments on 116 samples to assess the prediction accuracy of ChatGPT. Using Jaccard similarity as the metric, we varied the similarity threshold to evaluate the classification performance of predicted names. However, the Jaccard similarity does not retain the magnitude or direction of the vectors, reflecting the strength and polarity of the similarity. In addition, it ignores the order and context of the words, which results in missing potential syntactic or semantic variations. To solve this problem, we propose another validation process which not only detects whether or not an LLM captured semantic changes of a method, but also its structural changes in order to be able to generate a suitable name for this given method. Our results indicate that ChatGPT achieves a high success rate in predicting method names, obtaining 98% (Resp. 94%) when the threshold is set to 0.5 for the Cosine (resp. Jaccard) similarity. For a threshold of 1 (maximum similarity), ChatGPT maintains a notable performance with 49% (Resp. 74%) Cosine (resp. Jaccard) similarity. This demonstrates the potential of ChatGPT for automating method renaming tasks in software development workflows."
Automated Codebase Reconciliation using Large Language Models,"Gandhi, A; De, S; Chechik, M; Pandit, V; Kiehn, M; Chee, MC; Bedasso, Y",10.1109/Forge66646.2025.00011,2025,"Large-scale software projects frequently encounter the challenge of manually propagating code changes across branches-a process that is error-prone due to code divergence, conflicting dependencies, and branch-specific modifications. Automating code porting can streamline development workflows, accelerate development cycles, and improve team collaboration. However, achieving this automation presents significant hurdles, particularly in maintaining consistency and resolving conflicts during codebase integration. We propose a novel approach that integrates algorithmic analysis with artificial intelligence-driven code generation, leveraging multi-agent systems to automate the identification of porting requirements and the development of 'context-aware' modifications. Our comprehensive, end-to-end framework starts by extracting recent commits to evaluate divergence. It subsequently assesses the necessity for porting changes and employs large language model (LLM) based systems to generate adaptive code suggestions tailored to files exhibiting inconsistencies. Experimental results suggest a substantial decrease in manual work through pipeline-generated pull requests. Despite these promising outcomes, integrating LLMs into complex workflows presents challenges, such as handling intricate dependencies and ensuring alignment with a company's software development issue tracking and change management systems. This paper explores the potential and limitations of LLMs in advancing automation within software engineering and suggests future directions for enhancing these models to achieve industry-grade reliability."
Protecting the Whisper: A Security Assessment of Amazon CodeWhisperer's Generated Code,"Araujo, GF; Hayajneh, T",10.1007/978-3-031-85856-7_36,2025,"Code completion and generation tools are crucial in improving software development's efficiency and overall quality by assisting programmers. On this basis, Amazon CodeWhisperer is a prominent Large Language Model (LLM) and code assistant tool trained on Amazon and open-source code. Amazon CodeWhisperer also stands out as the only AI coding companion with security scanning capabilities to identify and recommend solutions for vulnerabilities in code. However, given the opensource data CodeWhisperer was trained on and the probabilistic nature of LLMs, it is inevitable that part of the generated code will contain security vulnerabilities. In this work, we first evaluate CodeWhisperer's generated code in regard to the Common Weakness Enumeration (CWE) list, where we found approximately 85% of the security-relevant scenarios to be vulnerable. Next, we tested the security scan tool, which was able to identify 62% of the demonstrably insecure code."
Vulnerability-Triggering Test Case Generation from Third-Party Libraries,"Gao, Y; Hu, X; Chen, ZR; Xu, TT; Yang, XH",10.1109/Forge66646.2025.00021,2025,"Open-source third-party libraries are widely used in software development. These libraries offer substantial advantages in terms of time and resource savings. However, a significant concern arises due to the publicly disclosed vulnerabilities within these libraries. Existing automated vulnerability detection tools often suffer from false positives and fail to accurately assess the propagation of inputs capable of triggering vulnerabilities from client projects to vulnerable code in libraries. In this paper, we propose a novel approach called VULEUT (Vulnerability Exploit Unit Test Generation), which combines vulnerability exploitation reachability analysis and LLM-based unit test generation. VULEUT is designed to automatically verify the exploitability of vulnerabilities in third-party libraries commonly used in Java client software projects. VULEUT first analyzes the client projects to determine the reachability of vulnerability conditions. And then, it leverages the Large Language Model (LLM) to generate unit tests for vulnerability confirmation. To evaluate the effectiveness of VULEUT, we collect 32 vulnerabilities from various third-party libraries and conduct experiments on 70 real client projects. Besides, we also compare our approach with two representative tools, i.e., TRANSFER and VESTA. Our results demonstrate the effectiveness of VULEUT, with 229 out of 292 generated unit tests successfully confirming vulnerability exploitation across 70 client projects, which outperforms baselines by 24%."
SpecRover: Code Intent Extraction via LLMs,"Ruan, HF; Zhang, YT; Roychoudhury, A",10.1109/ICSE55347.2025.00080,2025,"automatically producing bug fixes and feature additions. Such program improvement can be accomplished by a combination of large language model (LLM) and program analysis capabilities, in the form of an LLM agent. Since program repair or program improvement typically requires a specification of intended behavior - specification inference can be useful for producing high quality program patches. In this work, we examine efficient and low-cost workflows for iterative specification inference within an LLM agent. Given a GitHub issue to be resolved in a software project, our goal is to conduct iterative code search accompanied by specification inference - thereby inferring intent from both the project structure and behavior. The intent thus captured is examined by a reviewer agent with the goal of vetting the patches as well as providing a measure of confidence in the vetted patches. Our approach SpecRover is built on the open-source LLM agent AutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub issues, it shows more than 50% improvement in efficacy over AutoCodeRover. Compared to the open-source agents available, our work shows modest cost ($0.65 per issue) in resolving an average GitHub issue in SWE-Bench lite. The production of explanation by SpecRover allows for a better signal to be given to the developer, on when the suggested patches can be accepted with confidence. SpecRover also seeks to demonstrate the continued importance of specification inference in automated program repair, even as program repair technologies enter the LLM era."
Uncertainty-Aware Contrastive Learning With Hard Negative Sampling for Code Search Tasks,"Liu, H; Zhan, JQ; Zhang, Q",,2025,"Code search is a highly required technique for software development. In recent years, the rapid development of transformer-based language models has made it increasingly more popular to adapt a pre-trained language model to a code search task, where contrastive learning is typically adopted to semantically align user queries and codes in an embedding space. Considering that the same semantic meaning can be presented using diverse language styles in user queries and codes, the representation of queries and codes in an embedding space may thus be non-deterministic. To address the above-specified point, this paper proposes an uncertainty-aware contrastive learning approach for code search. Specifically, for both queries and codes, we design an uncertainty learning strategy to produce diverse embeddings by learning to transform the original inputs into Gaussian distributions and then taking a reparameterization trick. We also design a hard negative sampling strategy to construct query-code pairs for improving the effectiveness of uncertainty-aware contrastive learning. The experimental results indicate that our approach outperforms 10 baseline methods on a large code search dataset with six programming languages. The results also show that our strategies of uncertainty learning and hard negative sampling can really help enhance the representation of queries and codes leading to an improvement of the code search performance."
SpeedGen: Enhancing Code Efficiency through Large Language Model-Based Performance Optimization,"Purschke, N; Kirchner, S; Knoll, A",10.1109/SANER64311.2025.00045,2025,"We present SpeedGen, a novel framework that uses Large Language Models (LLMs) to automate code performance optimization. SpeedGen is designed to address software performance bottlenecks using a feedback-driven approach that profiles code to identify inefficiencies and iteratively refines the code to improve execution speed. We conducted a comprehensive evaluation of SpeedGen's capabilities across diverse codebases, benchmarking its performance against a leading large language model. Our results show that SpeedGen consistently reduces execution time and delivers significant performance improvements in various scenarios. The framework's ability to adapt to different domains underscores its scalability and robustness, making it a valuable tool for optimizing code in a wide range of applications. A key strength of SpeedGen is its ability to maintain the functional correctness of the code while achieving significant performance gains. This feature ensures that the optimized code remains reliable even when it undergoes significant transformations. By automating the optimization process, SpeedGen minimizes the need for manual intervention, streamlining the software development lifecycle and reducing time-consuming performance tuning efforts. The introduction of SpeedGen marks a major step forward in the integration of LLMs into software engineering and paves the way for future research and development in this area. With its ability to improve performance without compromising code integrity it lays the foundation for more advanced automated code optimization techniques, simplifying software development."
RAPRSUM: Retrieval-Augmented Code Summarization with Advanced Prototype Refinement,"Lin, YB; Wang, XQ; Zhan, SG; Wei, D; Chen, B",10.1142/S0218194025500317,2025,"Code summarization aims to automatically generate natural language descriptions for code snippets, thereby enhancing programmers' comprehension of the code and significantly improving code maintainability and development efficiency. Code reuse is a prevalent practice in software development, and information retrieval approaches, which leverage similar summaries from a corpus, have shown promising results. However, these approaches often heavily rely on the quality of the corpus, and discrepancies between retrieved and target summaries may result in inaccuracies or incomplete descriptions. In this paper, we propose RAPRSUM, a retrieval-augmented code summarization framework that integrates information retrieval and deep learning techniques. Specifically, RAPRSUM refines retrieved summary prototypes using a deep learning model to preserve essential information while eliminating redundancy. A pretrained large language model is then employed to generate the final summary by jointly considering the input code and the refined prototype. We evaluated RAPRSUM on three public datasets, demonstrating its significant improvement over existing baseline methods. Additional ablation studies confirm the effectiveness of the proposed modules. Finally, through case analysis and robustness verification, we highlight RAPRSUM's superior performance in generating high-quality code summaries."
SEGym: Optimizing Large Language Model Assisted Software Engineering Agents with Reinforcement Learning,"Stenzel, G; Schmid, K; KÃ¶lle, M; Altmann, P; Lingsch-Rosenfeld, M; Zorn, M; BÃ¼cher, T; Gabor, T; Wirsing, M; Belzner, L",10.1007/978-3-031-75434-0_8,2025,"Current software development agents based on large language models (LLMs) are often defined using heuristic methods, which can limit their flexibility and effectiveness. Moreover, the entry barriers for new researchers in this field are high, largely due to the complex infrastructure required to develop and optimize these agents. This paper proposes a new approach: modeling software development agents over LLMs as a partially observable Markov decision process (POMDP) to enable data-driven optimization. To support this approach, we introduce SEGym, a framework based on the Gym interface for reinforcement learning agents. SEGym simplifies the setup of optimization experiments for software development agents within the POMDP framework, making it more accessible for researchers to engage in this field."
Analysis of ChatGPT-Generated Codes Across Multiple Programming Languages,"Almanasra, S; Suwais, K",10.1109/ACCESS.2025.3538050,2025,"Our research focuses on the intersection of artificial intelligence (AI) and software development, particularly the role of AI models in automating code generation. With advancements in large language models like ChatGPT, developers can now generate code from natural language prompts, a task that traditionally required significant manual input and expertise. AI-generated code promises to boost productivity by enabling faster prototyping and automating repetitive coding tasks. However, as these models are increasingly adopted in real-world applications, questions surrounding their efficiency and code quality become critical. This research investigates ChatGPT-4o, a state-of-the-art language model, and its ability to generate functional, high-quality code in different programming languages. By comparing performance between Python and Java, the study seeks to shed light on AI's capabilities and limitations in code generation, addressing not only functional correctness but also broader software engineering concerns such as memory usage, runtime efficiency, and maintainability. The study addresses key questions related to the performance, code quality, and error management of AI-generated code by analyzing solutions for 300 data structure problems and 300 problems from the LeetCode platform. The findings reveal notable performance differences between the two languages: Java demonstrated superior runtime performance, particularly for medium and hard problems, while Python exhibited better memory efficiency across all complexity levels. The research also highlighted significant gaps in code quality, with both languages showing deficiencies in documentation and exception management. This study contributes to the literature by offering a comprehensive cross-language analysis of ChatGPT-4o's programming capabilities, addressing a gap in the evaluation of AI-generated code performance."
Leveraging artificial intelligence for computational urban analysis: building StreetRose with ChatGPT,"Ãobanli, FT; Kahraman, M",10.1007/s41324-025-00614-3,2025,"The emergence and rapid development of large language models (LLMs) has changed the way we interact with information and solve complex problems. The aim of this study is to explore how an urban analysis tool can be created and used effectively with the aid of a large language model. By focusing on this objective, we highlight both the potential and the limitations of ChatGPT. Using GPT-3.5 Turbo, one of the popular AI-powered Large Language Models, we built StreetRose 0.0.1, a Python-based application. OpenStreetMap was used as a data source. The development process followed the steps of data collection, software development, analysis and visualisation. The resulting tool, StreetRose, visualises street trends in settlements and provides insights into street network analysis. This study provides an example of the use of ChatGPT in urban studies and discusses its advantages and disadvantages. In addition, the tool provides practical information and visualisations that can help researchers interested in settlements and serve as a valuable asset for urban planning and street network analysis. The results show that there are areas for improvement and shortcomings that need to be addressed, although ChatGPT significantly speeds up the coding process. The study also highlights the transformative impact of LLMs on urban analysis and sets an example for future applications in this field."
Enhancing large language models for text-to-testcase generation,"Alagarsamy, S; Tantithamthavorn, C; Takerngsaksiri, W; Arora, C; Aleti, A",10.1016/j.jss.2025.112531,2025,"Context: Test-driven development is a widely employed software development practice that involves developing test cases based on requirements prior to writing the code. Traditional TDD is time-consuming, since writing test cases based on a given requirement is a complex task. Although various methods for automated test case generation have been proposed, they are not specifically tailored for TDD, where requirements instead of code serve as input. Objective: In this paper, we introduce a text-to-testcase generation approach based on a large language model (GPT-3.5) that is fine-tuned on our curated dataset with an effective prompt design. We aim to investigate the performance of LLMs-based text-to-testcase generation task based on two contexts, i.e., when LLMs are leveraged by fine-tuning and prompting. Method: Our approach involves enhancing the capabilities of basic GPT-3.5 for the text-to-testcase generation task that is fine-tuned on our curated dataset with an effective prompt design. We evaluated the effectiveness of our approach using a span of five large-scale open-source software projects. In total, we investigated 32 variations of eight LLMs (i.e., basic GPT-3.5-turbo, Incoder, Starcoder, CodeT5, Bloom, CodeGemma, CodeLlama, and Gemini). Results: Our approach generated 7k test cases for open source projects, achieving 78.5% syntactic correctness, 67.09% requirement alignment, 61.7% code coverage, and 18.9% mutation score which substantially outperforms all other LLMs. Furthermore, our ablation study demonstrates the substantial performance improvement of the fine-tuning and prompting components of the GPT-3.5 model. Conclusions: These findings lead us to conclude that fine-tuning and prompting should be considered in the future when building a language model for the text-to-testcase generation task."
Language Models in Software Development Tasks: An Experimental Analysis of Energy and Accuracy,"Alizadeh, N; Belchev, B; Saurabh, N; Kelbert, P; Castor, F",10.1109/MSR66628.2025.00109,2025,"The use of generative AI-based coding assistants like ChatGPT and Github Copilot is a reality in contemporary software development. Many of these tools are provided as remote APIs. Using third-party APIs raises data privacy and security concerns for client companies, which motivates the use of locally-deployed language models. In this study, we explore the tradeoff between model accuracy and energy consumption, aiming to provide valuable insights to help developers make informed decisions when selecting a language model. We investigate the performance of 18 families of LLMs in typical software development tasks on two real-world infrastructures, a commodity GPU and a powerful AI-specific GPU. Given that deploying LLMs locally requires powerful infrastructure which might not be affordable for everyone, we consider both full-precision and quantized models. Our findings reveal that employing a big LLM with a higher energy budget does not always translate to significantly improved accuracy. Additionally, quantized versions of large models generally offer better efficiency and accuracy compared to full-precision versions of medium-sized ones. Apart from that, not a single model is suitable for all types of software development tasks."
Adaptive Test Healing using LLM/GPT and Reinforcement Learning,"Mani, N; Attaranasl, S",10.1109/ICSTW64639.2025.10962516,2025,"Flaky tests disrupt software development pipelines by producing inconsistent results, undermining reliability and efficiency. This paper introduces a hybrid framework for adaptive test healing, combining Large Language Models (LLMs) like GPT with Reinforcement Learning (RL) to address test flakiness dynamically. LLMs analyze test logs to classify failures and extract contextual insights, while the RL agent learns optimal strategies for test retries, parameter tuning, and environment resets. Experimental results demonstrate the framework's effectiveness in reducing flakiness and improving CI/CD pipeline stability, outperforming traditional approaches. This work paves the way for scalable, intelligent test automation in dynamic development environments."
Automated Test Case Repair Using Language Models,"Yaraghi, AS; Holden, D; Kahani, N; Briand, L",10.1109/TSE.2025.3541166,2025,"Ensuring the quality of software systems through testing is essential, yet maintaining test cases poses significant challenges and costs. The need for frequent updates to align with the evolving system under test often entails high complexity and cost for maintaining these test cases. Further, unrepaired broken test cases can degrade test suite quality and disrupt the software development process, wasting developers' time. To address this challenge, we present TaRGET (Test Repair GEneraTor), a novel approach leveraging pre-trained code language models for automated test case repair. TaRGET treats test repair as a language translation task, employing a two-step process to fine-tune a language model based on essential context data characterizing the test breakage. To evaluate our approach, we introduce TaRBench, a comprehensive benchmark we developed covering 45,373 broken test repairs across 59 open-source projects. Our results demonstrate TaRGET's effectiveness, achieving a 66.1% exact match accuracy. Furthermore, our study examines the effectiveness of TaRGET across different test repair scenarios. We provide a practical guide to predict situations where the generated test repairs might be less reliable. We also explore whether project-specific data is always necessary for fine-tuning and if our approach can be effective on new projects."
A code completion approach combining pointer network and Transformer-XL network,"Zhang, XP; Liu, JX; Long, T; Hu, HZ",10.1007/s10489-025-06315-6,2025,"Code completion is a crucial aspect of contemporary integrated development environments (IDEs), as it not only streamlines the software development process but also bolsters the quality of software products. By leveraging large-scale codes to learn the probability distribution among code token units, deep learning methods have demonstrated significant improvements in the accuracy of token unit recommendations. However, the efficacy of code completion employing deep learning is often compromised by information loss. To mitigate this issue, we introduce a novel code language model that incorporates both the pointer network and the Transformer-XL architecture to surpass the constraints of current approaches in code completion. Our proposed model accepts as input the original code snippet and its corresponding abstract syntax tree (AST), utilizing the Transformer-XL model as the foundational architecture for capturing long-term dependencies. Additionally, we incorporate a pointer network as an adjunct component to forecast Out-of-Vocabulary (OoV) words. Our approach has been rigorously evaluated on the authentic PY150 and JS150 datasets. The comparative experimental results demonstrate the effectiveness of our model in improving the accuracy of the code completion task at the token unit level."
Unified modeling language code generation from diagram images using multimodal large language models,"Bates, A; Vavricka, R; Carleton, S; Shao, R; Pan, C",10.1016/j.mlwa.2025.100660,2025,"The Unified Modeling Language is a standardized visual language widely used for modeling and documenting the design of software systems. Although many tools are available that generate UML diagrams from UML code, generating executable UML code from image-based UML diagrams remains challenging. This paper proposes a new approach to generate UML code using a large multimodal language model automatically. Synthetic UML activity and sequence diagram datasets were created to train and test the model. We compared the standard fine-tuning with LoRA techniques to optimize base models. The experiments measured the code generation accuracy across different model sizes and training strategies. These results demonstrated that domain-adapted MM-LLMs perform for UML code generation automation, whereby, at the best model, it achieved BLEU and SSIM of 0.779 and 0.942 on sequence diagrams. This will enable the modernization of legacy systems and decrease the manual effort put into software development workflows."
Combining Large Language Models with Static Analyzers for Code Review Generation,"Jaoua, I; Ben Sghaier, O; Sahraoui, H",10.1109/MSR66628.2025.00038,2025,"Code review is a crucial but often complex, subjective, and time-consuming activity in software development. Over the past decades, significant efforts have been made to automate this process. Early approaches focused on knowledge-based systems (KBS) that apply rule-based mechanisms to detect code issues, providing precise feedback but struggling with complex, context-dependent cases. More recent work has shifted toward fine-tuning pre-trained language models for code review, enabling broader issue coverage but often at the expense of precision. In this paper, we propose a hybrid approach that combines the strengths of KBS and learning-based systems (LBS) to generate high-quality, comprehensive code reviews. Our method integrates knowledge at three distinct stages of the language model pipeline: during data preparation (Data-Augmented Training, DAT), at inference (Retrieval-Augmented Generation, RAG), and after inference (Naive Concatenation of Outputs, NCO). We empirically evaluate our combination strategies against standalone KBS and LBS fine-tuned on a real-world dataset. Our results show that these hybrid strategies enhance the relevance, completeness, and overall quality of review comments, effectively bridging the gap between rule-based tools and deep learning models."
CoEdPilot: Interactively Recommending Project-Wise Code Edits,"Huang, YH; Liu, CY; Lin, Y; Cai, YF; Jiang, B; Yang, P; Huang, ZY; Dong, JS",10.1007/s11390-025-5139-z,2025,"Incremental code editing, as a fundamental task in software development, requires developers to iteratively identify edit locations and modify code. However, existing language model-driven approaches primarily focus on generating edit solutions for a single location, failing to provide comprehensive end-to-end solutions. To address this limitation and support real-world editing scenarios, we propose CoEdPilot, a project-wide interactive code editing recommendation tool. CoEdPilot utilizes edit descriptions and edit history, and recommends the next edit location with solutions across the entire project. It further refines its recommendations based on user editing feedback, enabling an end-to-end, iterative, and interactive editing process. We implement CoEdPilot as a visual studio code extension that monitors user actions, identifies subsequent editing locations, and generates edits throughout the project. Its functionality is powered by a set of backend language models, which are trained on 180k high-quality commits from 471 open-source repositories. Extensive experiments demonstrate CoEdPilot's capabilities in accurately identifying editing locations (i.e., edit location predicted with an accuracy of 85.03%-88.99%) and generating high-quality edit solutions (i.e., generated edit content with a top-1 exact match rate (EMR) of 33.48%-48.94%). Our case study and user study of 18 participants further validate CoEdPilot's practicability."
Digital detector PET/CT increases Centiloid measures of amyloid in Alzheimer's disease: A head-to-head comparison of cameras,"Gillman, A; Bourgeat, P; Cox, T; Villemagne, VL; Fripp, J; Huang, K; Williams, R; Shishegar, R; O'Keefe, G; Li, SP; Krishnadas, N; Feizpour, A; Bozinovski, S; Rowe, CC; DorÃ©, V",10.1177/13872877241313063,2025,"Background The introduction of therapeutics for Alzheimer's disease has led to increased interest in precisely quantifying amyloid-beta (A beta) burden for diagnosis, treatment monitoring, and further clinical research. Recent positron emission tomography (PET) hardware innovations including digital detectors have led to superior resolution and sensitivity, improving quantitative accuracy. However, the effect of PET scanner on Centiloid remains relatively unexplored and is assumed to be minimized by harmonizing PET resolutions. Objective To quantify the differences in Centiloid between scanners in a paired cohort. Methods 36 participants from the Australian Imaging, Biomarker and Lifestyle study (AIBL) cohort were scanned within a year on two scanners. Each participant underwent F-18-NAV4694 imaging on two of the three scanners investigated, the Siemens Vision, the Siemens mCT and the Philips Gemini. We compared A beta Centiloid quantification between scanners and assessed the effectiveness of post-reconstruction PET resolution harmonization. We further compared the scanner differences in target sub-regions and with different reference regions to assess spatial variability. Results Centiloid from the Vision camera was found to be significantly higher compared to the Gemini and mCT; the difference was greater at high-Centiloid levels. Post-reconstruction resolution harmonization only accounted for and corrected similar to 20% of the Centiloid (CL) difference between scanners. We further demonstrated that residual differences have effects that vary spatially between different subregions of the Centiloid mask. Conclusions We have demonstrated that the type of PET scanner that a participant is scanned on affects Centiloid quantification, even when scanner resolution is harmonized. We conclude by highlighting the need for further investigation into harmonization techniques that consider scanner differences."
Enhancing Agile Requirements Change Management: Integrating LLMs with Fuzzy Best-Worst Method for Decision Support,"Aljohani, B; Aljuhani, A; Alsanoosy, T",,2025,"Agile Requirements Change Management (ARCM) in Global Software Development (GSD) posed significant challenges due to the dynamic nature of project requirements and the complexities of distributed team coordination. One approach used to mitigate these challenges and ensure efficient collaboration the identification and prioritization of success factors. Traditional Multi-Criteria Decision-Making methods, such as the Best-Worst Method (BWM), had been employed successfully to prioritize success factors. However, these methods often failed to capture the inherent uncertainties of decision-making in a GSD. To address this limitation, this study integrated Large Language Models (LLMs) with the Fuzzy Best-Worst Method (FBWM) to enhance prioritization accuracy and decision support. We propose a model for comparing the prioritization outcomes of human expert assessments and LLM-generated decisions evaluate the consistency and effectiveness of machine-generated decisions relative to those made by human experts. The findings indicate that the LLM-driven FBWM exhibit high reliability in mirroring expert judgments, demonstrating the potential LLMs to support strategic decision-making in ARCM. This study contributed to the evolving landscape of AI-driven project management by providing empirical evidence of LLMs' utility improving ARCM for GSD."
Prompting in the Wild: An Empirical Study of Prompt Evolution in Software Repositories,"Tafreshipour, M; Imani, A; Huang, E; de Almeida, ES; Zimmermann, T; Ahmed, I",10.1109/MSR66628.2025.00106,2025,"The adoption of Large Language Models (LLMs) is reshaping software development as developers integrate these LLMs into their applications. In such applications, prompts serve as the primary means of interacting with LLMs. Despite the widespread use of LLM-integrated applications, there is limited understanding of how developers manage and evolve prompts. This study presents the first empirical analysis of prompt evolution in LLM-integrated software development. We analyzed 1,262 prompt changes across 243 GitHub repositories to investigate the patterns and frequencies of prompt changes, their relationship with code changes, documentation practices, and their impact on system behavior. Our findings show that developers primarily evolve prompts through additions and modifications, with most changes occurring during feature development. We identified key challenges in prompt engineering: only 21.9% of prompt changes are documented in commit messages, changes can introduce logical inconsistencies, and misalignment often occurs between prompt changes and LLM responses. These insights emphasize the need for specialized testing frameworks, automated validation tools, and improved documentation practices to enhance the reliability of LLM-integrated applications."
An Empirical Study on Challenges for LLM Application Developers,"Chen, X; Gao, CY; Chen, CY; Zhang, GB; Liu, Y",10.1145/3715007,2025,"In recent years, large language models (LLMs) have seen rapid advancements, significantly impacting various fields such as computer vision, natural language processing, and software engineering. These LLMs, exemplified by OpenAI's ChatGPT, have revolutionized the way we approach language understanding and generation tasks. However, in contrast to traditional software development practices, LLM development introduces new challenges for AI developers in design, implementation, and deployment. These challenges span different areas (such as prompts, APIs, and plugins), requiring developers to navigate unique methodologies and considerations specific to LLM application development. Despite the profound influence of LLMs, to the best of our knowledge, these challenges have not been thoroughly investigated in previous empirical studies. To fill this gap, we present the first comprehensive study on understanding the challenges faced by LLM developers. Specifically, we crawl and analyze 29,057 relevant questions from a popular OpenAI developer forum. We first examine their popularity and difficulty. After manually analyzing 2,364 sampled questions, we construct a taxonomy of challenges faced by LLM developers. Based on this taxonomy, we summarize a set of findings and actionable implications for LLM-related stakeholders, including developers and providers (especially the OpenAI organization)."
GREEN-CODE: Learning to Optimize Energy Efficiency in LLM-based Code Generation,"Ilager, S; Briem, LF; Brandic, I",10.1109/CCGRID64434.2025.00068,2025,"Large Language Models (LLMs) are becoming integral to daily life, showcasing their vast potential across various Natural Language Processing (NLP) tasks. Beyond NLP, LLMs are increasingly used in software development tasks, such as code completion, modification, bug fixing, and code translation. Software engineers widely use tools like GitHub Copilot and Amazon Q, streamlining workflows and automating tasks with high accuracy. While the resource and energy intensity of LLM training is often highlighted, inference can be even more resource-intensive over time, as it's a continuous process with a high number of invocations. Therefore, developing resource-efficient alternatives for LLM inference is crucial for sustainability. This work proposes GREEN-CODE, a framework for energy-aware code generation in LLMs. GREEN-CODE performs dynamic early exit during LLM inference. We train a Reinforcement Learning (RL) agent that learns to balance the trade-offs between accuracy, latency, and energy consumption. Our approach is evaluated on two open-source LLMs, Llama 3.2 3B and OPT 2.7B, using the JavaCorpus and PY150 datasets. Results show that our method reduces the energy consumption between 23-50% on average for code generation tasks without significantly affecting accuracy."
PERFCODEGEN: Improving Performance of LLM Generated Code with Execution Feedback,"Peng, Y; Gotmare, AD; Lyu, MR; Xiong, CM; Savarese, S; Sahoo, D",10.1109/Forge66646.2025.00008,2025,"Large Language Models (LLMs) are widely adopted for assisting in software development tasks, yet their performance evaluations have narrowly focused on the functional correctness of generated code. Human programmers, however, expect AI assistants to generate not only correct but also optimally efficient code. We propose PERFCODEGEN, a training-free framework that enhances the performance of LLM-generated code by incorporating feedback based on runtime during test case execution into the self-refinement iterations. With PERFCODEGEN, we achieve speedups for a significantly higher proportion of problems compared to using the base LLM with sophisticated prompting techniques. Applied to open-weight language models like Phi-3-mini, PERFCODEGEN achieves code optimization rates comparable to naive prompting of powerful closed models like GPT-4. We achieve state-of-the-art code optimization on benchmarks such as HumanEval, MBPP, and APPS, frequently surpassing the ground truth reference solutions with PERFCODEGEN using GPT-3.5 and GPT-4. Additionally, we demonstrate the effectiveness of our approach in enhancing code quality across a range of open-weight LLMs of varying sizes including Phi-3-mini (3.8B), Llama 3 8B, Mixtral 8x7B (13B active), Command R (35B), and Llama 3 70B. PERFCODEGEN's effectiveness at generating performant code underscores the importance of integrating execution feedback into the code generation process, highlighting a path forward for more robust and reliable AI-driven software development."
Enhancing Developer Productivity: Benchmarking LLM-Powered Tools like GitHub Copilot and TabNine in Real-Time Coding Environments,"Slama, F; Lemire, D",10.1109/IDS66066.2025.00011,2025,"The adoption of Large Language Models (LLMs) is disrupting software development - including code generation, debugging, and project management. We classify LLM applications into six domains specializing in general usage, templated interactive development environments (IDEs) programming, complex reasoning, cloud programming, large-volume code management, and bilingual project programming. It assesses models including GPT-4, Codex, GitHub Copilot, TabNine, Claude 3 Opus, and Code Llama, with a focus on their impact on productivity, debugging correctness, and workflow efficiency. We offer concrete recommendations for developers to best leverage these tools. And even though the paper includes all kind of LLMs, I would say that the practical experimentation is target on IDE integrations (having in mind provisioning) and with the focus (actually based on another two LLMs for real-time completion) GitHub Copilot and TabNine, both extensions for Visual Studio Code. This benchmarking of their performance and usability in live coding environments provides critical insights into the capabilities and limitations of LLMs, enabling developers to strategically optimize their workflows by selecting and integrating the most intelligent tools available."
A Systematic Mapping Study of LLM Applications in Mobile Device Research,"Chen, C; Wang, B; Lin, YF",10.1007/978-981-96-0055-7_14,2025,"The extensive utilization of Large Language Models (LLMs) has significantly influenced academic research in mobile device-related fields, encompassing application testing, malware detection, voice control, and software development enhancement. Concurrently, the increasing demand for user access to LLMs on mobile devices for tasks like question answering has introduced new research directions, such as developing native LLMs by reducing parameter sizes. We aim to study the relationship between the evolution of LLMs and mobile device-related research, exploring their integration into traditional tasks and their adaptation to mobile platforms, from early transformer-based models to modern architectures like GPT-4. We have reviewed 55 recent papers, including 50 novel approaches, 1 benchmark, and 4 empirical studies, covering various aspects of LLM applications in mobile devices."
Comparative Analysis of Carbon Footprint in Manual vs. LLM-Assisted Code Development,"Cheung, KS; Kaul, M; Jahangirova, G; Mousavi, MR; Zie, E",10.1145/3711919.3728678,2025,"Large Language Models (LLM) have significantly transformed various domains, including software development. These models assist programmers in generating code, potentially increasing productivity and efficiency. However, the environmental impact of utilising these AI models is substantial, given their high energy consumption during both training and inference stages. This research aims to compare the energy consumption of manual software development versus an LLM-assisted approach, using Codeforces as a simulation platform for software development. The goal is to quantify the environmental impact and propose strategies for minimising the carbon footprint of using LLM in software development. Our results show that the LLM-assisted code generation leads on average to 32.72 higher carbon footprint than the manual one. Moreover, there is a significant correlation between task complexity and the difference in the carbon footprint of the two approaches."
"AI-Powered, But Power-Hungry? Energy Efficiency of LLM-Generated Code","Solovyeva, L; Weidmann, S; Castor, F",10.1109/Forge66646.2025.00012,2025,"Large language models (LLMs) are used in software development to assist in various tasks, e.g., code generation and code completion, but empirical evaluations of the quality of the results produced by these models focus on correctness and ignore other relevant aspects, such as their performance and energy efficiency. Studying the performance of LLM-produced programs is essential to understand how well LLMs can support the construction of performance- and energy-critical software, such as operating systems, servers, and mobile applications. This paper presents the first study analyzing the energy efficiency and performance of LLM-generated code for three programming languages Python, Java, and C++, on two platforms, a Mac and a PC, leveraging three frontier LLMs, Github Copilot, GPT-4o, and the recently-released OpenAI o1-mini, and targeting hard programming problems from LeetCode. Our results show that the models are much more successful in generating Python and Java than C++ code. Also, LLM-generated code sometimes surpasses an efficient human-written solution, although that is language-dependent and the language with the best results, Python, is the one where application performance and energy consumption tend to matter the least in practice. Furthermore, the performance of generated code is highly correlated across the two platforms, hinting at potential for results to be portable across platforms."
LLM-aided Test Generation for Custom Neural Network Hardware Accelerators,"Peccia, FN; Hald, T; Bringmann, O",10.1109/ETS63895.2025.11049634,2025,"Large Language Models (LLMs) are increasingly being utilized to automate tasks in software development, including test generation. This paper introduces a novel workflow for the automated generation of tests for custom hardware accelerators for Neural Networks (NN) using LLMs. The workflow is designed to handle the unique challenges posed by custom or proprietary accelerators, where the LLM has no prior knowledge about the hardware, and there are not enough code examples to fine-tune the LLM for it. A five-part prompt is proposed to encompass all the necessary information for the LLM to generate accurate tests. The Gemmini accelerator is selected as a use case, and tests are defined and generated using six different LLMs, each one evaluated at 4, 16 and 32-bit floating point precision. For each LLM and precision combination, latency and energy consumption are recorded for each generated test. We evaluate the correctness of the generated tests and the impact of adding the human-in-the-loop to apply minimal corrections to them if necessary. We define an efficiency metric to evaluate the tradeoff between consumed energy and correctly generated tests. The workflow evaluation reveals that LLMs are able to generate tests for custom hardware accelerators, with Codestral and Gemma 2 being the most effective LLMs for these tasks, capable of generating all the proposed tests with minimal human corrections. This workflow offers developers a new and efficient method to accelerate the test generation of custom hardware accelerators for NN, with potential applications in verifying new accelerators or new features of existing ones."
Iterative Proof-Driven Development LLM Prompt,"Bakharia, A",10.1145/3701716.3717811,2025,"This paper introduces Iterative Proof-Driven Development, a novel prompt engineering method designed to take advantage of GPT-4 level models that are able to follow detailed instructions. Iterative Proof-Driven Development is a structured prompt that outlines a test-driven process for solving complex mathematics and programming tasks. The prompt provides an LLM with instructions to decompose a problem into sub-problems, generate test cases, verify results, and integrate sub-solutions. While Chain of Thought (CoT) reasoning remains central to individual steps, the structured process ensures consistency and reliability. This method takes advantage of embedded code interpreters within chatbot user interfaces, enabling immediate execution, debugging, and iterative refinement without external tools. We demonstrate this prompting technique by guiding an LLM through the development of a small CSV processing library, illustrating each step and highlighting how the method ensures well-tested, logically consistent, and reliable outputs. Finally, we explore potential use cases, future directions for automation with agent-based systems (e.g., Langgraph), and the broader implications of prompt engineering for larger software development projects."
Towards Detecting Prompt Knowledge Gaps for Improved LLM-guided Issue Resolution,"Ehsani, R; Pathak, S; Chatterjee, P",10.1109/MSR66628.2025.00107,2025,"Large language models (LLMs) have become essential in software development, especially for issue resolution. However, despite their widespread use, significant challenges persist in the quality of LLM responses to issue resolution queries. LLM interactions often yield incorrect, incomplete, or ambiguous information, largely due to knowledge gaps in prompt design, which can lead to unproductive exchanges and reduced developer productivity. In this paper, we analyze 433 developer-ChatGPT conversations within GitHub issue threads to examine the impact of prompt knowledge gaps and conversation styles on issue resolution. We identify four main knowledge gaps in developer prompts: Missing Context, Missing Specifications, Multiple Context, and Unclear Instructions. Assuming that conversations within closed issues contributed to successful resolutions while those in open issues did not, we find that ineffective conversations contain knowledge gaps in 44.6% of prompts, compared to only 12.6% in effective ones. Additionally, we observe seven distinct conversational styles, with Directive Prompting, Chain of Thought, and Responsive Feedback being the most prevalent. We find that knowledge gaps are present in all styles of conversations, with Missing Context being the most repeated challenge developers face in issue-resolution conversations. Based on our analysis, we identify key textual and code-related heuristics-Specificity, Contextual Richness, and Clarity-that are associated with successful issue closure and help assess prompt quality. These heuristics lay the foundation for an automated tool that can dynamically flag unclear prompts and suggest structured improvements. To test feasibility, we developed a lightweight browser extension prototype for detecting prompt gaps, that can be easily adapted to other tools within developer workflows."
Unseen Horizons: Unveiling the Real Capability of LLM Code Generation Beyond the Familiar,"Zhang, YL; Xie, YF; Li, SS; Liu, K; Wang, C; Jia, ZY; Huang, XB; Song, J; Luo, CP; Zheng, ZZ; Xu, RL; Liu, YT; Zheng, S; Liao, XK",10.1109/ICSE55347.2025.00082,2025,"Recently, large language models (LLMs) have shown strong potential in code generation tasks. However, there are still gaps before they can be fully applied in actual software development processes. Accurately assessing the code generation capabilities of large language models has become an important basis for evaluating and improving the models. Some existing works have constructed datasets to evaluate the capabilities of these models. However, the current evaluation process may encounter the illusion of Specialist in Familiarity, primarily due to three gaps: the exposure of target code, case timeliness, and dependency availability. The fundamental reason for these gaps is that the code in current datasets may have been extensively exposed and exercised during the training phase, and due to the continuous training and development of LLM, their timeliness has been severely compromised. The key to solve the problem is to, as much as possible, evaluate the LLMs using code that they have not encountered before. Thus, the fundamental idea in this paper is to draw on the concept of code obfuscation, changing code at different levels while ensuring the functionality and output. To this end, we build a code-obfuscation based benchmark OBFUSEVAL. We first collect 1,354 raw cases from five real-world projects, including function description and code. Then we use three-level strategy (symbol, structure and semantic) to obfuscate descriptions, code and context dependencies. We evaluate four LLMs on OBFUSEVAL and compared the effectiveness of different obfuscation strategy. We use official test suites of these projects to evaluate the generated code. The results show that after obfuscation, the average decrease ratio of test pass rate can up to 62.5%."
LLM-AQuA-DiVeR: LLM-Assisted Quality Assurance Through Dialogues on Verifiable Specification with Requirement Owners,"Mitani, S; Moona, S; Matsuo, S; Burger, E",10.1109/RAIE66699.2025.00008,2025,"Quality Assurance (QA) is important for verifying software compliance with stakeholder requirements. QA faces a fundamental challenge of requirement interpretation ambiguity, which can result in insufficient software verification and failure in achieving the stakeholders' intended quality. The interpretation challenge intensifies in software development driven by Large Language Models (LLMs), where over-reliance can lead to missed quality-critical alternatives. However, existing works have paid limited attention to stakeholder involvement. We propose an LLM-assisted QA framework extending conventional LLM-driven development to enable stakeholder engagement in software verification. Our framework employs formal methods and rigorous testing to meet diverse quality demands, though this comprehensive verification introduces technical complexity affecting stakeholder engagement and verification costs. Our framework addresses these challenges through two key LLM roles: 1) an explanation assistant for stakeholder understanding, 2) a refinement assistant for incorporating stakeholder feedback while maintaining feasible verification costs. Our initial evaluation empirically demonstrates the framework's effectiveness through participant assessment scores, showing improved quality risk comprehension and efficient feedback incorporation in the verification process."
LLMs Meet Library Evolution: Evaluating Deprecated API Usage in LLM-based Code Completion,"Wang, C; Huang, KF; Zhang, J; Feng, YB; Zhang, L; Liu, Y; Peng, X",10.1109/ICSE55347.2025.00245,2025,"Large language models (LLMs), pre-trained or fine-tuned on large code corpora, have shown effectiveness in generating code completions. However, in LLM-based code completion, LLMs may struggle to use correct and up-to-date Application Programming Interfaces (APIs) due to the rapid and continuous evolution of libraries. While existing studies have highlighted issues with predicting incorrect APIs, the specific problem of deprecated API usage in LLM-based code completion has not been thoroughly investigated. To address this gap, we conducted the first evaluation study on deprecated API usage in LLM-based code completion. This study involved seven advanced LLMs, 145 API mappings from eight popular Python libraries, and 28,125 completion prompts. The study results reveal the status quo (i.e., API usage plausibility and deprecated usage rate) of deprecated API and replacing API usage in LLM-based code completion from the perspectives of model, prompt, and library, and indicate the root causes behind. Based on these findings, we propose two lightweight fixing approaches, REPLACEAPI and INSERTPROMPT, which can serve as baseline approaches for future research on mitigating deprecated API usage in LLM-based completion. Additionally, we provide implications for future research on integrating library evolution with LLMdriven software development."
Closing the Loop between User Stories and GUI Prototypes: An LLM-Based Assistant for Cross-Functional Integration in Software Development,"Kretzer, F; Kolthoff, K; Bartelt, C; Ponzetto, SP; Maedche, A",10.1145/3706598.3713932,2025,"Graphical user interfaces (GUIs) are at the heart of almost every software we encounter. GUIs are often created through a collaborative effort involving UX designers, product owners, and software developers, constantly facing changing requirements. Historically, problems in GUI development include a fragmented, poorly integrated tool landscape and high synchronization efforts between stakeholders. Recent approaches suggest using large language models (LLMs) to recognize requirements fulfillment in GUIs and automatically propose new GUI components. Based on ten interviews with practitioners, this paper proposes an LLM-based assistant as a Figma plug-in that bridges the gap between user stories and GUI prototyping. We evaluated the prototype with 40 users and 40 crowd-workers, showing that the effectiveness of GUI creation is improved by using LLMs to detect requirements' completion and generate new GUI components. We derive design rationales to support cross-functional integration in software development, ensuring that our plug-in integrates well into established processes."
Refuting LLM-generated Code with Reactive Task Comprehension,"Hebbar, SV; Harini, S; Kumar, V",10.1145/3724363.3729100,2025,"Large Language Models (LLMs) for code generation have improved to the point where they are being integrated into professional software development workflows. Since these models occasionally generate buggy code, it is important for students to develop the ability to refute such code (typically, by identifying a counterexample input on which the code fails to perform the desired task). To create counterexamples manually, prior work has suggested code comprehension and task comprehension as two necessary skills. In this paper, we anticipate advances in software development tools and consider a limited form of the latter skill - reactive task comprehension - where students only need to correctly state the code's desired behavior on inputs suggested by an automated system. We make two contributions. First, we demonstrate the feasibility of such a system based on existing LLMs and code coverage tools. Second, we show that reactive task comprehension is surprisingly effective in refuting LLM-generated buggy Python functions in the HumanEval+ dataset. Bearing in mind that students are likely to have access to increasingly sophisticated code generation models and assistive systems, we discuss the implications of our findings for introductory programming education."
GUIDE: LLM-Driven GUI Generation Decomposition for Automated Prototyping,"Kolthoff, K; Kretzer, F; Bartelt, C; Maedche, A; Ponzetto, SP",10.1109/ICSE-COMPANION66252.2025.00010,2025,"Graphical user interface (GUI) prototyping serves as one of the most valuable techniques for enhancing the elicitation of requirements, facilitating the visualization and refinement of customer needs and closely integrating the customer into the development activities. While GUI prototyping has a positive impact on the software development process, it simultaneously demands significant effort and resources. The emergence of Large Language Models (LLMs) with their impressive code generation capabilities offers a promising approach for automating GUI prototyping. Despite their potential, there is a gap between current LLM-based prototyping solutions and traditional user-based GUI prototyping approaches which provide visual representations of the GUI prototypes and direct editing functionality. In contrast, LLMs and related generative approaches merely produce text sequences or non-editable image output, which lacks both mentioned aspects and therefore impede supporting GUI prototyping. Moreover, minor changes requested by the user typically lead to an inefficient regeneration of the entire GUI prototype when using LLMs directly. In this work, we propose GUIDE, a novel LLM-driven GUI generation decomposition approach seamlessly integrated into the popular prototyping framework Figma. Our approach initially decomposes high-level GUI descriptions into fine-granular GUI requirements, which are subsequently translated into Material Design GUI prototypes, enabling higher controllability and more efficient adaption of changes. To efficiently conduct prompting-based generation of Material Design GUI prototypes, we propose a retrieval-augmented generation (RAG) approach to integrate the component library. Our preliminary evaluation demonstrates the effectiveness of GUIDE in bridging the gap between LLM generation capabilities and traditional GUI prototyping workflows, offering a more effective and controlled user-based approach to LLM-driven GUI prototyping. Video presentation of GUIDE is available at: https://youtu.be/C9RbhMxqpTU Index Terms-Automated GUI Prototyping,"
RepoChat: An LLM-Powered Chatbot for GitHub Repository Question-Answering,"Abedu, S; Menneron, L; Khatoonabadi, S; Shihab, E",10.1109/MSR66628.2025.00045,2025,"Software repositories contain a wealth of data about the software development process, such as source code, documentation, issue tracking, and commit histories. However, accessing and extracting meaningful insights from these data is time-consuming and requires technical expertise, posing challenges for software practitioners, especially non-technical stakeholders like project managers. Existing solutions, such as software engineering chatbots leveraging LLMs, have demonstrated significant limitations in retrieving relevant data to answer user questions. In this paper, we introduce RepoChat, a web-based tool designed to answer repository-related questions by synergizing LLMs with knowledge graphs. RepoChat operates in two steps: (1) the Data Ingestion step, where it collects and constructs a knowledge graph from repository metadata, such as commits, issues, files and users; and (2) the Interaction step, where it takes the users natural language question, translates it into graph queries using an LLM, executes these queries against the knowledge graph, and generates a user-friendly response to the question using the query results as context. We evaluate RepoChat by conducting a user study in which participants asked a series of repository-related questions representing common developer intents. RepoChat achieved an accuracy of 90%, correctly answering 36 out of 40 questions, demonstrating its effectiveness in accurately retrieving relevant information to answer user's questions. RepoChat is available at https://repochattool.streamlit.app/, and its source code is accessible on Zenodo [1]."
Toward efficient vibe coding: An LLM-based agent for low-code software development,"Malamas, N; Tsardoulias, E; Panayiotou, K; Symeonidis, AL",10.1016/j.cola.2025.101367,2025,"The Software Engineering (SE) domain increasingly adopts low-code and no-code approaches to simplify application development and deployment. Two dominant paradigms have emerged in this space: Model-driven Engineering (MDE), leveraging Domain-specific Languages (DSLs) to abstract implementation and reduce the knowledge and expertise required, and LLM-based vibe coding, where developers interact with Large Language Models (LLMs) using natural language, allowing for rapid prototyping and code generation through conversations. Although DSLs provide precise abstractions and formal correctness, they often require specialized knowledge and have a steep learning curve. Conversely, vibe coding enables fluid and natural interactions, but struggles with domain specificity and frequently produces erroneous or unstructured code, which is difficult to integrate into formal development workflows. To harness the strengths of both paradigms, we present DSL Agent, an LLM-powered conversational interface for DSL-based application development. The DSL Agent is embedded within Locsys, a modern low-code development platform. It combines the flexibility and intuitiveness of LLM-based vibe coding with the rigor of DSLs by dynamically generating accurate and valid DSL models based on user descriptions, embedded into a unified conversational interface that leverages prompt engineering and in-context learning techniques. This offers a simpler and more intuitive interface, accelerates the development process, and reduces the expertise barrier. The agent is evaluated by more than 130 workshop participants of varying expertise levels, on two DSLs of different complexity. Evaluation metrics, including valid model rate, user satisfaction, and development time, indicate a significant improvement in valid model generation, productivity, and ease of use compared to traditional DSL-based SE workflows. These results highlight the potential of the DSL Agent to improve the entire DSL-based development life cycle by offering an efficient, intuitive, and user-friendly interface."
Research directions for using LLM in software requirement engineering: a systematic review,"Hemmat, A; Sharbaf, M; Kolahdouz-Rahimi, S; Lano, K; Tehrani, SY",10.3389/fcomp.2025.1519437,2025,"Introduction Natural Language Processing (NLP) and Large Language Models (LLMs) are transforming the landscape of software engineering, especially in the domain of requirement engineering. Despite significant advancements, there is a notable lack of comprehensive survey papers that provide a holistic view of the impact of these technologies on requirement engineering. This paper addresses this gap by reviewing the current state of NLP and LLMs in requirement engineering.Methods We analyze trends in software requirement engineering papers, focusing on the application of NLP and LLMs. The review highlights their effects on improving requirement extraction, analysis, and specification, and identifies key patterns in the adoption of these technologies.Results The findings reveal an upward trajectory in the use of LLMs for software engineering tasks, particularly in requirement engineering. The review underscores the critical role of requirement engineering in the software development lifecycle and emphasizes the transformative potential of LLMs in enhancing precision and reducing ambiguities in requirement specifications.Discussion This paper identifies a growing interest and significant progress in leveraging LLMs for various software engineering tasks, particularly in requirement engineering. It provides a foundation for future research and highlights key challenges and opportunities in this evolving field."
ChatDL: An LLM-Based Defect Localization Approach for Software in IIoT Flexible Manufacturing,"Yang, HY; Zhou, YL; Liang, T; Kuang, L",10.1109/JIOT.2025.3531512,2025,"With the rapid advancement of flexible manufacturing in the Industrial Internet of Things (IIoT), there has been a significant increase in the number of IIoT devices and application software aimed at meeting various needs. The software defects may lead to delays or crashes in flexible manufacturing system, thereby affecting the production schedule. Automated software defect localization based on code changes can significantly reduce development and maintenance time costs, thereby maintaining the competitive edge of flexible manufacturing in the IIoT. Current efforts in software defect localization are primarily based on deep learning models or information retrieval models. This article investigates the performance of large language models (LLMs) in software defect localization and optimizes localization accuracy by combining it with an information retrieval model. Our empirical study reveals that GPT, given a software defect description, is unable to determine whether specific code changes are relevant. The model is unable to provide accurate answers, which aligns with the generative nature of LLMs where responses are generated according to probability distributions. However, the combined framework of LLMs and information retrieval models proposed in this article outperforms the current state-of-the-art models on public datasets. We conclude that LLMs can enhance localization performance when used as side information in conjunction with existing information retrieval models. The effectiveness of the framework has been validated through experiments conducted on publicly available datasets and in practical applications within IIoT projects. This offers valuable insights into the application and development of LLMs for defect localization in the software development and maintenance processes in the IIoT flexible manufacturing."
Exploring the Impact of LLM Prompting on Students' Learning,"Mutanga, MB; Msane, J; Mndaweni, TN; Hlongwane, BB; Ngcobo, NZ",10.3390/higheredu4030031,2025,"Integrating large language models (LLMs) into higher education, particularly in programming education, reshapes how students interact with learning materials and develop coding skills. However, while the general utility of LLMs like ChatGPT, Gemini, and Claude has been acknowledged, a critical gap exists in understanding how specific prompting strategies influence student learning outcomes. This issue is significant in the context of programming education, where problem-solving, critical thinking, and conceptual understanding are essential yet complex cognitive skills. Although prior research has classified prompting behaviors, it has largely failed to assess their impact on actual learning. To address this gap, we explored how IT students employ various prompting strategies when engaging with LLMs during programming tasks. A mixed-methods approach was adopted, primarily qualitative and supported by basic quantitative analysis, to examine 842 prompts generated by 140 students across four core software development modules. The results revealed five dominant prompting strategies, which varied significantly in how they facilitated learning. Our findings suggest that prompting strategies significantly shape how students interact with LLMs and influence the depth of their learning."
Breaking Barriers in Mobile Accessibility: A Study of LLM-Generated Native Android Interfaces,"Rabelo, DMF; Martins, RD; Santos, I; da Silva, PHG; Gama, K; Viana, W",10.1109/MOBILESoft66462.2025.00010,2025,"Rapid advancements in artificial intelligence, particularly in large language models (LLMs), have opened new opportunities for automating software development tasks, including the generation of code for mobile applications. This paper examines the potential of LLMs, such as ChatGPT, to improve the accessibility of native Android apps. It evaluates whether LLMgenerated code aligns with established accessibility standards, focusing on various screen layouts and prompt designs. Two studies were conducted to assess the accessibility of the generated interfaces. The first study analyzed the generation of seven types of mobile interfaces using less restrictive prompts. In contrast, the second study required the generated code to adhere to a specific interface programming model. After evaluation, 540 accessibility issues were identified, with interfaces generated using the Jetpack Compose framework showing better performance, although still in need of improvement. Interestingly, prompts explicitly requesting accessibility often produced more errors than standard prompts, highlighting the difficulties that LLMs face in comprehending and implementing accessibility requirements. This research emphasizes the need to refine LLM outputs when it comes to interface descriptions, as accessibility issues may be introduced into code intended for use by developers."
Enhancing TrUStAPIS Methodology in the Web of Things with LLM-Generated IoT Trust Semantics,"Ferraris, D; Kotis, K; Kalloniatis, C",10.1007/978-981-97-8798-2_7,2025,"In the Internet of Things (IoT) there are ecosystems where their physical 'smart' entities virtually interact with each other. Often, this interaction occurs among unknown entities, making trust an essential requirement to overcome uncertainty in several aspects of this interaction. However, trust is a complex concept, and incorporating it in IoT is still a challenging topic. For this reason, it is highly significant to specify and model trust in early stages of the System Development Life Cycle (SDLC) of IoT-integrated systems, thus enhancing the aforementioned task. TrUStAPIS is a requirements engineering methodology recently introduced for incorporating trust requirements during IoT-based system design. The scope of this paper is to provide an extension of TrUStAPIS by introducing IoT trust semantics compatible with the W3C Web of Things (WoT) recommendations generated with the assistance of Large Language Models (LLMs). Taking advantage of LLMs as a tool for integrating and refining existing methodologies, in this paper we present our work towards a revision of the TrUStAPIS methodology. In this work, we contribute a new conceptual model and a refined JSON-LD ontology that takes into account IoT trust semantics, providing eventually a valuable tool for software engineers to design and model IoT-based systems and services."
Effectiveness of symmetric metamorphic relations on validating the stability of code generation LLM,"Chan, PYP; Keung, J; Yang, Z",10.1016/j.jss.2024.112330,2025,"Pre-trained large language models (LLMs) are increasingly used in software development for code generation, with a preference for private LLMs over public ones to avoid the risk of exposing corporate secrets. Validating the stability of these LLMs' outputs is crucial, and our study proposes using symmetric Metamorphic Relations (MRs) from Metamorphic Testing (MT) for this purpose. Our study involved an empirical experiment with ten LLMs (eight private and two public) and two publicly available datasets. We defined seven symmetric MRs to generate Follow-up datasets from Source datasets for testing. Our evaluation aimed to detect violations (inconsistent predictions) between Source and Follow-up datasets and assess the effectiveness of MRs in identifying correct and incorrect non-violated predictions from ground truths. Results showed that one public and four private LLMs did not violate Case transformation of prompts MR. Furthermore, effectiveness and performance results indicated that proposed MRs are effective tools for explaining the instability of LLM's outputs by Case transformation of prompts, Duplication of prompts, and Paraphrasing of prompts. The study underscored the importance of enhancing LLMs' semantic understanding of prompts for better stability and highlighted potential future research directions, including exploring different MRs, enhancing semantic understanding, and applying symmetry to prompt engineering."
Exploring Automated Assertion Generation via Large Language Models,"Zhang, QJ; Sun, WF; Fang, CR; Yu, BW; Li, HY; Yan, M; Zhou, JY; Chen, ZY",10.1145/3699598,2025,"Unit testing aims to validate the correctness of software system units and has become an essential practice in software development and maintenance. However, it is incredibly time-consuming and labor-intensive for testing experts to write unit test cases manually, including test inputs (i.e., prefixes) and test oracles (i.e., assertions). Very recently, some techniques have been proposed to apply Large Language Models (LLMs) to generate unit assertions and have proven the potential in reducing manual testing efforts. However, there has been no systematic comparison of the effectiveness of these LLMs, and their pros and cons remain unexplored. To bridge this gap, we perform the first extensive study on applying various LLMs to automated assertion generation. The experimental results on two independent datasets show that studied LLMs outperform six state-of-the-art techniques with a prediction accuracy of 51.82%-58.71% and 38.72%-48.19%. The improvements achieve 29.60% and 12.47% on average. Besides, as a representative LLM, CodeT5 consistently outperforms all studied LLMs and all baselines on both datasets, with an average improvement of 13.85% and 26.64%, respectively. We also explore the performance of generated assertions in detecting real-world bugs, and find LLMs are able to detect 32 bugs from Defects4J on average, with an improvement of 52.38% against the most recent approach EDITAS. Inspired by the findings, we construct a simplistic retrieval-and-repair- enhanced LLM-based approach by transforming the assertion generation problem into a program repair task for retrieved similar assertions. Surprisingly, such a simplistic approach can further improve the prediction accuracy of LLMs by 9.40% on average, leading to new records on both datasets. Besides, we provide additional discussions from different aspects (e.g., the impact of assertion types and test lengths) to illustrate the capacity and limitations of LLM-based approaches. Finally, we further pinpoint various practical guidelines (e.g., the improvement of multiple candidate assertions) for advanced LLM-based assertion generation in the near future. Overall, our work underscores the promising future of adopting off-the-shelf LLMs to generate accurate and meaningful assertions in real-world test cases and reduce the manual efforts of unit testing experts in practical scenarios."
Technology adoption performance evaluation applied to testing industrial REST APIs,"Poth, A; Rrjolli, O; Arcuri, A",10.1007/s10515-024-00477-2,2025,"Testing is an important task within software development. To write test cases and integrate them into an automated test suite requires a significant amount of work. Given a set of requirements and specifications of a software, testing is needed to verify its correctness. When done manually, it is an expensive and error prone task. To facilitate such work, automated test-case generation via tools could be useful. Test-case generation can be facilitated by deterministic algorithm-driven approaches or non-deterministic approaches such as with AI (e.g., evolutionary and LLM). The different approaches come with their strengths and weaknesses, which must be considered when integrating these approaches into a product test procedure in industry. Several novel testing techniques and tools have been developed in academia and industry, but how effective they are and how to integrate them in real-world large industrial scenarios is still unclear. In this paper, a systematic approach is presented to evaluate test-case generation methodologies and integrate them into a scalable enterprise setup. The specific context is black-box testing of REST APIs, based on their OpenAPI schemas. The aim is to facilitate IT product development and service delivery. The proposed Technology Adoption Performance Evaluation (TAPE) approach is evaluated by a case study within the Group IT of Volkswagen AG. We evaluated existing tools such as OpenAPI Generator, EvoMaster and StarCoder which are built on different technologies. Our results show that these tools are of benefit for test engineers to facilitate test-case specification and design within the Group IT of Volkswagen AG."
Assessing output reliability and similarity of large language models in software development: A comparative case study approach,"Kim, DK; Ming, H",10.1016/j.infsof.2025.107787,2025,"Context: Generative large language models (LLMs) are increasingly used across various activities in software development, offering significant potential to enhance productivity. However, there is a lack of systematic study examining the reliability and similarity of the outputs from these models. Objective: This work presents a comparative analysis of the reliability-defined as the consistency and correctness of software artifacts-and similarity of LLM outputs in software development. Method: To accomplish the objective, we introduce a structured approach for assessing the reliability and similarity of outputs from five prominent LLMs-ChatGPT, Claude, Copilot, Gemini, and Meta-and apply it within two case studies focused on developing a food order and delivery system and a smart wallet system. Results: The study found that the overall output reliability of the models is rated at 0.82 with Claude outperforming other models at 0.92, followed by ChatGPT at 0.90, Copilot at 0.80, Meta at 0.75, and Gemini at 0.71. The models demonstrated an overall 57% similarity and 43% variability in their outputs, highlighting the uniqueness of models. Conclusions: While overall, LLMs exhibit decent reliability in their outputs with varying degrees, they still require human oversight and review of their outputs before implementation. LLMs present unique characteristics that practitioners should consider before adoption."
A Student-Centric Evaluation Survey to Explore the Impact of LLMs on UML Modeling,"Al-Ahmad, B; Alsobeh, A; Meqdadi, O; Shaikh, N",10.3390/info16070565,2025,"Unified Modeling Language (UML) diagrams serve as essential tools for visualizing system structure and behavior in software design. With the emergence of Large Language Models (LLMs) that automate various phases of software development, there is growing interest in leveraging these models for UML diagram generation. This study presents a comprehensive empirical investigation into the effectiveness of GPT-4-turbo in generating four fundamental UML diagram types: Class, Deployment, Use Case, and Sequence diagrams. We developed a novel rule-based prompt-engineering framework that transforms domain scenarios into optimized prompts for LLM processing. The generated diagrams were then synthesized using PlantUML and evaluated through a rigorous survey involving 121 computer science and software engineering students across three U.S. universities. Participants assessed both the completeness and correctness of LLM-assisted and human-created diagrams by examining specific elements within each diagram type. Statistical analyses, including paired t-tests, Wilcoxon signed-rank tests, and effect size calculations, validate the significance of our findings. The results reveal that while LLM-assisted diagrams achieve meaningful levels of completeness and correctness (ranging from 61.1% to 67.7%), they consistently underperform compared to human-created diagrams. The performance gap varies by diagram type, with Sequence diagrams showing the closest alignment to human quality and Use Case diagrams exhibiting the largest discrepancy. This research contributes a validated framework for evaluating LLM-generated UML diagrams and provides empirically-grounded insights into the current capabilities and limitations of LLMs in software modeling education."
"Efficient and Green Large Language Models for Software Engineering: Literature Review, Vision, and the Road Ahead","Shi, JK; Yang, Z; Lo, DV",10.1145/3708525,2025,"Large Language Models (LLMs) have recently shown remarkable capabilities in various software engineering tasks, spurring the rapid growth of the Large Language Models for Software Engineering (LLM4SE) area. However, limited attention has been paid to developing efficient LLM4SE techniques that demand minimal computational cost, time, and memory resources, as well as green LLM4SE solutions that reduce energy consumption, water usage, and carbon emissions. This article aims to redirect the focus of the research community toward the efficiency and greenness of LLM4SE, while also sharing potential research directions to achieve this goal. It commences with a brief overview of the significance of LLM4SE and highlights the need for efficient and green LLM4SE solutions. Subsequently, the article presents a vision for a future where efficient and green LLM4SE revolutionizes the LLM-based software engineering tool landscape, benefiting various stakeholders, including industry, individual practitioners, and society. The article then delineates a roadmap for future research, outlining specific research paths and potential solutions for the research community to pursue. While not intended to be a definitive guide, the article aims to inspire further progress, with the ultimate goal of establishing efficient and green LLM4SE as a central element in the future of software engineering. CCS Concepts: center dot General and reference-Surveys and overviews; center dot Software and its engineering-Software development techniques; center dot Computing methodologies-Artificial intelligence;"
Priv-IQ: A Benchmark and Comparative Evaluation of Large Multimodal Models on Privacy Competencies,"Shahriar, S; Dara, R",10.3390/ai6020029,2025,"Large language models (LLMs) and generative artificial intelligence (AI) have demonstrated notable capabilities, achieving human-level performance in intelligent tasks like medical exams. Despite the introduction of extensive LLM evaluations and benchmarks in disciplines like education, software development, and general intelligence, a privacy-centric perspective remains underexplored in the literature. We introduce Priv-IQ, a comprehensive multimodal benchmark designed to measure LLM performance across diverse privacy tasks. Priv-IQ measures privacy intelligence by defining eight competencies, including visual privacy, multilingual capabilities, and knowledge of privacy law. We conduct a comparative study evaluating seven prominent LLMs, such as GPT, Claude, and Gemini, on the Priv-IQ benchmark. Results indicate that although GPT-4o performs relatively well across several competencies with an overall score of 77.7%, there is room for significant improvements in capabilities like multilingual understanding. Additionally, we present an LLM-based evaluator to quantify model performance on Priv-IQ. Through a case study and statistical analysis, we demonstrate that the evaluator's performance closely correlates with human scoring."
Assisting early-stage software startups with LLMs: Effective prompt engineering and system instruction design,"Ahlgren, TL; Sunde, HF; Kemell, KK; Nguyen-Duc, A",10.1016/j.infsof.2025.107832,2025,"Context: Early-stage software startups, despite their strong innovative potential, experience high failure rates due to factors such as inexperience, limited resources, and market uncertainty. Generative AI technologies, particularly Large Language Models (LLMs), offer promising support opportunities; however, effective strategies for their integration into startup practices remain underexplored. Objective: This study investigates how prompt engineering and system instruction design can enhance the utility of LLMs in addressing the specific needs and challenges faced by early-stage software startups. Methods: A Design Science Research (DSR) methodology was adopted, structured into three iterative cycles. In the first cycle, use cases for LLM adoption within the startup context were identified. The second cycle experimented with various prompt patterns to optimize LLM responses for the defined use cases. The third cycle developed StartupGPT, an LLM-based assistant tailored for startups, exploring system instruction designs. The solution was evaluated with 25 startup practitioners through a combination of qualitative feedback and quantitative metrics. Results: The findings show that tailored prompt patterns and system instructions significantly enhance user perceptions of LLM support in real-world startup scenarios. StartupGPT received strong evaluation scores across key dimensions: satisfaction (93.33%), effectiveness (80%), efficiency (80%), and reliability (86.67%). Nonetheless, areas for improvement were identified, particularly in context retention, personalization of suggestions, communication tone, and sourcing external references. Conclusion: This study empirically validates the applicability of LLMs in early-stage software startups. It offers actionable guidelines for prompt and system instruction design and contributes both theoretical insights and a practical artifact - StartupGPT - that supports startup operations without necessitating costly LLM retraining."
Formal requirements engineering and large language models: A two-way roadmap,"Ferrari, A; Spoletini, P",10.1016/j.infsof.2025.107697,2025,"Context: Large Language Models (LLMs) have made remarkable advancements in emulating human linguistic capabilities, showing potential also in executing various requirements engineering (RE) tasks. However, despite their generally good performance, the adoption of LLM-generated solutions and artefacts prompts concerns about their correctness, fairness, and trustworthiness. Objective: This paper aims to address the concerns associated with the use of LLMs in RE activities. Specifically, it seeks to develop a roadmap that leverages formal methods (FMs) to provide guarantees of correctness, fairness, and trustworthiness when LLMs are utilised in RE. Symmetrically, it aims to explore how LLMs can be employed to make FMs more accessible. Methods: We use two sets of examples to show the current limits of FMs when used in software development and of LLMs when used for RE tasks. The highlighted limitations are addressed by proposing two roadmaps grounded in the current literature and technologies. Results: The proposed examples show the potential and limits of FMs in supporting software development and of LLMs when used for RE tasks. The initial investigation into how these limitations can be overcome has been concretised in two detailed roadmaps for the RE and, more largely, the software engineering community. Conclusion: The proposed roadmaps offer a promising approach to address the concerns of correctness, fairness, and trustworthiness associated with the use of LLMs in RE tasks through the use of FMs and to enhance the accessibility of FMs by utilising LLMs."
On Inter-Dataset Code Duplication and Data Leakage in Large Language Models,"LÃ³pez, JAH; Chen, BQ; Saad, M; Sharma, T; VarrÃ³, D",10.1109/TSE.2024.3504286,2025,"Motivation. Large language models (LLMs) have exhibited remarkable proficiency in diverse software engineering (SE) tasks, such as code summarization, code translation, and code search. Handling such tasks typically involves acquiring foundational coding knowledge on large, general-purpose datasets during a pre-training phase, and subsequently refining on smaller, task-specific datasets as part of a fine-tuning phase. Problem statement. Data leakage i.e., using information of the test set to perform the model training, is a well-known issue in training of machine learning models. A manifestation of this issue is the intersection of the training and testing splits. While intra-dataset code duplication examines this intersection within a given dataset and has been addressed in prior research, inter-dataset code duplication, which gauges the overlap between different datasets, remains largely unexplored. If this phenomenon exists, it could compromise the integrity of LLM evaluations because of the inclusion of fine-tuning test samples that were already encountered during pre-training, resulting in inflated performance metrics. Contribution. This paper explores the phenomenon of inter-dataset code duplication and its impact on evaluating LLMs across diverse SE tasks. Study design. We conduct an empirical study using the CodeSearchNet dataset (csn), a widely adopted pre-training dataset, and five fine-tuning datasets used for various SE tasks. We first identify the intersection between the pre-training and fine-tuning datasets using a deduplication process. Next, we pre-train two versions of LLMs using a subset of csn: one leaky LLM, which includes the identified intersection in its pre-training set, and one non-leaky LLM that excludes these samples. Finally, we fine-tune both models and compare their performances using fine-tuning test samples that are part of the intersection. Results. Our findings reveal a potential threat to the evaluation of LLMs across multiple SE tasks, stemming from the inter-dataset code duplication phenomenon. We also demonstrate that this threat is accentuated by the chosen fine-tuning technique. Furthermore, we provide evidence that open-source models such as CodeBERT, GraphCodeBERT, and UnixCoder could be affected by inter-dataset duplication. Based on our findings, we delve into prior research that may be susceptible to this threat. Additionally, we offer guidance to SE researchers on strategies to prevent inter-dataset code duplication."
GPIoT: Tailoring Small Language Models for IoT Program Synthesis and Development,"Shen, LM; Yang, Q; Huang, XY; Mal, ZJ; Zheng, YQ",10.1145/3715014.3722064,2025,"Code Large Language Models (LLMs) enhance software development efficiency by automatically generating code and documentation based on user requirements. However, code LLMs cannot synthesize specialized programs when tasked with IoT applications that require domain knowledge. While Retrieval-Augmented Generation (RAG) offers a promising solution by fetching relevant domain knowledge, it necessitates powerful cloud LLMs (e.g., GPT-4) to process user requirements and retrieved contents, which raises significant privacy concerns. This approach also suffers from unstable networks and prohibitive LLM query costs. Moreover, it is challenging to ensure the correctness and relevance of the fetched contents. To address these issues, we propose GPIoT, a code generation system for IoT applications by fine-tuning locally deployable Small Language Models (SLMs) on IoT-specialized datasets. SLMs have smaller model sizes, allowing efficient local deployment and execution to mitigate privacy concerns and network uncertainty. Furthermore, by fine-tuning SLMs with our IoT-specialized datasets, the SLMs' ability to synthesize IoT-related programs can be substantially improved. To evaluate GPIoT's capability in synthesizing programs for IoT applications, we develop a benchmark, IoTBench. Extensive experiments and user trials demonstrate the effectiveness of GPIoT in generating IoT-specialized code, outperforming state-of-the-art code LLMs with an average task accuracy increment of 64.7% and significant improvements in user satisfaction."
Enhancing Personalized Service Development with Virtual Agents and Upcycling Techniques,"Nakata, T; Chen, SA; Saiki, S; Nakamura, M",10.1007/s44227-024-00043-y,2025,"In this study, aimed at achieving Society 5.0, we explore efficient development methods for services tailored to individual user needs. The diversification of user demographics has led to swiftly changing service demands, increasing the necessity for personalized adaptation. However, developing services tailored to individual users requires specialized knowledge and incurs high costs. This research focuses on developing technologies that enable service development through a concrete understanding of user needs. Utilizing an interactive needs extraction system with a virtual agent (VA) developed in prior studies, along with the Sharing Upcycling Cases with Context and Evaluation for Efficient Software Development (SUCCEED) system, we aim to automate the extraction of user needs and provide developers with relevant insights. Prior research has found it challenging to grasp the outline of services to be developed from user statements. However, a new method using large language models (LLM) has enabled the automatic extraction of novel service needs. Experiments have shown the potential to automatically provide development insights for 67% of ideal needs."
Autonomous Agents in Software Development: A Vision Paper,"Rasheed, Z; Waseem, M; Sami, MA; Kemell, KK; Ahmad, A; Duc, AN; SystÃ¤, K; Abrahamsson, P",10.1007/978-3-031-72781-8_2,2025,"Large Language Models (LLM) are reshaping the field of Software Engineering (SE). They enable innovative methods for executing many SE tasks, including automation of entire process of Software Development Life Cycle (SDLC). However, only a limited number of existing works have thoroughly explored the potential of LLM based AI agents to automate the entire lifecycle in SE. In this paper, we demonstrate the success of our initial efforts in automating the entire lifecycle autonomously based on given software specification as input, which has shown remarkable efficiency and significantly reduced development time. Our preliminary results suggest that the careful implementation of AI agents can enhance the development lifecycle. We aim to streamline the SDLC by integrating all phases into an AI-driven chat interface, enhancing efficiency and transparency. Furthermore, we seek to enhance collaboration, creating an environment where stakeholders from various backgrounds can contribute, review, and refine ideas and requirements in real-time. This forward-looking direction guarantees to redefine the paradigms of SE and also make software creation more inclusive, collaborative, and efficient."
"LLMs in Mobile Apps: Practices, Challenges, and Opportunities","Hau, K; Hassan, S; Zhou, SR",10.1109/MOBILESoft66462.2025.00008,2025,"The integration of AI techniques has become increasingly popular in software development, enhancing performance, usability, and the availability of intelligent features. With the rise of large language models (LLMs) and generative AI, developers now have access to a wealth of high-quality open-source models and APIs from closed-source providers, enabling easier experimentation and integration of LLMs into various systems. This has also opened new possibilities in mobile application (app) development, allowing for more personalized and intelligent apps. However, integrating LLM into mobile apps might present unique challenges for developers, particularly regarding mobile device constraints, API management, and code infrastructure. In this project, we constructed a comprehensive dataset of 149 LLM-enabled Android apps and conducted an exploratory analysis to understand how LLMs are deployed and used within mobile apps. This analysis highlights key characteristics of the dataset, prevalent integration strategies, and common challenges developers face. Our findings provide valuable insights for future research and tooling development aimed at enhancing LLM-enabled mobile apps."
A Case Study Investigating the Role of Generative AI in Quality Evaluations of Epics in Agile Software Development,"Geyer, W; He, J; Sarkar, D; Brachman, M; Hammond, C; Heins, J; Ashktorab, Z; Rosemberg, C; Hill, C",10.1145/3729176.3729200,2025,"The broad availability of generative AI offers new opportunities to support various work domains, including agile software development. Agile epics are a key artifact for product managers to communicate requirements to stakeholders. However, in practice, they are often poorly defined, leading to churn, delivery delays, and cost overruns. In this industry case study, we investigate opportunities for large language models (LLMs) to evaluate agile epic quality in a global company. Results from a user study with 17 product managers indicate how LLM evaluations could be integrated into their work practices, including perceived values and usage in improving their epics. High levels of satisfaction indicate that agile epics are a new, viable application of AI evaluations. However, our findings also outline challenges, limitations, and adoption barriers that can inform both practitioners and researchers on the integration of such evaluations into future agile work practices."
Assessing LLMs for Front-end Software Architecture Knowledge,"Guerra, LPF; Ernst, N",10.1109/Designing66910.2025.00007,2025,"Large Language Models (LLMs) have demonstrated significant promise in automating software development tasks, yet their capabilities with respect to software design tasks remains largely unclear. This study investigates the capabilities of an LLM in understanding, reproducing, and generating structures within the complex VIPER architecture, a design pattern for iOS applications. We leverage Bloom's taxonomy to develop a comprehensive evaluation framework to assess the LLM's performance across different cognitive domains such as remembering, understanding, applying, analyzing, evaluating, and creating. Experimental results, using ChatGPT 4 Turbo 2024-04-09, reveal that the LLM excelled in higher-order tasks like evaluating and creating, but faced challenges with lower-order tasks requiring precise retrieval of architectural details. These findings highlight both the potential of LLMs to reduce development costs and the barriers to their effective application in real-world software design scenarios. This study proposes a benchmark format for assessing LLM capabilities in software architecture, aiming to contribute toward more robust and accessible AI-driven development tools."
ISAC-Driven Software Development With Large Language Models: A Foundation for Enhanced Public Life Support,"Huang, YQ; Jin, X; Zhang, L",10.1109/ACCESS.2025.3550156,2025,"The rapid evolution of integrated sensing and communication (ISAC) technologies has created an unprecedented data overflow, making traditional analysis methods obsolete. The emergence of large language models (LLMs) such as ChatGPT has revolutionized various fields, including software engineering, by enabling the development of novel communication-aware software applications. This study explores the synergy between ISAC and LLMs in software development, using the Internet of Vehicle-Monitoring Software as a case study. The experimental data shows that without LLM, the average packet loss rate increases from 0.2% to 1.2%. It can be seen that the potential of ISAC-LLM integration is to release new opportunities for data-driven innovation by combining communication awareness technology with large-scale software development. By integrating communication awareness technology with large-scale software development, the potential of ISAC-LLM convergence is demonstrated in unlocking new opportunities for data-driven innovation. The research lays the foundation for further exploring the capabilities of LLMs in software development and harnessing their potential for transformative impact."
Evaluating Large Language Models in Code Generation: INFINITE Methodology for Defining the Inference Index,"Christakis, N; Drikakis, D",10.3390/app15073784,2025,"This study introduces a new methodology for an Inference Index (InI) called the Inference Index In Testing Model Effectiveness methodology (INFINITE), aiming to evaluate the performance of Large Language Models (LLMs) in code generation tasks. The InI index provides a comprehensive assessment focusing on three key components: efficiency, consistency, and accuracy. This approach encapsulates time-based efficiency, response quality, and the stability of model outputs, offering a thorough understanding of LLM performance beyond traditional accuracy metrics. We apply this methodology to compare OpenAI's GPT-4o (GPT), OpenAI-o1 pro (OAI1), and OpenAI-o3 mini-high (OAI3) in generating Python code for two tasks: a data-cleaning and statistical computation task and a Long Short-Term Memory (LSTM) model generation task for forecasting meteorological variables such as temperature, relative humidity, and wind speed. Our findings demonstrate that GPT outperforms OAI1 and performs comparably to OAI3 regarding accuracy and workflow efficiency. The study reveals that LLM-assisted code generation can produce results similar to expert-designed models with effective prompting and refinement. GPT's performance advantage highlights the benefits of widespread use and user feedback. These findings contribute to advancing AI-assisted software development, providing a structured approach for evaluating LLMs in coding tasks and setting the groundwork for future studies on broader model comparisons and expanded assessment frameworks."
Beyond Snippet Assistance: A Workflow-Centric Framework for End-to-End AI-Driven Code Generation,"Sonkin, V; Tudose, C",10.3390/computers14030094,2025,"Recent AI-assisted coding tools, such as GitHub Copilot and Cursor, have enhanced developer productivity through real-time snippet suggestions. However, these tools primarily assist with isolated coding tasks and lack a structured approach to automating complex, multi-step software development workflows. This paper introduces a workflow-centric AI framework for end-to-end automation, from requirements gathering to code generation, validation, and integration, while maintaining developer oversight. Key innovations include automatic context discovery, which selects relevant codebase elements to improve LLM accuracy; a structured execution pipeline using Prompt Pipeline Language (PPL) for iterative code refinement; self-healing mechanisms that generate tests, detect errors, trigger rollbacks, and regenerate faulty code; and AI-assisted code merging, which preserves manual modifications while integrating AI-generated updates. These capabilities enable efficient automation of repetitive tasks, enforcement of coding standards, and streamlined development workflows. This approach lays the groundwork for AI-driven development that remains adaptable as LLM models advance, progressively reducing the need for human intervention while ensuring code reliability."
Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements,"Abtahi, SM; Azim, A",10.1109/Forge66646.2025.00017,2025,"This study examined code issue detection and revision automation by integrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and GPT-4o into software development workflows. A static code analysis framework detects issues such as bugs, vulnerabilities, and code smells within a large-scale software project. Detailed information on each issue was extracted and organized to facilitate automated code revision using LLMs. An iterative prompt engineering process is applied to ensure that prompts are structured to produce accurate and organized outputs aligned with the project requirements. Retrieval-augmented generation (RAG) is implemented to enhance the relevance and precision of the revisions, enabling LLM to access and integrate real-time external knowledge. The issue of LLM hallucinations-where the model generates plausible but incorrect outputs-is addressed by a custom-built Code Comparison App, which identifies and corrects erroneous changes before applying them to the codebase. Subsequent scans using the static code analysis framework revealed a significant reduction in code issues, demonstrating the effectiveness of combining LLMs, static analysis, and RAG to improve code quality, streamline the software development process, and reduce time and resource expenditure."
Measuring and Improving the Efficiency of Python Code Generated by LLMs Using CoT Prompting and Fine-Tuning,"Jonnala, R; Yang, J; Lee, Y; Liang, GB; Cao, ZC",10.1109/ACCESS.2025.3585742,2025,"The burgeoning sophistication of Artificial Intelligence (AI) has catalyzed the rapid proliferation of Large Language Models (LLMs) within software development. These models are increasingly employed to automate the generation of functionally correct code, address complex computational problems, and facilitate the debugging of existing software systems. However, LLM-generated code often faces challenges due to inherent inefficiencies, including redundant logical structures, factually inconsistent content (hallucinations), and programming errors. To address this issue, our research rigorously evaluated the computational efficiency of Python code generated by three prominent LLMs: GPT-4o-Mini, GPT-3.5-Turbo, and GPT-4-Turbo. The evaluation metrics encompass execution time, memory utilization, and peak memory consumption, while maintaining the functional correctness of the generated code. Leveraging the EffiBench benchmark datasets within the Google Vertex AI Workbench environment, across a spectrum of machine configurations, the study implemented a consistent seed parameter to ensure experimental reproducibility. Furthermore, we investigated the impact of two distinct optimization strategies: Chain-of-Thought (CoT) prompting and model fine-tuning. Our findings reveal a significant enhancement in efficiency metrics for GPT-4o-Mini and GPT-3.5-Turbo when employing CoT prompting; however, this trend was not observed for GPT-4-Turbo. Based on its promising performance with CoT prompting, we selected the GPT-4o-Mini model for subsequent fine-tuning, aiming to further enhance both its computational efficiency and accuracy. However, contrary to our expectations, fine-tuning the GPT-4o-Mini model led to a discernible degradation in both its accuracy and computational efficiency. In conclusion, this study provides empirical evidence suggesting that the deployment of high-CPU machine configurations, in synergy with the utilization of the GPT-4o-Mini model and CoT prompting techniques, yields demonstrably more efficient and accurate LLM-generated Python code, particularly within computationally intensive application scenarios."
DAWN: Designing Distributed Agents in a Worldwide Network,"Aminiranjbar, Z; Tang, JA; Wang, QD; Pant, S; Viswanathan, M",10.1109/ACCESS.2025.3588425,2025,"The rapid evolution of Large Language Models (LLMs) has transformed them from basic conversational tools into sophisticated entities capable of complex reasoning and decision-making. These advancements have led to the development of specialized LLM-based agents designed for diverse tasks such as coding and web browsing. As these agents become more capable, the need for a robust framework that facilitates global agentic communication and collaboration for building sophisticated software solutions has become increasingly important. Distributed Agents in a Worldwide Network (DAWN) addresses this need by providing an architectural framework that allows globally distributed agents of any provenance to be registered, discovered, and organized for building AI-based applications and solutions. In DAWN, a Principal Agent Service composes and oversees the execution of agentic applications. It delegates tasks to one or more Gateway Agent Services that provide for the discovery, registration, and connection of the most suitable agents to fit each application's needs. DAWN offers three operational modes: No-LLM mode for deterministic and classical software development, Copilot for decision-making augmented using AI, and LLM Agent for autonomous operations. Last but not least, DAWN ensures the safety and security of agent collaborations globally through a dedicated safety, security, and compliance layer, protecting the network against attackers and adhering to stringent security and compliance standards. These features make DAWN a robust framework for designing, developing, and deploying agent-based applications across business and consumer applications."
Hold on! is my feedback useful? evaluating the usefulness of code review comments,"Ahmed, S; Eisty, NU",10.1007/s10664-025-10617-1,2025,"Context:In collaborative software development, the peer code review process proves beneficial only when the reviewers provide useful comments.Objective:This paper investigates the usefulness of Code Review Comments (CR comments) through textual feature-based and featureless approaches.Method:We select three available datasets from both open-source and commercial projects. Additionally, we introduce new features from software and non-software domains. Moreover, we experiment with the presence of jargon, voice, and codes in CR Comments and classify the usefulness of CR Comments through featurization, bag-of-words, and transfer learning techniques.Results:Our models outperform the baseline by achieving state-of-the-art performance. Furthermore, the result demonstrates that the commercial gigantic LLM, GPT-4o, and non-commercial naive featureless approach, Bag-of-Word with TF-IDF, are more effective for predicting the usefulness of CR Comments.Conclusion:The significant improvement in predicting usefulness solely from CR Comments escalates research on this task. Our analyses portray the similarities and differences of domains, projects, datasets, models, and features for predicting the usefulness of CR Comments."
Programming Quantum Computers with Large Language Models,"Henderson, ER; Henderson, JM; Ange, J; Thornton, MA",10.1117/12.3049666,2025,"Large language models (LLMs) promise transformative change to fields as diverse as medical diagnosis, legal services, and software development. One reason for such an impact is LLMs' ability to make highly technical endeavors more accessible to a broader audience. Accessibility has long been a goal for the growing fields of quantum computing, informatics, and engineering, especially as more quantum systems become publicly available via cloud interfaces. Between programming quantum computers and using LLMs, the latter seems the more accessible task: while leveraging an LLM's fullest potential requires experience with prompt engineering, any literate person can provide queries and read responses. By contrast, designing and executing quantum programs-outside of those available online-requires significant background knowledge, from selection of operations for algorithm implementation to configuration choices for particular hardware specifications and providers. Current research is exploring LLM utility for classical software development, but there has been relatively little investigation into the same for quantum programming. Consequently, this work is a first look at how well an uncustomized, publicly available LLM can write straightforward quantum circuits. We examine how well OpenAI's ChatGPT (GPT-4) can write quantum circuits for two hardware providers: the superconducting qubit machines of IBM and the photonic devices of Xanadu. We find that ChatGPT currently fares substantially better with the former."
Do LLMs consider security? an empirical study on responses to programming questions,"Sajadi, A; Le, B; Nguyen, A; Damevski, K; Chatterjee, P",10.1007/s10664-025-10658-6,2025,"The widespread adoption of conversational LLMs for software development has raised new security concerns regarding the safety of LLM-generated content. Our motivational study outlines ChatGPT's potential in volunteering context-specific information to the developers, promoting safe coding practices. Motivated by this finding, we conduct a study to evaluate the degree of security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and Llama 3. We prompt these LLMs with Stack Overflow questions that contain vulnerable code to evaluate whether they merely provide answers to the questions or if they also warn users about the insecure code, thereby demonstrating a degree of security awareness. Further, we assess whether LLM responses provide information about the causes, exploits, and the potential fixes of the vulnerability, to help raise users' awareness. Our findings show that all three models struggle to accurately detect and warn users about vulnerabilities, achieving a detection rate of only 12.6% to 40% across our datasets. We also observe that the LLMs tend to identify certain types of vulnerabilities related to sensitive information exposure and improper input neutralization much more frequently than other types, such as those involving external control of file names or paths. Furthermore, when LLMs do issue security warnings, they often provide more information on the causes, exploits, and fixes of vulnerabilities compared to Stack Overflow responses. Finally, we provide an in-depth discussion on the implications of our findings, and demonstrated a CLI-based prompting tool that can be used to produce more secure LLM responses."
Human-In-The-Loop Software Development Agents: Challenges and Future Directions,"Pasuksmit, J; Takerngsaksiri, W; Thongtanunam, P; Tantithamthavorn, C; Zhang, RX; Wang, SY; Jiang, F; Li, J; Cook, E; Chen, K; Wu, M",10.1109/MSR66628.2025.00112,2025,"Multi-agent LLM-driven systems for software development are rapidly gaining traction, offering new opportunities to enhance productivity. At Atlassian, we deployed Human-in-the-Loop Software Development Agents to resolve Jira work items and evaluated the generated code quality using functional correctness testing and GPT-based similarity scoring. This paper highlights two major challenges: the high computational costs of unit testing and the variability in LLM-based evaluations. We also propose future research directions to improve evaluation frameworks for Human-In-The-Loop software development tools."
Identification and Optimization of Redundant Code Using Large Language Models,"Cynthia, ST",10.1109/CAIN66642.2025.00042,2025,"Redundant code is a persistent challenge in software development that makes systems harder to maintain, scale, and update. It adds unnecessary complexity, hinders bug fixes, and increases technical debt. Despite their impact, removing redundant code manually is risky and error-prone, often introducing new bugs or missing dependencies. While studies highlight the prevalence and negative impact of redundant code, little focus has been given to Artificial Intelligence (AI) system codebases and the common patterns that cause redundancy. Additionally, the reasons behind developers unintentionally introducing redundant code remain largely unexplored. This research addresses these gaps by leveraging large language models (LLMs) to automatically detect and optimize redundant code in AI projects. Our research aims to identify recurring patterns of redundancy and analyze their underlying causes, such as outdated practices or insufficient awareness of best coding principles. Additionally, we plan to propose an LLM agent that will facilitate the detection and refactoring of redundancies on a large scale while preserving original functionality. This work advances the application of AI in identifying and optimizing redundant code, ultimately helping developers maintain cleaner, more readable, and scalable codebases."
Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation,"Steenhoek, B; Tufanot, M; Sundaresan, N; Svyatkovskiy, A",10.1109/DeepTest66595.2025.00011,2025,"Software testing is a crucial but time-consuming aspect of software development, and recently, Large Language Models (LLMs) have gained popularity for automated test case generation. However, because LLMs are trained on vast amounts of open-source code, they often generate test cases that do not adhere to best practices and may even contain test smells (anti-patterns). To address this issue, we propose Reinforcement Learning from Static Quality Metrics (RLSQM), wherein we utilize Reinforcement Learning to generate high-quality unit tests based on static analysis-based quality metrics. First, we analyzed LLM-generated tests and show that LLMs frequently do generate undesirable test smells - up to 37% of the time. Then, we implemented lightweight static analysis-based reward model and trained LLMs using this reward model to optimize for five code quality metrics. Our experimental results demonstrate that the RL-optimized Codex model consistently generated higher-quality test cases than the base LLM, improving quality metrics by up to 23%, and generated nearly 100% syntactically-correct code. RLSQM also outperformed GPT-4 on all code quality metrics, in spite of training a substantially cheaper Codex model. We provide insights into how reliably utilize RL to improve test generation quality and show that RLSQM is a significant step towards enhancing the overall efficiency and reliability of automated software testing. Our data are available at this link: https://doi.org/10.6084/m9.figshare.25983166."
Enhancing the Validation of Human Factors in User Interface Software Testing with AI,"Goetz, M",10.1007/978-3-031-93861-0_4,2025,"This paper introduces a novel approach to human factor focused software testing in the loop that combines Optical Character Recognition (OCR), and large language models (LLMs) for scenariodriven, non-intrusive testing of graphical user interfaces (GUIs). By defining a testing scenario and analyzing a captured image of the UI, the system identifies visual elements using OCR and detection algorithms, storing them in a standardized format. This output serves as input for an LLM, which interprets scenario requirements and generates user actions, such as mouse clicks or keyboard inputs, to achieve the defined objectives. The system operates in a closed loop, iterating until the target outcome, e.g., the scenario is achieved, failures are detected, or usability metrics are generated. This non-intrusive method avoids software modification, making it ideal for cases where direct instrumentation is infeasible, like closed-source software. Fine-tuning the LLM for domain-specific metrics, such as cognitive load and task complexity, enhances usability testing by simulating users with varying expertise. The concept also supports environmental simulations, enabling realistic testing of safety-critical systems like automotive or aviation software. This idea offers a scalable and flexible addition to traditional GUI testing, enabling early in the software development loop human factor insights, and a first level of functionality validation for safety-relevant applications."
Do LLMs Provide Links to Code Similar to What They Generate? A Study with Gemini and Bing CoPilot,"Bifolco, D; Cassieri, P; Scanniello, G; Di Penta, M; Zampetti, F",10.1109/MSR66628.2025.00042,2025,"Large Language Models (LLMs) are currently used for various software development tasks, including generating code snippets to solve specific problems. Unlike reuse from the Web, LLMs are limited in providing provenance information about the generated code, which may have important trustworthiness and legal consequences. While LLM-based assistants may provide external links that are related to the generated code, we do not know how relevant such links are. This paper presents the findings of an empirical study assessing the extent to which 243 and 194 code snippets, across six programming languages, generated by Bing CoPilot and Google Gemini, likely originate from the links provided by these two LLM-based assistants. The study leverages automated code similarity assessments with thorough manual analysis. The study's findings indicate that the LLM-based assistants provide a mix of relevant and irrelevant links having a different nature. Specifically, although 66% of the links from Bing CoPilot and 28% from Google Gemini are relevant, LLMs-based assistants still suffer from serious provenance debt."
Fuzzy-Assisted Contrastive Decoding Improving Code Generation of Large Language Models,"Wang, S; Ding, L; Zhan, YB; Luo, Y; Liu, S; Ding, WP",10.1109/TFUZZ.2025.3575060,2025,"Large language models (LLMs) play a crucial role in intelligent code generation tasks. Most existing work focuses on pretraining or fine-tuning specialized code LLMs, e.g., CodeLlama. However, pretraining or fine-tuning a code LLM requires a vast corpus of data, significant computational resources, and considerable human effort. Compared to pretraining or fine-tuning LLMs, a simple and flexible method of contrastive decoding has garnered widespread attention to improve the text generation quality of LLMs. While contrastive decoding can indeed improve the text generation quality of LLMs, our research has found that directly using contrastive decoding: 1) introduces erroneous information into the logit distribution generated from normal prompts (i.e., user's input), particularly in the code generation of LLMs; 2) significantly impedes the inference and decoding time of LLMs. In this work, the limitations of using contrastive decoding directly are systematically highlighted, and a novel real-time fuzzy-assisted contrastive decoding (FCD) mechanism is proposed to improve the code generation quality of LLMs. The proposed FCD mechanism initially categorizes prompts into high-quality and low-quality groups based on the results of the evaluator (i.e., unit test) before integrating the LLM. Next, feature values (e.g., standard deviation, peak value, etc.) related to the logit distribution of predicted tokens during the LLM's inference process for both high-quality and low-quality prompts are extracted. Finally, the extracted feature values are used to train the fuzzy neural network (i.e, fuzzy min-max neural network) offline, allowing for the prejudgement of the reliability of the logit distribution for normal prompt outputs. This prevents the direct use of erroneous information from contrastive decoding and improves the code generation quality of LLMs. Through extensive experiments, it has been demonstrated that the proposed FCD mechanism can significantly improve the code generation quality of LLMs through FCD. Moreover, the FCD mechanism can also reduce the time required for inference and contrastive decoding."
Students' Perception of ChatGPT in Software Engineering: Lessons Learned from Five Courses,"Baresi, L; De Lucia, A; Di Marco, A; Di Penta, M; Di Ruscio, D; Mariani, L; Micucci, D; Palomba, F; Rossi, MT; Zampetti, F",10.1109/CSEET66350.2025.00023,2025,"A few years after their release, Large Language Models (LLMs)-based tools are becoming an essential component of software education, as calculators are used in math courses. When learning software engineering (SE), the challenge is the extent to which LLMs are suitable and easy to use for different software development tasks. In this paper, we report the findings and lessons learned from using LLM-based tools-ChatGPT in particular-in five SE courses from four universities. After instructing students on the LLM potentials in SE and about prompting strategies, we ask participants to complete a survey and be involved in semi-structured interviews. The collected results report (i) indications about the usefulness of the LLM for different tasks, (ii) challenges to prompt the LLM, i.e., interact with it, (iii) challenges to adapt the generated artifacts to their own needs, and (iv) wishes about some valuable features students would like to see in LLM-based tools. Although results vary among different courses, also because of students' seniority and course goals, the perceived usefulness is greater for lowlevel phases (e.g., coding or debugging/fault localization) than for analysis and design phases. Interaction and code adaptation challenges vary among tasks and are mostly related to the need for task-specific prompts, as well as better specification of the development context."
Leveraging LLMs for Trustworthy Software Engineering: Insights and Challenges,"Vieira, M",10.1109/MC.2025.3546204,2025,"Large language models (LLMs) are transforming software engineering by accelerating development, reducing complexity, and cutting costs. If fully integrated into the software lifecycle they will have the potential to drive design, development, and deployment. However, LLM-driven trustworthy software engineering requires addressing multiple challenges."
Impact of Developer Queries on the Effectiveness of Conversational Large Language Models in Programming,"Taneski, V; Karakatic, S; Rek, P; Jost, G",10.3390/app15126836,2025,"This study investigates the effects of LLM-based coding assistance on web application development by students using a frontend framework. Rather than comparing different models, it focuses on how students interact with LLM tools to isolate the impact of query type on coding success. To this end, participants were instructed to rely exclusively on LLMs for writing code, based on a given set of specifications, and their queries were categorized into seven types: Error Fixing (EF), Feature Implementation (FI), Code Optimization (CO), Code Understanding (CU), Best Practices (BP), Documentation (DOC), and Concept Clarification (CC). The results reveal that students who queried LLMs for error fixing (EF) were statistically more likely to have runnable code, regardless of prior knowledge. Additionally, students seeking code understanding (CU) and error fixing performed better, even when normalizing for previous coding ability. These findings suggest that the nature of the queries made to LLMs influences the success of programming tasks and provides insights into how AI tools can assist learning in software development."
Performance and interpretability analysis of code generation large language models,"Pendyala, VS; Thakur, NB",10.1016/j.neucom.2025.131461,2025,"As Large Language Models (LLMs) are increasingly getting integrated into software development workflows, understanding their reliability, error patterns and interpretability in real-world development scenarios is crucial for establishing their practical utility. This study evaluates and interprets the performance of 15 open-source LLM models, including Code LLaMa, Granite Code, DeepSeek-Coder-V2, and Yi-Coder on code translation and generation from requirements using the Rosetta Code dataset across diverse programming languages and tasks. Syntactic correctness and code quality are quantified using metrics such as CodeBLEU, chrF, and METEOR. Interpretability is explored through Feature Ablation and Shapley Value Sampling to elucidate prompt processing mechanisms. Results indicate high syntactic correctness and quality scores for models such as DeepSeek-Coder-V2 and Yi-Coder, alongside observed sensitivities to specific prompt components. This research provides quantitative and qualitative insights into the capabilities and limitations of open-source code-generating LLMs, informing model selection and the understanding of LLM-generated code."
Are LLMs Correctly Integrated into Software Systems?,"Shao, YC; Huang, YH; Shen, JW; Ma, L; Su, T; Wan, CC",10.1109/ICSE55347.2025.00204,2025,"Large language models (LLMs) provide effective solutions in various application scenarios, with the support of retrieval-augmented generation (RAG). However, developers face challenges in integrating LLM and RAG into software systems, due to lacking interface specifications, various requirements from software context, and complicated system management. In this paper, we have conducted a comprehensive study of 100 opensource applications that incorporate LLMs with RAG support, and identified 18 defect patterns. Our study reveals that 77% of these applications contain more than three types of integration defects that degrade software functionality, efficiency, and security. Guided by our study, we propose systematic guidelines for resolving these defects in software life cycle. We also construct an open-source defect library HYDRANGEA [1]."
Advancing Secure and Standard Source Code Generation Techniques,"Siddiq, ML",10.1109/ICSE-Companion66252.2025.00023,2025,"The rise of ChatGPT and GitHub Copilot has sparked a surge in developers leveraging large language models (LLMs) for code generation, aiming to automate software development processes. However, these tools can generate substandard and vulnerable code. Notably, a significant portion of developers in the US embrace LLMs due to productivity boost. However, research indicates that LLM-generated code may compromise security, with users often overestimating its reliability. To address these challenges, this proposal aims to enhance the quality and security of generated code in outputs. The proposal includes an empirical study of code generation models' training sets and benchmarks for code and security smells. It also consists of a framework, SALLM, to automatically benchmark code generation models from the security perspective. This proposal is a work in progress in creating quality datasets to reinforce the code generation model and generate standard and secure code. By establishing trust in LLM-based tools and generating secure and standard code, developers can confidently integrate them into their workflows and rely on them."
A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification,"Tihanyi, N; Charalambous, Y; Jain, R; Ferrag, MA; Cordeiro, LC",10.1109/AST66626.2025.00020,2025,"This paper presents a novel approach integrating Large Language Models (LLMs) with Formal Verification for automatic software vulnerability repair. Initially, we employ Bounded Model Checking (BMC) to identify vulnerabilities and extract counterexamples. Mathematical proofs and the stack trace of the vulnerabilities support these counterexamples. Using a specially designed prompt, we combine the source code with the identified vulnerability, including its stack trace and counterexample that specifies the line number and error type. This combined information is then fed into an LLM, which is instructed to attempt to fix the code. The new code is subsequently verified again using BMC to ensure the fix succeeded. We present the ESBMC-AI framework as a proof of concept, leveraging the well-recognized and industry-adopted Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained transformer model to detect and fix errors in C programs, particularly in critical software components. We evaluated our approach on 50, 000 C programs randomly selected from the FormAI dataset with their respective vulnerability classifications. Our results demonstrate ESBMC-AI's capability to automate the detection and repair of issues such as buffer overflow, arithmetic overflow, and pointer dereference failures with high accuracy. ESBMC-AI is a pioneering initiative, integrating LLMs with BMC techniques, offering potential integration into the continuous integration and deployment (CI/CD) process within the software development lifecycle."
A Catalog of Data Smells for Coding Tasks,"Vitale, A; Oliveto, R; Scalabrino, S",10.1145/3707457,2025,"Large Language Models (LLMs) are increasingly becoming fundamental in supporting software developers in coding tasks. The massive datasets used for training LLMs are often collected automatically, leading to the introduction of data smells. Previous work addressed this issue by using quality filters to handle some specific smells. Still, the literature lacks a systematic catalog of the data smells for coding tasks currently known. This article presents a Systematic Literature Review (SLR) focused on articles that introduce LLMs for coding tasks. We first extracted the quality filters adopted for training and testing such LLMs, inferred the root problem behind their adoption (data smells for coding tasks), and defined a taxonomy of such smells. Our results highlight discrepancies in the adoption of quality filters between pre-training and fine-tuning stages and across different coding tasks, shedding light on areas for improvement in LLM-based software development support."
LiCoEval: Evaluating LLMs on License Compliance in Code Generation,"Xu, WW; Gao, K; He, H; Zhou, MH",10.1109/ICSE55347.2025.00052,2025,"Recent advances in Large Language Models (LLMs) have revolutionized code generation, leading to widespread adoption of AI coding tools by developers. However, LLMs can generate license-protected code without providing the necessary license information, leading to potential intellectual property violations during software production. This paper addresses the critical, yet underexplored, issue of license compliance in LLM-generated code by establishing a benchmark to evaluate the ability of LLMs to provide accurate license information for their generated code. To establish this benchmark, we conduct an empirical study to identify a reasonable standard for striking similarity that excludes the possibility of independent creation, indicating a copy relationship between the LLM output and certain opensource code. Based on this standard, we propose LICOEVAL, to evaluate the license compliance capabilities of LLMs, i.e., the ability to provide accurate license or copyright information when they generate code with striking similarity to already existing copyrighted code. Using LICOEVAL, we evaluate 14 popular LLMs, finding that even top-performing LLMs produce a non-negligible proportion (0.88% to 2.01%) of code strikingly similar to existing open-source implementations. Notably, most LLMs fail to provide accurate license information, particularly for code under copyleft licenses. These findings underscore the urgent need to enhance LLM compliance capabilities in code generation tasks. Our study provides a foundation for future research and development to improve license compliance in AIassisted software development, contributing to both the protection of open-source software copyrights and the mitigation of legal risks for LLM users."
Software Performance Engineering for Foundation Model-Powered Software (FMware),"Hassan, AE",10.1145/3676151.3719357,2025,"This keynote examines the transformative impact of Foundation Models (FMs), particularly Large Language Models (LLMs), on software development, emphasizing the critical role of Software Performance Engineering (SPE) in ensuring FM-powered software (FMware) achieves essential performance goals such as throughput and latency. With the LLM market projected to reach $36.1 billion by 2030 [3], addressing SPE challenges has become increasingly urgent. Drawing extensively from comprehensive literature surveys, industry-academia interactions, customer feedback, and practical experience detailed in [6]. This keynote identifies four critical SPE challenges throughout the FMware lifecycle, discusses current state-of-practice solutions, proposes future research directions, and introduces a vision for an innovative SLA-aware runtime system designed to enhance the performance and efficiency of FMware."
Evaluating pre-trained Large Language Models on zero shot prompts for parallelization of source code,"Yadav, D; Mondal, S",10.1016/j.jss.2025.112543,2025,"Large Language Models (LLMs) have become prominent in the software development life cycle, yet the generation of performant source code, particularly through automatic parallelization, remains underexplored. This study compares 23 pre-trained LLMs against the Intel C Compiler (icc), a state-of-the-art auto-parallelization tool, to evaluate their effectiveness in transforming sequential C source code into parallelized versions. Using 30 kernels from the PolyBench C benchmarks, we generated 667 parallelized code versions to assess LLMs' zero-shot parallelization capabilities. Our experiments reveal that LLMs can outperform icc in non-functional aspects like speedup, with 26.66% of cases surpassing icc's performance. The best LLM-generated code achieved a 7.5x speedup compared to icc's 1.08x. However, only 90 of the 667 generated versions (13.5%) were error-free and functionally correct, underscoring significant reliability challenges. After filtering out versions with compilation errors or data race issues through detailed memory and threading analysis, notable performance gains were observed. Challenges include increased cache miss rates and branch misses with higher thread counts, indicating that simply adding threads does not ensure better performance. Optimizing memory access, managing thread interactions, and validating code correctness are critical for LLM-generated parallel code. Our findings demonstrate that, even without fine-tuning or advanced prompting techniques, pre-trained LLMs can compete with decades-old non-LLM compiler technology in zero-shot sequential-to-parallel code translation. This highlights their potential in automating code parallelization while emphasizing the need to address reliability and performance optimization challenges."
Trained without My Consent: Detecting Code Inclusion in Language Models Trained on Code,"Majdinasab, V; Nikanjam, A; Khomh, F",10.1145/3702980,2025,"Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers' codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To address this challenge, we propose a new approach, TraWiC; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an LLM's training dataset. We extract syntactic and semantic identifiers unique to each program to train a classifier for detecting code inclusion. In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an LLM. In comparison, the prevalent clone detection tool NiCad is only capable of detecting 47.64%. In addition to its remarkable performance, TraWiC has low resource overhead in contrast to pairwise clone detection that is conducted during the auditing process of tools like CodeWhisperer reference tracker, across thousands of code snippets."
Semantic API Alignment: Linking High-level User Goals to APIs,"Feldt, R; Coppola, R",10.1109/NLBSE66842.2025.00009,2025,"Large Language Models (LLMs) are becoming key in automating and assisting various software development tasks, including text-based tasks in requirements engineering but also in coding. Typically, these models are used to automate small portions of existing tasks, but we present a broader vision to span multiple steps from requirements engineering to implementation using existing libraries. This approach, which we call Semantic API Alignment (SEAL), aims to bridge the gap between a user's high-level goals and the specific functions of one or more APIs. In this work in progress paper, we propose a system architecture where a set of LLM-powered agents match such high-level objectives with appropriate API calls. This system could facilitate automated programming by finding matching links or, alternatively, explaining mismatches to guide manual intervention or further development. As an initial pilot, our paper demonstrates this concept by applying LLMs to Goal-Oriented Requirements Engineering (GORE), via sub-goal analysis, for aligning with REST API specifications, specifically through a case study involving a GitHub statistics API. We discuss the potential of our approach to enhance complex tasks in software development and requirements engineering and outline future directions for research."
Towards the Interoperability of Low-Code Platforms,"Alfonso, I; Conrardy, A; Cabot, J",10.1007/978-3-031-94590-8_1,2025,"With the promise of accelerating software development, low-code platforms (LCPs) are becoming popular across various industries. Nevertheless, there are still barriers hindering their adoption. Among them, vendor lock-in is a major concern, especially considering the lack of interoperability between these platforms. Typically, after modeling an application in one LCP, migrating to another requires starting from scratch remodeling everything (the data model, the graphical user interface, workflows, etc.) in the new platform. To overcome this situation, this work proposes an approach to improve the interoperability of LCPs by (semi)automatically migrating models specified in one platform to another one. Migration is performed via a combination of rule-based and LLM-based strategies. The pipelines are built on the BESSER framework, serving as a pivot between the LCPs."
Effective GUI Generation: Leveraging Large Language Models for Automated GUI Prototyping,"Fiebig, L; Kolthoff, K; Bartelt, C; Ponzetto, SP",,2025,"GUI prototyping is a common technique for requirements elicitation during software development and is essential in visualizing and communicating user requirements. However, creating GUI prototypes can be resource-intensive in terms of time and cost. This paper explores the innovative approach of integrating Large Language Models (LLMs) into the development of Graphical User Interface (GUI) prototypes to address this challenge. The primary objective of this work is to convert high-level text descriptions of mobile app interfaces into precise and detailed GUI prototypes. Various prompting strategies are adapted and evaluated within a structured GUI prototyping framework. The findings of this work highlight the feasibility of combining a state-of-the-art LLM with structured prompting approaches. This combination has successfully created high-quality GUI prototypes from textual descriptions, showcasing the significant potential of LLMs in the realm of GUI prototyping."
"Using Large Language Models for Aerospace Code Generation: Methods, Benchmarks, and Potential Values","He, R; Zhang, L; Lyu, MY; Lyu, LQ; Xue, CB",10.3390/aerospace12060498,2025,"In recent years, Large Language Models (LLMs) have witnessed rapid advancements, revolutionizing various domains. Within the realm of software development, code generation technology powered by LLMs has emerged as a prominent research focus. Despite its potential, the application of this technology in the aerospace sector remains in its nascent, exploratory phase. This paper delves into the intricacies of LLM-based code generation methods and explores their potential applications in aerospace contexts. It introduces RepoSpace, the pioneering warehouse-level benchmark test for code generation of spaceborne equipment. Comprising 825 samples from five actual projects, this benchmark offers a more precise evaluation of LLMs' capabilities in aerospace scenarios. Through extensive evaluations of seven state-of-the-art LLMs on RepoSpace, the study reveals that domain-specific differences significantly impact the code generation performance of LLMs. Existing LLMs exhibit subpar performance in specialized warehouse-level code generation tasks for aerospace, with their performance markedly lower than that of domain tasks. The research further demonstrates that Retrieval Augmented Generation (RAG) technology can effectively enhance LLMs' code generation capabilities. Additionally, the use of appropriate prompt templates can guide the models to achieve superior results. Moreover, high-quality documentation strings are found to be crucial in improving LLMs' performance in warehouse-level code generation tasks. This study provides a vital reference for leveraging LLMs for code generation in the aerospace field, thereby fostering technological innovation and progress in this critical domain."
Too Noisy To Learn: Enhancing Data Quality for Code Review Comment Generation,"Liu, CH; Lin, HY; Thongtanunam, P",10.1109/MSR66628.2025.00043,2025,"Code review is an important practice in software development, yet it is time-consuming and requires substantial effort. While open-source datasets have been used to train neural models for automating code review tasks, including review comment generation, these datasets contain a significant amount of noisy comments (e.g., vague or non-actionable feedback) that persist despite cleaning methods using heuristics and machine learning approaches. Such remaining noise may lead models to generate low-quality review comments, yet removing them requires a complex semantic understanding of both code changes and natural language comments. In this paper, we investigate the impact of such noise on review comment generation and propose a novel approach using large language models (LLMs) to further clean these datasets. Based on an empirical study on a large-scale code review dataset, our LLM-based approach achieves 66-85% precision in detecting valid comments. Using the predicted valid comments to fine-tune the state-of-the-art code review models (cleaned models) can generate review comments that are 13.0% - 12.4% more similar to valid human-written comments than the original models. We also find that the cleaned models can generate more informative and relevant comments than the original models. Our findings underscore the critical impact of dataset quality on the performance of review comment generation. We advocate for further research into cleaning training data to enhance the practical utility and quality of automated code review."
ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large Language Models for Code Generation,"Jiang, X; Dong, YH; Tao, YD; Liu, HY; Jin, Z; Li, G",10.1109/ICSE55347.2025.00133,2025,"Large language models (LLMs) have achieved impressive performance in code generation recently, offering programmers revolutionary assistance in software development. However, due to the auto-regressive nature of LLMs, they are susceptible to error accumulation during code generation. Once an error is produced, LLMs can merely continue to generate the subsequent code conditioned on it, given their inability to adjust previous outputs. Existing LLM-based approaches typically consider post-revising after code generation, leading to the challenging resolution of accumulated errors and the significant wastage of resources. Ideally, LLMs should rollback and resolve the occurred error in time during code generation, rather than proceed on the basis of the error and wait for post-revising after generation. In this paper, we propose ROCODE, which integrates the backtracking mechanism and program analysis into LLMs for code generation. Specifically, we employ program analysis to perform incremental error detection during the generation process. When an error is detected, the backtracking mechanism is triggered to priming rollback strategies and constraint regeneration, thereby eliminating the error early and ensuring continued generation on the correct basis. Experiments on multiple code generation benchmarks show that ROCODE can significantly reduce the errors generated by LLMs, with a compilation pass rate of 99.1%. The test pass rate is relatively improved by up to 23.8% compared to the best baseline approach. Compared to the post-revising baseline, the token cost is reduced by 19.3%. Moreover, our approach is model-agnostic and achieves consistent improvements across nine representative LLMs."
Dysregulation of lipid mediators in patients with frequent exacerbations of COPD,"Fisk, M; Gomez, EA; Sun, Y; Mickute, M; McEniery, C; Cockcroft, JR; Bolton, C; Fuld, J; Cheriyan, J; Yasmin; MacNee, W; Tal-Singer, R; Polkey, M; Wilkinson, I; Dalli, J",10.1183/23120541.00950-2023,2025,"Introduction Specialised pro-resolving mediators (SPMs) are endogenously produced lipid mediators (LMs) that regulate the propagation of inflammation and promote tissue repair. We hypothesised that SPM production is dysregulated in COPD and is associated with disease severity, defined by patients with stable COPD (no exacerbations) versus patients with frequent exacerbations. Methods LMs were measured in plasma samples from patients with COPD (stable patients and patients with frequent exacerbations) and from healthy controls, matched for age, sex and body mass index, using liquid chromatography-tandem mass spectrometry (LC-MS/MS). The LM profiles of controls were compared with those of stable COPD patients, and the LM profiles of stable COPD patients were compared with those of COPD patients with frequent exacerbations. We explored whether or not there was an association between LM profile and ever having a severe COPD exacerbation over 4.1 years of follow-up. Data are presented as mean +/- sem in pgmL(-1) for LMs, or mean +/- sd. Results 49 stable COPD patients had increased levels of pro-inflammatory mediators and some SPMs, compared with 28 controls (prostaglandin (PG)D-2: 13.97 +/- 2.44 versus 0.53 +/- 0.13; p<0.001; lipoxins: 226.83 +/- 23.84 versus 59.84 +/- 20.25; p<0.01, respectively). 52 patients with frequent exacerbations had lower levels of PGD(2) (3.07 +/- 0.97 versus 13.97 +/- 2.44; p<0.01) and SPMs (D-resolvins: 8.73 +/- 1.25 versus 34.53 +/- 8.95; p<0.01; lipoxins: 53.93 +/- 9.23 versus 226.83 +/- 23.84; p<0.01) than stable COPD patients, despite having a higher neutrophil count (5.28 +/- 2.16x10(9) L-1 versus 4.28 +/- 1.60x10(9) L-1; p=0.004). Among patients with frequent exacerbations, D-resolvin levels were independently inversely associated with occurrence of severe exacerbation (OR 0.88, 95% confidence interval (CI) 0.79-0.97; p=0.03) during follow-up. Conclusion These findings demonstrate distinct LM profiles of stable COPD patients and patients with frequent exacerbations. In those with exacerbations, D-resolvins were downregulated, compared with stable COPD patients, and associated with future risk of severe exacerbations during follow-up. Further work is needed to understand these findings."
AI-Driven Decision Support Systems in Agile Software Project Management: Enhancing Risk Mitigation and Resource Allocation,"Almalki, SS",10.3390/systems13030208,2025,"Agile software project management (ASPM) serves modern industries to conduct iterative development of complicated code bases. The decision-making process in Agile environments regularly depends on individual opinions, creating ineffective results for risk management and resource distribution. Artificial intelligence (AI) is a promising approach for handling these challenges by delivering data-based choices to project management. This research introduces an AI-based decision support system for improving risk reduction and resource distribution in ASPM. The system merges optimization frameworks and predictive analytics to enhance operational decision efficiency. The machine learning solution anchors data evaluation using AI models that simultaneously predict risks and strengthen decision power for resource scheduling. This analysis relied on project records and recent operational data to perform model validation and training procedures. Tests determined how the framework performed against contemporary Agile project management systems by measuring the completion speed of sprints, resource management practices, and risk prediction accuracy. The framework demonstrated better performance by predicting risks and simultaneously maximizing resources utilized during projects. The proposed framework outperformed traditional Agile applications, achieving 94% accuracy in risk identification and enhancing workload management by 25%, leading to an 18% improvement in sprint completion rates and overall project efficiency. These findings confirm that AI-driven decision support systems (DSSs) are crucial in enhancing Agile project management by enabling proactive risk mitigation and optimized resource allocation. By integrating AI-powered decision-making, the framework empowers organizations to improve project outcomes, streamline resource management, and facilitate the adoption of AI-driven methodologies within Agile systems."
Code Clone Detection Techniques Based on Large Language Models,"Almatrafi, AA; Eassa, FA; Sharaf, SA",10.1109/ACCESS.2025.3549780,2025,"Code duplication, commonly known as code cloning, is a persistent challenge in software development. While reusing code fragments boosts productivity, excessive cloning poses challenges to maintenance and elevates the risk of bugs. Therefore, integrating code clone detection into the development process is crucial. The extensive code-related knowledge inherent in Large Language Models (LLMs) renders them high-potential candidates for addressing diverse software engineering challenges. However, the effectiveness of LLMs in the specific task of code clone detection requires precise evaluation. This paper proposes an innovative methodology leveraging few-shot instruction-tuned GPT-3.5 Turbo and GPT-4 to detect code clones across all types, focusing on complex clones (Type-3 and Type-4). Unlike conventional approaches confined to specific language pairs or tasks, our method employs versatile language models, showcases generalization strengths for semantic understanding, and leverages instruction tuning with few-shot inference for task-specific adaptability in code clone detection. A conversational dataset was crafted from BigCloneBench for instruction tuning, enhancing task alignment and performance. This study evaluates the proficiency of LLMs in identifying code clones, analyzing the impact of instruction tuning, and assessing the efficiency across various clone types. Experimental results demonstrate these models achieving competitive performance against existing tools for overall and complex clone detection. Integration into an Integrated Development Environment (IDE) enables real-time detection and automated refactoring, bridging the gap between theoretical advancements and practical usability. This work highlights the potential of generalized LLMs setting a new standard in a field traditionally dominated by specialized tools and demonstrates their adaptability for complex challenges in code analysis and maintainability."
Boswellia serrata extract with low 3O-acetyl-11-keto-Î²-boswellic acid-content causes efficient lipid mediator class switch,"Nischang, V; Bachmann, V; Perkowski, BJ; MÃ¶bius, P; BÃ¶rner, F; Werner, M; Gomez, M; Abdel-Tawab, M; Jordan, PM; Werz, O",10.1016/j.phymed.2025.157342,2025,"Background: Extracts of the gum resin from Boswellia (B.) serrata Roxb. (Burseraceae), termed frankincense, are popular natural remedies for treatment of inflammatory disorders, and clinical trials confirm the antiinflammatory efficacy, especially in osteoarthritis. The pentacyclic triterpenes boswellic acids are exclusive to frankincense, where 3-O-acetyl-11-keto-(3-boswellic (AKBA) is considered as the main active principle. AKBA allosterically modulates lipoxygenases (LOX) in lipid mediator (LM) biosynthesis. This LOX modulation leads to suppression of pro-inflammatory leukotrienes while increasing formation of specialized pro-resolving mediators (SPM), a process called LM class switch that promotes inflammation resolution. Purpose: Here, we studied the role of AKBA in frankincense preparations for LM class-switching in human M1/M2 macrophage phenotypes by comparing the B. serrata extracts Boswellin (R) Super (BSR) and BoswelliaSan (R) (BOS), which contain 30 % and 3 % AKBA, respectively. Results: Concentration-response studies revealed only 2-fold increased effectiveness of BSR (30 % AKBA content) versus BOS (3 % AKBA content) for evoking 15-LOX product/SPM formation, and at doses corresponding to 10 mu M AKBA each, BOS was much more efficient than BSR. This implies a limited role of AKBA and indicates additional active principles in BOS, supported by the finding that BOS corresponding to only 6 mu M AKBA strongly exceeded the effects of 10 mu M AKBA. Addition of BOS to AKBA-stimulated cells concentration-dependently increased 15-LOX product formation. Finally, exogenous omega-3 substrate supply synergizes with BOS in analogy to AKBA by marked elevation of SPM/15-LOX product levels. Conclusions: B. serrata extracts promote a beneficial LM class switch in human macrophages, which might not be primarily attributed to AKBA, but to additional compounds."
Exploring the role of generative AI in enhancing cybersecurity in software development life cycle,"Al-Hashimi, HA; Khan, RA; Alwageed, HS; Algarni, AM; Ayouni, S; Almagrabi, AO",10.1016/j.array.2025.100509,2025,"Context: The rapid integration of Generative AI (GenAI) technologies in various sectors has introduced new opportunities and challenges. One of the areas where GenAI is gaining prominence is cybersecurity, particularly within the Software Development Life Cycle (SDLC). As cyber threats evolve, there is a growing need to explore innovative solutions to mitigate vulnerabilities during software development. Objectives: This study investigates the role of GenAI in enhancing cybersecurity in the SDLC. It examines current security practices, recent advancements in AI-driven security solutions, and the potential of GenAI to strengthen threat detection, vulnerability management, and risk mitigation. Additionally, the research identifies key opportunities and challenges associated with integrating GenAI into SDLC processes, highlighting its implications for secure software development and future industry practices. Methods: This research employs a mixed-methods approach to investigate the role of GenAI in cybersecurity. Specifically, it combines a Systematic Literature Review (SLR) with questionnaire-based data collection targeting software development and cyber defense experts. The SLR aims to identify prevailing themes and gaps, while the questionnaire gathers insights from IT professionals about their experiences and perspectives on GenAI systems. Results: Our research shows that GenAI technology enhances SDLC security by supporting development through vulnerability detection, threat modeling, secure coding practices, and incident response. However, our review shows that AI adoption introduces ethical risks alongside reliability issues with AI-created results and challenges to integrate it into standard development methods. Conclusion: The integration of GenAI into the SDLC offers significant potential for enhancing cybersecurity. While challenges such as algorithm transparency and the need for skilled professionals remain, the benefits of AI in proactive threat detection and response make it a promising tool for future cybersecurity strategies in software development."
Accelerating New Product Development: A Vision on Active Personas,"Simaremare, M; Edison, H",10.1007/978-3-031-85849-9_36,2025,"User participation and user feedback are essential to the success of new product development (NPD). Development teams use user feedback to derive requirement engineering artifacts, such as user scenarios, user stories, concept mindmaps, and user personas, to guide them in identifying and addressing a particular user problem. However, finding enough user participation to collect meaningful feedback is challenging, and less attention has been given to addressing this. In this paper, we propose Active Personas (APs), fictional users capable of generating contextual feedback through an interactive multi-modal interaction, such as text, voice, image, and video. APs enable development teams to gather feedback on their solutions through iterative internal experimentation. APs use generative artificial intelligence to enable dynamic multi-modal interaction and utilize user personas to generate contextual feedback. We aim to conduct a series of studies to further validate APs by applying the design science approach as guidance. We plan to develop an initial prototype of APs and conduct studies in a more controlled setting using open-source or completed projects to validate APs. Later, we will transition to ongoing projects in various types and domains."
Transforming natural language into code: advancing automated code synthesis for software development,"Al Shboul, B; Qaqour, A; Bani-Salameh, H; Almakadmeh, K",10.1007/s10586-025-05688-0,2025,"Automated code generation from natural language is a growing challenge in software engineering, enabling the direct translation of requirements into executable code. Machine learning-based automatic code synthesis emerged as a key solution, with transformer-based models playing a pivotal role in enhancing this process. This paper addresses the challenges of generating accurate, functional code efficiently. This paper presents a transformer-based T5 model fine-tuned on the multilingual XLCoST dataset for code generation. The model is evaluated using a novel systematic multi-level evaluation framework. The framework evaluates the model's across three distinct task complexity levels ranging from beginner to advanced. The evaluation results highlight the model's versatility and robustness. Furthermore, the model achieves a BLEU score of 30.093, demonstrating competitive performance compared to open-source baselines. The model's code generation abilities accelerates software development by reducing manual coding efforts, thereby improving workflow efficiency and software quality. These results confirm that transformer-based models are effective tools for automating software development, with promising implications for the future."
From Code to Concept: A Semantic Approach to AI Innovation Discovery in Open Source Software Repositories,"Novalija, I; Roman, D; Belotti, F; Alexiev, V; Rei, L; Avogadro, R; Khalilvandian, B; Bechev, B; Chinie, CA; Ciurea, I; Brank, J; Udroiu, C; Soylu, A; Palmonari, M",10.1109/ACCESS.2025.3590135,2025,"Artificial Intelligence (AI) is a transformative force driving innovation, yet tracking AI-related advancements remains challenging due to the rapid pace of development and unstructured data from platforms like GitHub. This paper proposes an AI-driven approach to innovation detection, leveraging GitHub as a data source to systematically identify and link AI projects to organizations. Key contributions include a domain-specific taxonomy comprising 7,490 AI topics, a modular pipeline for semantic annotation and entity linking, and a trend detection framework based on Singular Spectrum Analysis (SSA). A knowledge graph is constructed to represent relationships among AI topics, projects, and companies, thereby enabling structured innovation tracking. The approach addresses challenges such as data sparsity and noise, demonstrating strengths in semantic annotation and topic categorization. Results highlight the potential for accurately detecting AI innovations and linking them to organizational entities, offering valuable insights for researchers, companies, and policymakers. This work contributes a scalable, automated approach for AI innovation tracking, with future directions focusing on refining entity linking and expanding the knowledge graph to capture emerging trends."
"Making Software Development More Diverse and Inclusive: Key Themes, Challenges, and Future Directions","Hyrynsalmi, SM; Baltes, S; Brown, C; Prikladnicki, R; Rodriguez-perez, G; Serebrenik, A; Simmonds, J; Trinkenreich, B; Wang, Y; Liebel, G",10.1145/3711904,2025,"Introduction: Digital products increasingly reshape industries, influencing human behavior and decision-making. However, the software development teams developing these systems often lack diversity, which may lead to designs that overlook the needs, equal treatment or safety of diverse user groups. These risks highlight the need for fostering diversity and inclusion in software development to create safer, more equitable technology. Method: This research is based on insights from an academic meeting in June 2023 involving 23 software engineering researchers and practitioners. We used the collaborative discussion method 1-2-4-ALL as a systematic research approach and identified six themes around the theme challenges and opportunities to improve Software Developer Diversity and Inclusion (SDDI). We identified benefits, harms, and future research directions for the four main themes. Then, we discuss the remaining two themes, AI & SDDI and AI & Computer Science education, which have a cross-cutting effect on the other themes. Results: This research explores the key challenges and research opportunities for promoting SDDI, providing a roadmap to guide both researchers and practitioners. We underline that research around SDDI requires a constant focus on maximizing benefits while minimizing harms, especially to vulnerable groups. As a research community, we must strike this balance in a responsible way."
Empirical Evaluation of Prompting Strategies for Python Syntax Error Detection with LLMs,"Aloufi, N; Aljuhani, A",10.3390/app15169223,2025,"As large language models (LLMs) are increasingly integrated into software development, there is a growing need to assess how effectively they address subtle programming errors in real-world environments. Accordingly, this study investigates the effectiveness of LLMs in identifying syntax errors within large Python code repositories. Building on the bug in the code stack (BICS) benchmark, this research expands the evaluation to include additional models, such as DeepSeek and Grok, while assessing their ability to detect errors across varying code lengths and depths. Two prompting strategies-two-shot and role-based prompting-were employed to compare the performance of models including DeepSeek-Chat, DeepSeek-Reasoner, DeepSeek-Coder, and Grok-2-Latest with GPT-4o serving as the baseline. The findings indicate that the DeepSeek models generally outperformed GPT-4o in terms of accuracy (Acc). Notably, DeepSeek-Reasoner exhibited the highest overall performance, achieving an Acc of 86.6% and surpassing all other models, particularly when integrated prompting strategies were used. Nevertheless, all models demonstrated decreased Acc with increasing input length and consistently struggled with certain types of errors, such as missing quotations (MQo). This work provides insight into the current strengths and weaknesses of LLMs within real-world debugging environments, thereby informing ongoing efforts to improve automated software tools."
AndroCom: A Real-World Android Applications' Vulnerability Dataset to Assist with Automatically Detecting Vulnerabilities,"Arikan, KE; Yilmaz, EN",10.3390/app15052665,2025,"In the realm of software development, quality reigns supreme, but the ever-present danger of vulnerabilities threatens to undermine this fundamental principle. Insufficient early vulnerability identification is a key factor in releasing numerous apps with compromised security measures. The most effective solution could be using machine learning models trained on labeled datasets; however, existing datasets still struggle to meet this need fully. Our research constructs a vulnerability dataset for Android application source code, primarily based on the Common Vulnerabilities and Exposures (CVE) system, using data derived from real-world developers' vulnerability-fixing commits. This dataset was obtained by systematically searching such commits on GitHub using well-designed keywords. This study created the dataset using vulnerable code snippets from 366,231 out of 2.9 million analyzed repositories. All scripts used for data collection, processing, and refinement and the generated dataset are publicly available on GitHub. Experimental results demonstrate that fine-tuned Support Vector Machine and Logistic Regression models trained on this dataset achieve an accuracy of 98.71%, highlighting their effectiveness in vulnerability detection."
Bridging Academia and Industry: Leveraging Generative AI in a Software Engineering Course for Practical Industry Experiences,"Mejia, D; Holmes, EDV; Marroquin, J; Benario, JG",10.1145/3724363.3729036,2025,"The rapid adoption of generative AI across the tech industry demands a corresponding evolution in educational practices. By proactively incorporating generative AI, educational institutions can ensure their programs remain relevant and continue to provide students with the skills necessary for career success. This work presents an intro Software Engineering course, Software Development Studio (SDS), designed and implemented by Google in collaboration with faculty, to ensure students acquire industry-relevant skills. The course focuses on integrating generative AI tools into software engineering practices, mirroring the evolving methodologies used by professionals in the field. The curriculum emphasizes practical, real-world projects, providing early undergraduate computer science students hands-on experience using generative AI tools. Data collected during the Spring 2024 semester from students and faculty reveals a positive experience and enhancement of software engineering learning through the integration of generative AI."
Analyzing the Impact of Constant Feedback on Hybrid Agile Team Performance: Preliminary Results,"Awan, WN; Salman, I",10.1007/978-3-031-72781-8_6,2025,"This study investigates the impact of constant feedback in enhancing hybrid scrum team performance from a case study conducted with 24 undergraduate students organized in three teams. This research uses self-perceived performance surveys to identify factors that affect team performance. The objective is to enhance team performance by providing timely feedback for reflection and improvement based on identified challenges. Preliminary findings revealed that constant feedback, facilitated by self-perceived performance surveys after each sprint cycle, enables teams to address identified challenges and enhance performance progressively. This highlights the significance of timely feedback in enhancing team performance and productivity. Future work involves leveraging AI tools to analyze communication data collected throughout the study to understand well-being factors and their influence on a team's performance and productivity."
"Unlocking Potential with Generative AI Instruction: Investigating Mid-level Software Development Student Perceptions, Behavior, and Adoption","Benario, JG; Marroquin, J; Chan, MM; Holmes, EDV; Mejia, D",10.1145/3641554.3701859,2025,"Generative AI tools are rapidly evolving and impacting many domains, including programming. Computer Science (CS) instructors must address student access to these tools. While some advocate to ban the tools entirely, others suggest embracing them so that students develop the skills for utilizing the tools safely and responsibly. Studies indicate positive impacts, as well as cautions, on student outcomes when these tools are integrated into courses. We studied the impact of incorporating instruction on industry-standard generative AI tools into a mid-level software development course with students from 16 Minority Serving Institutions. 89% of student participants used generative AI tools prior to the course without any formal instruction. After formal instruction, students most frequently used generative AI tools for explaining concepts and learning new things. Students generally reported positive viewpoints on their ability to learn to program and learn problem-solving skills while using generative AI tools. Finally, we found that students: reported to understand their code when they work with generative AI tools, are critical about the outputs that generative AI tools provide, and check outputs of generative AI tools to ensure accuracy."
AI-driven cybersecurity framework for software development based on the ANN-ISM paradigm,"Khan, HU; Khan, RA; Alwageed, HS; Almagrabi, AO; Ayouni, S; Maddeh, M",10.1038/s41598-025-97204-y,2025,"With the increasing reliance on software applications, cybersecurity threats have become a critical concern for developers and organizations. The answer to this vulnerability is AI systems, which help us adapt a little better, as traditional measures in security have failed to respond to the upcoming threats. This paper presents an innovative cybersecurity framework using AI, by the Artificial Neural Network (ANN)-Interpretive Structural Modeling (ISM) model, to improve threat detection, vulnerability assessment, and risk response during software development. This framework helps realize dynamic, intelligent security as a part of the Software Development life cycle (SDLC). Initially, existing cybersecurity risks in software coding are systematically evaluated to identify potential gaps and integrate best practices into the proposed model. In the second phase, an empirical survey was conducted to identify and validate the findings of the systematic literature review (SLR). In the third phase, a hybrid approach is employed, integrating ANN for real-time threat detection and risk assessment. It utilizes ISM to analyze the relationships between cybersecurity risks and vulnerabilities, creating a structured framework for understanding interdependencies. A case study was conducted in the last stage to test and evaluate the AI-driven cybersecurity Mitigation Model for Secure Software Coding. A multi-level categorization system is also used to assess maturity across five key levels: Ad hoc, Planned, Standardized, Metrics-Driven, and Continuous Improvements. This study identifies 15 cybersecurity risks and vulnerabilities in software coding, along with 158 AI-driven best practices for mitigating these risks. It also identifies critical areas of insecure coding practices and develops a scalable model to address cybersecurity risks across different maturity levels. The results show that AI outperforms traditional systems in detecting security weaknesses and simultaneously fixing problems. During Levels 1-3 of the system improvement process, advanced security methods are used to protect against threats. Our analysis reveals that organizations at Levels 4 and 5 still need to entirely shift to using AI-based protection tools and techniques. The proposed system provides developers and managers with valuable insights, enabling them to select security enhancements tailored to their organization's development stages. It supports automated threat analysis, helping organizations stay vigilant against potential cybersecurity threats. The study introduces a novel ANN-ISM framework integrating AI tools with cybersecurity modeling formalisms. By merging AI systems with secure software coding principles, this research enhances the connection between AI-generated insights and real-world cybersecurity usage."
Design of a mobile application based on artificial intelligence to identify pain in non-communicating individuals with cerebral palsy,"Sabater-GÃ¡rriz, A; Gaya-Morey, FX; Buades-Rubio, JM; Manresa-Yee, C; Riquelme, I; Montoya, P; MartÃ­nez-Bueso, P",10.1016/j.ridd.2025.105058,2025,"Introduction: Pain assessment in individuals with cerebral palsy (CP), particularly those unable to self-report, is a significant challenge. Pain is the most common comorbidity in CP, yet current evaluation methods are often subjective and unreliable. An AI-based facial recognition system integrated into a mobile application could provide an objective, reliable tool for pain assessment in this population. Objectives: center dot Evaluate the feasibility of a mobile app using Artificial Intelligence for pain identification in non-communicative individuals with CP, developed by the research team. center dot Identify key application requirements, incorporating health app design recommendations, accessibility criteria, and universal design principles. center dot Define users' needs and functional demands to ensure effectiveness and usability. Methods: Three approaches were employed: 1. Literature review on automated facial recognition systems in CP. 2. Software development planning by a multidisciplinary team of physiotherapists and computer scientists. 3. Consultation with families, caregivers, legal guardians, healthcare professionals, and rehabilitation center managers to define functional demands, followed by collaboration with a software development company. Results: A systematic review identified seven studies on automated facial recognition systems for pain detection. However, only one of these systems-ePAT/PainCheck-has been developed into a functional mobile application for clinical use, though not specific to individuals with cerebral palsy. This underscores the novelty of the current initiative. The feasibility of our proposed app was confirmed, and key technical and functional requirements were outlined, including intuitive design, dual local/cloud processing, and mechanisms for system improvement. Stakeholders emphasized ease of use, and suggested incorporating features such as accuracy estimation, offline functionality, multi-language support, and open communication fields. Conclusions: This novel and feasible app represents a significant advance in pain assessment for and unique facial expressions."
Three-dimensional scapular kinematics during commonly used rehabilitation exercises in patients 12 weeks after reverse shoulder arthroplasty,"Vandenbosch, D; Van Tongel, A; Palmans, T; Maenhout, A; De Wilde, L; Cools, AM",10.1016/j.jse.2024.11.027,2025,"Background: Reverse shoulder arthroplasty (rTSA) is often used to restore functionality in patients with joint arthropathy and dysfunctional rotator cuff. As rTSA changes the biomechanical properties of the shoulder, an altered movement pattern of the arm and scapula is to be expected. Previous studies focused on changes of the scapulohumeral rhythm during functional elevation tasks. To our knowledge, no study exists examining scapular kinematics during rehabilitation exercises in a patient population with rTSA. Therefore, this study aimed to analyze 3-dimensional scapular kinematics during 3 commonly used rehabilitation exercises 12 weeks after rTSA surgery. It was hypothesized that scapular kinematics would be different between the horizontal and the vertical plane. Secondly, differences in scapular kinematics in the vertical plane between the open and closed chain were hypothesized. Methods: In this cross-sectional study, 48 patients (55 shoulders) aged 69 +/- 7.4 years participated. Scapular kinematics in terms of anterior/posterior tilt, external/internal rotation, and upward/downward rotation were registered during 3 rehabilitation exercises. Exercises varied based on the plane (horizontal or vertical) and the modality (closed or open chain). Data were analyzed using statistical parametric mapping (SPM). Results: Post hoc pairwise analysis within SPM (SPM(t)) revealed no significant differences among the exercises regarding posterior tilt. During vertical plane exercises (standing wall slide 120 degrees [WS] and standing reach 120 degrees [REACH]), the scapula was more upwardly rotated for most of the time (14.93%-74.63%, P < .001) than during the exercise in the horizontal plane (seated bench slide). No statistically significant differences were found between the vertical open chain (REACH) and the vertical closed chain exercise (WS). For the internal/external rotation, significant differences were found between the horizontal (seated bench slide) and the vertical plane (WS and REACH) (0%-100%, P <.001), and between the open (REACH) and closed chain (WS) vertical exercises (3.0%-91.5%, P < .001), with more external rotation when performing the movement in the vertical plane, and in the closed chain compared with the horizontal plane and the open chain, respectively. Conclusion: Scapular kinematics during rehabilitation exercises after rTSA differ, depending on the plane and modality of the exercise. Insight into the impact of changing the plane (horizontal/vertical), and training in open vs. closed chain, may assist physical therapists in the choice of exercises. (c) 2025 Journal of Shoulder and Elbow Surgery Board of Trustees. All rights are reserved, including those for text and data mining, AI training, and similar technologies."
Exploring Generative Pretrained Transformers to Support Sustainability Effect Identification - A Research Preview,"Paech, B; Kaiser, P; Bambazek, P; Groher, I; Seyff, N",10.1007/978-3-031-88531-0_16,2025,"[Context] Sustainability is increasingly recognized as a critical aspect of software development. [Problem] However, identifying the potential sustainability effects of software systems during requirements engineering remains a complex and time-consuming task. [Principal idea] To address this challenge, we explore the use of Generative Pretrained Transformers (GPTs) to automate the generation of these effects across various sustainability dimensions. In this research preview paper, we present our research goals, key research questions, initial findings and next steps. Despite several challenges identified, our tentative conclusion is that GPTs, i.e., ChatGPT, are capable of generating relevant sustainability effects. [Contributions] Our findings aim to contribute to both research and practice by fostering AI-driven approaches for integrating sustainability considerations into requirements engineering."
GRAPHiC: Utilizing Graph Structures and Class Weights in Code Comment Classification with Pretrained BERT Models,"Shah, PSU; Athar, MH; Saif, S; Tariq, MR; Afzal, AR",10.1109/NLBSE66842.2025.00014,2025,"Source code comments are an essential part of the software development process, and the classification of these comments into relevant categories is crucial for code maintenance. For this problem, we present GRAPHiC, a set of classifiers designed for multi-label classification of source code comments in Java, Python, and Pharo. As part of GRAPHiC, we train three separate classifiers on the NLBSE Code Comment Classification dataset, using GraphCodeBERT and incorporate class weights to address dataset imbalance. The classifiers achieve an average F1 score of 0.71, outperforming the SetFit baseline score of 0.63 by 12%. This paper highlights the effectiveness of GraphCodeBERT for code comment classification and explores areas for further research. The models and training code are publicly available to facilitate replication and further experimentation."
SCoPE: Evaluating LLMs for Software Vulnerability Detection,"GonÃ§alves, J; Dias, T; Maia, E; PraÃ§a, I",10.1007/978-3-031-76459-2_4,2025,"In recent years, code security has become increasingly important, especially with the rise of interconnected technologies. Detecting vulnerabilities early in the software development process has demonstrated numerous benefits. Consequently, the scientific community started using machine learning for automated detection of source code vulnerabilities. This work explores and refines the CVEFixes dataset, which is commonly used to train models for code-related tasks, specifically the C/C++ subset. To this purpose, the Source Code Processing Engine (SCoPE), a framework composed of strategized techniques that can be used to reduce the size and normalize C/C++ functions is presented. The output generated by SCoPE was used to create a new version of CVEFixes. This refined dataset was then employed in a feature representation analysis to assess the effectiveness of the tool's code processing techniques, consisting of fine-tuning three pretrained LLMs for software vulnerability detection. The results show that SCoPE successfully helped to identify 905 duplicates within the evaluated subset. The LLM results corroborate with the literature regarding their suitability for software vulnerability detection, with the best model achieving 53% F1-score."
CodeComClassify: Automating Code Comments Classification using BERT-based Language Models,"Alam, KA; Ali, W; Aziz, S; Haroon, M; Khan, MH; Ahmad, Z; Abbas, N",10.1109/NLBSE66842.2025.00017,2025,"Code comments are pivotal in enhancing code readability, maintainability, and team work in software development. As volume of code comments escalates in large software projects, it becomes impractical to manage and understand code comments, which prompts the need of exploring automated approaches for reliable handling of code comments. This paper introduces CodeComClassify, an automated code comments classification approach which is built on the Transformer-based pretrained model, distilbert-base-uncased model. CodeComClassify proficiently classifies code comments into 19 distinct categories across three languages: Java (7 categories), Python (5 categories), and Pharo (7 categories). The workflow involves cleaning and preprocessing datasets provided by NLBSE'24 Tool Competition, hyperparameter tuning, fine tuning pre-trained Transformer model, distilbert-base-uncased, and training a custom multi-label model for each language. Multi-label models for Java, Python, and Pharo are trained and evaluated on 10,555, 2,555 and 1,765 class-level code comments, respectively, extracted from 20 open source projects. The results indicate that distilbert-base-uncased model demonstrates a promising level of performance by achieving a 81% average F1-score (weighted average)."
Adapting Vision Foundation Models for Robust Cloud Segmentation in Remote Sensing Images,"Zou, XC; Zhang, S; Li, K; Wang, SY; Xing, JL; Jin, L; Lang, CY; Tao, P",10.1109/TGRS.2025.3597410,2025,"Cloud segmentation is a critical challenge in remote sensing image interpretation, as its accuracy directly impacts the effectiveness of subsequent data processing and analysis. Recently, vision foundation models (VFMs) have demonstrated powerful generalization capabilities across various visual tasks. In this article, we present a parameter-efficient adaptive approach, termed Cloud-Adapter, designed to enhance the accuracy and robustness of cloud segmentation. Our method leverages a VFM pretrained on general domain data, which remains frozen, eliminating the need for additional training. Cloud-Adapter incorporates a lightweight spatial perception module (SPM) that initially utilizes a convolutional neural network (ConvNet) to extract dense spatial representations. These multiscale features are then aggregated and serve as contextual inputs to an adapting module (AM), which modulates the frozen Transformer layers within the VFM. The experimental results demonstrate that the Cloud-Adapter approach, utilizing only 0.6% of the trainable parameters of the frozen backbone, achieves substantial performance gains. Cloud-Adapter consistently achieves state-of-the-art performance across various cloud segmentation datasets from multiple satellite sources, sensor series, data processing levels, land cover scenarios, and annotation granularities. Code and model checkpoints are available at https://xavierjiezou.github.io/Cloud-Adapter/"
Identifying Non-Functional Requirements From Unconstrained Documents Using Natural Language Processing and Machine Learning Approaches,"Shreda, QA; Hanani, AA",10.1109/ACCESS.2021.3052921,2025,"Requirements engineering is the first phase in software development life cycle and it also plays one of the most important and critical roles. Requirement document mainly contains both functional requirements and non-functional requirements. Non-functional requirements are significant to describe the properties and constraints of the system. Early identification of Non-functional requirement has direct impact on the system architecture and initial design decision. Practically, non-functional requirements are extracted manually from the document. This makes it tedious, time-consuming task and prone to various errors. In this paper, we propose an automatic approach to identify and classify non-functional requirements using semantic and syntactic analysis with machine learning approaches from unconstrained documents. We used A dataset of public requirements documents (PURE) that consists of 79 unconstrained requirements documents in different forms. In our approach, features were extracted from the requirement sentences using four different natural language processing methods including statistical and state-of-the-art semantic analysis presented by Google word2vec and bidirectional encoder representations from transformers models. The adopted approach can efficiently classify non-functional requirements with an accuracy between 84% and 87% using statistical vectorization method and 88% to 92% using word embedding semantic methods. Furthermore, by fusing different models trained on different features, the accuracy improves by 2.4% compared with the best individual classifier."
AI-Driven Innovations in Software Engineering: A Review of Current Practices and Future Directions,"Alenezi, M; Akour, M",10.3390/app15031344,2025,"The software engineering landscape is undergoing a significant transformation with the advent of artificial intelligence (AI). AI technologies are poised to redefine traditional software development practices, offering innovative solutions to long-standing challenges. This paper explores the integration of AI into software engineering processes, aiming to identify its impacts, benefits, and the challenges that accompany this paradigm shift. A comprehensive analysis of current AI applications in software engineering is conducted, supported by case studies and theoretical models. The study examines various phases of software development to assess where AI contributes most effectively. The integration of AI enhances productivity, improves code quality, and accelerates development cycles. Key areas of impact include automated code generation, intelligent debugging, predictive maintenance, and enhanced decision-making processes. AI is revolutionizing software engineering by introducing automation and intelligence into the development lifecycle. Embracing AI-driven tools and methodologies is essential for staying competitive in the evolving technological landscape."
Insights from the global education survey on the use of VR-haptics in dental education,"Bencharit, S; Quinn, B; Sittoni-Pino, MF; Arias-Herrera, S; Schick, SG; Rampf, S; Byrne, S; Shazib, MA; Ãrtengren, U; Lam, WYH; Liukkonen, M; Rice, D; Nagasawa, M; Ranauta, A; Zafar, S; BÃ¡gyi, K; Greany, TJ; Luai, AF; Oilo, M; Rederiene, G; Stolberg, R; GÃ¼l, G; Tricio, J; Chau, RCW; Pantea, M; Mutluay, M; LingstrÃ¶m, P; Klein, O; Usta, SN; Suominen, L; Felszeghy, S",10.3389/fdmed.2025.1576646,2025,"Background Haptics-enhanced virtual reality (VR-haptics), a supplementary tool for traditional oral health training, shows promise in enhancing knowledge acquisition, manual dexterity, performance, and student well-being.Aim The aim of this study was to understand dental educators' perceptions and needs regarding the acceptability and application of VR-haptics in dental education, as well as to gather suggestions for system improvements.Methods In this global cross-sectional study, the VR-Haptic Thinkers Consortium used a 28-item online questionnaire distributed to 1,023 participants by August 1, 2024. The survey included questions on general demographics, multiple choice and five-point Likert-style questions, and open-ended questions.Results A total of 378 responses were collected from 156 institutions. 57% of respondents had a dental doctorate degree and 59% had a PhD. VR-haptic trainers were used more often in preclinical training (94% of responses) than clinical training (46%). The three most common course types with VR-haptics incorporation were restorative, prosthodontic, and endodontic courses. Most respondents thought that the best approach to implementing VR-haptics is alongside phantom head training in the preclinical stage (58%). A third of the feedback on the challenges in VR-haptics utilization in dental training highlighted a need for further hardware and software development, while more than one-fourth cited economic issues in system acquisition and housing, and another one-fourth reported low acceptance of the technology among educators and students. The most mentioned enhancement requests for dental trainers were more diverse training scenarios (20%), improved software (19%) and hardware (19%) elements, and advancements in AI-based personalized training and monitoring (18%). Additionally, 10% of respondents suggested gamification features.Conclusions VR-haptic technology is constantly evolving and will likely become more and more accepted as an integral part of dental hand skill development to complement traditional preclinical training. Future research and development should emphasize transitioning from preclinical to clinical restorative, prosthodontic, endodontic, and implantology procedures as part of individualized education and patient care."
Bridging Precision and Complexity: A Novel Machine Learning Approach for Ambiguity Detection in Software Requirements,"Izhar, R; Bhatti, SN; Alharthi, SA",10.1109/ACCESS.2025.3529943,2025,"Ambiguity in software requirements is a significant challenge as it often leads to misunderstandings, implementation errors, and costly project delays. This research proposes a hybrid framework that combines rule-based techniques with machine learning to identify ambiguity in software requirements with precision and efficiency. The framework begins with a rule-based model that systematically detects ambiguities using a carefully prepared list of ambiguous phrases. The analysis utilizes a dataset of 1,553 software requirements drawn from diverse project domains. To capture more intricate ambiguities that traditional rule-based systems might miss, the framework integrates TF-IDF vectorization and a Random Forest classifier, enhancing the precision and coverage of classification. In addition, clustering analysis identifies patterns to provide deeper insights into ambiguous requirements, while sentiment analysis explores the relationship between ambiguity and the emotional tone of requirements. Together, these analyses offer a broader understanding of ambiguity trends and stakeholder perceptions. The framework's performance is validated using standard evaluation metrics, achieving an accuracy of 97%, precision of 97%, recall of 89%, and an F1-score of 92%, significantly surpassing traditional rule-based methodologies. This research advances automatic ambiguity detection by delivering a flexible and interpretable solution. The proposed approach enhances the clarity and quality of software requirements, strengthening requirements engineering practices and supporting more effective software development."
Explainable Security Requirements Classification Through Transformer Models,"Petrillo, L; Martinelli, F; Santone, A; Mercaldo, F",10.3390/fi17010015,2025,"Security and non-security requirements are two critical issues in software development. Classifying requirements is crucial as it aids in recalling security needs during the early stages of development, ultimately leading to enhanced security in the final software solution. However, it remains a challenging task to classify requirements into security and non-security categories automatically. In this work, we propose a novel method for automatically classifying software requirements using transformer models to address these challenges. In this work, we fine-tuned four pre-trained transformers using four datasets (the original one and the three augmented versions). In addition, we employ few-shot learning techniques by leveraging transfer learning models, explicitly utilizing pre-trained architectures. The study demonstrates that these models can effectively classify security requirements with reasonable accuracy, precision, recall, and F1-score, demonstrating that the fine-tuning and SetFit can help smaller models generalize, making them suitable for enhancing security processes in the Software Development Cycle. Finally, we introduced the explainability of fine-tuned models to elucidate how each model extracts and interprets critical information from input sequences through attention visualization heatmaps."
Learning With Explicit Shape Priors for Medical Image Segmentation,"You, X; He, JJ; Yang, J; Gu, Y",10.1109/TMI.2024.3469214,2025,"Medical image segmentation is a fundamental task for medical image analysis and surgical planning. In recent years, UNet-based networks have prevailed in the field of medical image segmentation. However, convolutional neural networks (CNNs) suffer from limited receptive fields, which fail to model the long-range dependency of organs or tumors. Besides, these models are heavily dependent on the training of the final segmentation head. And existing methods can not well address aforementioned limitations simultaneously. Hence, in our work, we proposed a novel shape prior module (SPM), which can explicitly introduce shape priors to promote the segmentation performance of UNet-based models. The explicit shape priors consist of global and local shape priors. The former with coarse shape representations provides networks with capabilities to model global contexts. The latter with finer shape information serves as additional guidance to relieve the heavy dependence on the learnable prototype in the segmentation head. To evaluate the effectiveness of SPM, we conduct experiments on three challenging public datasets. And our proposed model achieves state-of-the-art performance. Furthermore, SPM can serve as a plug-and-play structure into classic CNNs and Transformer-based backbones, facilitating the segmentation task on different datasets. Source codes are available at https://github.com/AlexYouXin/Explicit-Shape-Priors."
From Code Generation to Software Testing: AI Copilot With Context-Based Retrieval-Augmented Generation,"Wang, YC; Guo, SX; Tan, CW",10.1109/MS.2025.3549628,2025,"The rapid pace of large-scale software development places increasing demands on traditional testing methodologies. We propose a novel perspective on software testing, highlighting the transformative potential of AI-driven technologies in modern software development practices."
Software Engineering by and for Humans in an AI Era,"Abrahao, S; Grundy, J; PezzÃ¨, M; Storey, MA; Tamburri, DA",10.1145/3715111,2025,"The landscape of software engineering is undergoing a transformative shift driven by advancements in machine learning, Artificial Intelligence (AI), and autonomous systems. This roadmap article explores how these technologies are reshaping the field, positioning humans not only as end users but also as critical components within expansive software ecosystems. We examine the challenges and opportunities arising from this human-centered paradigm, including ethical considerations, fairness, and the intricate interplay between technical and human factors. By recognizing humans at the heart of the software lifecycle-spanning professional engineers, end users, and end user developers-we emphasize the importance of inclusivity, human-aligned workflows, and the seamless integration of AI-augmented socio-technical systems. As software systems evolve to become more intelligent and human-centric, software engineering practices must adapt to this new reality. This article provides a comprehensive examination of this transformation, outlining current trends, key challenges, and opportunities that define the emerging research and practice landscape, and envisioning a future where software engineering and AI work synergistically to place humans at the core of the ecosystem."
Explainable AI Framework for Software Defect Prediction,"GeÃ§er, BG; Tarhan, AK",10.1002/smr.70018,2025,"Software engineering plays a critical role in improving the quality of software systems, because identifying and correcting defects is one of the most expensive tasks in software development life cycle. For instance, determining whether a software product still has defects before distributing it is crucial. The customer's confidence in the software product will decline if the defects are discovered after it has been deployed. Machine learning-based techniques for predicting software defects have lately started to yield encouraging results. The software defect prediction system's prediction results are raised by machine learning models. More accurate models tend to be more complicated, which makes them harder to interpret. As the rationale behind machine learning models' decisions are obscure, it is challenging to employ them in actual production. In this study, we employ five different machine learning models which are random forest (RF), gradient boosting (GB), naive Bayes (NB), multilayer perceptron (MLP), and neural network (NN) to predict software defects and also provide an explainable artificial intelligence (XAI) framework to both locally and globally increase openness throughout the machine learning pipeline. While global explanations identify general trends and feature importance, local explanations provide insights into individual instances, and their combination allows for a holistic understanding of the model. This is accomplished through the utilization of Explainable AI algorithms, which aim to reduce the black-boxiness of ML models by explaining the reasoning behind a prediction. The explanations provide quantifiable information about the characteristics that affect defect prediction. These justifications are produced using six XAI methods, namely, SHAP, anchor, ELI5, LIME, partial dependence plot (PDP), and ProtoDash. We use the KC2 dataset to apply these methods to the software defect prediction (SDP) system, and provide and discuss the results."
SecureFalcon: Are We There Yet in Automated Software Vulnerability Detection With LLMs?,"Ferrag, MA; Battah, A; Tihanyi, N; Jain, R; Maimut, D; Alwahedi, F; Lestable, T; Thandi, NS; Mechri, A; Debbah, M; Cordeiro, LC",10.1109/TSE.2025.3548168,2025,"Software vulnerabilities can cause numerous problems, including crashes, data loss, and security breaches. These issues greatly compromise quality and can negatively impact the market adoption of software applications and systems. Traditional bug-fixing methods, such as static analysis, often produce false positives. While bounded model checking, a form of Formal Verification (FV), can provide more accurate outcomes compared to static analyzers, it demands substantial resources and significantly hinders developer productivity. Can Machine Learning (ML) achieve accuracy comparable to FV methods and be used in popular instant code completion frameworks in near real-time? In this paper, we introduce SecureFalcon, an innovative model architecture with only 121 million parameters derived from the Falcon-40B model and explicitly tailored for classifying software vulnerabilities. To achieve the best performance, we trained our model using two datasets, namely the FormAI dataset and the FalconVulnDB. The FalconVulnDB is a combination of recent public datasets, namely the SySeVR framework, Draper VDISC, Bigvul, Diversevul, SARD Juliet, and ReVeal datasets. These datasets contain the top 25 most dangerous software weaknesses, such as CWE-119, CWE-120, CWE-476, CWE-122, CWE-190, CWE-121, CWE-78, CWE-787, CWE-20, and CWE-762. SecureFalcon achieves 94% accuracy in binary classification and up to 92% in multiclassification, with instant CPU inference times. It outperforms existing models such as BERT, RoBERTa, CodeBERT, and traditional ML algorithms, promising to push the boundaries of software vulnerability detection and instant code completion frameworks."
"Comprehensive analysis of nonlinear effects in fiber optic communication systems: exploring SPM, XPM, SS, and FWM","Saiyyed, R; Sindhwani, M; Sachdeva, S; Pahuja, H; Shukla, MK",10.1007/s12596-025-02492-2,2025,"The elevated craving for exorbitant data transmission rates has conspicuously navigated noteworthy developments in fiber optic communication systems by concentrating on nonlinear optical phenomena. This research through inspection conveys an in-depth analysis of substantial nonlinear phenomena in optical fibers, including self-phase modulation (SPM), cross-phase modulation (XPM), self-steepening (SS), and four-wave mixing (FWM). The nonlinear refractive index of optical fibers engenders these phenomena, which greatly contribute to the capabilities and impediments of high-capacity Lightwave systems. While these nonlinearities may diminish signal quality via repercussions like signal distortion and noise, they can also be harnessed to elevate network efficiency through sophisticated mechanisms like dispersion control and pulse shaping. Through OptiSystem simulations, this indagation scrutinizes the ascendancy of SPM, XPM, SS, and FWM on the potential of optical communication systems, with a pivot on amplifying signal integrity and transmission efficiency. This probe examines ingenious perspectives to diminish deleterious nonlinear effects and meliorate system designs. Understanding these nonlinear processes permits the foundation of everlasting high-speed optical networks that can accommodate the proliferate exigency of current communication systems. By synthesizing insights into the behavior and management of nonlinear effects, this research succor to the inauguration of next-generation optical networks, which are vivacious for aggrandizing the inclusive efficiency and reliability of data transmission. The outcome accentuates the exigency of incorporating nonlinearities in the establishment and revamping of fiber optic systems, in due course paving the conduct for better efficient and reliable communication infrastructures. The investigation explores sophisticated mitigation strategies to ameliorate the performance and reliability of fiber optic networks. Optimized dispersion correction systems, including chirped fiber Bragg gratings, dispersion-compensating fibers, and digital signal processing (DSP)-based algorithms, recognize exact compensation for both linear and nonlinear dispersion effects. The utilization of sophisticated modulation formats such as orthogonal frequency division multiplexing, quadrature amplitude modulation, and carrier less amplitude phase modulation enhances spectral efficiency, mitigates noise, and reduces nonlinear interactions. Nonlinear compensation approaches, including optical phase conjugation, Volterra series-based DSP algorithms, and machine learning-based dynamic corrections, permit real-time reversal of nonlinear impairments. Furthermore, Raman amplification, large-core fibers, and power-level optimization ceiling nonlinear noise accretion. For solving crucial difficulties such as inter-channel crosstalk, spectrum widening, and timing jitter, solutions such as digital backpropagation, adaptive polarization control, and phase-conjugated twin waves are dispensed. Artificial intelligence (AI) based models, including neural networks, reinforcement learning, and hybrid AI-DSP frameworks, purvey predictive, adaptive management of nonlinearities in real time. Quantum technologies, utilizing quantum algorithms, variational quantum eigen solvers, and quantum machine learning, equip computational advances for effective modeling and optimization of nonlinear processes. These propounded techniques, along with hardware accelerations and scalable network optimizations, empower next-generation optical communication systems with ultra-high capacity, low BER, durable signal integrity, and considerable procurement in energy economy."
Source Code Error Understanding Using BERT for Multi-Label Classification,"Amin, MFI; Watanobe, Y; Rahman, MM; Shirafuji, A",10.1109/ACCESS.2024.3525061,2025,"Programming is an essential skill in computer science and across a wide range of engineering disciplines. However, errors, often referred to as 'bugs' in code, can be challenging to identify and rectify for both students learning to program and experienced professionals. Understanding, identifying, and effectively addressing these errors are critical aspects of programming education and software development. To aid in understanding and classifying these errors, we propose a multi-label error classification approach for source code using fine-tuned BERT models (BERT_Uncased and BERT_Cased). The models achieved average classification accuracies of 90.58% and 90.80%, exact match accuracies of 48.28% and 49.13%, and weighted F1 scores of 0.796 and 0.799, respectively. Precision, Recall, Hamming Loss, and ROC-AUC metrics further evaluate the effectiveness of our models. Additionally, we employed several combinations of large language models (CodeT5, CodeBERT) with machine learning classifiers (Decision Tree, Random Forest, Ensemble Learning, ML-KNN), demonstrating the superiority of our proposed approach. These findings highlight the potential of multi-label error classification to advance programming education, software engineering, and related research fields."
Enriching automatic test case generation by extracting relevant test inputs from bug reports,"OuÃ©draogo, WC; Plein, L; KaborÃ©, K; Habib, A; Klein, J; Lo, DV; BissyandÃ©, TF",10.1007/s10664-025-10635-z,2025,"The quality of software is closely tied to the effectiveness of the tests it undergoes. Manual test writing, though crucial for bug detection, is time-consuming, which has driven significant research into automated test case generation. However, current methods often struggle to generate relevant inputs, limiting the effectiveness of the tests produced. To address this, we introduce BRMiner, a novel approach that leverages Large Language Models (LLMs) in combination with traditional techniques to extract relevant inputs from bug reports, thereby enhancing automated test generation tools. In this study, we evaluate BRMiner using the Defects4J benchmark and test generation tools such as EvoSuite and Randoop. Our results demonstrate that BRMiner achieves a Relevant Input Rate (RIR) of 60.03% and a Relevant Input Extraction Accuracy Rate (RIEAR) of 31.71%, significantly outperforming methods that rely on LLMs alone. The integration of BRMiner's input enhances EvoSuite ability to generate more effective test, leading to increased code coverage, with gains observed in branch, instruction, method, and line coverage across multiple projects. Furthermore, BRMiner facilitated the detection of 58 unique bugs, including those that were missed by traditional baseline approaches. Overall, BRMiner's combination of LLM filtering with traditional input extraction techniques significantly improves the relevance and effectiveness of automated test generation, advancing the detection of bugs and enhancing code coverage, thereby contributing to higher-quality software development."
Ensembling Harmony Search Algorithm with case-based reasoning for software development effort estimation,"Mustyala, S; Bisi, M",10.1007/s10586-024-04858-w,2025,"Estimating software development effort is challenging in ensuring timely completion of projects and managing resources in software development companies. Inaccurate estimation of development efforts leads to significant financial losses and delays in the software project's completion. Due to dynamic requirements, technological evolution, inadequate historical data, human factors, and project complexity, the developed models cannot achieve satisfactory accuracy. Case-based reasoning (CBR) is a technique that uses data from previous projects to estimate the effort of the new project by identifying and adapting solutions that were successful in similar contexts. However, the effectiveness of CBR depends on tuning its multiple parameters, such as how past similar projects are retrieved, reused, adapted, and retained. In this paper, the Harmony Search Algorithm (HSA) is used to identify the best combination of traditional CBR parameters (feature selection, similarity measures, and the k-value count of closest neighbors, feature weighting) to accurately estimate the development effort. This paper uses the HSA to optimize the parameters for CBR, enhancing the accuracy of the estimation. The proposed CBR-HSA approach is validated using thirteen public datasets from the PROMISE repository, NASA, SEERA, and a subset of the ISBSG dataset. It is evaluated using six reliable evaluation metrics. The results obtained are promising, particularly in accuracy, statistical significance, and computational time compared to some existing models."
Evaluation of Generative AI Models in Python Code Generation: A Comparative Study,"Palla, D; Slaby, A",10.1109/ACCESS.2025.3560244,2025,"This study evaluates leading generative AI models for Python code generation. Evaluation criteria include syntax accuracy, response time, completeness, reliability, and cost. The models tested comprise OpenAI's GPT series (GPT-4 Turbo, GPT-4o, GPT-4o Mini, GPT-3.5 Turbo), Google's Gemini (1.0 Pro, 1.5 Flash, 1.5 Pro), Meta's LLaMA (3.0 8B, 3.1 8B), and Anthropic's Claude models (3.5 Sonnet, 3 Opus, 3 Sonnet, 3 Haiku). Ten coding tasks of varying complexity were tested across three iterations per model to measure performance and consistency. Claude models, especially Claude 3.5 Sonnet, achieved the highest accuracy and reliability. They outperformed all other models in both simple and complex tasks. Gemini models showed limitations in handling complex code. Cost-effective options like Claude 3 Haiku and Gemini 1.5 Flash were budget-friendly and maintained good accuracy on simpler problems. Unlike earlier single-metric studies, this work introduces a multi-dimensional evaluation framework that considers accuracy, reliability, cost, and exception handling. Future work will explore other programming languages and include metrics such as code optimization and security robustness."
Cognitive Agents Powered by Large Language Models for Agile Software Project Management,"Cinkusz, K; Chudziak, JA; Niewiadomska-Szynkiewicz, E",10.3390/electronics14010087,2025,"This paper investigates the integration of cognitive agents powered by Large Language Models (LLMs) within the Scaled Agile Framework (SAFe) to reinforce software project management. By deploying virtual agents in simulated software environments, this study explores their potential to fulfill fundamental roles in IT project development, thereby optimizing project outcomes through intelligent automation. Particular emphasis is placed on the adaptability of these agents to Agile methodologies and their transformative impact on decision-making, problem-solving, and collaboration dynamics. The research leverages the CogniSim ecosystem, a platform designed to simulate real-world software engineering challenges, such as aligning technical capabilities with business objectives, managing interdependencies, and maintaining project agility. Through iterative simulations, cognitive agents demonstrate advanced capabilities in task delegation, inter-agent communication, and project lifecycle management. By employing natural language processing to facilitate meaningful dialogues, these agents emulate human roles and improve the efficiency and precision of Agile practices. Key findings from this investigation highlight the ability of LLM-powered cognitive agents to deliver measurable improvements in various metrics, including task completion times, quality of deliverables, and communication coherence. These agents exhibit scalability and adaptability, ensuring their applicability across diverse and complex project environments. This study underscores the potential of integrating LLM-powered agents into Agile project management frameworks as a means of advancing software engineering practices. This integration not only refines the execution of project management tasks but also sets the stage for a paradigm shift in how teams collaborate and address emerging challenges. By integrating the capabilities of artificial intelligence with the principles of Agile, the CogniSim framework establishes a foundation for more intelligent, efficient, and adaptable software development methodologies."
Benchmarking large language models for automated labeling: The case of issue report classification,"Colavito, G; Lanubile, F; Novielli, N",10.1016/j.infsof.2025.107758,2025,"Context: Issue labeling is a fundamental task for software development as it is critical for the effective management of software projects. This practice involves assigning a label to issues, such as bug or feature request, denoting a task relevant to the project management. To date, large language models (LLMs) have been proposed to automate this task, including both fine-tuned BERT-like models and zero-shot GPT-like models. Objectives: In this paper, we investigate which LLMs offer the best trade-off between performance, response time, hardware requirements, and quality of the responses for issue report classification. Methods: We design and execute a comprehensive benchmark study to assess 22 generative decoder-only LLMs and 2 baseline BERT-like encoder-only models, which we evaluate on two different datasets of GitHub issues. Results: Generative LLMs demonstrate potential for zero-shot classification. However, their performance varies significantly across datasets and they require substantial computational resources for deployment. In contrast, BERT-like models show more consistent performance and lower resource requirements. Conclusions: Based on the empirical evidence provided in this study, we discuss implications for researchers and practitioners. In particular, our results suggest that fine-tuning BERT-like encoder-only models enables achieving consistent, state-of-the-art performance across datasets even in presence of a small amount of labeled data available for training."
Cypress Copilot: Development of an AI Assistant for Boosting Productivity and Transforming Web Application Testing,"Nettur, SB; Karpurapu, S; Nettur, U; Gajja, LS",10.1109/ACCESS.2024.3521407,2025,"In today's fast-paced software development environment, Agile methodologies demand rapid delivery and continuous improvement, making automated testing essential for maintaining quality and accelerating feedback loops. Our study addresses the challenges of developing and maintaining automation code for web-based application testing. In this paper, we propose a novel approach that leverages large language models (LLMs) and a novel prompt technique, few-shot chain, to automate code generation for web application testing. We chose the Behavior-Driven Development (BDD) methodology owing to its advantages and selected the Cypress tool for automating web application testing, as it is one of the most popular and rapidly growing frameworks in this domain. We comprehensively evaluated various OpenAI models, including GPT-4-Turbo, GPT-4o, and GitHub Copilot, using zero-shot and few-shot chain prompt techniques. Furthermore, we extensively validated with a vast set of test cases to identify the optimal approach. Our results indicate that the Cypress automation code generated by GPT-4o using a few-shot chained prompt approach excels in generating complete code for each test case, with fewer empty methods and improved syntactical accuracy and maintainability. Based on these findings, we developed a novel open-source Visual Studio Code (IDE) extension, Cypress Copilot utilizing GPT-4o and a few-shot chain prompt technique, which has shown promising results. Finally, we validate the Cypress Copilot tool by generating automation code for end-to-end web tests, demonstrating its effectiveness in testing various web applications and its ability to streamline development processes. More importantly, we are releasing this tool to the open-source community, as it has the potential to be a promising partner in enhancing productivity in web application automation testing."
The Impact of Generative AI on Creativity in Software Development: A Research Agenda,"Jackson, V; Vasilescu, B; Russo, D; Ralph, P; Prikladnicki, R; Izadi, M; D'Angelo, S; Inman, S; Andrade, A; van der Hoek, A",10.1145/3708523,2025,"As GenAI becomes embedded in developer toolchains and practices, and routine code is increasingly generated, human creativity will be increasingly important for generating competitive advantage. This article uses the McLuhan tetrad alongside scenarios of how GenAI may disrupt software development more broadly, to identify potential impacts GenAI may have on creativity within software development. The impacts are discussed along with a future research agenda comprising five connected themes that consider how individual capabilities, team capabilities, the product, unintended consequences, and society can be affected."
"A study on prompt design, advantages and limitations of ChatGPT for deep learning program repair","Cao, JL; Li, MZN; Wen, M; Cheung, SC",10.1007/s10515-025-00492-x,2025,"The emergence of large language models (LLMs) such as ChatGPT has revolutionized many fields. In particular, recent advances in LLMs have triggered various studies examining the use of these models for software development tasks, such as program repair, code understanding, and code generation. Prior studies have shown the capability of ChatGPT in repairing conventional programs. However, debugging deep learning (DL) programs poses unique challenges since the decision logic is not directly encoded in the source code. This requires LLMs to not only parse the source code syntactically but also understand the intention of DL programs. Therefore, ChatGPT's capability in repairing DL programs remains unknown. To fill this gap, our study aims to answer three research questions: (1) Can ChatGPT debug DL programs effectively? (2) How can ChatGPT's repair performance be improved by prompting? (3) In which way can dialogue help facilitate the repair? Our study analyzes the typical information that is useful for prompt design and suggests enhanced prompt templates that are more efficient for repairing DL programs. On top of them, we summarize the dual perspectives (i.e., advantages and disadvantages) of ChatGPT's ability, such as its handling of API misuse and recommendation, and its shortcomings in identifying default parameters. Our findings indicate that ChatGPT has the potential to repair DL programs effectively and that prompt engineering and dialogue can further improve its performance by providing more code intention. We also identified the key intentions that can enhance ChatGPT's program repairing capability."
The Future of AI-Driven Software Engineering,"Terragni, V; Vella, A; Roop, P; Blincoe, K",10.1145/3715003,2025,"A paradigm shift is underway in Software Engineering, with AI systems such as LLMs playing an increasingly important role in boosting software development productivity. This trend is anticipated to persist. In the next years, we expect a growing symbiotic partnership between human software developers and AI. The Software Engineering research community cannot afford to overlook this trend; we must address the key research challenges posed by the integration of AI into the software development process. In this article, we present our vision of the future of software development in an AI-driven world and explore the key challenges that our research community should address to realize this vision. CCS Concepts: center dot Software and its engineering -> Software testing and debugging; Designing software; Software design engineering;"
The Current Challenges of Software Engineering in the Era of Large Language Models,"Gao, CY; Hu, X; Gao, S; Xia, X; Jin, Z",10.1145/3712005,2025,"With the advent of large language models (LLMs) in the AI area, the field of software engineering (SE) has also witnessed a paradigm shift. These models, by leveraging the power of deep learning and massive amounts of data, have demonstrated an unprecedented capacity to understand, generate, and operate programming languages. They can assist developers in completing a broad spectrum of software development activities, encompassing software design, automated programming, and maintenance, which potentially reduces huge human efforts. Integrating LLMs within the SE landscape (LLM4SE) has become a burgeoning trend, necessitating exploring this emergent landscape's challenges and opportunities. The article aims at revisiting the software development lifecycle (SDLC) under LLMs, and highlighting challenges and opportunities of the new paradigm. The article first summarizes the overall process of LLM4SE, and then elaborates on the current challenges based on a through discussion. The discussion was held among more than 20 participants from academia and industry, specializing in fields such as SE and artificial intelligence. Specifically, we achieve 26 key challenges from seven aspects, including software requirement and design, coding assistance, testing code generation, code review, code maintenance, software vulnerability management, and data, training, and evaluation. We hope the achieved challenges would benefit future research in the LLM4SE field. CCS Concepts: center dot Software and its engineering -> Software development techniques;"
NoCodeGPT: A No-Code Interface for Building Web Apps With Language Models,"Monteiro, M; Branco, BC; Silvestre, S; Avelino, G; Valente, MT",10.1002/spe.3432,2025,"BackgroundLanguage models are increasingly used by software developers. However, it remains unclear whether their standard chat-based interfaces are suitable for software development-especially for users with limited programming experience.ObjectiveThis work presents a tool, called NoCodeGPT, that provides a customized interface for language models aimed at enabling the implementation of small web applications without writing code.MethodsWe first conducted an exploratory study in which three participants used ChatGPT to implement a simple web-based application. After that, we designed and implemented a customized GPT interface, called NoCodeGPT. To evaluate this new interface, we asked 14 students with limited web development experience to build two small web applications using only prompts.ResultsThe exploratory study showed that general-purpose chat interfaces like ChatGPT are not user-friendly for application development. One participant, for instance, was unable to complete any proposed user stories. In contrast, results with NoCodeGPT were encouraging: 9 out of 14 participants completed all user stories, while the remaining five completed at least half.ConclusionThe standard GPT interface is not well-suited for novice web developers. In response, we proposed, designed, and implemented a new interface that offers a more accessible experience for building web applications with language models."
SpecGen: Automated Generation of Formal Program Specifications via Large Language Models,"Ma, LZ; Liu, SQ; Li, Y; Xie, XF; Bu, L",10.1109/ICSE55347.2025.00129,2025,"In the software development process, formal program specifications play a crucial role in various stages, including requirement analysis, software testing, and verification. However, manually crafting formal program specifications is rather difficult, making the job time-consuming and labor-intensive. Moreover, it is even more challenging to write specifications that correctly and comprehensively describe the semantics of complex programs. To reduce the burden on software developers, automated specification generation methods have emerged. However, existing methods usually rely on predefined templates or grammar, making them struggle to accurately describe the behavior and functionality of complex real-world programs. To tackle this challenge, we introduce SpecGen, a novel technique for formal program specification generation based on Large Language Models (LLMs). Our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of LLMs. The process of SpecGen consists of two phases. The first phase employs a conversational approach that guides the LLM in generating appropriate specifications for a given program, aiming to utilize the ability of LLM to generate high-quality specifications. The second phase, designed for where the LLM fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. We evaluate SpecGen on two datasets, including the SV-COMP Java category benchmark and a manually constructed dataset containing 120 programs. Experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 279 out of 385 programs, outperforming the existing LLM-based approaches and conventional specification generation tools like Houdini and Daikon. Further investigations on the quality of generated specifications indicate that SpecGen can comprehensively articulate the behaviors of the input program."
Aerial-Drone-Based Tool for Assessing Flood Risk Areas Due to Woody Debris Along River Basins,"Barbero-GarcÃ­a, I; Guerrero-Sevilla, D; SÃ¡nchez-JimÃ©nez, D; MarquÃ©s-Mateu, A; GonzÃ¡lez-Aguilera, D",10.3390/drones9030191,2025,"River morphology is highly dynamic, requiring accurate datasets and models for effective management, especially in flood-prone regions. Climate change and urbanisation have intensified flooding events, increasing risks to populations and infrastructure. Woody debris, a natural element of river ecosystems, poses a dual challenge: while it provides critical habitats, it can obstruct water flow, exacerbate flooding, and threaten infrastructure. Traditional debris detection methods are time-intensive, hazardous, and limited in scope. This study introduces a novel tool integrating artificial intelligence (AI) and computer vision (CV) to detect woody debris in rivers using aerial drone imagery that is fully integrated into a geospatial Web platform (WebGIS). The tool identifies and segments debris, assigning risk levels based on obstruction severity. When using orthoimages as input data, the tool provides georeferenced locations and detailed reports to support flood mitigation and river management. The methodology encompasses drone data acquisition, photogrammetric processing, debris detection, and risk assessment, and it is validated using real-world data. The results show the tool's capacity to detect large woody debris in a fully automatic manner. This approach automates woody debris detection and risk analysis, making it easier to manage rivers and providing valuable data for assessing flood risk."
Fairness-aware practices from developers' perspective: A survey,"Voria, G; Sellitto, G; Ferrara, C; Abate, F; De Lucia, A; Ferrucci, F; Catolino, G; Palomba, F",10.1016/j.infsof.2025.107710,2025,"Context: Machine Learning (ML) technologies have shown great promise in many areas, but when used without proper oversight, they can produce biased results that discriminate against historically underrepresented groups. In recent years, the software engineering research community has contributed to addressing the need for ethical machine learning by proposing a number of fairness-aware practices, e.g., fair data balancing or testing approaches, that may support the management of fairness requirements throughout the software lifecycle. Nonetheless, the actual validity of these practices, in terms of practical application, impact, and effort, from the developers' perspective has not been investigated yet. Objective: This paper addresses this limitation, assessing the developers' perspective of a set of 28 fairness practices collected from the literature. Methods: We perform a survey study involving 155 practitioners who have been working on the development and maintenance of ML-enabled systems, analyzing the answers via statistical and clustering analysis to group fairness-aware practices based on their application frequency, impact on bias mitigation, and effort required for their application. Results: While all the practices are deemed relevant by developers, those applied at the early stages of development appear to be the most impactful. More importantly, the effort required to implement the practices is average and sometimes high, with a subsequent average application. Conclusion: The findings highlight the need for effort-aware automated approaches that ease the application of the available practices, as well as recommendation systems that may suggest when and how to apply fairness-aware practices throughout the software lifecycle."
Developer Recommendation and Team Formation in Collaborative Crowdsourcing Platforms,"Munir, Y; Umer, Q; Faheem, M; Akram, S; Jaffar, A",10.1109/ACCESS.2025.3558557,2025,"The Competitive Crowdsourcing Software Development (CSE) environment is a dynamic and evolving field that interests researchers and the software industry. This ecosystem ensures the timely delivery of cost-effective, innovative, and high-quality solutions. As this environment grows in popularity, it faces the crucial challenge of developer recommendation and team formation, which must be addressed to ensure its ongoing success and progress. This research offers a novel method for assembling teams and recommending developers. It combines keyword-based embeddings and fuzzy logic to match developers with appropriate tasks with exceptional accuracy. The technique utilizes KeyBERT to extract keywords and perform embedding to capture relevant skills and task requirements without emphasizing common words. The embeddings are optimized using a fuzzy logic framework. This framework categorizes the quality of developer-task pairings into three distinct levels: Strong, Average, and Weak. This approach enables developer recommendations and team formation, balancing precision, recall and F1-score across varying team sizes to achieve high overall accuracy by a percentage increase of 4.19%. and TF score increased by 4.05%. The proposed method consistently outperforms existing methods, allowing the formation of capable and reliable teams of varying sizes. This ensures the creation of high-performing, well-balanced teams that can effectively handle diverse tasks with the percentage improvement from Content-Based Filtering (CBF). The results in percentage increase (precision, recall and f1-score) of the proposed approach (with k=5) from CBF improve by 30.29%, 36.49%, and 33.45%, and with User-based Collaborative Filtering (UCF) and Random Recommendation (RR) performances increase by (19.58%, 15.98%, and 17.92%) and (160.17%, 215.07%, and 188.31%), respectively."
Seven recommendations for managing projectification,"Lovett, S; Abraham, T; Jung, J",10.1016/j.bushor.2023.10.001,2025,"Over the past half-century, projects and project-based work have become more and more important to organizations. This may be inevitable due to the increasing volatility, uncertainty, complexity, and ambiguity of our modern world. Many organizations routinely use project structures to develop new products, enter new markets, or upgrade IT systems, for example. A project is a set of tasks aimed at achieving a specific outcome, and a project structure is a temporary structure within which the tasks are performed. Projectification is a process of change whereby organizations increasingly adopt a project way of work. Hence, a projectified organization becomes a hybrid of temporary structures managed as projects and permanent structures managed as functions or departments. Projectification has many downsides, some of which stem from increased organizational complexity. Still, projectification fosters innovation and change, making it a necessary feature in many modern organizations. Ideally, projectification should be done gradually, and it requires significant changes throughout the organization. In this article, we present seven recommendations for senior managers in projectifying organizations. (c) 2025 Kelley School of Business, Indiana University. Published by Elsevier Inc. All rights are reserved, including those for text and data mining, AI training, and similar technologies."
An enhanced transformer-based framework for interpretable code clone detection,"Nashaat, M; Amin, R; Eid, AH; Abdel-Kader, RF",10.1016/j.jss.2025.112347,2025,"In software development, the replication of specific source code segments is known as code cloning. This practice allows reusing source code instead of developing these segments from scratch, enhancing software productivity. However, code cloning can introduce bugs, complicate code refactoring, and increase maintenance costs. Consequently, code clone detection (CCD) is an essential concern for the software industry. While various techniques have been proposed for detecting code clones, many existing tools generate a high ratio of false positives/negatives and a need for more contextual awareness. Therefore, this paper introduces CloneXformer, an innovative framework for code clone detection. The framework adopts a collaborative approach that harnesses multiple large language models for code understanding. The framework employs a preliminary phase to preprocess the input code, which helps the models understand and represent the code efficiently. Then, it captures the semantic level of the code and the syntactic level as it relies on a set of transformer-based models. Afterward, these models are finetuned to detect code clones with interpretable results that explain the detected clone types. Finally, the output of these models is combined to provide a unified final prediction. The empirical evaluation indicates that the framework improves detection performance, achieving an approximately 16.88 % higher F1 score than the state-of-the-art techniques."
Interpretability/Explainability Applied to Machine Learning Software Defect Prediction: An Industrial Perspective,"Stradowski, S; Madeyski, L",10.1109/MS.2024.3505544,2025,"We explore the interpretability and explainability of machine learning software defect prediction models in the context of commercial system-level testing in Nokia 5G. We evaluate how they can gain stakeholders' acceptance, analyze expectations, offer management strategies, and identify business impacts."
Software defect prediction based on residual/shuffle network optimized by upgraded fish migration optimization algorithm,"Liu, ZJ; Su, T; Zakharov, MA; Wei, GL; Lee, S",10.1038/s41598-025-91784-5,2025,"The study introduces a new method for predicting software defects based on Residual/Shuffle (RS) Networks and an enhanced version of Fish Migration Optimization (UFMO). The overall contribution is to improve the accuracy, and reduce the manual effort needed. The originality of this work rests in the synergic use of deep learning and metaheuristics to train the software code for extraction of semantic and structural properties. The model is tested on a variety of open-source projects, yielding an average accuracy of 93% and surpassing the performance of the state-of-the-art models. The results indicate an overall increase in the precision (78-98%), recall (71-98%), F-measure (72-96%), and Area Under the Curve (AUC) (78-99%). The proposed model is simple and efficient and proves to be effective in identifying potential defects, consequently decreasing the chance of missing these defects and improving the overall quality of the software as opposed to existing approaches. However, the analysis is limited to open-source projects and warrants further evaluation on proprietary software. The study enables a robust and efficient tool for developers. This approach can revolutionize software development practices in order to use artificial intelligence to solve difficult issues presented in software. The model offers high accuracy to reduce the software development cost, which can improve user satisfaction, and enhance the overall quality of software being developed."
Leveraging an Enhanced CodeBERT-Based Model for Multiclass Software Defect Prediction via Defect Classification,"Hussain, RG; Yow, KC; Gori, M",10.1109/ACCESS.2024.3525069,2025,"Ensuring software reliability through early-stage defect prevention and prediction is crucial, particularly as software systems become increasingly complex. Automated testing has emerged as the most practical approach to achieving bug-free and efficient code. In this context, machine learning-driven methods, especially those leveraging natural language models, have gained significant traction for developing effective techniques. This paper introduces a novel framework for automating software defect prediction, focusing on eight specific defects: SIGFPE, NZEC, LOGICAL, SYNTAX, SIGSEGV, SIGABRT, SEMANTIC, and LINKER. Our research involves a specialized dataset comprising nine classes, including eight common programming errors and one error-free class. The goal is to enhance software testing and development processes by identifying defects within code snippets. The proposed framework utilizes a CodeBERT-based algorithm for defect prediction, optimizing model hyperparameters to achieve superior accuracy. Comparative analysis against established models such as RoBERTa, Microsoft CodeBERT, and GPT-2 demonstrates that our approach yields significant improvements in prediction performance, with accuracy gains of up to 20% and 7% respectively in binary and multi class experimentation. Empirical studies validate the effectiveness of neural language models like CodeBERT for software defect prediction, highlighting substantial advancements in software testing and development techniques. These findings underscore the potential benefits of incorporating advanced machine learning models into the software development lifecycle."
Model Breadcrumbs: Scaling Multi-task Model Merging with Sparse Masks,"Davari, M; Belilovsky, E",10.1007/978-3-031-73226-3_16,2025,"The rapid development of AI systems has been greatly influenced by the emergence of foundation models. A common approach for targeted problems involves fine-tuning these pre-trained foundation models for specific target tasks, resulting in a rapid spread of models fine-tuned across a diverse array of tasks. This work focuses on the problem of merging multiple fine-tunings of the same foundation model derived from a spectrum of auxiliary tasks. We introduce a new simple method, Model Breadcrumbs, which consists of a sparsely defined weight set that guides model adaptation within the weight space of a pre-trained model. These breadcrumbs are constructed by subtracting the weights from a pre-trained model before and after fine-tuning, followed by a sparsification process that eliminates weight outliers and negligible perturbations. Our experiments demonstrate the effectiveness of Model Breadcrumbs to simultaneously improve performance across multiple tasks. This contribution aligns with the evolving paradigm of updatable machine learning, reminiscent of the collaborative principles underlying open-source software development, fostering a community-driven effort to reliably update machine learning models. Our method is shown to be more efficient and unlike previous proposals does not require hyperparameter tuning for each new task added. Through extensive experimentation involving various models, tasks, and modalities we establish that integrating Model Breadcrumbs offers a simple, efficient, and highly effective approach for constructing multi-task models and facilitating updates to foundation models.(1) (1) The code to reproduce our results is publicly available at: https://github.com/rezazzr/breadcrumbs"
"Africanus I. Scalable, distributed and efficient radio data processing with Dask-MS and Codex Africanus","Perkins, SJ; Kenyon, JS; Andati, LAL; Bester, HL; Smirnov, OM; Hugo, BV",10.1016/j.ascom.2025.100958,2025,"The physical configuration of new radio interferometers such as MeerKAT, SKA, ngVLA and DSA-2000 informs the development of software in two important areas. Firstly, tractably processing the sheer quantity of data produced by new instruments necessitates subdivision and processing on multiple nodes. Secondly, the sensitivity inherent in modern instruments due to improved engineering practices and greater data quantities necessitates the development of new techniques to capitalize on the enhanced sensitivity of modern interferometers. This produces a critical tension in radio astronomy software development: a fully optimized pipeline is desirable for producing science products in a tractable amount of time, but the design requirements for such a pipeline are unlikely to be understood upfront in the context of artefacts unveiled by greater instrument sensitivity. Therefore, new techniques must continuously be developed to address these artefacts and integrated into a full pipeline. As Knuth reminds us, Premature optimization is the root of all evil. This necessitates a fundamental trade-off between a trifecta of (1) performant code (2) flexibility and (3) ease-of-development. At one end of the spectrum, rigid design requirements are unlikely to capture the full scope of the problem, while throw-away research code is unsuitable for production use. This work proposes a framework for the development of radio astronomy techniques within the above trifecta. In doing so, we favour flexibility and ease-of-development over performance, but this does not necessarily mean that the software developed within this framework is slow. Practically this translates to using data formats and software from the Open Source Community. For example, by using NuMPy arrays and/or PANDAS dataframes, a plethora of algorithms immediately become available to the scientific developer. Focusing on performance, the breakdown of Moore's Law in the 2010s and the resultant growth of both multi-core and distributed (including cloud) computing, a fundamental shift in the writing of radio astronomy algorithms and the storage of data is required: It is necessary to shard data over multiple processors and compute nodes, and to write algorithms that operate on these shards in parallel. The growth in data volumes compounds this requirement. Given the fundamental shift in compute architecture we believe this is central to the performance of any framework going forward, and is given especial emphasis in this one. This paper describes two Python libraries, DASK-MS and CODEX AFRICANuS which enable the development of distributed High-Performance radio astronomy code with DASK. DASK is a lightweight Python parallelization and distribution framework that seamlessly integrates with the PyDATA ecosystem to address radio astronomy Big Data challenges."
Creating a standardized tool for the evaluation and comparison of artificial intelligence-based computer-aided detection programs in colonoscopy: a modified Delphi approach,"Gadi, SRV; Mori, Y; Misawa, M; East, JE; Hassan, C; Repici, A; Byrne, MF; von Renteln, D; Hewett, DG; Wang, P; Saito, Y; Matsubayashi, CO; Ahmad, OF; Sharma, P; Gross, SA; Sengupta, N; Mansour, N; Cherubini, A; Dinh, NN; Xiao, X; Mountney, P; Puyal, JGB; Little, G; Larocco, S; Conjeti, S; Seibt, H; Zur, D; Shimada, H; Berzin, TM; Brown, JRG",10.1016/j.gie.2024.11.042,2025,"Background and Aims: Multiple computer-aided detection (CADe) software programs have now achieved regulatory approval in the United States, Europe, and Asia and are being used in routine clinical practice to support colorectal cancer screening. There is uncertainty regarding how different CADe algorithms may perform. No objective methodology exists for comparing different algorithms. We aimed to identify priority scoring metrics for CADe evaluation and comparison. Methods: A modified Delphi approach was used. Twenty-five global leaders in CADe in colonoscopy, including endoscopists, researchers, and industry representatives, participated in an online survey over the course of 8 months. Participants generated 121 scoring criteria, 54 of which were deemed within the study scope and distributed for review and asynchronous e-mail-based open comment. Participants then scored criteria in order of priority on a 5-point Likert scale during ranking round 1. The top 11 highest priority criteria were re-distributed, with another opportunity for open comment, followed by a final round of priority scoring to identify the final 6 criteria. Results: Mean priority scores for the 54 criteria ranged from 2.25 to 4.38 after the first ranking round. The top 11 criteria after round 1 of ranking yielded mean priority scores ranging from 3.04 to 4.16. The final 6 highest priority criteria, including a tie for first-place ranking, were (1, tied) sensitivity (average, 4.16) and (1, tied) separate and independent validation of the CADe algorithm (average, 4.16); (3) adenoma detection rate (average, 4.08); (4) false-positive rate (average, 4.00); (5) latency (average, 3.84); and (6) adenoma miss rate (average, 3.68). Conclusions: This is the first reported international consensus statement of priority scoring metrics for CADe in colonoscopy. These scoring criteria should inform CADe software development and refinement. Future research should validate these metrics on a benchmark video dataset to develop a validated scoring instrument. (Gastrointest Endosc 2025;102:109-16.)"
Evaluating Coding Proficiency of Large Language Models: An Investigation Through Machine Learning Problems,"Ko, E; Kang, P",10.1109/ACCESS.2025.3553870,2025,"Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, but their effectiveness in coding workflows, particularly in machine learning (ML), requires deeper evaluation. This paper investigates the coding proficiency of LLMs such as GPT and Gemini by benchmarking their performance on three ML problems: Titanic, MNIST, and Steel Defect. These problems were chosen to encompass a range of challenges, including handling missing data, feature engineering, deep learning architectures, and multi-label classification. Using systematic prompts, we evaluated the LLMs' abilities in data preprocessing, hyperparameter tuning, and classifier generation, comparing their outputs with those of human developers and AutoML frameworks. Experimental results indicate that the human developer outperformed untuned LLMs in data preprocessing, maintaining a 3-5% accuracy advantage across datasets. However, GPT's hyperparameter tuning improved model performance by up to 6.3% in Titanic and 3.33% in Steel Defect, surpassing human-tuned models in some cases. In contrast, Gemini exhibited only marginal tuning improvements (0.19-1.78%) and failed to compensate for preprocessing inefficiencies. These findings show that while LLMs can assist with ML coding tasks, they exhibit varying levels of efficiency depending on task complexity and preprocessing requirements. GPT demonstrated superior hyperparameter tuning capabilities, whereas both LLMs struggled with intuitive data preprocessing, particularly in feature selection and transformation. This study provides practical insights into the strengths and limitations of LLMs in ML workflows, offering guidance for their effective integration into real-world applications."
How Developers Interact with AI: A Taxonomy of Human-AI Collaboration in Software Engineering,"Treude, C; Gerosa, MA",10.1109/Forge66646.2025.00033,2025,"Artificial intelligence (AI), including large language models and generative AI, is emerging as a significant force in software development, offering developers powerful tools that span the entire development lifecycle. Although software engineering research has extensively studied AI tools in software development, the specific types of interactions between developers and these AI-powered tools have only recently begun to receive attention. Understanding and improving these interactions has the potential to enhance productivity, trust, and efficiency in AI-driven workflows. In this paper, we propose a taxonomy of interaction types between developers and AI tools, identifying eleven distinct interaction types, such as auto-complete code suggestions, command-driven actions, and conversational assistance. Building on this taxonomy, we outline a research agenda focused on optimizing AI interactions, improving developer control, and addressing trust and usability challenges in AI-assisted development. By establishing a structured foundation for studying developer-AI interactions, this paper aims to stimulate research on creating more effective, adaptive AI tools for software development."
An NLP-Enabled Approach to Semantic Grouping for Improved Requirements Modularity and Traceability,"Izhar, R; Bhatti, SN; Alharthi, SA",,2025,"The escalating complexity of modern software systems has rendered the management of requirements increasingly arduous, often plagued by redundancy, inconsistency, and inefficiency. Traditional manual methods prove inadequate for addressing the intricacies of dynamic, large-scale datasets. In response, this research introduces SQUIRE (Semantic Quick Requirements Engineering), a cutting-edge automated framework leveraging advanced Natural Language Processing (NLP) techniques, specifically Sentence-BERT (SBERT) embeddings and hierarchical clustering, to semantically organize requirements into coherent functional clusters. SQUIRE is meticulously designed to enhance modularity, mitigate redundancy, and strengthen traceability within requirements engineering processes. Its efficacy is rigorously validated using real-world datasets from diverse domains, including attendance management, e-commerce systems, and school operations. Empirical evaluations reveal that SQUIRE outperforms conventional clustering methods, demonstrating superior intracluster cohesion and inter-cluster separation, while significantly reducing manual intervention. This research establishes SQUIRE as a scalable and domain-agnostic solution, effectively addressing the evolving complexities of contemporary software development. By streamlining requirements management and enabling software teams to focus on strategic initiatives, SQUIRE advances the state of NLP-driven methodologies in Requirements Engineering, offering a robust foundation for future innovations."
Aligning Data Debt with AI-Integrated Software Project Lifecycle Processes: A Standard-Based Mapping Approach,"AkgÃ¼l, NY; Temizel, TT; Top, ÃÃ; Akman, PD",10.1109/TechDebt66644.2025.00008,2025,"Artificial Intelligence (AI) technologies have become increasingly central to software development, enhancing efficiency with tools such as intelligent code assistants and driving innovations in products like chatbots, recommendation engines, and predictive analytics. Despite these advancements, the inherent complexity of AI-integrated software projects often leads to the accumulation of technical debt (TD), which can compromise the reliability and sustainability of systems in the long term. Managing TD effectively in these projects can be achieved by adapting international standards. Although these standards are not designed for TD management, they can be systematically applied to detect and address TD by aligning with AI system lifecycle processes. The aim of this study is to demonstrate how AI-related TD correlates with various AI lifecycle processes, thereby enabling systematic detection and management of TD in AI-integrated software projects. To achieve this, we studied 73 unique cases of TD, each reflecting either an instance or a root cause of data-related TD. These cases were subsequently mapped to the processes and activities outlined in the ISO/IEC 5338 AI Systems Lifecycle Processes standard. Subsequently, the accuracy of these mappings was validated bidirectionally by a large language model and two domain experts. Our findings revealed that data-related TD categories are associated with a diverse range of processes such as design definition, quality management and human resource management and tend to accumulate more significantly in certain areas within the AI lifecycle. This study not only serves as a proof of concept for developing a management approach for AI-related TD, but also enhances the body of knowledge on managing TD in AI projects by detailing how TD interacts with and impacts various AI lifecycle processes."
Copiloting the future: How generative AI transforms Software Engineering,"Banh, L; Holldack, F; Strobel, G",10.1016/j.infsof.2025.107751,2025,"Context: With rapid technological advancements, artificial intelligence (AI) has become integral to various sectors. Generative AI (GenAI) tools like ChatGPT or GitHub Copilot, with their unique content creation capabilities, pose transformative potential in Software Engineering by offering new ways to optimize software development processes. However, the integration into current processes also presents challenges that require a sociotechnical analysis to effectively realize GenAI's potential. Objective: This study investigates how GenAI can be leveraged in the domain of Software Engineering, exploring its action potentials and challenges to help businesses and developers optimize the adoption of this technology in their workflows. Method: We performed a qualitative study and collected data from expert interviews with eighteen professionals working in Software Engineering-related roles. Data analysis followed the principles of Grounded Theory to analyze how GenAI supports developers' goals, aligns with organizational practices, and facilitates integration into existing routines. Results: The findings demonstrate several opportunities of GenAI in Software Engineering to increase productivity in development teams. However, several key barriers were also identified, that should be accounted for in successful integrations. We synthesize the results in a grounded conceptual framework for GenAI adoption in Software Engineering. Conclusions: This study contributes to the discourse on GenAI in Software Engineering by providing a conceptual framework that aids in understanding the opportunities and challenges of GenAI. It offers practical guidelines for businesses and developers to enhance GenAI integration and lays the groundwork for future research on its impact in software development."
"Trust, transparency, and adoption in generative AI for software engineering: Insights from Twitter discourse","Basha, M; RodrÃ­guez-PÃ©rez, G",10.1016/j.infsof.2025.107804,2025,"Context: The rise of AI-driven coding assistants, such as GitHub Copilot and ChatGPT, are transforming software development practices. Despite their growing impact, informal user feedback on these tools is often neglected. Objective: This study aims to analyze Twitter/X conversations to understand user opinions on the benefits, challenges, and barriers associated with Code Generation Tools (CGTs) in software engineering. By incorporating diverse perspectives from developers, hobbyists, students, and critics, this research provides a comprehensive view of public sentiment. Methods: We employed a hybrid approach using BERTopic and open coding to collect and analyze data from approximately 90,000 tweets. The focus was on identifying themes and sentiments related to various CGTs. The study sought to determine the most frequently discussed topics and their related sentiment, followed by highlighting the reoccurring feedback or criticisms that could influence generative AI (GenAI) adoption in software engineering. Results: Our analysis identified several significant themes, including productivity enhancements, shifts in developer practices, regulatory uncertainty, and a demand for neutral GenAI content. While some users praised the efficiency benefits of CGTs, others raised concerns regarding intellectual property, transparency, and potential biases. Conclusion: The findings highlight that addressing issues of trust, accountability, and legal clarity is essential for the successful integration of CGTs in software development. These insights underscore the need for ongoing dialogue and refinement of CGTs to better align with user expectations and mitigate concerns."
State-of-the-Art and Challenges of Engineering ML-Enabled Software Systems in the Deep Learning Era,"Assres, G; Bhandari, G; Shalaginov, A; Gronli, TM; Ghinea, G",10.1145/3731597,2025,"Emerging from the software crisis of the 1960s, conventional software systems have vastly improved through Software Engineering (SE) practices. Simultaneously, Artificial Intelligence (AI) endeavors to augment or replace human decision-making. In the contemporary landscape, Machine Learning (ML), a subset of AI, leverages extensive data from diverse sources, fostering the development of ML-enabled (intelligent) software systems. While ML is increasingly utilized in conventional software development, the integration of SE practices in developing ML-enabled systems, especially across typical Software Development Life Cycle (SDLC) phases and methodologies in the post-2010 Deep Learning (DL) era, remains underexplored. Our survey of existing literature unveils insights into current practices, emphasizing the interdisciplinary collaboration challenges of developing ML-enabled software, including data quality, ethics, explainability, continuous monitoring and adaptation, and security. The study underscores the imperative for ongoing research and development with focus on data-driven hypotheses, non-functional requirements, established design principles, ML-first integration, automation, specialized testing, and use of agile methods."
Integrating ChatGPT into Software Development: Valuating Acceptance and Utilisation Among Developers,"Suryavanshi, P; Kapse, M; Sharma, V",10.14453/aabfj.v19i1.06,2025,"This study examines software developers' acceptance and utilisation of ChatGPT, analysing its potential as an AI-driven programming assistant. Using the UTAUT2 framework and judgmental sampling, data was gathered from 335 developers over six weeks, starting in April 2024. The research assesses ChatGPT's impact on developers' workflows, focusing on determinants like Performance Expectancy, Effort Expectancy, Social Influence, and Facilitating Conditions, with additional consideration for Personal Innovativeness. Structural equation modelling reveals that Facilitating Conditions and Hedonic Motivation significantly influence developers' Behavioral Intention to use ChatGPT. Findings indicate developers view ChatGPT as a tool that enhances productivity and enjoyment in coding tasks, yet concerns remain about potential dependency and the AI's reliability. Moderating effects of Gender and Experience show nuanced influences, with experienced developers more inclined toward innovation. This research provides valuable insights for optimising ChatGPT integration, underscoring the importance of supportive resources and further refinement of AI tools in development contexts."
Automated Classification and Identification of Non-Functional Requirements in Agile-Based Requirements Using Pre-Trained Language Models,"Alhaizaey, A; Al-Mashari, M",10.1109/ACCESS.2025.3570359,2025,"Non-functional requirements (NFRs) are critical factors for software quality and success. A frequently reported challenge in agile requirements engineering is that NFRs are often neglected due to the focus on functional requirements (FRs) and the limited capability of agile requirements documented as user stories to represent NFRs. With the emergence of transfer learning and large pre-trained language models, various applications in requirements engineering have become feasible, alleviating several longstanding challenges. This study evaluates transformer-based models for the automated identification and classification of NFRs. We leveraged transfer learning with pre-trained transformer models to automate the identification and classification of NFRs in agile textual requirements documented as user stories. A dataset of over 10k user stories was collected and labeled, and pre-trained transformer models, including BERT, RoBERTa, XLNet, and DistilBERT, were fine-tuned to automate the identification of NFRs. We incorporated Focal Loss during training to mitigate the dominance of functionally driven requirements and class imbalances. In addition, thorough experiments on hyperparameter optimization were employed using Bayesian hyperparameter optimization to obtain the combination of hyperparameters that best correlated with the aim of enhancing each model's performance. Our evaluation demonstrated that the finetuned pre-trained models significantly outperformed comparable prior approaches relying on rule-based techniques or traditional machine learning, with a fine-tuned BERT model achieving an F1 Score of 93.4 %. These findings highlight the potential of pre-trained language models in agile requirements engineering, enabling more efficient NFRs identification, reducing manual review burden, and facilitating a viable and efficient approach to address the neglect of NFRs in agile development processes."
Automated Testing for Service-Oriented Architecture: Leveraging Large Language Models for Enhanced Service Composition,"Altin, M; Mutlu, B; Kilinc, D; Cakir, A",10.1109/ACCESS.2025.3571994,2025,"This article explores the application of Large Language Models (LLMs), including proprietary models such as OpenAI's ChatGPT 4o and ChatGPT 4o-mini, Anthropic's Claude 3.5 Sonnet and Claude 3.7 Sonnet, and Google's Gemini 1.5 Pro, Gemini 2.0 Flash, and Gemini 2.0 Flash-Lite, as well as open-source alternatives including Qwen2.5-14B-Instruct-1M, and commercially accessed models such as DeepSeek R1 and DeepSeek V3, which were tested via APIs despite having open-source variants, to automate validation and verification in Application Programming Interface (API) testing within a Service-Oriented Architecture (SOA). Our system compares internal responses from the Enuygun Web Server against third-party API outputs in both JSON and XML formats, validating critical parameters such as flight prices, baggage allowances, and seat availability. We generated 100 diverse test scenarios across varying complexities (1-4 flight results) by randomly altering request and response parameters. Experimental results show that Google Gemini 2.0 Flash achieved high accuracy (up to 99.98%) with the lowest completion time (85.34 seconds), while Qwen2.5-14B-Instruct-1M exhibited limited capability in processing complex formats. Models such as OpenAI's ChatGPT and Anthropic's Claude Sonnet models also demonstrated strong performance in single-flight validation scenarios, making them suitable for low-latency, high-precision tasks. Our findings indicate that some open-source models can offer promising cost-effective alternatives, though performance significantly varies. This integration of LLMs reduced manual workload, improved test scalability, and enabled real-time validation across large-scale datasets. As LLM technologies mature, we anticipate further advances in automation, accuracy, and efficiency in software validation systems."
Explainable AI for trustworthy intelligent process monitoring,"Johannssen, A; Qiu, PH; Yeganeh, A; Chukhrova, N",10.1016/j.cie.2025.111407,2025,"Statistical control charts are often based on assumptions that do not hold in complex, high-dimensional and dynamic environments. To counter these weaknesses, control charts based on artificial intelligence (AI) techniques have emerged as a powerful alternative in recent years. However, their black-box nature limits transparency, interpretability and trustworthiness that are essential to realize Industry 5.0. To address that issue, this Short Communication discusses the necessity of embedding explainable artificial intelligence (XAI) in AI-based control charts. Incorporating XAI provides a solution by enhancing the interpretability of AI-based control charts while maintaining their high predictive accuracy. This paper also identifies key challenges in embedding XAI and outlines future research directions for responsible and trustworthy AI-based process monitoring."
The Role of Generative AI in Software Student CollaborAItion,"Kiesler, N; Smith, J; Leinonen, J; Fox, A; MacNeil, S; Ihantola, P",10.1145/3724363.3729040,2025,"Collaboration is a crucial part of computing education. The increase in AI capabilities over the last couple of years is bound to profoundly affect all aspects of systems and software engineering, including collaboration. In this position paper, we consider a scenario where AI agents would be able to take on any role in collaborative processes in computing education. We outline these roles, the activities and group dynamics that software development currently include, and discuss if and in what way AI could facilitate these roles and activities. The goal of our work is to envision and critically examine potential futures. We present scenarios suggesting how AI can be integrated into existing collaborations. These are contrasted by design fictions that help demonstrate the new possibilities and challenges for computing education in the AI era."
Managing expectations towards AI tools for software development: a multiple-case study,"Jensen, VV; Alami, A; Bruun, AR; Persson, JS",10.1007/s10257-025-00704-7,2025,"Software development organizations (SDOs) are increasingly working to adopt artificial intelligence (AI) tools, like GitHub Copilot, to meet varied expectations. Nevertheless, we know little about how SDOs manage these expectations. This paper investigates how different SDOs expect AI tools to impact software development, and how these expectations change after a period of considering and evaluating AI tools. We conducted a multiple-case study involving three SDOs. To elicit initial expectations towards AI tools, we collected data using semi-structured interviews and field visits. To assess the persistence of expectations towards AI tools, we collected data from meetings, a debriefing, and retrospectives on AI tools. We found three expectations particular to one SDO; four shared between two SDOs; and six pervasive across all SDOs. Five expectations did not persist after experiential learning with AI tools, due to platform- and SDO-related factors. SDOs must carefully manage their expectations towards AI tools due to the variety and complexity of expectations. Some expectations are niche-specific based on their compatibility with the unique SDOs' people- and structure-related aspects, while others are becoming mainstream for a broader array of SDOs. Recognizing factors that affect the persistence of expectations and how they manifest in the individual SDO will enable SDOs to form their initial expectations and understand how these might change during adoption of AI tools, supporting expectation management."
Generative Artificial Intelligence for Software Engineering-A Research Agenda,"Nguyen-Duc, A; Cabrero-Daniel, B; Przybylek, A; Arora, C; Khanna, D; Herda, T; Rafiq, U; Melegati, J; Guerra, E; Kemell, KK; Saari, M; Zhang, ZY; Le, HY; Quan, T; Abrahamsson, P",10.1002/spe.70005,2025,"ContextGenerative artificial intelligence (GenAI) tools have become increasingly prevalent in software development, offering assistance to various managerial and technical project activities. Notable examples of these tools include OpenAI's ChatGPT, GitHub Copilot, and Amazon CodeWhisperer.ObjectiveAlthough many recent publications have explored and evaluated the application of GenAI, a comprehensive understanding of the current development, applications, limitations, and open challenges remains unclear to many. Particularly, we do not have an overall picture of the current state of GenAI technology in practical software engineering usage scenarios.MethodWe conducted a literature review and focus groups for a duration of five months to develop a research agenda on GenAI for software engineering.ResultsWe identified 78 open research questions (RQs) in 11 areas of software engineering. Our results show that it is possible to explore the adoption of GenAI in partial automation and support decision-making in all software development activities. While the current literature is skewed toward software implementation, quality assurance and software maintenance, other areas, such as requirements engineering, software design, and software engineering education, would need further research attention. Common considerations when implementing GenAI include industry-level assessment, dependability and accuracy, data accessibility, transparency, and sustainability aspects associated with the technology.ConclusionsGenAI is bringing significant changes to the field of software engineering. Nevertheless, the state of research on the topic still remains immature. We believe that this research agenda holds significance and practical value for informing both researchers and practitioners about current applications and guiding future research."
Large language models and the problem of rhetorical debt,"Macarthur, M",10.1007/s00146-025-02403-w,2025,"This article offers broadly useful guidance for society's adaptation to the omnipresence of generative AI, with implications for every profession and academic discipline that involves writing or coding (recognized by some as a form of writing). Offering an interdisciplinary perspective grounded in the digital humanities, software development and writing across the curriculum, and building on performance historian Christopher Grobe's research on the role of arts and humanities expertise in AI development, I offer redefinitions of training data and prompt engineering. These essential yet misleading terms obscure the critical roles that humanities-based expertise has played in the development of GPTs and must play in guiding society's adaptation to generative AI. I also briefly review scholarship on what constitutes writing and what it means to teach writing. Next, I reflect on long-terms trends, in professional software development, of code sharing and reliance on automation, and the likely impact of imposing similar practices in professional writing. After identifying the fundamental problem of rhetorical debt and outlining its consequences, I further motivate my argument, in relation to the new economic value of expert writing. This new economic value necessitates a revaluation of the humanities-not only by computer science, the tech industry, and schools and universities, but by humanists themselves."
PragFormer: Data-Driven Parallel Source Code Classification with Transformers,"Harel, R; Kadosh, T; Hasabnis, N; Mattson, T; Pinter, Y; Oren, G",10.1007/s10766-024-00778-9,2025,"Multi-core shared memory architectures have become ubiquitous in computing hardware nowadays. As a result, there is a growing need to fully utilize these architectures by introducing appropriate parallelization schemes, such as OpenMP worksharing-loop constructs, to applications. However, most developers find introducing OpenMP directives to their code hard due to pervasive pitfalls in managing parallel shared memory. To assist developers in this process, many compilers, as well as source-to-source (S2S) translation tools, have been developed over the years, tasked with inserting OpenMP directives into code automatically. In addition to having limited robustness to their input format, these compilers still do not achieve satisfactory coverage and precision in locating parallelizable code and generating appropriate directives. Recently, many data-driven AI-based code completion (CC) tools, such as GitHub CoPilot, have been developed to ease and improve programming productivity. Leveraging the insights from existing AI-based programming-assistance tools, this work presents a novel AI model that can serve as a parallel-programming assistant. Specifically, our model, named PragFormer, is tasked with identifying for loops that can benefit from conversion to parallel worksharing-loop construct (OpenMP directive) and even predict the need for specific data-sharing attributes clauses on the fly. We created a unique database, named Open-OMP, specifically for this goal. Open-OMP contains over 32,000 unique code snippets from different domains, half of which contain OpenMP directives, while the other half do not. We experimented with different model design parameters for these tasks and showed that our best-performing model outperforms a statistically-trained baseline as well as a state-of-the-art S2S compiler. In fact, it even outperforms the popular generative AI model of ChatGPT. In the spirit of advancing research on this topic, we have already released source code for PragFormer as well as Open-OMP dataset to public. Moreover, an interactive demo of our tool, as well as a Hugging Face webpage to experiment with our tool, are already available."
The use of predetermined change control plans to enable the release of new versions of software as a medical device,"Dupreez, JA; Mcdermott, O",10.1080/17434440.2025.2468787,2025,"ObjectivesThis study investigates how Predetermined Change Control Plans (PCCPs) can support the Software Development Life Cycle (SDLC) of certain Software as a Medical Device (SaMD).MethodsTargeted surveys collected qualitative and quantitative data on the current regulatory change process for SaMD; the use of PCCPs; the potential parameters of PCCPs in terms of technical, clinical, usability, and administrative changes to SaMD; and whether PCCPs could be used more broadly for all SaMDs.ResultsResults indicate that the current regulatory approach is not fit for purpose, specifically regarding fast-moving SaMD or continuous-learning AI SaMD. There was strong support for PCCPs to cover device technology, usability, and administrative changes, while clinical changes had limited support and required further investigation. The EU lags behind the US and now the UK in addressing these challenges and should look to legislate and implement PCCPs to ensure ongoing innovation and investment in digital health technologies.ConclusionThis work is novel in the gathering of meaningful input from experts, practitioners, and regulatory professionals within the SaMD industry located in the EU, UK, and US on the value and need for PCCPs. This study has implications for practice and policy as it can inform SaMD guidance and legislation."
AN INNOVATIVE SOFTWARE DEVELOPMENT METHODOLOGY FOR DEEP LEARNING-DRIVEN VISUAL COMPUTING IN BUILT ENVIRONMENT APPLICATIONS,"Perera, P; Perera, S; Jin, XH; Rashidi, M; Nanayakkara, S; Yazbek, G; Yazbek, A",10.36680/j.itcon.2025.041,2025,"This paper presents an innovative software development methodology, the GENESIS (Generalised ENgineering for Embedded Software with Integrated AI System) Methodology, tailored for Deep Learning (DL)-driven visual computing applications in the built environment. IntegratingAI into embedded systems has presented unique challenges to the associated software development methodologies. The proposed GENESIS Methodology integrates Design Science Research principles with established Artificial Intelligence (AI) embedded software-specific software engineering practices. Further, the approach has co-opted and synthesised insights from recent studies on AI software development and software engineering methodologies, incorporating key elements. The GENESIS Methodology encompasses twelve key stages, from problem definition to monitoring and maintenance for the developed software systems, with the sharing of knowledge, focusing on data-centric development and model-driven AI approaches. The systematic integration of AI-specific software engineering stages within conventional software engineering methodology uniquely combines a research-driven approach. The emphasis on the importance of Convolutional Neural Networks (CNNs) for visual computing tasks details the technical considerations for training and evaluating Deep Learning models. The paper justifies adopting the Waterfall model for its structured approach, aligning with the needs of the technically complex systems. Finally, a software prototype development is presented using the proposed GENESIS Methodology, and the functionality is focused on the built environment, validated by achieving a 91.2% accuracy in identifying six types of concrete defects, demonstrating the accuracy of this approach in real-world applications. This comprehensive methodology aims to enhance the development ofAI-based visual computing applications in the built environment, offering a systematic framework."
Evaluating user story quality with LLMs: a comparative study,"Sharma, A; Tripathi, AK",10.1007/s10844-025-00939-3,2025,"Evaluating the quality of user stories is crucial for the success of agile software development. This paper investigates the efficacy of Large Language Models (LLMs) in assessing the quality of individual user stories using the Quality User Story (QUS) framework, which categorizes quality criteria into syntactic, semantic, and pragmatic dimensions. Leveraging three state-of-the-art LLMs-GPT-4o, GPT-4-Turbo, and GPT-3.5-Turbo-this study employs two prompting strategies: context minimal and context rich, to gauge performance across eight user story quality criteria. To ensure robust validation, we generated 960 user stories using alternative LLMs (Gemini and Meta AI's LLaMA3), which were then assessed for quality by 69 postgraduate students. The quality assessments were further verified by a team comprising a research scholar and a senior postgraduate student. The evaluation of these 960 user stories by the three LLMs under study reveal significant insights into their relative strengths and weaknesses. The results demonstrate that GPT-4o and GPT-4-Turbo exhibit superior performance in evaluating user stories, particularly excelling in syntactic and pragmatic criteria with minimal impact from additional contextual details. Conversely, GPT-3.5-Turbo reveals noticeable limitations, struggling to maintain effectiveness, particularly when handling richer contextual inputs. This research marks a pivotal step towards automated quality assessment in requirements engineering, highlighting both the potential and areas for improvement in leveraging LLMs for robust user story evaluation."
Improving Software Development Traceability with Structured Prompting,"Kim, DK",10.1080/08874417.2025.2470919,2025,"Generative large language models (LLMs) are increasingly integrated into software development to enhance efficiency across various phases, from requirements analysis to testing. However, the design of effective prompts for leveraging these AI capabilities in software development remains underexplored. This study aims to bridge this gap by developing prompt designs that enhance the traceability of software artifacts created by AI models. Employing design principles adapted from social sciences, the study introduces structured prompting that significantly improves traceability measures compared to unstructured prompting. Structured prompts were notably effective in the Design Modeling and Testing phases, showing improvements of 32.21 and 26.35 respectively with an overall average improvement of 23.92. Despite these advances, challenges persist, such as ensuring AI models adhere strictly to structured prompts, which can sometimes lead to outputs being overly specific or not fully compliant with the prompt instructions."
ROS package search for robot software development: a knowledge graph-based approach,"Wang, S; Mao, XJ; Yang, S; Wu, MH; Zhang, Z",10.1007/s11704-024-3660-9,2025,"In recent years, ROS (Robot Operating System) packages have become increasingly popular as a type of software artifact that can be effectively reused in robotic software development. Indeed, finding suitable ROS packages that closely match the software's functional requirements from the vast number of available packages is a nontrivial task using current search methods. The traditional search methods for ROS packages often involve inputting keywords related to robotic tasks into general-purpose search engines (e.g., Google) or code hosting platforms (e.g., GitHub) to obtain approximate results of all potentially suitable ROS packages. However, the accuracy of these search methods remains relatively low because the task-related keywords may not precisely match the functionalities offered by the ROS packages. To improve the search accuracy of ROS packages, this paper presents a novel semantic-based search approach that relies on the semantic-level ROS Package Knowledge Graph (RPKG) to automatically retrieve the most suitable ROS packages. Firstly, to construct the RPKG, we employ multi-dimensional feature extraction techniques to extract semantic concepts, including code file name, category, hardware device, characteristics, and function, from the dataset of ROS package text descriptions. The semantic features extracted from this process result in a substantial number of entities (32,294) and relationships (54,698). Subsequently, we create a robot domain-specific small corpus and further fine-tune a pre-trained language model, BERT-ROS, to generate embeddings that effectively represent the semantics of the extracted features. These embeddings play a crucial role in facilitating semantic-level understanding and comparisons during the ROS package search process within the RPKG. Secondly, we introduce a novel semantic matching-based search algorithm that incorporates the weighted similarities of multiple features from user search queries, which searches out more accurate ROS packages than the traditional keyword search method. To validate the enhanced accuracy of ROS package searching, we conduct comparative case studies between our semantic-based search approach and four baseline search approaches: ROS Index, GitHub, Google, and ChatGPT. The experiment results demonstrate that our approach achieves higher accuracy in terms of ROS package searching, outperforming the other approaches by at least 21% from 5 levels, including top1, top5, top10, top15, and top20."
Requirements Are All You Need: The Final Frontier for End-User Software Engineering,"Robinson, D; Cabrera, C; Gordon, AD; Lawrence, ND; Mennen, L",10.1145/3708524,2025,"What if end-users could own the software development lifecycle from conception to deployment using only requirements expressed in language, images, video or audio? We explore this idea, building on the capabilities that Generative AI brings to software generation and maintenance techniques. How could designing software in this way better serve end-users? What are the implications of this process for the future of end-user software engineering and the software development lifecycle? We discuss the research needed to bridge the gap between where we are today and these imagined systems of the future."
Assessment of the Impact of Solar Power Integration and AI Technologies on Sustainable Local Development: A Case Study from Serbia,"Benovic, A; Miskic, M; Pantovic, V; Vujicic, S; Vidojevic, D; Opacic, M; Jovanovic, F",10.3390/su17156977,2025,"As the global energy transition accelerates, the integration of solar power and artificial intelligence (AI) technologies offers new pathways for sustainable local development. This study examines four Serbian municipalities-& Scaron;abac, Sombor, Pirot, and & Ccaron;a & ccaron;ak-to assess how AI-enabled solar power systems can enhance energy resilience, reduce emissions, and support community-level sustainability goals. Using a mixed-method approach combining spatial analysis, predictive modeling, and stakeholder interviews, this research study evaluates the performance and institutional readiness of local governments in terms of implementing intelligent solar infrastructure. Key AI applications included solar potential mapping, demand-side management, and predictive maintenance of photovoltaic (PV) systems. Quantitative results show an improvement >60% in forecasting accuracy, a 64% reduction in system downtime, and a 9.7% increase in energy cost savings. These technical gains were accompanied by positive trends in SDG-aligned indicators, such as improved electricity access and local job creation in the green economy. Despite challenges related to data infrastructure, regulatory gaps, and limited AI literacy, this study finds that institutional coordination and leadership commitment are decisive for successful implementation. The proposed AI-Solar Integration for Local Sustainability (AISILS) framework offers a replicable model for emerging economies. Policy recommendations include investing in foundational digital infrastructure, promoting low-code AI platforms, and aligning AI-solar projects with SDG targets to attract EU and national funding. This study contributes new empirical evidence on the digital-renewable energy nexus in Southeast Europe and underscores the strategic role of AI in accelerating inclusive, data-driven energy transitions at the municipal level."
Evidence-Based Software Engineering Guidelines Revisited,"Pfleeger, SL; Kitchenham, BA",10.1109/TSE.2025.3526730,2025,"In 2002, the authors and their colleagues proposed some preliminary guidelines for empirical software engineering research. In this paper, we revisit them. We believe that for the purpose of supporting the development of project-based bespoke software, they still perform reasonably well. However, the worlds of software and software engineering have changed dramatically in the last 25 years. We suggest that new guidelines are needed to respond to changes in software practices, including the possibility of regulation of AI-based software products, and propose the scope of such guidelines."
Development of AI-Enhanced Smile Design Software for Ultra-Customized Aesthetic Outcomes,"Mohsin, L; Alenezi, N; Rashdan, Y; Hassan, A; Alenezi, M; Alam, MK; Noor, NFB; Akhter, F",10.4103/jpbs.jpbs_88_25,2025,"Background:Advancements in artificial intelligence (AI) have paved the way for ultra-customized aesthetic solutions in dentistry, particularly in smile design. Conventional smile design methods often fall short in providing a fully personalized outcome, necessitating the development of AI-enhanced software to optimize results by considering facial features, dental parameters, and patient preferences.Materials and Methods:A prototype AI-enhanced smile design software was developed using a combination of convolutional neural networks for facial analysis and generative adversarial networks for creating customized smile designs. The study involved 50 participants, each undergoing facial feature scanning, digital dental impressions, and patient-specific aesthetic input collection. The software's performance was evaluated based on user satisfaction, aesthetic quality, and procedural efficiency. A comparison was made with conventional smile design methods to assess improvements in outcomes.Results:The AI-enhanced software demonstrated significant improvements in aesthetic outcomes and efficiency. The mean patient satisfaction score was 9.2/10 compared to 7.5/10 with conventional methods. Aesthetic quality was rated higher by experts (mean score: 8.8/10 vs. 7.3/10), and the time required for smile design reduced by 40%. The integration of AI allowed for more precise customization, aligning with patient preferences and anatomical considerations.Conclusion:The development of AI-enhanced smile design software represents a significant step toward achieving ultra-customized aesthetic outcomes in dentistry. By integrating advanced facial analysis and design algorithms, the software offers a superior alternative to conventional methods, promising enhanced satisfaction, efficiency, and aesthetic precision."
Development of Artificial Intelligence-based Real-time Automatic Fusion of Multiparametric Magnetic Resonance Imaging and Transrectal Ultrasonography of the Prostate,"Cianflone, F; Maris, B; Bertolo, R; Veccia, A; Artoni, F; Pettenuzzo, G; Montanaro, F; Porcaro, AB; Bianchi, A; Malandra, S; Ditonno, F; Cerruto, MA; Zamboni, G; Fiorini, P; Antonelli, A",10.1016/j.urology.2025.03.004,2025,"OBJECTIVE To report the development of artificial intelligence (AI)-based software to allow for the autonomous fusion of transrectal ultrasound and multiparametric magnetic resonance images of the prostate to be used during transperineal prostate biopsies. MATERIALS AND METHODS This study was approved by the Institutional Review Board (protocol ID 3167CESC). The automatic software development for fusion biopsy involved 3 steps: (1) developing an AI component to segment the prostate during ultrasound; (2) developing the component to segment anatomical structures in magnetic resonance images using labeled datasets from the Cancer Imaging Archive and in-house scans; (3) developing the fusion component to register segmented ultrasound and magnetic resonance images using a 3-step process: pre-alignment, rigid alignment, and elastic fusion, validated by measuring the lesion distance between modalities. Statistical analysis included descriptive statistics and the Mann-Whitney U test, evaluating outcomes with Dice scores and average precision metrics. RESULTS The ultrasound component showed a Dice score of 0.87 with a test set of 75,357 images and 28,946 annotations. The magnetic resonance component achieved a Dice score of 0.85 on a test set of 2494 images and annotations. It also demonstrated a mean average precision of 0.80 for bounding boxes and 0.88 for segmented objects, both measured at a 50% intersection over union threshold. The fusion AI component reduced the median magnetic resonance-ultrasound lesion distance from 8 mm (inter-quartile ranges 6-9) after rigid fusion to 4 mm (interquartile ranges 3-5) after elastic fusion (P <.001). CONCLUSION A data processing pipeline and AI were created to allow for the autonomous fusion of ultrasound and magnetic resonance images to be ideally used during transperineal prostate biopsies."
Bots and insights: Combining perspectives of analytics and software development in systems analysis and design projects,"Sharma, M; Mchaney, R",10.1111/dsji.70005,2025,"Many management information systems (MIS) faculty have adopted a project-oriented approach in their systems analysis and design courses. In these courses, students use a software development methodology to create a web or mobile application project, which can be based on a predefined case or developed for an external stakeholder. Because most information systems programs emphasize cybersecurity, analytics, and artificial intelligence (AI), traditional systems analysis and design courses for web applications may seem one-dimensional in comparison. To address this limitation, we developed and implemented a project based on Merrill's Pebble-in-the-Pond instructional design. In this project, students were required to build an application with two key components: a chatbot and a dashboard, both integrated with the same database. These components cater to multiple user groups, allowing us to combine perspectives from analytics and AI within a single project. Drawing from our experience and feedback from students, we have compiled a set of recommendations for successfully implementing such a project."
Automated Microservices Identification Through Business Process Analysis: A Semantic-Driven Clustering Approach,"Oumoussa, I; Saidi, R",10.1109/ACCESS.2025.3571809,2025,"The rise of microservice architectures has further evolved software development practices, building upon the foundation established by web services in breaking down monolithic systems, while offering more finely detailed, coherent, and loosely interconnected services. Despite the transformative potential of microservices, the challenge of effectively pinpointing microservices that meet an organization's specific requirements remains a difficult task. This paper presents an extended approach for automating microservices identification by leveraging business processes (BPs). Addressing limitations in existing microservices identification techniques, this approach utilizes advanced Natural Language Processing (NLP) techniques-such as Named Entity Recognition (NER) and semantic analysis-to capture dependencies and align boundaries with business logic. This methodology applies unsupervised clustering algorithms, including K-means, K-medoids, and DBSCAN, to generate well-defined microservices by analyzing semantic similarity across BP activities. Evaluation on real-world case studies, such as the Bicing bicycle rental system, Cargo Tracking, and JPetStore, demonstrates its effectiveness, yielding high cohesion, low coupling, and superior granularity in microservices decomposition compared to traditional methods. Metrics such as the Silhouette Index, Afferent Coupling, and Instability further validate its performance. By automating microservices identification with a BP-centered framework, this approach aligns technical architecture with organizational goals, advancing both agility and scalability in software systems. This work marks a step forward in optimizing system modularity and provides a foundation for future scalability improvements."
Fixer-level supervised contrastive learning for bug assignment,"Wang, RC; Ji, XY; Tian, Y; Xu, SL; Sun, XB; Jiang, SJ",10.1007/s10664-025-10634-0,2025,"Deep neural network (DNN)-based approaches have rapidly gained prominence as a leading method in automating bug assignment, a task that is both time-intensive and critical for effective bug triage in software development. However, DNNs have been shown to be vulnerable - subtle perturbations to their inputs can lead these models to generate unpredictable and erroneous outputs. To mitigate this issue, contrastive learning (CL) has been increasingly adopted. CL is designed to learn discriminative representations by contrasting similar and dissimilar data points. Although CL has demonstrated its effectiveness in domains such as computer vision (CV) and natural language processing (NLP), its application in automating bug assignments remains unexplored. In this paper, we propose a fixer-level supervised contrastive learning method specifically tailored for automated bug assignment, aiming to enhance the robustness and effectiveness of DNN-based bug assignment approaches. Our approach calculates a similarity score between two bug fixers by assessing the semantic similarity of their historically resolved bug reports. By contrasting bug reports resolved by similar fixers (positive examples) against those addressed by different fixers (negative examples) within the same batch, this approach aims to learn robust bug report representations for bug assignment. We conducted an empirical study to evaluate the effectiveness of our approach against two widely used strategies for improving the robustness of neural networks. This evaluation also includes a baseline scenario where no specific strategy is applied, i.e., the original network. For this comparative analysis, we utilized three distinct neural network architectures, i.e., Bidirectional Long Short-Term Memory(Bi-LSTM) using Embeddings from Language Models (ELMo), Bi-LSTM with attention using ELMo, and Bidirectional Encoder Representations from Transformer (BERT), as well as three widely used datasets and a combined dataset composed of the above three datasets. We also investigated the impact of varying important hyperparameters on the considered approaches. Our experimental results show that the proposed approach has varying degrees of improvement over the original networks and those with baselines in terms of top-k (k=1, 5, 10) accuracy and Mean Reciprocal Rank (MRR), ranging from 0.83% to 11.14%. Moreover, all three studied neural networks with the proposed approach have lower degradations than the original networks and those with baselines against the adversarial examples generated by the Projected Gradient Descent (PGD) algorithm. This indicates that our approach can better enhance the robustness of all the networks studied than the baselines. Furthermore, the proposed approach achieves better results in small-size learning settings (5%, 10%, and 15% of the labelled bug reports of each dataset were used for model training)."
Cross-Level Requirements Tracing Based on Large Language Models,"Ge, CY; Wang, TT; Yang, XT; Treude, C",10.1109/TSE.2025.3572094,2025,"Cross-level requirements traceability, linking high-level requirements (HLRs) and low-level requirements (LLRs), is essential for maintaining relationships and consistency in software development. However, the manual creation of requirements links necessitates a profound understanding of the project and entails a complex and laborious process. Existing machine learning and deep learning methods often fail to fully understand semantic information, leading to low accuracy and unstable performance. This paper presents the first approach for cross-level requirements tracing based on large language models (LLMs) and introduces a data augmentation strategy (such as synonym replacement, machine translation, and noise introduction) to enhance model robustness. We compare three fine-tuning strategies-LoRA, P-Tuning, and Prompt-Tuning-on different scales of LLaMA models (1.1B, 7B, and 13B). The fine-tuned LLMs exhibit superior performance across various datasets, including six single-project datasets, three cross-project datasets within the same domain, and one cross-domain dataset. Experimental results show that fine-tuned LLMs outperform traditional information retrieval, machine learning, and deep learning methods on various datasets. Furthermore, we compare the performance of GPT and DeepSeek LLMs under different prompt templates, revealing their high sensitivity to prompt design and relatively poor result stability. Our approach achieves superior performance, outperforming GPT-4o and DeepSeek-r1 by 16.27% and 16.8% in F-measure on cross-domain datasets. Compared to the baseline method that relies on prompt engineering, it achieves a maximum improvement of 13.8%."
Navigating regulatory challenges across the life cycle of a SaMD,"Francesconi, M; Cangi, M; Tamarri, S; Conditi, N; Menicucci, C; Ravizza, A; Cattaneo, L; Bianchini, E",10.1016/j.jbi.2025.104856,2025,"Objective: Software as medical devices (SaMDs) have become part of clinical practice and the management of the development and control processes of the documentation associated with them are an integral part of many medical realities. The European Regulation, MDR (EU) 2017/745, introduces a classification rule (rule 11, Annex VIII) specifically for software, which provides more explicit requirements than in the past, leading to classification of many software to higher risk and therefore to more complex certification processes. In this context, planning and awareness of possible regulatory strategies and related standards are fundamental for the key stakeholders, but this complex landscape can be perceived as fragmented. The aim of this work is to provide an amalgamated overview of how the current EU normative framework integrates into the various phases of the life-cycle of a medical device software, trying to ensure its safe and effective development. Methods: In addition to the MDR, the main normative references relevant to the medical device software sector were taken into consideration. Specifically, the IEC 62304 standard clarifies the main processes of the software life-cycle, including the analysis of problems and changes, and the IEC 82304 standard completes its management by addressing activities relating to post-market phases and requirements. In addition, the various steps include also key points such as risk identification and control (ISO 14971), design, implementation and validation of usability requirements (IEC 62366) and in general the quality of the context in which the software is developed and maintained (ISO 13485). The application of these standards can support the activities of the various stakeholders and facilitate evidence of compliance with the regulatory requirements by MDR. Results: Based on the software life cycle, a mapping of the requirements from the entire normative framework analyzed over the various phases was implemented. Conclusions: A detailed and integrated picture of the regulatory context behind the life cycle of a SaMD has been provided: this can facilitate the implementation of a balanced and effective approach, including key aspects, such as risk management and usability processes, and ensuring safety for the end user."
Abstracting general syntax for XAI after decomposing explanation sub-components,"Wormald, S; Maldaner, MK; O'Connor, KD; Dizon-Paradis, OP; Woodard, DL",10.1007/s10462-025-11216-8,2025,"Healthcare providers, policymakers, and defense contractors need to understand many types of machine learning model behaviors. While eXplainable Artificial Intelligence (XAI) provides tools for interpreting these behaviors, few frameworks, surveys, and taxonomies produce succinct yet general notation to help researchers and practitioners describe their explainability needs and quantify whether these needs are met. Such quantified comparisons could help individuals rank XAI methods by their relevance to use-cases, select explanations best suited for individual users, and evaluate what explanations are most useful for describing model behaviors. This paper collects, decomposes, and abstracts subcomponents of common XAI methods to identify a mathematically grounded syntax that applies generally to describing modern and future explanation types while remaining useful for discovering novel XAI methods. The resulting syntax, introduced as the Qi-Framework, generally defines explanation types in terms of the information being explained, their utility to inspectors, and the methods and information used to produce explanations. Just as programming languages define syntax to structure, simplify, and standardize software development, so too the Qi-Framework acts as a common language to help researchers and practitioners select, compare, and discover XAI methods. Derivative works may extend and implement the Qi-Framework to develop a more rigorous science for interpretable machine learning and inspire collaborative competition across XAI research."
A data-driven API recommendation approach for service mashup composition,"Alam, KA; Haroon, M; Ain, Q; Inayat, I",10.1007/s13198-024-02568-5,2025,"The increasing availability of Web APIs has brought about a revolution in software development. Developers can now create innovative web applications by combining existing services. However, with so many APIs available, it can be challenging to identify the most suitable ones for a particular task. Many existing recommendation systems rely on keyword matching and historical data, which can limit their effectiveness when dealing with complex functional requirements and new mashup creation scenarios. This paper presents a new method for recommending web APIs to developers for mashup composition. Our goal is to improve the accuracy of recommendations, particularly when developers lack domain knowledge or encounter ambiguous functional descriptions. To achieve this, we propose a solution driven by natural text descriptions, which utilizes advanced techniques such as semantic enrichment and deep learning. The approach to recommendation methods combines content-based and quality-of-service (QoS) techniques with the advanced capabilities of BERT (Bidirectional et al. from Transformers) and Graph Generative Adversarial Networks (Graph GAN). BERT's contextual understanding of text allows us to capture more comprehensive functional descriptions, overcoming the limitations of traditional keyword matching. Meanwhile, Graph GAN helps us learn from existing mashup-service invocation records, leading to more accurate and relevant service recommendations. Our framework consists of a robust data and semantic enrichment component that employs paraphrase mining to extend the vocabulary and enhance semantic similarity measures. As a result, our recommendation system can handle various natural language queries and identify subtle contextual nuances in service descriptions."
Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice,"Kallina, E; BohnÃ©, T; Singh, J",10.1145/3715275.3732069,2025,"Responsible AI (rAI) guidance increasingly promotes stakeholder involvement (SHI) during AI development. At the same time, SHI is already common in commercial software development, but with potentially different foci. This study clarifies the extent to which established SHI practices are able to contribute to rAI efforts as well as potential disconnects - essential insights to inform and tailor future interventions that further shift industry practice towards rAI efforts. First, we analysed 56 rAI guidance documents to identify why SHI is recommended (i.e. its expected benefits for rAI) and uncovered goals such as redistributing power, improving socio-technical understandings, anticipating risks, and enhancing public oversight. To understand why and how SHI is currently practised in commercial settings, we then conducted an online survey (n=130) and semi-structured interviews (n=10) with AI practitioners. Our findings reveal that SHI in practice is primarily driven by commercial priorities (e.g. customer value, compliance) and several factors currently discourage more rAI-aligned SHI practices. This suggests that established SHI practices are largely not contributing to rAI efforts. Towards addressing this disconnect, we propose interventions and research opportunities to advance SHI for rAI development in real-world practice."
Digital transformation and the accounting for intangible assets in the public sector,"Heiling, J",10.1108/JPBAFM-09-2024-0177,2025,"Purpose This paper aims to examine the accounting and financial reporting treatment of intangible assets under International Public Sector Accounting Standards (IPSAS) in light of the digital transformation that takes place in the public sector.Design/methodology/approach This paper presents an empirical analysis of financial reporting practices for intangible assets, focusing on selected public sector entities.Findings The paper identifies challenges in the accounting for agile software development, for cloud computing as well as artificial intelligence (AI) systems and assesses whether the principles in IPSAS 31, Intangible Assets are sufficient to address the identified challenges. The paper concludes with a discussion of the findings and provides some recommendations not only for the IPSAS Board (IPSASB) but also for other accounting standard setters.Research limitations/implications Due to the limited number of entities subject to the empirical part of the paper, the study's findings are subject to limitations regarding generalizability. This paper addresses only a subset of the current accounting issues related to the digital transformation of public sector entities.Practical implications The study enhances the understanding of financial reporting for intangible assets by public sector entities in the digital age and offers insights that are of interest to accounting standard setters both in the public and the private sector.Originality/value The digital transformation of public sector entities presents significant challenges for the accounting profession. A key contribution of this paper is to highlight some of the reporting and accounting issues currently under discussion, offering insights that may guide accounting standard setters in identifying areas where future guidance is needed."
ViT-BF: vision transformer with border-aware features for visual tracking,"Yang, K; Zhang, WH; Li, P; Liang, JX; Peng, T; Chen, J; Li, L; Hu, XR; Liu, JP",10.1007/s00371-025-03964-z,2025,"Existing object trackers commonly approach the tracking process by utilizing classification and regression techniques. However, they often encounter difficulties in managing complex scenarios such as occlusions and appearance changes. Moreover, the quality of candidate boxes is a critical factor for affecting the tracking performance. To overcome these challenges, this study introduces a border-aware tracking framework based on a vision transformer (ViT), termed ViT-BF. Through the integration of a boundary alignment operation, ViT-BF extracts boundary features from the extremal points of objects, thereby enhancing classification and regression precision. To handle the dynamic appearance variations of objects, ViT-BF integrates a template update mechanism through a score prediction module (SPM), which enhances the tracker's robustness and accuracy. Experimental results reveal that ViT-BF demonstrates excellent performance across multiple benchmarks, including an AUC score of 85.0% on TrackingNet, 89.4% in normalized precision, and 84.4% in precision. The promising results extend to other standard datasets such as LaSOT, GOT-10k, and UAV123, validating our method's strong stability and adaptability in diverse tracking scenarios."
Multilabel classification for defect prediction in software engineering,"Pachouly, J; Ahirrao, S; Kotecha, K; Kulkarni, A; Alfarhood, S",10.1038/s41598-025-93242-8,2025,"With advancements in software development and artificial intelligence, defect prediction has gradually become an essential component of the software development lifecycle. Historically, defect prediction has been considered a multiclass classification problem because defect classes are mutually exclusive. However, software defects can belong to multiple categories simultaneously, making multilabel classification a more appropriate approach. A defect report typically contains the title, body, comments from developers and testers, and code snippets. We used these data items and performed data wrangling on these data to create a holistic summary of the defect report, which contains all the vital information that is useful for defect predictions. In this study, we investigated the multilabel dimension of the defect and performed multilabel classification using machine learning and deep learning techniques while considering the class imbalance and correlations between the labels. In the traditional classification methods, we used three classifiers: Multinomial Naive Bayes, Logistic Regression, and Random Forest. Multilayer Perceptron (MLP) and a Convolutional Neural Network (CNN) with Classifier Chains are used in deep learning. To check the dataset quality, appropriate feature selection, and data dimensionality reduction, we used the chi-square test. To handle the class imbalance, we used Non-Negative Least Squares (NNLS). Our experimental investigations showed significant improvements in the model performance across machine learning and deep learning once the dataset was balanced before training the models. Visual plots of evaluation metrics, such as Hamming loss, Recall, Precision, and F1-score, clearly demonstrated the analysis outcome."
"Automatic Program Assessment, Grading and Code Generation: Possible AI-Support in a Software Development Course","Borghoff, UM; Minas, M; MÃ¶ench, K",10.1007/978-3-031-81596-6_4,2025,"Computer science and related courses of study require a sound education in software engineering. We have designed and continuously developed a software engineering project course to serve these topics. The COVID-19 pandemic was a turning point in many ways, including for our course and the way we taught during and after that time. We describe how we adapted the course to these new pandemic conditions. In particular, we introduced an automated program assessment system that helped us keep the difficulty of all assignments constant, even during hybrid and online-only classes for the different student teams, while keeping the motivation of individual students in each team high. We started with a binary (pass/fail) grading system, but we saw that a fine-grained individual grading is worth investigating and might lead to higher acceptance and willingness to participate among students. Both automatic assessment and individual grading, and increasingly code generation, are a case for AI. We use the state of the art to show where AI can be applied to our specific programming tasks and where its limitations lie."
Human Digital Twins: Enhancing Interactions With Digital Ecosystems,"Laso, S; Herrera, JL; GalÃ¡n-JimÃ©nez, J; Berrocal, J",10.1109/MIC.2024.3509672,2025,"The emergence of digital twins (DTs) has transformed domains like Industry 4.0 or automotive, enabling advanced insights and predictive maintenance as well as driving efficiency and innovation. With the evolution toward human-centric domains, such as Industry 5.0 or intelligent vehicles, the need for human DTs (HDTs)-human digital representations with the aim to align human interactions with the system's design and its performance-arises. HDTs, linked to human interactions in various contexts, seek to improve well-being, safety, and productivity by integrating human factors into digital systems. However, they face unique considerations and challenges compared to traditional DTs. We provide an overview of distributed multicontext HDTs, highlighting their utility in various domains, and propose an architecture designed to address key challenges in their development and deployment, including data management, privacy, or interoperability. Additionally, we show the development of HDTs in practical and functional environments through a set of artifacts."
Open Source AI-based SE Tools: Opportunities and Challenges of Collaborative Software Learning,"Lin, ZH; Ma, W; Lin, T; Zheng, YW; Ge, JQ; Wang, J; Klein, J; Bissyande, T; Liu, Y; Li, L",10.1145/3708529,2025,"Large language models (LLMs) have become instrumental in advancing software engineering (SE) tasks, showcasing their efficacy in code understanding and beyond. AI code models have demonstrated their value not only in code generation but also in defect detection, enhancing security measures and improving overall software quality. They are emerging as crucial tools for both software development and maintaining software quality. Like traditional SE tools, open source collaboration is key in realizing the excellent products. However, with AI models, the essential need is in data. The collaboration of these AI-based SE models hinges on maximizing the sources of high-quality data. However, data, especially of high quality, often hold commercial or sensitive value, making them less accessible for open source AI-based SE projects. This reality presents a significant barrier to the development and enhancement of AI-based SE tools within the SE community. Therefore, researchers need to find solutions for enabling open source AI-based SE models to tap into resources by different organizations. Addressing this challenge, our position article investigates one solution to facilitate access to diverse organizational resources for open source AI models, ensuring that privacy and commercial sensitivities are respected. We introduce a governance framework centered on federated learning (FL), designed to foster the joint development and maintenance of open source AI code models while safeguarding data privacy and security. Additionally, we present guidelines for developers on AI-based SE tool collaboration, covering data requirements, model architecture, updating strategies, and version control. Given the significant influence of data characteristics on FL, our research examines the effect of code data heterogeneity on FL performance. We consider six different scenarios of data distributions and include four code models. We also include four most common FL algorithms. Our experimental findings highlight the potential for employing FL in the collaborative development and maintenance of AI-based SE models. We also discuss the key issues to be addressed in the co-construction process and future research directions."
Leveraging Cross-Project Similarity for Data Augmentation and Security Bug Report Prediction,"Ji, JF; Yang, G",10.1109/ACCESS.2025.3564818,2025,"Accurately identifying security bug reports remains a key challenge in software development. Due to the varying expertise of bug reporters, many security bug reports are incorrectly labeled as non-security bug reports, this increases the security risk of the software and the workload of developers to identify these incorrectly labeled reports from bug reports. This study aims to improve the prediction of security bug reports by addressing the class imbalance problem and enhancing the generalization ability of the model across projects. To achieve this goal, we propose a deep learning-based prediction method combined with a novel data augmentation method based on cross-project text similarity. The bug report data is collected from four open-source projects: Ambari, Camel, Derby, and Wicket, where the number of security bug reports is 56, 74, 179, and 47, respectively, and the number of non-security bug reports is significantly higher. To alleviate the imbalance phenomenon and leverage cross-project knowledge, we augment the dataset by identifying and merging semantically similar security bug reports from other projects. We evaluate 5 deep learning models, including CNN, LSTM, GRU, Transformer, and BERT. Our approach achieved F1 scores between 0.60 and 0.98, with the best performance using LSTM and GRU models, especially LSTM on Ambari, GRU on Camel and Ambari, they both achieved an F1 score of 0.98. The overall average F1 score is 0.77, a significant improvement over the baseline classification. The results show that data augmentation based on cross-project similarities is an effective strategy to improve security bug report prediction, especially in imbalanced datasets. This approach can help developers detect security-related issues more effectively, reduce the risk of misclassification, and enhance overall software security."
Cross Domain Few-Shot Line-Level Defect Prediction in Open Software Development via Meta Learning,"Li, ZD; Guo, SK; Zhang, LH; Li, H; Yang, ZG; Ning, Q; Ma, Q",10.1109/TCE.2025.3572334,2025,"Defect prediction tools are designed to assist practitioners in effectively prioritizing the risk files that are most likely to manifest software defects after release, with limited software quality assurance resources. It plays a vital role in code inspection activities. Currently, numerous automated defect prediction methods have been proposed to enhance the efficiency of code review and software quality. However, most previous methods often rely on a substantial number of labeled samples for training. Additionally, when confronted with tasks involving a broad domain span, these methods frequently struggle to effectively identify defective files and lines. To address this issue, we propose a model named DEF-Hunter for rapidly adapting to new file-level and line-level defect predictions when encountering new technical domains. DEF-Hunter consists of two components: defect identification component and transfer learning component. The defect identification component employs a hierarchical attention network to learn the hierarchical structure of the source code, capturing the surrounding code tokens and neighboring lines. It utilizes attention mechanisms to calculate risk scores for code tokens contributing to the prediction of defective files. The transfer learning component employs meta-learning to enhance the model's cross-domain generalization ability, achieving few-shot line-level defect prediction. The experimental results demonstrate that DEF-Hunter outperforms the state-of-the-art methods in both performance, cost-effectiveness and across metrics such as Balance Accuracy, AUC, MCC, and Recall@Top20%LOC, e.g., compared to state-of-the-art methods, DEF-Hunter improves the Balance Accuracy score by 29%, 29%, 17.49%, 7.32%, and 2.38%, respectively."
GENOME SEQUENCE ANALYSIS OF SEVERE ACUTE RESPIRATORY SYNDROME USING GENOANALYTICA MODEL,"Dubey, S; Verma, DK; Kumar, M",10.12694/scpe.v26i1.3533,2025,"We proposed a GenoAnalytica model for examining the SARS's genomics sequences. The technologies make proper data extraction from genomics sequences of viruses. We use the GenoAnalytica model, i.e. GenoCompute, and IGMiner Algorithm; to classify the range of genomics sequences, including recognizing the sequence variation from the datasets. The projected algorithm computes the nucleotide patterns and represents the nucleotide genome sequence of SARS (airborne virus) by IGMiner technique and works out on the GenoCompute to calculate computation time with minimum count in second. Along with this, we proposed a UMRA algorithm to compute the mutation rate of the genome sequence with minimum count in seconds as compared to traditional method. Furthermore, we work out the different datasets (China and Algeria datasets) and determine the whole variation at the index level inside the all genome sequence. This learning also signifies the performance evaluation on altering minsup using IGMiner and Aprori-based SPM. Also, we calculate the mutation rate of the genome sequence of airborne virus using Unique Mutation Rate Analysis algorithm. The severe acute respiratory syndrome coronavirus 2 has been responsible for the deadly COVID-19 pandemic. It has ruined limitless individuals all over the globe, and along with this, it continues to harm well-being and people's health. Healthcare specialists and Researchers can obtain insight into COVID-19's inherited variation or SAR-CoV-2 through cutting-edge Artificial Intelligence and genome sequence analysis tools."
Novel Optimization Approach and AI Performance Prediction of a Highly Sensitive V-Shaped PCF-SPR Sensor for Cancer Cell Detection,"Khemiri, R; Kaziz, S",10.1007/s11220-025-00615-4,2025,"For cancer cell detection, we present in this paper a novel optimized photonic crystal fiber (PCF)-based surface plasmon resonance (SPR) biosensor. The sensor features a V-shaped groove, which enhances sensitivity by improving electromagnetic coupling between the surface plasmon mode (SPM) and the core mode at the metal/dielectric interface. Key design parameters, including the diameter of the small air hole (d2), V-channel depth (h), and gold layer thickness (tg), are optimized through the Box-Behnken Design (BBD) method, reaching a maximum spectral sensitivity of 2142.86 nm/RIU for blood and breast cancer cells, an amplitude sensitivity of 632.50 RIU-1 for skin cancer cells, and a resolution of 4.66 x 10- 5 RIU. Furthermore, we employed an artificial neural network (ANN) model based on a Multi-Layer Perceptron (MLP) to accurately predict sensor performance, demonstrating the potential of machine learning in optimizing biosensor applications for biochemical and biological detection."
Challenges and Opportunities of the Human-Centered Design Approach: Case Study Development of an Assistive Device for the Navigation of Persons With Visual Impairment,"Chavarria, MA; Ortiz-Escobar, LM; Bacca-Cortes, B; Romero-Cano, V; Villota, I; PeÃ±a, JKM; SÃ¡nchez, JRL; Campo, O; Suter, S; Cabrera-LÃ³pez, JJ; PatiÃ±o, MFS; Caicedo-Bravo, EF; Stein, M; Hurst, S; SchÃ¶nenberger, K; Velarde, MR",10.2196/70694,2025,"Background: Visual impairment (VI) significantly impacts quality of life, particularly in autonomous pedestrian navigation. Limitations in independent navigation lead to frustration, diminished confidence, and risks to bodily integrity for individuals with VI. In Colombia, the pilot country of this study, approximately 2 million people live with some form of visual disability. Globally, only 1 in 10 people requiring assistive devices have access to them, with factors such as deficient product design stemming from limited knowledge of user expectations, local needs, and environmental constraints, posing significant challenges, particularly in low-and middle-income countries. Objective: We aimed to evaluate thefeasibility and limitations of applying the human-centered design (HCD) principles outlined by the International Organization for Standardization (ISO) 9241-210:2019 standard in assistivetechnology (AT) development forindividualswithVI in Colombia. Methods: We developed a prototype navigation device using the HCD principles, emphasizing a thorough analysis of user needs and environmental contexts. The project leveraged multidisciplinary collaboration to address challenges associated with user engagement and design adaptability while managing legal and bureaucratic constraints. The navigation system integrates artificial intelligence algorithms, specifically developed by the research team as part of this work, to enhance its adaptability and responsiveness to diverse environments. The development process featured iterative prototyping cycles, incorporating user feedback at each stage, all within the boundaries of applicable regulatory frameworks. Results: The development and evaluation of the initial prototype highlighted both the feasibility and key limitations of applying the ISO 9241-210:2019 HCD principles in AT for individuals with VI in the Colombian context. The prototype met several user-defined expectations by prioritizing affordability; extended battery life; autonomy in internet-constrained environments; and improved ergonomics, concealability, aesthetics, and obstacle detection. These achievements demonstrated the potential of HCD to guide context-sensitive innovation. However, the process also revealed significant barriers: limited legal and procedural clarity for engaging users in design phases, difficulties navigating ethics committees, and a lack of practical guidance within the ISO standard itself. These constraints, compounded by challenges in interdisciplinary collaboration, limited the depth and adaptability of user involvement across development stages. Conclusions: Implementing HCD principles in AT development shows promise for creating effective and affordable solutions tailored to user needs and contexts. However, legislative and methodological barriers must be addressed to fully realize HCD's potential. Future efforts should focus on aligning research methodologies with hardware and software development practices while integrating legislative frameworks to enhance the accessibility and effectiveness of AT innovations."
Improvement of FDG PET/CT and MRI concordance in temporal lobe epilepsy pre-surgical assessment using statistical parametric mapping Z-scores,"Kershaw, M; Li, X; Amada, H; Lu, Y; Sawlani, J; Bose, S; Sawlani, V; Hughes, S",10.1016/j.crad.2025.106838,2025,"AIM: This retrospective study evaluates the diagnostic performance of statistical parametric mapping (SPM) analysis of interictal F18-fluoro-deoxy-D-glucose positron emission tomography computed tomography (FDG PET/CT) in temporal lobe epilepsy (TLE) patients, aiming to enhance image reporting consistency and correlation between magnetic resonance imaging (MRI) and FDG PET/CT findings and boost confidence in the surgical decision-making. MATERIALS AND METHODS: Forty-nine TLE patients undergoing MRI and FDG PET/CT imaging at a tertiary epilepsy service were included. Images were visually interpreted by an experienced radiologist and nuclear medicine physician. SPM-based quantitative analysis for FDG PET/CT including Z score asymmetric index (ZAI) was performed. Statistical analyses include receiver operating characteristic curve and Cohen's k statistics. RESULTS: Significant differences in the standardised uptake value (SUV) ratio and ZAI were observed among left TLE, nonepilepsy, and right TLE (p < 0.01). The areas under the curves for left/nonleft and right/nonright groups were 0.838 and 0.780, respectively. The cutoff value to separate left TLE from nonepilepsy and right TLE was 0.305 with 89.7% sensitivity, 80.0% specificity, 94.6% positive predictive value (PPV), 66.7% negative predictive value (NPV), and a 0.697 Youden index for diagnosis. It was 0.190 to separate right TLE from the other 2 with 87.5% sensitivity, 75.6% specificity, 41.2% PPV, 96.9% NPV, and a 0.631 Youden index for diagnosis. The intermethod agreement between MRI and SUV ratio was moderate (k = 0.48; 95% CI, 0.320.65) and that between FDG PET/CT qualitative assessment and ZAI was moderate (k = 0.43; 95% CI, 0.10-0.76). CONCLUSION: FDG PET/CT-based SUV ratios and ZAI show promising diagnostic value in TLE patients, facilitating the integration of FDG PET/CT practice into presurgical assessment for medically refractory epilepsy. (c) 2025 The Royal College of Radiologists. Published by Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies."
A masked study to differentiate in vivo confocal microscopic features of Pythium insidiosum and fungal filaments,"Bagga, B; Ali, MH; Jain, KS; Gokhale, T; Joseph, J; Duwal, P; Mohanty, A; Gowtham, L; Sharma, S",10.1016/j.jtos.2025.03.001,2025,"Purpose To describe in vivo confocal microscopic features of Pythium insidiosum in patients with Pythium keratitis and compare with those observed in fungal keratitis. Method We collected in vivo confocal images of the cornea from patients with microbiologically confirmed Pythium and fungal keratitis, analysing five putative distinguishing features: filament width (broad or thin), granularity within the filament (present or absent), filament continuity or traceability, the presence or absence of loops, and the double track sign. Three masked observers were shown images with concealed identities and tasked with detecting Pythium filaments. After initial assessment and training, their detection rates were calculated and compared before and after training. We did perform imageJ (Open Source software project Fiji) analysis of all the images for objectively assessment. Results Sixty confocal images of Pythium (n = 32,15 patients) and fungal (n = 28,12 patients) keratitis were analysed. The continuity of filaments and the presence of loops emerged as strong predictors of Pythium, with adjusted odds ratios (OR) of 18.1 and 19.29, respectively, based on multivariate logistic regression and decision tree splits. Pre-training accuracy was 0.51, 0.52, and 0.56, but post-training (95 % CI) improved to 0.75 (0.62-0.85), 0.80 (0.67-0.89), and 0.86 (0.75-0.94). Correct identification rates for Pythium were 27, 28, and 29 (84-89 %) out of 32, and for fungus were 16, 21, and 24 (57.4-85.7 %) out of 28 images with sensitivity and specificity ranging from 70.7 to 87.5 % and 80-85 % respectively. ImageJ analysis revealed a significant difference between Pythium and fungal filaments in both width (9.30 +/- 1.21 mu vs. 6.20 +/- 0.88 mu, p < 0.001) and branching angle (83.92 +/- 13.57 degrees vs. 55.10 +/- 6.03 degrees, p < 0.001). Conclusions Based on our analysis, these features may be indicative of Pythium and could serve as a helpful reference for future prospective studies. However, further large scale studies and validation are needed to strengthen these observations."
Service recommendation method based on text view and interaction view,"Yu, T; Wang, YQ; Cheng, FY; Liang, T; Liu, HB",10.1038/s41598-025-96568-5,2025,"With the increasing prosperity of web service-sharing platforms, more and more software developers are integrating and reusing Web services when developing applications. This approach not only meets the needs of developers but also is cost-effective and widely used in the field of software development. Usually, software developers can browse, evaluate, and select corresponding Web services from a web service-sharing platform to create various applications with rich functionality. However, a large number of candidate Web services have placed a heavy burden on the selection decisions of software developers. Existing web service recommendation systems often face two challenges. Firstly, developers discover services by inputting development requirements, but the user's input is arbitrary and can not fully reflect the user's intention. Secondly, the application service interaction record is too sparse, reaching 99.9%, making it particularly difficult to extract services that meet the requirements. To address the above challenges, in this paper, we propose a service recommendation method based on text and interaction views (SRTI). Firstly, SRTI employs graph neural network algorithms to deeply mine the historical records, extract the features of applications and services, and calculate their preferences. Secondly, SRTI uses Transformer to analysis develop requirements and uses fully connected neural networks to deeply mine the matching degree between candidate services and development requirements. Finally, we integrate the above two to obtain the final service list. Extensive experiments on real-world datasets have shown that SRTI outperforms several state-of-the-art methods in service recommendation."
Predicting test failures induced by software defects: A lightweight alternative to software defect prediction and its industrial applicationâ,"Madeyski, L; Stradowski, S",10.1016/j.jss.2025.112360,2025,"Context: Machine Learning Software Defect Prediction (ML SDP) is a promising method to improve the quality and minimise the cost of software development. Objective: We aim to: (1) apropose and develop a Lightweight Alternative to SDP (LA2SDP) that predicts test failures induced by software defects to allow pinpointing defective software modules thanks to available mapping of predicted test failures to past defects and corrected modules, (2) preliminary evaluate the proposed method in a real-world Nokia 5G scenario. Method: We train machine learning models using test failures that come from confirmed software defects already available in the Nokia 5G environment. We implement LA2SDP using five supervised ML algorithms, together with their tuned versions, and use eXplainable AI (XAI) to provide feedback to stakeholders and initiate quality improvement actions. Results: We have shown that LA2SDP is feasible in vivo using test failure-to-defect report mapping readily available within the Nokia 5G system-level test process, achieving good predictive performance. Specifically, CatBoost Gradient Boosting turned out to perform the best and achieved satisfactory Matthew's Correlation Coefficient (MCC) results for our feasibility study. Conclusions: Our efforts have successfully defined, developed, and validated LA2SDP, using the sliding and expanding window approaches on an industrial data set."
"Ok Pal, we have to code that now: interaction patterns of programming beginners with a conversational chatbot","Mailach, A; Gorgosch, D; Siegmund, N; Siegmund, J",10.1007/s10664-024-10561-6,2025,"ContextChatbots based on large language models are becoming an important tool in modern software development, yet little is known about how programming beginners interact with this new technology to write code and acquire new knowledge. Thus, we are missing key ingredients to develop guidelines on how to adopt chatbots for becoming productive at programming.ObjectiveWith our research, we aim at identifying these ingredients. Specifically, we want to understand how programming beginners use conversational chatbots when writing source code.MethodTo this end, we study programming beginners' interaction with a chatbot in a CS2 course while they were solving programming assignments. Additionally, we evaluate the correctness of submitted solutions and compare them to solutions of beginners who did not use a conversational chatbot.ResultsWe analyzed 756 prompts of 129 conversations, most of them focusing on code generation. Interestingly, conversations that contain prompts asking for debugging or testing of code are linked with higher success rates, indicating that deeper engagement with code leads to higher quality code. Moreover, prompts without sufficient context often lead to unsatisfying results.ConclusionsWhile not surprising, this underpins the importance that programming beginners need to know how to use chatbots, instead of merely using it as code generators without investing time in code quality. Moreover, companies should employ prompt guidelines, in which code quality prompts might be enforced after a code generation prompt has been stated."
Maresin 1-LGR6 axis mitigates inflammation and posttraumatic osteoarthritis after transection of the anterior cruciate ligament in mice,"Leite, CBG; Fricke, HP; Tavares, LP; Nshimiyimana, R; Mekhail, J; Kilgallen, E; Killick, F; Whalen, JD; Lehoczky, JA; Serhan, CN; Charles, JF; Lattermann, C",10.1016/j.joca.2025.03.005,2025,"Objective Anterior cruciate ligament (ACL) tears frequently cause chronic inflammation and posttraumatic osteoarthritis (PTOA), with therapies failing to resolve persistent post-injury inflammation. Specialized pro-resolving mediators (SPMs), including Maresin1 (MaR1), show promise in resolving inflammation and promoting tissue repair. However, their role in PTOA remains underexplored. This study investigated inflammatory markers and MaR1 dynamics post-ACL injury, the role of the MaR1 receptor Leucine-rich Repeat-containing G protein-coupled receptor 6 (LGR6) in PTOA, and MaR1 ' s therapeutic potential in a mouse ACL transection (ACLT) model. Design Eight-week-old C57BL6/J male mice underwent ACLT, and synovial fluid, periarticular tissue, and tibiofemoral joints were collected at various time points post-surgery for analysis. LGR6-deficient mice were utilized to investigate the role of MaR1 signaling in inflammation resolution. Additionally, the effect of intraarticular MaR1 administration on PTOA progression was assessed. Results ACLT induced joint inflammation with leukocyte infiltration and elevated pro-inflammatory cytokines. MaR1 levels peaked early post-injury and were associated with a six-fold increase in LGR6 expression. LGR6 deficiency worsened inflammation and PTOA severity with higher histological Osteoarthritis Research Society International (OARSI) scores (mean difference 5.6[95%CI: 2.5-8.6], p<0.001) and microCT OA severity scores (mean difference 4.3[95%CI: 0.7-7.9], p=0.018). Intraarticular MaR1 treatment reduced leukocyte recruitment, suppressed pro-inflammatory gene expression, and ameliorated PTOA development, improving histological OARSI scores (mean difference -3.9[95%CI: -6.9 to -1.0], p=0.012) and microCT scores (mean difference -6.7[95%CI: -10.3 to -3.0], p=0.012). Conclusion This study suggests a critical role of MaR1 in resolving inflammation post-ACL injury and mitigating PTOA in mice. Targeting SPM pathways, particularly MaR1 and/or MaR1 mimetics, offers a promising strategy to prevent chronic joint inflammation and degeneration after ACL injury. (c) 2025 Osteoarthritis Research Society International. Published by Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies."
The AI Scrum Master: Using Large Language Models (LLMs) to Automate Agile Project Management Tasks,"Alliata, Z; Singhal, T; Bozagiu, AM",10.1007/978-3-031-72781-8_12,2025,"There is a high demand across industries for intelligent devices to automate and optimize processes. Generative AI has soared in popularity and it is the first AI technology to be used by everyone. It is already automating junior-level office work, as well as highly complex creative tasks. This paper studies how Generative AI can automate some of the Agile project management tasks, such as reporting and creating requirements that correctly cover the scope."
The Future of Software Development With GenAI: Evolving Roles of Software Personas,"Simsek, T; GÃ¼lseni, AC; Olcay, GA",10.1109/EMR.2024.3454112,2025,"Recent advancements in Generative AI (GenAI) algorithms have been reshaping the software development lifecycle (SDLC) landscape. These transformations not only enhance the speed, accuracy, and productivity of SDLC tasks but also influence the roles and existence of software personas within the SDLC ecosystem. This study evaluates GenAI's effects on the roles of software personas engaged in the typical SDLC phases: planning, design, development, testing, and deployment. When evaluating these effects, it becomes evident that GenAI's impact varies across roles and their relation to specific SDLC phases and activities. While some roles experience a substantial impact, achieving full AI-driven automation remains a challenge for roles that demand critical thinking, empathy, and other human supervision. Overall, GenAI's penetration will necessitate new skills and a redefinition of roles for many tasks within SDLC, where humans and AI will collaborate to create better, faster, and more efficient software."
"Always Nice and Confident, Sometimes Wrong: Developer's Experiences Engaging Generative AI Chatbots Versus Human-Powered Q&A Platforms","Li, JC; Mynatt, ED; Mishra, V; Bell, J",10.1145/3710927,2025,"Software engineers have historically relied on human-powered Q&A platforms like Stack Overflow (SO) as coding aids. With the rise of generative AI, developers have started to adopt AI chatbots, such as ChatGPT, in their software development process. Recognizing the potential parallels between human-powered Q&A platforms and AI-powered question-based chatbots, we investigate and compare how developers integrate this assistance into their real-world coding experiences by conducting a thematic analysis of 1700+ Reddit posts. Through a comparative study of SO and ChatGPT, we identified each platform's strengths, use cases, and barriers. Our findings suggest that ChatGPT offers fast, clear, comprehensive responses and fosters a more respectful environment than SO. However, concerns about ChatGPT's reliability stem from its overly confident tone and the absence of validation mechanisms like SO's voting system. Based on these findings, we synthesized the design implications for future GenAI code assistants and recommend a workflow leveraging each platform's unique features to improve developer experiences."
Knowledge Monopoly Risks in Generative AI-Assisted Software Development Lifecycle,"Wu, YY; Lu, XY; Lin, CH",10.1109/MC.2025.3532947,2025,"This article examines the emerging knowledge monopolization risks in generative artificial intelligence (AI)-assisted software development, proposing a comprehensive assessment framework and practical intervention strategies to help organizations maintain technical sovereignty while leveraging AI capabilities for development efficiency."
Smart Agile Prioritization and Clustering: An AI-Driven Approach for Requirements Prioritization,"Radwan, AM; Abdel-Fattah, MA; Mohamed, W",10.1109/ACCESS.2025.3589959,2025,"In Agile software development, requirements prioritization plays a crucial role in ensuring that critical functionalities are delivered efficiently. Traditional prioritization methods often suffer from scalability limitations, lack of automation, and difficulty in handling dependencies. This paper proposes Smart Agile Prioritization and Clustering (SAPC), an AI-driven approach that enhances requirements prioritization by leveraging Natural Language Processing (NLP), BERT embeddings, graph-based dependency modeling, and optimization techniques. The proposed model extracts and processes textual requirements, constructs a dependency graph to model interrelations, and applies PageRank to compute requirement importance. Feature fusion and dimensionality reduction using Uniform Manifold Approximation and Projection (UMAP) facilitate clustering, while Particle Swarm Optimization (PSO) determines the optimal number of clusters for efficient backlog prioritization. The effectiveness of SAPC is evaluated using functional requirements extracted from Software Requirement Specifications (SRS), product backlogs, and customer requests, along with a benchmark dataset for validation. Various machine learning algorithms are tested, with KNN and Random Forest demonstrating the highest accuracy and lowest Mean Squared Error (MSE), outperforming traditional prioritization techniques. The results highlight the potential of AI-based methods in automating and optimizing backlog management within Agile methodologies, offering a scalable and data-driven approach to enhanced requirements prioritization."
"A Comprehensive Framework for Intelligent, Scalable, and Performance-Optimized Software Development","Arshad, N; Butt, TA; Iqbal, M",10.1109/ACCESS.2025.3564139,2025,"Integrating Artificial Intelligence (AI) into the Software Development Life Cycle (SDLC) has become necessary to enhance efficiency, scalability, and performance in modern software systems. Instead of incorporating the AI functionality into their SDLC, traditional SDLC models typically add-on the AI software functionality after they have integrated AI functionality into their application or software process. Because of this, developers undergo inefficiencies in their development workflows, experience performance bottlenecks during testing, and experience challenges of incorporating AI to improve an application's performance through optimization. This paper proposes a new AI-Optimized Software Development Life Cycle (AI-SDLC), which is a holistic and comprehensive framework that encases the embedded AI capabilities and optimization strategies throughout the SDLC process during every stage of the system development, so that requirements-gathering, development, testing, and maintenance are hybrid software processes and not dictated by AI vs. traditional software development processes. AI-SDLC presents new development roles, such as AI Integration Specialist, Code Optimizer, and UX Optimization Specialist, which helps developers work across disciplines and increases collaborative interaction between traditional developers and AI engineers. AI-SDLC also utilizes an AI-driven automated hybrid software process in areas such as requirement elicitation, design/architecture validation, testing, deployment monitoring, and scalability to produce robust high-performance systems in all areas of practicing software development life cycle work. The discourse includes a rich case study based on a Smart Logistics Management System to demonstrate practical implementation of the AI-SDLC and how it facilitates improvement in system efficiency and improved user experience. Additionally, the discussion also highlights the possibilities of AI-SDLC practical implementation in other industrial domain areas such as e-Commerce, finance, aviation and enterprise solution based projects with practical considerations for implementation. In conclusion, the discussion provides findings that support AI-SDLC as a structured and intelligence-driven approach to Software Development Life Cycle implementation that addresses the weaknesses of traditional software design and development frameworks."
I Can't Take This Anymore! Understanding the Relationship Between Personality Traits and Tolerance of Generative AI Hallucinations,"Kadian, A; Merhi, MI; Gupta, M; Ghafoori, A; Dennehy, D",10.1109/TEM.2025.3606352,2025,"The use of generative artificial intelligence (AI) has increased drastically in recent years, offering transformative potential in creativity, productivity, and decision-making across various domains. However, concerns are growing among practitioners and academics about its tendency to produce inaccurate or nonsensical outputs-a phenomenon known as generative AI hallucinations. While scholars have begun exploring this phenomenon, most of the existing research focuses on technical issues and solutions. This research takes a behavioral approach to examine the association between individual personality traits and tolerance of generative AI hallucinations. Drawing on error management theory and the big five personality traits, we examine how individuals with different personality traits tolerate generative AI hallucinations. The study contributes to calls from management journals to investigate the challenges associated with generative AI use. Our findings provide important theoretical implications and offer actionable insights for engineering managers seeking to implement generative AI."
Early Results of an AI Multiagent System for Requirements Elicitation and Analysis,"Sami, MA; Waseem, M; Zhang, ZY; Rasheed, Z; SystÃ¤, K; Abrahamsson, P",10.1007/978-3-031-78386-9_20,2025,"In agile software development, user stories capture requirements from the user's perspective, emphasizing their needs and each feature's value. Writing concise and quality user stories is necessary for guiding software development. Alongside user story generation, prioritizing these requirements ensures that the most important features are developed first, maximizing project value. This study explores the use of Large Language Models (LLMs) to automate the process of user story generation, quality assessment, and prioritization. We implemented a multi-agent system using Generative Pre-trained Transformers (GPT), specifically GPT-3.5 and GPT-4o, to generate and prioritize user stories from the initial project description. Our experiments on a real-world project demonstrate that GPT-3.5 handled user story generation well, achieving a higher semantic similarity score comnpared to the GPT4o. Both models showed consistent performance in prioritizing requirements, effectively identifying the core features of the application. These early results indicate that LLMs have significant potential for automating requirements analysis, particularly generating and prioritizing user stories."
Beyond domain dependency in security requirements identification,"Casillo, F; Deufemia, V; Gravino, C",10.1016/j.infsof.2025.107702,2025,"Context: Early security requirements identification is crucial in software development, facilitating the integration of security measures into IT networks and reducing time and costs throughout software life-cycle. Objectives: This paper addresses the limitations of existing methods that leverage Natural Language Processing (NLP) and machine learning techniques for detecting security requirements. These methods often fall short in capturing syntactic and semantic relationships, face challenges in adapting across domains, and rely heavily on extensive domain-specific data. In this paper we focus on identifying the most effective approaches for this task, highlighting both domain-specific and domain-independent strategies. Method: Our methodology encompasses two primary streams of investigation. First, we explore shallow machine learning techniques, leveraging word embeddings. We test ensemble methods and grid search within and across domains, evaluating on three industrial datasets. Next, we develop several domain-independent models based on BERT, tailored to better detect security requirements by incorporating data on software weaknesses and vulnerabilities. Results: Our findings reveal that ensemble and grid search methods prove effective in domain-specific and domain-independent experiments, respectively. However, our custom BERT models showcase domain independence and adaptability. Notably, the CweCveCodeBERT model excels in Precision and F1-score, outperforming existing approaches significantly. It improves F1-score by similar to 3% and Precision by similar to 14% over the best approach currently in the literature. Conclusion: BERT-based models, especially with specialized pre-training, show promise for automating security requirement detection. This establishes a foundation for software engineering researchers and practitioners to utilize advanced NLP to improve security in early development phases, fostering the adoption of these state-of-the-art methods in real-world scenarios."
"Generative AI in Software Development: Challenges, Opportunities, and New Paradigms for Quality Assurance","Muratovic, F; Kearns-Manolatos, D; Alibage, A",10.1109/MC.2025.3556330,2025,"This research presents an overview of generative artificial intelligence's impact across the software development lifecycle. Through interviews, case study analysis, a thorough literature review, and original data analysis from proprietary Deloitte survey data, this research proposes strategic actions product, engineering, and technology leaders can take today."
Exploratory Test-Driven Development Study with ChatGPT in Different Scenarios,"Pancher, JC; Melegati, J; Guerra, EM",10.1007/978-3-031-94544-1_10,2025,"Generative AI has been rapidly adopted by the software development industry in various ways, offering innovative approaches to transforming requirements into working software. Combining Generative AI with Test-Driven Development (TDD) presents a creative method to accelerate this transformation. However, questions remain about ChatGPT's readiness for this challenge, including the techniques and best practices required for success and the scenarios where this approach can consistently deliver results. To explore these questions, we designed a study where a group of master's students performed programming assignments using TDD, first independently and then with the support of ChatGPT. The three assignments represent distinct scenarios: mathematical calculations (function), text processing (class), and system integration (class with dependencies). We performed a qualitative analysis of the submitted code and reports identifying key strategies that significantly influence success rates, such as providing contextual information, separating instructions in prompts following an iterative process, and assisting AI in fixing errors. Among the scenarios, the integration task achieved the highest performance. This study highlights the potential of leveraging Generative AI in TDD for software development and presents a list of effective strategies to maximize its impact. By applying these positive strategies and avoiding identified pitfalls, this research marks a step toward establishing best practices for integrating Generative AI with TDD in software engineering."
An Aggregated Dataset of Agile User Stories and Use Case Taxonomy for AI-Driven Research,"Alhaizaey, A; Al-Mashari, M",,2025,"Agile methodologies are considered revolutionary approaches in the development of systems and software. With the rapid advancement of artificial intelligence, natural language processing, and large language models, there is an increasing demand for high-quality datasets to support the design and development of intelligent, practical, and effective automation tools. However, researchers in Agile Requirements Engineering face significant challenges due to the limited availability of datasets, particularly those involving user stories. This paper presents a dataset ofover 10Kuser stories collected from academic sources and publicly accessible online repositories. These stories represent requirements formulated in accordance with Agile principles. The process of collecting and classifying data, as well as its use in a prior research project focused on identifying nonfunctional requirements, is described. The dataset was validated with substantial inter-annotator agreement and has been successfully employed in prior experiments, where a fine-tuned pre-trained language model achieved F1 scores above 93% in classifying non-functional requirements. Additionally, a structured taxonomy of potential research and practical use cases for this dataset is proposed, aiming to support researchers and practitioners in areas such as requirements analysis, automated generative tasks using generative language models, model development, and educational purposes."
Code and Craft: How Generative AI Tools Facilitate Job Crafting in Software Development,"Freise, LR; Bruhin, O; Ritz, E; Li, MM; Leimeister, JM",,2025,"The rapid evolution of the software development industry challenges developers to manage their diverse tasks effectively. Traditional assistant tools in software development often fall short of supporting developers efficiently. This paper explores how generative artificial intelligence (GAI) tools, such as Github Copilot or ChatGPT, facilitate job crafting-a process where employees reshape their jobs to meet evolving demands. By integrating GAI tools into workflows, software developers can focus more on creative problem-solving, enhancing job satisfaction, and fostering a more innovative work environment. This study investigates how GAI tools influence task, cognitive, and relational job crafting behaviors among software developers, examining its implications for professional growth and adaptability within the industry. The paper provides insights into the transformative impacts of GAI tools on software development job crafting practices, emphasizing their role in enabling developers to redefine their job functions."
EGPT-SPE: story point effort estimation using improved GPT-2 by removing inefficient attention heads,"Cheemaa, AS; Azhar, M; Arif, F; ul Haq, QM; Sohail, M; Iqbal, A",10.1007/s10489-025-06824-4,2025,"Estimating story points from user requirements is crucial in the Software Development Life Cycle (SDLC) as it impacts resource allocation and timelines; inaccuracies can lead to missed deadlines and increased costs, harming a company's reputation. While various techniques have emerged to automate this process, conventional machine learning methods often fail to understand the context of user requirements, and deep learning approaches face high computational costs. To address these issues, the Efficient GPT for Story Point Estimation (EGPT-SPE) algorithm optimizes the Multi-Head Attention module by removing inefficient heads, enhancing accuracy and reducing costs. Experiments on the Choetkiertikul dataset (23,313 issues across 16 open-source projects) and the TAWOS dataset (458,232 issues across 39 open-source projects from 12 public JIRA repositories) demonstrated a 5 to 15 percent accuracy improvement in both within-project and cross-project estimations, validating the algorithm's effectiveness in agile story point estimation."
Generative AI in Student Software Development Projects: A User Study on Experiences and Self-Assessment,"Borghoff, UM; Minas, M; Schopp, J",10.1145/3723010.3723012,2025,"The way software is developed is changing rapidly due to the general availability of generative AI tools. As a result, the software engineering education that is part of every computer science program needs to change. Especially in software engineering courses, such AI tools need to be used and practiced in a meaningful and useful way. The programming project is one such course at our university, and the curriculum will be expanded accordingly in the future. In this paper we describe our approach and a user study among the participants of the last programming project, in which we collected experiences with the use of current AI tools, in particular highlighting their usefulness and limitations. Our study focuses on identifying which aspects of the course students used AI tools for, evaluating successful applications, and uncovering remaining challenges."
Automated Code Comments Generation Using Large Language Models: Empirical Evaluation of T5 and BART,"Ghale, DP; Dabbagh, M",10.1109/ACCESS.2025.3597601,2025,"Source code documentation plays a significant role in the software development lifecycle, substantially improving the comprehensibility and maintainability of software projects. Despite its importance, documentation is usually dismissed or fails to satisfy the envisaged standards. Recently, Large Language Models (LLMs) have demonstrated the ability to address these challenges by automating code comments generation. Nevertheless, the empirical evaluation of these models is essential to assess their capabilities to produce accurate, contextually relevant and coherent documentation. In this paper, we have conducted an empirical study to investigate the capabilities of two prominent open-source LLMs, such as T5 and BART, developed by Google AI and Facebook AI, respectively, in automating the code comments generation for Python and Java code snippets. We have rigorously evaluated the performance of these models against four key metrics, such as BLEU, ROUGE, METEOR, and Smoothed BLEU. The comprehensive analysis of the evaluation results clearly highlights BART as the superior model over T5 for single-intent code comment generation."
MPLinker: Multi-template Prompt-tuning with adversarial training for Issue-commit Link recovery,"Wang, BC; Deng, Y; Luo, RQ; Liang, P; Bi, TT",10.1016/j.jss.2025.112351,2025,"In recent years, the pre-training, prompting and prediction paradigm, known as prompt-tuning, has achieved significant success in Natural Language Processing (NLP). Issue-commit Link Recovery (ILR) in Software Traceability (ST) plays an important role in improving the reliability, quality, and security of software systems. The current ILR methods convert the ILR into a classification task using pre-trained language models (PLMs) and dedicated neural networks. These methods do not fully utilize the semantic information embedded in PLMs, failing to achieve acceptable performance. To address this limitation, we introduce a novel paradigm: Multi- template Prompt-tuning with adversarial training for issue-commit Link recovery (MPLinker). MPLinker redefines the ILR task as a cloze task via template-based prompt-tuning and incorporates adversarial training to enhance model generalization and reduce overfitting. We evaluated MPLinker on six open-source projects using a comprehensive set of performance metrics. The experiment results demonstrate that MPLinker achieves an average F1-score of 96.10%, Precision of 96.49%, Recall of 95.92%, MCC of 94.04%, AUC of 96.05%, and ACC of 98.15%, significantly outperforming existing state-of-the-art methods. Overall, MPLinker improves the performance and generalization of ILR models and introduces innovative concepts and methods for ILR. The replication package for MPLinker is available at https://github.com/WTU-intelligent-software-development/ MPLinker."
A Roadmap for Software Testing in Open-Collaborative and AI-Powered Era,"Wang, Q; Wang, JJ; Li, MY; Wang, YW; Liu, Z",10.1145/3709355,2025,"Internet technology has given rise to an open-collaborative software development paradigm, necessitating the open-collaborative schema to software testing. It enables diverse and globally distributed contributions, but also presents significant challenges to efficient testing processes, coordination among personnel, and management of testing artifacts. At the same time, advancements in AI have enhanced testing capabilities and enabling automation, while also introducing new testing needs and unique challenges for AI-based systems. In this context, this article explores software testing in the open-collaborative and AI-powered era, focusing on the interrelated dimensions of process, personnel, and technology. Among them, process involves managing testing workflows and artifacts to improve efficiency, personnel emphasizes the role of individuals in ensuring testing quality through collaboration and contributions, while technology refers to AI methods that enhance testing capabilities and address challenges in AI-based systems. Furthermore, we delve into the challenges and opportunities arising from emerging technologies such as Large Language Models (LLMs) and the AI model-centric development paradigm."
Adaptive abstraction with AI for managing software antipatterns throughout the software lifecycle,"Andrade, R; Torres, J; Flores, P; Cabezas, E; Segovia, J",10.1109/Designing66910.2025.00013,2025,"Antipatterns, commonly described as solutions that initially appear promising but have negative consequences, present significant risks in software development, particularly with respect to security. This paper explores the classification of antipatterns in various domains, such as architecture, design, and implementation, while highlighting their impact on cybersecurity. We propose the integration of Artificial Intelligence (AI) and Machine Learning (ML) to detect and mitigate antipatterns in real-time through automated tooling and DevOps pipelines. Furthermore, the role of abuse stories in requirement elicitation and the application of UML diagrams for identifying antipatterns are examined. The paper culminates with a framework for AI-driven antipattern management within modern software development, illustrating its potential through case studies and practical experiments."
A ChatGPT-powered Prompt Engineering Framework for Generating Software Acceptance Criteria,"Rawson, J; Reddivari, S",10.1145/3696673.3723078,2025,"There has been a growing interest in using Natural Language Processing (NLP), such as OpenAI's ChatGPT for software engineering tasks, including requirements engineering, software design and software testing. This paper introduces a novel prompt engineering framework that aims to utilize ChatGPT for the generation of high-quality acceptance criteria in the software development process, particularly in the implementation and maintenance stages, by using curated prompts and inputs. The paper describes the development and possible implementation of the proposed framework."
Generative AI and Developer Workflows: How GitHub Copilot and ChatGPT Influence Solo and Pair Programming,"Stray, V; Moe, NB; Ganeshan, N; Kobbenes, S",,2025,"This study investigates the impact of generative AI (GAI) tools, like ChatGPT and GitHub Copilot, on software development. GAI tools can automate repetitive tasks, provide code suggestions, and enhance problem-solving. We conducted a case study in a large-scale agile organization, where we examined how GAI tools influence development efficiency and developers' workflow. During 25 days of observations, we observed 49 programming sessions. GAI was used in all 16 solo programming sessions and 30 of 33 pair programming sessions. Additionally, we conducted 14 interviews with developers and team leaders. In solo programming, GAI tools were said to improve efficiency and reduce stress, allowing more time for solving complex tasks. In contrast, the impact of GAI in pair programming was mixed. GAI tools were less frequently used in pair programming compared to solo programming. Our findings also highlight changes in software development, with developers leveraging GAI for information retrieval and coding assistance. However, challenges such as dependency on AI tools and the accuracy of AI-generated suggestions persist."
Predicting software effort using BERT-based word embeddings,"Maiga, S; Bilgaiyan, S; Sagnika, S",10.1007/s13198-025-02746-z,2025,"Accurate software effort estimation is essential for effective project planning and resource allocation, particularly in Agile software development where evolving requirements challenge traditional methods. This study explores the potential of pre-trained BERT (Bidirectional Encoder Representations from Transformers) models, a state-of-the-art NLP technique, to improve estimation accuracy. We compare the performance of the BERT base and BERT large models in diverse project scenarios. The results show that BERT Base consistently outperforms BERT Large in cross-repository and project-based contexts, owing to its computational efficiency and adaptability. A combined CNN and BERT Base model further enhances story point prediction for new projects, achieving superior accuracy and robustness. These findings highlight the practical advantages of leveraging BERT Base in Agile environments, offering valuable insights for researchers, software developers, and project managers. Future work will focus on external validation using commercial datasets, alternative deep learning architectures, and improved fine-tuning strategies to further advance effort estimation practices."
Exploring the Impact of Intervention Methods on Developers' Security Behavior in a Manipulated ChatGPT Study,"Serafini, R; Yardim, A; Naiakshina, A",10.1145/3706598.3713989,2025,"Increased AI use in software development raises concerns about AI-generated code security. We investigated the impact of security prompts, insecure AI suggestion warnings, and the use of password storage guidelines (OWASP, NIST) on the security behavior of software developers when presented with insecure AI assistance. In an online lab setting, we conducted a study with 76 freelance developers who completed a password storage task divided into four conditions. Three conditions included a manipulated ChatGPT-like AI assistant, suggesting an insecure MD5 implementation. We found a high level of trust in AI-generated code, even when insecure suggestions were presented. While security prompts, AI warnings, and guidelines improved security awareness, 32% of those notified about insecure AI recommendations still accepted weak implementation suggestions, mistakenly considering it secure and often expressing confidence in their choice. Based on our results, we discuss security implications and provide recommendations for future research."
The Role of Generative AI Models in Requirements Engineering: A Systematic Literature Review,"Vasudevan, P; Reddivari, S",10.1145/3696673.3723053,2025,"The software engineering field is experiencing rapid growth, driven by recent advancements in Artificial Intelligence (AI), particularly in Generative AI (GenAI) and Large Language Models (LLMs). Requirements Engineering (RE), a critical phase in software development, involves gathering and defining software requirements. However, research on the impact of GenAI and LLMs within RE remains limited. This paper examines the adoption of GenAI in RE, with the aim of exploring its practical implications, identifying current research trends, and highlighting areas for future development. To achieve this, a systematic literature review was conducted, addressing three research questions and analyzing 44 studies published over the past decade. The findings reveal that GenAI models, especially LLMs, are extensively employed in a variety of RE tasks, underscoring the versatility and potential of LLMs in enhancing the RE process."
Transforming Software Architecture Design with Intelligent Assistants- A Comparative Analysis,"Ramachandran, R",10.1109/SOUTHEASTCON56624.2025.10971683,2025,"As artificial intelligence tools such as GitHub Copilot, Chat Generative Pre-Trained Transformer (ChatGPT), or Black Box Artificial Intelligence (BlackBoxAI) become more embedded in software development workflows, they are reshaping how architects approach modern software design. While enterprises are increasingly leveraging Generative Artificial Intelligence (GenAI) tools to enhance developer productivity, limited research has been conducted on their impact on software architects. Traditionally, architects utilize various tools to create architectural blueprints, including Unified Modelling Language (UML) diagrams, class diagrams, sequence diagrams, use case diagrams, and state diagrams. Many of these diagrams follow repetitive structures that can be automated with well-defined contexts. This paper examines the role of three AI tools in assisting architects during the design phase, assessing how closely the generated outputs adhere to established architectural principles. By analyzing deviations, the study explores how refined prompts and additional context can improve accuracy. Research also includes cross tool comparison. Ultimately, this research aims to evaluate the potential of AI assistant tools to enhance productivity, ensure design consistency, and support architectural decision-making, while providing best practices for integrating AI into modern software architecture."
Generating reliable software project task flows using large language models through prompt engineering and robust evaluation,"Sarim, M; Masood, F; Maheshwari, M; Faridi, AR; Shamsan, AH",10.1038/s41598-025-19170-9,2025,"Large Language Models (LLMs) offer promising capabilities for converting unstructured software documentation into structured task flows, yet their outputs often lack procedural reliability critical for software engineering. This paper presents a comprehensive framework that benchmarks five leading LLMs-Gemini 2.5 Pro, Grok 3, GPT-Omni, DeepSeek-R1, and LLaMA-3-across five prompting strategies, including Zero-Shot, Chain-of-Thought, and ISO 21502-Guided, using real-world software tutorials from the Build Your Own X repository. We introduce the Hybrid Semantic Similarity Metric (HSSM), which combines SentenceTransformer embeddings with context-aware key-term overlap, capturing both semantic fidelity and procedural coherence. Compared to traditional metrics like BERTScore, SBERT, and USE, HSSM demonstrates significantly lower variance (CV: 1.5-2.9%) and stronger correlation with human judgments. Our results show that even minimal prompting (Zero-Shot) can yield highly aligned task flows (HSSM: 96.33%) when evaluated with robust metrics. This work offers a scalable evaluation paradigm for LLM-assisted software planning, with implications for AI-driven project management, prompt engineering, and procedural generation in software education and tooling."
What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards GenAI,"Choudhuri, R; Trinkenreich, B; Pandita, R; Kalliamvakou, E; Steinmacher, I; Gerosa, M; Sanchez, C; Sarma, A",10.1109/ICSE55347.2025.00087,2025,"Generative AI (genAI) tools, such as ChatGPT or Copilot, are advertised to improve developer productivity and are being integrated into software development. However, misaligned trust, skepticism, and usability concerns can impede the adoption of such tools. Research also indicates that AI can be exclusionary, failing to support diverse users adequately. One such aspect of diversity is cognitive diversity-variations in users' cognitive styles-that leads to divergence in perspectives and interaction styles. When an individual's cognitive style is unsupported, it creates barriers to technology adoption. Therefore, to understand how to effectively integrate genAI tools into software development, it is first important to model what factors affect developers' trust and intentions to adopt genAI tools in practice? We developed a theoretically grounded statistical model to (1) identify factors that influence developers' trust in genAI tools and (2) examine the relationship between developers' trust, cognitive styles, and their intentions to use these tools in their work. We surveyed software developers (N=238) at two major global tech organizations: GitHub Inc. and Microsoft; and employed Partial Least Squares-Structural Equation Modeling (PLS-SEM) to evaluate our model. Our findings reveal that genAI's system/output quality, functional value, and goal maintenance significantly influence developers' trust in these tools. Furthermore, developers' trust and cognitive styles influence their intentions to use these tools in their work. We offer practical suggestions for designing genAI tools for effective use and inclusive user experience."
Exploring the application of artificial intelligence in project management: A systematic literature review,"Bachari, MS; Solouki, A; Ghanbari, H",10.5267/j.jpm.2025.5.002,2025,"Projects play a crucial role in the success and development of industries, organizations and businesses, hence making project management an important practice which needs to be up to date with new trends and modern technology such as artificial intelligence (AI). With the advent of artificial intelligence there have been a number of studies aimed to design and introduce new ways and means of utilizing this phenomenon into project management. This research aims to find AI methods, tools, approaches, models and frameworks for each of the project management knowledge domains introduced by PMboK. The methodology followed the PRISMA guidelines for systematic literature reviews to collect, screen and analyze the literature to find relevant studies. The findings presented bibliographic data on the topic and current trends, frequently used AI methods and project management techniques and tools which benefit from these AI methods under each project management domain."
"Can Generative AI Contribute to both productivity gains and human Flourishing, and in fine satisfaction at work? Research on GitHub Copilot use in Software Development","Ngwenyama, O; Kanita, N; Rowe, F",,2025,"Artificial Intelligence (AI), particularly Generative Artificial Intelligence (GAI), has profoundly transformed the professional landscape. These technologies present new opportunities to enhance productivity, potentially offsetting the societal risks they pose. This study aims to assess whether GAI contributes to satisfaction at work and whether some of this improvement can be explained by productivity gains, human flourishing and reduced frustration when using GAI. To explore this, we employed the CHAID (Chi-square Automatic Interaction Detection) method and ANOVA analyses to investigate the use of GitHub Copilot by software developers. Our findings show that using GitHub Copilot boosted developers' productivity, their sense of flourishing at work, reduced their frustration and increased their satisfaction."
Design of An Eye-Tracking Study Towards Assessing the Impact of Generative AI Use on Code Summarization,"Mohamed, S; Ismail, N; Hernandez, KA; Parvin, A; Oliver, M; Parra, E",10.1145/3715669.3725868,2025,"As large language models (LLMs) become more integrated into software engineering and computer science education, it is crucial to understand their impact on student learning. While recent research has explored student perceptions of generative AI, little is known about how these tools influence students' cognitive processes during programming tasks, such as code comprehension, a valuable skill in software development and maintenance. This paper presents the design of a study that aims to investigate how computer science students interact with LLMs, such as Google's Gemini, in the context of code summarization using eye-tracking. This study will examine differences in visual attention, fixation behaviors, and performance of students engaged in code summarization with and without AI assistance across varying experience levels."
A Semiautomated Approach for Detecting Ambiguities in Software Requirements Using SpanBERT and Named Entity Recognition,"Talha, F; Tahir, T; Nadeem, T",10.1002/smr.70041,2025,"Ambiguous user requirements present a challenge in software requirement engineering. A manual approach to handling ambiguity is time-consuming. Software requirements are essential inputs to software development processes, including architecture and design, implementation, and testing. Requirement ambiguities lead to project cost overruns, delays in project delivery, and poor software product quality. Timely identification and correction of ambiguity can result in better software systems that meet product objectives and satisfy the needs of all stakeholders. This study explores various natural language processing techniques and SpanBERT (a variant of BERT). This research proposes a semiautomated approach for detecting anaphoric, coordination, and missing condition ambiguities in functional requirements. The proposed approach is validated on a new, original dataset containing 425 functional requirements from 16 domains. The ambiguities identified through our approach are compared with those detected manually and by ChatGPT. Our approach outperforms ChatGPT in detecting ambiguities. The proposed approach will aid project managers and requirement engineers in identifying ambiguities in requirement specifications, thereby helping to reduce cost overruns and delays in the software development process caused by requirement ambiguities."
LLM4Model: Automated Requirements Specification Model Authoring,"Rajbhoj, A; Somase, A; Sant, T; Vale, S; Kulkarni, V",,2025,"Traditionally, early stages of the Software Development Life Cycle (SDLC), such as requirement elicitation and analysis are manual, require significant expertise. Creation of purposive machine-processable models is an intellect-intensive task requiring business domain as well as modeling expertise, which is in short supply. To address this, we propose the LLM4Model tool, a configurable, extensible, domain-agnostic solution for automating model authoring with Large Language Models (LLMs) to reduce cognitive load. We demonstrate its effectiveness with two real-world case studies, showing automated authoring of requirements specification model and sharing the results."
Augmenting software quality assurance with AI and automation using PyTest-BDD,"Zhao, XF; Wang, H; Ding, JQ; Hu, ZM; Tian, QQ; Wang, Y",10.1007/s10515-025-00566-w,2025,"This paper explores the integration of Artificial Intelligence (AI) and automation within the Behavior-Driven Development (BDD) paradigm, using the PyTest-BDD framework, to enhance Software Quality Assurance (SQA) processes. Traditional SQA methods struggle with the increasing complexity and rapid release cycles of modern software development. This research demonstrates how AI can address these challenges through intelligent test generation, prioritization, and anomaly detection. The proposed framework utilizes Natural Language Processing (NLP) to analyze requirements, machine learning (ML) to generate and prioritize test scenarios, and deep learning (DL) for anomaly detection, all within the PyTest-BDD ecosystem. This approach fosters a collaborative environment between human testers and AI agents, leading to more robust testing with reduced human overhead.The framework offers reduced human error, faster feedback loops, and increased team collaboration, thereby reducing development time and improving software reliability. AI-powered test prioritization and anomaly detection are shown to be effective in identifying subtle defects. The modular and extensible nature of the framework allows for a flexible and scalable testing system."
"Enhancing Secure Software Development with AZTRM-D: An AI-Integrated Approach Combining DevSecOps, Risk Management, and Zero Trust","Coston, I; Hezel, KD; Plotnizky, E; Nojoumian, M",10.3390/app15158163,2025,"This paper introduces the Automated Zero Trust Risk Management with DevSecOps Integration (AZTRM-D) framework, a novel approach that embeds security throughout the entire Secure Software and System Development Life Cycle (S-SDLC). AZTRM-D strategically unifies three established methodologies: DevSecOps practices, the NIST Risk Management Framework (RMF), and the Zero Trust (ZT) model. It then significantly augments their capabilities through the pervasive application of Artificial Intelligence (AI). This integration shifts traditional, often fragmented, security paradigms towards a proactive, automated, and continuously adaptive security posture. AI serves as the foundational enabler, providing real-time threat intelligence, automating critical security controls, facilitating continuous vulnerability detection, and enabling dynamic policy enforcement from initial code development through operational deployment. By automating key security functions and providing continuous oversight, AZTRM-D enhances risk mitigation, reduces vulnerabilities, streamlines compliance, and significantly strengthens the overall security posture of software systems, thereby addressing the complexities of modern cyber threats and accelerating the delivery of secure software."
Innovative Learning Platforms Enhance Knowledge Transfer in Software Project Management: A Paradigm for New Services Development,"Srisuksa, N; Wiriyapinit, M; Bhattarakosol, P; Phuthong, T",10.18421/TEM142-48,2025,"This study developed and evaluated an innovative learning platform for knowledge transfer in software project management. The literature review revealed the challenges of transferring complex, tacit project management knowledge, especially in remote work environments. A new service development approach was employed to identify 19 user requirements and design an application incorporating personalized learning, simulated problem-solving, and gamification elements. The prototype was developed using Figma software and underwent rigorous testing. A technology acceptance model evaluation with 30 usefulness (mean 4.17 for facilitating knowledge transfer) and ease of use (mean 4.40 for device model acceptance were also assessed. The study contributes to theory by extending research on knowledge-based dynamic capabilities in project environments and provides a practical framework for developing similar applications. Future research should explore the long-term impacts and integration of emerging technologies like artificial intelligence."
Usage of Generative Artificial Intelligence for the Design of Graphic User Interfaces: Initial Approaches,"Paz, F; Paz-Sifuentes, FA; Moquillaza, A; Zavala, A; Huayta, J; Acevedo, V; Carrillo, L; Falconi, F",10.1007/978-3-031-93236-6_4,2025,"The use of generative artificial intelligence has been increasing in recent times. This technology is used to create different types of content from multiple sources of information. The content that generative AI provides to its users can be highly reliable if trained with real data and verified sources. The area of Computer Science is no exception, and there are some approaches to using generative artificial intelligence as a support tool in several phases of the software construction process. This research study discusses how generative artificial intelligence can be used within the User-Centered Design process to develop graphical interfaces. In this study, we present a set of available tools that can be used for the phases: (1) analysis of the context of use, (2) definition of requirements, and (3) elaboration of solution proposals in a software development scenario. Likewise, we present some examples of applying the identified tools developed in the academic field. Through the results, we show what the technology is capable of and offer academia a space to analyze the possibilities for the future. The discussions and conclusions are presented, as well as the possible works derived from this first approach to applying artificial intelligence in the design of graphical software interfaces."
The Impact of Generative AI Coding Assistants on Developers Who Are Visually Impaired,"Flores-Saviaga, C; Hanrahan, BV; Imteyaz, K; Clarke, S; Savage, S",10.1145/3706598.3714008,2025,"The rapid adoption of generative AI in software development has impacted the industry, yet its effects on developers with visual impairments remain largely unexplored. To address this gap, we used an Activity Theory framework to examine how developers with visual impairments interact with AI coding assistants. For this purpose, we conducted a study where developers who are visually impaired completed a series of programming tasks using a generative AI coding assistant. We uncovered that, while participants found the AI assistant beneficial and reported significant advantages, they also highlighted accessibility challenges. Specifically, the AI coding assistant often exacerbated existing accessibility barriers and introduced new challenges. For example, it overwhelmed users with an excessive number of suggestions, leading developers who are visually impaired to express a desire for AI timeouts. Additionally, the generative AI coding assistant made it more difficult for developers to switch contexts between the AI-generated content and their own code. Despite these challenges, participants were optimistic about the potential of AI coding assistants to transform the coding experience for developers with visual impairments. Our findings emphasize the need to apply activity-centered design principles to generative AI assistants, ensuring they better align with user behaviors and address specific accessibility needs. This approach can enable the assistants to provide more intuitive, inclusive, and effective experiences, while also contributing to the broader goal of enhancing accessibility in software development."
Generative AI Meets Knowledge Management: Insights From Software Development Practices,"Kirchner, K; Bolisani, E; Kassaneh, TC; Scarso, E; Taraghi, N",10.1002/kpm.70004,2025,"Recent developments in generative artificial intelligence (GenAI) have raised the interest of knowledge management (KM) scholars in artificial intelligence. By harnessing GenAI, KM processes become more efficient, scalable and adaptable to the needs of organisations and users. Taking the theoretical lens of the Technology-Organisation-Environment (TOE) framework for technology adoption, this study investigates the factors influencing the adoption of GenAI among software developers, as a specific group of knowledge workers, in knowledge creation and sharing. Our interviews with 11 developers from 8 countries were analysed by combining an inductive and a deductive approach. We identified five themes alongside the TOE framework, emphasising the adoption factors of GenAI in software development, particularly within KM. Based on our findings, we discuss how GenAI is adopted for KM in software development. In particular, the interviewees liked the GenAI affordances of solving simpler programming tasks efficiently and rapidly. However, GenAI requires expertise to review and modify the code, write suitable prompts and evaluate the reliability of the provided output. Knowledge exchange with fellow programmers is partly, but not entirely, replaced by exchange with GenAI as a new development team member, as it is more efficient. Nevertheless, continuous learning, adaptation and ethical consideration are needed to realise the full benefits of GenAI tools in software development. This study's findings provide broad practical insights into addressing challenges stemming from GenAI integration in KM processes. Additionally, it prompts a critical reflection on the necessity of revising existing KM theoretical models in light of the emergence of AI-generated knowledge."
Generative Artificial Intelligence in Agile Software Development Processes: A Literature Review Focused on User eXperience,"Cornide-Reyes, H; Monsalves, D; DurÃ¡n, E; Silva-Aravena, F; Morales, J",10.1007/978-3-031-93536-7_16,2025,"The integration of Generative Artificial Intelligence (GenAI) into Agile Software Development (ASD) is reshaping the way development teams automate tasks, optimize processes, and enhance user experience (UX). This study presents a systematic literature review to analyze the impact of GenAI on ASD, focusing on three research questions: (1) How can GenAI tools optimize user experience in agile software development projects? (2) What are agile teams' main challenges when integrating GenAI tools into software development projects? and (3) What stages of the agile software development cycle benefit from implementing GenAI tools? A total of 21 relevant studies published between 2020 and 2024 were selected and analyzed. Findings indicate that GenAI improves UX by facilitating automated test generation, personalized user interfaces, and enhanced documentation processes. However, challenges such as data quality, model transparency, security vulnerabilities, and team resistance hinder its adoption. Moreover, the research highlights that GenAI contributes across multiple ASD phases, including planning (requirement analysis and user story generation), implementation (automated code generation and debugging), testing (self-generated test cases), maintenance (documentation and refactoring), and retrospectives (data-driven team performance analysis). Despite its growing adoption, the study reveals a gap in empirical evaluations of GenAI's long-term impact on Agile methodologies. Future research should explore hybrid frameworks that balance automation and human oversight, longitudinal studies on GenAI's adoption trends, and strategies to ensure ethical and bias-free AI implementation in Agile environments. The findings contribute to a deeper understanding of GenAI's transformative role in software development and provide practical insights for industry professionals and researchers."
Artificial intelligence in web accessibility: A systematic mapping study,"Campoverde-Molina, M; LujÃ¡n-Mora, S",10.1016/j.csi.2025.104055,2026,"The popularization and new renaissance of artificial intelligence (AI) have inspired the creation of new applications that help developers improve website accessibility that benefits users with and without disabilities. Therefore, this research presents a systematic mapping study (SMS) because AI in web accessibility has been gaining more interest nowadays with the exposure of works that require an SMS to systematize and consolidate the literature. Through a literature review using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA), 53 studies from ACM Digital Library, IEEE Xplore, Scopus, and Web of Science were identified for inclusion in this review. The main results of this SMS are APIs with AI, web applications and plugins with AI, image and voice recognition with AI, limitations and challenges of AI in web accessibility, correction and testing of web accessibility with AI, automatic correction of web accessibility with AI, web navigation with conversational agents with AI, web and mobile application accessibility with AI, good practices in web accessibility for AI, accessibility of web forms and elements with AI. According to the results, in the studies, alternative texts were created for the images of the websites, AI helped generate accessible HTML code using well-defined prompts, AI tools must comply with Web Content Accessibility Guidelines (WCAG), machine learning was the most used approach, the most used language models were large language models (LLM) and accessibility barrier correction using ChatGPT. The primary contribution of this SMS lies in its analysis of the current state of AI research related to web accessibility and the identification of trends and gaps in this research area. This SMS is intended for researchers, programmers, and software development companies that may use language models, AI tools, or emerging technologies in web accessibility to mitigate website accessibility barriers."
Public Perceptions of Generative AI: Insights from Social Q&A Platforms,"Liao, J; Lee, CS",10.1007/978-3-031-94171-9_33,2025,"Generative AI (GenAI) discussions on social Q&A platforms reveal diverse public perceptions and user engagement shaped by platform-specific characteristics. This study analyzed discussions on Zhihu, Quora, and Reddit using topic modeling, topic similarity analysis and sentiment analysis, uncovering distinct user priorities and emotional responses. Zhihu focused on content creation, ethics, and personalized learning, with polarized sentiments. Quora emphasized business strategies, customer support, and technical learning, showing predominantly positive emotions. Reddit explored a wide range of topics, including communication tools and software development, with balanced sentiments. Cross-platform analysis revealed shared interests in communication, translation, and education, with notable overlaps in personalized learning and technical applications. These insights highlight how user demographics influence GenAI discourse, offering practical guidance for tailoring tools to regional needs. The comparative results have both practical and theoretical implications, offering valuable insights for stakeholders to tailor applications and strategies for different user groups related to GenAI."
Dynamic Requirement Prioritization in Software Development Using Artificial Intelligence,"Iuonas, CF; Titu, AM; Iuonas, ID",10.1007/978-3-031-95197-8_9,2025,"The rapid evolution of software development methods needs flexible ways to prioritize requirements. Traditional approaches often fail to manage the complexity and changing needs of modern projects. This paper explores how Artificial Intelligence (AI) changes requirement prioritization by offering adaptable, data-driven solutions. The study discusses key AI methods, their use in agile workflows, and their effect on project efficiency and risk reduction. Additionally, this research initiates further analysis to ensure data verification within projects, aiming to establish stable datasets as a foundation for effective AI implementation in prioritization processes."
Applying a Prompt Pattern Sequence for Decision-Making in Microservices Architectures,"Maranhao, JJ Jr; Melegati, J; Guerra, E",10.1007/978-3-031-84617-5_2,2025,"Software architecture decisions are critical in a software project to ensure that systems meet functional and non-functional requirements. Microservice architectures have become popular in the industry, having a high amount of material available that was used in the training of large language models (LLMs). This paper explores the use of generative AI tools, such as ChatGPT, guided by a prompt pattern sequence to support architectural decision-making in microservices architectures. The proposed approach aims to provide structured guidance to software architects, helping them navigate in complex design challenges. To evaluate the prompt sequence, we conducted studies that revisited important architectural decisions made by large companies in the context of microservices architectures. Two industry case studies are presented: one involving the management of a large set of components in a financial institution, and the other focused on the front-end approach for a large-scale e-commerce platform in a pharmaceutical chain. The results demonstrate how five distinct prompt patterns deliver actionable insights tailored to each project's unique technical and business constraints, enabling more informed decision-making. Retrospective feedback from architects highlights the effectiveness of the proposed prompt pattern sequence, which proposed solutions aligned to what was actually implemented. The findings suggest that generative AI, guided by wellstructured prompt patterns, can support the decision-making process in microservice architectures."
Artificial Intelligence for Software Engineering: The Journey So Far and the Road Ahead,"Ahmed, I; Aleti, A; Cai, HP; Chatzigeorgiou, A; He, PJ; Hu, X; Pezze, M; Poshyvanyk, D; Xia, X",10.1145/3719006,2025,"Artificial intelligence and recent advances in deep learning architectures, including transformer networks and large language models, change the way people think and act to solve problems. Software engineering, as an increasingly complex process to design, develop, test, deploy, and maintain large-scale software systems for solving real-world challenges, is profoundly affected by many revolutionary artificial intelligence tools in general and machine learning in particular. In this roadmap for artificial intelligence in software engineering, we highlight the recent deep impact of artificial intelligence on software engineering by discussing successful stories of applications of artificial intelligence to classic and new software development challenges. We identify the new challenges that the software engineering community has to address in the coming years to successfully apply artificial intelligence in software engineering, and we share our research roadmap toward the effective use of artificial intelligence in the software engineering profession, while still protecting fundamental human values. We spotlight three main areas that challenge the research in software engineering: the use of generative artificial intelligence and large language models for engineering large software systems, the need of large and unbiased datasets and benchmarks for training and evaluating deep learning and large language models for software engineering, and the need of a new code of digital ethics to apply artificial intelligence in software engineering."
Data-Driven Requirements Elicitation from App Reviews Framework Based on BERT,"Mihany, FA; Galal-Edeen, GH; Hassanein, EE; Moussa, H",10.3390/app15179709,2025,"Market-Driven Requirements Engineering (MDRE) integrates traditional Requirements Engineering (RE) practices, such as Requirements Elicitation and requirements prioritization, with market analysis. Offering software products to an open market has become a trend, yet it has many challenges. In MDRE, there are diverse sources for requirements including support teams, subcontractors, sales, and marketing teams. So, the MDRE process must provide ongoing requirements gathering techniques to ensure no crucial requirements are overlooked. It is generally possible for users to search and download software applications through app stores (such as the Google Play Store and Apple App Store) for various purposes. Users are allowed to express their opinions about the software applications by writing text messages which are widely known as app reviews. Utilizing these app reviews as a source of requirements while planning to develop a similar software application may have promising results. Therefore, the concept of App Reviews Utilization has emerged and can be applied for various purposes. This research utilizes app reviews in Requirements Elicitation while developing a software product in the market-driven development context. Furthermore, these reviews may be noisy and informally expressed. This paper proposes a framework, Automatic Requirements Elicitation from App Reviews (AREAR), that integrates Natural Language Processing (NLP) techniques with pre-trained Language Models to automatically elicit requirements from available app reviews while developing a market-driven software product. AREAR employed the Bidirectional Encoder Representation from the Transformers (BERT) Language Model. The proposed framework achieved an improved Accuracy and F1 score as compared to previous research."
AI-Based Automotive Test Case Generation: An Action Research Study on Integration of Generative AI into Test Automation Frameworks,"Karlsson, A; Lindmaa, E; Sun, SM; Staron, M",10.1007/978-3-031-78392-0_4,2025,"Generative AI is transforming software development, particularly in unit and regression testing. However, it's rarely used in Hardware-in-the-Loop (HIL) testing due to hardware-specific environments. This paper examines integrating GitHub Copilot into automotive test automation frameworks, focusing on Volvo's Test Automation Framework (TAF). It explores how Copilot can automate test case generation and compares AI-generated test cases with manually written ones in terms of reliability and robustness. Using an iterative action research methodology, the study evaluates the functional suitability of AI-generated test cases and the challenges of integration. Results show that in the first iteration, 23% of AI-generated test cases passed in Jenkins and received high functionality scores. In the second iteration, this increased to 36%. These findings highlight the potential of Generative AI to enhance HIL testing."
AI-Powered IT Project Management: Analyzing the Effectiveness of Advanced Project Management Tools to Ensure Project Efficiency,"Das, JK; Elegbe, I; Coffie, L; Khadka, R; Chen, L; Ji, YM",10.1109/SOUTHEASTCON56624.2025.10971718,2025,"The integration of Artificial Intelligence (AI) into project management has significantly transformed traditional methodologies, particularly within the Information Technology (IT) sector. This study investigates the effectiveness of AI-powered tools, specifically Asana, Jira, and Monday.com to improve project management efficiency through automation, resource optimization, and decision-making processes. Using a systematic literature review and quantitative analysis, the research highlights the capabilities and limitations of these tools in the management of complex IT projects. The study findings reveal that AI-powered platforms facilitate real-time tracking, predictive analytics and dynamic resource allocation, which collectively contribute to reduced project timelines and improved outcomes. However, challenges such as data quality, integration complexities, and the need for skilled personnel remain prevalent. The study was summarized with strategic recommendations for organizations to incorporate automation, leverage predictive analytics, and improve data integrity, thus maximizing the benefits of AI in project management. This research underscores the critical role of AI in modernizing project management practices, which ultimately leads to increased productivity and project success rates in the IT domain."
"Unified AI and ML framework in DevSecOps practices, solving real-world problems","Muthukrishnan, H; Viradia, V; Yadav, D",10.1109/SOUTHEASTCON56624.2025.10971458,2025,"Artificial Intelligence (AI) and Machine Learning (ML) are evolving daily and advancing human progression in every field of endeavor like arts, science, and business. Information Technology (IT) is the field where this transformation is most accelerated due to its role in enabling digital transformation across other industries. DevSecOps is the standard set of practices that emerged as the ideal solution for managing the software development life cycle in this digital transformation journey. DevSecOps practices are also evolving along with the IT industry, and there are challenges and opportunities to improve continuously. This research paper introduces a conceptual framework for implementing a unified AI and ML solution to optimize the DevSecOps practice of any organization and constantly improve. This framework aims to solve some of the most challenging problems in DevSecOps practices by accelerating software development, reducing defects, increasing security scanning accuracy, enhancing automation, improving performance testing, and streamlining operations. Our solution accomplishes this using consolidated DevSecOps data-driven predictive analytics output from the Unified AI and ML model. By introducing this conceptual framework, the paper encourages more research and development in the core area of DevSecOps practices, which function as the cornerstone of digital transformation across all industries."
Selecting Appropriate Process Models for IT Projects: Towards a Tool-Supported Decision Approach,"Harr, MD; Seufert, S",10.1007/978-3-031-80119-8_28,2025,"The appropriate selection of suitable process models plays an important role for IT project success. To aid in decision-making, IT project management literature offers a plethora of decision models for selecting suitable process models, however, hybrid process models are often neglected and adoption in practice is low or non-existent. To address this challenge, we draw on contingency theory to develop and implement a tool-supported decision model for the selection and evaluation of appropriate process models for IT projects, thereby leveraging artificial intelligence and machine learning in the context of a self-enforcing network. Our model provides an objective tool to assess process model suitability. Results from a conducted online survey with project management experts indicate high validity. Therefore, we contribute to the field of IT project management by expanding AI-based decision models for selecting and evaluating process models through extending the range of covered models and implementing inherent weighting of criteria."
SPERT: Reinforcement Learning-Enhanced Transformer Model for Agile Story Point Estimation,"Younas, W; Chen, R; Zhao, J; Iqbal, T; Sharaf, M; Imran, A",10.1142/S0218194025500044,2025,"Story point estimation is a key practice in Agile project management that assigns effort values to user stories, helping teams manage workloads effectively. Inaccurate story point estimation can lead to project delays, resource misallocation and budget overruns. This study introduces Story Point Estimation using Reinforced Transformers (SPERT), a novel model that integrates transformer-based embeddings with reinforcement learning (RL) to improve the accuracy of story point estimation. SPERT utilizes Bidirectional Encoder Representations from Transformers (BERT) embeddings, which capture the deep semantic relationships within user stories, while the RL component refines predictions dynamically based on project feedback. We evaluate SPERT across multiple Agile projects and benchmark its performance against state-of-the-art models, including SBERT-XG, LHC-SE, Deep-SE and TF-IDF-SE. Results demonstrate that SPERT outperforms these models in terms of Mean Absolute Error (MAE), Median Absolute Error (MdAE) and Standardized Accuracy (SA). Statistical analysis using Wilcoxon tests and A12 effect size confirms the significance of SPERT's performance, highlighting its ability to generalize across diverse projects and improve estimation accuracy in Agile environments."
Success with Agile Project Management: Looking back and into the future,"Koudriachov, C; Tam, C; Aparicio, M",10.1016/j.jss.2025.112428,2025,"We show what the influential factors and practical strategies are that contribute to agile project management success. The research model comprises three people-related factors (personal characteristics, team capability, and customer involvement), three technological factors (gamification, artificial intelligence, and marketing intelligence), and one dependent variable (agile project management success). Based on 143 questionnaire responses, our findings reaffirm the positive impact of personal characteristics and customer involvement while challenging the roles of gamification and team capability, suggesting that their effects are more contextdependent than previously thought. Our findings also highlight that agile project management success depends on the interplay between remote work and team capability, with strong team skills being highly important for agile methodologies, especially in traditional office settings."
Can ChatGPT Suggest Patterns? An Exploratory Study About Answers Given by AI-Assisted Tools to Design Problems,"Maranhao, JJ Jr; Correia, FF; Guerra, EM",10.1007/978-3-031-72781-8_14,2025,"General-purpose AI-assisted tools, such as ChatGPT, have recently gained much attention from the media and the general public. That raised questions about in which tasks we can apply such a tool. A good code design is essential for agile software development to keep it ready for change. In this context, identifying which design pattern can be appropriate for a given scenario can be considered an advanced skill that requires a high degree of abstraction and a good knowledge of object orientation. This paper aims to perform an exploratory study investigating the effectiveness of an AI-assisted tool in assisting developers in choosing a design pattern to solve design scenarios. To reach this goal, we gathered 56 existing questions used by teachers and public tenders that provide a concrete context and ask which design pattern would be suitable. We submitted these questions to ChatGPT and analyzed the answers. We found that 93% of the questions were answered correctly with a good level of detail, demonstrating the potential of such a tool as a valuable resource to help developers to apply design patterns and make design decisions."
Generative AI in the Workplace: Affective Affordances and Employee Flourishing,"Barbala, A; Ulfsnes, R; Stray, V; Wivestad, VT",,2025,"With the advent of Generative AI (GenAI) and its application in various industries, research has begun showing how this new technology might impact aspects such as teamwork, perceived productivity and the nature of work tasks. However, there is still a lack of studies investigating the emotional dimensions of GenAI use, emphasizing the affective experiences materializing from employing such tools. This study explores the potential of GenAI tools to enhance employee flourishing in a workplace context. By building on interviews with 30 individuals across four tech-intensive companies we investigate how these tools not only provide new possibilities for work practices but also elicit positive affective experiences among users. Drawing on affect studies and affordance theory, this paper suggests that GenAI's serendipitous nature and ability to streamline mundane tasks not only can enhance operational capabilities but also inspire a dynamic interplay of anticipation and excitement. Our findings point to that the integration of GenAI introduces novel affective affordances that potentially can enhance workplace flourishing, thereby augmenting the traditional understanding of user-tool interaction in professional environments."
Breaking the Cycle of Recurring Failures: Generative AI and Root Cause Analysis in Legacy Banking Systems,"Jin, SY; Bei, ZD; Chen, BC; Xia, Y",10.1109/AIOPS66738.2025.00010,2025,"The banking legacy systems and fragmented ownership often lead to superficial incident resolutions without addressing root causes, resulting in recurring failures. To solve this, we use Generative AI (GenAI) agents combined with the Five Whys technique. Our approach draws knowledge from problem descriptions, change request data, and historical incident records. This method uncovered that 70% of the issues previously blamed on management or vendors in a financial company were due to internal code problems. In a case study, we analyzed 5,000 projects and identified over 400 files with the same root cause. Automating root cause analysis moves the process from reactive to proactive, effectively resolving code-level issues."
Transforming user story definition: From deterministic to AI automation,"Avasiloaie, AP; Semenescu, A; Popovici, EC; Chiva, IC",10.33436/v35i2y202505,2025,"Agile software development necessitates the definition of user stories; however, the conventional manual process frequently leads to inefficiencies and inconsistencies. This paper investigates the shift from the deterministic user story creation to the AI-powered automation, addressing the limitations of the manual methods and the benefits of the AI integration. The study highlights the development of a deterministic JIRA extension for e-commerce applications. Despite its efficiency, the deterministic tool lacked adaptability across different domains. To overcome this, an AI-powered enhancement utilizing OpenAI's API was introduced, enabling scalable and accurate user story generation through natural language processing and machine learning. AI-driven tools automate user story creation, improving accuracy, reducing misinterpretation risks, and streamlining workflows. The research traces the evolution from a deterministic automation app using templates to an AI-driven app, emphasizing the role of prompt engineering in refining AI-generated outputs. The results demonstrate that AI integration not only enhances efficiency but also extends user story definition to diverse application domains, contributing to more scalable and adaptable software development practices."
Secure Intelligence Development Lifecycle (SIDL) Model for Vulnerability Detection,"Dhanush, V; Chandra, TS; Divakarla, U; Chandrasekaran, K",10.1007/978-3-031-83790-6_6,2025,"This paper presents the Secure Intelligence Development Life-Cycle, a Software Development Life-Cycle created to address the need for advanced security and intelligent technologies, such as artificial intelligence and machine learning, at the development level. It differs from standard software development models that usually involve security measures only in the later stages of development; this one integrates security measures right from the early stage of requirements gathering through design, development, and deployment. The embedding of security from the onset allows real-time vulnerability detection and response with adaptability to emerging threats. It has also improved threat detection, made better decisions, and enhanced system resilience with intelligent processes. It is most effective with sectors like healthcare, finance, and critical infrastructure for secure and complex software systems."
My Code Is Less Secure with Gen AI: Surveying Developers' Perceptions of the Impact of Code Generation Tools on Security,"Kudriavtseva, A; Hotak, NA; Gadyatskaya, O",10.1145/3672608.3707778,2025,"Background: Generative AI (GAI) tools like GitHub Copilot and ChatGPT are transforming software development by automating code generation and enhancing developers' productivity. However, since these tools are often trained on open-source repositories, they may inadvertently reproduce vulnerable code, raising concerns about the security of AI-generated outputs. Aims: In this paper, we aim to investigate how developers perceive code security when using GAI tools. Method: We conducted a survey with 105 software developers with diverse experience levels to gather their perceptions regarding the security of generated code and their suggestions for improving it. Results: While developers reported increased development speed when using GAI tools, many spend additional time on security reviews and documentation of the generated code, and they are worried about the overreliance on AI and vulnerabilities in the code. Only about a quarter of the developers expressed confidence in the code generated by AI, and, moreover, experienced developers perceive that their proficiency in secure coding decreases when using GAI tools. Our results provide organizations with a better understanding of the risks associated with GAI tools and help improve their software security programs."
Learning without Limits: Analysing the Usage of Generative AI in a Summative Assessment,"Clift, L; Petrovska, O",10.1145/3702212.3702214,2025,"This paper explores how Generative AI (GenAI) can be introduced within summative assessment components in software engineering education. We present an example of an assessment which allows learners to use GenAI in a freeform, constructionist manner, as part of a large, software development project. This work is inspired by previously executed AI-focused assessments and surveys, which explicitly indicate that learners on an Applied Software Engineering Degree Apprenticeship Programme want to formally learn how to use GenAI tools when programming and their employers want to see these skills from graduates. The learning outcome of the assignment was for learners to explore a typical developmental pipeline as a solo developer, moving from design to development to finished product. Learners were marked exclusively on their end product and understanding of application components, not the written code itself, resulting in an assessment where the end product and project were prioritised over foundational code (which was adequately assessed in other components). The results show that all learners used GenAI to some extent during their project, and in all cases, they found it beneficial for large programming tasks. Learners were generally able to produce a larger, more comprehensive and more ambitious project, compared to previous years. It is proposed that removing the barrier to GenAI - and demystifying it - can encourage a constructionist approach to its use, and normalise it as a potential tool for programming."
A Field Study on the Use of Gen AI to Support Computing Education,"Ocay, A; Rodrigo, MM",10.1007/978-3-031-93724-8_17,2025,"This work presents a threefold study that investigates the potential use of Gen AI tools in computing education through perspectives from the industry and the academe. Generative AI tools in programming education have shown potential advantages to programming instruction for learners and educators. These tools provide benefits such as assisted problem-solving, code-generation capabilities, assistance in debugging codes, and other programming-related tasks. While integrating these tools in programming education promises conceivable potential, its implementation as an education practice can also pose a challenge to pedagogical design, to answer whether these tools help learning or impede the acquisition of students' programming skills and knowledge. The study employs mixed-methods data analysis, integrating quantitative and qualitative statistical and data collection procedures. The target participants of this threefold study are industry practitioners, teachers, and students in computing education. Data collection procedures and instruments were developed and tested for use in the study. The experimental research design was employed to investigate the effect of Chat GPT as the Gen AI tool in computer programming tasks among first-year computing students. Our preliminary results show that industry practices fully adopt an AI-based software development workflow to improve developer productivity while teaching methods remain constant using a mix of traditional and innovative methods, which is evident from the teacher-made machine problem sets assigned to students. Student performance has significantly improved with the use of Chat GPT in the experimental study compared to students without the use of Gen AI, which coincides with existing studies."
Comparative Analysis of AI Models for Python Code Generation: A HumanEval Benchmark Study,"Bayram, A; Dalveren, GGM; Derawi, M",10.3390/app15189907,2025,"This study conducts a comprehensive comparative analysis of six contemporary artificial intelligence models for Python code generation using the HumanEval benchmark. The evaluated models include GPT-3.5 Turbo, GPT-4 Omni, Claude 3.5 Sonnet, Claude 3.7 Sonnet, Claude Sonnet 4, and Claude Opus 4. A total of 164 Python programming problems were utilized to assess model performance through a multi-faceted methodology incorporating automated functional correctness evaluation via the Pass@1 metric, cyclomatic complexity analysis, maintainability index calculations, and lines-of-code assessment. The results indicate that Claude Sonnet 4 achieved the highest performance with a success rate of 95.1%, followed closely by Claude Opus 4 at 94.5%. Across all metrics, models developed by Anthropic Claude consistently outperformed those developed by OpenAI GPT by margins exceeding 20%. Statistical analysis further confirmed the existence of significant differences between the model families (p < 0.001). Anthropic Claude models were observed to generate more sophisticated and maintainable solutions with superior syntactic accuracy. In contrast, OpenAI GPT models tended to adopt simpler strategies but exhibited notable limitations in terms of reliability. These findings offer evidence-based insights to guide the selection of AI-powered coding assistants in professional software development contexts."
AI for Productivity: Transforming Enterprise Software Development,"Jain, A; Tiwari, R",10.1109/MC.2025.3543181,2025,"Through a survey study and real-world examples, this study explores how artificial intelligence (AI)-powered project management, code generation, automated testing, and AI-enhanced DevOps are revolutionizing development workflows. The article also highlights best practices for implementing AI effectively and the key considerations and challenges involved, along with an analysis of the emerging trends and the potential of AI in driving continuous improvement in enterprise environments."
Uncovering the Challenges: A Study of Corner Cases in Bug-Inducing Commits,"Serifoglu, A; TÃ¼zÃ¼m, E",10.1109/SANER64311.2025.00066,2025,"In software development, accurately identifying bug-inducing commits (BICs) is crucial for maintaining code integrity and ensuring the reliability of software systems. The complexities involved in pinpointing the exact commits responsible for bugs necessitate a thorough investigation of the underlying issues and limitations of existing tools and algorithms. This study investigates and identifies corner cases in BIC identification, clarifying definitions and examining issues with existing algorithms and tools. By analyzing these cases, we aim to reveal challenges faced by current methods and propose insights for future improvements. We evaluated the SZZ algorithm and two large language models, GPT-4o and Llama 3.1, using a curated repository of corner-case bugs with detailed reports. This setup allowed us to assess the strengths and weaknesses of both traditional algorithms and LLMs. The SZZ algorithm achieved a recall of 0.8 and a precision of 0.36, resulting in an F1 score of 0.5 for corner cases and a recall of 1 and a precision of 0.5 for non-corner cases with an F1 score of 0.67. In comparison, the LLMs showed varied performance: for corner cases, Llama had an MRR of 0.7, while GPT scored 0.5. For non-corner cases, both models performed better, with an MRR of 0.875. Corner cases in BIC identification expose limitations in current methods, emphasizing the need for improved approaches to accurately handle these challenges."
Seamful AI for Creative Software Engineering: Use in Software Development Workflows,"Inman, S; Murillo, A; D'Angelo, S; Brown, A; Green, C",10.1109/MS.2025.3534085,2025,"We explore the differences in goals for designing AI tools for productivity compared to creativity and propose strategies to elevate creativity in the software engineering workflow. Specifically, we consider the role of seamfulness in software development workflows as a way to support creativity."
Smart Grid Assistive AI in Requirement Engineering: Improving the Modeling of Use Cases and Architecture Models with LLMs,"Kuchenbuch, R; Lehnhoff, S; Sauer, J",10.1145/3679240.3734601,2025,"The IEC 62559 Use Case Methodology and Smart Grid Architecture Model (SGAM) Framework are crucial for fostering a shared understanding in the development of ICT-based power systems and their components within energy-related Requirements Engineering. However, discrepancies in interpretation among diverse stakeholders often diminish the quality of IEC 62559 Use Cases and SGAM Models, potentially leading to errors and costly setbacks in later project stages. This paper introduces the Smart Grid Assistive AI in Requirements Engineering (SGAAIRE), an intelligent system that leverages Large Language Models to enhance the quality of IEC 62559 Use Case descriptions and SGAM Models. Through a demonstration scenario featuring built-in quality defects, SGAAIRE is shown to effectively address common issues as an extension to a Use Case Management Repository. The proposed system supports the improvement of IEC 62559 Use Cases and SGAM Models, enabling the development of third-party tools aimed at advancing research in AI for energy-related requirements management."
Enhancing Pull Request Reviews: Leveraging Large Language Models to Detect Inconsistencies Between Issues and Pull Requests,"Isik, AT; Ãaglar, HK; TÃ¼zÃ¼n, E",10.1109/Forge66646.2025.00027,2025,"Context: Efficient Pull Request (PR) review process is critical in software development. This process includes checking the alignment between PRs and their corresponding issues. The traditional manual PR review often struggles with identifying inconsistencies between the intended improvements or fixes outlined in issues and the actual changes proposed in PRs. This difference can lead to overlooked inconsistencies in the PR acceptance process. Objective: We aim to enhance the PR review process by leveraging modern LLMs to detect inconsistencies between issue descriptions and code changes in submitted PRs. Method: We manually labeled a statistically significant sample of PRs from the Transformers repository to assess their alignment with corresponding issue descriptions. Each PR was categorized into one of four groups: exact, missing, tangling, or missing and tangling. This labeled dataset served as the benchmark for evaluating the performance of four widely used models: Llama-3.1-70B-Instruct, Llama-3.1-405B-Instruct, GPT-4o, and GPT-4o mini. The models were tested using three distinct prompts designed to capture different aspects of issues and PRs. Each model was tasked with identifying tangled and missing elements, and their outputs were compared against the manually labeled data to assess their accuracy and reliability. Results: The manual labeling process in the stratified-sampled Transformers repository revealed the following distribution of PR-issue pair alignments: 68.04% were exact, 16.5% were missing, 13.40% were tangling, and 2.06% exhibited both missing and tangling characteristics. A strong correlation was observed between PR merge status and exact alignment, with 75.46% of merged PRs classified as exact, compared to only 29.03% of unmerged PRs. These findings highlight opportunities for improving the current code review process. For automated classification, the most effective prompt configuration combined issue text, PR text, and PR diff, enabling better detection of alignment inconsistencies. Among the models tested, GPT-4o and Llama-3.1-405B-Instruct delivered the highest performance, achieving the best F1 weighted scores of 0.5948 and 0.6190, respectively. Conclusion: Despite a notable correlation between PR merge status and exact alignment, our analysis revealed that merged PRs can still contain inconsistencies, such as missing or tangling changes. While the tested LLMs showed potential in automating PR-issue alignment, their current performance is limited. This underscores the need for further refinement to enhance their accuracy and reliability. Improved LLM-based tools could streamline the PR review process, reducing manual effort and enhancing code quality."
Bridging the gap between user stories and feature models by leveraging version control systems: A step towards software product line migration,"Georges, T; Huchard, M; KÃ¶nig, M; Nebut, C; Tibermacine, C",10.1016/j.infsof.2025.107889,2025,"Context: Throughout the software lifecycle, a significant amount of knowledge is accumulated around the source code. In our work, we focus on agile software requirements, particularly user stories, and on issues and merge requests in version control systems, that have been opened for implementing user stories. Objective: The objective of this paper is to present a method that leverages this knowledge to guide an SPL migration. Methods: We consider merge requests in version control systems as the link between user stories (requirements) and the source code (implementation). The method combines Natural Language Processing (NLP) and clustering to identify features from user stories and hierarchically organize them. Relational Concept Analysis (RCA) is then used to compute logical rules from the hierarchy of features, using their links with the products and the source code. The logical rules are finally transformed into constraints in the produced feature model. Results: The method was implemented and evaluated on a dataset from an industrial partner. The results showed the efficiency of our method in synthesizing feature models for an SPL migration of the partner's code base. Conclusion: The proposed method synthesizes feature models to guide an SPL migration based on agile software development practices and demonstrates its effectiveness on a real industrial dataset."
Constructing Enterprise Digital Twins by Augmenting LLMs with MDE,"Barat, S; Mulpuru, D; Yadav, A; Korabu, R; Thogaru, H; Kulkarni, V",10.1145/3717383.3717391,2025,"Over the past year, Large Language Models (LLMs) have proven their value across a diverse range of industrial applications starting from supporting software development to automating customer interactions and enhancing process automation. We harness their potential for constructing Enterprise Digital Twins (EDTs), an emerging decision-making aid for a wide range of business sectors. EDT offers an effective in silico business experimentation leading to evidence-based informed decision-making, but its construction requires deep domain expertise spanning multiple aspects of enterprises across multiple stakeholders. Moreover, constructing an effective EDT demands seamless coordination between domain experts and expert modelers. These critical dependencies make the EDT construction challenging. This paper investigates the role of LLMs as domain experts and expert modelers to reduce excessive dependencies on both specializations and their coordination to an extent. Our approach integrates meta-modelling and Model Driven Engineering (MDE) techniques to effectively utilize LLMs with increased precision to alleviate the cognitive burden on domain experts and provide a systematic metamodel guided method for constructing purposive digital twins. We illustrate the approach and demonstrate its efficacy using a real-life EDT use case."
Exploring the Potential of Llama Models in Automated Code Refinement: A Replication Study,"Caumartin, G; Qin, QL; Chatragadda, S; Panjrolia, J; Li, H; Costa, DE",10.1109/SANER64311.2025.00070,2025,"Code reviews are an integral part of software development and have been recognized as a crucial practice for minimizing bugs and favouring higher code quality. They serve as an important checkpoint before committing code and play an essential role in knowledge transfer between developers. However, code reviews can be time-consuming and can stale the development of large software projects. In a recent study, Guo et al. assessed how ChatGPT3.5 can help the code review process. They evaluated the effectiveness of ChatGPT in automating the code refinement tasks, where developers recommend small changes in the submitted code. While Guo et al.'s study showed promising results, proprietary models like ChatGPT pose risks to data privacy and incur extra costs for software projects. In this study, we explore alternatives to ChatGPT in code refinement tasks by including two open-source, smaller-scale large language models: CodeLlama and Llama 2 (7B parameters). Our results show that, if properly tuned, the Llama models, particularly CodeLlama, can achieve reasonable performance, often comparable to ChatGPT in automated code refinement. However, not all code refinement tasks are equally successful: tasks that require changing existing code (e.g., refactoring) are more manageable for models to automate than tasks that demand new code. Our study highlights the potential of open-source models for code refinement, offering cost-effective, privacy-conscious solutions for real-world software development."
Data-Driven Predictive Modelling of Agile Projects Using Explainable Artificial Intelligence,"ForouzeshNejad, AA; Arabikhan, F; Gegov, A; Jafari, R; Ichtev, A",10.3390/electronics14132609,2025,"One of the fundamental challenges in managing software and information technology projects is monitoring and predicting project status at the end of each sprint, release or project. Agile project management has emerged over the past two decades, significantly impacting project success. However, no comprehensive approach based on the features of this approach has been found in studies to monitor and predict the status of a sprint, release or project. This study aims to develop a data-driven approach for predicting the status of software projects based on agility features. For this purpose, 22 agility features were first identified to evaluate and predict the status of projects in four aspects: Endurance, Effectiveness, Efficiency, and Complexity. The findings indicate that the aspects of Effectiveness and Efficiency have the greatest impact on project success. Additionally, the results show that features related to team work, team capacity, experience and project objectives have the most significant impact on project success. An artificial neural network algorithm was then used, and a model was developed to predict project status, which was optimized using the Neural Architecture Search algorithm with a 93 percent accuracy rate. The neural network model was interpreted using the SHapley Additive exPlanations (SHAP) algorithm, and sensitivity analysis was performed on the important components. Finally, the behavior of the projects in each category was analyzed and evaluated using the Apriori algorithm."
Generating Automotive Code: Large Language Models for Software Development and Verification in Safety-Critical Systems,"Kirchner, S; Knoll, AC",10.1109/IV64158.2025.11097503,2025,"Developing safety-critical automotive software presents significant challenges due to increasing system complexity and strict regulatory demands. This paper proposes a novel framework integrating Generative Artificial Intelligence (GenAI) into the Software Development Lifecycle (SDLC). The framework uses Large Language Models (LLMs) to automate code generation in languages such as C++, incorporating safety-focused practices such as static verification, test-driven development and iterative refinement. A feedback-driven pipeline ensures the integration of test, simulation and verification for compliance with safety standards. The framework is validated through the development of an Adaptive Cruise Control (ACC) system. Comparative benchmarking of LLMs ensures optimal model selection for accuracy and reliability. Results demonstrate that the framework enables automatic code generation while ensuring compliance with safety-critical requirements, systematically integrating GenAI into automotive software engineering. This work advances the use of AI in safety-critical domains, bridging the gap between state-of-the-art generative models and real-world safety requirements."
ChatGPT for Tailoring Software Documentation for Managers and Developers,"Bala, S; Sahling, K; Haase, J; Mendling, J",10.1007/978-3-031-72781-8_11,2025,"In many agile software development projects, documentation is often missing, outdated, or written with only a technical perspective. Existing literature recognizes the importance of documentation quality, especially when it comes to its readability for diverse audiences. While recent advances in Large Language Models (LLMs) offer the potential to tackle these issues, the use of LLMs for software documentation remains unexplored. This paper investigates the use of ChatGPT to improve and adapt documentation to specific audiences. We apply ChatGPT-4 for alternative documentation production and measure the resulting text characteristics and readability. Twenty-five experts from management and development rate these different versions. Results show the suitability of ChatGPT for generating high-quality text for both audiences, with managers benefiting more from an adapted version."
Knowledge-guided large language models are trustworthy API recommenders,"Wei, HW; Su, XH; Zheng, WN; Tao, WX; Yu, HL; Kuang, YQ",10.1007/s10515-025-00518-4,2025,"Application Programming Interface (API) recommendation aims to recommend APIs for developers that meet their functional requirements, which can compensate for developers' lack of API knowledge. In team-based software development, developers often need to implement functionality based on specific interface parameter types predefined by the software architect. Therefore, we propose APIRecommendation under specific Interface Parameter Types (APIRIP), a special variant of the API recommendation task that requires the recommended APIs to conform to the interface parameter types. To realize APIRIP, we enlist the support of Large Language Models (LLMs). However, LLMs are susceptible to the phenomenon known as hallucination, wherein they may recommend untrustworthy API sequences. Instances of this include recommending fictitious APIs, APIs whose calling conditions cannot be satisfied, or API sequences that fail to conform to the interface parameter types. To mitigate these issues, we propose a Knowledge-guided framework forLLM-based API Recommendation (KG4LLM), which incorporates knowledge-guided data augmentation and beam search. The core idea of KG4LLM is to leverage API knowledge derived from the Java Development Kit (JDK) documentation to enhance the trustworthiness of LLM-generated recommendations. Experimental results demonstrate that KG4LLM can improve the trustworthiness of recommendation results provided by LLM and outperform advanced LLMs in the APIRIP task."
Enhancing Stakeholder Analysis with AI: A Comparative Study of Productivity and Quality in the Educational Context,"Kanabar, V; Kaloyanova, K",10.4108/eetsis.9376,2025,"This paper examines the application of generative artificial intelligence in stakeholder management while studying the business aspects of software development and project management in two different universities. It explores a novel intersection of AI with software development and project management practices, offering valuable insights for both academia and industry. By investigating how students use AI alongside traditional methods under supervision, this study evaluates the effectiveness, quality of results, and creativity of students' project assignments in identifying stakeholders and defining communication strategies. The findings suggest that AI can enhance work completion speed and contribute to greater project success due to a more complete identification of stakeholders and formulation of innovative stakeholder engagement strategies. There is a consensus, within this context, that while AI can be invaluable for project stakeholder management, human judgment remains essential."
From Today's Code to Tomorrow's Symphony: The AI Transformation of Developer's Routine by 2030,"Qiu, KT; Puccinelli, N; Ciniselli, M; DI Grazia, L",10.1145/3709353,2025,"In the rapidly evolving landscape of software engineering, the integration of AI into the Software Development Lifecycle (SDLC) heralds a transformative era for developers. Recently, we have assisted to a pivotal shift have become a crucial element for coding, debugging, and software design. In this article, we provide a comparative analysis between the current state of AI-assisted programming in 2024 and our projections for 2030, by exploring how AI advancements are set to enhance the implementation phase, fundamentally altering developers' roles from manual coders to orchestrators of AI-driven development ecosystems. We envision HyperAssistant, an augmented AI tool that offers comprehensive support to 2030 developers, addressing current limitations in mental health support, fault detection, code optimization, team interaction, and skill development. We emphasize AI as a complementary force, augmenting developers' capabilities rather than replacing them, leading to the creation of sophisticated, reliable, and secure software solutions. Our vision seeks to anticipate the evolution of programming practices, challenges, and future directions, shaping a new paradigm where developers and AI collaborate more closely, promising a significant leap in SE efficiency, security, and creativity."
IT professionals trust in artificial intelligence vs. human experts for achieving sustainable development goals,"Glavas, D; Grolleau, G; Mzoughi, N",10.1016/j.sftr.2025.101153,2025,"This study investigates trust of information technology (IT) professionals in artificial intelligence (AI), human experts, and their combination for achieving Sustainable Development Goals (SDGs). Through a survey of IT project managers and frequent AI users across France, the United Kingdom, and Belgium, our findings reveal that respondents place significantly higher trust in human experts and combinations of AI and human expertise, compared to AI. Notably, individuals who most frequently use AI technology show stronger preference for human-AI collaborative approaches, suggesting that familiarity with AI leads to better understanding of its complementary role with human expertise. The study demonstrates that successful AI implementation for achieving SDGs requires careful integration with human oversight rather than standalone deployment."
Coordination Mechanisms in AI Development: Practitioner Experiences on Integrating UX Activities,"Bruun, A; van Berkel, N; Raptis, D; Law, ELC",10.1145/3706598.3713200,2025,"Software development relies on collaboration and alignment between a variety of roles, including software developers and user experience designers. The increasing focus on artificial intelligence in today's development projects has given rise to new challenges in this collaboration. We extend previous work on the process of designing human-AI systems by analysing collaborative practices between UX designers and AI developers through Mintzberg's theory on coordination mechanisms. We conducted 15 in-depth interviews with UX designers and AI developers currently working on AI projects. We contribute by identifying how coordination mechanisms impact the UX design process when developing AI systems, inter-team (a)symmetries in power relations, and a growing need for tools and cross-disciplinary knowledge to support these collaborative efforts. In particular, we outline the risks of coordinating AI development work through the standardisation of output and skills in separately organised UX and AI development teams."
Blending Language Models and Domain-Specific Languages in Computer Science Education. A Case Study on API RESTFul,"Jurado, F; Rodriguez, FD; Chavarriaga, E; Rojas, L",10.9781/ijimai.2025.09.005,2025,"Since Computer Science students are used to applying both General Purpose Programming Languages (GPPLs) and Domain-Specific Languages (DSLs), Generative Artificial Intelligence based on Language Models (LMs) can help them on automatic tasks, allowing them to focus on more creative tasks and higher skills. However, the teaching and evaluation of technical tasks in Computer Science can be inefficient and prone to errors. Thus, the main objective of this article is to explore the performance of LMs compared to that of undergraduate Computer Science students in a specific case study: designing and implementing RESTful APIs DSLs. This research aims to determine if LMs can enhance the efficiency and accuracy of these processes. Our case study involved 39 students and 5 different LMs that must use the two DSLs we also designed for their task assignment. To evaluate performance, we applied uniform criteria to student and LMs-generated solutions, enabling a comparative analysis of accuracy and effectiveness. With a case study comparing performance between students and LMs, this article contributes to checking to what extent LMs are able to carry out software development tasks involving the use of new DSLs specially designed for highly specific settings in a similar way as well-qualified Computer Science students are able to. The results underscore the importance of well-defined DSLs and effective prompting processes for optimal LM performance. Specifically, LMs demonstrated high variability in task execution, with two GPT-based LMs achieving similar grades to those scored by the best of the students for every task, obtaining 0.78 and 0.92 on a normalized scale [0, 1], with 0.23 and 0.14 Standard Deviation for ChatGPT-4 and ChatGPT-4o respectively. After the experience, we can conclude that a well-defined DSL and a proper prompting process, providing the LM with metadata, persistent prompts, and a good knowledge base, are crucial for good LM performance. When LMs receive the right prompts, both large and small LMs can achieve excellent results depending on the task."
SoTaNa: An Open-Source Software Engineering Instruction-Tuned Model,"Shi, ES; Wang, YL; Zhang, FJ; Chen, B; Zhang, HY; Wang, YL; Guo, DY; Du, L; Han, S; Zhang, DM; Sun, HB",10.1109/Forge66646.2025.00010,2025,"Software development plays a crucial role in driving innovation and efficiency in modern societies. To meet the demands of this dynamic field, there is a growing need for an effective software development assistant. However, existing large language models represented by ChatGPT suffer from limited accessibility, including training data and model weights. Although other large open-source models like LLaMA have shown promise, they still struggle with understanding human intent. In this paper, we present SoTaNa, an open-source software engineering instruction-tuned model. SoTaNa utilizes ChatGPT to generate high-quality instruction-based data for the domain of software engineering and employs a parameter-efficient fine-tuning approach to enhance the open-source foundation model, LLaMA. We evaluate the effectiveness of SoTaNa in answering Stack Overflow questions and demonstrate its capabilities. Additionally, we discuss its capabilities in code summarization and generation, as well as the impact of varying the volume of generated data on model performance. Notably, SoTaNa can run on a single GPU, making it accessible to a broader range of researchers. Our code, model weights, and data are publicly available at https://github.com/DeepSoftwareAnalytics/SoTaNa."
Novel Audiobook System Based on BERT,"Zhu, HZ; Wei, XX; Zhao, YN; Ling, XF; Zhu, YG; Liu, CY",10.1007/978-981-96-3349-4_15,2025,"With the help of advanced natural language processing technology, the novel audiobook system transforms traditional novel text into vivid sound. In this thesis, a design method for novel audiobooking system based on Bert is proposed, and a general and complete software development environment is constructed. The system uses the Python text processing library to preprocess the novel text, and converts the processed text into speech with emotion and intonation through the Edge-TTS speech synthesis engine, and supports personalized adjustment of multiple languages and speech speeds. The test and application effect show that the system has the characteristics of friendly interface and easy operation, and has strong practicability and application prospects."
Towards AI-Assisted Correctness-by-Construction Software Development,"Kodetzki, M; Bordis, T; Kirsten, M; Schaefer, I",10.1007/978-3-031-75387-9_14,2025,"In recent years, research on artificial intelligence (AI) has made great progress. AI-tools are getting better in simulating human reasoning and behavior every day. In this paper, we discuss the extent to which AI-tools can support Correctness-by-Construction (CbC) engineering. This is an approach of formal methods for developing functionally correct programs incrementally on the basis of a formal specification. Using sound refinement rules, the correctness of the constructed program can already be guaranteed in the development process. We analyze the CbC process regarding steps for potential AI-tool support in the tool CorC, which implements CbC. We classify the findings in five areas of interest. Based on existing work, expert knowledge, and prototypical experiments, we discuss for each of the areas whether and to what extent AI-tools can support CbC software development. We address the risk of AI-tools in formal methods and present our vision of AI-integration in the tool CorC to support developers in constructing programs using CbC engineering."
Enhancing Software Usability Through LLMs: A Prompting and Fine-Tuning Framework for Analyzing Negative User Feedback,"Alsaleh, N; Alnanih, R; Alowidi, N",10.3390/computers14090363,2025,"In today's competitive digital landscape, application usability plays a critical role in user satisfaction and retention. Negative user reviews offer valuable insights into real-world usability issues, yet traditional analysis methods often fall short in scalability and contextual understanding. This paper proposes an intelligent framework that utilizes large language models (LLMs), including GPT-4, Gemini, and BLOOM, to automate the extraction of actionable usability recommendations from negative app reviews. By applying prompting and fine-tuning techniques, the framework transforms unstructured feedback into meaningful suggestions aligned with three core usability dimensions: correctness, completeness, and satisfaction. A manually annotated dataset of Instagram negative reviews was used to evaluate model performance. Results show that GPT-4 consistently outperformed other models, achieving BLEU scores up to 0.64, ROUGE scores up to 0.80, and METEOR scores up to 0.90-demonstrating high semantic accuracy and contextual relevance in generated recommendations. Gemini and BLOOM, while improved through fine-tuning, showed significantly lower performance. This study also introduces a practical, web-based tool that enables real-time review analysis and recommendation generation, supporting data-driven, user-centered software development. These findings illustrate the potential of LLM-based frameworks to enhance software usability analysis and accelerate feedback-driven design processes."
Reimagining Unit Test Generation With AI: A Journey From Evolutionary Models to Transformers,"Esubalew, SZ; Assefa, BG",10.1109/ACCESS.2025.3597049,2025,"The rapid evolution of software development necessitates efficient unit testing to ensure reliability, yet manual test case generation is labor-intensive and often inadequate for agile workflows. Despite advancements, a comprehensive review of AI-driven unit test case generation, particularly for Java, is lacking, motivating this study to address this gap. The paper examines AI algorithms for unit test case generation, focusing on Java-specific challenges like class hierarchies and dependency injection. We propose a novel taxonomy categorizing methods into traditional machine learning (e.g., genetic algorithms, SVMs), deep learning (e.g., RNNs, GNNs), and transformer-based approaches (e.g., PLBART, enhanced by LoRA and QLoRA). Key contributions include: 1) a structured taxonomy for comparing AI methods based on effectiveness, usability, and maintainability; 2) a Java-specific focus addressing enterprise system complexities; and 3) identification of research gaps, such as scalability and assertion accuracy. Findings reveal that transformer-based models like A3Test and ChatUniTest achieve up to 59% test case correctness and 77% focal method coverage, outperforming traditional methods, though challenges in computational cost and assertion accuracy persist. Tools are evaluated for integration with CI/CD pipelines and advanced capabilities like parameter-efficient fine-tuning (PEFT) using LoRA and QLoRA. This review provides a roadmap for researchers and practitioners to advance automated, high-quality unit testing for Java software quality."
Streamlining medical software development with CARE lifecycle and CARE agent: an AI-driven technology readiness level assessment tool,"Hart, SN; Day, PL; Garcia, CA",10.1186/s12911-025-03099-0,2025,"BackgroundDeveloping medical software requires navigating complex regulatory, ethical, and operational challenges. A comprehensive framework that supports both technical maturity and clinical safety is essential for effective artificial intelligence and machine learning system deployment. This paper introduces the Clinical Artificial Intelligence Readiness Evaluator Lifecycle and the Clinical Artificial Intelligence Readiness Evaluator Agent-a framework and AI-driven tool designed to streamline technology readiness level assessments in medical software development.MethodsWe developed the framework using an iterative process grounded in collaborative stakeholder analysis. Key institutional stakeholders-including clinical informatics experts, data engineers, ethicists, and operational leaders-were engaged to identify and prioritize the regulatory, ethical, and technical requirements unique to clinical AI/ML development. This approach, combined with a thorough review of existing methodologies, informed the creation of a lifecycle model that guides technology maturation from initial concept to full deployment. The AI-driven tool was implemented using a retrieval-augmented generation strategy and evaluated through a synthetic use case (the Diabetes Outcome Predictor). Evaluation metrics included the proportion of correctly addressed assessment questions and the overall time required for automated review, with human adjudication validating the tool's performance.ResultsThe findings indicate that the proposed framework effectively captures the complexities of clinical AI development. In the synthetic use case, the AI-driven tool identified that 32.8% of the assessment questions remained unanswered, while human adjudication confirmed discrepancies in 19.4% of these instances. These outcomes suggest that, when fully refined, the automated assessment process can reduce the need for extensive multi-stakeholder involvement, accelerate project timelines, and enhance resource efficiency.ConclusionsThe Clinical Artificial Intelligence Readiness Evaluator Lifecycle and Agent offer a robust and methodologically sound approach for evaluating the maturity of medical AI systems. By integrating stakeholder-driven insights with an AI-based assessment process, this framework lays the groundwork for more streamlined, secure, and effective clinical AI development. Future work will focus on optimizing retrieval strategies and expanding validation across diverse clinical applications."
Enhancing Software Quality with AI: A Transformer-Based Approach for Code Smell Detection,"Ali, I; Rizvi, SSH; Adil, SH",10.3390/app15084559,2025,"Software quality assurance is a critical aspect of software engineering, directly impacting maintainability, extensibility, and overall system performance. Traditional machine-learning techniques, such as gradient boosting and support vector machines (SVM), have demonstrated effectiveness in code smell detection but require extensive feature engineering and struggle to capture intricate semantic dependencies in software structures. In this study, we introduce Relation-Aware BERT (RABERT), a novel transformer-based model that integrates relational embeddings to enhance automated code smell detection. By modeling interdependencies among software complexity metrics, RABERT surpasses classical machine-learning methods, achieving an accuracy of 90.0% and a precision of 91.0%. However, challenges such as low recall (53.0%) and computational overhead indicate the need for further optimization. We present a comprehensive comparative analysis between classical machine-learning models and transformer-based architectures, evaluating their computational efficiency and predictive capabilities. Our findings contribute to the advancement of AI-driven software quality assurance, offering insights into optimizing transformer-based models for practical deployment in software development workflows. Future research will focus on lightweight transformer variants, cost-sensitive learning techniques, and cross-language generalizability to enhance real-world applicability."
Adaptive Software Development: A Comprehensive Framework Integrating Artificial Intelligence for Sustainable Evolution,"Soureya, YG; Amougou, N; Ngossaha, JM; Tsakou, SB; Ndjodo, MF",10.34028/iajit/22/2/4,2025,"This research presents an innovative methodological framework for software development that integrates Artificial Intelligence (AI) techniques, Software Product Lines (SPL), and Lehmans'[24] aging factors. The main objective is to improve the efficiency and adaptability of design processes for residential spaces through intelligent automation. This framework covers the entire software development life cycle, utilizing AI algorithms to optimize design and respond to the evolving needs of users while maximizing resource usage. A case study on a connected home concretely illustrates the application of this framework, demonstrating its effectiveness in creating dynamic and personalized designs. Furthermore, it addresses the issue of software sustainability by incorporating aging laws throughout their life cycle, an aspect often overlooked in existing solutions. By combining product line engineering and AI techniques, this framework offers a structured approach that promotes both sustainability and personalization. It has the potential to transform practices across various sectors, such as healthcare, finance, and education, while fostering a culture of sustainable innovation. However, its effectiveness also depends on the skills and experience of development teams, highlighting the importance of considering human factors in its application."
"Mindsets, Cultures, and Technologies in Support of End-User Development","Fischer, G",10.1007/978-3-031-95452-8_18,2025,"End-User Development (EUD) represents a fundamental shift in how individuals engage with technology, enabling them to transition from passive consumers to active co-creators and designers of their digital environments. While much of the existing research focuses on the technological aspects of EUD, this paper argues that its success depends equally on cultivating the right mindsets and participatory cultures that foster sustained engagement and innovation. By taking an interdisciplinary approach (drawing from software development, learning sciences, and socio-technical system design), our research has examined key enablers of EUD, including meta-design principles, cultures of participation, and distributed cognition. Of equal importance is the analysis of critical challenges that hinder the broader adoption of EUD, such as overcoming passive mindsets, reducing the learning effort required for meaningful participation, mitigating participation overload, and striking a balance between user autonomy and structured guidance. Grounded in long-term research activities and diverse perspectives on the desirability of EUD, we propose a framework that integrates mindsets, cultures, and technologies to guide future design directions, including (1) expanding EUD across diverse domains to foster broader adoption and impact; (2) examining the mutual influences between AI and EUD; and (3) deriving design guidelines for leveraging ChatGPT tools to augment rather than replace human creativity and learning."
Automation of Monitoring and Optimization of Docker Containers Using Artificial Intelligence,"Stanisic, S; Lukovic, V; Belotic, B",10.1109/INFOTEH64129.2025.10959216,2025,"The increasing adoption of Docker containers in modern software development requires effective monitoring and optimization to ensure high performance and efficient resource utilization. This paper provides an overview of the various tools and technologies that leverage artificial intelligence (AI) to automate the monitoring and optimization of Docker containers. The authors explore the capabilities of AI-driven tools such as machine learning-based anomaly detection, predictive analytics, and automated resource management. By examining these tools, the authors highlight their benefits in enhancing container performance, scalability, and availability. Through comparative analysis, the strengths and limitations of each tool and their impact on container management are discussed. The findings underscore the potential of AI to revolutionize container monitoring and optimization, leading to more efficient and robust IT infrastructure."
Artificial Intelligence (AI) and Knowledge-Based Engineering (KBE) in Ship Design: Bridging Tradition and Technology Through ACQUAINT,"Shahzad, T; Wang, P; van Lith, P; Hoffmans, J",10.5957/JSPD.01240002,2025,"Despite the limited use of artificial intelligence/knowledge-based engineering (AI/KBE) in industries with small series or one-off designs, our study demonstrates the technical feasibility and potential benefits of implementing AI/KBE in ship design processes. This research presents the development of ACQUAINT, uniquely designed to address the complexities inherent in bespoke shipbuilding. Central to this module is a robust AI-driven inference engine, integrated seamlessly with AutoCAD through a Python-based interface, facilitating a novel approach in shipbuilding's detail and production design phases. The module's capability to generate optimal designs autonomously-grounded in a deep understanding of design rules, constraints, and requirements-substantially reduces the reliance on human interaction. Our initial proof of concept with ACQUAINT showcases measurable advancements in ship design accuracy and efficiency, highlighting AI KBE's transformative impact in shipbuilding and setting a foundation for future research and practical applications."
Learning AI Coding Style for Software Plagiarism Detection,"Ambati, SH; Stakhanova, N; Branca, E",10.1007/978-3-031-64954-7_24,2025,"Software plagiarism is the reuse of software code without proper attribution and in violation of software licensing agreements or copyright laws. With the popularity of open-source software and the rapid emergence of AI Large Language Models such as ChatGPT and Google Bard, the concerns of plagiarized AI-generated code have been rising. Code attribution has been used to aid in the detection of software plagiarism cases. In this paper, we investigate the authorship of AI-generated code. We analyze the feasibility of code attribution approaches to verify authorship of source code generated by AI-based tools and investigate scenarios when plagiarized AI code can be identified. We perform an attribution analysis of an AI-generated source code on a large sample of programs written by software developers and generated by ChatGPT and Google Bard tools. We believe our work offers valuable insights for both academia and the software development community while contributing to the research in the authorship style of the fast-growing AI conversational models, ChatGPT and Bard."
How Do Computer Science Students Perceive Self-Study with Open-Source Repositories for Building AI/ML Systems?,"Azamnouri, A; Koch, NN; Bogner, J; Wagner, S",10.1109/CSEET66350.2025.00046,2025,"The world of software development has fundamentally changed because of the explosive growth of opensource repositories in recent years. Open-source repositories have become a valuable tool for software developers and researchers because they are free and usually easy to use. Likewise, learning Artificial Intelligence (AI) and Machine Learning (ML) skills are in high demand, especially among software engineering students, as they increasingly require AI skills to drive innovation, solve complex problems, and remain competitive. There are several AI/ML open-source projects that contain code explanations, e.g., comments and/or documentation, making them potential educational tools. However, it is currently unclear how well AI novices can benefit from these resources. Hence, we studied how computer science bachelor students perceive self-study with open-source repositories to build more complex AI/ML systems to gauge the usefulness of these repositories. After a learning period, we surveyed the perception and learning outcomes from the viewpoint of 112 students. By analyzing the responses, we found that 75% of the students stated that they could now build complex AI/ML systems if provided with enough documentation and descriptions and are motivated to work on them. While this indicates that learning or improving AI/ML skills via open-source repositories is promising, more research beyond self-reporting is needed."
AI for DevSecOps: A Landscape and Future Opportunities,"Fu, M; Rasuksmit, J; Tantithamthavorn, C",10.1145/3712190,2025,"DevOps has emerged as one of the most rapidly evolving software development paradigms. With the growing concerns surrounding security in software systems, the DevSecOps paradigm has gained prominence, urging practitioners to incorporate security practices seamlessly into the DevOps workflow. However, integrating security into the DevOps workflow can impact agility and impede delivery speed. Recently, the advancement of artificial intelligence (AI) has revolutionized automation in various software domains, including software security. AI-driven security approaches, particularly those leveraging machine learning or deep learning, hold promise in automating security workflows. They have the potential to reduce manual efforts and can be incorporated into DevOps practices to support consistent delivery speed while aligning with the principles of the DevSecOps paradigm. This article seeks to contribute to the critical intersection of AI and DevSecOps by presenting a comprehensive landscape of AI-driven security techniques applicable to DevOps and identifying avenues for enhancing security, trust, and efficiency in software development processes. We analyzed 99 research papers spanning from 2017 to 2023. Specifically, we address two key research questions (RQs). In RQ1, we identified 12 security tasks associated with the DevSecOps process and reviewed existing AI-driven security approaches, the problems they addressed, and the 65 benchmarks used to evaluate those approaches. Drawing insights from our findings, in RQ2, we discussed state-of-the-art AI-driven security approaches, highlighted 15 challenges in existing research, and proposed 15 corresponding avenues for future opportunities."
Mapping system requirements to emotional impact: a semi-automated approach,"Roy, M; Deb, N; Cortesi, A; Chaki, N",10.1007/s11334-025-00618-0,2025,"Software has become an integral part of modern life, serving various purposes in personal, professional, and recreational domains. This research addresses the growing need for software systems that are not only functional but also emotionally resonant with users. While traditional software development focuses on functional and non-functional requirements, the emotional impact of systems on users, referred to as emotional requirements (ERs), remains underexplored. Existing research on ERs has limitations, focusing primarily on specific domains like healthcare or gaming and employing manual, data-dependent elicitation methods. This work proposes a novel and semi-automated approach to derive ERs from system requirements. The proposed methodology addresses three main concerns: (1) identifying requirements with potential emotional triggers, (2) defining a software-specific taxonomy of ERs, and (3) proposing appropriate artifacts and methodology for ER elicitation. A two-way approach is proposed for ER elicitation using formal goal modeling and generative AI. The framework leverages insights from emotional knowledge bases, user behavior patterns, and contextual analysis to systematically derive ERs from requirements. The proposed methodology is experimentally evaluated using case studies of software of different domains."
Self-Assembled Plasmonic Magnifier: A New Platform for Ultra-Sensitive Detection of Respiratory Viruses Using Surface-Enhanced Raman Spectroscopy,"Guo, LX; Wang, QY; Xing, YW; Zhao, XJ; Ma, RH; Liu, DP; Gao, P; Li, Y",10.1021/acs.analchem.5c02722,2025,"Accurate and sensitive pathogen detection is critical for the prevention and control of respiratory viral infections, which pose significant threats to global health, particularly for vulnerable populations such as children, the elderly, and immunocompromised individuals. Here, we present a novel detection platform, Surface-Enhanced Raman Scattering combined with Artificial Intelligence (SERS-AI). At the core of this platform lies the independently developed self-assembled plasmonic magnifier (SPM), an enhanced substrate. Unlike traditional SERS substrates that rely on random aggregation or prefabricated nanostructures, this platform employs a virus-triggered self-assembly mechanism. Through the electrostatic attraction of C12 DNA molecules and the aggregation regulation of calcium ions, the self-assembled plasmonic magnifier (SPM) can significantly increase the probability of forming highly localized plasmonic hotspots near viral particles. This virus-associated hotspot formation strategy, which enhances the correlation between hotspot distribution and viral particles, significantly improves the specificity, intensity, and reproducibility of signals. Integrating surface-enhanced Raman spectroscopy (SERS) with artificial intelligence (AI) technology, the platform enables rapid, accurate, and label-free identification and quantitative analysis of respiratory viruses. The platform demonstrated exceptional sensitivity and reproducibility in detecting respiratory syncytial virus, human adenovirus type 5, influenza B virus, and H1N1 virus, with unique SERS fingerprints showing strong linear correlations with viral concentrations. The AI-driven spectral analysis allowed accurate differentiation of these viruses in serum and saliva samples, achieving detection within 2 min. Detection limits reached as low as 5 x 10-5 copies/mL demonstrating robustness and reliability even in complex biological matrices. This SERS-AI-SPM platform represents a significant breakthrough in SERS technology by integrating advanced nanomaterial engineering with AI-powered data analysis. Its rapid, sensitive, and reliable performance underscores its transformative potential in clinical diagnostics, large-scale epidemic prevention, and personalized medicine. This innovation provides a powerful tool for real-time infectious disease monitoring and public health management."
Specification Completion for Sustainable Software Development via Sustainability-Driven Mining,"Ailane, MT; Rubner, C; Rausch, A",10.1109/GREENS66463.2025.00005,2025,"In an era where digital transformation intersects with environmental sustainability, the software development industry must integrate practices that minimize ecological impacts. This paper proposes a comprehensive framework for specification completion through specification mining, aimed at embedding sustainability into software development processes. Utilizing an Extended Abstraction Refinement Model (EARM), we enhance the DevOps lifecycle by mapping observed behavior to development artifacts, ensuring sustainability metrics are accessible and actionable for stakeholders such as requirements engineers, software architects, and developers. Our approach leverages emergent behavior analysis to identify sustainability impacts that manifest during runtime, enabling targeted, energy-efficient interventions without incurring the rebound effect of exhaustive evaluations. By incorporating natural language processing (NLP) techniques for automated specification mapping, the framework refines software models iteratively, integrating real-world sustainability insights. This methodology supports the reduction of the carbon footprint in software products while preserving performance and quality, contributing to the alignment of software engineering with global sustainability objectives."
Investigating the Impact of AI-Assisted Tools on Software PractitionerWell-Being,"Meem, FN; Johnson, B",10.1145/3707640.3731915,2025,"The increasing adoption of AI-assisted tools, like ChatGPT, in software development presents both opportunities and challenges. While these tools enhance productivity and streamline tasks, they also introduce new job demands, such as cognitive overload, that can impact practitioners' well-being. Well-being in this context includes mental, emotional, and physical health factors, such as job satisfaction, stress, burnout, and engagement. This paper presents preliminary findings from a study grounded in the Job DemandsResources (JD-R) model aimed at exploring how AI-assisted tools impact practitioners' well-being. We designed a comprehensive survey to investigate key factors such as job demands, organizational and social resources, and their interplay. Our findings thus far suggest that while AI-assisted tools can increase productivity and improve focus, it can also cause mental fatigue, cognitive strain, and blurred work-life boundaries. Insights from our ongoing efforts provide a critical foundation for responsible integration of AI-assisted tools in software development."
Business Logic Vulnerabilities in the Digital Era: A Detection Framework Using Artificial Intelligence,"Metin, B; Wynn, M; Tunali, A; Kepir, Y",10.3390/info16070585,2025,"Digitalisation can positively impact the efficiency of real-world business processes, but may also introduce new cybersecurity challenges. One area that is particularly vulnerable to cyber-attacks is the business logic embedded in processes in which flaws may exist. This is especially the case when these processes are within web-based applications and services, which is increasingly becoming the norm for many organisations. Business logic vulnerabilities (BLVs) can emerge following the software development process, which may be difficult to detect by vulnerability detection tools. Through a systematic literature review and interviews with industry practitioners, this study identifies key BLV types and the challenges in detecting them. The paper proposes an eight-stage operational framework that leverages Artificial Intelligence (AI) for enhanced BLV detection and mitigation. The research findings contribute to the rapidly evolving theory and practice in this field of study, highlighting the current reliance on manual detection, the contextual nature of BLVs, and the need for a hybrid, multi-layered approach integrating human expertise with AI tools. The study concludes by emphasizing AI's potential to transform cybersecurity from a reactive to a proactive defense against evolving vulnerabilities and threats."
SynergyBug: A deep learning approach to autonomous debugging and code remediation,"Chen, H",10.1038/s41598-025-08226-5,2025,"Bug detection and resolution are pivotal to maintaining the quality, reliability, and performance of software systems. Manual debugging, along with traditional static rule-based methods, proves inefficient when applied to complex software structures in contemporary times. SynergyBug combines BERT and GPT-3 to autonomously detect and repair bugs across multiple sources. It resolves essential requirements by implementing an automated system that diagnoses and resolves software bugs automatically, thus minimising human involvement. The framework unites BERT as a contextual machinery with GPT-3 to produce bug fix generation capabilities. The semantic pattern within bug reports, together with error logs and documentation, feeds into BERT for contextual embedding generation. GPT-3 applies the generated embeddings to produce code fixes, code snippets, as well as detailed explanations that address detected problems. The system achieves continuous automatic debugging by enhancing both detection and resolution steps into one unified process. The experimental outcomes prove that it achieves superior performance than conventional bug detection methods by reaching 98.79% accuracy alongside 97.23% precision and 96.56% recall. The system demonstrated exceptional detection strength for functional and performance, and security bugs, where the detection rates reached 94% and 90% and 92%, respectively. SynergyBug showed its ability to expand as it processed bug reports exceeding 100,000 cases without noticeably impacting system performance. This proposed system provides faster debugging capabilities to improve the quality of the complete software development process. This paper discusses as a tool that can revolutionise bug management through proactive instead of just reactive strategies. The implementation of human monitoring within safety programs and managing training system biases represent essential organisational factors. The study terminates by recognising SynergyBug as a crucial development leading toward automated debugging tools that maintain operational safety within intricate software systems."
A Framework for Compliance with Regulation (EU) 2024/1689 for Small and Medium-Sized Enterprises,"Stampernas, S; Lambrinoudakis, C",10.3390/jcp5030040,2025,"The European Union's Artificial Intelligence Act (EU AI Act) is expected to be a major legal breakthrough in an attempt to tame AI's negative aspects by setting common rules and obligations for companies active in the EU Single Market. Globally, there is a surge in investments to encourage research, development and innovation in AI that originates both from governments and private firms. The EU recognizes that the new Regulation (EU) 2024/1689 is difficult for start-ups and SMEs to cope with and it announced the release of tools, in the near future, to ease that difficulty. To facilitate the active participation of SMEs in the AI arena, we propose a framework that could assist them to better comply with the challenging EU AI Act during the development life cycle of an AI system. We use the spiral SDLC model and we map its phases and development tasks to the legal provisions of Regulation (EU) 2024/1689. Furthermore, the framework can be used to promote innovation, improve their personnel's expertise, reduce costs and help the companies avoid the proposed substantial fines described in the Act."
ChatGPT Choreography: Discovering Developer Dialogues and Potential Software Development Lifecycle Applications,"Swaraj, A; Kumar, S; Sharma, LM",10.1109/MC.2025.3532347,2025,"We analyze a large dataset of 17,913 ChatGPT-developer dialogues using topic modeling and thematic analysis. Our multimethod approach identifies 10 themes, revealing key insights into ChatGPT's application across software development lifecycle tasks and the time required for effective issue resolution."
RAG-Driven multiple assertions generation with large language models,"Liu, Z; Wang, HL; Xu, TT; Wang, B",10.1007/s10664-025-10641-1,2025,"Software testing is one of the most crucial parts of the software development life cycle. Developers spend substantial amount of time and effort on software testing. Recently, there has been a growing scholarly interest in the automation of software testing. However, recent studies have revealed significant limitations in the quality and efficacy of the generated assert statements. These limitations primarily arise due to: (i) the inherent complexity involved in generating assert statements that are both meaningful and effective; (ii) the challenge of capturing the relationship between multiple assertions in a single test case. In recent research, deep learning techniques have been employed to generate meaningful assertions. However, it is typical for a single assertion to be generated for each test case, which contradicts the current situation where over 40% of test cases contain multiple assertions. Compared with deep learning techniques, the advantages of large language models (LLMs) in test generation tasks have been proven. This paper proposes a new approach named ALLMAssert (Augmented Large Language Model Assertion Generation) to automatically generate multiple assertions for test methods. ALLMAssert exploits two LLMs to collaboratively generate test assertions for developers. ALLMAssert first fine-tune the codellama-34B-instruct model to obtain a specialized model for multi-assert generation. We then mine more contextual information in the Java project. Through a series of information augmentation steps, we prompt the base LLM to correct the assert statements generated by the fine-tuned LLM. To evaluate the effectiveness of our approach, we conduct extensive experiments on the dataset built on the top of Methods2Test dataset. Experimental results show that ALLMAssert achieves scores of 56.61%, 20.43%, and 15.07% in terms of CodeBLEU, accuracy and perfect prediction and substantially outperforms the baselines. Furthermore, we evaluate the effectiveness of ALLMAssert on the task of bug detection and the result indicates that the assert sequences generated by ALLMAssert can assist in exposing 76 real-world bugs extracting from Defects4J, outperforming the SOTA approaches by a large margin as well."
Security Analysis of Automated Code Generation: Structural Vulnerabilities in AI-Generated Code,"Yoo, SH; Kim, HJ",10.31803/tg-20250225095135,2025,"AI-driven code generation enhances operational efficiency; however, it also introduces security vulnerabilities due to insufficient human oversight during development. This study examines the susceptibilities inherent in AI-generated code through a hybrid methodology that combines Ghidra for static analysis with Valgrind and Frida for dynamic evaluation to identify structural deficiencies. We analysed 20 C language programs generated by ChatGPT, with in-depth examination of representative samples focusing on binary-level vulnerabilities and runtime behaviour. Our findings reveal that AI-generated code contains 6.4% more vulnerabilities than human-written equivalents, with significantly higher rates in network security (+18.8%), file operations (+12.4%), and error handling (+12.4%). Notable vulnerabilities include memory leaks (1,068 bytes in 34 blocks), weak encryption implementations (fixed XOR keys), and inconsistent resource management. Conventional security tools showed significant detection limitations, failing to identify approximately 53.3% of vulnerabilities in AI-generated code-a 19.7% lower detection efficiency compared to human-written code. Static analysis tools struggled with function signature changes and control flow modifications, while dynamic tools showed limited efficacy in identifying runtime vulnerabilities unique to AI-generated code. To address these challenges, we propose an AI code security framework that integrates static-dynamic analysis, AI-specific vulnerability pattern recognition, and automated patch generation. This research establishes a foundational approach for fortifying AI-generated code through systematic vulnerability analysis, thereby enhancing security in software development pipelines increasingly reliant on automated code generation technologies."
Quantitative Assessment of Generative Large Language Models on Design Pattern Application,"Kim, DK",10.32604/cmc.2025.062552,2025,"Design patterns offer reusable solutions for common software issues, enhancing quality.The advent of generative large language models (LLMs) marks progress in software development, but their efficacy in applying design patterns is not fully assessed. The recent introduction of generative large language models (LLMs) like ChatGPT and CoPilot has demonstrated significant promise in software development. They assist with a variety of tasks including code generation, modeling, bug fixing, and testing, leading to enhanced efficiency and productivity. Although initial uses of these LLMs have had a positive effect on software development, their potential influence on the application of design patterns remains unexplored. This study introduces a method to quantify LLMs' ability to implement design patterns, using Role-Based Metamodeling Language (RBML) for a rigorous specification of the pattern's problem, solution, and transformation rules. The method evaluates the pattern applicability of a software application using the pattern's problem specification. If deemed applicable, the application is input to the LLM for pattern application. The resulting application is assessed for conformance to the pattern's solution specification and for completeness against the pattern's transformation rules. Evaluating the method with ChatGPT 4 across three applications reveals ChatGPT's high proficiency, achieving averages of 98% in conformance and 87% in completeness, thereby demonstrating the effectiveness of the method. Using RBML, this study confirms that LLMs, specifically ChatGPT 4, have great potential in effective and efficient application of design patterns with high conformance and completeness. This opens avenues for further integrating LLMs into complex software engineering processes."
"Redefining the Programmer: Human-AI Collaboration, LLMs, and Security in Modern Software Engineering","De La Cruz, E; Le, H; Meduri, K; Nadella, GS; Gonaygunta, H",10.32604/cmc.2025.068137,2025,"The rapid integration of artificial intelligence (AI) into software development, driven by large language models (LLMs), is reshaping the role of programmers from traditional coders into strategic collaborators within Industry 4.0 ecosystems. This qualitative study employs a hermeneutic phenomenological approach to explore the lived experiences of Information Technology (IT) professionals as they navigate a dynamic technological landscape marked by intelligent automation, shifting professional identities, and emerging ethical concerns. Findings indicate that developers are actively adapting to AI-augmented environments by engaging in continuous upskilling, prompt engineering, interdisciplinary collaboration, and heightened ethical awareness. However, participants also voiced growing concerns about the reliability and security of AI-generated code, noting that these tools can introduce hidden vulnerabilities and reduce critical engagement due to automation bias. Many described instances of flawed logic, insecure patterns, or syntactically correct but contextually inappropriate suggestions, underscoring the need for rigorous human oversight. Additionally, the study reveals anxieties around job displacement and the gradual erosion of fundamental coding skills, particularly in environments where AI tools dominate routine development tasks. These findings highlight an urgent need for educational reforms, industry standards, and organizational policies that prioritize both technical robustness and the preservation of human expertise. As AI becomes increasingly embedded in software engineering workflows, this research offers timely insights into how developers and organizations can responsibly integrate intelligent systems to promote accountability, resilience, and innovation across the software development lifecycle."
Correctness assessment of code generated by Large Language Models using internal representations,"Bui, TD; Vu, TT; Nguyen, TT; Nguyen, S; Vo, HD",10.1016/j.jss.2025.112570,2025,"Ensuring the correctness of code generated by Large Language Models (LLMs) presents a significant challenge-driven software development. Existing methods predominantly rely on black-box (closed-box) approaches evaluate correctness post-generation failing to utilize the rich insights embedded in the LLMs' internal during code generation. This limitation leads to delayed error detection, increased debugging costs, reduced reliability in deployed AI-assisted coding workflows. In this paper, we introduce OPENIA, a novel-box (open-box) framework that leverages these internal representations to assess the correctness of LLMgenerated code. By systematically analyzing the intermediate states of representative open-source code LLMs, including DeepSeek-Coder, CODE LlAmA, and MAgICODER, across diverse code generation benchmarks, we found these internal representations encode latent information, which strongly correlates with the correctness the generated code. Building on these insights, OPENIA uses a white-box/open-box approach to make informed predictions about code correctness, offering significant advantages in adaptability and robustness over traditional blackbox methods and zero-shot approaches. Our results show that OPENIA consistently outperforms baseline models, achieving higher accuracy, precision, recall, and F1-Scores with up to a 2X improvement standalone code generation and a 3X enhancement in repository-specific scenarios. By unlocking the potential-process signals, OPENIA paves the way for more proactive and efficient quality assurance mechanisms LLM-assisted code generation."
Rethinking Software Testing for Modern Development,"Saxena, A",10.1109/MC.2025.3554094,2025,"This article explores the shift from manual to automated testing, emphasizing the role of artificial intelligence and machine learning in enhancing efficiency and quality assurance in the software development lifecycle, highlighting innovative security testing for open source software and examining artificial intelligence's impact on testing frameworks."
A Dual-Stage Framework for Behavior-Enhanced Automated Code Generation in Industrial-Scale Meta-Models,"Ma, T; Dai, SL; Gao, YF; Xu, FJ; Fang, L",10.1109/ACCESS.2025.3614174,2025,"Industrial model-driven engineering often struggles to generate functionally complete code, especially dynamic behaviors, from large-scale Meta-models, typically yielding only static structures. This paper introduces the (SD2)-D-2 (Static-Structure & Dynamic-Behavior Dual-Stage) framework, a novel approach enhancing automated code generation through two key innovations. First, the Dual-Path Fusion stage constructs a semantically rich static code skeleton by integrating structural semantics from XMI with data constraints from XSD. Second, the dynamic behavior injection engine leverages model annotations and associations to inject operational capabilities into the static foundation. A language-independent Intermediate Code Model unifies semantic information, facilitating transformations via Query/View/Transformation-Relations and domain-specific languages. Validation using industrial Meta-models such as AUTOSAR (Automotive Open System Architecture) and OPC UA (Open Platform Communications Unified Architecture) demonstrates significant improvements in automation, code quality, and development efficiency. This work also discusses Large Language Model limitations for reliable Meta-model-based code generation and posits (SD2)-D-2's potential, particularly with Retrieval Augmented Generation, to improve their code synthesis fidelity. Key contributions include advanced model transformation, behavior-centric code enhancement, and a pathway for more dependable AI-assisted software development."
Few-Shot Evaluation of Vision Language Models for Detecting Visual Defects in Autonomous Vehicle Software Requirement Specifications,"Bukhary, N; Ahmad, M; Rashad, K; Rai, S; Shapsough, S; Kaddoura, Y; Dghaym, D; Zualkernan, I",10.1109/ACCESS.2025.3586554,2025,"Software Requirements Specifications (SRS) are crucial for defining system functionality, constraints, and objectives, particularly in safety-critical applications like autonomous vehicles (AV). Ensuring these requirements are precise, unambiguous, and complete is important for developing reliable self-driving systems. While traditional SRS are predominantly text-based, AV requirements pose unique challenges as they often include visual elements such as images and diagrams. Although Large Language Models (LLMs) have enhanced traditional text-based requirements, Vision-Language Models (VLMs) remain largely unexplored for evaluating defects in visual elements. This paper investigates the few-shot performance of three large-scale VLMs on their ability to detect various types of ambiguities, inconsistencies, and incompleteness in visual traffic scenario images that could be used in AV requirements specifications. Using Soft prompting and Chain-of-Thought (CoT) prompting, we evaluated the effectiveness of GPT-4, GEMINI, and ClaudeAI in detecting six types of visual defects. GPT-4 achieved the highest overall performance among the evaluated models, with the highest F1-score of 0.63 in detecting Spatial Incompleteness. CoT prompting notably enhanced defect detection performance for both GEMINI and ClaudeAI across all defect types. While CoT prompting did not significantly improve GPT-4's detection capabilities, it significantly enhanced the ability of all models to explain the reasoning behind the identified defects. Despite promising results, none of the models could fully automate the requirements validation process, primarily due to their lack of domain-specific knowledge. Nevertheless, our findings suggest that fine tuning the models and structured prompt design offer the potential to improve VLM precision in visual defect detection."
Why Do Developers Engage with ChatGPT in Issue-Tracker? Investigating Usage and Reliance on ChatGPT-Generated Code,"Das, JK; Mondal, S; Roy, CK",10.1109/SANER64311.2025.00015,2025,"Large language models (LLMs) like ChatGPT have shown the potential to assist developers with coding and debugging tasks. However, their role in collaborative issue resolution is underexplored. In this study, we analyzed 1,152 Developer-ChatGPT conversations across 1,012 issues in GitHub to examine the diverse usage of ChatGPT and reliance on its generated code. Our contributions are fourfold. First, we manually analyzed 289 conversations to understand ChatGPT's usage in the GitHub Issues. Our analysis revealed that ChatGPT is primarily utilized for ideation, whereas its usage for validation (e.g., code documentation accuracy) is minimal. Second, we applied BERTopic modeling to identify key areas of engagement on the entire dataset. We found that backend issues (e.g., API management) dominate conversations, while testing is surprisingly less covered. Third, we utilized the CPD clone detection tool to check if the code generated by ChatGPT was used to address issues. Our findings revealed that ChatGPT-generated code was used as-is to resolve only 5.83% of the issues. Fourth, we estimated sentiment using a RoBERTa-based sentiment analysis model to determine developers' satisfaction with different usages and engagement areas. We found positive sentiment (i.e., high satisfaction) about using ChatGPT for refactoring and addressing data analytics (e.g., categorizing table data) issues. On the contrary, we observed negative sentiment when using ChatGPT to debug issues and address automation tasks (e.g., GUI interactions). Our findings show the unmet needs and growing dissatisfaction among developers. Researchers and ChatGPT developers should focus on developing task-specific solutions that help resolve diverse issues, improving user satisfaction and problem-solving efficiency in software development."
Efficient Modeling and Usage of Scratchpad Memory for Artificial Intelligence Accelerators,"KÃ¶ksal, CIR; YalÃ§in, SBO",10.3390/electronics14051032,2025,"Deep learning accelerators play a crucial role in enhancing computation-intensive AI applications. Optimizing system resources-such as shared caches, on-chip SRAM, and data movement mechanisms-is essential for achieving peak performance and energy efficiency. This paper explores the trade-off between last-level cache (LLC) and scratchpad memory (SPM) usage in accelerator-based SoCs. To evaluate this trade-off, we introduce a high-speed simulator for estimating the timing performance of complex SoCs and demonstrate the benefits of SPM utilization. Our work shows that dynamic reconfiguration of the LLC into an SPM with prefetching capabilities reduces cache misses while improving resource utilization, performance, and energy efficiency. With SPM usage, we achieve up to 13x speedup and a 10% reduction in energy consumption for CNN backbones. Additionally, our simulator significantly outperforms state-of-the-art alternatives, running 3000x faster than gem5-SALAM for fixed-weight convolution computations and up to 64,000x faster as weight size increases. These results validate the effectiveness of both the proposed architecture and simulator in optimizing deep learning workloads."
Cognitive Biases: Understanding and Designing Fair AI Systems for Software Development,"Adepoju, S; Adepoju, M",,2025,"Artificial Intelligence (AI) systems increasingly influence decisions that affect people's lives, making fairness a core requirement. However, cognitive biases, systematic deviations in human judgment, can enter AI through data, modeling choices, and oversight, amplifying social inequities. This paper examines how three bias channels, data, algorithmic, and human, manifest across the software development lifecycle and synthesizes practical strategies for mitigation. Using a qualitative review of recent scholarship and real-world case studies, we distill a lightweight diagnostic framework that helps practitioners identify bias sources, evaluate mitigation options against effectiveness, feasibility, transparency, and scalability, and institutionalize routine audits. We illustrate the framework with representative vignettes and summarize trade-offs between fairness goals and model performance. Our analysis recommends diverse and well-documented datasets, fairness-aware learning and evaluation, third-party audits, and cross-functional collaboration as mutually reinforcing levers. The paper contributes a developer-oriented map of cognitive bias risks across data, model, and human processes, a four-criterion rubric for comparing mitigation techniques, and an actionable checklist that teams can embed in their pipelines. The results aim to support software and product teams in building AI systems that are both accurate and equitable."
SeventhWorkshop on Emerging Software Engineering Education(WESEE 2025),"Rathore, SS; Tiwari, S; Farooq, SUS",10.1145/3717383.3721236,2025,"The seventh Workshop on Emerging Software Engineering Education (WESEE) aims to discuss and examine the development of learning environments that are influencing the pedagogical strategies for the education of software engineering courses in institutions, specifically through the adoption of Generative AI (GenAI) tools and techniques. Additionally, the workshop aims to examine how industries are utilizing GenAI tools and technologies for teaching software development methods and how the developers are utilizing the material for self-learning and skill acquisition. The report is an overview of the upcoming seventh edition of WESEE, which will be held on 20.. h February 2025 at NIT Kurukshetra. The workshop will be held alongside the 18th Innovations in Software Engineering Conference (ISEC 2025)."
ReqCompletion: domain-enhanced automatic completion for software requirements,"Lian, XL; Ma, JP; Lv, HY; Zhang, L",10.1007/s00766-025-00441-w,2025,"Software requirements are the driving force behind software development. As the cornerstone of the entire software lifecycle, the efficiency of crafting requirement specifications and the quality of these requirements significantly influence the duration of software development. Despite massive research on requirements elicitation, the reality is that requirements are often painstakingly crafted manually, word by word. This manual process is not only time-consuming but also prone to issues such as the misuse of terminology. To address these challenges, we introduce ReqCompletion, an approach designed to recommend the next token in real-time for given prefix of requirements description. ReqCompletion comprises two phases.Firstly, we have designed and implemented the token completion approach. We integrate a knowledge-injection module and a pointer network into GPT-2, which stands as the largest available GPT model that allows for fine-tuning on specialized downstream tasks. Second, we design a distil phase to speed up the completion by downsizing the ReqCompletion. Empirical evaluations using two public datasets demonstrate that ReqCompletion surpasses all baselines in performance (Recall@7 gains up to 65.87% than the second-best model). Furthermore, the effectiveness of its two pivotal design elements has been substantiated through rigorous ablation studies. Regarding the distillation phase, we downsized the model to just 11.6% of its original size, resulting in a 4.07-fold speedup while maintaining a recall value of 98.9%. The utility of our work has been evaluated preliminarily through a small user study."
From Users to Co-Designers: Youth Participation in Understanding Cyberbullying,"Verma, K; Davis, B; Milosevic, T; Umbach, R",10.1145/3713043.3731485,2025,"The manifestation of cyberbullying (CB) on social media platforms (SMP) evolves alongside advancements in Internet communication technologies (ICT). Current automated CB detection techniques, often relying on artificial intelligence (AI) are overly simplistic, overlooking the multimodal nature of user-generated content and the relational nature of CB and treating it as a binary problem. Addressing these limitations, this mixed methods study involves young people (aged 13-17) in developing CB detection software following the principles of the Software Development Life Cycle (SDLC). To explore the perspectives of young people on online harms related to CB across various SMPs, we first surveyed 104 participants aged 13-17 in Ireland. Following the call of the European Union Joint Research Center (EU JRC) to integrate young people's perspectives into the design of AI systems that serve their needs, we facilitate co-design sessions with junior researchers (aged 15-16) during a week-long research internship. In these sessions, participants adapt existing CB scenarios to reflect the realities of today's SMP and ICT environments, ensuring the outcomes are relevant and youth-informed. Additionally, the junior researchers help craft more than 550 text messages(1) across multiple cyberbullying scenarios that reflect bystander-enabler and defender behaviours (e.g., escalate or de-escalate the cyberbullying behaviour). Tailored to reflect the dynamics of the comment-thread and mulit-party conversations, these messages are current under review by experts. This structured and generative input positions them from users to co-designers of a youth-informed benchmark dataset that will support the evaluation of future CB detection systems."
"Ethical considerations in the AI lifecycle for design, developing and adopting AI in public sector-the case of Finland","AlamÃ¤ki, A; Khan, UA; Lagstedt, A",10.12821/ijispm130401,2025,"This study explores the role of ethics in all phases of AI projects. Through qualitative interviews with the public sector actors in Finland, the study identifies key ethical concerns related to the lifecycle of AI. The findings highlight the need for embedding ethical requirements throughout the AI system lifecycle and emphasize the role of human-centered AI systems. By utilizing empirical data from multiple public sector case organizations, this study provided both theoretical insights and practical guidelines for developing ethically aligned AI systems. The findings emphasize the need for a comprehensive, lifecycle-oriented approach to ethical AI design, development, adoption, and use. The AI lifecycle spans various phases that collectively shape the ethical impact of AI applications. This research provides empirical insights into how ethical considerations can be practically integrated from the design to adoption phases of AI. By embedding ethical practices throughout the lifecycle, organizations can anticipate and mitigate risks more effectively."
Collaboration with Generative AIto improve Requirements Changeâ,"Kong, Y; Zhang, N; Duan, ZH; Yu, B",10.1016/j.csi.2025.104013,2025,"Requirements Change (RC) is a critical aspect of the software development process, involving modifications throughout almost the entire software life cycle. Despite its significance, RC remains a highly challenging process due to the complexity of software systems and the inherent uncertainty associated with changes. While Large Language Models (LLMs) have demonstrated promising potential in various fields, particularly in Software Engineering (SE), there is limited research on LLMs for SE specifically addressing real software systems RC. To solve this, we propose an innovative approach, named Satisfy Requirements Change (SRC), which utilizes prompt engineering to improve the RC of actual software systems through human-machine collaboration. Specifically, ChatGPT is prompted to complete the entire RC process, encompassing system modeling, confirmation positioning, program modification, and property verification. Additionally, we conduct a RC application case on a real Java system and demonstrate through case study that our approach is effective in improving RC."
SRB-ELL: A Vector-Friendly Sparse Matrix Format for SpMV on Scratchpad-Augmented Architectures,"Zhang, S; Bai, WQ; Zhang, ZM; Xie, XC; Tang, XB",10.3390/app15179811,2025,"Sparse Matrix-Vector Multiplication (SpMV) is a critical computational kernel in high-performance computing (HPC) and artificial intelligence (AI). However, its irregular memory access patterns lead to frequent cache misses on multi-level cache hierarchies, significantly degrading performance. Scratchpad memory (SPM), a software-managed, low-latency on-chip memory, offers improved data locality and control, making it a promising alternative for irregular workloads. To enhance SpMV performance, we propose a vectorized execution framework targeting SPM-augmented processors. Recognizing the limitations of traditional formats for vectorization, we introduce Sorted-Row-Block ELL (SRB-ELL), a new matrix storage format derived from ELLPACK (ELL). SRB-ELL stores only non-zero elements, partitions the matrix into row blocks, and sorts them by block size to improve load balance and SIMD efficiency. We implement and evaluate SRB-ELL on a custom processor architecture with integrated SPM using the gem5 simulator. Experimental results show that, compared to vectorized CSR-based SpMV, the SRB-ELL design achieves up to 1.48x speedup and an average of 1.19x."
Software Development for Brain Glioma Detection Using Magnetic Resonance Imaging and Deep Learning Techniques,"Gaspar-Vargas, LE; Torres-Calva, KA; Ruiz-Vanoye, JA; DÃ­az-Parra, O; Simancas-Acevedo, E; Salgado-RamÃ­rez, JC",10.61467/2007.1558.2025.v16i3.1132,2025,"The detection of brain gliomas represents a critical clinical challenge, necessitating early and precise diagnostic methods to improve patient outcomes. Here, we present the development of a deep learning-based system for glioma detection, employing an ensemble of ResNet18, VGG16 and DenseNet121 models trained on MRI scans. Preprocessing comprised dataset curation, image normalisation and mask generation via K-means clustering. The trained models are integrated into a web application that enables users to upload scans and obtain immediate diagnostic feedback. Experimental results demonstrate high accuracy and robust segmentation performance. This research underscores the potential of artificial intelligence (AI) to augment conventional medical imaging techniques and support clinical diagnosis."
Project-Based Learning Connecting Robotics and Artificial Intelligence,"Posekany, A",10.1007/978-3-031-83523-0_30,2025,"The combination of knowledge and skill sets from robotics and Artificial Intelligence has proven as a powerful catalyst for students' learning experiences, when applying available resources and knowledge acquired through vocational training. Within the Department of IT at an Austrian vocational high school, our students actively engage in projects that combine robotics and AI. Our diploma theses extend beyond mere classroom theory, allowing interested students to apply their knowledge in authentic, real-world scenarios in the form of thesis projects which span different engineering and IT disciplines. Our goal is to emphasize hands-on experiences and encourage our students to design, construct and program robots, even with the addition of AI technology, such as image recognition and classification trained for specific tasks. Through this practical immersion, our students gain a deeper understanding of robotics and AI, disciplines that are at the forefront of today's technological innovation. We worked with two groups of students on interdisciplinary projects bridging the gap between robotics and AI and based on our students' feedback found an increase in motivation to learn not only about the fields themselves, but also about related fields, from mathematical theory to better understand the intricate workings of AI algorithms to electronics and working with microcontrollers. Personal interviews with involved students have also pointed toward an increased motivation through the intense cooperation between the team members as well as the teachers responsible for supporting the project teams through their thesis projects. Projects connecting robotics and AI empower students to become adaptable, creative problem-solvers which is a crucial foundation for success in the twenty-first century. By fostering collaboration and critical thinking, while enhancing students' technical skills and equipping them with the adaptability and creativity they require, this educational approach prepares students to thrive in a rapidly changing world where both disciplines play pivotal roles."
Using contrastive language-image pre-training for Thai recipe recommendation,"Chuenbanluesuk, T; Plodprong, V; Karoon, W; Rueangsri, K; Pojam, S; Siriborvornratanakul, T",10.1007/s10579-025-09816-5,2025,"In this study, we utilized CLIP (Contrastive Language-Image Pretraining) as a recommendation model for retrieving images of Thai food recipes relevant to a query ingredient text in the Thai language. We scraped a total of 22,500 food recipes online and used the recipe image and ingredient list as an image-text pair for training. After preprocessing the data, we trained CLIP using four variations of image and text encoders, employing distilbert-base-th-cased and wangchanberta-base-att-spm-uncased as text encoders, and ResNet50 and EfficientNet-B2 as image encoders. The model trained with distilbert-base-th-cased and ResNet50 achieved the best top-10 accuracy of 17.1%. CLIP applications for the Thai language are rare due to the language's complexity and difficulty in embedding. However, our study demonstrates that CLIP can comprehend the relationship between image and text in the Thai language, particularly for ingredients with unique shapes or colors and popular recipes in the dataset."
BEACon-TD: Classifying Technical Debt and its types across diverse software projects issues using transformers,"Shivashankar, K; Orucevic, M; Kruke, MM; Martini, A",10.1016/j.jss.2025.112435,2025,"Technical Debt (TD) identification in software projects issues is crucial for maintaining code quality, reducing long-term maintenance costs, and improving overall project health. This study advances TD identification in issues tracker using transformer-based models, addressing the critical need for accurate and efficient TD identification in large-scale software development. Our methodology employs multiple binary classifiers for TD and its type, combined through ensemble learning, to enhance accuracy and robustness in detecting various forms of TD. We train and evaluate these models on a comprehensive dataset from GitHub Archive Issues (2015-2024), supplemented with industrial data validation. We demonstrate that in-project fine-tuned transformer models significantly outperform task-specific finetuned models in TD classification, highlighting the importance of project-specific context in accurate TD identification. Our research also reveals the superiority of specialized binary classifiers over multi-class models for TD and its type identification, enabling more targeted debt resolution strategies. A comparative analysis shows that the smaller DistilRoBERTa model is more effective than larger language models like GPTs for TD classification tasks, especially after fine-tuning, offering insights into efficient model selection for specific TD detection tasks. The study also assesses generalization capabilities using metrics such as MCC, AUC ROC, Recall, and F1 score, focusing on model effectiveness, fine-tuning impact, and relative performance. By validating our approach on out-of-distribution and real-world industrial datasets, we ensure practical applicability, addressing the diverse nature of software projects. This research significantly enhances TD detection and offers a more nuanced understanding of TD types, contributing to improved software maintenance strategies in both academic and industrial settings. The release of our curated dataset aims to stimulate further advancements in TD classification research, ultimately enhancing software project outcomes and development practices by enabling early TD identification and management."
An Efficient Methodology for the Categorization of Software Requirements Using Natural Language Processing and Similarity Analysis,"Izhar, R; Cosh, K; Ramingwon, L; Ramingwong, S; Bhatti, SN",10.1109/ACCESS.2025.3568504,2025,"The classification of software requirements into functional (FRs) and non-functional requirements (NFRs) is indispensable for the efficacious implementation of software systems. Traditionally, this endeavor reliant on manual exertion, this process has proven to be both protracted and inherently susceptible to errors, and inaccuracies. Recent advancements in machine learning (ML) have begun to offer promising avenues for automation, enhancing both the efficiency and accuracy of requirement classification. The following research study proposes NLPReqClassifier, a lightweight automated classification model that integrates Term Frequency-Inverse Document Frequency (TF-IDF) and Cosine Similarity. Leveraging the PROMISE dataset, which is enriched continually by academic and professional input, this model addresses the existing shortcomings of traditional classification methods. The proposed model utilizes a dual-evaluation approach, ensuring relevance and precision across varied software development contexts. By incorporating iterative feedback into the model's training process, this research not only aligns with academic standards but also meets the practical demands of the industry. The model's performance was tested in both academic settings dataset and real-world industry dataset, particularly focusing on its application in Enterprise Resource Planning (ERP) systems. The proposed model demonstrated superior capability to categorize a broad spectrum of software requirements accurately, outperforming existing traditional methodologies in terms of adaptability and efficiency. It showed significant improvements over traditional classification methods, particularly in its ability to dynamically adapt to new and evolving requirements. The dual-evaluation process verified the model's effectiveness, showcasing high precision and recall rates in both controlled and practical environments."
Leveraging machines to derive domain models from user stories,"Bragilovski, M; van Can, AT; Dalpiaz, F; Sturm, A",10.1007/s00766-025-00442-9,2025,"Domain models play a crucial role in software development, as they provide means for communication among stakeholders, for eliciting requirements, and for representing the information structure behind a database scheme or for model-driven development. However, creating such models is a tedious activity and automated support may assist in obtaining an initial domain model that can later be enriched by human analysts. In this paper, we compare the effectiveness of various approaches for deriving domain models from a given set of user stories. We contrast human derivation (of both experts and novices) with machine derivation; for the latter, we compare (i) the Visual Narrator: an existing rule-based NLP approach; (ii) a machine learning classifier that we feature engineered; and (iii) a generative AI approach that we constructed via prompt engineering with multiple configurations. Based on a benchmark dataset comprising nine collections of user stories and their corresponding domain models, the evaluation shows that while no approach matches human performance, large language models (LLMs) are not statistically outperformed by human experts in deriving classes. Additionally, a tuned version of the machine learning approach achieves results close to human performance in deriving associations. To better understand the results, we qualitatively analyze them and identify differences in the types of false positives as well as other factors that affect performance."
Large Language Models for C Test Case Generation: A Comparative Analysis,"Guzu, A; Nicolae, G; Cucu, H; Burileanu, C",10.3390/electronics14112284,2025,"Software testing is a crucial yet time-consuming aspect of software development. Writing comprehensive unit tests that accurately verify whether a function or an entire program behaves as intended requires considerable effort from developers, particularly when handling numerous edge cases. This study explores how Large Language Models (LLMs) can streamline this process by automatically generating effective unit tests. We evaluate various LLMs on their capability to interpret problem specifications, analyze source code across multiple programming languages, and generate suitable test cases. The effectiveness of these test cases is assessed using the Pass@1 and line coverage metrics. Our findings reveal that LLMs perform significantly better when provided with both the problem description and the corresponding solution code, particularly in the C programming language. Additionally, we observe substantial performance improvements when example test cases are included in the prompt, leading to higher Pass@1 scores and enhanced code coverage, particularly with more advanced LLMs."
Detecting Redundancies Between User Stories with Graphs and Large Language Models,"Hofmann, LS; Lauer, A; Kosiol, J; Kesper, A; Wieber, P; Rabieyan, A; Taentzer, G",10.1007/978-3-031-88531-0_18,2025,"[Context and motivation] User stories (USs) are a widely used notation for requirements in agile software development. [Question/problem] In large software projects, redundancies between USs can easily occur, and unresolved redundancies can impact software quality. It is crucial for requirements engineers to know where redundancy occurs in their projects. However, some forms of redundancy may be acceptable. [Principal ideas/results] We present two automated approaches for detecting redundancy in a set of USs in order to prevent a decrease of software quality due to the realisation of redundant USs. The first approach is based on annotation graphs, containing the main actions and entities of a US. By design, this approach effectively identifies a strict form of redundancy. The second approach detects redundancies of a more semantic nature using large language models (LLMs). [Contribution] We present the concepts and tools of both approaches and evaluate their potential and limitations by applying them to a large corpus of USs. Our results show that the inherently fuzzy LLM-based approach is able to detect most of the strict redundancies and additionally finds many more non-strict semantic redundancies. Thus, this study contributes to the advancement of automated quality control of USs."
Leveraging Large Language Models for Usability Testing: a Preliminary Study,"Calvano, M; Curci, A; Lanzilotti, R; Piccinno, A; Ragone, A",10.1145/3708557.3716341,2025,"Despite growing efforts to prioritize user experience in product development, software organizations often perform little or no usability engineering activities. Therefore, it is crucial to develop strategies to integrate them effectively into software development processes. The rapid advances in Artificial Intelligence have significantly influenced various aspects of daily life, particularly with the emergence of Large Language Models (LLMs), which can serve as promising tools to support activities to enhance the usability of software products. This paper presents a study investigating the potential of LLMs to assist practitioners in conducting usability tests. Specifically, we conducted an experiment where LLMs generate usability test tasks. Our goal is to assess whether AI can effectively support evaluators by comparing tasks generated by LLMs to those defined by usability experts. The findings indicate that while LLMs can provide valuable support, effective usability testing still requires human oversight and expert intervention."
Leveraging Large Language Models for Code Translation and Software Development in Scientific Computing,"Dhruv, A; Dubey, A",10.1145/3732775.3733572,2025,"The emergence of foundational models and generative artificial intelligence (GenAI) is poised to transform productivity in scientific computing, especially in code development, refactoring, and translating from one programming language to another. However, because the output of GenAI cannot be guaranteed to be correct, manual intervention remains necessary. Some of this intervention can be automated through task-specific tools, alongside additional methodologies for correctness verification and effective prompt development. We explored the application of GenAI in assisting with code translation, language interoperability, and codebase inspection within a legacy Fortran codebase used to simulate particle interactions at the Large Hadron Collider (LHC). In the process, we developed a tool, CodeScribe, which combines prompt engineering with user supervision to establish an efficient process for code conversion. In this paper, we demonstrate how CodeScribe assists in converting Fortran code to C++, generating Fortran-C APIs for integrating legacy systems with modern C++ libraries, and providing developer support for code organization and algorithm implementation. We also address the challenges of AI-driven code translation and highlight its benefits for enhancing productivity in scientific computing workflows."
Curriculum for a New Academic Program with Bachelor's Degrees in Intelligent Systems Engineering and Computer Engineering,"Solo, AMG",10.1007/978-3-031-85930-4_35,2025,"This research paper proposes a curriculum for a six-year academic program with a bachelor's degree (honors) in intelligent systems engineering and a bachelor's degree (honors) in computer engineering. This program includes courses in digital electronics, analog electronics, computer organization, computer architecture, computer communication networks, data structures, compiler design, operating system design, firmware design, database systems, computer graphics and virtual reality design, static and dynamic website design, development of chatbots and voice assistants, software engineering methodology, knowledge-based systems, fuzzy logic, neural networks, evolutionary computation, evolutionary multiojective optimization, machine learning, image processing, computer vision, pattern recognition, voice recognition, natural language processing, data science, control systems, intelligent control systems, robotics, digital signal processing, signals and systems, mathematics, engineering physics, biology, etc. These degrees will allow graduates to have a good understanding of all of the main branches of intelligent systems engineering and computer engineering."
Rethinking Technological Investment and Cost-Benefit: A Software Requirements Dependency Extraction Case Study,"Ginde, G; Ruhe, G; Saunders, C",10.1109/ACCESS.2025.3556313,2025,"Machine Learning (ML) is widely used for different purposes within Software Engineering. It can substantially improve the efficiency and effectiveness of organizations. While various methods and techniques exist, all of them have strengths and weaknesses under varying scenarios and contexts. Thus far, the selection and implementation of ML techniques rely almost exclusively on accuracy criteria. This narrow perspective ignores crucial considerations of anticipated costs of the ML activities versus the projected benefits gained from applying the results. Thus, in this study we introduce a return-on-investment (ROI) perspective to evaluate ML techniques in Software Engineering, offering a novel lens to assess their true value beyond traditional benchmarks. We present findings for an approach that addresses this gap by enhancing the accuracy criterion with return on investment (ROI) considerations. Specifically, we extract dependencies from textual descriptions of software requirements and analyze the performance of two state-of-the-art ML techniques: Random Forest and Bidirectional Encoder Representations from Transformers (BERT), a encoder only Large Language Model. Drawing upon two publicly available data sets, we compare decision-making based on 1) exclusively on accuracy and 2) on ROI analysis to provide decision support for the selection and usage of ML classification methods. As such, our results showed that, 1) chasing model accuracy improvisation through increased annotated data does not generate expected returns in traditional ML methods. 2) For complex ML algorithms, the need for larger annotated dataset investment cost is justified by the higher returns, however, the trade-offs between accuracy and ROI become evident."
"Curious, Critical Thinker, Empathetic, and Ethically Responsible: Essential Soft Skills for Data Scientists in Software Engineering","LeÃ§a, MD; Santos, RD",10.1109/ICSE-SEIS66351.2025.00021,2025,"Background. As artificial intelligence and AI-powered systems continue to grow, the role of data scientists has become essential in software development environments. Data scientists face challenges related to managing large volumes of data and addressing the societal impacts of AI algorithms, which require a broad range of soft skills. Goal. This study aims to identify the key soft skills that data scientists need when working on AI-powered projects, with a particular focus on addressing biases that affect society. Method. We conducted a thematic analysis of 87 job postings on LinkedIn and 11 interviews with industry practitioners. The job postings came from companies in 12 countries and covered various experience levels. The interviews featured professionals from diverse backgrounds, including different genders, ethnicities, and sexual orientations, who worked with clients from South America, North America, and Europe. Results. While data scientists share many skills with other software practitioners-such as those related to coordination, engineering, and management-there is a growing emphasis on innovation and social responsibility. These include soft skills like curiosity, critical thinking, empathy, and ethical awareness, which are essential for addressing the ethical and societal implications of AI. Conclusion. Our findings indicate that data scientists working on AI-powered projects require not only technical expertise but also a solid foundation in soft skills that enable them to build AI systems responsibly, with fairness and inclusivity. These insights have important implications for recruitment and training within software companies and for ensuring the long-term success of AI-powered systems and their broader societal impact."
FractalSync: Lightweight Scalable Global Synchronization of Massive Bulk Synchronous Parallel AI Accelerators,"Isachi, V; Nadalini, A; Gallotta, RF; Garofalo, A; Conti, F; Rossi, D",10.1145/3719276.3725203,2025,"The slow-down of technology scaling and the emergence of Artificial Intelligence (AI) workloads have led computer architects to increasingly exploit parallelization coupled with hardware acceleration to keep pushing the performance envelope. However, this solution comes with the challenge of synchronization of processing elements (PEs) in massive heterogeneous many-core platforms. To address this challenge, we propose FractalSync, a hardware accelerated synchronization mechanism for Bulk Synchronous Parallel (BSP) systems. We integrate FractalSync in MAGIA, a scalable tilebased AI accelerator, with each tile featuring a RISC-V-coupled matrix-multiplication (MatMul) accelerator, scratchpad memory (SPM), and a DMA connected to a global mesh Network-on-Chip (NoC). We study the scalability of the proposed barrier synchronization scheme on tile meshes ranging from 2x2 PEs to 16x16 PEs to evaluate its design boudaries. Compared to a synchronization scheme based on software atomic memory operations (AMOs), the proposed solution achieves up to 43x speedup on synchronization, introducing a negligible area overhead (< 0.01%). FractalSync closes timing at MAGIA's target 1GHz frequency."
Towards an Adoption Framework to Foster Trust in AI-Assisted Software Engineering,"BarÃ³n, MM",10.1109/CAIN66642.2025.00038,2025,"The adoption of artificial intelligence (AI) tools, specifically those based on large language models (LLMs), has quickly led to turbulent changes in software engineering practice. However, despite their potential, AI-based tools are often thrust into the development process with little consideration towards establishing trust, a key factor in the adoption of new tooling. With this work, we address this trust gap by focusing on the needs of software engineers to facilitate a trust-based, synergistic adoption framework in AI-assisted software engineering (AI4SE). We aim to identify the key factors influencing trust in AI4SE and to design a practical adoption framework that covers the concrete steps and best practices for a trust-based integration of AI4SE. Finally, we evaluate the designed framework by applying it to a real-world use case with a professional software development team. By treating trust as a core design goal, this research provides practical guidance to improve the developer experience and facilitate a seamless adoption of AI4SE tooling."
Smarter Crowdsourcing With NLP and Attention Mechanisms for Task Complexity Prediction,"Munir, Y; Umer, Q; Aslam, MW; Faheem, M; Hakami, H",10.1109/ACCESS.2025.3615745,2025,"Competitive Crowdsourcing Software Development (CCSD) has emerged as a powerful tool for developing software solutions, attracting researchers and the development market. Using crowdsourced collective intelligence, CCSD ensures the delivery of innovative, cost-effective, and high-quality solutions within specified time frames, making it an attractive approach for addressing complex challenges in software development. However, as the CCSD environment gains popularity, it also introduces challenges, particularly in predicting job complexity, which must be tackled to optimize the crowdsourcing process. In software platforms, client organizations register and upload jobs related to development projects. These jobs are manually reviewed by the copilots responsible for assessing the complexity of the job, a process that can lead to delays and an overburden of experts. To streamline this process, we propose an approach that automatically predicts job complexity and classifies it accordingly. We collect data from the TopCoder CCSD platform, focusing on projects related to software development. The collected data are pre-processed and tokenized using NLP techniques. We convert the text data into a word embedding vector matrix using a pre-trained GloVe model, which captures the semantic and contextual meaning of the text. The Sequence Attention (SA) mechanism is introduced in LSTM to identify the key parts of the input sequence that are most relevant for predicting the output, thereby improving job complexity classification. The proposed approach is then trained on these word embeddings using SA-LSTM and benchmarked against LSTM and other state-of-the-art techniques. The proposed approach, GloVe-based (GB) SA-LSTM, outperforms other approaches by achieving an accuracy improvement of 31.4%, 152.04%, 20.07%, 10.99%, 9.16%, and 17.64% over ZeroR, RP, LR, BERT, SVM, and DT. To verify the impact of SA, the proposed approach is compared with GB-LSTM. The GB-SA-LSTM outperforms GB-LSTM by 4.88%. This enhanced accuracy in task complexity prediction within software crowdsourcing platforms can significantly contribute to the efficient and effective development of crowdsourced projects."
LLM4TDG: test-driven generation of large language models based on enhanced constraint reasoning,"Liu, JQ; Liang, RG; Zhu, XX; Zhang, Y; Liu, YL; Liu, QX",10.1186/s42400-024-00335-4,2025,"With the evolution of modern software development paradigms, component reuse, and low-code approaches have emerged as mainstream in software development. However, developers often lack an in-depth understanding of reused code. The inability of components to operate autonomously leads to insufficient testing of software functionalities and security, further exacerbating the contradiction between the increasing complexity of software architectures and the demand for accurate and efficient software automation testing. This, in turn, increases the frequency of software supply chain security incidents. This paper proposes a test-driven generation framework, LLM4TDG, based on large language models (LLMs). By formally defining the constraint dependency graph and converting it into context constraints, LLMs' ability to understand natural language descriptions such as test requirements and documents is enhanced. Constraint reasoning and backtracking mechanisms are then used to generate test drivers that satisfy the defined constraints automatically. Using the EvalPlus dataset, we evaluate the comprehensive capabilities of LLM4TDG in test case generation using four general-domain LLMs and five code-generation-domain LLMs. The experimental results indicate that our approach significantly enhances LLMs' ability to comprehend constraints in testing objectives, achieving a 47.62% increase in constraint understanding across 147 testing tasks. Employing LLM4TDG significantly improves the average pass@k metric of all LLMs by 10.41%. The pass@k metric for CodeQwen-chat has improved by up to 18.66%. The metric surpasses the state-of-the-art GPT-4, with a performance of 92.16% on HUMANEVAL and 87.14% on HUMANEVAL+, which enhances the error correction and functional correctness in test-driven code generation. Meanwhile, Our experiments were conducted on a dataset of Python third-party libraries containing malicious behavior in the context of security testing tasks, validating the effectiveness of our method in real-world applications and its generalization capabilities."
Polarimetric scattering analysis using a Brillouin-Wigner perturbative scheme,"Franco, M; Calzetta, E; Grings, F",10.1364/JOSAA.558427,2025,"This work presents a Brillouin-Wigner-type perturbative scheme (P-S) for deriving analytical expressions for the eigenvalues and eigenvectors of the coherence matrix, enabling computationally efficient polarimetric scattering analysis. The scheme's generality allows its application with any scattering models that verify reciprocity. As an example, it is applied using the second-order small perturbation method (SPM) for both single rough surfaces and layered media. Comparison of P-S results with direct numerical calculations of the coherence matrix demonstrates high accuracy across all studied parameters, with relative mean errors ranging from 0.2% to 7%. (c) 2025 Optica Publishing Group. All rights, including for text and data mining (TDM), Artificial Intelligence (AI) training, and similar technologies, are reserved."
Formulating an Engineering Framework for Future AI Certification in Aviation,"Christensen, JM; Stefani, T; Girija, AA; Hoemann, E; Vogt, A; Werbilo, V; Durak, U; KÃ¶ster, F; KrÃ¼ger, T; Hallerbach, S",10.3390/aerospace12060482,2025,"A continuous increase in artificial intelligence (AI)-based functions can be expected for future aviation systems, posing significant challenges to traditional development processes. Established systems engineering frameworks, such as the V-model, are not adequately addressing the novel challenges associated with AI-based systems. Consequently, the European Union Aviation Safety Agency (EASA) introduced the W-shaped process, an advancement of the V-model, to set a regulatory framework for the novel challenges of AI Engineering. In contrast, the agile Development Operations (DevOps) approach, widely adopted in software development, promotes a never-ending iterative development process. This article proposes a novel concept that integrates aspects of DevOps into the W-shaped process to create an AI Engineering framework suitable for aviation-specific applications. Furthermore, it builds upon proven ideas and methods using AI Engineering efforts from other domains. The proposed extension of the W-shaped process, compatible with ongoing standardizations from the G34/WG-114 Standardization Working Group, a joint effort between EUROCAE and SAE, addresses the need for a rigorous development process for AI-based systems while acknowledging its limitations and potential for future advancements. The proposed framework allows for a re-evaluation of the AI/ML constituent based on operational information, enabling improvements of the system's capabilities with each iteration."
BTAL: An imbalance software bug report triage approach based on BERT-TextCNN,"Zhang, YM; Sun, YH; Shi, Y; Jiang, SJ; Yuan, G",10.1016/j.infsof.2025.107731,2025,"With the expansion of software project scale, a large number of software bug reports have been generated. Bug triage is an indispensable task in software development and maintenance, which directly affects the efficiency of software bug fixing and maintenance cost. Existing bug triage methods often fail to make full use of the useful information in defect reports, resulting in the neglect of some auxiliary information that is critical to the defect triage task. Meanwhile, they also fail to take into account the uncertainty of developers' work and the gap in their activity, thus leading to the imbalance in software defect report datasets. To address this issue, we propose an imbalance software bug report triage method, BTAL, based on BERT-TextCNN. Firstly, the method utilizes multiple information from software bug reports and employs BERT and TextCNN models for vector representation and feature extraction. Then, the output feature vectors are input into the softmax function to obtain the probabilities of bug reports being triaged to developers. To solve the problem of imbalanced datasets, we propose an adaptive loss function that can adaptively adjust the loss weights based on different categories of samples. This helps the network reduce its focus on majority classes and increase its focus on minority classes, thereby improving triage accuracy. Experimental results on five large-scale open-source software projects, namely GCC, NetBeans, Eclipse, Mozilla, and OpenOffice, demonstrated the effectiveness of the BTAL method in solving software bug triage problems, outperforming current state-of-the-art models."
Towards Quality Assurance of Natural Language in Code,"Zhong, RY",10.1109/ICSE-Companion66252.2025.00056,2025,"The quality of code in software development is critical to ensuring robust, functional, and maintainable applications. While static analysis techniques can improve code quality, they often overlook the natural language (NL) components of code, such as comments and logging statements, which play an essential role in program development and maintenance. In this thesis, we aim to address quality assurance for these NL elements through two primary objectives. First, we plan to propose a new benchmark to facilitate consistent comparisons among existing methods. Second, we tackle logging statement quality by identifying a broader range of defect types and introducing an automated detection and repair tool. This work addresses critical limitations in current research, which often focuses on single defect types and lacks repair functionality, by enabling a comprehensive and practical approach to NL quality in code. Our contributions provide a foundation for ongoing improvements in code maintainability and contribute to a more reliable software development lifecycle."
Developing Critical Thinking with AI Coding Assistants: An Educational Experience focusing on Testing and Legacy Code,"Blasquez, I",10.1145/3724363.3729050,2025,"The rise of AI coding assistants, like GitHub Copilot, is transforming software development. These tools promise productivity gains and support for various tasks, from code generation to explain legacy systems. However, their integration into education raises pedagogical challenges: How can we use their potential without compromising students' autonomy and mastery of fundamental concepts? How can we stay critical of their limitations? This paper explores these questions through a structured educational experience. It is based on a guided tutorial designed to confront students with the assistants' limitations. A simultaneous questionnaire is provided to allow students to take the necessary time to thoroughly analyze the assistant's responses. The tutorial has three stages. It introduces students to real-world scenarios of increasing complexity: getting started, implementing business rules, and working on a legacy project. This approach helps develop skills such as critical thinking, prompt refinement, and error correction. The results also show that well-supervised use of coding assistants can enhance teaching in testing and working with legacy code. They help students overcome initial roadblocks and encourage them to think about best practices. Furthermore, students themselves highlight the importance of supervising these tools to maintain their autonomy."
Software Fairness Debt: Building a Research Agenda for Addressing Bias in AI Systems,"De Souza, R; Fronchetti, F; Freire, S; Spinola, R",10.1145/3709357,2025,"Ensuring fairness in software systems has become a critical concern in software engineering. Motivated by this challenge, this article explores the multifaceted nature of bias in software systems, providing a comprehensive understanding of its origins, manifestations, and impacts. Through a scoping study, we identified the primary causes of fairness deficiencies in software development and highlighted their adverse effects on individuals and communities, including instances of discrimination and the perpetuation of inequalities. Our investigation culminated in the introduction of the concept of software fairness debt. In addition to defining fairness debt, we propose a socio-technical roadmap that addresses broader aspects of fairness in AI-driven systems. This roadmap is structured around six goals: bridging the gap between research and real-world applications, developing a framework for fairness debt, equipping practitioners with tools and knowledge, improving bias mitigation, integrating fairness tools into industry practice, and enhancing explainability and transparency in AI systems. This roadmap provides a holistic approach to managing biases in software systems through software fairness debt, offering actionable steps for both research and practice. By guiding researchers and practitioners, our roadmap aims to foster the development of more equitable and socially responsible software systems, ensuring fairness is embedded throughout the software lifecycle."
Contrasting to Spark Creativity in Software Development: Tactics Used By High-Performing Teams,"Petre, M; Shaw, M",10.1109/MS.2025.3538670,2025,Three decades of empirical research on high performing teams indicates that creativity can be promoted by a development culture that supports investigation of a wide range of design alternatives. This article describes contrasting and creativity moves as key development behaviors observed to support a culture of creativity.
Service-Oriented Requirements Elicitation Through Systematic Questionnaire Design: A Problem-Driven GenAI Approach,"Rauer, J; Pham, TKB; Supakkul, S; Hill, T; Chung, L",10.1007/978-981-96-0805-8_17,2025,"Service-oriented requirements elicitation emphasizes understanding and addressing stakeholder needs. As a critical phase in software development, it relies heavily on clear and effective communication with stakeholders. One valuable tool for facilitating this communication is the questionnaire. However, the challenge lies in the fact that there are an infinite number of potential questions that could be asked. Often, critical questions are forgotten or omitted, leading to incomplete requirements. Additionally, wrong questions or poorly phrased questions can result in incorrect requirements. To address these challenges, we leverage GenAI to identify well-written, critical questions. In our proposed Problem-Driven GenAI Requirements Elicitation Process, question formulation and requirements gathering are systematic, iterative, incremental, and interleaving. Our approach focuses on the four key dimensions of Requirements Engineering (RE): Functional Requirements/Non-Functional Requirements, Problem/Solution, Product/Process, and Stakeholder. Taking the cross-product of these four dimensions, we delineate 24 distinct sectors. In order to ask the right questions and the right number of questions, we use GenAI to conduct experiments that systematically generate questions across these sectors. This process helps uncover critical questions for the development of Theia, an indoor navigation system designed for visually impaired individuals. As part of our evaluation, we compare student-generated questions for Theia with those created by ChatGPT 4.0, under the guidance of Requirements Engineers. The questions are then assessed based on desirable properties of questioning. In our limited studies, we find that our problem-driven GenAI approach reduces the occurrence of omitted and wrong questions by 54.45%, depending on the sector. We believe this approach is important as it helps narrow down the infinite number of potential questions and helps address critical questions."
COPYRIGHT PROTECTION ON WORKS GENERATED BY ARTIFICIAL INTELLIGENCE,"Aronov, A; Idrysheva, S",10.15407/scine21.01.112,2025,"Introduction. Articial intelligence (AI) has profoundly impacted various aspects of human life, including text generation, software development, and art creation. Many sport and business news articles available online have been authored by AI. Under the current legal frameworks in many jurisdictions, AI-generated works have generally been regarded as tools. However, the evolution of advanced AI technologies has signicantly challenged this traditional perspective. Problem Statement. The rise of AI has introduced signicant challenges to intellectual property law, particularly copyright. In the context of copyright, AI-generated works have sparked legal disputes regarding whetherAI can be recognized as the author of creative works, how such works should be protected, and who holds the rights to them. Purpose. This study aims to critically analyze copyright issues related to AI-generated works, identify the legal regulations governing such works in developed countries, and propose recommendations to enhance Kazakhstani copyright law forAI-generated outputs. Materials and Methods. The research has employed comparative legal analysis, general scientic methods, and specic scientic approaches. Results. Drawing on foreign practices, the study has concluded that the individual who has made the necessary arrangements for the creation of AI-generated works should be recognized as the author. Conclusions. The paper provides practical recommendations for improving Kazakhstani legislation on copyright protection forAI-generated works. The findings may serve as a valuable resource for future legal research on regulating copyright for AI-generated outputs."
Generating Domain Models with LLMs Using Instruction Tuning: A Research Preview,"Ãelikmasat, G; ÃzgÃ¶vde, A; Aydemir, FB",10.1007/978-3-031-88531-0_11,2025,"[Context and Motivation] Domain models are valuable tools for facilitating software development by providing structured insights into the problem space. [Question/problem] Building domain models requires time, domain, and modeling expertise; therefore, it is an expensive task. Due to this cost, domain models are often overlooked in agile development, where the primary form of documentation is user stories that capture actors, their desires, and, optionally, the rationale. [Principal ideas] Automated domain model generation from user stories can support the software development process in agile settings. Recent advances in large language models offer promise in automating such tasks through improved language understanding and reasoning. This research preview investigates the potential of open-source large language models to automatically generate domain models from natural language requirements, employing instruction tuning on a substantial dataset of user stories and their corresponding domain models. By comparing instruction-tuned and pre-trained models, we aim to assess the specific impact of instruction tuning on model generation quality. [Contributions] This study is, to our knowledge, the first to apply instruction tuning on large language models specifically for domain model generation. Through qualitative and quantitative evaluations of factors like completeness and correctness, we assess the performance and quality of the instruction-tuned models."
Prompt Alchemy: Automatic Prompt Refinement for Enhancing Code Generation,"Ye, SX; Sun, ZY; Wang, GQ; Guo, LW; Liang, QY; Li, Z; Liu, Y",10.1109/TSE.2025.3589634,2025,"Code generation has gained increasing attention as a task to automate software development by transforming high-level descriptions into executable code. While large language models (LLMs) are effective in generating code, their performance heavily relies on the quality of input prompts. Current prompt engineering methods involve manual effort in designing prompts, which can be time-consuming and yield inconsistent results, potentially constraining the efficacy of LLMs in practical applications. This paper introduces Prochemy, a novel approach for automatically refining prompts iteratively to enhance code generation. Prochemy addresses the limitations of manual prompt engineering by automating the optimization process, ensuring prompt consistency during inference, and aligning with multi-agent systems. It iteratively refines prompts based on model performance, using an optimized final prompt to improve consistency and reliability across tasks. We evaluate Prochemy on both natural language-based code generation and code translation tasks using three series of LLMs. Results show that when combining Prochemy with existing approaches, it outperforms baseline prompting methods. It achieves improvements of 5.0% (GPT-3.5-Turbo) and 1.9% (GPT-4o) over zero-shot baselines on HumanEval. For the state-of-the-art LDB, Prochemy + LDB outperforms standalone methods by 1.2-1.8%. For code translation, Prochemy elevates GPT-4o's performance on Java-to-Python (AVATAR) from 74.5 to 84.1 (+12.9%) and Python-to-Java from 66.8 to 78.2 (+17.1%). Furthermore, considering that the o1-mini model integrates prompt engineering techniques, Prochemy can continue to show good performance among it, further validating its effectiveness in code generation and translation tasks. Additionally, Prochemy is designed to be plug-and-play, optimizing prompts with minimal human intervention and seamlessly bridging the gap between simple prompts and complex frameworks."
LEGO: Synthesizing IoT Device Components Based on Static Analysis and Large Language Models,"Liu, LW; Wang, T; Chen, W; Wei, J; Wang, W; Wu, GQ",10.1145/3729482,2025,"IoT device components-digital representations of IoT devices within a platform and typically developed using Software Development Kits (SDKs)-are essential for ensuring seamless connectivity between IoT platforms and physical devices. However, developing these components demands extensive domain knowledge, as developers must understand the necessary elements of an IoT device and effectively utilize SDKs. Unfortunately, limited research has focused on automating this process, resulting in labor-intensive, time-consuming development. To tackle these challenges, we introduce LEGO, a method for synthesizing IoT device components based on the observation that APIs provided by device SDKs would eventually call network protocol methods to access physical devices. LEGO analyzes the SDK source code to identify candidate APIs that communicate with physical devices. Using static analysis, it generates a dataflow-enhanced call graph, extracts call paths containing network protocol methods, and heuristically identifies APIs that invoke these methods. To efficiently classify each API type and infer relevant device properties, LEGO employs a large language model-based program comprehension technique with an information-augmented prompt. LEGO then synthesizes device components using a platform-specific template, built from a common IoT device component model. It assembles IoT device components by populating the template with inferred properties and identified APIs, enabling developers to efficiently develop device components with minimal SDK knowledge. Comprehensive experiments on a set of open-source device SDKs and ten real-world IoT devices demonstrate the efficiency and effectiveness of LEGO in creating IoT device components."
Towards Enhancing Task Prioritization in Software Development Through Transformer-Based Issues Classification,"Haugerud, KM; Shivashankar, K; Martini, A",10.1007/978-3-031-78386-9_17,2025,"Prioritizing tasks is extremely beneficial, but also difficult for software development teams. Assigning priority to tasks is also time-consuming, especially in projects with a high volume of new issues. Consequently, many issues in GitHub are not labelled. An effective priority tool can streamline this process by suggesting priority labels, saving developers' time and enabling faster identification of high-impact product improvements. In this paper we investigate the application of text classification using Transformer models to automatically assign priority labels to software development issues. We used data from the GitHub and Jira vast datasets to develop state-of-the-art machine learning models (Transformers) to automatically classify the priority of text issues. We thoroughly evaluated the generalizability of our models by using issues that are self-tagged by developers in projects that were not part of the training (Out-of-Distribution) and we adapted our models to specific projects by incorporating part of the issues in the training (fine-tuning) to improve performance. Our experiments show that results vary but can reach a performance of correctly labeling 80% of high priority issues in a project. Our results indicate that Transformers have the potential to assist developers in (semi-)automatically assigning priority labels to their issues and therefore reducing overhead. We find that fine-tuning improve significantly the performance by adapting the machine learning models to specific projects, but further research is needed to optimize this approach."
Code Summarization Using Mamba-SoTaNa,"Lee, J; Ju, E; Ryu, D",10.1109/BigComp64353.2025.00038,2025,"Effectively understanding and managing source code has become increasingly important as modern software systems grow in complexity. This study aims to explore the potential of a model that can maximize memory efficiency in resource-constrained environments and enhance performance on large codebases, providing practical benefits for software development and maintenance. To achieve this, the Mamba model was integrated into SoTaNa, and experiments were conducted to measure code summarization performance using BLEU, ROUGE-L, METEOR, and BERTScore metrics. The performance of the Mamba-SoTaNa model was compared with that of the existing SoTaNa and LLaMA models. The results showed that the Mamba-SoTaNa model outperformed the LLaMA model but did not surpass the performance of the SoTaNa model. In conclusion, this study highlights the need for further research and optimization to improve the code summarization performance of the Mamba model."
A Systematic Survey on Large Language Models for Static Code Analysis,"Salih, HAM; Sarhan, QI",10.14500/aro.12082,2025,"code analysis plays a pivotal role in improving software quality, security, and maintainability by detecting vulnerabilities, errors, and programming issues in source code without executing it. Recent advancements in artificial intelligence, especially the development of large language models (LLMs), such as ChatGPT, have enabled transformational opportunities in this domain. Thus, it is essential to explore this emerging field of research from many perspectives. This systematic survey focuses on the use of LLMs for static code analysis, detailing their applications, advantages, contexts, limitations, etc. The study examines research papers published on the topic from reputable literature databases to answer several research questions regarding the state-of-the-art use of LLMs in static code analysis. In addition, different research gaps and challenges were identified and discussed alongside many directions. The results of this study demonstrate how LLMs can enhance static code analysis and address existing limitations, paving the way for developers and researchers to employ LLMs for a more affordable and effective software development process."
Integrating In Vitro Propagation and Machine Learning Modeling for Efficient Shoot and Root Development in Aronia melanocarpa,"Yaman, M; Palaz, EB; Isak, MA; Demirel, S; IzgÃ¼, T; Adali, S; Demirel, F; Simsek, Ã; Popescu, GC; Popescu, M",10.3390/horticulturae11080886,2025,"Aronia melanocarpa (black chokeberry) is a medicinally valuable small fruit species, yet its commercial propagation remains limited by low rooting and genotype-specific responses. This study developed an efficient, callus-free micropropagation and rooting protocol using a Shrub Plant Medium (SPM) supplemented with 5 mg/L BAP in large 660 mL jars, which yielded up to 27 shoots per explant. Optimal rooting (100%) was achieved with 0.5 mg/L NAA + 0.25 mg/L IBA in half-strength SPM. In the second phase, supervised machine learning models, including Random Forest (RF), XGBoost, Gaussian Process (GP), and Multilayer Perceptron (MLP), were employed to predict morphogenic traits based on culture conditions. XGBoost and RF outperformed other models, achieving R2 values exceeding 0.95 for key variables such as shoot number and root length. These results demonstrate that data-driven modeling can enhance protocol precision and reduce experimental workload in plant tissue culture. The study also highlights the potential for combining physiological understanding with artificial intelligence to streamline future in vitro applications in woody species."
Exploiting Vision-Language Models in GUI Reuse,"Niu, V; Alshammari, W; Iluru, NM; Teeleti, PV; Niu, N; Bhowmik, T; Zhang, JZ",10.1109/ICSR66718.2025.00009,2025,"Graphical user interface (GUI) prototyping helps to clarify requirements and keep stakeholders engaged in software development. While contemporary approaches retrieve GUIs relevant to a user's query, little support exists for the actual reuse, i.e., for using an existing GUI to create a new one. To shorten the gap, we investigate GUI-centered reuse via one of the latest artificial intelligence (AI) techniques-vision-language models (VLMs). We report an empirical study involving 73 university students working on ten GUI reuse tasks. Each task is associated with different reuse directions recommended by VLMs and by a natural language (NL) method. In addition, a focused GUI element is provided to offer a starting point for making the actual changes. Our results show that VLMs significantly outperform the NL method in making reuse recommendations, but surprisingly, the focused GUI elements are not consistently modified during reuse. With the assessments made by four experienced designers, we further offer insights into the creativity of human-reuse and AI-reuse results."
Novice Perceptions on Effective Elements of PostgreSQL Error Messages,"Taipalus, T; Grahn, H; Rionummi, S; Siitonen, V; Vartianen, T; Zhidkikh, D",10.1145/3732790,2025,"SQL compiler error messages are the primary way users receive feedback when they encounter syntax errors or other issues in their SQL queries. Effective error messages can enhance the user experience by providing clear, informative, and actionable feedback. Despite the age of SQL compilers, it still remains largely unclear what contributes to an effective SQL error message. With 2,052 answers yielded by 165 participants for qualitative analysis, this study is an attempt to understand what novices perceive as effective elements in SQL error messages. The results uniformly indicate that communicating the precise error position, articulating what is wrong in the query with clear natural language, and showing hints on how to fix the error are perceived as the most effective elements for error recovery. These insights have potential to be utilized in providing more effective error messages in SQL compilers and SQL learning environments, and for guiding generative AI for enhanced error messages in order to minimize frustration caused by cryptic error messages, improving learning and adoption, and reducing debugging time."
Responsible AI in Agile Software Engineering - An Industry Perspective,"Ulfsnes, R; Moe, NB; Emmerhoff, J; Floryan, M; Griva, A; Gundelsby, JH; Barbala, AM; Conboy, K",10.1007/978-3-031-72781-8_4,2025,"There is a rapid emergence of tools, methods, and guidance for the use of AI across all parts of the software development process, from requirements gathering to code generation to testing and user feedback. However, AI raises many concerns regarding responsible use, and there is a need to understand and develop principles for what responsible software development entails in practice in an agile context, as well as carefully evaluate the incorporation of AI tools and methods in software engineering. We draw on experience from Bespot, Knowit, Schibsted, and Spotify to identify challenges faced by companies pioneering the use of AI in their software development efforts and start charting a roadmap for responsible AI in software engineering."
Learning Global Context and Fine Structures for Enhanced Hyperspectral Subpixel Mapping,"Zhou, W; Ma, AL; He, D; Zhong, YF",10.1109/LGRS.2025.3531664,2025,"Subpixel mapping (SPM) is a crucial technique in remote sensing imagery analysis, aimed at characterizing subpixel distribution within the mixed pixels. Traditional SPM methods and convolutional neural network (CNN)-based SPM methods primarily rely on local spatial autocorrelation, which limits their ability to capture long-range dependencies between distant locations or objects. To address this limitation, we propose a global-local spatial dependence integrator for the SPM method (GLSDSPM) that employs both CNN and the vision transformer as dual-path structures to model global context and local spatial dependencies efficiently. Besides, the previous SPM methods often struggle to accurately reconstruct high-quality spatial patterns for linear features, such as slender rivers and roads, due to insensitivity to textures and sharp, high-frequency details. To overcome this challenge, we integrate a linear pattern refinement module (LPRM) into GLSDSPM, which adaptively focuses on thin and long local structures to accurately capture high-frequency features and detailed information. Two experiments conducted on the Pavia and Houston hyperspectral images prove that the proposed method achieves superior performance, outperforming the state-of-the-art by 4.13% and 3.95% in overall accuracy (OA), respectively."
Rethinking Software Development Considering Collaboration with AI Assistants,"Donato, B",10.1109/ICSE-Companion66252.2025.00045,2025,"The integration of AI into software development is transforming coding practices, with tools like GitHub Copilot marking the beginning of AI-assisted development. However, the interaction between developers and AI assistants is not yet well understood, presenting both opportunities and challenges. This PhD thesis addresses three core objectives: (1) improving the effectiveness of developer-AI interactions, (2) redefining development practices to incorporate AI as an essential collaborator, and (3) exploring collaborative dynamics within mixed teams of developers and AI agents. The aim is to establish methodologies that can boost productivity and software quality, paving the way for seamless human-AI collaboration in development."
Structural Semantic Enhancement: Better integrating code semantics for vulnerability detection,"Wang, SH; Wu, Y; Cui, ZF; Chen, L",10.1016/j.infsof.2025.107824,2025,"Code vulnerability detection is particularly critical in software development and maintenance because it may prevent software instability, data leakage, or more serious security threats. Traditional code vulnerability detection methods usually rely on static analysis. While static analysis covers the entire code base and detects early errors, it may struggle with highly complex code structures, leading to potential false positives or false negatives. Deep learning has introduced new opportunities for detecting vulnerabilities but faces challenges with complex code structures and logical relationships. Efforts to integrate natural language processing embeddings into models like Graph Neural Networks aim to enhance semantic understanding but depend on the quality of the NLP model and embeddings. To address these challenges, we propose a methodology centered around the Structural Semantic Enhancement Method (SSEM), which combines the semantic understanding of deep learning with structured code information provided by static analysis. Specifically, our method extracts the key information of control flow graphs and data dependency graphs and designs specialized SSEM with attention mechanisms. Based on two large-scale datasets, including more than 40,000 code snippets, we experimentally validated the effectiveness of the proposed method. Experimental results show that our method performs better in identifying potential vulnerabilities in code compared to traditional deep learning methods and advanced deep learning vulnerability detection models."
Co-Pilot for Project Managers: Developing a PDF-Driven AI Chatbot for Facilitating Project Management,"Alam, K; Bhuiyan, MH; Islam, MS; Chowdhury, A; Bhuiyan, ZA; Ahmmed, S",10.1109/ACCESS.2025.3548519,2025,"Our AI-driven PDF Chatbot is specialized for Project Management (PM) Automation and acts as a virtual Project Manager that offers continuous support to global teams. It interprets PDF data like SRS reports and interview transcripts by utilizing Open-Assistant's SFT-1 12B Model. Insights from interviews of 15 project managers have enriched the knowledge base of our chatbot and ultimately enabled informative responses to the stakeholders of the project. Advanced AI techniques ensure efficient text preprocessing, including tokenization, numerical normalization, lowercasing, removing punctuation, removing extra spaces, recursive character text splitter, and lemmatization. It is primarily tailored for e-commerce project and provides precise guidance based on e-commerce data and risk management factors. With an average cosine similarity of 80.80% and semantic similarity score of 85.21%, it consistently aligns with PDF Contents and optimize the project management phases & methodologies. This innovation enhances Human-Robot Interaction, PM Automation, facilitates decision-making, and enables uninterrupted communication. While AI-driven PDF chatbots like ChatPDF and SciSummary exist, our chatbot is uniquely focused on automating project management tasks, providing tailored insights for e-commerce projects and decision-making, thus offering a breakthrough in PM automation. To ensure the chatbot's robustness in context-aware responds, we compare our chatbot with ChatPDF and Sci-summary which are some PDF driven chatbots. Making our work available open-source on https://github.com/codewithkhurshed/SPM-project-repo can enhance its accessibility and promote future research opportunities in PDF driven chatbot development."
Striking a balance between diversity and regularity: a preference-guided transformer for individual mobility prediction,"Li, GY; Xu, Y; Gui, ZP; Guo, XG; Tang, LL",10.1080/13658816.2025.2534159,2025,"Human mobility modeling and prediction are central research topics in GIScience. Although deep learning has led to significant advances in these fields, existing trajectory prediction models still face challenges in capturing the complexity of individual mobility behavior. Regression-based models often overestimate the diversity of human mobility, whereas classification models tend to underestimate it. This study attributes these biases to the models' limitations in recognizing the spatial relationships among activity locations and mobility heterogeneity across individuals. To address these challenges, we propose the Spatial Preference Map-based Transformer (SPM-Former), explicitly integrating spatial proximity and mobility heterogeneity to enhance trajectory sequence prediction. To capture individual mobility characteristics, SPM-Former utilizes the Spatial Preference Map (SPM) to represent individuals' spatial visitation preferences and adjacency relationships between locations. Then, we introduce two encoding modules to decode the information hidden within the SPM: one for encoding trajectory-level spatial-temporal information and another for embedding individual-level overall mobility features. Furthermore, we propose a novel optimization method, SPM-Loss, to assess prediction accuracy from the global spatial distribution perspective. Experimental results on a large-scale dataset from Japan demonstrate that SPM-Former outperforms state-of-the-art classification-based models, achieving approximately 3% and 20% improvements in trajectory sequence similarity and overall spatial feature similarity, respectively."
"Software in Metrology for the Digital Age: Addressing Compliance Challenges with MID, NAWID, and MDR","Koval, M; Tesar, J; Stanek, M; Foltynek, J",10.1051/epjconf/202532302001,2025,"In the digital era, software is integral to devices regulated by European directives like MID, NAWID, and MDR, ensuring accuracy, reliability, and safety. While metrology has been crucial, software now plays an equally significant role, enabling data processing, automation, and new functionalities. Emerging technologies like AI, IoT, or cloud computing introduce compliance challenges. This article analyzes and compares software requirements in MID, NAWID, and MDR, focusing on software lifecycle, regulatory gaps, and risk assessment."
Unveiling code clone patterns in open source VR software: an empirical study,"Chen, HS; Huang, ZS; Xu, YF; Huang, WJ; Wang, XH; Chen, JF; Li, HT; Peng, KB; Liu, F; He, S",10.1007/s10515-025-00536-2,2025,"Code cloning is frequently observed in software development, often leading to a variety of maintenance and security issues. While substantial research has been conducted on code cloning in traditional software, to the best of my knowledge, there is a lack of studies on cloning in virtual reality (VR) software that consider its unique nature, particularly the presence of numerous serialized files in conjunction with the source code. In this paper, we conduct the first large-scale quantitative empirical analysis of software clones in 345 open-source VR projects, using the NiCad detector for source code clone detection and large language models (LLMs) for identifying serialized file clones. Our study leads to a number of insights into cloning phenomena in VR software, guided by seven carefully formulated research questions. These findings, along with their implications, are anticipated to provide useful guidance for both researchers and software developers within the VR field."
Not My First Choice: Why Software Testing Struggles to Attract Early-Career Professionals,"Mulgund, P; Singh, R; Gillenson, M; Li, YF; Oliver, C",10.1145/3716489.3728443,2025,"This study investigates why software testing remains a less attractive career path for new graduates and early-career professionals despite its key role in preserving software quality. A qualitative thematic analysis of 615 online forum discussions is conducted to uncover structural, cultural and individual barriers that underpin negative perceptions associated with the role. We ground our findings in the Job Demands-Resources (JD-R) framework and highlight how software testers shoulder demands pertaining to tight timelines, emotional labor, and automation- and AI-related threats without access to adequate resources such as job autonomy, career pathways and recognition. The study contributes to IS literature by examining a critical but often overlooked profession, and develops a foundation for organizations to address this structural imbalance to enhance both testers' well-being and software quality. Notably, our findings and proposed interventions are aimed to reverse the undervaluation of software testers and promote a more equitable and inclusive IT workforce."
A credibility and consistency-oriented stochastic aggregation framework for heterogeneous multi-attribute large-scale group decision making with several attribute sets,"Li, WW; Yi, PT; Zhang, DN",10.1016/j.engappai.2025.112584,2025,"In large-scale group decision making, experts usually provide their individual preferences on attribute values, as well as attribute sets. The coexistence of large-scale heterogeneous information poses a significant challenge to the credibility and consistency on decision making. To this issue, the paper proposes a credibility and consistency-oriented stochastic aggregation framework, including four primarily research points. Firstly, a simplified transformation method is developed to convert heterogeneous attribute values into individual attribute superiority-probability-based pairwise comparison matrix (IA-SPMs), which saves transformation cost as well as provides abundant references for credibility analysis. Secondly, the deviation-based credibility measures and the credibility-based weighting methods are proposed. Thirdly, a cluster-based aggregation operator is introduced by considering group consistent preferences on attributes selection to get the collective SPM (C-SPM). Fourthly, the ranking probability matrix (RPM) and the possibility ranking result are calculated based on the CSPM. Using experiment and application analyses we illustrate that the proposed methods can enhance the credibility of decision outcomes, as well as the stability of the results. This research can provide technical support for effective fusion of heterogeneous information from multiple sources in the artificial intelligence (AI) era, and has broad application potential in large-scale democratic decision making and pre-evaluation engineering projects."
Measuring how computer science research translates into innovation and development,"Cinus, F; Septiandri, A; Constantinides, M; Quercia, D",10.1140/epjds/s13688-025-00568-4,2025,"What factors determine the impact of a scientific paper? Does its impact extend to patents and software development, or is it primarily confined to academic circles? While current literature predominantly adopts a descriptive approach, emphasizing patent citations as indicators of industry impact, the role of research in driving software development is overlooked. To address this gap, we quantitatively assessed the impact of research papers on both patents and software repositories. With a computational social science approach, we collected, curated, and analyzed a large-scale dataset of 200K papers published between 1980 and 2022 across the research areas of AI, Computer Vision, Data Mining, Databases, HCI, and NLP, including conferences like NeurIPS, ICML, ACL, CVPR, CHI, KDD, and The Web Conference. We found that, on average, 7.1% of papers from these venues became patents and 11.6% went into repositories-significantly higher than top general science journals (3.8% for patents and 0.02% for repositories). Despite being a minority, these papers have received a disproportionate number of citations-4% of AI papers became patents, and 18% went into repositories, yet they have received 29% and 42% of the area's academic citations, respectively. However, after correcting for papers published at different times with survival analysis, we found that there is a significant time lag between patents or repositories and papers (10-15 years for patents, 5 years for repositories in Computer Vision and NLP, and even longer for top general science journals at 30 years). As for consistent trends, Deep Learning has become exponentially popular, and papers with code are becoming the norm. Finally, we showed that a paper's publication venue and the extent to which a paper builds upon (un)conventional knowledge determine the impact on patents and repositories, with greater conventionality predicting impact on patents, and lesser conventionality predicting impact on repositories."
Automated categorization of software security requirements: an NLP and ML based approach,"Batool, R; Naseer, A; Maqbool, A; Kayani, M",10.1007/s00766-025-00443-8,2025,"Accurate categorization of software requirements into security and non-security categories is crucial for project management as it helps in prioritization, resource allocation, and risk management. The automated labeling of software requirements helps organizations to quickly classify and organize vast amounts of requirements data, reducing time and effort required for manual labeling. In this paper, we investigate the automatic labeling of software requirement sentences by utilizing TF-IDF in conjunction with individual keyword comparison (IKC) and Combined Keyword Comparison techniques. The validation of software requirements is performed using three different classifiers including Logistic Regression, Support Vector Machine (SVM), and Random Forest. In this study, we utilize the PROMISE, DOSSPRE, and DONSPRE datasets to conduct our analysis. Experimental results show that the SVM classifier achieved superior performance, attaining an accuracy of 94% after applying the proposed automated score-based labeling (ASBL) technique using IKC at the median threshold. The research contribution of this paper is twofold. Firstly, the suggested automated score-based labeling (ASBL) technique can significantly enhance the efficiency and accuracy of requirement categorization in software development projects, hence reduce the potential for human error in the labeling process. Secondly, a new dataset is created with more detailed consideration on security requirements of software projects."
PassionNet: An innovative framework for duplicate and conflicting requirements identification,"Saleem, S; Asim, MN; Dengel, A",10.1016/j.eswa.2025.128684,2025,"Early detection of duplicate and conflicting requirements in software development lifecycle is crucial to achieve software project efficiency, quality, and market success. Primarily, duplicate detection requires identifying semantic equivalence, intent alignment, functional overlap, and domain-specific terminology variations between differently worded requirements. Whereas, conflict detection demands recognising logical contradictions, constraint violations, resource conflicts and temporal incompatibilities between requirements. To handle multidimensional demands of two different task types, researchers have developed 32 AI based duplicate and conflicting requirement detection predictors. However, despite the utility of sophisticated large language models (LLMs) and sampling techniques, existing approaches significantly lack in performance because they fail to comprehensively handle multi-dimensional demands of both tasks. To address these gaps, this paper presents a modular framework PassionNet which implements a novel strategy of integrating 10 different multi-dimensional similarity assessments with the contextual understanding of 8 unique language model variants. The framework enables three distinct pipeline types: language model-based pipelines that capture semantic intent, similarity knowledge-driven pipelines that detect lexical, structural and distributional patterns, and hybrid pipelines that combine both approaches to simultaneously assess all dimensions of requirement relationships. Our experimental evaluation of 760 pipelines across six public datasets demonstrates that hybrid pipelines outperform the other two approaches in terms of F1-score as compared to state-of-the-art methods. Specifically, the hybrid pipeline achieves an improvement in F1-score of approximately 4% on the WorldVista dataset, 5% on the UAV dataset and 3 % on the Pure dataset as compared to the state-of-the-art models. Statistical validation through t-tests confirms the significance of these improvements (p < 0.1 with 10 permutations, approaching zero with 1000 permutations). The results provide empirical evidence that effective requirement analysis requires simultaneously assessing semantic, lexical, structural, and logical dimensions of requirements rather than focusing on isolated aspects. To facilitate software engineers, researchers and practitioners, PassionNet web application is deployed at https://sds_requirement_engineering.opendfki.de/."
Intelligent Semantic Matching (ISM) for Video Tutorial Search using Transformer Models,"Tayeb, AJ; Haiduc, S",10.1109/MSR66628.2025.00108,2025,"The rise in the number and diversity of available software development video tutorials has enhanced digital learning for developers but also introduced challenges in locating relevant content efficiently. Existing video search methods, including keyword-based approaches and tools like CodeTube and TechTube, rely primarily on retrieval algorithms such as BM25, which fail to capture the semantic nuances and user intentions behind search queries. To address these limitations, we introduce ISM, an approach that uses SBERT to generate semantically rich vectors from video tutorial transcripts to improve the search for programming video tutorials. By segmenting transcripts and implementing a re-ranking process, ISM effectively preserves context and enhances the relevance of search results. Additionally, ISM generates informative video summaries using GPT-4, allowing developers to quickly assess the relevance of video content. To evaluate our approach, we first performed a quantitative study comparing ISM with the baseline TechTube. The results revealed that ISM performs better in both video retrieval and fragment identification, achieving a Hit@5 score of 0.95 and an average F1 score of 0.70 compared to the baseline's 0.58 and 0.52, respectively. We also performed a user study, which revealed that users strongly preferred the semantic matching capabilities and AI-generated summaries of our approach. This work advances the state-of-the-art in programming video tutorial search and summarization by offering more nuanced and user-aligned retrieval and summarization mechanisms."
CodeDoctor: multi-category code review comment generation,"Li, YL; Wu, YH; Wang, ZA; Huang, L; Wang, JJ; Li, JP; Huang, MY",10.1007/s10515-025-00491-y,2025,"Code review is an effective software quality assurance activity. However, this process is labor-intensive and time-consuming, requiring reviewers to carefully review under various categories (e.g., function, refactoring, documentation, etc) to generate review comments. Several approaches have been proposed for automatic review comment generation, although they can generate review comments, they hardly cover all manual review comments. Because most of these approaches simply utilize the information of submitted code and review comments, not fully modeling the features of code review (i.e., ignoring review category, the association of issue snippets and review comments). In this paper, we propose CodeDoctor, an automatic review comment generator with data augmentation and category-aware encoder-decoder to generate multi-category review comments. It consists of three main phases: (1) Data augmentation phase, which classifies review comments and builds review exemplars (i.e., the pairs of issue snippet and its comment) to augment review data by using a large language model (LLM) with prompt engineering and feedback loops; (2) Encoder phase, which encodes the inputs (i.e., review category, diff code and review exemplar) into semantic and token representations; (3) Decoder phase, which designs a category-focused decoder to capture the most relevant information of given category for multi-category review comment generation. Evaluations with five commonly-used and state-of-the-art baselines on two datasets show that CodeDoctor outperforms all baselines, with 1770% higher average BLEU-4, 111% higher average ROUGE-L and 49% higher average F1 than the best baseline. Furthermore, a human evaluation also confirms the significant potential of applying CodeDoctor in practical usage. Our approach can relieve the burden of reviewers by automatically generating multi-category review comments, and helps developers better detect code issues as early as possible, thereby facilitating software development."
Autonomous AI-Driven Measurement and Characterization of 2D Materials Using Scanning Probe Microscopy,"Sung, J; Heo, S; Kim, D; Kwon, Y; You, J; Joo, Y; Hwang, E; Lee, J; Cho, SJ; Yang, H; Kim, Y",10.1002/sstr.202500379,2025,"Scanning probe microscopy (SPM) has become a valuable tool for probing physical properties and nanoscale materials and devices. However, conventional SPM imaging requires manual identification of regions of interest and heavily depends on human intuition for image interpretation, which severely limits the ability to collect large datasets and conduct objective analysis. In this work, an AI-assisted autonomous SPM framework is presented for microstructural and electrical property characterization of 2D materials with high efficiency. By analyzing topographic features through advanced clustering algorithms, the approach employs accurate image segmentation of complex geometries and multilevel thickness variations in overlapping 2D MoWTe2 flakes. To demonstrate its scalability, this autonomous workflow is applied to over 100 MoWTe2 flakes. Levering SPM's multimodal imaging capabilities, the framework simultaneously extracts flake thickness and work function, allowing for direct correlation between these properties. This deep-learning-driven autonomous approach mitigates the need for manual intervention, significantly accelerating the exploration and characterization of nanomaterials across diverse material systems."
Binary and multi-class classification of Self-Admitted Technical Debt: How far can we go?,"Fontana, FA; Di Rocco, J; Di Ruscio, D; Di Salle, A; Nguyen, PT",10.1016/j.infsof.2025.107862,2025,"Context: Aiming for a trade-off between short-term efficiency and long-term stability, software teams resort to sub-optimal solutions, neglecting the best software development practices. Such solutions may induce technical debt (TD), triggering maintenance issues. To facilitate future fixing, developers mark code with any issues using textual comments, resulting in Self-Admitted Technical Debt (SATD). Detecting SATD in source code is crucial since it helps programmers locate potentially erroneous snippets, allowing for suitable interventions, and improving code quality. There are two main types of SATD detection, i.e., binary classification and multi-class classification, grouping TD comments into SATD/Non-SATD categories, and multiple categories, respectively. Objective: We attempt to understand to which extent state-of-the-art research has addressed the issue of detecting SATD, both binary and multi-class classification. Based on this investigation, we also propose a practical approach for the detection of SATD using Large Language Models (LLMs). Methods: First, we conducted a literature review to understand to which extent the two types of classification have been tackled by existing research. Second, we developed SALA, a dual-purpose tool on top of Natural Language Processing (NLP) techniques and neural networks to deal with both types of classification. An empirical evaluation has been performed to compare SALA with state-of-the-art baselines. Results: The literature review reveals that while binary classification has been well studied, multi-class classification has not received adequate attention. The empirical evaluation shows that SALA obtains a promising performance, and outperforms the baselines with respect to various quality metrics. Conclusion: We conclude that more effort needs to be spent to tackle multi-class classification of SATD. To this end, LLMs hold the potential, albeit with more rigorous investigation on possible fine-tuning and prompt engineering strategies."
Foundation Models for Automatic Issue Labeling,"Colavito, G",10.1109/ICSE-Companion66252.2025.00038,2025,"Foundation models are transforming software engineering practices through their ability to understand and generate code, process natural language, and automate various development tasks. Despite their potential, effectively applying these models to specialized software engineering tasks remains challenging due to the need for domain-specific understanding and accurate labeling of data. This research project investigates how foundation models can be leveraged to automate labeling tasks in software engineering, with a specific focus on issue classification as a representative case study. Issue tracking systems, while essential for collaborative software development, often suffer from misclassification problems that require significant manual effort to correct. We explore how foundation models can be adapted to automatically label issues accurately, reducing the need for manual intervention while maintaining high-quality classification. The project examines several key aspects: the capabilities of different foundation models in understanding software engineering artifacts, methods for adapting these models to specific labeling tasks through techniques like prompt engineering and few-shot learning, and approaches for integrating automated labeling into real-world scenarios. This research contributes to the broader understanding of how foundation models can be effectively applied to reduce manual labeling efforts across various software engineering contexts, using issue classification as a concrete demonstration of their potential."
Innovating for Tomorrow: The Convergence of Software Engineering and Green AI,"Cruz, L; Franch, X; MartÃ­nez-FernÃ¡ndez, S",10.1145/3712007,2025,"The latest advancements in machine learning, specifically in foundation models, are revolutionizing the frontiers of existing software engineering (SE) processes. This is a bi-directional phenomenon, where (1) software systems are now challenged to provide AI-enabled features to their users, and (2) AI is used to automate tasks within the software development lifecycle. In an era where sustainability is a pressing societal concern, our community needs to adopt a long-term plan enabling a conscious transformation that aligns with environmental sustainability values. In this article, we reflect on the impact of adopting environmentally friendly practices to create AI-enabled software systems and make considerations on the environmental impact of using foundation models for software development."
Intelligent Root Cause Localization in MicroService Systems: A Survey and New Perspectives,"Fu, N; Cheng, G; Teng, Y; Dai, GY; Yu, S; Chen, ZH",10.1145/3736755,2025,"Root cause localization is the process of monitoring system behavior and analyzing fault patterns from behavioral data. It is applicable in software development, network operations, and cloud computing. However, with the advent of microservice architectures and cloud-native technologies, root cause localization becomes an arduous task. Frequent updates in systems result in large-scale data and complex dependencies. Traditional analysis methods relying on manual experience and predefined rules have limited data processing and cannot learn new fault patterns from historical knowledge. Artificial Intelligence techniques have emerged as powerful tools to leverage historical knowledge and are now widely used in root cause localization. In this article, we provide a structured overview and a qualitative analysis of root cause localization in microservice systems. To begin with, we review the literature in this area and abstract a workflow of root cause localization, including multimodal data collection, intelligent root cause analysis, and performance evaluation. In particular, we highlight the role played by Artificial Intelligence techniques. Finally, we discuss some open challenges and research directions and propose an end-to-end framework from a new perspective, providing insights for future works."
Leveraging Machine Learning for Enhanced Bug Triaging in Open-Source Software Projects,"Adhikari, N; Bista, R; Ferreira, JC",10.1109/ACCESS.2025.3595011,2025,"Bug triaging-the process of classifying and assigning software issues to appropriate developers-is a critical yet challenging task in large-scale software development. Manual triaging is time-consuming, inconsistent, and prone to human bias, which often delays issue resolution and misallocates developer resources. This study explores the application of machine learning to automate and improve bug triaging efficiency and accuracy. Using a dataset of over 122,000 issues from the microsoft/vscode GitHub repository, we evaluate several machine learning models including Bidirectional LSTM, CNN-LSTM, Random Forest, and Multinomial Naive Bayes. Our primary contribution is the development of an Augmented Bidirectional LSTM model that integrates enriched textual features and contextual metadata. This model, optimized using Optuna, outperforms traditional baselines, achieving a Micro F1-score of 0.6469 and Hamming Loss of 0.0133 for label prediction, and a Micro F1-score of 0.5974 with Hamming Loss of 0.0062 for assignee recommendation. In addition to demonstrating strong predictive performance, we present a robust end-to-end pipeline for data preprocessing, augmentation, model training, and evaluation using multi-label classification techniques. The study highlights how deep learning architectures, in combination with feature engineering and hyperparameter tuning, can provide scalable and generalizable components to support the automation of bug triaging. These findings contribute to the growing field of intelligent software maintenance by offering data-driven approaches that can support developer workflows and improve issue management efficiency in open-source environments."
A First Look at Conventional Commits Classification,"Zeng, QH; Zhang, YX; Qiu, ZQ; Liu, H",10.1109/ICSE55347.2025.00011,2025,"Modern distributed software development relies on commits to control system versions. Commit classification plays a vital role in both industry and academia. The widely-used commit classification framework was proposed in 1976 by Swanson and includes three base classes: perfective, corrective, and adaptive. With the increasing complexity of software development, the industry has shifted towards a more fine-grained commit category, i.e., adopting Conventional Commits Specification (CCS) for delicacy management. The new commit framework requires developers to classify commits into ten distinct categories, such as feat, fix, and docs. However, existing studies mainly focus on the three-category classification, leaving the definition and application of the fine-grained commit categories as knowledge gaps. This paper reports a preliminary study on this mechanism from its application status and problems. We also explore ways to address these identified problems. We find that a growing number of projects on GitHub are adopting CCS. By qualitatively analyzing 194 issues from GitHub and 100 questions from Stack Overflow about the CCS application, we categorized four main challenges developers encountered when using CCS. The most common one is CCS-type confusion. To address these challenges, we propose a clear definition of CCS types based on existing variants. Further, we designed an approach to automatically classify commits into CCS types, and the evaluation results demonstrate a promising performance. Our work facilitates a deeper comprehension of the present fine-grained commit categorization and holds the potential to alleviate application challenges significantly."
"Responsible Digitalization or the Digitalization of Responsibility? Work, Technology and Responsibility Practices","Pieters, W",10.1007/978-3-031-88881-6_1,2025,"Around new technologies with large potential impact, such as artificial intelligence, there is often a call for responsible innovation. This requires early involvement of stakeholders to identify relevant values, in order to make sure that those values can be taken into account in the design. However, it is not always clear to what extent such adaptations in the design find a place in the practices surrounding the use of the new technology in work environments. For example, if artificial intelligence is made explainable, why and how would users have an incentive to make use of such a feature? In order to better understand these issues, I argue that we should focus not only on responsible design, but also on the way in which new technologies reshape responsibility practices in organizations, including for example seeking, receiving and challenging advice, searching for information, or communicating decisions. I discuss several examples of the impact of digitalization on responsibility practices, based on which I distinguish four different relations between digitalization and responsibility practices. This then leads to a research agenda with relevant questions surrounding this topic. By engaging in this type of research and understanding the actual responsibility practices, the practical feasibility of responsible digitalization would be increased."
The Role of Ethics in Requirements Engineering for Developing Information Systems,"Kern, CJ; HÃ¼bner, K; Poss, L; SchÃ¶nig, S; Kroenung, J",10.1007/978-3-031-92474-3_14,2025,"This systematic review examines the integration of ethical considerations in Requirements Engineering (RE), analyzing methodological approaches and implementation challenges across software development practices. Through comprehensive literature analysis, we identify five primary frameworks: data governance, expert systems, stakeholder engagement, ethical oversight, and participatory methods. Our findings reveal critical success factors, including stakeholder communication protocols, transparent data handling practices, and human-centered governance structures. While established frameworks like ETHICS and IEEE 7000-2021 standard offer promising directions, their practical implementation remains challenging. Key obstacles include responsibility diffusion among stakeholders, ethical-functional requirement conflicts, and resource-intensive processes. The study exposes a significant research gap in practitioner guidance for ethical value trade-offs. We propose recommendations encompassing structured ethical frameworks, enhanced stakeholder participation mechanisms, and formal oversight protocols."
Key Factors Affecting the Cost of Medical Software Development: Cost and Modeling,"Li, LY; Bin Mansor, Z; Zhao, XY; Li, MM; Wang, XP",10.1109/ICCCS65393.2025.11069650,2025,"In the current context of the booming development of medical informatization, the number of medical software projects is increasing day by day, and the estimation of their development costs has become a core challenge in project management. The cost of medical software development is affected by the interaction of numerous complex factors such as personnel, equipment, time, quality, and regulatory compliance. This paper comprehensively reviews these key factors, delves into existing cost estimation models, compares the advantages and disadvantages of different methods, and highlights recent advancements in improving estimation accuracy using technologies such as machine learning and optimization algorithms. This research results indicate that hybrid approaches, which integrate multiple advanced technologies, have strong potential to significantly enhance the accuracy and efficiency of medical software cost estimation."
Advanced code slicing with pre-trained model fine-tuned for open-source component malware detection,"Wang, YS; Pang, SY; Fan, ZJ; Shang, S; Yao, YP; Jiang, ZW; Liu, BX",10.1093/comjnl/bxaf029,2025,"Open Source Software (OSS) is an essential part of modern software development, with platforms such as PyPI for Python, NPM for JavaScript, and RubyGems for Ruby facilitating code sharing and reuse. However, these repositories also pose significant security risks due to potential software supply chain attacks, where payloads are injected into components, propagating threats to downstream users and critical infrastructure. Existing automatic malicious component detection tools, particularly for PyPI, struggle to distinguish between subtle differences in malicious and benign behaviors, leading to high false positive rates. To address these issues, we systematically compare and explore these subtle differences, offering a more refined and accurate detection method, Open-Source Component Code Slices BERT (OCS-BERT). OCS-BERT leverages taint-based program slicing to isolate sensitive behavior segments and fine-tunes pre-trained model to capture subtle semantic differences across programming languages. This system excels in detecting malicious Python components and exhibits encouraging cross-language transferability to JavaScript's NPM and Ruby's RubyGems. Additionally, OCS-BERT successfully detected 107 malicious components from a total of 25,759 newly-uploaded PyPI components, taking two weeks to complete the process. This achievement demonstrates the effectiveness of our method, which serves as a potent enhancement to the current repertoire of software supply chain detection methodologies."
Geographic Prior Guided Subpixel Mapping for Fine-Grained Urban Tree Cover Reconstruction,"Xue, JQ; Zhang, ZH; Zhou, Y; Yuan, LN; He, D; Liu, XP",10.1109/LGRS.2025.3557845,2025,"Benefiting from long-term time series and large spatial coverage, Sentinel-2 has been widely used in urban tree cover retrieval. However, the mixed pixel effects in Sentinel-2 imagery make it challenging to accurately identify urban tree covers. To address this problem, subpixel mapping (SPM) is developed to reconstruct a high-resolution urban tree cover from medium-resolution imagery. While deep-learning-based SPM seeks fine-grained patterns solely within medium-resolution feature spaces and spatiotemporal fusion-based SPM leverages additional high-resolution imagery from different times at the same location, both face limitations: the former lacks detailed spatial constraints, and the latter struggles with acquiring geographically aligned imagery. To address these challenges, this study proposes a geographic prior guided SPM (GPSPM) approach for urban tree cover reconstruction. The geographic prior is grounded in the scaling law of geography, a fundamental principle of spatial heterogeneity stating that high-resolution imagery contains far more detailed features (e.g., small tree parcels) than lower resolution imagery. These fine-grained features enhance SPM by providing robust cross-scale spatial prior based on a teacher-student domain adaptation training framework. Besides, considering the geometric feature discrepancy and long-tail distribution exists across different geographic scales, cross-scale image mosaicking and resampling strategy are further developed. Experiments on public urban tree cover dataset demonstrate that the proposed method improves the intersection over union (IoU) of urban tree cover by approximately 5% compared to traditional unsupervised SPM and shows significant improvements in spatial detail quality."
Leveraging RAG and LLMs for Access Control Policy Extraction From User Stories in Agile Software Development,"Aboukadri, S; Ouaddah, A; Mezrioui, A; El Asri, I",10.1109/ACCESS.2025.3586203,2025,"Agile development has become increasingly popular among software development teams due to its capacity to deliver and update software rapidly while accommodating evolving requirements. Within this dynamic context, access control policies are critical for ensuring the security of systems by defining who can access specific resources under given conditions. However, identifying and documenting these policies often rely on manual, time-intensive processes prone to errors and oversight. This paper proposes an innovative framework leveraging Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) to automate the extraction and organization of access control policies from user stories and software documentation. The framework focuses on the early stages of the development lifecycle, capturing access control requirements as expressed in natural language artifacts. It comprises two core components: 1) a pipeline for extracting and categorizing access control policies, enabling precise mappings between roles, actions, and resources, and 2) an interactive chatbot designed to support Security Operations Center (SOC) analysts in evaluating suspicious access requests by providing contextualized insights into access policies. By integrating advanced natural language processing techniques with retrieval-based augmentation, the framework aims to reinforce access control mechanisms by improving visibility, and providing contextualized insights for security analysts."
Copilot's Island of Joy Balancing Individual Satisfaction with Team Interaction in Agile Development,"Wivestad, VT; Barbala, A; Stray, V",10.1007/978-3-031-72781-8_13,2025,"This study assesses the integration of GitHub Copilot into agile software development practices in one of Norway's largest public sector organizations. Through a quasi-experimental survey of 115 participants, we differentiate the attitudes of users and non-users of GitHub Copilot regarding their development routines. Findings reveal that Copilot users experience significantly greater focus on engaging tasks and less dependence on colleagues compared to non-users, while non-users maintain a more cautious stance on AI use in the public sector. Further, while users generally showed more positive attitudes and fewer frustrations, these differences were not statistically significant. The study advocates for a mindful adoption of AI tools in agile settings, balancing individual benefits with interdependence and team unity."
AI-driven green testing : Optimizing efficiency and sustainability in software testing,"Singhal, MK; Gunawat, C",10.47974/JIOS-2001,2025,"Software has become integral to daily life, continually expanding with increasingly complex features to meet growing expectations. However, managing these complexities- from understanding application dependencies to ensuring reliability through rigorous For organizations, delivering robust and dependable software is crucial for maintaining business success and reputation. Therefore, a significant portion of the software development life cycle is dedicated to testing. Engineers often write thousands of test cases to validate new features and prevent regressions. Despite these efforts, current regression testing methods are inefficient, as they often require developers to run all test cases irrespective of whether the code paths have been altered. This paper proposes an AI-driven approach to optimize regression testing. By analyzing specific trends, the AI identifies and prioritizes the most relevant test cases, thereby reducing execution time and resource consumption. This approach promises to mitigate inefficiencies associated with traditional testing methods, offering a more cost-effective and timely solution for software development cycles."
Stage-Gate Is Not Waterfall ... Find Out Why and How,"Cooper, RG",10.1109/EMR.2024.3426228,2025,"As the creator of Stage-Gate, I often hear it incorrectly referred to as a linear process like Waterfall, commonly used in software development. This is a misconception. This article outlines the current versions of Stage-Gate, Waterfall, and a similar process, Phase-Review, along with data on the popularity of each. Agile for physical products and lean development are also discussed. To ensure objectivity, artificial intelligence was used to compare Waterfall to Stage-Gate."
Responsible AI in the Software Industry: A Practitioner-Centered Perspective,"LeÃ§a, MD; Bento, M; Santos, RD",10.1109/RAIE66699.2025.00011,2025,"Responsible AI principles provide ethical guidelines for developing AI systems, yet their practical implementation in software engineering lacks thorough investigation. Therefore, this study explores the practices and challenges faced by software practitioners in aligning with these principles. Through semi-structured interviews with 21 practitioners, we investigated their methods, concerns, and strategies for addressing Responsible AI in software development. Our findings reveal that while practitioners frequently address fairness, inclusiveness, and reliability, principles such as transparency and accountability receive comparatively less attention in their practices. This scenario highlights gaps in current strategies and the need for more comprehensive frameworks to fully operationalize Responsible AI principles in software engineering."
Fostering agility through leadership in technology firms,"Bhatnagr, P",10.1108/LODJ-07-2024-0432,2025,"PurposeThis study aims to examine the relationships between transformational, transactional, and adaptive leadership styles and organisational agility in the India technology sector, with a focus on the mediating roles of organisational culture and dynamic capabilities in fostering agility within the STARA (Smart Technologies, Artificial Intelligence, Robotics, and Algorithms) environment.Design/methodology/approachQuantitative research was conducted using a cross-sectional survey design, and participants were drawn from different levels of technology firms in India. Stratified random sampling was used to obtain more generalised results, and 400 respondents completed the survey. The data were analysed through partial least squares structural equation modelling (PLS-SEM) using SmartPLS software.FindingsTransactional leadership had the most positive relationship with dynamic capabilities, transactional leadership had the most positive relationship with organisational culture, and dynamic capabilities had the most positive relationship with organisational agility.Practical implicationsThis study highlights that technology firms should foster a culture of innovation, collaboration, and adaptability by aligning leadership practices with organisational goals. Practical actions include implementing leadership development programs focused on creativity, establishing cross-functional collaboration platforms, setting up innovation laboratories, incentivising idea generation, and adopting agile project management methodologies. When supported by transformational and adaptive leadership, these initiatives can stimulate innovation and enhance agility in the STARA environment.Originality/valueThis study contributes to the literature by applying dynamic capabilities and competing values frameworks to analyse the relationships among leadership styles, organisational culture, dynamic capabilities, and organisational agility in STARA. This study is beneficial for scholars and managers aiming to improve agility in technology-intensive contexts."
Knowledge Management in Financial Software Support Team: A Prototype Based on Ontology,"Lazaretti, MGC; TenÃ³rio, N; Teixeira, TM; Silva, VACE; Becker, S",10.1007/978-3-031-87569-4_16,2025,"In the software development companies, where knowledge is complex and shapes the development and maintenance of products, Knowledge Management (KM) efficiently presents a huge challenge. Tools designed for maintaining software products, such as bug tracking systems, primarily automate tasks without fully addressing the organization of knowledge. Incorporating an ontology to map out knowledge for easy retrieval, emerges as a promising strategy to enhance the effectiveness of these tools, mainly in times of artificial intelligence. This study introduces a prototype based on ontology addressed to improve KM within a software development company. Throughout a field research approach in a company customer support department, this paper utilized data analysis and interactions with a key company representative to evaluate the prototype's ability to tackle identified challenges. Our findings suggest that ontology is relevant as a viable tool for aiding to arrange the organizational knowledge, though further investigation is required to quantify the benefits of this solution in terms of investment return."
Interacting with vector databases by means of domain-specific language,"Akik, E; Vjestica, M; Dimitrieski, V; Celikovic, M; Kordic, S; Ristic, S",10.1515/comp-2025-0036,2025,"Vector database management systems have been recognized as a crucial innovation in the era dominated by artificial intelligence, where vast and high-dimensional datasets are generated at unprecedented scales. These systems are designed to efficiently handle, store, retrieve, and analyze high-dimensional vector data, while uncovering patterns within unstructured and heterogeneous datasets. The ability of vector database management systems to perform fast and accurate similarity searches allows contextual data retrieval. Access to vector databases is often facilitated through application code tailored to proprietary application programming interfaces and query languages, varying in syntax and terminology used among vector database management systems of different vendors. A state of tight coupling, interoperability challenges, and difficulties during transitions between vector database management systems that ultimately affect usability is thereby produced. To address these issues, we propose a model-driven software development solution that incorporates vecDSL - a domain-specific language serving as its central component - to provide a uniform approach to accessing vector databases. The goal of the proposed solution is to have vector database management simplified and its interactions streamlined, thereby ensuring that end-user efficiency is enhanced through the utilization of vecDSL. Concepts uniformly used in vecDSL are expected to ease learning and eliminate database-specific adjustments, while abstraction provided by the language aims to simplify testing and enable efficient performance assessments across different vector database management systems. In this article, we describe the syntax and usage of vecDSL, as well as the application of the MDSD-based solution in supporting the interaction with diverse vector databases. We also include the evaluation of the proposed vecDSL syntax, to examine its ability in addressing current issues and explore its potential for further development."
Trustworthiness of Large Language Models for Code,"Khati, D",10.1109/ICSE-Companion66252.2025.00063,2025,"In recent years, Large Language Models for code (LLMc) have transformed the landscape of software engineering (SE), demonstrating significant efficacy in tasks such as code completion, summarization, review, tracing, translation, test case generation, clone detection, and bug fixing. Notably, GitHub Copilot and Google's CodeBot exemplify how LLMc contributes to substantial time and effort savings in software development. However, the widespread application of these models has raised critical concerns regarding their trustworthiness. The lack of well-defined trust metrics beyond mere accuracy poses significant risks, including potential security vulnerabilities and compromised data integrity. This dissertation proposes solving this pressing need by developing a comprehensive framework to evaluate LLMc's trustworthiness. We aim to establish contextualized definitions of trust, distrust, and trustworthiness specific to LLMc, identify key influencing factors, and create a standardized evaluation framework encompassing both model-based attributes and human-centric considerations. We will validate the framework's effectiveness through rigorous empirical studies and user evaluations and provide insights for targeted improvements in LLMc development. This dissertation seeks to enhance the reliability and transparency of LLMc, fostering their responsible integration into software engineering practices and paving the way for more trustworthy AI-assisted code generation."
FocalTransNet: A Hybrid Focal-Enhanced Transformer Network for Medical Image Segmentation,"Liao, M; Yang, RX; Zhao, YQ; Liang, W; Yuan, JS",10.1109/TIP.2025.3602739,2025,"CNNs have demonstrated superior performance in medical image segmentation. To overcome the limitation of only using local receptive field, previous work has attempted to integrate Transformers into convolutional network components such as encoders, decoders, or skip connections. However, these methods can only establish long-distance dependencies for some specific patterns and usually neglect the loss of fine-grained details during downsampling in multi-scale feature extraction. To address the issues, we present a novel hybrid Transformer network called FocalTransNet. Specifically, we construct a focal-enhanced (FE) Transformer module by introducing dense cross-connections into a CNN-Transformer dual-path structure and deploy the FE Transformer throughout the entire encoder. Different from existing hybrid networks that employ embedding or stacking strategies, the proposed model allows for a comprehensive extraction and deep fusion of both local and global features at different scales. Besides, we propose a symmetric patch merging (SPM) module for downsampling, which can retain the fine-grained details by establishing a specific information compensation mechanism. We evaluated the proposed method on four different medical image segmentation benchmarks. The proposed method outperforms previous state-of-the-art convolutional networks, Transformers, and hybrid networks. The code for FocalTransNet is publicly available at https://github.com/nemanjajoe/FocalTransNet"
Intelligent cache prefetchers in HPC architecture,"Fargo, F; Diamond, M; Franza, O; Foose, P; Adiletta, J; Adiletta, M; Steely, SC Jr",10.1007/s10586-024-04854-0,2025,"Reinforcement learning (RL) is revolutionizing the field of Artificial Intelligence (AI) and represents a step ahead towards building an optimal and autonomous system with a higher level of understanding (Arulkumaran et al. in IEEE Signal Processing Mag 34(6):26-38, 2017). One of the main goals for AI is to produce fully autonomous agents to interact with several features and learn the optimal behavior to optimize. Applications vary in data access patterns and a static hardware configuration is not idea for all phases of a workload. Today Xeon cores have multiple data prefetchers which fetch the next sets of data to be used, however, there are problems with these prefetchers as they may interact in destructive ways. This destructive behavior can cause several problems such as an increase in cache pollution, bottlenecks in the memory bandwidth, and additional occupancy to critical path demand queues. Managing the aggressiveness of the prefetchers are necessary to mitigate these problems. Current hardware prefetchers manage the aggressiveness of prefetchers by monitoring telemetry such as memory bandwidth and accuracy. However, there are problems with this approach as the telemetry data does not necessarily correlate with the overall system performance. In addition, other solutions show optimizing prefetchers individually to manage the system performance rather than allowing multiple features to work together. This research introduces hierarchical smart agents using reinforcement learning to find the optimal aggressiveness for the MLC prefetchers on runtime managed by the Smart Prefetchers Manager (SPM). We have expanded our previous work and evaluated more workloads on a hierarchical model and applied reinforcement learning in addition to offline training approach. This approach is implemented and evaluated on single core, single process environment to optimize the three Mid-level Cache (MLC) prefetchers on run time. Results demonstrated that using the reinforcement learning can optimize up to 7.18% improvement in instructions per cycle (IPC) over the state-of-the-art hardware solution."
Can Artificial Intelligence Identify the Ideal Ablation Area for the Patients With Atypical Atrial Flutter? A Proof of Concept Study Using Developed Software,"Amagasaki, M; Hoshiyama, T; Meitoma, T; Kiyama, M; Morihisa, K; Ogura, Y; Tsujita, K",10.1111/jce.70058,2025,"BackgroundCatheter ablation for atypical atrial flutter (AFL) is challenging owing to its complex circuit. Consequently, the success rate varies depending on the circuit complexity and the operator's level of experience.ObjectiveThis study aimed to create software using artificial intelligence to identify the critical isthmus of atypical AFL on 3D mapping.MethodsRaw data from 19 episodes of atypical AFL were extracted from the 3D mapping system. Local data, including location, activation timing, and voltage amplitude, were used to design software to identify areas where wavefronts converge and conduction slowing (which are characteristics of the optimal treatment area), that is, the critical isthmus, in patients with atypical AFL. Subsequently, the newly developed software was validated by evaluating the concordance between the software-estimated critical isthmus location and the actual AFL termination area.ResultsThis newly developed software identified the estimated critical isthmus in all cases in 24.9 (6.7-46.2) seconds. The concordance rate between Top 1 prediction area and actual AFL termination area was high of 79% (95% confidence interval: 56.7%-91.5%). Although 4 of the 19 episodes did not show concordance, it is assumed that ablation at the estimated critical isthmus may have resulted in arrhythmia termination in two cases.ConclusionThis newly developed software identified the optimal treatment area for the atypical AFL with high accuracy. Use of this software may enable faster and more precise treatments. Furthermore, it may allow less experienced operators to achieve results that are comparable to those of experienced operators."
Towards Universal Modeling Language for Neural Networks,"Barzdins, J; Kalnins, A; Barzdins, P",10.22364/bjmc.2025.13.1.03,2025,"Effective modeling is essential in both system and software development, serving as a key method for facilitating understanding, guiding design, and enabling communication among stakeholders. However, traditional universal system modeling languages like UML and SysML fall short when it comes to neural network modeling, where the structure, training, and deployment processes demand more detailed and specialized representations. Conversely, domain-specific languages like Keras, TensorFlow, PyTorch, and tools like Netron and Deep Learning Studio are too closely tied to specific implementation environments. This creates a significant challenge: the need to develop a universal modeling language specifically for neural networks that is both sufficiently simple (requiring a description of around ten pages) and capable of providing a detailed description of neural networks and their management. The main contribution of this paper is the introduction of such a language, called UM1NN, along with a detailed description and its application demonstrated through two important use cases: describing GPT-2 and defining the fine-tuning of GPT-2 for Question-Answering."
Cloud software code generation via knowledge graphs and multi-modal learning,"Zhang, FL; Chen, HM; Chen, Q; Liu, JQ",10.1186/s13677-025-00758-5,2025,"As cloud computing continues to experience rapid growth, the demand for cloud-native applications is escalating, leading to more complex and diverse development requirements. In this context, automated code generation plays a pivotal role in accelerating cloud-native application development. However, existing studies often overlook the full potential of leveraging multiple program representations, typically focusing on a single form derived from well-structured programs. In this paper, we demonstrate that integrating multiple program representations, specifically, code snippets and their associated ASTs can significantly enhance code generation for cloud-native applications. We construct a code knowledge graph (CodeKG) to retrieve related programs based on textual and structural similarities, enabling richer contextual information for generation. To realize this integration, we design a framework that employs shared-retentive networks (shared-RetNet) and an AST-based Transformer to extract and align features from natural language, code tokens, and ASTs. By applying contrastive loss and cross-attention mechanisms, our method effectively fuses diverse modalities to strengthen code generation performance. Experiments on two processed open-source datasets demonstrate that our method, without relying on extensive pre-training, achieves superior results compared to existing pretrained models across BLEU-4, CodeBLEU, and ROUGE-L metrics, paving the way for more efficient and intelligent cloud-native software development."
Participatory AI: A Method for Integrating Inclusive and Ethical Design Considerations into Autonomous System Development,"Stimson, CE; Raper, R",10.1007/978-3-031-72059-8_13,2025,"There has been significant work in the field of AI Ethics pertaining to how it might offer guidelines for developers to design, develop and deploy AI in an ethical way. Recently, the European Union's AI Act has introduced a risk-based regulation approach for AI system development. However, despite the additional requirements the AI Act places on developers to ensure that their systems are created with transparency, fairness, and accountability etc., there is no formalised methodology for how this might be achieved. Drawing on the history of collaborative and emancipatory technology design in Scandinavia, this paper proposes a software development methodology founded on the ethics and praxis-based principles of Participatory Design. Integrating this approach into the established 'Waterfall Method', it offers developers a practical way of embedding ethics in AI development, and to thereby satisfy the requirements imposed by the new regulations."
Gen AI in Computing Education: A Gap Analysis of Pedagogical Practices and Industry Expectations,"Ocay, AB; Rodrigo, MMT",10.1145/3724389.3730793,2025,"Traditional software development relied heavily on manual coding handled solely by human programmers, assisted by smart editors and IDEs. With the rise of Gen AI, much of the coding and debugging process has been automated, significantly improving productivity. For instance, Amazon reported saving 4,500 manyears and $260 million using its AI tool in 2024. This shift raises questions for computing education: What skills should students learn if coding can be automated? The study explores the gaps between current teaching practices and industry expectations, involving (n=26) teachers and (n=19) industry practitioners in the Philippines through a mixed-methods survey. Findings highlight the need for students to develop both foundational computing skills and AI-related competencies, such as prompt engineering and integrating AI tools into development workflows."
Enhancing Software Sustainability: Leveraging Large Language Models to Evaluate Security Requirements Fulfillment in Requirements Engineering,"Subahi, AF",10.3390/systems13020114,2025,"In the digital era, cybersecurity is integral for preserving national security, digital privacy, and social sustainability. This research emphasizes the role of non-functional equirements (NFRs) in developing secure software systems that enhance societal wellbeing by ensuring data protection, user privacy, and system robustness. Specifically, this study introduces a proof-of-concept approach by leveraging machine learning (ML) models to classify NFRs and identify security-related issues early in the software development lifecycle. Two experiments were conducted to assess the effectiveness of different models for binary and multi-class classification tasks. In Experiment 1, BERT-based models and artificial neural networks (ANNs) were fine-tuned to classify NFRs into security and non-security categories using a dataset of 803 statements. BERT-based models outperformed ANNs, achieving higher accuracy, precision, recall, and ROC-AUC scores, with hyperparameter tuning further enhancing the results. Experiment 2 assessed logistic regression (LR), a support vector machine (SVM), and XGBoost for the multi-class classification of security-related NFRs into seven categories. The SVM and XGBoost showed strong performance, achieving high precision and recall in specific categories. The findings demonstrate the effectiveness of advanced ML models in automating NFR classification, improving software security, and supporting social sustainability. Future work will explore hybrid approaches to enhance scalability and accuracy."
Empowering Agile-Based Generative Software Development through Human-AI Teamwork,"Zhang, S; Xing, ZC; Guo, RH; Xu, FZ; Chen, L; Zhang, ZY; Zhang, XW; Feng, ZY; Zhuang, ZQ",10.1145/3702987,2025,"In software development, the raw requirements proposed by users are frequently incomplete, which impedes the complete implementation of software functionalities. With the emergence of large language models, the exploration of generating software through user requirements has attracted attention. Recent methods with the top-down waterfall model employ a questioning approach for requirement completion, attempting to explore further user requirements. However, users, constrained by their domain knowledge, result in a lack of effective acceptance criteria during the requirement completion, failing to fully capture the implicit needs of the user. Moreover, the cumulative errors of the waterfall model can lead to discrepancies between the generated code and user requirements. The Agile methodologies reduce cumulative errors of the waterfall model through lightweight iteration and collaboration with users, but the challenge lies in ensuring semantic consistency between user requirements and the code generated by the agent. To address these challenges, we propose AgileGen, an agile-based generative software development through human-AI teamwork. Unlike existing questioning agents, AgileGen adopts a novel collaborative approach that breaks free from the constraints of domain knowledge by initiating the end-user perspective to complete the acceptance criteria. By introducing the Gherkin language, AgileGen attempts for the first time to use testable requirement descriptions as a bridge for semantic consistency between requirements and code, aiming to ensure that software products meet actual user requirements by defining user scenarios that include acceptance criteria. Additionally, we innovate in the human-AI teamwork model, allowing users to participate in decision-making processes they do well and significantly enhancing the completeness of software functionality. To ensure semantic consistency between requirements and generated code, we derive consistency factors from Gherkin to drive the subsequent software code generation. Finally, to improve the reliability of user scenarios, we also introduce a memory pool mechanism, collecting user decision-making scenarios and recommending them to new users with similar requirements. AgileGen, as a user-friendly interactive system, significantly outperformed existing best methods by 16.4% and garnered higher user satisfaction."
LOMOS: An AI-Based Runtime Security Monitoring System Fit for the Cloud Continuum,"Costa, JP; Ratkajec, H; Vladusic, D; Martincic, T; Cernivec, A; Cinkelj, J; Davi, R; Favrin, S; Gorza, L; di Marco, G",10.1007/978-3-031-90203-1_1,2025,"Given the challenges faced by various industries in the global digital transformation process, it is essential to perform detection of anomalies, consuming system logs collected and returning anomaly score, which should significantly enhance the visualization of vulnerabilities and improve the overall security posture of systems. This paper presents LOg MOnitoring System (LOMOS), a robust AI technology and methodology for anomaly detection on logs, tailored to adapt to new data sensitivity concerns. LOMOS facilitates the creation of informative metrics/variables with significant screening capabilities, addressing the critical need for real-time monitoring of stack conditions to fuel its self-healing mechanisms. The proposed system is designed to detect security related events and incidents within the deployed application environment and is deployable automatically, providing users with timely notifications about security episodes. In this paper, we demonstrate the advantages of this approach in the continuous detection of vulnerabilities, threats and malware in production infrastructures and during software development phases, appearing in the infrastructure when new services or features are added, or simply when new vulnerabilities are discovered in existing (outdated) services. By seamlessly integrating this novel transformer-based anomaly detection methodology with the cloud continuum, it facilitates a smooth and secure digital transformation process, ensuring a comprehensive adherence to evolving security requirements while supporting the dynamic nature of modern infrastructures."
Cross-Language Code Smell Detection via Transfer Learning,"Sandouka, R; Aljamaan, H",10.3390/app15179293,2025,"Code smells are code structures that indicate a potential issue in code design or implementation. These issues could affect the processes of code testing and maintenance, and overall software quality. Therefore, it is important to detect code smells in the early stages of software development to enhance system quality. Most studies have focused on detecting code smells of a single programming language. This article explores TL for cross-language code smell detection, where Java is the source, and both C# and Python are the target datasets, focusing on Large Class, Long Method, and Long Parameter List code smells. We conducted a comparison study across two transfer learning approaches-instance-based (Importance Weighting Classifier, Nearest Neighbors Weighting, and Transfer AdaBoost) and parameter-based (Transfer Tree, Transfer Forest)-with various base models. The results showed that the instance-based approach outperformed the parameter-based approach, particularly with Transfer AdaBoost using ensemble learning base models. The Transfer AdaBoost approach with Gradient Boosting and Extra Trees achieved consistent and robust results across both C# and Python, with an 83% winning rate, as indicated by the Wilcoxon signed-rank test. These findings underscore the effectiveness of transfer learning for cross-language code smell detection, supporting its generalizability across different programming languages."
"Evaluating Software Development Agents: Patch Patterns, Code Quality, and Issue Complexity in Real-World GitHub Scenarios","Chen, Z; Jiang, LX",10.1109/SANER64311.2025.00068,2025,"In recent years, AI-based software engineering has progressed from pre-trained models to advanced agentic workflows, with Software Development Agents representing the next major leap. These agents, capable of reasoning, planning, and interacting with external environments, offer promising solutions to complex software engineering tasks. However, while much research has evaluated code generated by large language models (LLMs), comprehensive studies on agent-generated patches, particularly in real-world settings, are lacking. This study addresses that gap by evaluating 4,892 patches from 10 top-ranked agents on 500 real-world GitHub issues from SWE-Bench Verified, focusing on their impact on code quality. Our analysis shows no single agent dominated, with 170 issues unresolved, indicating room for improvement. Even for patches that passed unit tests and resolved issues, agents made different file and function modifications compared to the gold patches from repository developers, revealing limitations in the benchmark's test case coverage. Most agents maintained code reliability and security, avoiding new bugs or vulnerabilities; while some agents increased code complexity, many reduced code duplication and minimized code smells. Finally, agents performed better on simpler codebases, suggesting that breaking complex tasks into smaller sub-tasks could improve effectiveness. This study provides the first comprehensive evaluation of agent-generated patches on real-world GitHub issues, offering insights to advance AI-driven software development."
"Agile and Lean: Organization, Products and Development","Saltz, J; Anderson, E; Sutherland, A; Stray, V",,2025,"Agile and Lean software development methods have improved the success and speed of software development projects over more than 20 years. They continue to evolve with newer methods such as DevOps and BizDevOps becoming prevalent, often being used in conjunction with traditional Agile and Lean techniques. At the same time, business and technological environments are rapidly evolving as well. Workforces are much more geographically dispersed and asynchronous than envisioned 20 years ago. Additionally, the use of AI in development projects is rapidly increasing. More research in these new contexts is needed to keep Agile and Lean relevant over the long term."
Tracking menopause: An SDK Data Audit for intimate infrastructures of datafication with ChatGPT4o,"Pybus, J; Mir, M",10.1177/14614448251314401,2025,"This article presents a novel methodology to examine the tracking infrastructures that extend datafication across a sample of 14 menopause-related applications. The Software Development Kit (SDK) Data Audit is a mixed methodology that explores how personal data are accessed in apps using ChatGPT4o to account for how digital surveillance transpires via SDKs. Our research highlights that not all apps are equal amid ubiquitous datafication, with a disproportionate number of SDK services provided by Google, Meta, and Amazon. Our three key findings include: (1) an empirical approach for auditing SDKs; (2) a means to account for modular SDK infrastructure; and (3) the central role that App Events-micro-data points that map every action we make inside of apps-play in the data-for-service economy that SDKs enable. This work is intended to open up space for more critical research on the tracking infrastructures of datafication within our apps in any domain."
An Effective And Efficient Renewable Energy Generation Forecasting Via Meteorological Assistance,"Tian, ZY; Lv, L; Deng, WC; Chen, ZK",10.6180/jase.202507_28(7).0020,2025,"Accurate signal pattern mining of renewable energy generation forecasting (REGF) is important to the days- ahead power scheduling of renewable energy power systems. Despite achieving excellent performance with current methods, two issues still persist. (1) They solely utilize historical meteorological signal data to assist in power signal forecasting and neglect valuable information in future information of meteorological signals, consequently limiting their performance. (2) They pursue predictive performance by designing complex architectures and mechanisms, which may lead to insufficient model generalization. To this end, an effective and efficient MLP architecture is proposed to mine REGF signal patterns in renewable energy power systems (SPM-REPS), which contains power signal forecast architecture and meteorological signal forecast architecture. Two architectures seamlessly collaborate in forecasting power generation patterns, which achieves better performance. Meanwhile, time-correlation and feature-correlation strategies are devised within MLP networks to capture both intra-sequence and inter-sequence correlations of signal variables like transformer- and RNNbased methods. Furthermore, a theoretical analysis of linear architecture is given to prove the progressiveness of SPM-REPS. Finally, numerous experiments, conducted on common datasets (CSG-PV and CSG-wind) from Chinese State Grid, demonstrate SPM-REPS sets a new benchmark in mining REGF signal patterns of REPS."
"Alleviation of vanadium toxicity through individual and combined supplementation of spermine and spermidine in soybean: Insights into oxidative stress management, detoxification mechanisms, and root exudates","Basit, F; Khalid, M; Bahadur, A; Bhat, JA; Ahmad, A; Ahmad, P",10.1016/j.sajb.2025.06.016,2025,"Vanadium (V) induced toxicity avowed the necessity of alleviating strategy to remediate the V-contaminated soil and limit its accumulation in the food chain. Polyamines (PAs) viz., spermine (Spd) and spermidine (Spm) are essential for promoting plant development and stress resilience. However, very little is known about the role of polyamines' in the mitigation of V-phytotoxicity. Here, we assessed the role of Spd and Spm to reduce the harmful effects of V-toxicity in the soybean (G. max L.) seedlings. Our results revealed that Spd (1.0 mM) and Spm (1.0 mM) significantly reduced the V (100 mM) induced phytotoxicity by reducing the accumulation of V in the roots and leaves via activating the V detoxification mechanism and root exudates, as well as amended the function of light-harvesting pigments (total chlorophylls), and gas exchange indices, which eventually enhanced plant's biomass and growth. Furthermore, PAs (Spd and/or Spm) stabilized the membrane integrity by positively regulating the antioxidative defense system including the ascorbate-glutathione (AsA-GSH) cycle and lowered the excessive production of reactive oxygen species (ROS) such as H2O2 and O2, and malondialdehyde (MDA) in V-stressed soybean seedlings. Inclusively, our results indicated that the supplementation of PAs (Spd and/or Spm) shows potential for improving the soybean tolerance against V stress while preserving vital physiological processes. (c) 2025 SAAB. Published by Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies."
EmotiCloud: Cloud System to Monitor Patients Using AI Facial Emotion Recognition,"LÃ³pez-Echeverry, AM; LÃ³pez-FlÃ³rez, S; Bedoya-Guapacha, J; De-La-Prieta, F",10.3390/systems13090750,2025,"Comprehensive healthcare seeks to uphold the right to health by providing patient-centred care in both personal and work environments. However, the unequal distribution of healthcare services significantly restricts access in remote or underserved areas-a challenge that is particularly critical in mental health care within low-income countries. On average, there is only one psychiatrist for every 200,000 people, which severely limits early diagnosis and continuous monitoring in patients' daily environments. In response to these challenges, this research explores the feasibility of implementing an information system that integrates cloud computing with an intelligent Facial Expression Recognition (FER) module to enable psychologists to remotely and periodically monitor patients' emotional states. This approach enhances comprehensive clinical assessments, supporting early detection, ongoing management, and personalised treatment in mental health care. This applied research follows a descriptive and developmental approach, aiming to design, implement, and evaluate an intelligent cloud-based solution that enables remote monitoring of patients' emotional states through Facial Expression Recognition (FER). The methodology integrates principles of user-centred design, software engineering best practices, and machine learning model development, ensuring a robust and scalable solution aligned with clinical and technological requirements. The development process followed the Software Development Life Cycle (SDLC) and included functional, performance, and integration testing. To assess overall system quality, we defined an evaluation framework based on ISO/IEC 25010 quality characteristics: functional suitability, performance efficiency, usability, and security. The intelligent FER model achieved strong validation results, with a loss of 0.1378 and an accuracy of 96%, as confirmed by the confusion matrix and associated performance metrics."
Automated requirements engineering framework for agile model-driven development,"Umar, MA; Lano, K; Abubakar, AK",10.3389/fcomp.2025.1537100,2025,"Introduction Advances in requirements engineering, driven by various paradigms and methodologies, have significantly influenced software development practices. The integration of agile methodologies and model-driven development (MDE) has become increasingly critical in modern software engineering. MDE emphasizes the use of models throughout the development process, necessitating structured approaches for handling requirements written in natural language.Methods This paper proposes an automated requirements engineering framework for agile model-driven development to enhance the formalization and analysis of textual requirements. The framework employs machine learning models to extract essential components from requirements specifications, focusing specifically on class diagrams. A comprehensive dataset of requirements specification problems was developed to train and validate the framework's effectiveness.Results The framework was evaluated using comparative evaluation and two real-world experimental studies in the medical and information systems domains. The results demonstrated its applicability in diverse and complex software development environments, highlighting its ability to enhance requirements formalization.Discussion The findings contribute to the advancement of automated requirements engineering and agile model-driven development, reinforcing the role of machine learning in improving software requirements analysis. The framework's success underscores its potential for widespread adoption in software development practices."
Optimising Construction Site Auditing: A Novel Methodology Integrating Ground Drones and Building Information Modelling (BIM) Analysis,"Guerrero-Sevilla, D; RodrÃ­guez-GÃ³mez, R; Morcillo-Sanz, A; Gonzalez-Aguilera, D",10.3390/drones9040277,2025,"Monitoring and management of construction sites are critical to ensuring project success, efficiency, and safety. Traditional methods often struggle to provide real-time, accurate, and comprehensive data, leading to delays, cost overruns, and errors. This paper presents a novel methodology utilising a ground drone for auditing construction sites to detect changes and deviations from planned Building Information Modelling (BIM). The methodology focuses on developing a novel tool that facilitates Scan-vs-BIM auditing through time. Experimental results are presented, demonstrating the effectiveness and accuracy of the proposed methodology for assessing structural discrepancies. This research contributes to advancing construction auditing practices by integrating state-of-the-art technologies and innovative techniques, ultimately enhancing project monitoring and management processes in the construction industry."
Learning Natural Consistency Representation for Face Forgery Video Detection,"Zhang, DC; Xiao, ZH; Li, SK; Lin, FZ; Li, JM; Ge, SM",10.1007/978-3-031-73010-8_24,2025,"Face Forgery videos have elicited critical social public concerns and various detectors have been proposed. However, fully-supervised detectors may lead to easily overfitting to specific forgery methods or videos, and existing self-supervised detectors are strict on auxiliary tasks, such as requiring audio or multi-modalities, leading to limited generalization and robustness. In this paper, we examine whether we can address this issue by leveraging visual-only real face videos. To this end, we propose to learn the Natural Consistency representation (NACO) of real face videos in a self-supervised manner, which is inspired by the observation that fake videos struggle to maintain the natural spatiotemporal consistency even under unknown forgery methods and different perturbations. Our NACO first extracts spatial features of each frame by CNNs then integrates them into Transformer to learn the long-range spatiotemporal representation, leveraging the advantages of CNNs and Transformer on local spatial receptive field and long-term memory respectively. Furthermore, a Spatial Predictive Module (SPM) and a Temporal Contrastive Module (TCM) are introduced to enhance the natural consistency representation learning. The SPM aims to predict random masked spatial features from spatiotemporal representation, and the TCM regularizes the latent distance of spatiotemporal representation by shuffling the natural order to disturb the consistency, which could both force our NACO more sensitive to the natural spatiotemporal consistency. After the representation learning stage, a MLP head is fine-tuned to perform the usual forgery video classification task. Extensive experiments show that our method outperforms other state-of-the-art competitors with impressive generalization and robustness."
Virtual reality force feedback spine surgery simulator training for pedicle screw placement: assessing the impact of seniority and spinal region,"Yuan, L; Zhang, ZQ; Zhou, FF; Xia, T; Xu, NF; Sun, ZR",10.1186/s12909-025-07695-6,2025,"Objective To evaluate the efficacy of Virtual Reality Force Feedback Spine Surgery Training Simulators (VRFF-SSTS) in enhancing the skills of spinal pedicle screw placement (PSP) among orthopedic fellows in continuing medical education, and whether influenced by seniority and spinal location. Methods A multi-centered, cross-sectional study enrolled fellows from three tertiary care referral hospitals. Participants were categorized into three groups (A, B, and C) based on their post-graduation year (PGY), reflecting their hierarchical order of spine-surgical training: A (PGY1-5), B (PGY6-10), and C (> 10). Participants performed SPSP in the spinal saw bone model using the free-hand technique first, followed by training on IVRSS. The outcome before and after training was assessed on placement accuracy and overall competency through an objective rating scale. Results Sixty-four participants were included, with PGY A, B, and C 18 (28.1%),30 (46.9%), and 16 (25.0%) participants, respectively. Pre-training data indicated that screw placement accuracy improved with increasing seniority. The accuracy of lumbar pedicle screw placement surpassed that of atlantoaxial, subaxial cervical, and thoracic vertebrae. Post-training, there was a significant enhancement in screw placement accuracy across all groups compared to pre-training (p < 0.001), with no significant differences observed between seniority levels and spinal regions. Overall competency significantly improved following training, with PGY B and C fellows, who had prior experience with pedicle screw insertion, demonstrating more satisfactory performance. Conclusion VRFF-SSTS proves to be an invaluable tool for training surgical skills, capable of enhancing the accuracy of pedicle screw placement for junior spine surgeons within a constrained training period. However, the overall competency in pedicle screw placement remains superior among experienced surgeons compared to their junior counterparts, underscoring the ongoing importance of clinical practice."
Development and Evaluation of Viajefy: A Tourism Information System using TF-IDF Algorithm,"Arruejo, AC; Arruejo, RC",,2025,"The study aimed to design, develop, and evaluate a tourism information system called Viajefy, incorporating the TF-IDF algorithm and assessing its model performance. It employed Feature-Driven Development as the software development method. It utilized the Cross-Industry Standardized Process for Data Mining for the data mining process of the TF-IDF algorithm, serving as its recommender agent feature. The confusion matrix evaluation tool was used to assess the algorithm's performance, yielding an accuracy of 97%, a precision of 93%, and a recall of 90%. Results showed that the recommender agent of the software application was proven reliable based on the algorithm's performance criteria in terms of accuracy, precision, and recall, and the system received a Very Highly Acceptable rating of 4.74. This software application is one of the first studies along tourism information systems for Ilocos Sur, Philippines, to integrate a recommender agent to help the Provincial Government of Ilocos Sur advertise attractions and establishments to be managed by the said government, where one of the Seven Wonders of the World, Vigan City, is situated."
Named entity recognition for function point descriptions in software cost estimation processes,"Zhao, BY; Zou, XF; Xin, SJ; Liu, D",10.1504/IJSNET.2025.148196,2025,"With the advancement of software technology, the industry's informatisation level has improved, but the growing size and complexity of software have raised costs. Consequently, assessing software project costs early is crucial. Function point analysis, the primary method for cost evaluation, quantifies functional elements like external data inputs and outputs to measure software size from the user's perspective. However, it heavily relies on manual effort, especially in extracting function point descriptions, leading to errors and inefficiency. This paper proposes an entity recognition model to address these challenges, integrating a BiLSTM-CRF framework with CNN layers and hierarchical learning. A domain-specific dictionary is developed to enhance the model's performance. Experimental results show that the proposed method outperforms BERT by improving accuracy by 0.42% and recall by 1.04%. The method achieves 95.37% accuracy in entity recognition for a sensor data system, demonstrating its effectiveness and reliability in software cost evaluation."
"Trust Dynamics in AI-Assisted Development: Definitions, Factors, and Implications","Sabouri, S; Eibl, P; Zhou, XY; Ziyadi, M; Medvidovic, N; Lindemann, L; Chattopadhyay, S",10.1109/ICSE55347.2025.00199,2025,"Software developers increasingly rely on AI code generation utilities. To ensure that good code is accepted into the code base and bad code is rejected, developers must know when to trust an AI suggestion. Understanding how developers build this intuition is crucial to enhancing developer-AI collaborative programming. In this paper, we seek to understand how developers (1) define and (2) evaluate the trustworthiness of a code suggestion and (3) how trust evolves when using AI code assistants. To answer these questions, we conducted a mixedmethod study consisting of an in-depth exploratory survey with (n=29) developers followed by an observation study (n=10). We found that comprehensibility and perceived correctness were the most frequently used factors to evaluate code suggestion trustworthiness. However, the gap in developers' definition and evaluation of trust points to a lack of support for evaluating trustworthy code in real-time. We also found that developers often alter their trust decisions, keeping only 52% of original suggestions. Based on these findings, we extracted four guidelines to enhance developer-AI interactions. We validated the guidelines through a survey with (n=7) domain experts and survey members (n=8). We discuss the validated guidelines, how to apply them, and tools to help adopt them."
Improving Co-Decoding Based Security Hardening of Code LLMs Leveraging Knowledge Distillation,"Li, D; Shu, SF; Yan, M; Liu, ZX; Liu, C; Zhang, XH; Lo, D",10.1109/TSE.2025.3591791,2025,"Large Language Models (LLMs) have been widely adopted by developers in software development. However, the massive pretraining code data is not rigorously filtered, allowing LLMs to learn unsafe coding patterns. Several prior studies have demonstrated that code LLMs tend to generate code with potential vulnerabilities. The widespread adoption of intelligent programming assistants poses a significant threat to the software development process. Existing approaches to mitigating this risk primarily involve constructing secure data that are free of vulnerabilities and then retraining or fine-tuning the models. However, such an effort is resource intensive and requires significant manual supervision. When the model parameters are too large (e.g., more than 1 billion) or multiple models with the same parameter scale have the same optimization needs (e.g., to avoid outputting vulnerable code), the above work will become unaffordable. To address this challenge, in previous work, we proposed CoSec, an approach to improve the security of code LLMs with different parameters by utilizing an independent and very small parametric security model as a decoding navigator. Despite CoSec's excellent performance, we found that there is still room for improving: 1) its ability to maintain the functional correctness of hardened targets, and 2) the security of the generated code. To address the above issues, we propose CoSec+, a hardening framework consisting of three phases: 1) Functional Correctness Alignment, which improves the functional correctness of the security base with knowledge disstillation; 2) Security Training, which yields an independent, but much smaller security model; and 3) Co-decoding, where the security model iteratively reasons about the next token along with the target model. Due to the higher confidence that a well-trained security model places in secure and correct tokens, it guides the target base model to generate more secure code, even as it improves the functional correctness of the target base model. We have conducted extensive experiments in several code LLMs (i.e., CodeGen, StarCoderBase, DeepSeekCoder and Qwen2.5-Coder), and the results show that our approach is effective in improving the functional correctness and security of the models. The evaluation results show that CoSec+ can deliver a 0.8% to 37.7% improvement in security across models of various parameter sizes and families; moreover, it preserves the functional correctness of the target base models-achieving functional-correctness gains of 0.7% to 51.1% for most of those models."
Artificial intelligence for context-aware visual change detection in software test automation,"Moradi, M; Yan, K; Colwell, D; Asgari, R",10.1007/s13748-025-00408-6,2025,"Automated software testing is integral to the software development process, streamlining workflows and ensuring product reliability. Visual testing, particularly for user interface (UI) and user experience (UX) validation, plays a vital role in maintaining software quality. However, conventional techniques such as pixel-wise comparison and region-based visual change detection often fail to capture contextual similarities, subtle variations, and spatial relationships between UI elements. In this paper, we propose a novel graph-based approach for context-aware visual change detection in software test automation. Our method leverages a machine learning model (YOLOv5) to detect UI controls from software screenshots and constructs a graph that models their contextual and spatial relationships. This graph structure is then used to identify correspondences between UI elements across software versions and to detect meaningful changes. The proposed method incorporates a recursive similarity computation that combines structural, visual, and textual cues, offering a robust and holistic model of UI changes. We evaluate our approach on a curated dataset of real-world software screenshots and demonstrate that it reliably detects both simple and complex UI changes. Our method significantly outperforms pixel-wise and region-based baselines, especially in scenarios requiring contextual understanding. We also discuss current limitations related to dataset diversity, baseline complexity, and model generalization, and outline planned future improvements. Overall, our work advances the state of the art in visual change detection and provides a practical solution for enhancing the reliability and maintainability of evolving software interfaces."
BR-Hunter: Detect Information Types of Bug Reports From Online Community Discussions,"Liu, HJ; Xiong, JY; Guo, SK; Li, H; Yang, ZG; Li, CC",10.1109/TR.2025.3615161,2025,"In community-based software development, live-chatting services are increasingly used to discuss bugs encountered during development. Many methods have emerged to identify bugs and produce bug reports, which further improve the efficiency of software development. However, previous methods still face challenges in understanding complex conversational structures and classifying sentences in bug reports, as entertaining or meaningless utterances often lower the quality of constructed bug reports. To address this issue, we propose a method named BR-Hunter, which comprises the following four components. Specifically, the data preprocessing component disentangles and denoises the live chats, while the utterance embedding component aims to extract the semantic features of each utterance in the conversations. The bug report identification component then models the conversation as a feature graph and uses Graph Neural Networks to identify conversations containing bug reports, thereby solving Challenge 1. Finally, the bug report synthesis (BRS) component tackles Challenge 2 by classifying and reassembling sentences from conversations containing bug reports, leveraging fine-tuned BERT and prompt learning techniques. Extensive experiments conducted on eight open source projects demonstrate that BR-Hunter achieves high accuracy in identifying bug reports. Compared to baseline methods, BR-Hunter improves the average F1 score by 36.41%, 24.80%, 68.92%, 46.77%, 52.84%, 25.80%, 25.25%, and 4.19%, respectively. And BR-Hunter also achieves an average improvement of 10.34% on the BRS task, compared with the state-of-the-art method."
Overcoming Multi-Legacy Application Challenges through Building Dynamic Capabilities for Low-Code Adoption,"Naqvi, SAA; Zimmer, MP; Syed, R; Drews, P; Basole, RC",,2025,"Incumbent organizations face challenges in managing multiple legacy applications. Legacy applications limit the integration of new applications and hinder the use of modern application development technologies such as cloud and AI. We present a case study of an incumbent organization that developed dynamic capabilities (DC) to leverage low-code for effectively tackling these challenges. By conducting 18 expert interviews, our work identifies key capabilities such as legacy application sensing, seizing low-code opportunities, and orchestrating transformative shifts in software development practices. Our study makes two key contributions. First, we demonstrate how organizations develop DC to use low-code as a new option to overcome challenges stemming from siloed legacy application. Second, we develop a process model that explains how organizations can develop DC to leverage low-code for achieving end-to-end process coverage, advance software development practices, and increase the flexibility of the IT infrastructure."
Structural Relation Multi-Class Token Transformer for Weakly Supervised Semantic Segmentation,"Peng, DJ; Kameyama, W",10.1587/transinf.2024EDP7125,2025,"Weakly Supervised Semantic Segmentation (WSSS) aims to train models to identify and delineate objects within an image using limited training data such as image-level labels. While recent works mainly focus on exploring class-specific knowledge to improve the quality of class activation maps, we contend that relying solely on this approach within a non-hierarchical architecture fails to adequately capture the structural relationships within images. Drawing inspiration from fully supervised semantic segmentation designs, which use hierarchical multi-scale feature maps for predicting the dense masks, we propose a novel architecture that integrates a Structural Relation Multi-class Token Transformer (SR-MCT) with WSSS. This model employs multi-scale structural tokens, generated by a Spatial Prior Module (SPM), which interact not only with patch tokens to encode structural relations, but also with multi-class tokens to integrate class-specific knowledge into complex structural embeddings. The proposed Structural Relation Multi-class Token Attention effectively builds long-range dependencies among structural tokens, patch tokens, and multi-class tokens simultaneously. Experimental results and ablation studies on PASCAL VOC 2012 and MS COCO 2014 demonstrate that our proposed SR-MCT can enhance baseline performance and outperform other state-ofthe-art methods."
Research on Hybrid Collaborative Development Model Based on Multi-Dimensional Behavioral Information,"Gao, SL; Liao, W; Shu, T; Zhao, ZN; Wang, YQ",10.3390/app15094907,2025,"This paper aims to propose a hybrid collaborative development model based on multi-dimensional behavioral information (HCDMB) to deal with systemic problems in modern software engineering, such as the low efficiency of cross-stage collaboration, the fragmentation of the intelligent tool chain, and the imperfect human-machine collaboration mechanism. This paper focuses on the stages of requirements analysis, software development, software testing and software operation and maintenance in the process of software development. By integrating the multi-dimensional characteristics of the development behavior track, collaboration interaction record and product application data in the process of project promotion, the mixture of experts (MoE) model is introduced to break through the rigid constraints of the traditional tool chain. Reinforcement learning combined with human feedback is used to optimize the MoE dynamic routing mechanism. At the same time, the few-shot context learning method is used to build different expert models, which further improve the reasoning efficiency and knowledge transfer ability of the system in different scenarios. The HCDMB model proposed in this paper can be viewed as an important breakthrough in the software engineering collaboration paradigm, so as to provide innovative solutions to the many problems faced by dynamic requirements and diverse scenarios based on artificial intelligence technology in the field of software engineering involving different project personnel."
Real-time optical imaging acquisition and processing in Python: a practical guide using CAS,"Hughes, MR",10.1364/AO.564458,2025,"Real-time data acquisition and processing is an important step in the development of new approaches to optical imaging in research laboratories. Python is increasingly used for scientific computing and allows for the straightforward application of artificial intelligence models using popular frameworks such as PyTorch. However, achieving high-speed image capture and processing in real time is challenging and requires extensive development work, a particular problem for academic labs where research teams may lack specialist expertise in software development. This note provides guidelines for achieving high performance in Python for optical imaging applications and introduces an open-source framework CAS for rapid prototyping of imaging system software. CAS includes a hardware abstraction layer for cameras, a ready-made GUI, which can easily be customized, as well as support for using multiple CPU cores for parallelism. By providing an open-source and flexible Python-based solution, CAS can support research teams to more quickly develop real-time imaging systems."
Software vulnerability detection under poisoning attacks using CNN-based image processing,"GonzÃ¡lez-Manzano, L; Garcia-Alfaro, J",10.1007/s10207-025-00989-2,2025,"Design flows, code errors, or inadequate countermeasures may occur in software development. Some of them lead to vulnerabilities in the code, opening the door to attacks. Assorted techniques are developed to detect vulnerable code samples, making artificial intelligence techniques, such as Machine Learning (ML), a common practice. Nonetheless, the security of ML is a major concern. This includes the the case of ML-based detection whose training process is affected by data poisoning. More generally, vulnerability detection can be evaded unless poisoning attacks are properly handled. This paper tackles this problem. A novel vulnerability detection system based on ML-based image processing, using Convolutional Neural Network (CNN), is proposed. The system, hereinafter called IVul, is evaluated under the presence of backdoor attacks, a precise type of poisoning in which a pattern is introduced in the training data to alter the expected behavior of the learned models. IVul is evaluated with more than three thousand code samples associated with two representative programming languages (C# and PHP). IVul outperforms other comparable state-of-the-art vulnerability detectors in the literature, reaching 82%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$82\%$$\end{document} to 99%\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$99\%$$\end{document} detection accuracy. Besides, results show that the type of attack may affect a particular language more than another, though, in general, PHP is more resilient to proposed attacks than C#."
"Proinflammatory Cytokines, Type I Interferons, and Specialized Proresolving Mediators Hallmark the Influence of Vaccination and Marketing on Backgrounded Beef Cattle","McAllister, HR; Capik, SF; Harvey, KM; Ramirez, BI; Valeris-Chacin, RJ; Woolums, AR; Karisch, BB; Morley, PS; Scott, MA",10.3390/vetsci12090834,2025,"Cattle marketed through auction market systems and/or that remain unvaccinated are considered higher risk for BRD, but impacts on host response remain unclear. We sought to identify specific genomic patterns of beef calves vaccinated against BRD viruses or not and commercially marketed or directly transported in a split-plot randomized controlled trial. Forty-one calves who remained clinically healthy from birth through backgrounding were selected (randomly stratified) from a larger cohort of cattle (n = 81). Treatment groups included VAX/DIRECT (n = 12), VAX/AUCTION (n = 11), NOVAX/DIRECT (n = 7), and NOVAX/AUCTION (n = 11). Blood RNA was acquired across five time points, sequenced, and bioinformatically processed via HISAT2 and StringTie2. Significant transcriptional changes (FDR < 0.05) were observed at backgrounding entry (T5) in NOVAX/AUCTION cattle exhibiting 2809 uniquely differentially expressed genes and relative activation of immune, inflammatory, and metabolic pathways with upregulation of interferon-stimulated genes (e.g., IFIT3, MX2, and TRIM25) and downregulation of specialized proresolving mediator (SPM) enzymes (ALOX5 and ALOX15). VAX/AUCTION cattle exhibited modulated immune activation and preserved expression of SPM-associated genes when compared to NOVAX/AUCTION cattle. Both marketing route and vaccination shape the molecular immune landscape during high-stress transitions, with preweaning vaccination potentially modulating this response. This study provides mechanistic insight into how management practices influence immunological resilience and highlights the value of integrating transcriptomics into BRD risk mitigation."
CFD-DEM-SPM modeling of permeability and pressure drop in cohesive zones of heterogeneous alternating layer beds for low-carbon blast furnace ironmaking,"Li, Q; Jiang, Y; Dan, JY; Ma, SW",10.1016/j.partic.2025.09.008,2025,"To reduce greenhouse gas emissions in ironmaking, the steel industry is advancing innovative low-carbon blast furnace (BF) technologies. A critical challenge for implementing such innovations lies in optimizing permeability within the BF's cohesive zone (CZ), which directly impacts operational stability and efficiency. This study employs a coupled computational fluid dynamics-discrete element method (CFD-DEM) to calibrate Young's modulus by respectively fitting the relationship between Young's modulus and temperature, as well as pressure drop, based on a reported lab-scale softening and smelting experimental data of ore-coke heterogeneous alternating layer packed beds resembling BFs, and develops a softening particle model (SPM). The SPM establishes a temperature-dependent relationship between mechanical properties of softened ore particles and CZ conditions in industrial-scale BFs. Simulations of particle shrinkage behavior and pressure drop trends using the CFD-DEM-SPM framework demonstrate strong correlation with experimental data, validating its accuracy for predictive analysis. Furthermore, this study investigates how layer arrangement configurations, size ratios between ore and coke particles, and coke blending proportions influence CZ characteristics. Key findings identify an optimal batch weight configuration to enhance permeability within the CZ while maintaining operational stability. Additionally, results indicate that increasing the relative particle size of ore compared to coke or enhancing the proportion of blended coke in burden mixes improves CZ permeability, offering actionable strategies for reducing carbon intensity in BF operations. These insights provide critical guidance toward developing low-carbon BF processes compatible with global climate targets. (c) 2025 Chinese Society of Particuology and Institute of Process Engineering, Chinese Academy of Sciences. Published by Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies."
Examining the Air Quality Index and its adverse effects: an analytical study for smart cities development,"Rastogi, R; Garg, P; Rastogi, M; Srivastava, P; Jain, R; Jain, P; Gupta, N",10.1504/IJEWM.2025.146423,2025,"The proposed air quality observing framework utilises an air quality record that can be effectively translated. Objective of this research study is to show the truth of the gases being discharged around parts of Delhi, India. The Air Quality Index (AQI) has been created by the Environmental Protection Agency (EPA), to give everyday levels of air contamination and a lot of increasingly precise, effectively reasonable data. The AQI is a table created by this Environmental Protection Agency (EPA) to give every day and precise data about air contamination to normal residents. AQI considers four significant poisons: sulphur dioxide, carbon monoxide, nitrogen dioxide and ozone and benzene, SPM. By recalling information on day-by-day contamination levels, the EPA can indicate a number about its AQI run. The assessment results affirm the ability of the proposed framework for constant and spatial observing of air quality. Also, it is workable for the overall population to arrive at the aftereffects of air quality observing continuously."
MATLAB Application for User-Friendly Design of Fully Convolutional Data Description Models for Defect Detection of Industrial Products and Its Concurrent Visualization,"Nagata, F; Sakata, S; Watanabe, K; Habib, MK; Ghani, ASA",10.3390/machines13040328,2025,"In this paper, a fully convolutional data description (FCDD) model is applied to defect detection and its concurrent visualization for industrial products and materials. The authors' propose a MATLAB application that enables users to efficiently and in a user-friendly way design, train, and test various kinds of neural network (NN) models for defect detection. Models supported by the application include the following original designs: convolutional neural network (CNN), transfer learning-based CNN, NN-based support vector machine (SVM), convolutional autoencoder (CAE), variational autoencoder (VAE), fully convolution network (FCN) (such as U-Net), and YOLO. However, FCDD is not yet supported. This paper includes the software development of the MATLAB R2024b application, which is extended to be able to build FCDD models. In particular, a systematic threshold determination method is proposed to obtain the best performance for defect detection from FCDD models. Also, through three different kinds of defect detection experiments, the usefulness and effectiveness of FCDD models in terms of defect detection and its concurrent visualization are quantitatively and qualitatively evaluated by comparing conventional transfer learning-based CNN models."
GOAT: a novel global-local optimized graph transformer framework for predicting student performance in collaborative learning,"Peng, TH; Yue, Q; Liang, Y; Ren, J; Luo, J; Yuan, HT; Wu, WJ",10.1038/s41598-025-93052-y,2025,"Collaborative learning is a prevalent learning method, and modeling and predicting student performance in such paradigms is an important task. Most current methods analyze this complex task solely based on the frequency of student activities, overlooking the rich spatial and temporal features present in these activities, as well as the diverse textual content provided by various learning artifacts. To address these challenges, we choose a software engineering course as the study subject, where students are required to team up and complete a software project together. In this paper, we propose a novel Global-local Optimized grAph Transformer framework for collaborative learning, termed GOAT. Specifically, we first construct the dynamic knowledge concept-enhanced interaction graphs with nodes representing both students and relevant software engineering concepts, and edges illustrating interactions. Additionally, we incorporate spatial-aware and temporal-aware modules to capture the respective information, enabling the modeling of dynamic interactions within and across learning teams over time. A global-local optimization module is introduced to model intricate relationships within and between teams, highlighting commonalities and differences among team members. Our framework is backed by theoretical analysis and validated through extensive experiments on real-world datasets, which demonstrate its superiority over existing methods."
Establishing Traceability Between Natural Language Requirements and Software Artifacts by Combining RAG and LLMs,"Ali, SJ; Naganathan, V; Bork, D",10.1007/978-3-031-75872-0_16,2025,"Software Engineering aims to effectively translate stakeholders' requirements into executable code to fulfill their needs. Traceability from natural language use case requirements to classes in a UML class diagram, subsequently translated into code implementation, is essential in systems development and maintenance. Tasks such as assessing the impact of changes and enhancing software reusability require a clear link between these requirements and their software implementation. However, establishing such links manually across extensive codebases is prohibitively challenging. Requirements, typically articulated in natural language, embody semantics that clarify the purpose of the codebase. Conventional traceability methods, relying on textual similarities between requirements and code, often suffer from low precision due to the semantic gap between high-level natural language requirements and the syntactic nature of code. The advent of Large Language Models (LLMs) provides new methods to address this challenge through their advanced capability to interpret both natural language and code syntax. Furthermore, representing code as a knowledge graph facilitates the use of graph structural information to enhance traceability links. This paper introduces an LLM-supported retrieval augmented generation approach for enhancing requirements traceability to the class diagram of the code, incorporating keyword, vector, and graph indexing techniques, and their integrated application. We present a comparative analysis against conventional methods and among different indexing strategies and parameterizations on the performance. Our results demonstrate how this methodology significantly improves the efficiency and accuracy of establishing traceability links in software development processes."
Revitalizing STEM Education: integrating STEM into National curricula with active learning,"Brito, MA; Aguiar, M; AraÃºjo, S; Varajao, J; Santos, C",10.1080/02635143.2025.2533193,2025,"BackgroundThe increasing popularity of generative Artificial Intelligence (AI) technologies further emphasizes the urgent need for a skilled workforce prepared for the evolving job market. Our research aims to propose the integration of STEM into National Curricula via active learning.PurposeThis paper provides insights gathered from an educational intervention case study that examines the development, training of educators, and incorporation of active learning educational scenarios. These scenarios aim to introduce students to programming through low-code software development. Through an interdisciplinary collaboration between technology and science educators, students engaged in developing mobile applications focused on public health topics like nutrition and hygiene.SampleThe scenarios were implemented in six schools, involving active participation from thirty-two classes - sixteen study classes and sixteen control classes. Each scenario unfolded over approximately 5 to 8 sessions. The first scenario, designed for 7th-grade students, engaged 95 participants, the second for 8th-grade involved 105 students, and the third had 112 participants.Design and methodsThe formulation and execution of the course design were guided by the ADDIE instructional design model framework, encompassing stages such as Analysis, Design, Development, Implementation, and Evaluation.ResultsPositive feedback from educators regarding the preparation and execution of educational scenarios provides evidence that this approach cultivates conducive learning environments and facilitates holistic student development. Moreover, our findings suggest that, from the educators' perspective, students enhance their scientific skills and demonstrate increased interest in science. Additionally, survey results among educators indicate a favorable perception of students' attitudes toward the implemented course design.ConclusionOur primary contribution lies in proposing an interdisciplinary approach to incorporate integrated STEM educational scenarios into national curricula, in line with UNESCO's recent recommendation for a comprehensive curriculum reform prioritizing the transition from basic digital literacy to advanced computational thinking skills."
Recommendations for Developing Effective Inclusivity Initiatives in Research Software Engineering,"Tenquist, M; Azman, A; Meaden, R; Onikan, A; Jay, C; Banerji, A",10.1109/MCSE.2025.3539076,2025,"In the United Kingdom, research software engineering is not only less diverse than the overall workforce in terms of race and gender but also falls behind both academia and the commercial software sector. This limits the availability of the diverse perspectives and skills needed to tackle today's complex research challenges. To develop meaningful inclusivity initiatives, we need to understand the workplace experiences of underrepresented individuals who contribute to research software development. However, such sociological research often places a significant emotional burden on participants, which is not always balanced by sufficient benefits. In this article, we introduce 10 guidelines for conducting research that promote ownership and equity for participants from underrepresented groups, with recommendations specific to the research software community. Our guidelines are rooted in a coproduction approach, partnering with underrepresented individuals throughout the research process. This ensures that the most pressing issues are addressed, leading to initiatives that can positively influence research software culture and benefit research outcomes."
EmoReflex: an AI-powered emotion-centric developer insights platform,"Madampe, K; Grundy, J; Nguyen, M; Welstead-Cloud, E; Huynh, VT; Doan, L; Lay, W; Hashim, S",10.1007/s10515-025-00488-7,2025,"There has been great interest in better understanding software engineer emotions during development. But how to do this? We built a prototype AI-powered Emotion-centric Developer Insights Platform, EmoReflex, to support developers to report and reflect how they feel when working on various tasks across different metrics. It also assists their managers to get insights into their team's emotional health, and provides them with recommendations to guide them handle the team's emotional wellbeing. We present our tool prototype and evaluation results generated by a user study conducted with two user groups consisting of twenty developers and twenty managers. We present some design implications derived from our user study that can be used to inform design decisions in emotion-centric software development tools."
Feature Learning via Correlation Analysis for Effective Duplicate Detection,"Yang, G; Ji, J; Kim, T",10.3390/app15031411,2025,"With the growing reliance on software, the frequency of software bugs has increased significantly. To address these issues, users or developers typically submit bug reports, which developers analyze and resolve. However, many submitted bug reports are duplicates of previously reported issues, creating inefficiencies in the bug resolution process. To enhance developer productivity, an automatic method for detecting duplicate bug reports is essential. In this study, we present a novel approach for identifying duplicate and nonduplicate bug reports using feature learning through correlation analysis. Our method utilizes bug report features, including product and component information, extracted from bug repositories. The process begins with preprocessing the bug reports to ensure data quality. Next, a feature selection algorithm identifies relevant features, which are then used to train a machine learning model based on bidirectional encoder representations from transformers (BERT). The proposed model's effectiveness was evaluated across multiple datasets: Apache, JDT, Platform, KDE, Core, Firefox, and Thunderbird. Our results show detection accuracies of 91.41%, 88.66%, 86.08%, 92.94%, 90.68%, 88.25%, and 91.62%, respectively. These outcomes represent a significant improvement of 32% to 41% compared to baseline models, including convolutional neural networks (CNNs), long short-term memory networks (LSTMs), convolutional LSTMs (CNN-LSTMs), Naive Bayes classifiers, and random forest classifiers. Our findings show that the proposed model is highly effective for duplicate bug report prediction and offers substantial advancements over existing methods. This approach has the potential to streamline bug management processes and improve overall software development efficiency."
PROSE-TO-P4: LEVERAGING HIGH LEVEL LANGUAGES,"Dumitru, MV; Badoiu, VA; Raiciu, C",,2025,"Languages such as P4 and NPL have enabled a wide and diverse range of networking applications that take advantage of programmable data planes. However, software development in these languages is difficult. To address this issue, high-level languages have been designed to offer programmers powerful abstractions that reduce the time, effort and domain-knowledge required for developing networking applications, as well as to allow writing portable and modular code. These languages are then translated by a compiler into P4/NPL code. Inspired by the recent success of Large Language Models (LLMs) in the task of code generation, we propose to raise the level of abstraction even higher, employing LLMs to translate prose into high-level networking code. We analyze this problem, focusing on the motivation and opportunities, as well as the challenges involved, and sketch out a roadmap for the development of a system that can generate high-level data plane code from natural language instructions. We present some promising preliminary results on generating Lucid code from natural language."
A Comparison of Large Language Models and Genetic Programming for Program Synthesis,"Sobania, D; Petke, J; Briesch, M; Rothlauf, F",10.1109/TEVC.2024.3410873,2025,"Large language models have recently become known for their ability to generate computer programs, especially through tools, such as GitHub Copilot, a domain where genetic programming (GP) has been very successful so far. Although they require different inputs (free-text versus input/output examples) their goal is the same-program synthesis. Therefore, in this work, we compare how well GitHub Copilot and GP perform on common program synthesis benchmark problems. We study the structure and diversity of the generated programs by using well-known software metrics. We find that GitHub Copilot and GP solve a similar number of benchmark problems (85.2% versus 77.8%, respectively). We find that GitHub Copilot generated smaller and less complex programs as GP, while GP is able to find new and unique problem solving strategies. This increase in diversity of solutions comes at a cost. When analyzing the success rates for 100 runs per problem, GitHub Copilot outperforms GP on over 50% of the problems."
Teaching Machine Learning as Part of Agile Software Engineering,"Chenoweth, S; Linos, PK",10.1109/TE.2025.3572355,2025,"Contribution: A novel undergraduate course design at the intersection of software engineering (SE) and machine learning (ML) based on industry-reported challenges. Background: ML professionals report that building ML systems is different enough that one needs new knowledge about how to infuse ML into software production. For instance, various experts need to be deeply involved with these SE projects, such as business analysts, data scientists, statisticians, and software engineers. Intended Outcomes: The creation of a table detailing and matching industry challenges with course learning objectives, course topics, instructional units, and other related activities. Application Design: Course content was derived from interviewing industry professionals with related experience as well as surveying undergraduate computer science and engineering students. The proposed course style is designed to emulate real-world ML-based SE. Findings: Experienced IT professionals testify that the synergy between ML and agile SE is maturing and now becoming the standard practice. Thus, industry-derived content for a pilot undergraduate course has been successfully crafted at the intersection of SE and ML."
Cloud-Based Certification Review Web Application with Fingerprint Authentication for Computer Engineering Students,"Bohol, KKA; De Jesus, MSA; Pangilinan, AYB; Dequilla-Nicerio, M",10.1109/IC4e65071.2025.11075487,2025,"The study aims to enhance the certification review process for Computer Engineering (CPE) students by developing a comprehensive Certification Review Application. The primary objective is to create an intuitive platform that ensures secure exam preparation by integrating advanced technologies such as cloud computing and biometric authentication. The proposed system utilizes cloud technology to access study materials and resources conveniently. Biometric authentication adds to a layer of security, ensuring that only authorized users can access the platform. The Certification Review Application will also offer personalized study plans tailored to individual students' learning needs. The system testing took place on campus, and thirty (30) computer engineering students participated in evaluating the proposed system. The results show that the developed system is convenient for the user to use in terms of messaging features, viewing announcements, taking learning tasks and assessments with an average of 46.7% with an interpretation of Satisfied and 43.3% with an understanding of Very Satisfied indicating that the end-users were highly satisfied with its features."
"Exploring Code Language Models for Automated HLS-based Hardware Generation: Benchmark, Infrastructure and Analysis","Gai, JH; Chen, HM; Wang, ZC; Zhou, HY; Zhao, WR; Lane, N; Fan, HX",10.1145/3658617.3697616,2025,"Recent advances in code generation have illuminated the potential of employing large language models (LLMs) for general-purpose programming languages such as Python and C++, opening new opportunities for automating software development and enhancing programmer productivity. The potential of LLMs in software programming has sparked significant interest in exploring automated hardware generation and automation. Although preliminary endeavors have been made to adopt LLMs in generating hardware description languages (HDLs) such as Verilog and SystemVerilog, several challenges persist in this direction. First, the volume of available HDL training data is substantially smaller compared to that for software programming languages. Second, the pre-trained LLMs, mainly tailored for software code, tend to produce HDL designs that are more error-prone. Third, the generation of HDL requires a significantly higher number of tokens compared to software programming, leading to inefficiencies in cost and energy consumption. To tackle these challenges, this paper explores leveraging LLMs to generate High-Level Synthesis (HLS)-based hardware design. Although code generation for domain-specific programming languages is not new in the literature, we aim to provide experimental results, insights, benchmarks, and evaluation infrastructure to investigate the suitability of HLS over low-level HDLs for LLM-assisted hardware design generation. To achieve this, we first finetune pre-trained models for HLS-based hardware generation, using a collected dataset with text prompts and corresponding reference HLS designs. An LLM-assisted framework is then proposed to automate end-to-end hardware code generation, which also investigates the impact of chain-of-thought and feedback loops promoting techniques on HLS- design generation. Comprehensive experiments demonstrate the effectiveness of our methods."
Question-Answer Methodology for Vulnerable Source Code Review via Prototype-Based Model-Agnostic Meta-Learning,"Corona-Fraga, P; Hernandez-Suarez, A; Sanchez-Perez, G; Toscano-Medina, LK; Perez-Meana, H; Portillo-Portillo, J; Olivares-Mercado, J; Villalba, LJG",10.3390/fi17010033,2025,"In cybersecurity, identifying and addressing vulnerabilities in source code is essential for maintaining secure IT environments. Traditional static and dynamic analysis techniques, although widely used, often exhibit high false-positive rates, elevated costs, and limited interpretability. Machine Learning (ML)-based approaches aim to overcome these limitations but encounter challenges related to scalability and adaptability due to their reliance on large labeled datasets and their limited alignment with the requirements of secure development teams. These factors hinder their ability to adapt to rapidly evolving software environments. This study proposes an approach that integrates Prototype-Based Model-Agnostic Meta-Learning(Proto-MAML) with a Question-Answer (QA) framework that leverages the Bidirectional Encoder Representations from Transformers (BERT) model. By employing Few-Shot Learning (FSL), Proto-MAML identifies and mitigates vulnerabilities with minimal data requirements, aligning with the principles of the Secure Development Lifecycle (SDLC) and Development, Security, and Operations (DevSecOps). The QA framework allows developers to query vulnerabilities and receive precise, actionable insights, enhancing its applicability in dynamic environments that require frequent updates and real-time analysis. The model outputs are interpretable, promoting greater transparency in code review processes and enabling efficient resolution of emerging vulnerabilities. Proto-MAML demonstrates strong performance across multiple programming languages, achieving an average precision of 98.49%, recall of 98.54%, F1-score of 98.78%, and exact match rate of 98.78% in PHP, Java, C, and C++."
Application of an Innovative Methodology to Build Infrastructure for Digital Transformation of Health Systems: Developmental Program Evaluation,"Buchan, MC; Katapally, TR; Bhawra, J",10.2196/53339,2025,"Background: The current public health crises we face, including communicable disease pandemics such as COVID-19, require cohesive societal efforts to address decision-making gaps in our health systems. Digital health platforms that leverage big data ethically from citizens can transform health systems by enabling real-time data collection, communication, and rapid responses. However, the lack of standardized and evidence-based methods to develop and implement digital health platforms currently limits their application. Objective: This study aims to apply mixed evaluation methods to assess the development of a rapid response COVID-19 digital health platform before public launch by engaging with the development and research team, which consists of interdisciplinary Methods: Using a developmental evaluation approach, this study conducted (1) a qualitative survey assessing digital health platform objectives, modifications, and challenges administered to 5 key members of the software development team and (2) a role-play pilot with 7 key stakeholders who simulated 8 real-world users, followed by a self-report survey, to evaluate the utility of the digital health platform for each of its objectives. Survey data were analyzed using an inductive thematic analysis approach. Postpilot test survey data were aggregated and synthesized by participant role. Results: The digital health platform met original objectives and was expanded to accommodate the evolving needs of potential users and COVID-19 pandemic regulations. Key challenges noted by the development team included navigating changing government policies and supporting the data sovereignty of platform users. Strong team cohesion and problem-solving were essential in the overall success of program development. During the pilot test, participants reported positive experiences interacting with the platform and found its features relatively easy to use. Users in the community member role felt that the platform accurately reflected their risk of contracting COVID-19, but reported some challenges interacting with the interface. Those in the decision maker role found the data visualizations helpful for understanding complex information. Both participant groups highlighted the utility of a tutorial for future users. Conclusions: Evaluation of the digital health platform development process informed our decisions to integrate the research team more cohesively with the development team, a practice that is currently uncommon given the use of external technology vendors in health research. In the short term, the developmental evaluation resulted in shorter sprints, and the role-play exercise enabled improvements to the log-in process and user interface ahead of public deployment. In the long term, this exercise informed the decision to include a data scientist as part of both teams going forward to liaise with researchers throughout the development process. More interdisciplinarity was also integrated into the research process by providing health system training to computer programmers, a key factor in human-centered artificial intelligence development."
A Framework for Efficient Development and Debugging of Role-Playing Agents with Large Language Models,"Takagi, H; Moriya, S; Sato, T; Nagao, M; Higuchi, K",10.1145/3708359.3712119,2025,"We propose a framework that leverages large language models (LLMs) to semi-automate the development and debugging of role-playing agents, reducing the need for extensive manual effort. Role-playing agents powered by LLMs offer scalable solutions that enhance communication and interaction in various applications, such as employee training, healthcare, and software development. However, creating prompts manually is a time-consuming process, and sequential debugging increases the difficulty of anticipating conversation flow, resulting in increased cognitive load. Our framework addresses these challenges by generating and summarizing dialogue examples, providing a clearer overview of conversation flow and reduce mental workload. It also enhances role-playing quality by mitigating LLMs' tendency to produce generic or vague responses. In a user study, the proposed method significantly improved perceived workload and five of the six NASA-TLX dimensions. Moreover, it can generate agents comparable to those created with expertly crafted prompts. This framework is model-agnostic, enabling integration of advancements in LLM capabilities and prompting techniques, and is applicable to diverse domains."
DSKIPP: A Prompt Method to Enhance the Reliability in LLMs for Java API Recommendation Task,"Yang, JB; Wu, WJ; Ren, J",10.1002/stvr.1913,2025,"In the realm of software development, selecting the appropriate Java application programming interfaces (APIs) from a vast pool remains a significant challenge for developers. This research addresses this complexity by tackling the limitations of current API recommendation methods, which often struggle to align API suggestions with the specific queries and development contexts. In this paper, we introduce a novel prompt method named DSKIPP (Development Scenario, key Knowledge and Intention's Progressive Prompt), designed to enhance the efficiency of large language models (LLMs) in Java API recommendations. Firstly, we devise an overview of DSKIPP which conducts LLMs through a sequential process: first, inferring the package level, followed by the class level, and ultimately the method level as an API comprises three distinct components at varying levels-package, class and method. Secondly, at each level, DSKIPP assists LLMs in deducing the development scenario associated with a query and the essential key knowledge relevant to that scenario. This approach enables LLMs to gain a more profound contextual understanding of the query's intention. Moreover, during the inference process at the class and method level, we implement a self-check mechanism enabling LLMs to validate the results and ensure a more reasoned and reliable outcome. To validate the efficiency of DSKIPP, comparison and ablation experiments are both conducted within Java programming environment. The comparison results affirm that our method outperforms the current state-of-the-art technologies in API recommendation tasks, while the ablation results shed light on why DSKIPP can enhance the reliability of API recommendations in LLMs. This research contributes to the field by offering a more reliable and context-sensitive solution for API recommendation in software development."
Supporting the identification of prevalent quality issues in code changes by analyzing reviewers' feedback,"Iftikhar, U; BÃ¶rstler, J; Bin Ali, N; Kopp, O",10.1007/s11219-025-09720-9,2025,"Context: Code reviewers provide valuable feedback during the code review. Identifying common issues described in the reviewers' feedback can provide input for devising context-specific software development improvements. However, the use of reviewer feedback for this purpose is currently less explored. Objective: In this study, we assess how automation can derive more interpretable and informative themes in reviewers' feedback and whether these themes help to identify recurring quality-related issues in code changes. Method: We conducted a participatory case study using the JabRef system to analyze reviewers' feedback on merged and abandoned code changes. We used two promising topic modeling methods (GSDMM and BERTopic) to identify themes in 5,560 code review comments. The resulting themes were analyzed and named by a domain expert from JabRef. Results: The domain expert considered the identified themes from the two topic models to represent quality-related issues. Different quality issues are pointed out in code reviews for merged and abandoned code changes. While BERTopic provides higher objective coherence, the domain expert considered themes from short-text topic modeling more informative and easy to interpret than BERTopic-based topic modeling. Conclusions: The identified prevalent code quality issues aim to address the maintainability-focused issues. The analysis of code review comments can enhance the current practices for JabRef by improving the guidelines for new developers and focusing discussions in the developer forums. The topic model choice impacts the interpretability of the generated themes, and a higher coherence (based on objective measures) of generated topics did not lead to improved interpretability by a domain expert."
Generative API Recommendation Based on Global Semantics and Local Context,"Li, SM; Yu, DJ; Chen, X; Fan, XL; Luo, DF; Wu, T; Yan, WL",10.1142/S0218194025500275,2025,"During software development, developers often need appropriate but unfamiliar APIs to implement a specific functionality. Under such circumstances, developers tend to leverage search tools to seek for the relevant APIs. However, there are always semantic gaps between query words and APIs, which negatively affects the performance of these tools. In this study, we introduce Glo-APIRec, a method that combines global semantics with local context to estimate the semantic relevance between query words and APIs to recommend APIs. In this method, the Transformer model is employed to obtain global semantics, while the Word2Vec model is utilized to capture local context using a fixed-size window. First, Glo-APIRec collects millions of Java projects from GitHub to construct the corpus. Afterward, a set of tuples consisting of words and APIs is built by extracting comments and API sequences from the source code files. Finally, Transformer is employed to capture long distance semantics about API sequences and code comments. Meanwhile, Word2Vec is used to generate word vectors to capture the local context by introducing the random shuffling strategy to break the positions of words and APIs in the tuples. We evaluate the performance of Glo-APIRec with 30 sentence-level queries. Experimental results show that Glo-APIRec can achieve 0.600 in terms of SuccessRate for top-1 recommendation and 0.900 for top-10 recommendation. When recommending 10 APIs, Glo-APIRec can achieve 0.480, 0.703 and 0.717 in terms of precision, Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG), and outperforms the state-of-the-art method by 26.2%, 31.7% and 27.9%, respectively."
Requirements Traceability Link Recovery via Retrieval-Augmented Generation,"Hey, T; Fuchss, D; Keim, J; Koziolek, A",10.1007/978-3-031-88531-0_27,2025,"[Context and Motivation] In software development, various interrelated artifacts are created. Access to information on the relation between these artifacts eases understanding of the system and enables tasks such as change impact and software reusability analyses. Manual trace link creation is labor-intensive and costly, and thus is often missing in projects. Automation could enhance the development and maintenance efficiency. [Question/Problem] Current methods for automatically recovering traceability links between different types of requirements do not achieve the necessary performance to be applied in practice, or require pre-existing links for machine learning. [Principal Ideas and Results] We propose to address this limitation by leveraging large language models (LLMs) with retrieval-augmented generation (RAG) for inter-requirements traceability link recovery. In an empirical evaluation on six benchmark datasets, we show that chain-of-thought prompting can be beneficial, open-source models perform comparably to proprietary ones, and that the approach can outperform state-of-the-art and baseline approaches. [Contribution] This work presents an approach for inter-requirements traceability link recovery using RAG and provides the first empirical evidence of its performance."
Learning through development of a digital manufacturing system in a learning factory using low-code/no-code platforms,"Bradley, R; Salim, SS; Anthony, BW",10.1016/j.mfglet.2025.09.001,2025,"This study demonstrates how low-code/no-code (LCNC) platforms can enable undergraduate students without software development backgrounds to design and build digital manufacturing systems. Students developed an IoT-enabled Manufacturing Execution System using Tulip Interfaces-an LCNC platform, focusing on applications like inventory tracking, machine monitoring, and digital work instructions in the FrED Factory-a learning factory at MIT. Evaluation through a pilot study showed students gained a strong understanding of smart manufacturing concepts while spending most of their time on systems design rather than software development. Individual interviews followed by a post-interview survey highlighted that the average percentage of time split between systems design and debugging the LCNC platform was 70-30% respectively. Additionally, all students responded with strongly agree to the question of whether the project enhanced their understanding of smart manufacturing concepts. LCNC platforms offer a practical, accessible approach to teaching digital manufacturing and can accelerate skill development in both educational and industrial settings. (c) 2025 Society of Manufacturing Engineers (SME). Published by Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies."
FormalSpecCpp: A Dataset of C plus plus Formal Specifications created using LLMs,"Chakraborty, M; Pirkelbauer, P; Yi, Q",10.1109/MSR66628.2025.00113,2025,"FormalSpecCpp is a dataset designed to fill the gap in standardized benchmarks for verifying formal specifications in C++ programs. To the best of our knowledge, this is the first comprehensive collection of C++ programs with well-defined preconditions and postconditions. It provides a structured benchmark for evaluating specification inference tools and testing the accuracy of generated specifications. Researchers and developers can use this dataset to benchmark specification inference tools, fine-tune Large Language Models (LLMs) for automated specification generation, and analyze the role of formal specifications in improving program verification and automated testing. By making this dataset publicly available, we aim to advance research in program verification, specification inference, and AI-assisted software development. The dataset and the code are available at https://github.com/MadhuNimmo/FormalSpecCpp."
Enhancing Project-Specific Code Completion by Inferring Internal API Information,"Deng, L; Ren, XX; Ni, C; Liang, M; Lo, D; Liu, ZX",10.1109/TSE.2025.3592823,2025,"Project-specific code completion, which aims to complete code based on the context of the project, is an important and practical software engineering task. The state-of-the-art approaches employ the retrieval-augmented generation (RAG) paradigm and prompt large language models (LLMs) with information retrieved from the target project for project-specific code completion. In practice, developers always define and use custom functionalities, namely internal APIs, to facilitate the implementation of specific project requirements. Thus, it is essential to consider internal API information for accurate project-specific code completion. However, existing approaches either retrieve similar code snippets, which do not necessarily contain related internal API information, or retrieve internal API information based on import statements, which usually do not exist when the related internal APIs haven't been used in the file. Therefore, these project-specific code completion approaches face challenges in effectiveness or practicability. To this end, this paper aims to enhance project-specific code completion by locating internal API information without relying on import statements. We first propose a method to infer internal API information. Our method first extends the representation of each internal API by constructing its usage examples and functional semantic information (i.e., a natural language description of the function's purpose) and constructs a knowledge base. Based on the knowledge base, our method uses an initial completion solution generated by LLMs to infer the API information necessary for completion. Based on this method, we propose a code completion approach that enhances project-specific code completion by integrating similar code snippets and internal API information. Furthermore, we developed a benchmark named ProjBench, which consists of recent, large-scale real-world projects and is free of leaked import statements. We evaluated the effectiveness of our approach on ProjBench and an existing benchmark CrossCodeEval. Experimental results show that our approach outperforms the base-performing approach by an average of +5.91 in code exact match and +6.26 in identifier exact match, corresponding to relative improvements of 22.72% and 18.31%, respectively. We also show our method complements existing ones by integrating it into various baselines, boosting code match by +7.77 (47.80%) and identifier match by +8.50 (35.55%) on average."
Prototyping of Automated Guided Vehicle for Teaching Practical Mechatronics,"Ria, A; Dini, P; Bucchi, F",10.3390/educsci15030294,2025,"This paper presents an innovative approach to teaching mechatronics at the bachelor's level, using the design and construction of an Automated Guided Vehicle (AGV) as a comprehensive example of a mechatronic system. The course, titled Laboratory of Electronic Systems, is part of a newly established professionalizing bachelor's degree program at the University of Pisa, focused on techniques for mechanics and production. This program was developed to meet industry demands for technically skilled personnel with an engineering-related background but without the need for a full traditional engineering education. The course is designed to provide students with hands-on experience, integrating fundamental concepts from mechanical, electronic, and control engineering, along with software development. The curriculum emphasizes practical applications rather than theoretical depth, aligning with the program's goal of preparing students for operational roles in industrial settings. We present the course structure, educational objectives, and the interdisciplinary nature of mechatronics as addressed in this teaching approach. A dedicated section outlines the critical steps involved in the AGV prototype development, highlighting practical challenges and learning opportunities. The effectiveness of the course is assessed through the evaluation of student projects, specifically via a technical report and a final discussion on the design of a mechatronic system. The results demonstrate the value of a project-based learning approach in equipping students with the practical skills and knowledge required for careers in mechatronics and industrial automation."
Refactoring for Dockerfile Quality: A Dive into Developer Practices and Automation Potential,"Ksontini, E; Mastouri, M; Khalsi, R; Kessentini, W",10.1109/MSR66628.2025.00116,2025,"Docker, the industry standard for packaging and deploying applications, leverages Infrastructure as Code (IaC) principles to facilitate the creation of images through Dockerfiles. However, maintaining Dockerfiles presents significant challenges. Refactoring, in particular, is often a manual and complex process. This paper explores the utility and practicality of automating Dockerfile refactoring using 600 Dockerfiles from 358 open-source projects. Our study reveals that Dockerfile image size and build duration tend to increase as projects evolve, with developers often postponing refactoring efforts until later stages in the development cycle. This trend motivates the automation of refactoring. To achieve this, we leverage In Context Learning (ICL) along with a score-based demonstration selection strategy. Our approach leads to an average reduction of 32% in image size and a 6% decrease in build duration, with improvements in understandability and maintainability observed in 77% and 91% of cases, respectively. Additionally, our analysis shows that automated refactoring reduces Dockerfile image size by 2x compared to manual refactoring and 10x compared to smell-fixing tools like PARFUM. This work establishes a foundation for automating Dockerfile refactoring, indicating that such automation could become a standard practice within CI/CD pipelines to enhance Dockerfile quality throughout every step of the software development lifecycle."
Reducing Falls Among Older Adult Residents: An Evidence-Based Practice Quality Improvement Initiative,"Awotundun, AY",10.1016/j.nurpra.2025.105511,2025,"In United States, 800,000 falls occur yearly, and 1 in 3 residents will fall again within a year. This nurse practitioner-led evidence-based practice quality improvement project took place in a nursing home in the northeastern United States. This project focused on the Individualized Multidisciplinary Immediate Fall Response Program developed by the Agency for Healthcare Research and Quality. It project included residents in skilled units, from 65 years and older. Falls data were analyzed using descriptive statistics over an 8week period. There were 41 falls preintervention compared with 30 falls postintervention, a decrease of 27%. (c) 2025 Elsevier Inc. All rights are reserved, including those for text and data mining, AI training, and similar technologies."
A Transmission Axle Fault Diagnosis System for Massive Rapid Transit System With Enhanced Attention-Based Shuffle Networks,"Yao, L; Su, H; Cheng, M",10.1109/ACCESS.2025.3604605,2025,"The transmission axle faults can cause severe damage to gearbox components and jeopardize motor power transmission. A transmission axle fault (TAF) diagnosis method for train propulsion systems in mass rapid transit (MRT) networks is proposed, utilizing an Enhanced Attention-based Shuffle Network (EASN). Time-domain vibration signals, directly acquired from sensors mounted on the transmission axles, are employed as input to the EASN model. The proposed model is specifically designed for deployment on lightweight AI edge devices, with an emphasis on computational efficiency and real-time performance for onboard diagnostic applications. The EASN architecture is composed of multiple building blocks, referred to as Split-Attention Shuffle Units (SASUs). Each SASU integrates a Shuffling Processing Module (SPM) and a Split Attention Module (SAM) in a cascaded configuration. While the SPM is based on ShuffleNet, which is known for its computational efficiency but relatively lower classification accuracy, the proposed SASU mitigates this limitation through the introduction of an even channel shuffling mechanism combined with a hybrid attention strategy. The hybrid attention scheme leverages both Spatial Excitation (SPE) and Squeeze-and-Excitation (SnE) mechanisms, significantly enhancing the network's diagnostic accuracy without compromising its lightweight design. Experimental results demonstrate that the proposed EASN achieves a Top-1 classification accuracy of 93.9%, representing a 5.6% improvement over ResNet-50 while reducing the model size by 98.8%. Compared with lightweight models such as MobileNet V2, EASN improves accuracy by 19.4% with only a 28.6% increase in parameter size. These findings indicate that EASN offers an effective balance between diagnostic accuracy and model compactness, making it well-suited for real-time, edge-based fault detection in mass rapid transit systems."
On Security Weaknesses and Vulnerabilities in Deep Learning Systems,"Lai, ZZ; Chen, HM; Sun, RX; Zhang, Y; Xue, MH; Yuan, D",10.1109/TDSC.2024.3482707,2025,"The security guarantee of AI-enabled software systems (particularly using deep learning techniques as a functional core) is pivotal against the adversarial attacks exploiting software vulnerabilities. However, little attention has been paid to a systematic investigation of vulnerabilities in such systems. A common situation learned from the open source software community is that deep learning engineers frequently integrate off-the-shelf or open-source learning frameworks into their ecosystems. In this work, we specifically look into deep learning (DL) framework and perform the first systematic study of vulnerabilities in DL systems through a comprehensive analysis of identified vulnerabilities from Common Vulnerabilities and Exposures (CVE) and open-source DL tools, including TensorFlow, Caffe, OpenCV, Keras, and PyTorch. We propose a two-stream data analysis framework to explore vulnerability patterns from various databases. We investigate the unique DL frameworks and libraries development ecosystems that appear to be decentralized and fragmented. By revisiting the Common Weakness Enumeration (CWE) List, which provides the traditional software vulnerability related practices, we observed that it is more challenging to detect and fix the vulnerabilities throughout the DL systems lifecycle. Moreover, we conducted a large-scale empirical study of 3,049 DL vulnerabilities to better understand the patterns of vulnerability and the challenges in fixing them."
Clinical data integration and processing challenges in healthcare caused by contemporary software design,"Petri, V; Antoine, T; Emma, B; Herve, H; Emma, F; Hana, V",10.1177/20552076251374233,2025,"Objective The quantity of patient data in healthcare is exponentially increasing. While big data and artificial intelligence have emerged across the fields, in healthcare, such rapid development is hindered by numerous factors. Predominantly, health-care software developed decades ago cannot foresee the demands of modern data processing and analysis. We present the challenges, remedies, and steps of efficient patient data integration that have been co-developed with clinicians at Lenval Children's University Hospital in Nice, France.Methods In collaboration with pediatricians, we created an integration framework that integrated a patient's germane historical data (from the past 10 years) for research purposes. The clinical data presented in this study were collected between 2012 and 2021 in the Lenval Children's University Hospital Pediatric Emergency Department.Results We present the architecture of a clinical data warehouse (CDW) and demonstrate its use. CDW can also host doctoral notes, which is the key element for creating large language models that can help predict patient outcomes and provide critical information to health-care professionals. We also conducted several tests on the utilization of this new CDW, recorded multiple challenges on data integration, and gave three suggestions on software design. The CDW we created represents a solid foundation for future machine learning models of patient flow, hospital economics, and studies on rare diseases at CHU-Lenval.Conclusion Although the integration framework is grounded in pediatrics, the challenges discussed, and the proposed remedies are relevant for software development across medical specializations. Our recommendations for software design can help with future secondary usage of Electronic Health Record."
"Industrial applications of artificial intelligence in software defects prediction: Systematic review, challenges, and future works","Daza, A; Apaza-Perez, G; Samanez-Torres, K; Benites-Noriega, J; Gonzales, OL; Condori-Cutipa, PC",10.1016/j.compeleceng.2025.110411,2025,"Software defect prediction is a constant challenge in industrial software engineering and represents a significant problem for quality and cost in software development worldwide. The purpose of this study is to gain a deeper understanding of the quartiles, countries, keywords, techniques, metrics, tools, platforms or languages, variables, data sources, and datasets used in software defect prediction. A comprehensive search of 45 articles from 2019 to 2023, using 5 databases (Scopus, ProQuest, ScienceDirect, EBSCOhost, and Web of Science), was conducted following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analysis) methodology. Results show that 60.00 % of the studies were carried out in 2023, and 68.89 % of journals were in the Q1 and Q2 quartiles. The most common techniques were Support Vector Machine (42.22 %) and Random Forest (35.56 %). The most commonly used evaluation metrics were Accuracy and F1-Score (68.89 %). Python was the main programming language (35.56 %), with Kilo (thousands) of lines of code (31.11 %) and Cyclomatic complexity (26.67 %) as key variables. Finally, NASA's Metrics Data Program Data Repository was the most used data source (31.11 %) with a dataset ranging from a minimum of 759 instances and 37 attributes to a maximum of 3579 instances and 38 attributes from 5 projects: CM1, MW1, PC1, PC3, and PC4. This systematic review provides scientific evidence on how machine learning algorithms aid in predicting software defects and improving development processes. In addition, it offers a detailed discussion by identifying trends, limitations, successful approaches, and areas for improvement, providing valuable recommendations for future research."
Toward Tool-Agnostic Guidelines for Expert Debugging Strategies,"Safarpour, H",10.1109/ICST62969.2025.10988987,2025,"Debugging is an essential but not fully understood component of the software development life cycle. Although developers utilize diverse strategies-from manual code inspections to sophisticated AI-assisted techniques-the cognitive processes underlying expert debugging remain ambiguous. This research seeks to elucidate the cognitive and behavioral patterns that inform expert debuggers' decision-making and convert these insights into tool-agnostic guidelines. By adopting a Design Science methodology [1] and integrating think-aloud protocols, cognitive modeling (e.g., using ACT-R), and comparative analyses, we aim to generate actionable principles that enhance debugging efficiency and consistency. These guidelines will be iteratively refined through empirical validation in collaboration with cognitive psychology experts, and the outcomes will be disseminated through both conferences and high-impact journals. Our contributions are intended to assist researchers in reducing bias during tool evaluation and to provide developers and educators with robust, human-centered debugging practices."
"How Scientists Use Jupyter Notebooks: Goals, Quality Attributes, and Opportunities","Huang, RQQ; Ravi, S; He, M; Tian, BY; Lerner, S; Coblenz, M",10.1109/ICSE55347.2025.00232,2025,"Computational notebooks are intended to prioritize the needs of scientists, but little is known about how scientists interact with notebooks, what requirements drive scientists' software development processes, or what tactics scientists use to meet their requirements. We conducted an observational study of 20 scientists using Jupyter notebooks for their day-to-day tasks, finding that scientists prioritize different quality attributes depending on their goals. A qualitative analysis of their usage shows (1) a collection of goals scientists pursue with Jupyter notebooks, (2) a set of quality attributes that scientists value when they write software, and (3) tactics that scientists leverage to promote quality. In addition, we identify ways scientists incorporated AI tools into their notebook work. From our observations, we derive design recommendations for improving computational notebooks and future programming systems for scientists. Key opportunities pertain to helping scientists create and manage state, dependencies, and abstractions in their software, enabling more effective reuse of clearly-defined components."
Improving Retrieval-Augmented Deep Assertion Generation via Joint Training,"Zhang, QJ; Fang, CR; Zheng, Y; Qian, RX; Yu, SC; Zhao, Y; Zhou, JY; Yang, Y; Zheng, T; Chen, ZY",10.1109/TSE.2025.3545970,2025,"Unit testing attempts to validate the correctness of basic units of the software system under test and has a crucial role in software development and testing. However, testing experts have to spend a huge amount of effort to write unit test cases manually. Very recent work proposes a retrieve-and-edit approach to automatically generate unit test oracles, i.e., assertions. Despite being promising, it is still far from perfect due to some limitations, such as splitting assertion retrieval and generation into two separate components without benefiting each other. In this paper, we propose AG-RAG, a retrieval-augmented automated assertion generation (AG) approach that leverages external codebases and joint training to address various technical limitations of prior work. Inspired by the plastic surgery hypothesis, AG-RAG attempts to combine relevant unit tests and advanced pre-trained language models (PLMs) with retrieval-augmented fine-tuning. The key insight of AG-RAG is to simultaneously optimize the retriever and the generator as a whole pipeline with a joint training strategy, enabling them to learn from each other. Particularly, AG-RAG builds a dense retriever to search for relevant test-assert pairs (TAPs) with semantic matching and a retrieval-augmented generator to synthesize accurate assertions with the focal-test and retrieved TAPs as input. Besides, AG-RAG leverages a code-aware language model CodeT5 as the cornerstone to facilitate both assertion retrieval and generation tasks. Furthermore, AG-RAG designs a joint training strategy that allows the retriever to learn from the feedback provided by the generator. This unified design fully adapts both components specifically for retrieving more useful TAPs, thereby generating accurate assertions. AG-RAG is a generic framework that can be adapted to various off-the-shelf PLMs. We extensively evaluate AG-RAG against six state-of-the-art AG approaches on two benchmarks and three metrics. Experimental results show that AG-RAG significantly outperforms previous AG approaches on all benchmarks and metrics, e.g., improving the most recent baseline EditAS by 20.82% and 26.98% in terms of accuracy. AG-RAG also correctly generates 1739 and 2866 unique assertions that all baselines fail to generate, 3.45X and 9.20X more than EditAS. We further demonstrate the positive contribution of our joint training strategy, e.g., AG-RAG improving a variant without the retriever by an average accuracy of 14.11%. Besides, adopting other PLMs can provide substantial advancement, e.g., AG-RAG with four different PLMs improving EditAS by an average accuracy of 9.02%, highlighting the generalizability of our framework. Overall, our work demonstrates the promising potential of jointly fine-tuning the PLM-based retriever and generator to predict accurate assertions by incorporating external knowledge sources, thereby reducing the manual efforts of unit testing experts in practical scenarios."
Quantifying behavior-based gender discrimination on collaborative platforms,"VÃ¡sÃ¡rhelyi, O; Vedres, B",10.1093/pnasnexus/pgaf026,2025,"Digital collaborative platforms have become crucial venues of career advancement and individual success in many creative fields, from engineering to the arts. Gender discrimination related to behavioral choices of users is a key component to gendered disadvantage on platforms. Such platforms carried the promise of opening avenues of advancement to previously discriminated groups, such as women, as platforms lack managerial gatekeepers with conventional prejudice. We analyzed the extent of behavior-based gender discrimination on two digital platforms, GitHub and Behance, focused on software development and fine arts and design. We found that the main cause of women's disadvantage in attention, success, and survival is largely due to the gender typicality of their behavior that varies between 60 and 90% of the total disadvantage of women. Men and women are penalized if they follow highly female-like behavior, while categorical gender is no longer significant. As platforms employ algorithmic tools and AI systems to manage users' activity and visibility, and recommend new projects to collaborate, stereotypes associated with behavior can have long-lasting consequences."
"Democratizing Digital Transformation: A Multisector Study of Low-Code Adoption Patterns, Limitations, and Emerging Paradigms","Shi, ZW; Dong, JY; Gan, YH",10.3390/app15126481,2025,"Low-code development platforms (LCDPs) have emerged as transformative tools for accelerating digital transformation across industries by enabling rapid application development with minimal hand-coding. This paper synthesizes existing research and industry practices to explore the adoption, benefits, challenges, and future directions of low-code technologies in key sectors: automotive, equipment manufacturing, aerospace, electronics, and energy. Drawing on academic literature, industry reports, and case studies, this review highlights how low-code bridges the gap between IT and domain experts while addressing sector-specific demands. The study emphasizes the significant impact of LCDPs on operational efficiency, innovation acceleration, and the democratization of software development. However, it also identifies critical challenges related to customization, interoperability, security, and usability. The paper concludes with a discussion of emerging trends, including enhanced AI/ML integration, edge computing, open-source ecosystems, and sector-specific platform evolution, which are poised to shape the future of low-code development. Ultimately, this research underscores the potential of low-code platforms to drive sustainable digital transformation while addressing the complex needs of modern industries."
People and Management Debt in ML-Integrated Software Projects: Structuring Industry Insights,"Dayan-Akman, P; Ãzcan-Top, Ã; Akman, TT",10.1109/ACCESS.2025.3595609,2025,"The accelerated development of Machine Learning (ML) tools, combined with broader access to frameworks and infrastructures, has driven the rapid adoption of ML-based solutions in industry. However, their integration into software systems introduces unique challenges, particularly for managing technical debt (TD). Traditional TD research focuses primarily on technical issues, but in ML systems, people and management factors, referred to as nontechnical debt (NTD), play a critical role in TD accumulation and persistence. In this study, we investigate the underexplored dimension of NTD in ML-integrated software systems, focusing on people- and management-related factors. Using Design Science Research (DSR) methodology, we developed an artifact in an iterative incremental manner that categorizes NTD issues in ML systems. As part of this process, we conducted semi-structured interviews with 18 professionals from 15 companies, examining 22 ML projects. Through thematic analysis, we identified 15 NTD categories, 10 of which relate to people debt, and the remaining 5 to management debt. Each category is associated with underlying causes, short-term fixes, and potential solutions. Our findings show that NTD in ML projects frequently arise from inadequate decision-making practices, particularly those related to technology adoption, knowledge management, and human resource planning. Additional sources of NTD include challenges in team dynamics, such as insufficient collaboration, poor skill integration, and ineffective team structuring, as well as communication barriers rooted in organizational culture and team interactions. These factors collectively and substantially impact project outcomes. While band-aid solutions may provide short-term relief, they frequently contribute to accumulation over time. To support practitioners and researchers, our study complements the proposed artifact with actionable recommendations informed by expert perspectives and literature."
Nurse and Other Healthcare Managers' Experiences and Recommendations for Patient Incident Reporting Processes and Real-Time Software Development: A Qualitative Study,"Koskiniemi, S; Jukarainen, L; SyyrilÃ¤, T; Manias, E; HÃ¤meen-Anttila, K; HÃ¤rkÃ¤nen, M",10.1111/jan.70220,2025,"Aims To (1) analyse managers' experiences with handling patient safety incident reports in an incident reporting software, identifying key challenges; (2) analyse the incident report processes from the managers' perspective; (3) examine managers' perceptions of ways to support and improve health professionals' experiences of report-handling processes; and (4) investigate how, from their point of view, incident reporting software should be developed in the future.Design A descriptive qualitative study.Methods Interviews and focus group discussions on Microsoft Teams from 11/2024 to 3/2025, including 16 participants, analysis with deductive and inductive content analysis.Results Of 16 participants, 15 were managers and one was a patient safety expert. Most were nurse managers (n = 9). Four discussion themes were divided into 30 categories. Participants highlighted the need to improve the reporting software's terminology, classification and analysis tools. The use of artificial intelligence was desired but not currently integrated into the software. Participants were unsure of their skills to use all the software features. Clear and transparent handling processes, feedback, managers' behaviour and communication methods were seen as key to improving staff's experience with report processes. A real-time warning system was considered beneficial for various incident types. Specific questions must be answered before further developing such systems.Conclusion This study deepened the understanding of reporting software's challenges regarding its handling features. The handling processes of incident reports had multiple shortcomings, which may negatively affect health professionals' experiences in report handling. Real-time warning systems could assist healthcare managers in processing reports.Implications for the Profession and/or Patient Care Organisational-level guidance for incident report processing is needed. Improvements to report processing and reporting software can improve shared learning and understanding of the status of patient safety.Patient or Public Contribution No patient or public contribution.Reporting Method COnsolidated criteria for REporting Qualitative research Checklist."
Privacy Impact Tree Analysis (PITA): A Tree-Based Privacy Threat Modeling Approach,"Van Landuyt, D",10.1109/TSE.2025.3573380,2025,"Threat modeling involves the early identification, prioritization and mitigation of relevant threats and risks, during the design and conceptualization stages of the software development life-cycle. Tree-based analysis is a structured risk analysis technique that starts from the articulation of possible negative outcomes and then systematically refines these into sub-goals, events or intermediate steps that contribute to this outcome becoming reality. While tree-based analysis techniques are widely adopted in the area of safety (fault tree analysis) or in cybersecurity (attack trees), this type of risk analysis approach is lacking in the area of privacy. To alleviate this, we present privacy impact tree analysis (PITA), a novel tree-based approach for privacy threat modeling. Instead of starting from safety hazards or attacker goals, PITA starts from listing the potential privacy impacts of the system under design, i.e., specific scenarios in which the system creates or contributes to specific privacy harms. To accommodate this, PITA provides a taxonomy, distinguishing between privacy impact types that pertain (i) data subject identity, (ii) data subject treatment, (iii) data subject control and (iv) treatment of personal data. In addition, a pragmatic methodology is presented that leverages both the hierarchical nature of the tree structures and the early ranking of impacts to focus the privacy engineering efforts. Finally, building upon the privacy impact notion as captured in the privacy impact trees, we provide a refinement of the foundational concept of the overall or aggregated 'privacy footprint' of a system. The approach is demonstrated and validated in three complex and contemporary real-world applications, through which we highlight the added value of this tree-based privacy threat analysis approach that refocuses on privacy harms and impacts."
E-PRedictor: an approach for early prediction of pull request acceptance,"Chen, KX; Bao, LF; Hu, X; Xia, X; Yang, XH",10.1007/s11432-022-3953-4,2025,"A pull request (PR) is an event in Git where a contributor asks project maintainers to review code he/she wants to merge into a project. The PR mechanism greatly improves the efficiency of distributed software development in the open-source community. Nevertheless, the massive number of PRs in an open-source software (OSS) project increases the workload of developers. To reduce the burden on developers, many previous studies have investigated factors that affect the chance of PRs getting accepted and built prediction models based on these factors. However, most prediction models are built on the data after PRs are submitted for a while (e.g., comments on PRs), making them not useful in practice. Because integrators still need to spend a large amount of effort on inspecting PRs. In this study, we propose an approach named E-PRedictor (earlier PR predictor) to predict whether a PR will be merged when it is created. E-PRedictor combines three dimensions of manual statistic features (i.e., contributor profile, specific pull request, and project profile) and deep semantic features generated by BERT models based on the description and code changes of PRs. To evaluate the performance of E-PRedictor, we collect 475192 PRs from 49 popular open-source projects on GitHub. The experiment results show that our proposed approach can effectively predict whether a PR will be merged or not. E-PRedictor outperforms the baseline models (e.g., Random Forest and VDCNN) built on manual features significantly. In terms of F1@Merge, F1@Reject, and AUC (area under the receiver operating characteristic curve), the performance of E-PRedictor is 90.1%, 60.5%, and 85.4%, respectively."
CoCoCoLa: Code Completion Control Language,"Nhat; Zaytsev, V",10.1145/3742876.3742883,2025,"In software development, the efficiency and accuracy of code completion systems are crucial for productivity and code-base discovery. From simple spell checkers to advanced AI-powered tools, there are more ways to complete your code than ever. This results in an explosion in the number of possible valid proposals, especially when working with today's increasingly large codebases. Over the years, a lot of effort has been put into developing effective ranking systems to prioritise proposals with more potential. Yet developers still often struggle with an overwhelming number of suggestions, leading to reduced productivity and increased cognitive load. In this paper, instead of just performing completion by name, we propose CoCoCoLa - an alternative approach to give back the control over the presented proposals to the developer. By investigating the recorded code completion events, frequencies of desirable code elements' properties were calculated to identify useful control factors. To avoid adding further complexity to the completion process, we propose a simple language, defined within the boundary of a valid identifier of the 50+ most popular software languages in 2024. This language allows developers to specify and filter for desired properties of the proposals."
Multi-label software requirement smells classification using deep learning,"Alem, AL; Gebretsadik, KK; Mengistie, SA; Admas, MF",10.1038/s41598-025-86673-w,2025,"Software requirement smell detection is an important part of establishing high-quality software specifications. These smells, which frequently indicate difficulties like ambiguity, vagueness, or incompleteness, can lead to misunderstandings and mistakes in the latter phases of software development. Traditionally, identifying requirement smells was a manual process, time-consuming, prone to inconsistency, and human mistakes. Moreover, the previous machine learning and deep learning research was insufficient for detecting multiple smells in a single requirement statement. To address this problem, we developed a multi-label software requirement smell model to detect multiple software requirement smells in a single requirement. Therefore, this study explores a deep learning-based approach to multi-label classification of software requirement smells, incorporating advanced neural network architectures such as LSTM, Bi-LSTM, and GRU with combined word embedding like ELMo and Word2Vec. We collected and prepared an 8120 requirements dataset from different sources categorized into 11 linguistic aspects and we used a binary relevance multi-label classification strategy in which each category was treated independently and used the F1-macro average of each label of the smell. Next, we built models that can classify software requirement smell in a multi-label manner using deep learning algorithms. After executing numerous experiments with different parameters in the Bi-LSTM, LSTM, and GRU models, we obtained 90.3%, 89%, and 88.7% of F1-score macro averages with ELMo, respectively. Therefore, Bi-LSTM achieved a greater F1-score macro average than the other algorithms."
Identification of Intracranial Germ Cell Tumors Based on Facial Photos: Exploratory Study on the Use of Deep Learning for Software Development,"Li, YN; He, YX; Liu, YW; Wang, BC; Li, B; Qiu, XG",10.2196/58760,2025,"Background: Primary intracranial germ cell tumors (iGCTs) are highly malignant brain tumors that predominantly occur in children and adolescents, with an incidence rate ranking third among primary brain tumors in East Asia (8%-15%). Due to their insidious onset and impact on critical functional areas of the brain, these tumors often result in irreversible abnormalities in growth and development, as well as cognitive and motor impairments in affected children. Therefore, early diagnosis through advanced screening techniques is vital for improving patient outcomes and quality of life. Objective: This study aimed to investigate the application of facial recognition technology in the early detection of iGCTs in children and adolescents. Early diagnosis through advanced screening techniques is vital for improving patient outcomes and quality of life. Methods: A multicenter, phased approach was adopted for the development and validation of a deep learning model, GVisageNet, dedicated to the screening of midline brain tumors from normal controls (NCs) and iGCTs from other midline brain tumors. The study comprised the collection and division of datasets into training (n=847, iGCTs=358, NCs=300, other midline brain tumors=189) and testing (n=212, iGCTs=79, NCs=70, other midline brain tumors=63), with an additional independent validation dataset (n=336, iGCTs=130, NCs=100, other midline brain tumors=106) sourced from 4 medical institutions. A regression model using clinically relevant, statistically significant data was developed and combined with GVisageNet outputs to create a hybrid model. This integration sought to assess the incremental value of clinical data. The model's predictive mechanisms were explored through correlation analyses with endocrine indicators and stratified evaluations based on the degree of hypothalamic-pituitary-target axis damage. Performance metrics included area under the curve (AUC), accuracy, sensitivity, and specificity. Results: On the independent validation dataset, GVisageNet achieved an AUC of 0.938 (P<.01) in distinguishing midline brain tumors from NCs. Further, GVisageNet demonstrated significant diagnostic capability in distinguishing iGCTs from the other midline brain tumors, achieving an AUC of 0.739, which is superior to the regression model alone (AUC=0.632, P<.001) but less than the hybrid model (AUC=0.789, P=.04). Significant correlations were found between the GVisageNet's outputs and 7 endocrine indicators. Performance varied with hypothalamic-pituitary-target axis damage, indicating a further understanding of the working mechanism of GVisageNet. Conclusions: GVisageNet, capable of high accuracy both independently and with clinical data, shows substantial potential for early iGCTs detection, highlighting the importance of combining deep learning with clinical insights for personalized health care."
Enhancing software effort estimation with random forest tuning and adaptive decision strategies,"Varshini, AGP; Kumari, KA; Ramakrishnan, S",10.1038/s41598-025-14372-7,2025,"Software Effort estimation (SEE) is a vital task for project management as it is essential for resource allocation and project planning. Numerous algorithms have been investigated for forecasting software effort, yet achieving precise predictions remains a significant hurdle in the software industry. To achieve optimal accuracy, machine learning algorithms are employed. Remarkably, Random Forest (RF) algorithm produced better accuracy when compared with various algorithms. In this paper, the prediction is extended by increasing the number of trees and Improved Random Forest (IRF) is implemented by including three decision techniques such as residual analysis, partial dependence plots and feature engineering to improve prediction accuracy. To make improved random forest to be adaptive, it is further extended in this paper by integrating three techniques such as: Bayesian Optimization with Deep Kernel Learning (BO-DKL) to adaptively set hyperparameters, Time-Series Residual Analysis to detect autocorrelation patterns among model error, and Explainable AI techniques Shapley Additive Explanations (SHAP) and Local Interpretable Model-Agnostic Explanations (LIME) to improve feature interpretability. This Improved Adaptive Random Forest (IARF) mutually contributes to a comprehensive evaluation and improvement of accuracy in prediction. Metrics used for evaluation are Mean Absolute Error (MAE), Root Mean Square Error (RMSE), R-Squared, Mean Absolute Percentage Error (MAPE), Mean Absolute Scaled Error (MASE) and Prediction Interval Coverage Probability (PICP). Overall, the improved adaptive RF model had an average improvement ratio of 18.5% on MAE, 20.3% on RMSE, 3.8% on R2, 5.4% on MAPE, 7% reduction in MASE and a 3-5% improvement in PICP across all data sets compared to the Random Forest model, with much improved prediction accuracy. These findings validate that the combination of adaptive learning methods and explainability-based adjustments considerably improves accuracy of software effort estimation models and facilitates more trustworthy decision-making in software development projects."
Pytester: Deep reinforcement learning for text-to-testcase generation,"Takerngsaksiri, W; Charakorn, R; Tantithamthavorn, C; Li, YF",10.1016/j.jss.2025.112381,2025,"Test-driven development (TDD) is a widely-employed software development practice that mandates writing test cases based on a textual description before writing the actual code. While writing test cases is the centerpiece of TDD, it is time-consuming, expensive, and often shunned by developers. To address these issues associated with TDD, automated test case generation approaches have recently been investigated. Such approaches take source code as input, but not the textual description. Therefore, existing work does not fully support true TDD, as actual code is required to generate test cases. In addition, current deep learning-based test case generation approaches are trained with one learning objective, i.e., to generate test cases that are exactly matched with the ground-truth test cases. However, such approaches may limit the model's ability to generate different yet correct test cases. In this paper, we introduce PYTESTER, a Text-to-Testcase generation approach that can automatically generate syntactically correct, executable, complete, and effective test cases while being aligned with a given textual description. We evaluate PYTESTER on the public APPS benchmark dataset, and the results show that our Deep RL approach enables PYTESTER, a small language model, to outperform much larger language models like GPT3.5, StarCoder, and InCoder. Our findings suggest that future research could consider improving small over large LMs for better resource efficiency by integrating the SE domain knowledge into the design of reinforcement learning architecture."
Tab: template-aware bug report title generation via two-phase fine-tuned models,"Liu, X; Xu, YK; Sun, WF; Huang, NQ; Sun, S; Li, Q; Yang, D; Yan, M",10.1007/s10515-025-00505-9,2025,"Bug reports play a critical role in the software development lifecycle by helping developers identify and resolve defects efficiently. However, the quality of bug report titles, particularly in open-source communities, can vary significantly, which complicates the bug triage and resolution processes. Existing approaches, such as iTAPE, treat title generation as a one-sentence summarization task using sequence-to-sequence models. While these methods show promise, they face two major limitations: (1) they do not consider the distinct components of bug reports, treating the entire report as a homogeneous input, and (2) they struggle to handle the variability between template-based and non-template-based reports, often resulting in suboptimal titles. To address these limitations, we propose TAB, a hybrid framework that combines a Document Component Analyzer based on a pre-trained BERT model and a Title Generation Model based on CodeT5. TAB addresses the first limitation by segmenting bug reports into four components-Description, Reproduction, Expected Behavior, and Others-to ensure better alignment between input and output. For the second limitation, TAB uses a divergent approach: for template-based reports, titles are generated directly, while for non-template reports, DCA extracts key components to improve title relevance and clarity. We evaluate TAB on both template-based and non-template-based bug reports, demonstrating that it significantly outperforms existing methods. Specifically, TAB achieves average improvements of 170.4-389.5% in METEOR, 67.8-190.0% in ROUGE-L, and 65.7-124.5% in chrF(AF) compared to baseline approaches on template-based reports. Additionally, on non-template-based reports, TAB shows an average improvement of 64% in METEOR, 3.6% in ROUGE-L, and 14.8% in chrF(AF) over the state-of-the-art. These results confirm the robustness of TAB in generating high-quality titles across diverse bug report formats."
Instruct or Interact? Exploring and Eliciting LLMs' Capability in Code Snippet Adaptation Through Prompt Engineering,"Zhang, THR; Yu, Y; Mao, XJ; Wang, SW; Yang, K; Lu, Y; Zhang, Z; Zhao, YX",10.1109/ICSE55347.2025.00104,2025,"Code snippet adaptation is a fundamental activity in the software development process. Unlike code generation, code snippet adaptation is not a free creation, which requires developers to tailor a given code snippet in order to fit specific requirements and the code context. Recently, large language models (LLMs) have confirmed their effectiveness in the code generation task with promising results. However, their performance on code snippet adaptation, a reuse-oriented and context-dependent code change prediction task, is still unclear. To bridge this gap, we conduct an empirical study to investigate the performance and issues of LLMs on the adaptation task. We first evaluate the adaptation performances of three popular LLMs and compare them to the code generation task. Our result indicates that their adaptation ability is weaker than generation, with a nearly 15% decrease on pass@1 and more contextrelated errors. By manually inspecting 200 cases, we further investigate the causes of LLMs' sub-optimal performance, which can be classified into three categories, i.e., Unclear Requirement, Requirement Misalignment and Context Misapplication. Based on the above empirical research, we propose an interactive prompting approach to eliciting LLMs' ability on the adaptation task. Specifically, we enhance the prompt by enriching the context and decomposing the task, which alleviates context misapplication and improves requirement understanding. Besides, we enable LLMs' reflection by requiring them to interact with a human or a LLM counselor, compensating for unclear requirement. Our experimental result reveals that our approach greatly improve LLMs' adaptation performance. The best-performing HumanLLM interaction successfully solves 159 out of the 202 identified defects and improves the pass@1 and pass@5 by over 40% compared to the initial instruction-based prompt. Considering human efforts, we suggest multi-agent interaction as a tradeoff, which can achieve comparable performance with excellent generalization ability. We deem that our approach could provide methodological assistance for autonomous code snippet reuse and adaptation with LLMs."
Understanding Flaky Tests Through Linguistic Diversity: A Cross-Language and Comparative Machine Learning Study,"Ahmad, A; Sun, X; Naeem, MR; Javed, Y; Akour, M; Sandahl, K",10.1109/ACCESS.2025.3553626,2025,"Software development is significantly impeded by flaky tests, which intermittently pass or fail without requiring code modifications, resulting in a decline in confidence in automated testing frameworks. Code smells (i.e., test case or production code) are the primary cause of test flakiness. In order to ascertain the prevalence of test smells, researchers and practitioners have examined numerous programming languages. However, one isolated experiment was conducted, which focused solely on one programming language. Across a variety of programming languages, such as Java, Python, C++, Go, and JavaScript, this study examines the predictive accuracy of a variety of machine learning classifiers in identifying flaky tests. We compare the performance of classifiers such as Random Forest, Decision Tree, Naive Bayes, Support Vector Machine, and Logistic Regression in both single-language and cross-language settings. In order to ascertain the impact of linguistic diversity on the flakiness of test cases, models were trained on a single language and subsequently tested on a variety of languages. The following key findings indicate that Random Forest and Logistic Regression consistently outperform other classifiers in terms of accuracy, adaptability, and generalizability, particularly in cross-language environments. Additionally, the investigation contrasts our findings with those of previous research, exhibiting enhanced precision and accuracy in the identification of flaky tests as a result of meticulous classifier selection. We conducted a thorough statistical analysis, which included t-tests, to assess the importance of classifier performance differences in terms of accuracy and F1-score across a variety of programming languages. This analysis emphasizes the substantial discrepancies between classifiers and their effectiveness in detecting flaky tests. The datasets and experiment code utilized in this study are accessible through an open source GitHub repository to facilitate reproducibility is available at: https://github.com/PELAB-LiU/FlakyCrossLanguage. Our results emphasize the effectiveness of probabilistic and ensemble classifiers in improving the reliability of automated testing, despite certain constraints, including the potential biases introduced by language-specific structures and dataset variability. This research provides developers and researchers with practical insights that can be applied to the mitigation of flaky tests in a variety of software environments."
Personality-based pair programming: toward intrinsic motivation alignment in very small entities,"Valovy, M; Buchalcevova, A",10.7717/peerj-cs.2774,2025,"Aim This study explores whether personality-based role assignments (Pilot, Navigator, Solo) can raise intrinsic motivation in pair programming, focusing on designing a framework and process extension for the resource-constrained environment of very small entities (VSEs). Method We employed a mixed-methods design across three quasi-experimental datasets (n = 73 participants), applying linear mixed-effects (LME) modeling to assess motivational outcomes and thematically analyzing (n = 25) interviews for socio-psychological insights. Findings Openness strongly correlates with Pilot roles; Extraversion & Agreeableness favor Navigator roles; and Neuroticism aligns more comfortably with Solo roles-each yielding substantial boosts in intrinsic motivation (up to 60-65%). Twelve qualitative themes underscore the influence of mentorship, pairing constellations, and flow disruptions on developer experiences. Implications Building on these results, we propose the role-optimization motivation alignment (ROMA) framework, mapped to the ISO/IEC 29110 Software Basic Profile and Agile Guidelines, with practical tasks (T1-T7) to facilitate systematic role-trait alignments in small agile teams. Although our data primarily involve Gen-Z undergraduates, the recurring patterns suggest broader applicability, further supported by a separately published application for ongoing generalizability. Conclusion Personality-driven role optimization may significantly enhance collaboration and developer satisfaction in VSEs, though further studies in professional settings and investigations into AI-assisted or distributed pair programming are warranted."
The Virtual Co-design of Sleep Solved - A Case Study of an Educational Sleep App Designed with Teens,"Duffy, A; Bennett, SE; Yardley, L; Moreno, S",10.1007/978-3-031-85575-7_1,2025,"Background: Sleeplessness is an emerging epidemic amongst young people. Numerous apps exist to mediate sleep problems using a variety of CBT-i workshop design approaches. Virtually crowdsourcing co-design, however, provides the promise of rapid and vastly increased data. The rapid co-design of mHealth apps is an important part of the emerging big data, digital health citizen era. Objective: This exploratory case study explored the virtual, crowdsourced co-design of Sleep Solved-an educational mHealth sleep app designed with teens, to learn which virtual methods were used to engage teen co-designers and how these methods can be scaled up. Methods: We conducted an enquiry-based iterative case study utilising the Bayazit 3-stage model. 85 teens participated over 11 months. Data was thematically analysed over several design iterations. Results: Rapid virtual feedback allowed for quick pivots in a short time frame. Four stages of feedback from teens led to iterative changes to scientific information contextualisation and user experience, from lo-fidelity mock-ups through to a coded app beta. Conclusion: The co-design of Sleep Solved exemplified the potential of virtually crowdsourcing teens in mHealth. Key to this evolution will be the ability to leverage big data utilising AI and machine learning approaches to data collation and synthesization, such that meaningful and contextual findings can be applied in line with software development timelines."
Retrieval-Augmented Fine-Tuning for Improving Retrieve-and-Edit Based Assertion Generation,"Li, HY; Sun, WF; Yan, M; Xu, L; Li, Q; Zhang, XH; Zhang, HY",10.1109/TSE.2025.3558403,2025,"Unit Testing is crucial in software development and maintenance, aiming to verify that the implemented functionality is consistent with the expected functionality. A unit test is composed of two parts: a test prefix, which drives the unit under test to a particular state, and a test assertion, which determines what the expected behavior is under that state. To reduce the effort of conducting unit tests manually, Yu et al. proposed an integrated approach (integration for short), combining information retrieval with a deep learning-based approach to generate assertions for test prefixes, and obtained promising results. In our previous work, we found that the overall performance of integration is mainly due to its success in retrieving assertions. Moreover, integration is limited to specific types of edit operations and struggles to understand the semantic differences between the retrieved focal-test (focal-test includes a test prefix and a unit under test) and the input focal-test. Based on these insights, we then proposed a retrieve-and-edit approach named EditAS to learn the assertion edit patterns to improve the effectiveness of assertion generation in our prior study. Despite being promising, we find that the effectiveness of EditAS can be further improved. Our analysis shows that: (1) The editing ability of EditAS still has ample room for improvement. Its performance degrades as the edit distance between the retrieval assertion and ground truth increases. Specifically, the average accuracy of EditAS is 12.38% when the edit distance is greater than 5. (2) EditAS lacks a fine-grained semantic understanding of both the retrieved focal-test and the input focal-test themselves, which leads to many inaccurate token modifications. In particular, an average of 25.57% of the incorrectly generated assertions that need to be modified are not modified, and an average of 6.45% of the assertions that match the ground truth are still modified. Thanks to pre-trained models employing pre-training paradigms on large-scale data, they tend to have good semantic comprehension and code generation abilities. In light of this, we propose EditAS(2), which improves retrieval-and-edit based assertion generation through retrieval-augmented fine-tuning. Specifically, EditAS(2) first retrieves a similar focal-test from a predefined corpus and treats its assertion as a prototype. Then, EditAS(2) uses a pre-trained model, CodeT5, to learn the semantics of the input and similar focal-tests as well as assertion editing patterns to automatically edit the prototype. We first evaluate the EditAS(2) for its inference performance on two large-scale datasets, and the experimental results show that EditAS(2) outperforms state-of-the-art assertion generation methods and pre-trained models, with average performance improvements of 15.93%-129.19% and 11.01%-68.88% in accuracy and CodeBLEU, respectively. We also evaluate the performance of EditAS(2) in detecting real-world bugs from Defects4J. The experimental results indicate that EditAS(2) achieves the best bug detection performance among all the methods."
Formal Verification for Preventing Misconfigured Access Policies in Kubernetes Clusters,"Sissodiya, A; Chiquito, E; Bodin, U; Kristiansson, J",10.1109/ACCESS.2025.3597504,2025,"Kubernetes clusters now underpin the bulk of modern production workloads, recent 2024 Cloud Native Computing Foundation surveys report >96% enterprise adoption, stretching from 5G edge nodes and AI/ML pipelines to heavily-regulated fintech and healthcare back-ends. Every action in those environments funnels through the API server, so a single access-control slip can jeopardise an entire fleet. Yet most deployments still rely on a patchwork of Role-Based Access Control (RBAC) rules and policy-as-code admission controllers such as OPA Gatekeeper or Kyverno. In practice these controls are brittle: minor syntactic oversights, wildcard privileges, or conflicting rules can silently create privilege-escalation paths that elude linters and manual review. This paper presents a framework that models both RBAC and admission policies as first-order logic and uses an SMT solver to exhaustively search for counter-examples to stated security invariants before policies reach the cluster. The approach detects policy conflicts, unreachable denies, and unintended permissions. Three real-world case studies are presented to illustrate how the framework reveals latent misconfigurations and validates the soundness of the corrected rules. These case studies include a supply-chain image bypass, an RBAC shadow-admi escalation, and a multi-tenant namespace breach. To aid replication and further study, we release a fully scripted GitHub testbed: a Minikube cluster, AuthzForce PDP, admission-webhook adapter, and Z3-backed CLI that recreates each scenario and verifies policies end-to-end. While the framework does not address runtime threats, it closes a critical verification gap and substantially raises the bar for attackers targeting the most widely deployed orchestration platform."
Exploring Large Language Models for Analyzing Open Source License Conflicts: How Far Are We?,"Cui, X; Wu, JZ; Ling, X; Luo, TY; Yang, MT; Ou, WX",10.1109/ICSE-Companion66252.2025.00083,2025,"With the rapid growth of the open source software (OSS) ecosystem, the use of open source has become the predominant model for contemporary software development. OSS licenses define the conditions for the reuse, distribution, and modification of OSS and form the foundation of the open source ecosystem. However, recent research shows that over half (53%) of OSS software experiences license conflicts, adversely affecting the sustainability of OSS and community collaboration and leading to significant legal risks. Researchers propose various methods for detecting license conflicts, yet these approaches face challenges such as limited license coverage and insufficient model accuracy. The recent emergence of large language models (LLMs) offers new opportunities for license conflict detection. However, there remains a lack of in-depth and systematic research on utilizing LLMs for this purpose. To address this challenge, we propose L(3)icNexus, an effective tool for automatically detecting license conflicts using LLMs. Specifically, L(3)icNexus employs a joint labeling method based on embedded model label inference and expert verification and constructs a domain dataset consisting of 3,238 OSS licenses. Subsequently, L(3)icNexus proposes the AdaFine approach, combining Domain-Adaptive Pre-Training (DAPT) and Supervised Fine-Tuning (SFT), resulting in the License-Llama3-8B model. This model identifies terms, infers OSS license attitudes, and autonomously understands licenses end-to-end. Finally, L(3)icNexus generates summaries of the rights and obligations associated with licenses using License-Llama3-8B, and detects conflicts by extracting the license hierarchy of OSS. Experimental results demonstrate that L(3)icNexus achieves an F1-score of 85.58% in license term and attitude recognition, surpassing the best results of other methods by 20.69%. Moreover, an empirical study conducted on license conflict detection for 500 popular GitHub projects reveals that L(3)icNexus achieves a false positive rate of 5.88% and a false negative rate of 2.47%. The performance of L(3)icNexus exceeds that of existing state-of-the-art methods, illustrating the potential of LLMs in addressing license conflict detection. We summarize the insights from this research and release the OSS license dataset and License-Llama3-8B model weights on Hugging Face to encourage further exploration in related fields (Dataset available: this URL; Model available: this URL)."
What Types of Automated Tests do Developers Write?,"Ivankovic, M; Rimanic, L; Budiselic, I; Petrovic, G; Fraser, G; Just, R",10.1109/AST66626.2025.00015,2025,"Software testing is a widely adopted quality assurance technique that assesses whether a software system meets a given specification. The overall goal of software testing is to develop effective tests that capture desired program behaviors and reveal defects. Automated software testing is an essential part of modern software development processes, in particular those that focus on continuous integration and deployment. Existing test classifications (e.g., unit vs. integration vs. system tests) and testing best practices offer general conceptual frameworks, but instantiating these conceptual models requires a definition of what is considered a unit, or even a test. These conceptual models are rarely explicated in the literature or documentation which makes interpretation and generalization of results (e.g., comparisons between unit and integration testing efficacy) difficult. Additionally, comparatively little is known about how developers operationalize software testing in modern industrial contexts, how they write and automate software tests, and how well those tests fit into existing classifications. Since software engineering processes have substantially evolved, it is time to revisit and refine test classifications to support future research on software testing efficacy and best practices. This is especially important with the advent of AI-generated test code, where those classifications may be used to automatically classify the types of generated tests or to formulate the desired test output. This paper presents a novel test classification framework, developed using insights and data on what types of tests developers write in practice. The data was collected in an industrial setting at Google and involves tens of thousands of developers and tens of millions of tests. The developed classification framework is precise enough that it can be encoded in an automated analysis. We describe our proof-of-concept implementation and report on the development approach and costs. We also report on the results of applying the automated classification to all tests in Google's repository and on what types of automated tests developers write."
Evolving funding strategies for research software: Insights from an international survey of research funders,"Jensen, EA; Katz, DS",10.1371/journal.pone.0329833,2025,"Contemporary research heavily depends on software. Research software, comprising source code, algorithms, scripts, computational workflows, and executables produced during or specifically for research, is crucial in advancing scholarly knowledge. However, this aspect of contemporary research can only thrive if research funders effectively support it. This survey study of international research funders addresses the research questions: 1) How do international funders currently support research software? 2) What challenges are funders aiming to tackle with their research software programs? 3) How successful do funders think their programs are? Survey results reveal a variegated funding landscape for research software, encompassing open-source projects, open-science tools, discipline-specific add-ons, infrastructure software, data science and AI tools, and general technology projects that include research software. Funders reported working to integrate research software into their formal definitions of research, codified in funding models and policies. Funders have been working to revise policies and adopt international frameworks such as ADORE.software to acknowledge research software's role better. Respondents described innovative funding models designed to support research software more effectively than traditional research funding mechanisms. Supporting Research Software Engineers (RSEs) was another priority. Funding programs aimed to provide financial support, career development, and recognition for RSEs. Fostering collaboration between RSEs and other researchers was a less prevalent but noteworthy target for research funders. Promoting open-science principles and open source software development and maintenance was prioritized by research funders with targeted policies and programs. Overall, the reported initiatives aimed to ensure long-term research software accessibility, sustainability, and impact, with robust community engagement helping to contribute to a more effective research ecosystem. Finally, where funding programs for research software have been running for long enough to make an assessment, these efforts were overwhelmingly viewed as successful by the research funder representatives in our study."
Validation of Automated Standardization Performance for ECDaim Software Developed Using a Taiwan-Specific Database,"Ni, YC; Yang, HC; Hsiao, IT; Tseng, FP; Lin, CY; Lin, WB; Lyu, ZJ",10.1007/s40846-025-00941-8,2025,"Purpose This study evaluates the difference between automatic standardization for the ECDaim platform and expert manual standardization and validates the applicability of the automated method for SPECT brain perfusion images for neurodegenerative diseases. Methods The ECDaim platform is employed to compare automatic and expert manual standardization methods for Tc-99 m-ECD SPECT images using data from a local dementia database (390 subjects: NC, AD, LBD, and VaD). All images are subject to spatial normalization and quantitative evaluation using the Hausdorff Distance (HD) and the Dice Coefficient (DC). Whole-brain and small-brain regions, such as the hippocampus, posterior cingulate cortex and precuneus, are analyzed. Differences in uptake ratios for the four brain lobes are measured to determine the performance in terms of quantitative imaging. Statistical parametric mapping (SPM) is used to compare group differences and FWE correction is applied to visualize significant voxel-wise differences between AD and control groups. Results Whole-brain analysis shows that the HD values for automatic and manual standardization methods are less than 5 mm for all four groups (NC, AD, LBD, and VaD) and the volume overlap (DC) exceeds 0.95, so there are minimal differences between methods. For small brain regions, HD values are less than 2 mm and DC values range from 0.75 to 0.92, with lower values for the PCC. Quantitative measurements of regional uptake ratios for different brain lobes show that the percentage error for all groups is less than 2%. Two-sample t-tests to compare the AD and NC groups show similar spatial distributions for the two standardization methods in terms of voxel-wise statistical results. Conclusion This study uses HD and DC to determine the consistency between automatic and expert manual standardization of Tc-99 m-ECD SPECT images and shows that results are similar for both. For the 390 images that are studied, less than 5% are excluded as outliers, so the automatic method is robust and applicable for different disease groups. These results demonstrate that the ECDaim platform unifies the process of imaging analysis and may reduce variations for different manual procedures, improve diagnostic consistency and allow the integration of AI models for clinical decision support."
An adaptive model for cross-domain code search,"Fang, MG; Wang, L; Hu, HZ",10.1016/j.infsof.2025.107827,2025,"Context: Research on code search is one of the important research directions in the field of computer science. As software scales continue to grow and complexity increases, developers need to frequently search for and understand existing code in their daily work. Code search research aims to enhance the efficiency and accuracy of code search, including aspects such as natural language-based code search, code similarity comparison, code recommendation systems, and more. By delving into code search technologies, developers can more swiftly locate and comprehend the code they need, thereby boosting the efficiency and quality of software development. Objective: However, the reliance of deep learning-based code search models on large datasets and the substantial time needed to acquire model parameters can impose substantial economic costs. Furthermore, such models have certain limitations in their adaptability and perform sub-optimally when applied to a new dataset (i.e., Cross-Domain code search). Methods: To address these issues, we propose an Adaptive Cross-Domain code search model based on Self-Attention (ACD-SA), which is the first attempt to introduce a self-attention model into cross-domain code search. First, the fastText word embedding tool is employed to obtain the initial vector. Second, self-attention is utilized to effectively characterize the internal structure information of the initial vector to obtain the feature vector and model parameters. Next, a word matching matrix is constructed from the feature vectors to generate the initial grammatical information vector. Subsequently, a long-short term memory network (LSTM) is utilized to train the initial grammatical information vector and extract grammatical patterns. Finally, cross-domain code search analysis is performed by combining domain-specific word matching matrices and grammar patterns. Results: To verify the effectiveness of ACD-SA in cross-domain code search studies, an experimental comparative analysis is conducted on a training dataset and a target dataset. In comparison to existing baseline models, such as CodeHow, DeepCS, BAVE, and AdaCS, the experimental results demonstrate that ACD-SA yields superior results for Hit@2, Hit@3, Hit@5, Hit@10, and MRR. Conclusion: By analyzing the defects and shortcomings of existing methods in cross-domain code search, the article proposes an ACD-SA cross-domain code search model.ACD-SA only needs to be trained on large datasets and the model is applied to code search applications on domain-specific datasets. On the one hand, ACD-SA solves the problem that traditional code search needs to spend a lot of time on the collection or crawling of large datasets and the training of model parameters in each search task. On the other hand, ACD-SA makes up for the singularity of the existing code search model for dataset adaptation and realizes cross-domain code search."
The QuADRANT project: Enhancing quality and safety in radiological procedures through clinical audit,"Brusadin, G; Brady, AP; Hierath, M; Howlett, DC",10.1016/j.canrad.2025.104689,2025,"Purpose. - The project entitled Quality Improvement Through Clinical Audit in Diagnostic (Including Interventional) Radiology, Radiotherapy and Nuclear Medicine (Including Therapies) (QuADRANT) was conceived to thoroughly assess the state of clinical audit implementation across Europe, regarding the medical application of ionizing radiation. The central aim was to elevate the quality and safety standards in radiological procedures. This initiative arose from the understanding that consistent and rigorous clinical audit is fundamental for enhancing patient outcomes and adhering to safety regulations in cancer treatment using radiation. The project aimed to address the observed variations in how clinical audit was being applied across Europe. Material and methods. - The QuADRANT project employed a comprehensive methodology to gather data. This included an extensive review of existing literature, European regulations, and international guidelines pertinent to clinical audit in radiological procedures. National surveys were conducted, reaching out to representatives from European Union member states and other selected countries to collect information on national frameworks, methodologies, existing barriers, and facilitating factors. In-depth case studies, involving site visits and interviews with various stakeholders such as healthcare professionals and policymakers, provided deeper insights. Expert consultations with specialists in medical physics, radiation oncology, and quality assurance further enriched the data. Workshops were also held to engage national stakeholders in discussions about legal requirements, benefits, and good practices of clinical audit. The collected data was systematically analysed to identify key trends, common challenges, successful strategies, and areas ripe for improvement, leading to the formulation of evidence-based recommendations. Results. - The project observed significant variations in the adoption and maturity of clinical audit programs across European countries, even within individual nations. While some countries demonstrated well-established practices, particularly concerning dosimetry audits in radiotherapy, a widespread and comprehensive clinical audit program covering all radiological procedures was often lacking. Data collection for audit purposes was frequently inconsistent, with common deviations from standard practice noted in patient data registration, diagnosis, treatment details, and the recording of adverse events. Many countries reported limited financial and human resources allocated to clinical audit. A general lack of awareness among healthcare professionals regarding the specific requirements and benefits of clinical audit was also identified. Organizational culture sometimes presented resistance to systematic quality improvement initiatives. Challenges were noted in data quality and the burden associated with manual data entry for audit purposes. Furthermore, a lack of central coordination at the national level often resulted in fragmented audit efforts. Discussion. - The observed variations in clinical audit implementation highlight the necessity for a more harmonized and robust approach across Europe. The absence of standardized methodologies impedes effective benchmarking and the sharing of best practices among institutions and countries. The resource limitations and lack of awareness underscore a need for increased investment and targeted education campaigns to promote a culture of continuous quality improvement. The identified data collection inconsistencies indicate a critical area for intervention, suggesting the need for improved digital infrastructure and standardized reporting mechanisms to facilitate meaningful audits. For radiotherapy specifically, the findings emphasize that while some aspects like dosimetry are well-audited, a broader, more systematic audit of the entire treatment pathway, including advanced technologies and multidisciplinary team interactions, is often underdeveloped. Conclusion. - The QuADRANT project successfully provided a comprehensive overview of clinical audit practices in radiological procedures throughout Europe, identifying both achievements and substantial shortcomings. It underscored the critical need for strengthening national infrastructure, increasing resource allocation, and elevating the prioritisation of clinical audit. The project outcomes were published by the European Commission as part of Radiation Protection Series. Furthermore, the project's insights were instrumental in shaping the European Commission's recommendations on clinical audits establishing a crucial framework for harmonizing national clinical audit systems. The ongoing impact of the project is evident in subsequent initiatives like the Clinical Audit Implementation in Europe (CLAUD-IT) project on improving clinical audit practice in European Union member states radiological procedures. (c) 2025 Societe franc, aise de radiotherapie oncologique (SFRO). Published by Elsevier Masson SAS. All rights are reserved, including those for text and data mining, AI training, and similar technologies."
Application of Generative AI in Software Development,"Chan, J; Peko, G; Sundaram, D; Hassna, G",,2025,
